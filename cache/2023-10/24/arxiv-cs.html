<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri 20 Oct 23  to  Mon 23 Oct 23, announced Tue, 24 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item724">Cross-lists</a></li>
<li><a href="#item796">Replacements</a></li>
</ul>
<small>[ total of 1324 entries:  <b>1-1324</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 24 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13697" title="Abstract">arXiv:2310.13697</a> [<a href="/pdf/2310.13697" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sitting on a gold mine: the story of the process industry&#x27;s automatic  formation of a digital twin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azangoo%2C+M">Mohammad Azangoo</a>, 
<a href="/search/cs?searchtype=author&query=Sierla%2C+S">Seppo Sierla</a>, 
<a href="/search/cs?searchtype=author&query=Vyatkin%2C+V">Valeriy Vyatkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The use of a software tool chain to generate Digital Twins (DTs)
automatically can speed up digitization and lower development costs.
Engineering documents and system data are just two examples of source
information that can be used to generate a DT. After proposing a general plan
for semi-automatic generation of a DT for a process system, this work describe
our efforts to extract necessary information for the generation of a DT of a
process system from existing information in a factory floor like piping and
instrumentation diagrams (P&amp;IDs). To extract initial raw model data, techniques
such as image, pattern, and text recognition can be used, and then an
intermediate graph model can be generated and modified based on requirements.
In order to increase the system's adaptability and reliability, this research
will delve deeper into the steps involved in creating and manipulating an
intermediate graph model.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13698" title="Abstract">arXiv:2310.13698</a> [<a href="/pdf/2310.13698" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ego-perspective enhanced fitness training experience of AR Try to Move  game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, 2 tables, 2023 International Conference on Machine Learning and Automation (CONF-MLA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">AR, a recent emerging technology, has been widely used in entertainment to
provide users with immersive, interactive, and, sometimes, engaging
experiences. The process of rehabilitation treatment and motor training process
is often boring, and it is well known that users' exercise efficiency is often
not as efficient as in a rehabilitation institution. Thus far, there is no
effective upper limb sports rehabilitation training game based on the
ego-perspective. Hence, with the objective of enhancing the enjoyment
experience in rehabilitation and more effective remote rehabilitation training,
this work aims to provide an AR Try to Move game and a convolutional neural
network (CNN) for identifying and classifying user gestures from a
self-collected AR multiple interactive gestures dataset. Utilizing an AR game
scoring system, users are incentivized to enhance their upper limb muscle
system through remote training with greater effectiveness and convenience.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13699" title="Abstract">arXiv:2310.13699</a> [<a href="/pdf/2310.13699" title="Download PDF">pdf</a>, <a href="/format/2310.13699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interaction in Metaverse: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Z">Zirun Gan</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+W">Wensheng Gan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhenlian Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuehua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Human-computer interaction (HCI) emerged with the birth of the computer and
has been upgraded through decades of development. Metaverse has attracted a lot
of interest with its immersive experience, and HCI is the entrance to the
Metaverse for people. It is predictable that HCI will determine the immersion
of the Metaverse. However, the technologies of HCI in Metaverse are not mature
enough. There are many issues that we should address for HCI in the Metaverse.
To this end, the purpose of this paper is to provide a systematic literature
review on the key technologies and applications of HCI in the Metaverse. This
paper is a comprehensive survey of HCI for the Metaverse, focusing on current
technology, future directions, and challenges. First, we provide a brief
overview of HCI in the Metaverse and their mutually exclusive relationships.
Then, we summarize the evolution of HCI and its future characteristics in the
Metaverse. Next, we envision and present the key technologies involved in HCI
in the Metaverse. We also review recent case studies of HCI in the Metaverse.
Finally, we highlight several challenges and future issues in this promising
area.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13700" title="Abstract">arXiv:2310.13700</a> [<a href="/pdf/2310.13700" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Heritage: An Open-Source Multiplatform AR Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Green%2C+C">Corrie Green</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">AI NeRF algorithms, capable of cloud processing, have significantly reduced
hardware requirements and processing efficiency in photogrammetry pipelines.
This accessibility has unlocked the potential for museums, charities, and
cultural heritage sites worldwide to leverage mobile devices for artifact
scanning and processing. However, the adoption of augmented reality platforms
often necessitates the installation of proprietary applications on users'
mobile devices, which adds complexity to development and limits global
availability. This paper presents a case study that demonstrates a
cost-effective pipeline for visualizing scanned museum artifacts using mobile
augmented reality, leveraging an open-source embedded solution on a website.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13701" title="Abstract">arXiv:2310.13701</a> [<a href="/pdf/2310.13701" title="Download PDF">pdf</a>, <a href="/format/2310.13701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment and treatment of visuospatial neglect using active learning  with Gaussian processes regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Boi%2C+I">Ivan De Boi</a>, 
<a href="/search/cs?searchtype=author&query=Embrechts%2C+E">Elissa Embrechts</a>, 
<a href="/search/cs?searchtype=author&query=Schatteman%2C+Q">Quirine Schatteman</a>, 
<a href="/search/cs?searchtype=author&query=Penne%2C+R">Rudi Penne</a>, 
<a href="/search/cs?searchtype=author&query=Truijen%2C+S">Steven Truijen</a>, 
<a href="/search/cs?searchtype=author&query=Saeys%2C+W">Wim Saeys</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Visuospatial neglect is a disorder characterised by impaired awareness for
visual stimuli located in regions of space and frames of reference. It is often
associated with stroke. Patients can struggle with all aspects of daily living
and community participation. Assessment methods are limited and show several
shortcomings, considering they are mainly performed on paper and do not
implement the complexity of daily life. Similarly, treatment options are sparse
and often show only small improvements. We present an artificial intelligence
solution designed to accurately assess a patient's visuospatial neglect in a
three-dimensional setting. We implement an active learning method based on
Gaussian process regression to reduce the effort it takes a patient to undergo
an assessment. Furthermore, we describe how this model can be utilised in
patient oriented treatment and how this opens the way to gamification,
tele-rehabilitation and personalised healthcare, providing a promising avenue
for improving patient engagement and rehabilitation outcomes. To validate our
assessment module, we conducted clinical trials involving patients in a
real-world setting. We compared the results obtained using our AI-based
assessment with the widely used conventional visuospatial neglect tests
currently employed in clinical practice. The validation process serves to
establish the accuracy and reliability of our model, confirming its potential
as a valuable tool for diagnosing and monitoring visuospatial neglect. Our VR
application proves to be more sensitive, while intra-rater reliability remains
high.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13702" title="Abstract">arXiv:2310.13702</a> [<a href="/pdf/2310.13702" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Swarm Intelligence (CSI) Enables Rapid Group Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosenberg%2C+L">Louis Rosenberg</a>, 
<a href="/search/cs?searchtype=author&query=Willcox%2C+G">Gregg Willcox</a>, 
<a href="/search/cs?searchtype=author&query=Schumann%2C+H">Hans Schumann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Copyright 2023 IEEE. arXiv admin note: substantial text overlap with <a href="/abs/2309.12366">arXiv:2309.12366</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">When generating insights from human groups, conversational deliberation is a
key method for exploring issues, surfacing ideas, debating options, and
converging on solutions. Unfortunately, real-time conversations are difficult
to scale, losing effectiveness in groups above 4 to 7 members. Conversational
Swarm Intelligence (CSI) is a new technology that enables large human groups to
hold real-time conversations using techniques modeled on the dynamics of
biological swarms. Through a novel use of Large Language Models (LLMs), CSI
enables real-time dialog among small groups while simultaneously fostering
content propagation across a much larger group. This combines the benefits of
small-scale deliberative reasoning and large-scale groupwise intelligence. In
this study, we engage a group of 81 American voters from one political party in
real-time deliberation using a CSI platform called Thinkscape. We then task the
group with (a) forecasting which candidate from a set of options will achieve
the most national support, and (b) indicating the specific reasons for this
result. After only six minutes of deliberation, the group of 81 individuals
converged on a selected candidate and surfaced over 400 reasons justifying
various candidates, including 206 justifications that supported the selected
candidate. We find that the selected candidate was significantly more supported
by group members than the other options (p&lt;0.001) and that this effect held
even after six minutes of deliberation, demonstrating that CSI provides both
the qualitative benefits of conversational focus groups and the quantitative
benefits of largescale polling.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13703" title="Abstract">arXiv:2310.13703</a> [<a href="/pdf/2310.13703" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Designing a Medication Notification Application with Multi-Channel  Reminders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chanane%2C+N">Nawal Chanane</a>, 
<a href="/search/cs?searchtype=author&query=Mirza%2C+F">Farhaan Mirza</a>, 
<a href="/search/cs?searchtype=author&query=Naeem%2C+M+A">M. Asif Naeem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Evidence suggests that medication adherence applications (apps) are one of
the most effective methods to remind patients to take medication on time.
Reminders via apps are overwhelming today, consumers discard using them after a
brief period of initial usage, eventually becoming unfavourable towards them
and not using them at all. This study aims to qualitatively determine the key
features and design of medication reminder apps that facilitate or disrupt
usage from the users perceptive. Three focus groups were conducted with
participants aged between 15 and 65+ (N= 12). The participants evaluated a
smart medication reminder prototype, then sketched and discussed their thoughts
and perceptions within the group. Participants identified, 1) Multi-channel
reminders, 2) Medication intake acknowledgement for reporting and 3) Seamless
addition of medications and associated reminders as important elements.
Understanding consumers needs and concerns will inform the future development
of medication reminder apps that are acceptable and valuable to consumers.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13704" title="Abstract">arXiv:2310.13704</a> [<a href="/pdf/2310.13704" title="Download PDF">pdf</a>, <a href="/ps/2310.13704" title="Download PostScript">ps</a>, <a href="/format/2310.13704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> My Machine and I: ChatGPT and the Future of Human-Machine Collaboration  in Africa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oguine%2C+M+B">Munachimso Blessing Oguine</a>, 
<a href="/search/cs?searchtype=author&query=Oguine%2C+C+G">Chidera Godsfavor Oguine</a>, 
<a href="/search/cs?searchtype=author&query=Oguine%2C+K+J">Kanyifeechukwu Jane Oguine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Recent advancements in technology have necessitated a paradigm shift in the
people use technology necessitating a new research field called Human-Machine
collaboration. ChatGPT, an Artificial intelligence (AI) assistive technology,
has gained mainstream adoption and implementation in academia and industry;
however, a lot is left unknown about how this new technology holds for
Human-Machine Collaboration in Africa. Our survey paper highlights to answer
some of these questions. To understand the effectiveness of ChatGPT on
human-machine collaboration we utilized reflexive thematic analysis to analyze
(N= 51) articles between 2019 and 2023 obtained from our literature search. Our
findings indicate the prevalence of ChatGPT for human-computer interaction
within academic sectors such as education, and research; trends also revealed
the relatively high effectiveness of ChatGPT in improving human-machine
collaboration.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13705" title="Abstract">arXiv:2310.13705</a> [<a href="/pdf/2310.13705" title="Download PDF">pdf</a>, <a href="/format/2310.13705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large language models in textual analysis for gesture selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hensel%2C+L+B">Laura B. Hensel</a>, 
<a href="/search/cs?searchtype=author&query=Yongsatianchot%2C+N">Nutchanon Yongsatianchot</a>, 
<a href="/search/cs?searchtype=author&query=Torshizi%2C+P">Parisa Torshizi</a>, 
<a href="/search/cs?searchtype=author&query=Minucci%2C+E">Elena Minucci</a>, 
<a href="/search/cs?searchtype=author&query=Marsella%2C+S">Stacy Marsella</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 25th ACM International Conference on Multimodal Interaction. ICMI.
  2023. 1-10
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Gestures perform a variety of communicative functions that powerfully
influence human face-to-face interaction. How this communicative function is
achieved varies greatly between individuals and depends on the role of the
speaker and the context of the interaction. Approaches to automatic gesture
generation vary not only in the degree to which they rely on data-driven
techniques but also the degree to which they can produce context and speaker
specific gestures. However, these approaches face two major challenges: The
first is obtaining sufficient training data that is appropriate for the context
and the goal of the application. The second is related to designer control to
realize their specific intent for the application. Here, we approach these
challenges by using large language models (LLMs) to show that these powerful
models of large amounts of data can be adapted for gesture analysis and
generation. Specifically, we used ChatGPT as a tool for suggesting
context-specific gestures that can realize designer intent based on minimal
prompts. We also find that ChatGPT can suggests novel yet appropriate gestures
not present in the minimal training data. The use of LLMs is a promising avenue
for gesture generation that reduce the need for laborious annotations and has
the potential to flexibly and quickly adapt to different designer intents.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13707" title="Abstract">arXiv:2310.13707</a> [<a href="/pdf/2310.13707" title="Download PDF">pdf</a>, <a href="/format/2310.13707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoLinter: A Linting Framework for Choropleth Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+F">Fan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+A">Arlen Fan</a>, 
<a href="/search/cs?searchtype=author&query=MacEachren%2C+A+M">Alan M. MacEachren</a>, 
<a href="/search/cs?searchtype=author&query=Maciejewski%2C+R">Ross Maciejewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in IEEE Transactions on Visualization and Computer Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Visualization linting is a proven effective tool in assisting users to follow
established visualization guidelines. Despite its success, visualization
linting for choropleth maps, one of the most popular visualizations on the
internet, has yet to be investigated. In this paper, we present GeoLinter, a
linting framework for choropleth maps that assists in creating accurate and
robust maps. Based on a set of design guidelines and metrics drawing upon a
collection of best practices from the cartographic literature, GeoLinter
detects potentially suboptimal design decisions and provides further
recommendations on design improvement with explanations at each step of the
design process. We perform a validation study to evaluate the proposed
framework's functionality with respect to identifying and fixing errors and
apply its results to improve the robustness of GeoLinter. Finally, we
demonstrate the effectiveness of the GeoLinter - validated through empirical
studies - by applying it to a series of case studies using real-world datasets.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13709" title="Abstract">arXiv:2310.13709</a> [<a href="/pdf/2310.13709" title="Download PDF">pdf</a>, <a href="/format/2310.13709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redefining Access to Large Audiovisual Archives through Embodied  Experiences in Immersive Environments: Creativity &amp; Cognition 2022 --  Graduate Student Symposium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alliata%2C+G">Giacomo Alliata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 14th Conference on Creativity and Cognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Audiovisual archives are the mnemonic archives of the 21st century, with
important cultural institutions increasingly digitizing their video
collections. However, these remain mostly inaccessible, due to the sheer amount
of content combined with the lack of innovative forms of engagement through
compelling frameworks for their exploration. The present research therefore
aims at redefining access to large video collections through embodied
experiences in immersive environments. The author claims that, once users are
empowered to be actors of the experience rather than mere spectators, their
creativity is stimulated and narrative can emerge.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13710" title="Abstract">arXiv:2310.13710</a> [<a href="/pdf/2310.13710" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Creation and Humanization of Digital Life: Consciousness  Simulation and Human-Machine Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qikang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Digital life, a form of life generated by computer programs or artificial
intelligence systems, it possesses self-awareness, thinking abilities,
emotions, and subjective consciousness. Achieving it involves complex neural
networks, multi-modal sensory integration [1, 2], feedback mechanisms, and
self-referential processing [3]. Injecting prior knowledge into digital life
structures is a critical step. It guides digital entities' understanding of the
world, decision-making, and interactions. We can customize and personalize
digital life, it includes adjusting intelligence levels, character settings,
personality traits, and behavioral characteristics. Virtual environments
facilitate efficient and controlled development, allowing user interaction,
observation, and active participation in digital life's growth. Researchers
benefit from controlled experiments, driving technological advancements. The
fusion of digital life into the real world offers exciting possibilities for
human-digital entity collaboration and coexistence.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13711" title="Abstract">arXiv:2310.13711</a> [<a href="/pdf/2310.13711" title="Download PDF">pdf</a>, <a href="/format/2310.13711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Sensor-free Affect Detection: A Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Morais%2C+F">Felipe de Morais</a>, 
<a href="/search/cs?searchtype=author&query=Goldoni%2C+D">Di&#xf3;gines Goldoni</a>, 
<a href="/search/cs?searchtype=author&query=Kautzmann%2C+T">Tiago Kautzmann</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+R">Rodrigo da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Jaques%2C+P+A">Patricia A. Jaques</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Emotions and other affective states play a pivotal role in cognition and,
consequently, the learning process. It is well-established that computer-based
learning environments (CBLEs) that can detect and adapt to students' affective
states can enhance learning outcomes. However, practical constraints often pose
challenges to the deployment of sensor-based affect detection in CBLEs,
particularly for large-scale or long-term applications. As a result,
sensor-free affect detection, which exclusively relies on logs of students'
interactions with CBLEs, emerges as a compelling alternative. This paper
provides a comprehensive literature review on sensor-free affect detection. It
delves into the most frequently identified affective states, the methodologies
and techniques employed for sensor development, the defining attributes of
CBLEs and data samples, as well as key research trends. Despite the field's
evident maturity, demonstrated by the consistent performance of the models and
the application of advanced machine learning techniques, there is ample scope
for future research. Potential areas for further exploration include enhancing
the performance of sensor-free detection models, amassing more samples of
underrepresented emotions, and identifying additional emotions. There is also a
need to refine model development practices and methods. This could involve
comparing the accuracy of various data collection techniques, determining the
optimal granularity of duration, establishing a shared database of action logs
and emotion labels, and making the source code of these models publicly
accessible. Future research should also prioritize the integration of models
into CBLEs for real-time detection, the provision of meaningful interventions
based on detected emotions, and a deeper understanding of the impact of
emotions on learning.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13712" title="Abstract">arXiv:2310.13712</a> [<a href="/pdf/2310.13712" title="Download PDF">pdf</a>, <a href="/format/2310.13712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Guidance and Interaction Strategies for LLM Use on Learner  Performance and Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+H">Harsh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Musabirov%2C+I">Ilya Musabirov</a>, 
<a href="/search/cs?searchtype=author&query=Reza%2C+M">Mohi Reza</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiakai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Kuzminykh%2C+A">Anastasia Kuzminykh</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J+J">Joseph Jay Williams</a>, 
<a href="/search/cs?searchtype=author&query=Liut%2C+M">Michael Liut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Personalized chatbot-based teaching assistants can be crucial in addressing
increasing classroom sizes, especially where direct teacher presence is
limited. Large language models (LLMs) offer a promising avenue, with increasing
research exploring their educational utility. However, the challenge lies not
only in establishing the efficacy of LLMs but also in discerning the nuances of
interaction between learners and these models, which impact learners'
engagement and results. We conducted a formative study in an undergraduate
computer science classroom (N=145) and a controlled experiment on Prolific
(N=356) to explore the impact of four pedagogically informed guidance
strategies and the interaction between student approaches and LLM responses.
Direct LLM answers marginally improved performance, while refining student
solutions fostered trust. Our findings suggest a nuanced relationship between
the guidance provided and LLM's role in either answering or refining student
input. Based on our findings, we provide design recommendations for optimizing
learner-LLM interactions.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13713" title="Abstract">arXiv:2310.13713</a> [<a href="/pdf/2310.13713" title="Download PDF">pdf</a>, <a href="/format/2310.13713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subjective visualization experiences: impact of visual design and  experimental design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koesten%2C+L">Laura Koesten</a>, 
<a href="/search/cs?searchtype=author&query=Dimmery%2C+D">Drew Dimmery</a>, 
<a href="/search/cs?searchtype=author&query=Gleicher%2C+M">Michael Gleicher</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+T">Torsten M&#xf6;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In contrast to objectively measurable aspects (such as accuracy, reading
speed, or memorability), the subjective experience of visualizations has only
recently gained importance, and we have less experience how to measure it. We
explore how subjective experience is affected by chart design using multiple
experimental methods. We measure the effects of changes in color, orientation,
and source annotation on the perceived readability and trustworthiness of
simple bar charts. Three different experimental designs (single image rating,
forced choice comparison, and semi-structured interviews) provide similar but
different results. We find that these subjective experiences are different from
what prior work on objective dimensions would predict. Seemingly
inconsequential choices, like orientation, have large effects for some methods,
indicating that study design alters decision-making strategies. Next to
insights into the effect of chart design, we provide methodological insights,
such as a suggested need to carefully isolate individual elements in charts to
study subjective experiences.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13714" title="Abstract">arXiv:2310.13714</a> [<a href="/pdf/2310.13714" title="Download PDF">pdf</a>, <a href="/format/2310.13714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A study of the impact of generative AI-based data augmentation on  software metadata classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumari%2C+T">Tripti Kumari</a>, 
<a href="/search/cs?searchtype=author&query=Charan%2C+C+S">Chakali Sai Charan</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Ayan Das</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents the system submitted by the team from IIT(ISM) Dhanbad in
FIRE IRSE 2023 shared task 1 on the automatic usefulness prediction of
code-comment pairs as well as the impact of Large Language Model(LLM) generated
data on original base data towards an associated source code. We have developed
a framework where we train a machine learning-based model using the neural
contextual representations of the comments and their corresponding codes to
predict the usefulness of code-comments pair and performance analysis with
LLM-generated data with base data. In the official assessment, our system
achieves a 4% increase in F1-score from baseline and the quality of generated
data.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13715" title="Abstract">arXiv:2310.13715</a> [<a href="/pdf/2310.13715" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Deception: Generative Artificial Intelligence in Social  Engineering and Phishing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmitt%2C+M">Marc Schmitt</a>, 
<a href="/search/cs?searchtype=author&query=Flechais%2C+I">Ivan Flechais</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The advancement of Artificial Intelligence (AI) and Machine Learning (ML) has
profound implications for both the utility and security of our digital
interactions. This paper investigates the transformative role of Generative AI
in Social Engineering (SE) attacks. We conduct a systematic review of social
engineering and AI capabilities and use a theory of social engineering to
identify three pillars where Generative AI amplifies the impact of SE attacks:
Realistic Content Creation, Advanced Targeting and Personalization, and
Automated Attack Infrastructure. We integrate these elements into a conceptual
model designed to investigate the complex nature of AI-driven SE attacks - the
Generative AI Social Engineering Framework. We further explore human
implications and potential countermeasures to mitigate these risks. Our study
aims to foster a deeper understanding of the risks, human implications, and
countermeasures associated with this emerging paradigm, thereby contributing to
a more secure and trustworthy human-computer interaction.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13717" title="Abstract">arXiv:2310.13717</a> [<a href="/pdf/2310.13717" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quand le jeu vid&#xe9;o est le catalyseur d&#x27;exp&#xe9;rimentations  th&#xe9;&#xe2;trales (2014-2019)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gagner%C3%A9%2C+G">Georges Gagner&#xe9;</a> (INREV, AIAC, UP8, UPL), 
<a href="/search/cs?searchtype=author&query=Plessiet%2C+C">C&#xe9;dric Plessiet</a> (INREV, AIAC, UP8, UPL)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in French language
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> {\'E}ditions du Centre de Recherche et d'Histoire Inter-m{\'e}dias
  (CHRI). Le jeu vid{\'e}o au carrefour de l'histoire, des arts et des
  m{\'e}dias, pp.209-219, 2023, 978-2-494077-00-3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">The meeting between the director and researcher Georges Gagner{\'e} and the
digital artist and researcher C\'edric Plessiet at Paris 8 University through
experiments crossing live performance and video game led to the development of
tools from game techniques video and dedicated to theatrical experimentation on
a mixed stage involving physical actors and avatars. The genesis of
AvatarStaging setup and AKN_Regie tool is detailed through the various
research-creations on which they were formalized and used. AKN_Regie is a
plugin of the videogame engine Unreal Engine, programmed in blueprint, and
dedicated to real time 3D previsualization and avatar direction.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13718" title="Abstract">arXiv:2310.13718</a> [<a href="/pdf/2310.13718" title="Download PDF">pdf</a>, <a href="/format/2310.13718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Workflow Approach to Visualization-Based Storytelling with Cultural  Heritage Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liem%2C+J">Johannes Liem</a>, 
<a href="/search/cs?searchtype=author&query=Kusnick%2C+J">Jakob Kusnick</a>, 
<a href="/search/cs?searchtype=author&query=Beck%2C+S">Samuel Beck</a>, 
<a href="/search/cs?searchtype=author&query=Windhager%2C+F">Florian Windhager</a>, 
<a href="/search/cs?searchtype=author&query=Mayr%2C+E">Eva Mayr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> VIS4DH 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Stories are as old as human history - and a powerful means for the engaging
communication of information, especially in combination with visualizations.
The InTaVia project is built on this intersection and has developed a platform
which supports the workflow of cultural heritage experts to create compelling
visualization-based stories: From the search for relevant cultural objects and
actors in a cultural knowledge graph, to the curation and visual analysis of
the selected information, and to the creation of stories based on these data
and visualizations, which can be shared with the interested public.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13719" title="Abstract">arXiv:2310.13719</a> [<a href="/pdf/2310.13719" title="Download PDF">pdf</a>, <a href="/format/2310.13719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of ELO Rating Scheme in MOBA Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yuhan Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is not intended for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">ELO rating system is proposed by Arpad Elo, a Hungarian-American physics
professor. Originally, it was proposed for the ranking system of chess players,
but it was soon adapted to many other zero-sum sports fields like football,
baseball, basketball , etc. Nowadays, besides the traditional sports games,
computer/video games are also playing an important role in social lives
especially among the teenagers. In most of the online competition games,
player's performance is usually scored and recorded by the game's ranking
system. Meanwhile, ranking system like ladder in Dota is not the only the
metric for the players to evaluate their gaming strength, an ELO rating score
based on players in-game performance is also a decisive factor for gamers'
matching. Namely, the matching system will refer to players' score in the
ranking system and performance score system to ensure the matched players will
promisingly undergo a balanced game without one team dramatically overwhelming
the other. ELO scheme and its variants in modern online competition games aims
to ensuring the expected winning rate for each team approaches 50\%. However,
ELO rating is also causing compliments among players. In this research, I will
dig into the advantages and drawbacks of leveraging ELO ranking system in
online games and why it is still employed by game developers despite the fact
that it is disliked by most of the players. Also, a new effort based rating
scheme will be proposed and compared with ELO scheme under the simulation
environment.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13720" title="Abstract">arXiv:2310.13720</a> [<a href="/pdf/2310.13720" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eye Tracking for Tele-robotic Surgery: A Comparative Evaluation of  Head-worn Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%BCter%2C+R">Regine B&#xfc;ter</a>, 
<a href="/search/cs?searchtype=author&query=Soberanis-Mukul%2C+R+D">Roger D. Soberanis-Mukul</a>, 
<a href="/search/cs?searchtype=author&query=Puentes%2C+P+R">Paola Ruiz Puentes</a>, 
<a href="/search/cs?searchtype=author&query=Ghazi%2C+A">Ahmed Ghazi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J+Y">Jie Ying Wu</a>, 
<a href="/search/cs?searchtype=author&query=Unberath%2C+M">Mathias Unberath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Purpose: Metrics derived from eye-gaze-tracking and pupillometry show promise
for cognitive load assessment, potentially enhancing training and patient
safety through user-specific feedback in tele-robotic surgery. However, current
eye-tracking solutions' effectiveness in tele-robotic surgery is uncertain
compared to everyday situations due to close-range interactions causing extreme
pupil angles and occlusions. To assess the effectiveness of modern
eye-gaze-tracking solutions in tele-robotic surgery, we compare the Tobii Pro 3
Glasses and Pupil Labs Core, evaluating their pupil diameter and gaze stability
when integrated with the da Vinci Research Kit (dVRK). Methods: The study
protocol includes a nine-point gaze calibration followed by pick-and-place task
using the dVRK and is repeated three times. After a final calibration, users
view a 3x3 grid of AprilTags, focusing on each marker for 10 seconds, to
evaluate gaze stability across dVRK-screen positions with the L2-norm.
Different gaze calibrations assess calibration's temporal deterioration due to
head movements. Pupil diameter stability is evaluated using the FFT from the
pupil diameter during the pick-and-place tasks. Users perform this routine with
both head-worn eye-tracking systems. Results: Data collected from ten users
indicate comparable pupil diameter stability. FFTs of pupil diameters show
similar amplitudes in high-frequency components. Tobii Glasses show more
temporal gaze stability compared to Pupil Labs, though both eye trackers yield
a similar 4cm error in gaze estimation without an outdated calibration.
Conclusion: Both eye trackers demonstrate similar stability of the pupil
diameter and gaze, when the calibration is not outdated, indicating comparable
eye-tracking and pupillometry performance in tele-robotic surgery settings.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13721" title="Abstract">arXiv:2310.13721</a> [<a href="/pdf/2310.13721" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feminist epistemology for machine learning systems design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klumbyte%2C+G">Goda Klumbyte</a>, 
<a href="/search/cs?searchtype=author&query=Piehl%2C+H">Hannah Piehl</a>, 
<a href="/search/cs?searchtype=author&query=Draude%2C+C">Claude Draude</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Workshop A Toolbox of Feminist Wonder - Theories and methods that can make a difference, Conference on Computer-Supported Cooperative Work and Social Computing CSCW 23, October 14-18, 2023, Minneapolis, MN, USA, 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This paper presents a series of feminist epistemological concepts as tools
for developing critical, more accountable, and contextualised approaches to
machine learning systems design. Namely, we suggest that the methods of
situated knowledges or situating, figurations or figuring, diffraction or
diffracting, and critical fabulation or speculation can be productively
actualised in the field of machine learning systems design. We also suggest
that the meta-method for doing this actualisation requires not so much
translation but transposition - a creative and critical adaptation to speak to
machine learning contexts.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13724" title="Abstract">arXiv:2310.13724</a> [<a href="/pdf/2310.13724" title="Download PDF">pdf</a>, <a href="/format/2310.13724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puig%2C+X">Xavier Puig</a>, 
<a href="/search/cs?searchtype=author&query=Undersander%2C+E">Eric Undersander</a>, 
<a href="/search/cs?searchtype=author&query=Szot%2C+A">Andrew Szot</a>, 
<a href="/search/cs?searchtype=author&query=Cote%2C+M+D">Mikael Dallaire Cote</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tsung-Yen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Partsey%2C+R">Ruslan Partsey</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+R">Ruta Desai</a>, 
<a href="/search/cs?searchtype=author&query=Clegg%2C+A+W">Alexander William Clegg</a>, 
<a href="/search/cs?searchtype=author&query=Hlavac%2C+M">Michal Hlavac</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S+Y">So Yeon Min</a>, 
<a href="/search/cs?searchtype=author&query=Vondru%C5%A1%2C+V">Vladim&#xed;r Vondru&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Gervet%2C+T">Theophile Gervet</a>, 
<a href="/search/cs?searchtype=author&query=Berges%2C+V">Vincent-Pierre Berges</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+J+M">John M. Turner</a>, 
<a href="/search/cs?searchtype=author&query=Maksymets%2C+O">Oleksandr Maksymets</a>, 
<a href="/search/cs?searchtype=author&query=Kira%2C+Z">Zsolt Kira</a>, 
<a href="/search/cs?searchtype=author&query=Kalakrishnan%2C+M">Mrinal Kalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>, 
<a href="/search/cs?searchtype=author&query=Chaplot%2C+D+S">Devendra Singh Chaplot</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+U">Unnat Jain</a>, 
<a href="/search/cs?searchtype=author&query=Batra%2C+D">Dhruv Batra</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+A">Akshara Rai</a>, 
<a href="/search/cs?searchtype=author&query=Mottaghi%2C+R">Roozbeh Mottaghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="http://aihabitat.org/habitat3">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Multiagent Systems (cs.MA); Robotics (cs.RO)

</div>
<p class="mathjax">We present Habitat 3.0: a simulation platform for studying collaborative
human-robot tasks in home environments. Habitat 3.0 offers contributions across
three dimensions: (1) Accurate humanoid simulation: addressing challenges in
modeling complex deformable bodies and diversity in appearance and motion, all
while ensuring high simulation speed. (2) Human-in-the-loop infrastructure:
enabling real human interaction with simulated robots via mouse/keyboard or a
VR interface, facilitating evaluation of robot policies with human input. (3)
Collaborative tasks: studying two collaborative tasks, Social Navigation and
Social Rearrangement. Social Navigation investigates a robot's ability to
locate and follow humanoid avatars in unseen environments, whereas Social
Rearrangement addresses collaboration between a humanoid and robot while
rearranging a scene. These contributions allow us to study end-to-end learned
and heuristic baselines for human-robot collaboration in-depth, as well as
evaluate them with humans in the loop. Our experiments demonstrate that learned
robot policies lead to efficient task completion when collaborating with unseen
humanoid agents and human partners that might exhibit behaviors that the robot
has not seen before. Additionally, we observe emergent behaviors during
collaborative task execution, such as the robot yielding space when obstructing
a humanoid agent, thereby allowing the effective completion of the task by the
humanoid agent. Furthermore, our experiments using the human-in-the-loop tool
demonstrate that our automated evaluation with humanoids can provide an
indication of the relative ordering of different policies when evaluated with
real human collaborators. Habitat 3.0 unlocks interesting new features in
simulators for Embodied AI, and we hope it paves the way for a new frontier of
embodied human-AI interaction capabilities.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13725" title="Abstract">arXiv:2310.13725</a> [<a href="/pdf/2310.13725" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing drug and cell line representations via contrastive learning  for improved anti-cancer drug prioritization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+P+J">Patrick J. Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=D%2C+X+N+P">Xia Ning Ph.D</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, 4 figures, 4 tables, 11 supplementary tables, 1 supplementary note, submitted to Nature Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Due to cancer's complex nature and variable response to therapy, precision
oncology informed by omics sequence analysis has become the current standard of
care. However, the amount of data produced for each patients makes it difficult
to quickly identify the best treatment regimen. Moreover, limited data
availability has hindered computational methods' abilities to learn patterns
associated with effective drug-cell line pairs. In this work, we propose the
use of contrastive learning to improve learned drug and cell line
representations by preserving relationship structures associated with drug
mechanism of action and cell line cancer types. In addition to achieving
enhanced performance relative to a state-of-the-art method, we find that
classifiers using our learned representations exhibit a more balances reliance
on drug- and cell line-derived features when making predictions. This
facilitates more personalized drug prioritizations that are informed by signals
related to drug resistance.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13730" title="Abstract">arXiv:2310.13730</a> [<a href="/pdf/2310.13730" title="Download PDF">pdf</a>, <a href="/format/2310.13730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Localizing and Editing Knowledge in Text-to-Image Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Samyadeep Basu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+N">Nanxuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Morariu%2C+V">Vlad Morariu</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Manjunatha%2C+V">Varun Manjunatha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have
achieved unprecedented quality of photorealism with state-of-the-art FID scores
on MS-COCO and other generation benchmarks. Given a caption, image generation
requires fine-grained knowledge about attributes such as object structure,
style, and viewpoint amongst others. Where does this information reside in
text-to-image generative models? In our paper, we tackle this question and
understand how knowledge corresponding to distinct visual attributes is stored
in large-scale text-to-image diffusion models. We adapt Causal Mediation
Analysis for text-to-image models and trace knowledge about distinct visual
attributes to various (causal) components in the (i) UNet and (ii) text-encoder
of the diffusion model. In particular, we show that unlike generative
large-language models, knowledge about different attributes is not localized in
isolated components, but is instead distributed amongst a set of components in
the conditional UNet. These sets of components are often distinct for different
visual attributes. Remarkably, we find that the CLIP text-encoder in public
text-to-image models such as Stable-Diffusion contains only one causal state
across different visual attributes, and this is the first self-attention layer
corresponding to the last subject token of the attribute in the caption. This
is in stark contrast to the causal states in other language models which are
often the mid-MLP layers. Based on this observation of only one causal state in
the text-encoder, we introduce a fast, data-free model editing method
Diff-QuickFix which can effectively edit concepts in text-to-image models.
DiffQuickFix can edit (ablate) concepts in under a second with a closed-form
update, providing a significant 1000x speedup and comparable editing
performance to existing fine-tuning based editing methods.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13746" title="Abstract">arXiv:2310.13746</a> [<a href="/pdf/2310.13746" title="Download PDF">pdf</a>, <a href="/format/2310.13746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairBranch: Fairness Conflict Correction on Task-group Branches for Fair  Multi-Task Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Arjun Roy</a>, 
<a href="/search/cs?searchtype=author&query=Koutlis%2C+C">Christos Koutlis</a>, 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+S">Symeon Papadopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Ntoutsi%2C+E">Eirini Ntoutsi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The generalization capacity of Multi-Task Learning (MTL) becomes limited when
unrelated tasks negatively impact each other by updating shared parameters with
conflicting gradients, resulting in negative transfer and a reduction in MTL
accuracy compared to single-task learning (STL). Recently, there has been an
increasing focus on the fairness of MTL models, necessitating the optimization
of both accuracy and fairness for individual tasks. Similarly to how negative
transfer affects accuracy, task-specific fairness considerations can adversely
influence the fairness of other tasks when there is a conflict of fairness loss
gradients among jointly learned tasks, termed bias transfer. To address both
negative and bias transfer in MTL, we introduce a novel method called
FairBranch. FairBranch branches the MTL model by assessing the similarity of
learned parameters, grouping related tasks to mitigate negative transfer.
Additionally, it incorporates fairness loss gradient conflict correction
between adjoining task-group branches to address bias transfer within these
task groups. Our experiments in tabular and visual MTL problems demonstrate
that FairBranch surpasses state-of-the-art MTL methods in terms of both
fairness and accuracy.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13747" title="Abstract">arXiv:2310.13747</a> [<a href="/pdf/2310.13747" title="Download PDF">pdf</a>, <a href="/format/2310.13747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALDi: Quantifying the Arabic Level of Dialectness of Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keleg%2C+A">Amr Keleg</a>, 
<a href="/search/cs?searchtype=author&query=Goldwater%2C+S">Sharon Goldwater</a>, 
<a href="/search/cs?searchtype=author&query=Magdy%2C+W">Walid Magdy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Transcribed speech and user-generated text in Arabic typically contain a
mixture of Modern Standard Arabic (MSA), the standardized language taught in
schools, and Dialectal Arabic (DA), used in daily communications. To handle
this variation, previous work in Arabic NLP has focused on Dialect
Identification (DI) on the sentence or the token level. However, DI treats the
task as binary, whereas we argue that Arabic speakers perceive a spectrum of
dialectness, which we operationalize at the sentence level as the Arabic Level
of Dialectness (ALDi), a continuous linguistic variable. We introduce the
AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences
(17% from news articles and 83% from user comments on those articles) which are
manually labeled with their level of dialectness. We provide a detailed
analysis of AOC-ALDi and show that a model trained on it can effectively
identify levels of dialectness on a range of other corpora (including dialects
and genres not included in AOC-ALDi), providing a more nuanced picture than
traditional DI systems. Through case studies, we illustrate how ALDi can reveal
Arabic speakers' stylistic choices in different situations, a useful property
for sociolinguistic analyses.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13754" title="Abstract">arXiv:2310.13754</a> [<a href="/pdf/2310.13754" title="Download PDF">pdf</a>, <a href="/format/2310.13754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating sleep-stage classification: how age and early-late sleep  affects classification performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moris%2C+E">Eugenia Moris</a>, 
<a href="/search/cs?searchtype=author&query=Larrabide%2C+I">Ignacio Larrabide</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Sleep stage classification is a common method used by experts to monitor the
quantity and quality of sleep in humans, but it is a time-consuming and
labour-intensive task with high inter- and intra-observer variability. Using
Wavelets for feature extraction and Random Forest for classification, an
automatic sleep-stage classification method was sought and assessed. The age of
the subjects, as well as the moment of sleep (early-night and late-night), were
confronted to the performance of the classifier. From this study, we observed
that these variables do affect the automatic model performance, improving the
classification of some sleep stages and worsening others.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13759" title="Abstract">arXiv:2310.13759</a> [<a href="/pdf/2310.13759" title="Download PDF">pdf</a>, <a href="/format/2310.13759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-label Open-set Audio Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+S">Sripathi Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Cartwright%2C+M">Mark Cartwright</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the Workshop on Detection and Classification of Acoustic Scenes and Events, 2023 (DCASE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Current audio classification models have small class vocabularies relative to
the large number of sound event classes of interest in the real world. Thus,
they provide a limited view of the world that may miss important yet unexpected
or unknown sound events. To address this issue, open-set audio classification
techniques have been developed to detect sound events from unknown classes.
Although these methods have been applied to a multi-class context in audio,
such as sound scene classification, they have yet to be investigated for
polyphonic audio in which sound events overlap, requiring the use of
multi-label models. In this study, we establish the problem of multi-label
open-set audio classification by creating a dataset with varying unknown class
distributions and evaluating baseline approaches built upon existing
techniques.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13760" title="Abstract">arXiv:2310.13760</a> [<a href="/pdf/2310.13760" title="Download PDF">pdf</a>, <a href="/format/2310.13760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Abstractiveness of Summarization Models through Calibrated  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hwanjun Song</a>, 
<a href="/search/cs?searchtype=author&query=Shalyminov%2C+I">Igor Shalyminov</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Siffi Singh</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Kaisheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+S">Saab Mansour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP-Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Sequence-level knowledge distillation reduces the size of Seq2Seq models for
more efficient abstractive summarization. However, it often leads to a loss of
abstractiveness in summarization. In this paper, we propose a novel approach
named DisCal to enhance the level of abstractiveness (measured by n-gram
overlap) without sacrificing the informativeness (measured by ROUGE) of
generated summaries. DisCal exposes diverse pseudo summaries with two
supervision to the student model. Firstly, the best pseudo summary is
identified in terms of abstractiveness and informativeness and used for
sequence-level distillation. Secondly, their ranks are used to ensure the
student model to assign higher prediction scores to summaries with higher
ranks. Our experiments show that DisCal outperforms prior methods in
abstractive summarization distillation, producing highly abstractive and
informative summaries.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13766" title="Abstract">arXiv:2310.13766</a> [<a href="/pdf/2310.13766" title="Download PDF">pdf</a>, <a href="/format/2310.13766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-BEV: Height-aware Bird&#x27;s-Eye-View Segmentation and Neural Map-based  Relocalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Camiletto%2C+A+B">Andrea Boscolo Camiletto</a>, 
<a href="/search/cs?searchtype=author&query=Bochicchio%2C+A">Alfredo Bochicchio</a>, 
<a href="/search/cs?searchtype=author&query=Liniger%2C+A">Alexander Liniger</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Dengxin Dai</a>, 
<a href="/search/cs?searchtype=author&query=Gawel%2C+A">Abel Gawel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Efficient relocalization is essential for intelligent vehicles when GPS
reception is insufficient or sensor-based localization fails. Recent advances
in Bird's-Eye-View (BEV) segmentation allow for accurate estimation of local
scene appearance and in turn, can benefit the relocalization of the vehicle.
However, one downside of BEV methods is the heavy computation required to
leverage the geometric constraints. This paper presents U-BEV, a U-Net inspired
architecture that extends the current state-of-the-art by allowing the BEV to
reason about the scene on multiple height layers before flattening the BEV
features. We show that this extension boosts the performance of the U-BEV by up
to 4.11 IoU. Additionally, we combine the encoded neural BEV with a
differentiable template matcher to perform relocalization on neural SD-map
data. The model is fully end-to-end trainable and outperforms transformer-based
BEV methods of similar computational complexity by 1.7 to 2.8 mIoU and
BEV-based relocalization by over 26% Recall Accuracy on the nuScenes dataset.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13767" title="Abstract">arXiv:2310.13767</a> [<a href="/pdf/2310.13767" title="Download PDF">pdf</a>, <a href="/format/2310.13767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph AI in Medicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+R">Ruth Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M+M">Michelle M. Li</a>, 
<a href="/search/cs?searchtype=author&query=Noori%2C+A">Ayush Noori</a>, 
<a href="/search/cs?searchtype=author&query=Queen%2C+O">Owen Queen</a>, 
<a href="/search/cs?searchtype=author&query=Zitnik%2C+M">Marinka Zitnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In clinical artificial intelligence (AI), graph representation learning,
mainly through graph neural networks (GNNs), stands out for its capability to
capture intricate relationships within structured clinical datasets. With
diverse data -- from patient records to imaging -- GNNs process data
holistically by viewing modalities as nodes interconnected by their
relationships. Graph AI facilitates model transfer across clinical tasks,
enabling models to generalize across patient populations without additional
parameters or minimal re-training. However, the importance of human-centered
design and model interpretability in clinical decision-making cannot be
overstated. Since graph AI models capture information through localized neural
transformations defined on graph relationships, they offer both an opportunity
and a challenge in elucidating model rationale. Knowledge graphs can enhance
interpretability by aligning model-driven insights with medical knowledge.
Emerging graph models integrate diverse data modalities through pre-training,
facilitate interactive feedback loops, and foster human-AI collaboration,
paving the way to clinically meaningful predictions.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13768" title="Abstract">arXiv:2310.13768</a> [<a href="/pdf/2310.13768" title="Download PDF">pdf</a>, <a href="/format/2310.13768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PACE: Human and Camera Motion Estimation from in-the-wild Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kocabas%2C+M">Muhammed Kocabas</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Molchanov%2C+P">Pavlo Molchanov</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yunrong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+M+J">Michael J. Black</a>, 
<a href="/search/cs?searchtype=author&query=Hilliges%2C+O">Otmar Hilliges</a>, 
<a href="/search/cs?searchtype=author&query=Kautz%2C+J">Jan Kautz</a>, 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+U">Umar Iqbal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV 2024. Project page: <a href="https://nvlabs.github.io/PACE/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a method to estimate human motion in a global scene from moving
cameras. This is a highly challenging task due to the coupling of human and
camera motions in the video. To address this problem, we propose a joint
optimization framework that disentangles human and camera motions using both
foreground human motion priors and background scene features. Unlike existing
methods that use SLAM as initialization, we propose to tightly integrate SLAM
and human motion priors in an optimization that is inspired by bundle
adjustment. Specifically, we optimize human and camera motions to match both
the observed human pose and scene features. This design combines the strengths
of SLAM and motion priors, which leads to significant improvements in human and
camera motion estimation. We additionally introduce a motion prior that is
suitable for batch optimization, making our approach significantly more
efficient than existing approaches. Finally, we propose a novel synthetic
dataset that enables evaluating camera motion in addition to human motion from
dynamic videos. Experiments on the synthetic and real-world RICH datasets
demonstrate that our approach substantially outperforms prior art in recovering
both human and camera motions.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13771" title="Abstract">arXiv:2310.13771</a> [<a href="/pdf/2310.13771" title="Download PDF">pdf</a>, <a href="/format/2310.13771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copyright Violations and Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karamolegkou%2C+A">Antonia Karamolegkou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%B8gaard%2C+A">Anders S&#xf8;gaard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models may memorize more than just facts, including entire chunks of
texts seen during training. Fair use exemptions to copyright laws typically
allow for limited use of copyrighted material without permission from the
copyright holder, but typically for extraction of information from copyrighted
materials, rather than {\em verbatim} reproduction. This work explores the
issue of copyright violations and large language models through the lens of
verbatim memorization, focusing on possible redistribution of copyrighted text.
We present experiments with a range of language models over a collection of
popular books and coding problems, providing a conservative characterization of
the extent to which language models can redistribute these materials. Overall,
this research highlights the need for further examination and the potential
impact on future developments in natural language processing to ensure
adherence to copyright regulations. Code is at
\url{https://github.com/coastalcph/CopyrightLLMs}.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13772" title="Abstract">arXiv:2310.13772</a> [<a href="/pdf/2310.13772" title="Download PDF">pdf</a>, <a href="/format/2310.13772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+T">Tianshi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kreis%2C+K">Karsten Kreis</a>, 
<a href="/search/cs?searchtype=author&query=Fidler%2C+S">Sanja Fidler</a>, 
<a href="/search/cs?searchtype=author&query=Sharp%2C+N">Nicholas Sharp</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+K">Kangxue Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Videos and more results on <a href="https://research.nvidia.com/labs/toronto-ai/texfusion/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (2023) 4169-4181
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present TexFusion (Texture Diffusion), a new method to synthesize textures
for given 3D geometries, using large-scale text-guided image diffusion models.
In contrast to recent works that leverage 2D text-to-image diffusion models to
distill 3D objects using a slow and fragile optimization process, TexFusion
introduces a new 3D-consistent generation technique specifically designed for
texture synthesis that employs regular diffusion model sampling on different 2D
rendered views. Specifically, we leverage latent diffusion models, apply the
diffusion model's denoiser on a set of 2D renders of the 3D object, and
aggregate the different denoising predictions on a shared latent texture map.
Final output RGB textures are produced by optimizing an intermediate neural
color field on the decodings of 2D renders of the latent texture. We thoroughly
validate TexFusion and show that we can efficiently generate diverse, high
quality and globally coherent textures. We achieve state-of-the-art text-guided
texture synthesis performance using only image diffusion models, while avoiding
the pitfalls of previous distillation-based methods. The text-conditioning
offers detailed control and we also do not rely on any ground truth 3D textures
for training. This makes our method versatile and applicable to a broad range
of geometry and texture types. We hope that TexFusion will advance AI-based
texturing of 3D assets for applications in virtual reality, game design,
simulation, and more.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13774" title="Abstract">arXiv:2310.13774</a> [<a href="/pdf/2310.13774" title="Download PDF">pdf</a>, <a href="/format/2310.13774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seq2seq is All You Need for Coreference Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wiseman%2C+S">Sam Wiseman</a>, 
<a href="/search/cs?searchtype=author&query=Stratos%2C+K">Karl Stratos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing works on coreference resolution suggest that task-specific models
are necessary to achieve state-of-the-art performance. In this work, we present
compelling evidence that such models are not necessary. We finetune a
pretrained seq2seq transformer to map an input document to a tagged sequence
encoding the coreference annotation. Despite the extreme simplicity, our model
outperforms or closely matches the best coreference systems in the literature
on an array of datasets. We also propose an especially simple seq2seq approach
that generates only tagged spans rather than the spans interleaved with the
original text. Our analysis shows that the model size, the amount of
supervision, and the choice of sequence representations are key factors in
performance.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13775" title="Abstract">arXiv:2310.13775</a> [<a href="/pdf/2310.13775" title="Download PDF">pdf</a>, <a href="/ps/2310.13775" title="Download PostScript">ps</a>, <a href="/format/2310.13775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The second-order zero differential spectra of some APN and other maps  over finite fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+K">Kirpa Garg</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+S+U">Sartaj Ul Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Riera%2C+C">Constanza Riera</a>, 
<a href="/search/cs?searchtype=author&query=Stanica%2C+P">Pantelimon Stanica</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR); Number Theory (math.NT)

</div>
<p class="mathjax">The Feistel Boomerang Connectivity Table and the related notion of
$F$-Boomerang uniformity (also known as the second-order zero differential
uniformity) has been recently introduced by Boukerrou et al.~\cite{Bouk}. These
tools shall provide a major impetus in the analysis of the security of the
Feistel network-based ciphers. In the same paper, a characterization of almost
perfect nonlinear functions (APN) over fields of even characteristic in terms
of second-order zero differential uniformity was also given. Here, we find a
sufficient condition for an odd or even function over fields of odd
characteristic to be an APN function, in terms of second-order zero
differential uniformity. Moreover, we compute the second-order zero
differential spectra of several APN or other low differential uniform
functions, and show that our considered functions also have low second-order
zero differential uniformity, though it may vary widely, unlike the case for
even characteristic when it is always zero.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13778" title="Abstract">arXiv:2310.13778</a> [<a href="/pdf/2310.13778" title="Download PDF">pdf</a>, <a href="/ps/2310.13778" title="Download PostScript">ps</a>, <a href="/format/2310.13778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Properties in Computation Tree Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+R">Rajarshi Roy</a>, 
<a href="/search/cs?searchtype=author&query=Neider%2C+D">Daniel Neider</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">We consider the problem of automatically inferring specifications in the
branching-time logic, Computation Tree Logic (CTL), from a given system.
Designing functional and usable specifications has always been one of the
biggest challenges of formal methods. While in recent years, works have focused
on automatically designing specifications in linear-time logics such as Linear
Temporal Logic (LTL) and Signal Temporal Logic (STL), little attention has been
given to branching-time logics despite its popularity in formal methods. We
intend to infer concise (thus, interpretable) CTL formulas from a given finite
state model of the system in consideration. However, inferring specification
only from the given model (and, in general, from only positive examples) is an
ill-posed problem. As a result, we infer a CTL formula that, along with being
concise, is also language-minimal, meaning that it is rather specific to the
given model. We design a counter-example guided algorithm to infer a concise
and language-minimal CTL formula via the generation of undesirable models. In
the process, we also develop, for the first time, a passive learning algorithm
to infer CTL formulas from a set of desirable and undesirable Kripke
structures. The passive learning algorithm involves encoding a popular CTL
model-checking procedure in the Boolean Satisfiability problem.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13780" title="Abstract">arXiv:2310.13780</a> [<a href="/pdf/2310.13780" title="Download PDF">pdf</a>, <a href="/format/2310.13780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Modular Framework for Implicit 3D-0D Coupling in Cardiac Mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brown%2C+A+L">Aaron L. Brown</a>, 
<a href="/search/math?searchtype=author&query=Salvador%2C+M">Matteo Salvador</a>, 
<a href="/search/math?searchtype=author&query=Shi%2C+L">Lei Shi</a>, 
<a href="/search/math?searchtype=author&query=Pfaller%2C+M+R">Martin R. Pfaller</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+Z">Zinan Hu</a>, 
<a href="/search/math?searchtype=author&query=Harold%2C+K+E">Kaitlin E. Harold</a>, 
<a href="/search/math?searchtype=author&query=Hsiai%2C+T">Tzung Hsiai</a>, 
<a href="/search/math?searchtype=author&query=Vedula%2C+V">Vijay Vedula</a>, 
<a href="/search/math?searchtype=author&query=Marsden%2C+A+L">Alison L. Marsden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">In numerical simulations of cardiac mechanics, coupling the heart to a model
of the circulatory system is essential for capturing physiological cardiac
behavior. A popular and efficient technique is to use an electrical circuit
analogy, known as a lumped parameter network or zero-dimensional (0D) fluid
model, to represent blood flow throughout the cardiovascular system. Due to the
strong physical interaction between the heart and the blood circulation,
developing accurate and efficient numerical coupling methods remains an active
area of research. In this work, we present a modular framework for implicitly
coupling three-dimensional (3D) finite element simulations of cardiac mechanics
to 0D models of blood circulation. The framework is modular in that the
circulation model can be modified independently of the 3D finite element
solver, and vice versa. The numerical scheme builds upon a previous work that
combines 3D blood flow models with 0D circulation models (3D fluid - 0D fluid).
Here, we extend it to couple 3D cardiac tissue mechanics models with 0D
circulation models (3D structure - 0D fluid), showing that both mathematical
problems can be solved within a unified coupling scheme. The effectiveness,
temporal convergence, and computational cost of the algorithm are assessed
through multiple examples relevant to the cardiovascular modeling community.
Importantly, in an idealized left ventricle example, we show that the coupled
model yields physiological pressure-volume loops and naturally recapitulates
the isovolumic contraction and relaxation phases of the cardiac cycle without
any additional numerical techniques. Furthermore, we provide a new derivation
of the scheme inspired by the Approximate Newton Method of Chan (1985),
explaining how the proposed numerical scheme combines the stability of
monolithic approaches with the modularity and flexibility of partitioned
approaches.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13781" title="Abstract">arXiv:2310.13781</a> [<a href="/pdf/2310.13781" title="Download PDF">pdf</a>, <a href="/format/2310.13781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Much Consistency Is Your Accuracy Worth?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+J+K">Jacob K. Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Marasovi%C4%87%2C+A">Ana Marasovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BlackboxNLP 2023 accepted paper camera-ready version; 6 pages main, 3 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Contrast set consistency is a robustness measurement that evaluates the rate
at which a model correctly responds to all instances in a bundle of minimally
different examples relying on the same knowledge. To draw additional insights,
we propose to complement consistency with relative consistency -- the
probability that an equally accurate model would surpass the consistency of the
proposed model, given a distribution over possible consistencies. Models with
100% relative consistency have reached a consistency peak for their accuracy.
We reflect on prior work that reports consistency in contrast sets and observe
that relative consistency can alter the assessment of a model's consistency
compared to another. We anticipate that our proposed measurement and insights
will influence future studies aiming to promote consistent behavior in models.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13782" title="Abstract">arXiv:2310.13782</a> [<a href="/pdf/2310.13782" title="Download PDF">pdf</a>, <a href="/format/2310.13782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Free Knowledge Distillation Using Adversarially Perturbed OpenGL  Shader Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frank%2C+L">Logan Frank</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J">Jim Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Knowledge distillation (KD) has been a popular and effective method for model
compression. One important assumption of KD is that the original training
dataset is always available. However, this is not always the case due to
privacy concerns and more. In recent years, "data-free" KD has emerged as a
growing research topic which focuses on the scenario of performing KD when no
data is provided. Many methods rely on a generator network to synthesize
examples for distillation (which can be difficult to train) and can frequently
produce images that are visually similar to the original dataset, which raises
questions surrounding whether privacy is completely preserved. In this work, we
propose a new approach to data-free KD that utilizes unnatural OpenGL images,
combined with large amounts of data augmentation and adversarial attacks, to
train a student network. We demonstrate that our approach achieves
state-of-the-art results for a variety of datasets/networks and is more stable
than existing generator-based data-free KD methods. Source code will be
available in the future.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13787" title="Abstract">arXiv:2310.13787</a> [<a href="/pdf/2310.13787" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Illicit Activity Detection using XAI: A Multimodal Graph-LLM  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nicholls%2C+J">Jack Nicholls</a>, 
<a href="/search/cs?searchtype=author&query=Kuppa%2C+A">Aditya Kuppa</a>, 
<a href="/search/cs?searchtype=author&query=Le-Khac%2C+N">Nhien-An Le-Khac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Financial cybercrime prevention is an increasing issue with many
organisations and governments. As deep learning models have progressed to
identify illicit activity on various financial and social networks, the
explainability behind the model decisions has been lacklustre with the
investigative analyst at the heart of any deep learning platform. In our paper,
we present a state-of-the-art, novel multimodal proactive approach to
addressing XAI in financial cybercrime detection.
<br />We leverage a triad of deep learning models designed to distill essential
representations from transaction sequencing, subgraph connectivity, and
narrative generation to significantly streamline the analyst's investigative
process. Our narrative generation proposal leverages LLM to ingest transaction
details and output contextual narrative for an analyst to understand a
transaction and its metadata much further.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13788" title="Abstract">arXiv:2310.13788</a> [<a href="/pdf/2310.13788" title="Download PDF">pdf</a>, <a href="/ps/2310.13788" title="Download PostScript">ps</a>, <a href="/format/2310.13788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Integer Points Counting in Parametric Polyhedra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gribanov%2C+D">D. Gribanov</a>, 
<a href="/search/cs?searchtype=author&query=Malyshev%2C+D">D. Malyshev</a>, 
<a href="/search/cs?searchtype=author&query=Pardalos%2C+P">P. Pardalos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">In this paper, we consider the counting function $E_P(y) = |P_{y} \cap
Z^{n_x}|$ for a parametric polyhedron $P_{y} = \{x \in R^{n_x} \colon A x \leq
b + B y\}$, where $y \in R^{n_y}$. We give a new representation of $E_P(y)$,
called a \emph{piece-wise step-polynomial with periodic coefficients}, which is
a generalization of piece-wise step-polynomials and integer/rational Ehrhart's
quasi-polynomials. In terms of the computational complexity, our result gives
the fastest way to calculate $E_P(y)$ in certain scenarios. The most remarkable
cases are the following:
<br />1) Consider a parametric polyhedron $P_y$ defined by a standard-form system
$A x = y,\, x \geq 0$ with a fixed number of equalities. We show that there
exists an $poly\bigl(n, \|A\|_{\infty}\bigr)$ preprocessing-algorithm that
returns a polynomial-time computable representation of $E_P(y)$. That is,
$E_(y)$ can be computed by a polynomial-time algorithm for any given $y \in
Q^k$;
<br />2) Again, assuming that the co-dimension is fixed, we show that
integer/rational Ehrhart's quasi-polynomials of a polytope can be computed by
FPT-algorithms, parameterized by sub-determinants of $A$ or its elements;
<br />3) Our representation of $E_P(y)$ is more efficient than other known
approaches, if the matrix $A$ has bounded elements, especially if the matrix
$A$ is sparse in addition;
<br />Additionally, we provide a discussion about possible applications in the area
of compiler optimization. In some "natural" assumptions on a program code, our
approach has the fastest complexity bounds.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13791" title="Abstract">arXiv:2310.13791</a> [<a href="/pdf/2310.13791" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of Machine Learning Algorithms for Solar Irradiance  Forecasting in Smart Grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soleymani%2C+S">Saman Soleymani</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadzadeh%2C+S">Shima Mohammadzadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 3 tables, to appear in the 13th Smart Grid Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The increasing global demand for clean and environmentally friendly energy
resources has caused increased interest in harnessing solar power through
photovoltaic (PV) systems for smart grids and homes. However, the inherent
unpredictability of PV generation poses problems associated with smart grid
planning and management, energy trading and market participation, demand
response, reliability, etc. Therefore, solar irradiance forecasting is
essential for optimizing PV system utilization. This study proposes the
next-generation machine learning algorithms such as random forests, Extreme
Gradient Boosting (XGBoost), Light Gradient Boosted Machine (lightGBM)
ensemble, CatBoost, and Multilayer Perceptron Artificial Neural Networks
(MLP-ANNs) to forecast solar irradiance. Besides, Bayesian optimization is
applied to hyperparameter tuning. Unlike tree-based ensemble algorithms that
select the features intrinsically, MLP-ANN needs feature selection as a
separate step. The simulation results indicate that the performance of the
MLP-ANNs improves when feature selection is applied. Besides, the random forest
outperforms the other learning algorithms.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13792" title="Abstract">arXiv:2310.13792</a> [<a href="/pdf/2310.13792" title="Download PDF">pdf</a>, <a href="/format/2310.13792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Densest Subhypergraph: Negative Supermodular Functions and Strongly  Localized Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gleich%2C+D+F">David F. Gleich</a>, 
<a href="/search/cs?searchtype=author&query=Veldt%2C+N">Nate Veldt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Dense subgraph discovery is a fundamental primitive in graph and hypergraph
analysis which among other applications has been used for real-time story
detection on social media and improving access to data stores of social
networking systems. We present several contributions for localized densest
subgraph discovery, which seeks dense subgraphs located nearby a given seed
sets of nodes. We first introduce a generalization of a recent
$\textit{anchored densest subgraph}$ problem, extending this previous objective
to hypergraphs and also adding a tunable locality parameter that controls the
extent to which the output set overlaps with seed nodes. Our primary technical
contribution is to prove when it is possible to obtain a strongly-local
algorithm for solving this problem, meaning that the runtime depends only on
the size of the input set. We provide a strongly-local algorithm that applies
whenever the locality parameter is at least 1, and show why via counterexample
that strongly-local algorithms are impossible below this threshold. Along the
way to proving our results for localized densest subgraph discovery, we also
provide several advances in solving global dense subgraph discovery objectives.
This includes the first strongly polynomial time algorithm for the densest
supermodular set problem and a flow-based exact algorithm for a densest
subgraph discovery problem in graphs with arbitrary node weights. We
demonstrate the utility of our algorithms on several web-based data analysis
tasks.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13793" title="Abstract">arXiv:2310.13793</a> [<a href="/pdf/2310.13793" title="Download PDF">pdf</a>, <a href="/format/2310.13793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified View of Evaluation Metrics for Structured Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunmo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gantt%2C+W">William Gantt</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tongfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+A+S">Aaron Steven White</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2023 Main Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a conceptual framework that unifies a variety of evaluation
metrics for different structured prediction tasks (e.g. event and relation
extraction, syntactic and semantic parsing). Our framework requires
representing the outputs of these tasks as objects of certain data types, and
derives metrics through matching of common substructures, possibly followed by
normalization. We demonstrate how commonly used metrics for a number of tasks
can be succinctly expressed by this framework, and show that new metrics can be
naturally derived in a bottom-up way based on an output structure. We release a
library that enables this derivation to create new metrics. Finally, we
consider how specific characteristics of tasks motivate metric design
decisions, and suggest possible modifications to existing metrics in line with
those motivations.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13795" title="Abstract">arXiv:2310.13795</a> [<a href="/pdf/2310.13795" title="Download PDF">pdf</a>, <a href="/format/2310.13795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Human Behind the Data: Reflections from an Ongoing Co-Design and  Deployment of a Data-Navigation Interface for Front-Line Emergency Housing  Shelter Staff
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masrani%2C+T+W">Teale W Masrani</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H+A">Helen Ai He</a>, 
<a href="/search/cs?searchtype=author&query=Messier%2C+G">Geoffrey Messier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">On any night in Canada, at least 35,000 individuals experience homelessness.
These individuals use emergency shelters to transition out of homelessness and
into permanent housing. We designed and deployed a technology to support
front-line staff at the largest emergency housing shelter in Calgary, Canada.
Over a period of five months in 2022, we worked closely with front-line staff
to co-design an interface for supporting a holistic understanding of client
context and facilitating decision-making. The tool is currently in-use and our
collaboration is ongoing. In this paper, we reflect on preliminary findings
regarding the second iteration of the tool. We find that supporting shelter
staff in understanding the human behind the data was a critical component of
design. This work contributes to literature on how data tools may be integrated
into homeless shelters in a way that aligns with shelters' values.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13798" title="Abstract">arXiv:2310.13798</a> [<a href="/pdf/2310.13798" title="Download PDF">pdf</a>, <a href="/format/2310.13798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specific versus General Principles for Constitutional AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sandipan Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuntao Bai</a>, 
<a href="/search/cs?searchtype=author&query=Kadavath%2C+S">Saurav Kadavath</a>, 
<a href="/search/cs?searchtype=author&query=Askell%2C+A">Amanda Askell</a>, 
<a href="/search/cs?searchtype=author&query=Callahan%2C+A">Andrew Callahan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Anna Chen</a>, 
<a href="/search/cs?searchtype=author&query=Goldie%2C+A">Anna Goldie</a>, 
<a href="/search/cs?searchtype=author&query=Balwit%2C+A">Avital Balwit</a>, 
<a href="/search/cs?searchtype=author&query=Mirhoseini%2C+A">Azalia Mirhoseini</a>, 
<a href="/search/cs?searchtype=author&query=McLean%2C+B">Brayden McLean</a>, 
<a href="/search/cs?searchtype=author&query=Olsson%2C+C">Catherine Olsson</a>, 
<a href="/search/cs?searchtype=author&query=Evraets%2C+C">Cassie Evraets</a>, 
<a href="/search/cs?searchtype=author&query=Tran-Johnson%2C+E">Eli Tran-Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Durmus%2C+E">Esin Durmus</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>, 
<a href="/search/cs?searchtype=author&query=Kernion%2C+J">Jackson Kernion</a>, 
<a href="/search/cs?searchtype=author&query=Kerr%2C+J">Jamie Kerr</a>, 
<a href="/search/cs?searchtype=author&query=Ndousse%2C+K">Kamal Ndousse</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Karina Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Elhage%2C+N">Nelson Elhage</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Newton Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Schiefer%2C+N">Nicholas Schiefer</a>, 
<a href="/search/cs?searchtype=author&query=DasSarma%2C+N">Nova DasSarma</a>, 
<a href="/search/cs?searchtype=author&query=Rausch%2C+O">Oliver Rausch</a>, 
<a href="/search/cs?searchtype=author&query=Larson%2C+R">Robin Larson</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shannon Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kravec%2C+S">Shauna Kravec</a>, 
<a href="/search/cs?searchtype=author&query=Telleen-Lawton%2C+T">Timothy Telleen-Lawton</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+T+I">Thomas I. Liao</a>, 
<a href="/search/cs?searchtype=author&query=Henighan%2C+T">Tom Henighan</a>, 
<a href="/search/cs?searchtype=author&query=Hume%2C+T">Tristan Hume</a>, 
<a href="/search/cs?searchtype=author&query=Hatfield-Dodds%2C+Z">Zac Hatfield-Dodds</a>, 
<a href="/search/cs?searchtype=author&query=Mindermann%2C+S">S&#xf6;ren Mindermann</a>, 
<a href="/search/cs?searchtype=author&query=Joseph%2C+N">Nicholas Joseph</a>, 
<a href="/search/cs?searchtype=author&query=McCandlish%2C+S">Sam McCandlish</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+J">Jared Kaplan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Human feedback can prevent overtly harmful utterances in conversational
models, but may not automatically mitigate subtle problematic behaviors such as
a stated desire for self-preservation or power. Constitutional AI offers an
alternative, replacing human feedback with feedback from AI models conditioned
only on a list of written principles. We find this approach effectively
prevents the expression of such behaviors. The success of simple principles
motivates us to ask: can models learn general ethical behaviors from only a
single written principle? To test this, we run experiments using a principle
roughly stated as "do what's best for humanity". We find that the largest
dialogue models can generalize from this short constitution, resulting in
harmless assistants with no stated interest in specific motivations like power.
A general principle may thus partially avoid the need for a long list of
constitutions targeting potentially harmful behaviors. However, more detailed
constitutions still improve fine-grained control over specific types of harms.
This suggests both general and specific principles have value for steering AI
safely.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13800" title="Abstract">arXiv:2310.13800</a> [<a href="/pdf/2310.13800" title="Download PDF">pdf</a>, <a href="/format/2310.13800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large  Language Models on Sequence to Sequence Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sottana%2C+A">Andrea Sottana</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+K">Kai Zou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) evaluation is a patchy and inconsistent
landscape, and it is becoming clear that the quality of automatic evaluation
metrics is not keeping up with the pace of development of generative models. We
aim to improve the understanding of current models' performance by providing a
preliminary and hybrid evaluation on a range of open and closed-source
generative LLMs on three NLP benchmarks: text summarisation, text
simplification and grammatical error correction (GEC), using both automatic and
human evaluation. We also explore the potential of the recently released GPT-4
to act as an evaluator. We find that ChatGPT consistently outperforms many
other popular models according to human reviewers on the majority of metrics,
while scoring much more poorly when using classic automatic evaluation metrics.
We also find that human reviewers rate the gold reference as much worse than
the best models' outputs, indicating the poor quality of many popular
benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs
in a way which aligns reasonably closely to human judgement despite
task-specific variations, with a lower alignment in the GEC task.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13802" title="Abstract">arXiv:2310.13802</a> [<a href="/pdf/2310.13802" title="Download PDF">pdf</a>, <a href="/format/2310.13802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Molecular Properties Prediction Through Latent Space Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soares%2C+E">Eduardo Soares</a>, 
<a href="/search/cs?searchtype=author&query=Kishimoto%2C+A">Akihiro Kishimoto</a>, 
<a href="/search/cs?searchtype=author&query=Brazil%2C+E+V">Emilio Vital Brazil</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+S">Seiji Takeda</a>, 
<a href="/search/cs?searchtype=author&query=Kajino%2C+H">Hiroshi Kajino</a>, 
<a href="/search/cs?searchtype=author&query=Cerqueira%2C+R">Renato Cerqueira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, 4 Figures - Submited to the AI4Science Workshop - Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Pre-trained Language Models have emerged as promising tools for predicting
molecular properties, yet their development is in its early stages,
necessitating further research to enhance their efficacy and address challenges
such as generalization and sample efficiency. In this paper, we present a
multi-view approach that combines latent spaces derived from state-of-the-art
chemical models. Our approach relies on two pivotal elements: the embeddings
derived from MHG-GNN, which represent molecular structures as graphs, and
MoLFormer embeddings rooted in chemical language. The attention mechanism of
MoLFormer is able to identify relations between two atoms even when their
distance is far apart, while the GNN of MHG-GNN can more precisely capture
relations among multiple atoms closely located. In this work, we demonstrate
the superior performance of our proposed multi-view approach compared to
existing state-of-the-art methods, including MoLFormer-XL, which was trained on
1.1 billion molecules, particularly in intricate tasks such as predicting
clinical trial drug toxicity and inhibiting HIV replication. We assessed our
approach using six benchmark datasets from MoleculeNet, where it outperformed
competitors in five of them. Our study highlights the potential of latent space
fusion and feature integration for advancing molecular property prediction. In
this work, we use small versions of MHG-GNN and MoLFormer, which opens up an
opportunity for further improvement when our approach uses a larger-scale
dataset.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13805" title="Abstract">arXiv:2310.13805</a> [<a href="/pdf/2310.13805" title="Download PDF">pdf</a>, <a href="/format/2310.13805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalizing flow-based deep variational Bayesian network for seismic  multi-hazards and impacts estimation from InSAR imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuechun Li</a>, 
<a href="/search/cs?searchtype=author&query=Burgi%2C+P+M">Paula M. Burgi</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Noh%2C+H+Y">Hae Young Noh</a>, 
<a href="/search/cs?searchtype=author&query=Wald%2C+D+J">David J. Wald</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Susu Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Onsite disasters like earthquakes can trigger cascading hazards and impacts,
such as landslides and infrastructure damage, leading to catastrophic losses;
thus, rapid and accurate estimates are crucial for timely and effective
post-disaster responses. Interferometric Synthetic aperture radar (InSAR) data
is important in providing high-resolution onsite information for rapid hazard
estimation. Most recent methods using InSAR imagery signals predict a single
type of hazard and thus often suffer low accuracy due to noisy and complex
signals induced by co-located hazards, impacts, and irrelevant environmental
changes (e.g., vegetation changes, human activities). We introduce a novel
stochastic variational inference with normalizing flows derived to jointly
approximate posteriors of multiple unobserved hazards and impacts from noisy
InSAR imagery.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13807" title="Abstract">arXiv:2310.13807</a> [<a href="/pdf/2310.13807" title="Download PDF">pdf</a>, <a href="/format/2310.13807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to (Learn at Test Time)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Dalal%2C+K">Karan Dalal</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chloe Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Guestrin%2C+C">Carlos Guestrin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinlei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We reformulate the problem of supervised learning as learning to learn with
two nested loops (i.e. learning problems). The inner loop learns on each
individual instance with self-supervision before final prediction. The outer
loop learns the self-supervised task used by the inner loop, such that its
final prediction improves. Our inner loop turns out to be equivalent to linear
attention when the inner-loop learner is only a linear model, and to
self-attention when it is a kernel estimator. For practical comparison with
linear or self-attention layers, we replace each of them in a transformer with
an inner loop, so our outer loop is equivalent to training the architecture.
When each inner-loop learner is a neural network, our approach vastly
outperforms transformers with linear attention on ImageNet from 224 x 224 raw
pixels in both accuracy and FLOPs, while (regular) transformers cannot run.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13809" title="Abstract">arXiv:2310.13809</a> [<a href="/pdf/2310.13809" title="Download PDF">pdf</a>, <a href="/format/2310.13809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Low-Dimensional Sensing Mapless Navigation of Terrestrial  Mobile Robots Using Double Deep Reinforcement Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Moraes%2C+L+D">Linda Dotto de Moraes</a>, 
<a href="/search/cs?searchtype=author&query=Kich%2C+V+A">Victor Augusto Kich</a>, 
<a href="/search/cs?searchtype=author&query=Kolling%2C+A+H">Alisson Henrique Kolling</a>, 
<a href="/search/cs?searchtype=author&query=Bottega%2C+J+A">Jair Augusto Bottega</a>, 
<a href="/search/cs?searchtype=author&query=Grando%2C+R+B">Ricardo Bedin Grando</a>, 
<a href="/search/cs?searchtype=author&query=Cukla%2C+A+R">Anselmo Rafael Cukla</a>, 
<a href="/search/cs?searchtype=author&query=Gamarra%2C+D+F+T">Daniel Fernando Tello Gamarra</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 20th IEEE Latin American Robotics Symposium - LARS 2023 and 15th
  Brazilian Symposium on Robotics- SBR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this study, we present two distinct approaches within the realm of Deep
Reinforcement Learning (Deep-RL) aimed at enhancing mapless navigation for a
ground-based mobile robot. The research methodology primarily involves a
comparative analysis between a Deep-RL strategy grounded in the foundational
Deep Q-Network (DQN) algorithm, and an alternative approach based on the Double
Deep Q-Network (DDQN) algorithm. The agents in these approaches leverage 24
measurements from laser range sampling, coupled with the agent's positional
differentials and orientation relative to the target. This amalgamation of data
influences the agents' determinations regarding navigation, ultimately
dictating the robot's velocities. By embracing this parsimonious sensory
framework as proposed, we successfully showcase the training of an agent for
proficiently executing navigation tasks and adeptly circumventing obstacles.
Notably, this accomplishment is attained without a dependency on intricate
sensory inputs like those inherent to image-centric methodologies. The proposed
methodology is evaluated in three different real environments, revealing that
Double Deep structures significantly enhance the navigation capabilities of
mobile robots compared to simple Q structures.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13810" title="Abstract">arXiv:2310.13810</a> [<a href="/pdf/2310.13810" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Better Match for Drivers and Riders: Reinforcement Learning at Lyft
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azagirre%2C+X">Xabi Azagirre</a>, 
<a href="/search/cs?searchtype=author&query=Balwally%2C+A">Akshay Balwally</a>, 
<a href="/search/cs?searchtype=author&query=Candeli%2C+G">Guillaume Candeli</a>, 
<a href="/search/cs?searchtype=author&query=Chamandy%2C+N">Nicholas Chamandy</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Benjamin Han</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+A">Alona King</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyungjun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Loncaric%2C+M">Martin Loncaric</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+S">S&#xe9;bastien Martin</a> (SM), 
<a href="/search/cs?searchtype=author&query=Narasiman%2C+V">Vijay Narasiman</a>, 
<a href="/search/cs?searchtype=author&query=Zhiwei">Zhiwei</a> (Tony)Qin, 
<a href="/search/cs?searchtype=author&query=Richard%2C+B">Baptiste Richard</a>, 
<a href="/search/cs?searchtype=author&query=Smoot%2C+S">Sara Smoot</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+S">Sean Taylor</a>, 
<a href="/search/cs?searchtype=author&query=van+Ryzin%2C+G">Garrett van Ryzin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zamoshchin%2C+A">Alex Zamoshchin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">To better match drivers to riders in our ridesharing application, we revised
Lyft's core matching algorithm. We use a novel online reinforcement learning
approach that estimates the future earnings of drivers in real time and use
this information to find more efficient matches. This change was the first
documented implementation of a ridesharing matching algorithm that can learn
and improve in real time. We evaluated the new approach during weeks of
switchback experimentation in most Lyft markets, and estimated how it benefited
drivers, riders, and the platform. In particular, it enabled our drivers to
serve millions of additional riders each year, leading to more than $30 million
per year in incremental revenue. Lyft rolled out the algorithm globally in
2021.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13812" title="Abstract">arXiv:2310.13812</a> [<a href="/pdf/2310.13812" title="Download PDF">pdf</a>, <a href="/format/2310.13812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Yet Another Model for Arabic Dialect Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Ajinkya Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Aldarmaki%2C+H">Hanan Aldarmaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACCEPTED AT ArabicNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we describe a spoken Arabic dialect identification (ADI) model
for Arabic that consistently outperforms previously published results on two
benchmark datasets: ADI-5 and ADI-17. We explore two architectural variations:
ResNet and ECAPA-TDNN, coupled with two types of acoustic features: MFCCs and
features exratected from the pre-trained self-supervised model UniSpeech-SAT
Large, as well as a fusion of all four variants. We find that individually,
ECAPA-TDNN network outperforms ResNet, and models with UniSpeech-SAT features
outperform models with MFCCs by a large margin. Furthermore, a fusion of all
four variants consistently outperforms individual models. Our best models
outperform previously reported results on both datasets, with accuracies of
84.7% and 96.9% on ADI-5 and ADI-17, respectively.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13817" title="Abstract">arXiv:2310.13817</a> [<a href="/pdf/2310.13817" title="Download PDF">pdf</a>, <a href="/format/2310.13817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Based Forecasting-Aided State Estimation in Active  Distribution Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alduhaymi%2C+M">Malek Alduhaymi</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+R">Ravindra Singh</a>, 
<a href="/search/eess?searchtype=author&query=Nazir%2C+F+U">Firdous Ul Nazir</a>, 
<a href="/search/eess?searchtype=author&query=Pal%2C+B+C">Bikash C. Pal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Operating an active distribution network (ADN) in the absence of enough
measurements, the presence of distributed energy resources, and poor knowledge
of responsive demand behaviour is a huge challenge. This paper introduces
systematic modelling of demand response behaviour which is then included in
Forecasting Aided State Estimation (FASE) for better control of the network.
There are several innovative elements in tuning parameters of FASE-based,
demand profiling, and aggregation. The comprehensive case studies for three UK
representative demand scenarios in 2023, 2035, and 2050 demonstrated the
effectiveness of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13818" title="Abstract">arXiv:2310.13818</a> [<a href="/pdf/2310.13818" title="Download PDF">pdf</a>, <a href="/format/2310.13818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FATA-Trans: Field And Time-Aware Transformer for Sequential Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xin Dai</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shubham Jain</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yujie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C+M">Chin-Chia Michael Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zhongfang Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is accepted by ACM International Conference on Information and Knowledge Management (CIKM) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sequential tabular data is one of the most commonly used data types in
real-world applications. Different from conventional tabular data, where rows
in a table are independent, sequential tabular data contains rich contextual
and sequential information, where some fields are dynamically changing over
time and others are static. Existing transformer-based approaches analyzing
sequential tabular data overlook the differences between dynamic and static
fields by replicating and filling static fields into each transformer, and
ignore temporal information between rows, which leads to three major
disadvantages: (1) computational overhead, (2) artificially simplified data for
masked language modeling pre-training task that may yield less meaningful
representations, and (3) disregarding the temporal behavioral patterns implied
by time intervals. In this work, we propose FATA-Trans, a model with two field
transformers for modeling sequential tabular data, where each processes static
and dynamic field information separately. FATA-Trans is field- and time-aware
for sequential tabular data. The field-type embedding in the method enables
FATA-Trans to capture differences between static and dynamic fields. The
time-aware position embedding exploits both order and time interval information
between rows, which helps the model detect underlying temporal behavior in a
sequence. Our experiments on three benchmark datasets demonstrate that the
learned representations from FATA-Trans consistently outperform
state-of-the-art solutions in the downstream tasks. We also present
visualization studies to highlight the insights captured by the learned
representations, enhancing our understanding of the underlying data. Our codes
are available at https://github.com/zdy93/FATA-Trans.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13819" title="Abstract">arXiv:2310.13819</a> [<a href="/pdf/2310.13819" title="Download PDF">pdf</a>, <a href="/format/2310.13819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LanPose: Language-Instructed 6D Object Pose Estimation for Robotic  Assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+B">Bowen Fu</a>, 
<a href="/search/cs?searchtype=author&query=Leong%2C+S+K">Sek Kun Leong</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+Y">Yan Di</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiwen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Comprehending natural language instructions is a critical skill for robots to
cooperate effectively with humans. In this paper, we aim to learn 6D poses for
roboticassembly by natural language instructions. For this purpose,
Language-Instructed 6D Pose Regression Network (LanPose) is proposed to jointly
predict the 6D poses of the observed object and the corresponding assembly
position. Our proposed approach is based on the fusion of geometric and
linguistic features, which allows us to finely integrate multi-modality input
and map it to the 6D pose in SE(3) space by the cross-attention mechanism and
the language-integrated 6D pose mapping module, respectively. To validate the
effectiveness of our approach, an integrated robotic system is established to
precisely and robustly perceive, grasp, manipulate and assemble blocks by
language commands. 98.09 and 93.55 in ADD(-S)-0.1d are derived for the
prediction of 6D object pose and 6D assembly pose, respectively. Both
quantitative and qualitative results demonstrate the effectiveness of our
proposed language-instructed 6D pose estimation methodology and its potential
to enable robots to better understand and execute natural language
instructions.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13820" title="Abstract">arXiv:2310.13820</a> [<a href="/pdf/2310.13820" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FERI: A Multitask-based Fairness Achieving Algorithm with Applications  to Fair Organ Transplantation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Can Li</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+D">Dejian Lai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoqian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Liver transplantation often faces fairness challenges across subgroups
defined by sensitive attributes like age group, gender, and race/ethnicity.
Machine learning models for outcome prediction can introduce additional biases.
To address these, we introduce Fairness through the Equitable Rate of
Improvement in Multitask Learning (FERI) algorithm for fair predictions of
graft failure risk in liver transplant patients. FERI constrains subgroup loss
by balancing learning rates and preventing subgroup dominance in the training
process. Our experiments show that FERI maintains high predictive accuracy with
AUROC and AUPRC comparable to baseline models. More importantly, FERI
demonstrates an ability to improve fairness without sacrificing accuracy.
Specifically, for gender, FERI reduces the demographic parity disparity by
71.74%, and for the age group, it decreases the equalized odds disparity by
40.46%. Therefore, the FERI algorithm advances fairness-aware predictive
modeling in healthcare and provides an invaluable tool for equitable healthcare
systems.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13821" title="Abstract">arXiv:2310.13821</a> [<a href="/pdf/2310.13821" title="Download PDF">pdf</a>, <a href="/format/2310.13821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometric Learning with Positively Decomposable Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da+Costa%2C+N">Nathael Da Costa</a>, 
<a href="/search/cs?searchtype=author&query=Mostajeran%2C+C">Cyrus Mostajeran</a>, 
<a href="/search/cs?searchtype=author&query=Ortega%2C+J">Juan-Pablo Ortega</a>, 
<a href="/search/cs?searchtype=author&query=Said%2C+S">Salem Said</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Differential Geometry (math.DG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Kernel methods are powerful tools in machine learning. Classical kernel
methods are based on positive-definite kernels, which map data spaces into
reproducing kernel Hilbert spaces (RKHS). For non-Euclidean data spaces,
positive-definite kernels are difficult to come by. In this case, we propose
the use of reproducing kernel Krein space (RKKS) based methods, which require
only kernels that admit a positive decomposition. We show that one does not
need to access this decomposition in order to learn in RKKS. We then
investigate the conditions under which a kernel is positively decomposable. We
show that invariant kernels admit a positive decomposition on homogeneous
spaces under tractable regularity assumptions. This makes them much easier to
construct than positive-definite kernels, providing a route for learning with
kernels for non-Euclidean data. By the same token, this provides theoretical
foundations for RKKS-based methods in general.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13822" title="Abstract">arXiv:2310.13822</a> [<a href="/pdf/2310.13822" title="Download PDF">pdf</a>, <a href="/format/2310.13822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Attacks on Fairness of Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Binchi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yushun Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yada Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Minnan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jundong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fairness-aware graph neural networks (GNNs) have gained a surge of attention
as they can reduce the bias of predictions on any demographic group (e.g.,
female) in graph-based applications. Although these methods greatly improve the
algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully
designed adversarial attacks. In this paper, we investigate the problem of
adversarial attacks on fairness of GNNs and propose G-FairAttack, a general
framework for attacking various types of fairness-aware GNNs in terms of
fairness with an unnoticeable effect on prediction utility. In addition, we
propose a fast computation technique to reduce the time complexity of
G-FairAttack. The experimental study demonstrates that G-FairAttack
successfully corrupts the fairness of different types of GNNs while keeping the
attack unnoticeable. Our study on fairness attacks sheds light on potential
vulnerabilities in fairness-aware GNNs and guides further research on the
robustness of GNNs in terms of fairness. The open-source code is available at
https://github.com/zhangbinchi/G-FairAttack.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13824" title="Abstract">arXiv:2310.13824</a> [<a href="/pdf/2310.13824" title="Download PDF">pdf</a>, <a href="/format/2310.13824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plausibility Processing in Transformer Language Models: Focusing on the  Role of Attention Heads in GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryu%2C+S+H">Soo Hyun Ryu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP-findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The goal of this paper is to explore how Transformer language models process
semantic knowledge, especially regarding the plausibility of noun-verb
relations. First, I demonstrate GPT2 exhibits a higher degree of similarity
with humans in plausibility processing compared to other Transformer language
models. Next, I delve into how knowledge of plausibility is contained within
attention heads of GPT2 and how these heads causally contribute to GPT2's
plausibility processing ability. Through several experiments, it was found
that: i) GPT2 has a number of attention heads that detect plausible noun-verb
relationships; ii) these heads collectively contribute to the Transformer's
ability to process plausibility, albeit to varying degrees; and iii) attention
heads' individual performance in detecting plausibility does not necessarily
correlate with how much they contribute to GPT2's plausibility processing
ability.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13825" title="Abstract">arXiv:2310.13825</a> [<a href="/pdf/2310.13825" title="Download PDF">pdf</a>, <a href="/format/2310.13825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Spatially-Coupled Product-Like Codes Using Zipper Codes With  Irregular Degree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukmadji%2C+A+Y">Alvin Y. Sukmadji</a>, 
<a href="/search/cs?searchtype=author&query=Kschischang%2C+F+R">Frank R. Kschischang</a>, 
<a href="/search/cs?searchtype=author&query=Shehadeh%2C+M">Mohannad Shehadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 11 figures, paper accepted for the GLOBECOM 2023 Workshop on Channel Coding Beyond 5G
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Zipper codes with irregular variable degree are studied. Two new interleaver
maps -- chevron and half-chevron -- are described. Simulation results with
shortened double-error-correcting Bose--Chaudhuri--Hocquenghem constituent
codes show that zipper codes with chevron and half-chevron interleaver maps
outperform staircase codes when the rate is below 0.86 and 0.91, respectively,
at $10^{-8}$ output bit error rate operating point. In the miscorrection-free
decoding scheme, both zipper codes with chevron and half-chevron interleaver
maps outperform staircase codes. However, constituent decoder miscorrections
induce additional performance gaps.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13828" title="Abstract">arXiv:2310.13828</a> [<a href="/pdf/2310.13828" title="Download PDF">pdf</a>, <a href="/format/2310.13828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+S">Shawn Shan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenxin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Passananti%2C+J">Josephine Passananti</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haitao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B+Y">Ben Y. Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data poisoning attacks manipulate training data to introduce unexpected
behaviors into machine learning models at training time. For text-to-image
generative models with massive training datasets, current understanding of
poisoning attacks suggests that a successful attack would require injecting
millions of poison samples into their training pipeline. In this paper, we show
that poisoning attacks can be successful on generative models. We observe that
training data per concept can be quite limited in these models, making them
vulnerable to prompt-specific poisoning attacks, which target a model's ability
to respond to individual prompts.
<br />We introduce Nightshade, an optimized prompt-specific poisoning attack where
poison samples look visually identical to benign images with matching text
prompts. Nightshade poison samples are also optimized for potency and can
corrupt an Stable Diffusion SDXL prompt in &lt;100 poison samples. Nightshade
poison effects "bleed through" to related concepts, and multiple attacks can
composed together in a single prompt. Surprisingly, we show that a moderate
number of Nightshade attacks can destabilize general features in a
text-to-image generative model, effectively disabling its ability to generate
meaningful images. Finally, we propose the use of Nightshade` and similar tools
as a last defense for content creators against web scrapers that ignore
opt-out/do-not-crawl directives, and discuss possible implications for model
trainers and content creators.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13829" title="Abstract">arXiv:2310.13829</a> [<a href="/pdf/2310.13829" title="Download PDF">pdf</a>, <a href="/format/2310.13829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Representation of Permutation-Invariant Functions on Vectors  and Tensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tabaghi%2C+P">Puoya Tabaghi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A main object of our study is multiset functions -- that is,
permutation-invariant functions over inputs of varying sizes. Deep Sets,
proposed by \cite{zaheer2017deep}, provides a \emph{universal representation}
for continuous multiset functions on scalars via a sum-decomposable model.
Restricting the domain of the functions to finite multisets of $D$-dimensional
vectors, Deep Sets also provides a \emph{universal approximation} that requires
a latent space dimension of $O(N^D)$ -- where $N$ is an upper bound on the size
of input multisets. In this paper, we strengthen this result by proving that
universal representation is guaranteed for continuous and discontinuous
multiset functions though a latent space dimension of $O(N^D)$. We then
introduce \emph{identifiable} multisets for which we can uniquely label their
elements using an identifier function, namely, finite-precision vectors are
identifiable. Using our analysis on identifiable multisets, we prove that a
sum-decomposable model for general continuous multiset functions only requires
a latent dimension of $2DN$. We further show that both encoder and decoder
functions of the model are continuous -- our main contribution to the existing
work which lack such a guarantee. Also this provides a significant improvement
over the aforementioned $O(N^D)$ bound which was derived for universal
representation of continuous and discontinuous multiset functions. We then
extend our results and provide special sum-decomposition structures to
universally represent permutation-invariant tensor functions on identifiable
tensors. These families of sum-decomposition models enables us to design deep
network architectures and deploy them on a variety of learning tasks on
sequences, images, and graphs.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13831" title="Abstract">arXiv:2310.13831</a> [<a href="/pdf/2310.13831" title="Download PDF">pdf</a>, <a href="/format/2310.13831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers for Trajectory Optimization with Application to Spacecraft  Rendezvous
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guffanti%2C+T">Tommaso Guffanti</a>, 
<a href="/search/cs?searchtype=author&query=Gammelli%2C+D">Daniele Gammelli</a>, 
<a href="/search/cs?searchtype=author&query=D%27Amico%2C+S">Simone D&#x27;Amico</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 IEEE Aerospace Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Reliable and efficient trajectory optimization methods are a fundamental need
for autonomous dynamical systems, effectively enabling applications including
rocket landing, hypersonic reentry, spacecraft rendezvous, and docking. Within
such safety-critical application areas, the complexity of the emerging
trajectory optimization problems has motivated the application of AI-based
techniques to enhance the performance of traditional approaches. However,
current AI-based methods either attempt to fully replace traditional control
algorithms, thus lacking constraint satisfaction guarantees and incurring in
expensive simulation, or aim to solely imitate the behavior of traditional
methods via supervised learning. To address these limitations, this paper
proposes the Autonomous Rendezvous Transformer (ART) and assesses the
capability of modern generative models to solve complex trajectory optimization
problems, both from a forecasting and control standpoint. Specifically, this
work assesses the capabilities of Transformers to (i) learn near-optimal
policies from previously collected data, and (ii) warm-start a sequential
optimizer for the solution of non-convex optimal control problems, thus
guaranteeing hard constraint satisfaction. From a forecasting perspective,
results highlight how ART outperforms other learning-based architectures at
predicting known fuel-optimal trajectories. From a control perspective,
empirical analyses show how policies learned through Transformers are able to
generate near-optimal warm-starts, achieving trajectories that are (i) more
fuel-efficient, (ii) obtained in fewer sequential optimizer iterations, and
(iii) computed with an overall runtime comparable to benchmarks based on convex
optimization.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13833" title="Abstract">arXiv:2310.13833</a> [<a href="/pdf/2310.13833" title="Download PDF">pdf</a>, <a href="/format/2310.13833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mufei Li</a>, 
<a href="/search/cs?searchtype=author&query=Krea%C4%8Di%C4%87%2C+E">Eleonora Krea&#x10d;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Potluru%2C+V+K">Vamsi K. Potluru</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/Graph-COM/GraphMaker">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large-scale graphs with node attributes are fundamental in real-world
scenarios, such as social and financial networks. The generation of synthetic
graphs that emulate real-world ones is pivotal in graph machine learning,
aiding network evolution understanding and data utility preservation when
original data cannot be shared. Traditional models for graph generation suffer
from limited model capacity. Recent developments in diffusion models have shown
promise in merely graph structure generation or the generation of small
molecular graphs with attributes. However, their applicability to large
attributed graphs remains unaddressed due to challenges in capturing intricate
patterns and scalability. This paper introduces GraphMaker, a novel diffusion
model tailored for generating large attributed graphs. We study the diffusion
models that either couple or decouple graph structure and node attribute
generation to address their complex correlation. We also employ node-level
conditioning and adopt a minibatch strategy for scalability. We further propose
a new evaluation pipeline using models trained on generated synthetic graphs
and tested on original graphs to evaluate the quality of synthetic data.
Empirical evaluations on real-world datasets showcase GraphMaker's superiority
in generating realistic and diverse large-attributed graphs beneficial for
downstream tasks.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13836" title="Abstract">arXiv:2310.13836</a> [<a href="/pdf/2310.13836" title="Download PDF">pdf</a>, <a href="/format/2310.13836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Model&#x27;s Embedded Representations May Detect Distribution  Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsou%2C+A">Adam Tsou</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+M">Max Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Engel%2C+A">Andrew Engel</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Tony Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Distribution shifts between train and test datasets obscure our ability to
understand the generalization capacity of neural network models. This topic is
especially relevant given the success of pre-trained foundation models as
starting points for transfer learning (TL) models across tasks and contexts. We
present a case study for TL on a pre-trained GPT-2 model onto the Sentiment140
dataset for sentiment classification. We show that Sentiment140's test dataset
$M$ is not sampled from the same distribution as the training dataset $P$, and
hence training on $P$ and measuring performance on $M$ does not actually
account for the model's generalization on sentiment classification.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13838" title="Abstract">arXiv:2310.13838</a> [<a href="/pdf/2310.13838" title="Download PDF">pdf</a>, <a href="/format/2310.13838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CNN-based Prediction of Partition Path for VVC Fast Inter Partitioning  Using Motion Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Riviere%2C+M">Marc Riviere</a>, 
<a href="/search/cs?searchtype=author&query=Guionnet%2C+T">Thomas Guionnet</a>, 
<a href="/search/cs?searchtype=author&query=Roumy%2C+A">Aline Roumy</a>, 
<a href="/search/cs?searchtype=author&query=Guillemot%2C+C">Christine Guillemot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The Versatile Video Coding (VVC) standard has been recently finalized by the
Joint Video Exploration Team (JVET). Compared to the High Efficiency Video
Coding (HEVC) standard, VVC offers about 50% compression efficiency gain, in
terms of Bjontegaard Delta-Rate (BD-rate), at the cost of a 10-fold increase in
encoding complexity. In this paper, we propose a method based on Convolutional
Neural Network (CNN) to speed up the inter partitioning process in VVC.
Firstly, a novel representation for the quadtree with nested multi-type tree
(QTMT) partition is introduced, derived from the partition path. Secondly, we
develop a U-Net-based CNN taking a multi-scale motion vector field as input at
the Coding Tree Unit (CTU) level. The purpose of CNN inference is to predict
the optimal partition path during the Rate-Distortion Optimization (RDO)
process. To achieve this, we divide CTU into grids and predict the Quaternary
Tree (QT) depth and Multi-type Tree (MT) split decisions for each cell of the
grid. Thirdly, an efficient partition pruning algorithm is introduced to employ
the CNN predictions at each partitioning level to skip RDO evaluations of
unnecessary partition paths. Finally, an adaptive threshold selection scheme is
designed, making the trade-off between complexity and efficiency scalable.
Experiments show that the proposed method can achieve acceleration ranging from
16.5% to 60.2% under the RandomAccess Group Of Picture 32 (RAGOP32)
configuration with a reasonable efficiency drop ranging from 0.44% to 4.59% in
terms of BD-rate, which surpasses other state-of-the-art solutions.
Additionally, our method stands out as one of the lightest approaches in the
field, which ensures its applicability to other encoders.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13841" title="Abstract">arXiv:2310.13841</a> [<a href="/pdf/2310.13841" title="Download PDF">pdf</a>, <a href="/format/2310.13841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast hyperboloid decision tree algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chlenski%2C+P">Philippe Chlenski</a>, 
<a href="/search/cs?searchtype=author&query=Turok%2C+E">Ethan Turok</a>, 
<a href="/search/cs?searchtype=author&query=Moretti%2C+A">Antonio Moretti</a>, 
<a href="/search/cs?searchtype=author&query=Pe%27er%2C+I">Itsik Pe&#x27;er</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Hyperbolic geometry is gaining traction in machine learning for its
effectiveness at capturing hierarchical structures in real-world data.
Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial
advantages and consistently deliver state-of-the-art results across diverse
applications. However, hyperbolic classifiers often grapple with computational
challenges. Methods reliant on Riemannian optimization frequently exhibit
sluggishness, stemming from the increased computational demands of operations
on Riemannian manifolds. In response to these challenges, we present hyperDT, a
novel extension of decision tree algorithms into hyperbolic space. Crucially,
hyperDT eliminates the need for computationally intensive Riemannian
optimization, numerically unstable exponential and logarithmic maps, or
pairwise comparisons between points by leveraging inner products to adapt
Euclidean decision tree algorithms to hyperbolic space. Our approach is
conceptually straightforward and maintains constant-time decision complexity
while mitigating the scalability issues inherent in high-dimensional Euclidean
spaces. Building upon hyperDT we introduce hyperRF, a hyperbolic random forest
model. Extensive benchmarking across diverse datasets underscores the superior
performance of these models, providing a swift, precise, accurate, and
user-friendly toolkit for hyperbolic data analysis.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13844" title="Abstract">arXiv:2310.13844</a> [<a href="/pdf/2310.13844" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-level, Forming Free, Bulk Switching Trilayer RRAM for Neuromorphic  Computing at the Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaeseoung Park</a> (1), 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ashwani Kumar</a> (1), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yucheng Zhou</a> (1), 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sangheon Oh</a> (1), 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeong-Hoon Kim</a> (1), 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhan Shi</a> (1), 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Soumil Jain</a> (2), 
<a href="/search/cs?searchtype=author&query=Hota%2C+G">Gopabandhu Hota</a> (1), 
<a href="/search/cs?searchtype=author&query=Nagle%2C+A+L">Amelie L. Nagle</a> (3), 
<a href="/search/cs?searchtype=author&query=Schuman%2C+C+D">Catherine D. Schuman</a> (4), 
<a href="/search/cs?searchtype=author&query=Cauwenberghs%2C+G">Gert Cauwenberghs</a> (2), 
<a href="/search/cs?searchtype=author&query=Kuzum%2C+D">Duygu Kuzum</a> (1) ((1) Department of Electrical and Computer Engineering, (2) Department of Bioengineering, University of California, San Diego, CA, USA. (3) Department of Computer Science, Massachusetts Institute of Technology, MA, USA. (4) Department of Electrical Engineering and Computer Science, University of Tennessee, TN, USA.)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Resistive memory-based reconfigurable systems constructed by CMOS-RRAM
integration hold great promise for low energy and high throughput neuromorphic
computing. However, most RRAM technologies relying on filamentary switching
suffer from variations and noise leading to computational accuracy loss,
increased energy consumption, and overhead by expensive program and verify
schemes. Low ON-state resistance of filamentary RRAM devices further increases
the energy consumption due to high-current read and write operations, and
limits the array size and parallel multiply &amp; accumulate operations.
High-forming voltages needed for filamentary RRAM are not compatible with
advanced CMOS technology nodes. To address all these challenges, we developed a
forming-free and bulk switching RRAM technology based on a trilayer metal-oxide
stack. We systematically engineered a trilayer metal-oxide RRAM stack and
investigated the switching characteristics of RRAM devices with varying
thicknesses and oxygen vacancy distributions across the trilayer to achieve
reliable bulk switching without any filament formation. We demonstrated bulk
switching operation at megaohm regime with high current nonlinearity and
programmed up to 100 levels without compliance current. We developed a
neuromorphic compute-in-memory platform based on trilayer bulk RRAM crossbars
by combining energy-efficient switched-capacitor voltage sensing circuits with
differential encoding of weights to experimentally demonstrate high-accuracy
matrix-vector multiplication. We showcased the computational capability of bulk
RRAM crossbars by implementing a spiking neural network model for an autonomous
navigation/racing task. Our work addresses challenges posed by existing RRAM
technologies and paves the way for neuromorphic computing at the edge under
strict size, weight, and power constraints.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13845" title="Abstract">arXiv:2310.13845</a> [<a href="/pdf/2310.13845" title="Download PDF">pdf</a>, <a href="/format/2310.13845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augment with Care: Enhancing Graph Contrastive Learning with Selective  Spectrum Perturbation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kaiqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haoyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In recent years, Graph Contrastive Learning (GCL) has shown remarkable
effectiveness in learning representations on graphs. As a component of GCL,
good augmentation views are supposed to be invariant to the important
information while discarding the unimportant part. Existing augmentation views
with perturbed graph structures are usually based on random topology corruption
in the spatial domain; however, from perspectives of the spectral domain, this
approach may be ineffective as it fails to pose tailored impacts on the
information of different frequencies, thus weakening the agreement between the
augmentation views. By a preliminary experiment, we show that the impacts
caused by spatial random perturbation are approximately evenly distributed
among frequency bands, which may harm the invariance of augmentations required
by contrastive learning frameworks. To address this issue, we argue that the
perturbation should be selectively posed on the information concerning
different frequencies. In this paper, we propose GASSER which poses tailored
perturbation on the specific frequencies of graph structures in spectral
domain, and the edge perturbation is selectively guided by the spectral hints.
As shown by extensive experiments and theoretical analysis, the augmentation
views are adaptive and controllable, as well as heuristically fitting the
homophily ratios and spectrum of graph structures.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13848" title="Abstract">arXiv:2310.13848</a> [<a href="/pdf/2310.13848" title="Download PDF">pdf</a>, <a href="/format/2310.13848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FABULA: Intelligence Report Generation Using Retrieval-Augmented  Narrative Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranade%2C+P">Priyanka Ranade</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">Anupam Joshi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining (ASONAM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Narrative construction is the process of representing disparate event
information into a logical plot structure that models an end to end story.
Intelligence analysis is an example of a domain that can benefit tremendously
from narrative construction techniques, particularly in aiding analysts during
the largely manual and costly process of synthesizing event information into
comprehensive intelligence reports. Manual intelligence report generation is
often prone to challenges such as integrating dynamic event information,
writing fine-grained queries, and closing information gaps. This motivates the
development of a system that retrieves and represents critical aspects of
events in a form that aids in automatic generation of intelligence reports.
<br />We introduce a Retrieval Augmented Generation (RAG) approach to augment
prompting of an autoregressive decoder by retrieving structured information
asserted in a knowledge graph to generate targeted information based on a
narrative plot model. We apply our approach to the problem of neural
intelligence report generation and introduce FABULA, framework to augment
intelligence analysis workflows using RAG. An analyst can use FABULA to query
an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be
used to augment prompting of a Large Language Model (LLM) during intelligence
report generation. Our evaluation studies show that the plot points included in
the generated intelligence reports have high semantic relevance, high
coherency, and low data redundancy.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13849" title="Abstract">arXiv:2310.13849</a> [<a href="/pdf/2310.13849" title="Download PDF">pdf</a>, <a href="/format/2310.13849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dual-Stream Neural Network Explains the Functional Segregation of  Dorsal and Ventral Visual Pathways in Human Brains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+M">Minkyu Choi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaokai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongming Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The human visual system uses two parallel pathways for spatial processing and
object recognition. In contrast, computer vision systems tend to use a single
feedforward pathway, rendering them less robust, adaptive, or efficient than
human vision. To bridge this gap, we developed a dual-stream vision model
inspired by the human eyes and brain. At the input level, the model samples two
complementary visual patterns to mimic how the human eyes use magnocellular and
parvocellular retinal ganglion cells to separate retinal inputs to the brain.
At the backend, the model processes the separate input patterns through two
branches of convolutional neural networks (CNN) to mimic how the human brain
uses the dorsal and ventral cortical pathways for parallel visual processing.
The first branch (WhereCNN) samples a global view to learn spatial attention
and control eye movements. The second branch (WhatCNN) samples a local view to
represent the object around the fixation. Over time, the two branches interact
recurrently to build a scene representation from moving fixations. We compared
this model with the human brains processing the same movie and evaluated their
functional alignment by linear transformation. The WhereCNN and WhatCNN
branches were found to differentially match the dorsal and ventral pathways of
the visual cortex, respectively, primarily due to their different learning
objectives. These model-based results lead us to speculate that the distinct
responses and representations of the ventral and dorsal streams are more
influenced by their distinct goals in visual attention and object recognition
than by their specific bias or selectivity in retinal inputs. This dual-stream
model takes a further step in brain-inspired computer vision, enabling parallel
neural networks to actively explore and understand the visual surroundings.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13850" title="Abstract">arXiv:2310.13850</a> [<a href="/pdf/2310.13850" title="Download PDF">pdf</a>, <a href="/format/2310.13850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ecologically Valid Explanations for Label Variation in NLI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan-Jiang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chenhao Tan</a>, 
<a href="/search/cs?searchtype=author&query=de+Marneffe%2C+M">Marie-Catherine de Marneffe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings at EMNLP 2023. Overlap with previous version <a href="/abs/2304.12443">arXiv:2304.12443</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Human label variation, or annotation disagreement, exists in many natural
language processing (NLP) tasks, including natural language inference (NLI). To
gain direct evidence of how NLI label variation arises, we build LiveNLI, an
English dataset of 1,415 ecologically valid explanations (annotators explain
the NLI labels they chose) for 122 MNLI items (at least 10 explanations per
item). The LiveNLI explanations confirm that people can systematically vary on
their interpretation and highlight within-label variation: annotators sometimes
choose the same label for different reasons. This suggests that explanations
are crucial for navigating label interpretations in general. We few-shot prompt
large language models to generate explanations but the results are
inconsistent: they sometimes produces valid and informative explanations, but
it also generates implausible ones that do not support the label, highlighting
directions for improvement.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13852" title="Abstract">arXiv:2310.13852</a> [<a href="/pdf/2310.13852" title="Download PDF">pdf</a>, <a href="/format/2310.13852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradual Domain Adaptation: Theory and Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yifei He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2204.08200">arXiv:2204.08200</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Unsupervised domain adaptation (UDA) adapts a model from a labeled source
domain to an unlabeled target domain in a one-off way. Though widely applied,
UDA faces a great challenge whenever the distribution shift between the source
and the target is large. Gradual domain adaptation (GDA) mitigates this
limitation by using intermediate domains to gradually adapt from the source to
the target domain. In this work, we first theoretically analyze gradual
self-training, a popular GDA algorithm, and provide a significantly improved
generalization bound compared with Kumar et al. (2020). Our theoretical
analysis leads to an interesting insight: to minimize the generalization error
on the target domain, the sequence of intermediate domains should be placed
uniformly along the Wasserstein geodesic between the source and target domains.
The insight is particularly useful under the situation where intermediate
domains are missing or scarce, which is often the case in real-world
applications. Based on the insight, we propose $\textbf{G}$enerative Gradual
D$\textbf{O}$main $\textbf{A}$daptation with Optimal $\textbf{T}$ransport
(GOAT), an algorithmic framework that can generate intermediate domains in a
data-dependent way. More concretely, we first generate intermediate domains
along the Wasserstein geodesic between two given consecutive domains in a
feature space, then apply gradual self-training to adapt the source-trained
classifier to the target along the sequence of intermediate domains.
Empirically, we demonstrate that our GOAT framework can improve the performance
of standard GDA when the given intermediate domains are scarce, significantly
broadening the real-world application scenarios of GDA. Our code is available
at https://github.com/yifei-he/GOAT.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13853" title="Abstract">arXiv:2310.13853</a> [<a href="/pdf/2310.13853" title="Download PDF">pdf</a>, <a href="/format/2310.13853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Discrete-time Networked Competitive Bivirus SIS Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gracy%2C+S">Sebin Gracy</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Ji Liu</a>, 
<a href="/search/eess?searchtype=author&query=Basar%2C+T">Tamer Basar</a>, 
<a href="/search/eess?searchtype=author&query=Uribe%2C+C+A">Cesar A. Uribe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The paper deals with the analysis of a discrete-time networked competitive
bivirus susceptible-infected-susceptible (SIS) model. More specifically, we
suppose that virus 1 and virus 2 are circulating in the population and are in
competition with each other. We show that the model is strongly monotone, and
that, under certain assumptions, it does not admit any periodic orbit. We
identify a sufficient condition for exponential convergence to the disease-free
equilibrium (DFE). Assuming only virus 1 (resp. virus 2) is alive, we establish
a condition for global asymptotic convergence to the single-virus endemic
equilibrium of virus 1 (resp. virus 2) -- our proof does not rely on the
construction of a Lyapunov function. Assuming both virus 1 and virus 2 are
alive, we establish a condition which ensures local exponential convergence to
the single-virus equilibrium of virus 1 (resp. virus 2). Finally, we provide a
sufficient (resp. necessary) condition for the existence of a coexistence
equilibrium.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13854" title="Abstract">arXiv:2310.13854</a> [<a href="/pdf/2310.13854" title="Download PDF">pdf</a>, <a href="/format/2310.13854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exponential weight averaging as damped harmonic motion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patsenker%2C+J">Jonathan Patsenker</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Henry Li</a>, 
<a href="/search/cs?searchtype=author&query=Kluger%2C+Y">Yuval Kluger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures. ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The exponential moving average (EMA) is a commonly used statistic for
providing stable estimates of stochastic quantities in deep learning
optimization. Recently, EMA has seen considerable use in generative models,
where it is computed with respect to the model weights, and significantly
improves the stability of the inference model during and after training. While
the practice of weight averaging at the end of training is well-studied and
known to improve estimates of local optima, the benefits of EMA over the course
of training is less understood. In this paper, we derive an explicit connection
between EMA and a damped harmonic system between two particles, where one
particle (the EMA weights) is drawn to the other (the model weights) via an
idealized zero-length spring. We then leverage this physical analogy to analyze
the effectiveness of EMA, and propose an improved training algorithm, which we
call BELAY. Finally, we demonstrate theoretically and empirically several
advantages enjoyed by BELAY over standard EMA.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13855" title="Abstract">arXiv:2310.13855</a> [<a href="/pdf/2310.13855" title="Download PDF">pdf</a>, <a href="/format/2310.13855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author  Prompt Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+P">Pengfei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+S">Simiao Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+B">Bowen Song</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Q">Qiang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jian Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Charles%2C+D">Denis Charles</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have made impressive progress in natural
language processing. These models rely on proper human instructions (or
prompts) to generate suitable responses. However, the potential of LLMs are not
fully harnessed by commonly-used prompting methods: many human-in-the-loop
algorithms employ ad-hoc procedures for prompt selection; while auto prompt
generation approaches are essentially searching all possible prompts randomly
and inefficiently. We propose Evoke, an automatic prompt refinement framework.
In Evoke, there are two instances of a same LLM: one as a reviewer
(LLM-Reviewer), it scores the current prompt; the other as an author
(LLM-Author), it edits the prompt by considering the edit history and the
reviewer's feedback. Such an author-reviewer feedback loop ensures that the
prompt is refined in each iteration. We further aggregate a data selection
approach to Evoke, where only the hard samples are exposed to the LLM. The hard
samples are more important because the LLM can develop deeper understanding of
the tasks out of them, while the model may already know how to solve the easier
cases. Experimental results show that Evoke significantly outperforms existing
methods. For instance, in the challenging task of logical fallacy detection,
Evoke scores above 80, while all other baseline methods struggle to reach 20.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13856" title="Abstract">arXiv:2310.13856</a> [<a href="/pdf/2310.13856" title="Download PDF">pdf</a>, <a href="/format/2310.13856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implications of Annotation Artifacts in Edge Probing Test Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S+R">Sagnik Ray Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Kalra%2C+J">Jushaan Kalra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted CoNLL 2023, code: <a href="https://github.com/Josh1108/EPtest.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Edge probing tests are classification tasks that test for grammatical
knowledge encoded in token representations coming from contextual encoders such
as large language models (LLMs). Many LLM encoders have shown high performance
in EP tests, leading to conjectures about their ability to encode linguistic
knowledge. However, a large body of research claims that the tests necessarily
do not measure the LLM's capacity to encode knowledge, but rather reflect the
classifiers' ability to learn the problem. Much of this criticism stems from
the fact that often the classifiers have very similar accuracy when an LLM vs a
random encoder is used. Consequently, several modifications to the tests have
been suggested, including information theoretic probes. We show that commonly
used edge probing test datasets have various biases including memorization.
When these biases are removed, the LLM encoders do show a significant
difference from the random ones, even with the simple non-information theoretic
probes.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13859" title="Abstract">arXiv:2310.13859</a> [<a href="/pdf/2310.13859" title="Download PDF">pdf</a>, <a href="/format/2310.13859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not all Fake News is Written: A Dataset and Analysis of Misleading Video  Headlines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y+Y">Yoo Yeon Sung</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+N">Naeemul Hassan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Polarization and the marketplace for impressions have conspired to make
navigating information online difficult for users, and while there has been a
significant effort to detect false or misleading text, multimodal datasets have
received considerably less attention. To complement existing resources, we
present multimodal Video Misleading Headline (VMH), a dataset that consists of
videos and whether annotators believe the headline is representative of the
video's contents. After collecting and annotating this dataset, we analyze
multimodal baselines for detecting misleading headlines. Our annotation process
also focuses on why annotators view a video as misleading, allowing us to
better understand the interplay of annotators' background and the content of
the videos.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13861" title="Abstract">arXiv:2310.13861</a> [<a href="/pdf/2310.13861" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining the Influence of Job Satisfaction on Individual Innovation and  Its Components: Considering the Moderating Role of Technostress
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daneshmandi%2C+F">Fatemeh Daneshmandi</a>, 
<a href="/search/cs?searchtype=author&query=Hessari%2C+H">Hassan Hessari</a>, 
<a href="/search/cs?searchtype=author&query=Nategh%2C+T">Tahmineh Nategh</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+A">Ali Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Background: Employee innovation is a crucial aspect of organizations in the
current era. Therefore, studying the factors influencing individual innovation
is vital and unavoidable. Undoubtedly, job satisfaction is a significant
variable in management sciences. Nowadays, all organizations are interconnected
with technology. Objective: This research explores the relationship between job
satisfaction and individual innovation, including its components, and the
moderating role of technostress. Research Method: This study, in terms of
purpose, is applied, and in terms of data collection method, it is a
descriptive survey. Data collection tools included the Technostress Inventory
by Tarafdar and colleagues (2007), Janssen's Individual Innovation
Questionnaire (2000), and the Job Satisfaction Survey (JSS) by Spector (1994).
The validity and reliability of these questionnaires were confirmed. The sample
size for this study was 215, and data analysis was performed using SPSS and
SMART-PLS software. Findings: Job satisfaction has a significant and positive
relationship with individual innovation, idea generation, idea promotion, and
idea implementation. Technostress moderates the relationship between job
satisfaction and individual innovation, as well as idea generation and idea
promotion. However, technostress does not play a moderating role in the
relationship between job satisfaction and idea implementation. Conclusion:
Based on the obtained results, organizations should take necessary measures to
increase job satisfaction and reduce technostress among their employees.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13862" title="Abstract">arXiv:2310.13862</a> [<a href="/pdf/2310.13862" title="Download PDF">pdf</a>, <a href="/format/2310.13862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Competitive Advantage Attacks to Decentralized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yuqi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Minghong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Decentralized federated learning (DFL) enables clients (e.g., hospitals and
banks) to jointly train machine learning models without a central orchestration
server. In each global training round, each client trains a local model on its
own training data and then they exchange local models for aggregation. In this
work, we propose SelfishAttack, a new family of attacks to DFL. In
SelfishAttack, a set of selfish clients aim to achieve competitive advantages
over the remaining non-selfish ones, i.e., the final learnt local models of the
selfish clients are more accurate than those of the non-selfish ones. Towards
this goal, the selfish clients send carefully crafted local models to each
remaining non-selfish one in each global training round. We formulate finding
such local models as an optimization problem and propose methods to solve it
when DFL uses different aggregation rules. Theoretically, we show that our
methods find the optimal solutions to the optimization problem. Empirically, we
show that SelfishAttack successfully increases the accuracy gap (i.e.,
competitive advantage) between the final learnt local models of selfish clients
and those of non-selfish ones. Moreover, SelfishAttack achieves larger accuracy
gaps than poisoning attacks when extended to increase competitive advantages.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13864" title="Abstract">arXiv:2310.13864</a> [<a href="/pdf/2310.13864" title="Download PDF">pdf</a>, <a href="/format/2310.13864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RECAP: Towards Precise Radiology Report Generation via Dynamic Disease  Progression Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+W">Wenjun Hou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaishuai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automating radiology report generation can significantly alleviate
radiologists' workloads. Previous research has primarily focused on realizing
highly concise observations while neglecting the precise attributes that
determine the severity of diseases (e.g., small pleural effusion). Since
incorrect attributes will lead to imprecise radiology reports, strengthening
the generation process with precise attribute modeling becomes necessary.
Additionally, the temporal information contained in the historical records,
which is crucial in evaluating a patient's current condition (e.g., heart size
is unchanged), has also been largely disregarded. To address these issues, we
propose RECAP, which generates precise and accurate radiology reports via
dynamic disease progression reasoning. Specifically, RECAP first predicts the
observations and progressions (i.e., spatiotemporal information) given two
consecutive radiographs. It then combines the historical records,
spatiotemporal information, and radiographs for report generation, where a
disease progression graph and dynamic progression reasoning mechanism are
devised to accurately select the attributes of each observation and
progression. Extensive experiments on two publicly available datasets
demonstrate the effectiveness of our model.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13870" title="Abstract">arXiv:2310.13870</a> [<a href="/pdf/2310.13870" title="Download PDF">pdf</a>, <a href="/format/2310.13870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Approximation of Similarity Graphs with Kernel Density Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macgregor%2C+P">Peter Macgregor</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">He Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Spotlight paper at NeurIPS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Constructing a similarity graph from a set $X$ of data points in
$\mathbb{R}^d$ is the first step of many modern clustering algorithms. However,
typical constructions of a similarity graph have high time complexity, and a
quadratic space dependency with respect to $|X|$. We address this limitation
and present a new algorithmic framework that constructs a sparse approximation
of the fully connected similarity graph while preserving its cluster structure.
Our presented algorithm is based on the kernel density estimation problem, and
is applicable for arbitrary kernel functions. We compare our designed algorithm
with the well-known implementations from the scikit-learn library and the FAISS
library, and find that our method significantly outperforms the implementation
from both libraries on a variety of datasets.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13876" title="Abstract">arXiv:2310.13876</a> [<a href="/pdf/2310.13876" title="Download PDF">pdf</a>, <a href="/format/2310.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Transformer Using Cross-Channel attention for Object  Detection in Remote Sensing Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bahaduri%2C+B">Bissmella Bahaduri</a>, 
<a href="/search/cs?searchtype=author&query=Ming%2C+Z">Zuheng Ming</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fangchen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Mokraou%2C+A">Anissa Mokraou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ICASSP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Object detection in Remote Sensing Images (RSI) is a critical task for
numerous applications in Earth Observation (EO). Unlike general object
detection, object detection in RSI has specific challenges: 1) the scarcity of
labeled data in RSI compared to general object detection datasets, and 2) the
small objects presented in a high-resolution image with a vast background. To
address these challenges, we propose a multimodal transformer exploring
multi-source remote sensing data for object detection. Instead of directly
combining the multimodal input through a channel-wise concatenation, which
ignores the heterogeneity of different modalities, we propose a cross-channel
attention module. This module learns the relationship between different
channels, enabling the construction of a coherent multimodal input by aligning
the different modalities at the early stage. We also introduce a new
architecture based on the Swin transformer that incorporates convolution layers
in non-shifting blocks while maintaining fixed dimensions, allowing for the
generation of fine-to-coarse representations with a favorable
accuracy-computation trade-off. The extensive experiments prove the
effectiveness of the proposed multimodal fusion module and architecture,
demonstrating their applicability to multimodal aerial imagery.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13881" title="Abstract">arXiv:2310.13881</a> [<a href="/pdf/2310.13881" title="Download PDF">pdf</a>, <a href="/format/2310.13881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Adaptive Coding for Two-Way Wiretap Channel with or without Cost  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+M">Masahito Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanling Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">This paper studies the secrecy results for the two-way wiretap channel
(TW-WC) with an external eavesdropper under a strong secrecy metric. Employing
non-adaptive coding, we analyze the information leakage and the decoding error
probability, and derive inner bounds on the secrecy capacity regions for the
TW-WC under strong joint and individual secrecy constraints. For the TW-WC
without cost constraint, both the secrecy and error exponents could be
characterized by the conditional R\'enyi mutual information in a concise and
compact form. And, some special cases secrecy capacity region and sum-rate
capacity results are established, demonstrating that adaption is useless in
some cases or the maximum sum-rate that could be achieved by non-adaptive
coding. For the TW-WC with cost constraint, we consider the peak cost
constraint and extend our secrecy results by using the constant composition
codes. Accordingly, we characterize both the secrecy and error exponents by a
modification of R\'enyi mutual information, which yields inner bounds on the
secrecy capacity regions for the general discrete memoryless TW-WC with cost
constraint. Our method works even when a pre-noisy processing is employed based
on a conditional distribution in the encoder and can be easily extended to
other multi-user communication scenarios.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13883" title="Abstract">arXiv:2310.13883</a> [<a href="/pdf/2310.13883" title="Download PDF">pdf</a>, <a href="/format/2310.13883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Model Predictive Control for Enhanced Fast Charging on Electric  Vehicles through Integrated Power and Thermal Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+Q">Qiuhao Hu</a>, 
<a href="/search/eess?searchtype=author&query=Amini%2C+M+R">Mohammad Reza Amini</a>, 
<a href="/search/eess?searchtype=author&query=Wiese%2C+A">Ashley Wiese</a>, 
<a href="/search/eess?searchtype=author&query=Kolmanovsky%2C+I">Ilya Kolmanovsky</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+J">Jing Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 62nd Conference on Decision and Control (CDC), December 13-15, 2023, Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper explores the synergies between integrated power and thermal
management (iPTM) and battery charging in an electric vehicle (EV). A
multi-objective model predictive control (MPC) framework is developed to
optimize the fast charging performance while enforcing the constraints in the
power and thermal loops. The approach takes into account the coupling of the
battery and cabin thermal management. The case study of a commercial EV
demonstrates that the proposed method can effectively meet the requirements of
fast charging and thermal management when accurate preview information is
available. However, failure to predict the charging event can result in
performance degradation with longer charging time. A time-varying weighting
strategy is proposed to enhance charging performance in the presence of
uncertainty. This strategy leverages the battery state-of-charge (SOC) and
adjusts the priority of the multi-objective MPC at different phases during
charging. Simulated results using a commercial EV use case show improved
robustness in charging time using the proposed strategy.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13886" title="Abstract">arXiv:2310.13886</a> [<a href="/pdf/2310.13886" title="Download PDF">pdf</a>, <a href="/format/2310.13886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport-based Nonlinear Filtering in High-dimensional Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Jarrah%2C+M">Mohammad Al-Jarrah</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+N">Niyizhen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+B">Bamdad Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Taghvaei%2C+A">Amirhossein Taghvaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper addresses the problem of nonlinear filtering, i.e., computing the
conditional distribution of the state of a stochastic dynamical system given a
history of noisy partial observations. The primary focus is on scenarios
involving degenerate likelihoods or high-dimensional states, where traditional
sequential importance resampling (SIR) particle filters face the weight
degeneracy issue. Our proposed method builds on an optimal transport
interpretation of nonlinear filtering, leading to a simulation-based and
likelihood-free algorithm that estimates the Brenier optimal transport map from
the current distribution of the state to the distribution at the next time
step. Our formulation allows us to harness the approximation power of neural
networks to model complex and multi-modal distributions and employ stochastic
optimization algorithms to enhance scalability. Extensive numerical experiments
are presented that compare our method to the SIR particle filter and the
ensemble Kalman filter, demonstrating the superior performance of our method in
terms of sample efficiency, high-dimensional scalability, and the ability to
capture complex and multi-modal distributions.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13888" title="Abstract">arXiv:2310.13888</a> [<a href="/pdf/2310.13888" title="Download PDF">pdf</a>, <a href="/format/2310.13888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a General Framework for Continual Learning with Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jingyi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a generalized version of our HiDe-Prompt and will be presented in the IMOL workshop in NeurIPS 2023. arXiv admin note: text overlap with <a href="/abs/2310.07234">arXiv:2310.07234</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we present a general framework for continual learning of
sequentially arrived tasks with the use of pre-training, which has emerged as a
promising direction for artificial intelligence systems to accommodate
real-world dynamics. From a theoretical perspective, we decompose its objective
into three hierarchical components, including within-task prediction,
task-identity inference, and task-adaptive prediction. Then we propose an
innovative approach to explicitly optimize these components with
parameter-efficient fine-tuning (PEFT) techniques and representation
statistics. We empirically demonstrate the superiority and generality of our
approach in downstream continual learning, and further explore the
applicability of PEFT techniques in upstream continual learning. We also
discuss the biological basis of the proposed framework with recent advances in
neuroscience.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13889" title="Abstract">arXiv:2310.13889</a> [<a href="/pdf/2310.13889" title="Download PDF">pdf</a>, <a href="/format/2310.13889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Experimental Study of Model-based Control for Planar Handed Shearing  Auxetics Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=St%C3%B6lzle%2C+M">Maximilian St&#xf6;lzle</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>, 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Parallel robots based on Handed Shearing Auxetics (HSAs) can implement
complex motions using standard electric motors while maintaining the complete
softness of the structure, thanks to specifically designed architected
metamaterials. However, their control is especially challenging due to varying
and coupled stiffness, shearing, non-affine terms in the actuation model, and
underactuation. In this paper, we present a model-based control strategy for
planar HSA robots enabling regulation in task space. We formulate equations of
motion, show that they admit a collocated form, and design a P-satI-D feedback
controller with compensation for elastic and gravitational forces. We
experimentally identify and verify the proposed control strategy in closed
loop.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13890" title="Abstract">arXiv:2310.13890</a> [<a href="/pdf/2310.13890" title="Download PDF">pdf</a>, <a href="/format/2310.13890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COVIDFakeExplainer: An Explainable Machine Learning based Web  Application for Detecting COVID-19 Fake News
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Warman%2C+D">Dylan Warman</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+A">Muhammad Ashad Kabir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Fake news has emerged as a critical global issue, magnified by the COVID-19
pandemic, underscoring the need for effective preventive tools. Leveraging
machine learning, including deep learning techniques, offers promise in
combatting fake news. This paper goes beyond by establishing BERT as the
superior model for fake news detection and demonstrates its utility as a tool
to empower the general populace. We have implemented a browser extension,
enhanced with explainability features, enabling real-time identification of
fake news and delivering easily interpretable explanations. To achieve this, we
have employed two publicly available datasets and created seven distinct data
configurations to evaluate three prominent machine learning architectures. Our
comprehensive experiments affirm BERT's exceptional accuracy in detecting
COVID-19-related fake news. Furthermore, we have integrated an explainability
component into the BERT model and deployed it as a service through Amazon's
cloud API hosting (AWS). We have developed a browser extension that interfaces
with the API, allowing users to select and transmit data from web pages,
receiving an intelligible classification in return. This paper presents a
practical end-to-end solution, highlighting the feasibility of constructing a
holistic system for fake news detection, which can significantly benefit
society.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13892" title="Abstract">arXiv:2310.13892</a> [<a href="/pdf/2310.13892" title="Download PDF">pdf</a>, <a href="/format/2310.13892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specify Robust Causal Representation from Mixed Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Furui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2202.08388">arXiv:2202.08388</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning representations purely from observations concerns the problem of
learning a low-dimensional, compact representation which is beneficial to
prediction models. Under the hypothesis that the intrinsic latent factors
follow some casual generative models, we argue that by learning a causal
representation, which is the minimal sufficient causes of the whole system, we
can improve the robustness and generalization performance of machine learning
models. In this paper, we develop a learning method to learn such
representation from observational data by regularizing the learning procedure
with mutual information measures, according to the hypothetical factored causal
graph. We theoretically and empirically show that the models trained with the
learned causal representations are more robust under adversarial attacks and
distribution shifts compared with baselines. The supplementary materials are
available at https://github.com/ymy $4323460 / \mathrm{CaRI} /$.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13893" title="Abstract">arXiv:2310.13893</a> [<a href="/pdf/2310.13893" title="Download PDF">pdf</a>, <a href="/format/2310.13893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hidden Adversarial Vulnerabilities of Medical Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darzi%2C+E">Erfan Darzi</a>, 
<a href="/search/cs?searchtype=author&query=Dubost%2C+F">Florian Dubost</a>, 
<a href="/search/cs?searchtype=author&query=Sijtsema%2C+N+M">Nanna. M. Sijtsema</a>, 
<a href="/search/cs?searchtype=author&query=van+Ooijen%2C+P+M+A">P.M.A van Ooijen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we delve into the susceptibility of federated medical image
analysis systems to adversarial attacks. Our analysis uncovers a novel
exploitation avenue: using gradient information from prior global model
updates, adversaries can enhance the efficiency and transferability of their
attacks. Specifically, we demonstrate that single-step attacks (e.g. FGSM),
when aptly initialized, can outperform the efficiency of their iterative
counterparts but with reduced computational demand. Our findings underscore the
need to revisit our understanding of AI security in federated healthcare
settings.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13894" title="Abstract">arXiv:2310.13894</a> [<a href="/pdf/2310.13894" title="Download PDF">pdf</a>, <a href="/format/2310.13894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VOICE-ZEUS: Impersonating Zoom&#x27;s E2EE-Protected Static Media and Textual  Communications via Simple Voice Manipulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alatawi%2C+M">Mashari Alatawi</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+N">Nitesh Saxena</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The authentication ceremony plays a crucial role in verifying the identities
of users before exchanging messages in end-to-end encryption (E2EE)
applications, thus preventing impersonation and man-in-the-middle (MitM)
attacks. Once authenticated, the subsequent communications in E2EE apps benefit
from the protection provided by the authentication ceremony. However, the
current implementation of the authentication ceremony in the Zoom application
introduces a potential vulnerability that can make it highly susceptible to
impersonation attacks. The existence of this vulnerability may undermine the
integrity of E2EE, posing a potential security risk when E2EE becomes a
mandatory feature in the Zoom application. In this paper, we examine and
evaluate this vulnerability in two attack scenarios, one where the attacker is
a malicious participant and another where the attacker is a malicious Zoom
server with control over Zoom's server infrastructure and cloud providers. Our
study aims to comprehensively examine the Zoom authentication ceremony, with a
specific focus on the potential for impersonation attacks in static media and
textual communications. We simulate a new session injection attack on Zoom E2EE
meetings to evaluate the system's susceptibility to simple voice manipulations.
Our simulation experiments show that Zoom's authentication ceremony is
vulnerable to a simple voice manipulation, called a VOICE-ZEUS attack, by
malicious participants and the malicious Zoom server. In this VOICE-ZEUS
attack, an attacker creates a fingerprint in a victim's voice by reordering
previously recorded digits spoken by the victim. We show how an attacker can
record and reorder snippets of digits to generate a new security code that
compromises a future Zoom meeting. We conclude that stronger security measures
are necessary during the group authentication ceremony in Zoom to prevent
impersonation attacks.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13895" title="Abstract">arXiv:2310.13895</a> [<a href="/pdf/2310.13895" title="Download PDF">pdf</a>, <a href="/format/2310.13895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RTSUM: Relation Triple-based Interpretable Summarization with  Multi-level Salience Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Seonglae Cho</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Yonggi Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">HoonJae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+M">Myungha Jang</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongha Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we present RTSUM, an unsupervised summarization framework that
utilizes relation triples as the basic unit for summarization. Given an input
document, RTSUM first selects salient relation triples via multi-level salience
scoring and then generates a concise summary from the selected relation triples
by using a text-to-text language model. On the basis of RTSUM, we also develop
a web demo for an interpretable summarizing tool, providing fine-grained
interpretations with the output summary. With support for customization
options, our tool visualizes the salience for textual units at three distinct
levels: sentences, relation triples, and phrases. The codes,are publicly
available.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13896" title="Abstract">arXiv:2310.13896</a> [<a href="/pdf/2310.13896" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPTutor: an open-source AI pair programming tool alternative to Copilot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ray Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Justa Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Damien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pierce Hung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The emergence of Large Language Models (LLMs) has improved software
development efficiency, but their performance can be hindered by training data
limitations and prompt design issues. Existing LLM development tools often
operate as black boxes, with users unable to view the prompts used and unable
to improve performance by correcting prompts when errors occur. To address the
aforementioned issues, GPTutor was introduced as an open-source AI pair
programming tool, offering an alternative to Copilot. GPTutor empowers users to
customize prompts for various programming languages and scenarios, with support
for 120+ human languages and 50+ programming languages. Users can fine-tune
prompts to correct the errors from LLM for precision and efficient code
generation. At the end of the paper, we underscore GPTutor's potential through
examples, including demonstrating its proficiency in interpreting and
generating Sui-Move, a newly introduced smart contract language, using prompt
engineering.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13897" title="Abstract">arXiv:2310.13897</a> [<a href="/pdf/2310.13897" title="Download PDF">pdf</a>, <a href="/format/2310.13897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly  the Star-Free Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Angluin%2C+D">Dana Angluin</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+D">David Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+A">Andy Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We consider transformer encoders with hard attention (in which all attention
is focused on exactly one position) and strict future masking (in which each
position only attends to positions strictly to its left), and prove that the
class of languages recognized by these networks is exactly the star-free
languages. Adding position embeddings increases the class of recognized
languages to other well-studied classes. A key technique in these proofs is
Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the
star-free languages, we relate transformers to first-order logic, temporal
logic, and algebraic automata theory.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13899" title="Abstract">arXiv:2310.13899</a> [<a href="/pdf/2310.13899" title="Download PDF">pdf</a>, <a href="/format/2310.13899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FHT-Map: Feature-based Hierarchical Topological Map for Relocalization  and Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kun Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gaoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhenhua Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Topological maps are favorable for their small storage compared to geometric
map. However, they are limited in relocalization and path planning
capabilities. To solve this problem, a feature-based hierarchical topological
map (FHT-Map) is proposed along with a real-time map construction algorithm for
robot exploration. Specifically, the FHT-Map utilizes both RGB cameras and
LiDAR information and consists of two types of nodes: main node and support
node. Main nodes will store visual information compressed by convolutional
neural network and local laser scan data to enhance subsequent relocalization
capability. Support nodes retain a minimal amount of data to ensure storage
efficiency while facilitating path planning. After map construction with robot
exploration, the FHT-Map can be used by other robots for relocalization and
path planning. Experiments are conducted in Gazebo simulator, and the results
demonstrate that the proposed FHT-Map can effectively improve relocalization
and path planning capability compared with other topological maps. Moreover,
experiments on hierarchical architecture are implemented to show the necessity
of two types of nodes.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13900" title="Abstract">arXiv:2310.13900</a> [<a href="/pdf/2310.13900" title="Download PDF">pdf</a>, <a href="/format/2310.13900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Proof of Solvency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bateni%2C+H">Hamid Bateni</a>, 
<a href="/search/cs?searchtype=author&query=Kambakhsh%2C+K">Keyvan Kambakhsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The Private Proof of Solvency is a groundbreaking solution in the realm of
Proof of Solvency, offering a secure, efficient, and privacy-preserving method
for crypto custody providers such as centralized cryptocurrency exchanges or
enterprise custody providers. By leveraging the inherent state concept of every
blockchain and pioneering cryptographic techniques like zkp, our approach
ensures businesses can prove their reserves without revealing their
transactions, addresses, or the total amount of liabilities.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13901" title="Abstract">arXiv:2310.13901</a> [<a href="/pdf/2310.13901" title="Download PDF">pdf</a>, <a href="/format/2310.13901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Hyperparameter-Agnostic DNN Training via Dynamical System  Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiscko%2C+C">Carmel Fiscko</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Aayushya Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+Y">Yihan Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Soummya Kar</a>, 
<a href="/search/cs?searchtype=author&query=Pileggi%2C+L">Larry Pileggi</a>, 
<a href="/search/cs?searchtype=author&query=Sinopoli%2C+B">Bruno Sinopoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We present a stochastic first-order optimization method specialized for deep
neural networks (DNNs), ECCO-DNN. This method models the optimization variable
trajectory as a dynamical system and develops a discretization algorithm that
adaptively selects step sizes based on the trajectory's shape. This provides
two key insights: designing the dynamical system for fast continuous-time
convergence and developing a time-stepping algorithm to adaptively select step
sizes based on principles of numerical integration and neural network
structure. The result is an optimizer with performance that is insensitive to
hyperparameter variations and that achieves comparable performance to
state-of-the-art optimizers including ADAM, SGD, RMSProp, and AdaGrad. We
demonstrate this in training DNN models and datasets, including CIFAR-10 and
CIFAR-100 using ECCO-DNN and find that ECCO-DNN's single hyperparameter can be
changed by three orders of magnitude without affecting the trained models'
accuracies. ECCO-DNN's insensitivity reduces the data and computation needed
for hyperparameter tuning, making it advantageous for rapid prototyping and for
applications with new datasets. To validate the efficacy of our proposed
optimizer, we train an LSTM architecture on a household power consumption
dataset with ECCO-DNN and achieve an optimal mean-square-error without tuning
hyperparameters.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13906" title="Abstract">arXiv:2310.13906</a> [<a href="/pdf/2310.13906" title="Download PDF">pdf</a>, <a href="/format/2310.13906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Driving Behavior for Autonomous Vehicles Based on Gramian  Angular Field Vision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+J">Junwei You</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Ying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuoyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhangchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zilin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yifeng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+B">Bin Ran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Effective classification of autonomous vehicle (AV) driving behavior emerges
as a critical area for diagnosing AV operation faults, enhancing autonomous
driving algorithms, and reducing accident rates. This paper presents the
Gramian Angular Field Vision Transformer (GAF-ViT) model, designed to analyze
AV driving behavior. The proposed GAF-ViT model consists of three key
components: GAF Transformer Module, Channel Attention Module, and Multi-Channel
ViT Module. These modules collectively convert representative sequences of
multivariate behavior into multi-channel images and employ image recognition
techniques for behavior classification. A channel attention mechanism is
applied to multi-channel images to discern the impact of various driving
behavior features. Experimental evaluation on the Waymo Open Dataset of
trajectories demonstrates that the proposed model achieves state-of-the-art
performance. Furthermore, an ablation study effectively substantiates the
efficacy of individual modules within the model.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13908" title="Abstract">arXiv:2310.13908</a> [<a href="/pdf/2310.13908" title="Download PDF">pdf</a>, <a href="/format/2310.13908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical simulation of an extensible capsule using regularized Stokes  kernels and overset finite differences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Agarwal%2C+D">Dhwanit Agarwal</a>, 
<a href="/search/math?searchtype=author&query=Biros%2C+G">George Biros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we present a novel numerical scheme for simulating deformable
and extensible capsules suspended in a Stokesian fluid. The main feature of our
scheme is a partition-of-unity (POU) based representation of the surface that
enables asymptotically faster computations compared to spherical-harmonics
based representations. We use a boundary integral equation formulation to
represent and discretize hydrodynamic interactions. The boundary integrals are
weakly singular. We use the quadrature scheme based on the regularized Stokes
kernels. We also use partition-of unity based finite differences that are
required for the computational of interfacial forces. Given an N-point surface
discretization, our numerical scheme has fourth-order accuracy and O(N)
asymptotic complexity, which is an improvement over the O(N^2 log(N))
complexity of a spherical harmonics based spectral scheme that uses
product-rule quadratures. We use GPU acceleration and demonstrate the ability
of our code to simulate the complex shapes with high resolution. We study
capsules that resist shear and tension and their dynamics in shear and
Poiseuille flows. We demonstrate the convergence of the scheme and compare with
the state of the art.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13912" title="Abstract">arXiv:2310.13912</a> [<a href="/pdf/2310.13912" title="Download PDF">pdf</a>, <a href="/format/2310.13912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Motion Refinement for Unsupervised Face Animation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Jiale Tao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Shuhang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wen Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+L">Lixin Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised face animation aims to generate a human face video based on the
appearance of a source image, mimicking the motion from a driving video.
Existing methods typically adopted a prior-based motion model (e.g., the local
affine motion model or the local thin-plate-spline motion model). While it is
able to capture the coarse facial motion, artifacts can often be observed
around the tiny motion in local areas (e.g., lips and eyes), due to the limited
ability of these methods to model the finer facial motions. In this work, we
design a new unsupervised face animation approach to learn simultaneously the
coarse and finer motions. In particular, while exploiting the local affine
motion model to learn the global coarse facial motion, we design a novel motion
refinement module to compensate for the local affine motion model for modeling
finer face motions in local areas. The motion refinement is learned from the
dense correlation between the source and driving images. Specifically, we first
construct a structure correlation volume based on the keypoint features of the
source and driving images. Then, we train a model to generate the tiny facial
motions iteratively from low to high resolution. The learned motion refinements
are combined with the coarse motion to generate the new image. Extensive
experiments on widely used benchmarks demonstrate that our method achieves the
best results among state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13913" title="Abstract">arXiv:2310.13913</a> [<a href="/pdf/2310.13913" title="Download PDF">pdf</a>, <a href="/format/2310.13913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Training on Large-Scale Generated Docking Conformations with  HelixDock to Unlock the Potential of Protein-ligand Structure Prediction  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lihang Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Donglong He</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xianbin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanzhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaonan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+H">Hua Chai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jingzhou He</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yonghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xiaomin Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Molecular docking, a pivotal computational tool for drug discovery, predicts
the binding interactions between small molecules (ligands) and target proteins
(receptors). Conventional physics-based docking tools, though widely used, face
limitations in precision due to restricted conformational sampling and
imprecise scoring functions. Recent endeavors have employed deep learning
techniques to enhance docking accuracy, but their generalization remains a
concern due to limited training data. Leveraging the success of extensive and
diverse data in other domains, we introduce HelixDock, a novel approach for
site-specific molecular docking. Hundreds of millions of binding poses are
generated by traditional docking tools, encompassing diverse protein targets
and small molecules. Our deep learning-based docking model, a SE(3)-equivariant
network, is pre-trained with this large-scale dataset and then fine-tuned with
a small number of precise receptor-ligand complex structures. Comparative
analyses against physics-based and deep learning-based baseline methods
highlight HelixDock's superiority, especially on challenging test sets. Our
study elucidates the scaling laws of the pre-trained molecular docking models,
showcasing consistent improvements with increased model parameters and
pre-train data quantities. Harnessing the power of extensive and diverse
generated data holds promise for advancing AI-driven drug discovery.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13914" title="Abstract">arXiv:2310.13914</a> [<a href="/pdf/2310.13914" title="Download PDF">pdf</a>, <a href="/format/2310.13914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cold Diffusion on the Replay Buffer: Learning to Plan from Known Good  States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zidan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Oba%2C+T">Takeru Oba</a>, 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Takuma Yoneda</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+R">Rui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+M">Matthew Walter</a>, 
<a href="/search/cs?searchtype=author&query=Stadie%2C+B+C">Bradly C. Stadie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Learning from demonstrations (LfD) has successfully trained robots to exhibit
remarkable generalization capabilities. However, many powerful imitation
techniques do not prioritize the feasibility of the robot behaviors they
generate. In this work, we explore the feasibility of plans produced by LfD. As
in prior work, we employ a temporal diffusion model with fixed start and goal
states to facilitate imitation through in-painting. Unlike previous studies, we
apply cold diffusion to ensure the optimization process is directed through the
agent's replay buffer of previously visited states. This routing approach
increases the likelihood that the final trajectories will predominantly occupy
the feasible region of the robot's state space. We test this method in
simulated robotic environments with obstacles and observe a significant
improvement in the agent's ability to avoid these obstacles during planning.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13915" title="Abstract">arXiv:2310.13915</a> [<a href="/pdf/2310.13915" title="Download PDF">pdf</a>, <a href="/format/2310.13915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vida%2C+K">Karina Vida</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+J">Judith Simon</a>, 
<a href="/search/cs?searchtype=author&query=Lauscher%2C+A">Anne Lauscher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">With language technology increasingly affecting individuals' lives, many
recent works have investigated the ethical aspects of NLP. Among other topics,
researchers focused on the notion of morality, investigating, for example,
which moral judgements language models make. However, there has been little to
no discussion of the terminology and the theories underpinning those efforts
and their implications. This lack is highly problematic, as it hides the works'
underlying assumptions and hinders a thorough and targeted scientific debate of
morality in NLP. In this work, we address this research gap by (a) providing an
overview of some important ethical concepts stemming from philosophy and (b)
systematically surveying the existing literature on moral NLP w.r.t. their
philosophical foundation, terminology, and data basis. For instance, we analyse
what ethical theory an approach is based on, how this decision is justified,
and what implications it entails. Our findings surveying 92 papers show that,
for instance, most papers neither provide a clear definition of the terms they
use nor adhere to definitions from philosophy. Finally, (c) we give three
recommendations for future research in the field. We hope our work will lead to
a more informed, careful, and sound discussion of morality in language
technology.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13917" title="Abstract">arXiv:2310.13917</a> [<a href="/pdf/2310.13917" title="Download PDF">pdf</a>, <a href="/format/2310.13917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beamforming Design for the Distributed RISs-aided THz Communications  with Double-Layer True Time Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Gangcan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wencai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Wanming Hao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we investigate the reconfigurable intelligent surface
(RIS)-aided terahertz (THz) communication system with the sparse radio
frequency chains antenna structure at the base station (BS). To overcome the
beam split of the BS, different from the conventional single-layer
true-time-delay (TTD) scheme, we propose a double-layer TTD scheme that can
effectively reduce the number of large-range delay devices, which involve
additional insertion loss and amplification circuitry. Next, we analyze the
system performance under the proposed double-layer TTD scheme. To relieve the
beam split of the RIS, we consider multiple distributed RISs to replace an
ultra-large size RIS. Based on this, we formulate an achievable rate
maximization problem for the distributed RISs-aided THz communications via
jointly optimizing the hybrid analog/digital beamforming, time delays of the
double-layer TTD network and reflection coefficients of RISs. Considering the
practical hardware limitation, the finite-resolution phase shift, time delay
and reflection phase are constrained. To solve the formulated problem, we first
design an analog beamforming scheme including optimizing phase shift and time
delay based on the RISs' locations. Then, an alternatively optimization
algorithm is proposed to obtain the digital beamforming and reflection
coefficients based on the minimum mean square error and coordinate update
techniques. Finally, simulation results show the effectiveness of the proposed
scheme.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13920" title="Abstract">arXiv:2310.13920</a> [<a href="/pdf/2310.13920" title="Download PDF">pdf</a>, <a href="/format/2310.13920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New lower order mixed finite element methods for linear elasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xuehai Huang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Y">Yaqian Zhou</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+Y">Yangxing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">New lower order $H(\textrm{div})$-conforming finite elements for symmetric
tensors are constructed in arbitrary dimension. The space of shape functions is
defined by enriching the symmetric quadratic polynomial space with the
$(d+1)$-order normal-normal face bubble space. The reduced counterpart has only
$d(d+1)^2$ degrees of freedom. In two dimensions, basis functions are
explicitly given in terms of barycentric coordinates. Lower order conforming
finite element elasticity complexes starting from the Bell element, are
developed in two dimensions. These finite elements for symmetric tensors are
applied to devise robust mixed finite element methods for the linear elasticity
problem, which possess the uniform error estimates with respect to the Lam\'{e}
coefficient $\lambda$, and superconvergence for the displacement. Numerical
results are provided to verify the theoretical convergence rates.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13921" title="Abstract">arXiv:2310.13921</a> [<a href="/pdf/2310.13921" title="Download PDF">pdf</a>, <a href="/format/2310.13921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UnifiedSSR: A Unified Framework of Sequential Search and Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiayi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+G">Gao Cong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenzhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In this work, we propose a Unified framework of Sequential Search and
Recommendation (UnifiedSSR) for joint learning of user behavior history in both
search and recommendation scenarios. Specifically, we consider user-interacted
products in the recommendation scenario, user-interacted products and
user-issued queries in the search scenario as three distinct types of user
behaviors. We propose a dual-branch network to encode the pair of interacted
product history and issued query history in the search scenario in parallel.
This allows for cross-scenario modeling by deactivating the query branch for
the recommendation scenario. Through the parameter sharing between dual
branches, as well as between product branches in two scenarios, we incorporate
cross-view and cross-scenario associations of user behaviors, providing a
comprehensive understanding of user behavior patterns. To further enhance user
behavior modeling by capturing the underlying dynamic intent, an
Intent-oriented Session Modeling module is designed for inferring
intent-oriented semantic sessions from the contextual information in behavior
sequences. In particular, we consider self-supervised learning signals from two
perspectives for intent-oriented semantic session locating, which encourage
session discrimination within each behavior sequence and session alignment
between dual behavior sequences. Extensive experiments on three public datasets
demonstrate that UnifiedSSR consistently outperforms state-of-the-art methods
for both search and recommendation.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13922" title="Abstract">arXiv:2310.13922</a> [<a href="/pdf/2310.13922" title="Download PDF">pdf</a>, <a href="/format/2310.13922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Map and Agent Geometry for Autonomous Driving Motion  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jier Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In autonomous driving, deep learning enabled motion prediction is a popular
topic. A critical gap in traditional motion prediction methodologies lies in
ensuring equivariance under Euclidean geometric transformations and maintaining
invariant interaction relationships. This research introduces a groundbreaking
solution by employing EqMotion, a theoretically geometric equivariant and
interaction invariant motion prediction model for particles and humans, plus
integrating agent-equivariant high-definition (HD) map features for context
aware motion prediction in autonomous driving. The use of EqMotion as backbone
marks a significant departure from existing methods by rigorously ensuring
motion equivariance and interaction invariance. Equivariance here implies that
an output motion must be equally transformed under the same Euclidean
transformation as an input motion, while interaction invariance preserves the
manner in which agents interact despite transformations. These properties make
the network robust to arbitrary Euclidean transformations and contribute to
more accurate prediction. In addition, we introduce an equivariant method to
process the HD map to enrich the spatial understanding of the network while
preserving the overall network equivariance property. By applying these
technologies, our model is able to achieve high prediction accuracy while
maintain a lightweight design and efficient data utilization.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13923" title="Abstract">arXiv:2310.13923</a> [<a href="/pdf/2310.13923" title="Download PDF">pdf</a>, <a href="/format/2310.13923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversified Outlier Exposure for Out-of-Distribution Detection via  Informative Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Geng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiangchao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+G">Gang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Out-of-distribution (OOD) detection is important for deploying reliable
machine learning models on real-world applications. Recent advances in outlier
exposure have shown promising results on OOD detection via fine-tuning model
with informatively sampled auxiliary outliers. However, previous methods assume
that the collected outliers can be sufficiently large and representative to
cover the boundary between ID and OOD data, which might be impractical and
challenging. In this work, we propose a novel framework, namely, Diversified
Outlier Exposure (DivOE), for effective OOD detection via informative
extrapolation based on the given auxiliary outliers. Specifically, DivOE
introduces a new learning objective, which diversifies the auxiliary
distribution by explicitly synthesizing more informative outliers for
extrapolation during training. It leverages a multi-step optimization method to
generate novel outliers beyond the original ones, which is compatible with many
variants of outlier exposure. Extensive experiments and analyses have been
conducted to characterize and demonstrate the effectiveness of the proposed
DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13925" title="Abstract">arXiv:2310.13925</a> [<a href="/pdf/2310.13925" title="Download PDF">pdf</a>, <a href="/format/2310.13925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-optimized Joint Generative and Contrastive Learning for Sequential  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yongjing Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Pengpeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Junhua Fang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jianfeng Qu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guanfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+F">Fuzhen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+V+S">Victor S. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaofang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Sequential Recommendation (SR) has received increasing attention due to its
ability to capture user dynamic preferences. Recently, Contrastive Learning
(CL) provides an effective approach for sequential recommendation by learning
invariance from different views of an input. However, most existing data or
model augmentation methods may destroy semantic sequential interaction
characteristics and often rely on the hand-crafted property of their
contrastive view-generation strategies. In this paper, we propose a
Meta-optimized Seq2Seq Generator and Contrastive Learning (Meta-SGCL) for
sequential recommendation, which applies the meta-optimized two-step training
strategy to adaptive generate contrastive views. Specifically, Meta-SGCL first
introduces a simple yet effective augmentation method called
Sequence-to-Sequence (Seq2Seq) generator, which treats the Variational
AutoEncoders (VAE) as the view generator and can constitute contrastive views
while preserving the original sequence's semantics. Next, the model employs a
meta-optimized two-step training strategy, which aims to adaptively generate
contrastive views without relying on manually designed view-generation
techniques. Finally, we evaluate our proposed method Meta-SGCL using three
public real-world datasets. Compared with the state-of-the-art methods, our
experimental results demonstrate the effectiveness of our model and the code is
available.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13931" title="Abstract">arXiv:2310.13931</a> [<a href="/pdf/2310.13931" title="Download PDF">pdf</a>, <a href="/ps/2310.13931" title="Download PostScript">ps</a>, <a href="/format/2310.13931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory and power design for aerial CRNs with colluding eavesdroppers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Hongjiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiacheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haosi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Ki-Hong Park</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+I+S">Imran Shafique Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gaofeng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures.submitted to the IEEE journal for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Unmanned aerial vehicles (UAVs) can provide wireless access services to
terrestrial users without geographical limitations and will become an essential
part of the future communication system. However, the openness of wireless
channels and the mobility of UAVs make the security of UAV-based communication
systems particularly challenging. This work investigates the security of aerial
cognitive radio networks (CRNs) with multiple uncertainties colluding
eavesdroppers. A cognitive aerial base station transmits messages to cognitive
terrestrial users using the spectrum resource of the primary users. All
secondary terrestrial users and illegitimate receivers jointly decode the
received message. The average secrecy rate of the aerial CRNs is maximized by
jointly optimizing the UAV's trajectory and transmission power. An iterative
algorithm based on block coordinate descent and successive convex approximation
is proposed to solve the non-convex mixed-variable optimization problem.
Numerical results verify the effectiveness of our proposed algorithm and show
that our scheme improves the secrecy performance of airborne CRNs.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13932" title="Abstract">arXiv:2310.13932</a> [<a href="/pdf/2310.13932" title="Download PDF">pdf</a>, <a href="/ps/2310.13932" title="Download PostScript">ps</a>, <a href="/format/2310.13932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory and Power Design for Aerial Multi-User Covert Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Hongjiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiacheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+I+S">Imran Shafique Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gaofeng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 9 figures, submitted to the IEEE journal for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Unmanned aerial vehicles (UAVs) can provide wireless access to terrestrial
users, regardless of geographical constraints, and will be an important part of
future communication systems. In this paper, a multi-user downlink dual-UAVs
enabled covert communication system was investigated, in which a UAV transmits
secure information to ground users in the presence of multiple wardens as well
as a friendly jammer UAV transmits artificial jamming signals to fight with the
wardens. The scenario of wardens being outfitted with a single antenna is
considered, and the detection error probability (DEP) of wardens with finite
observations is researched. Then, considering the uncertainty of wardens'
location, a robust optimization problem with worst-case covertness constraint
is formulated to maximize the average covert rate by jointly optimizing power
allocation and trajectory. To cope with the optimization problem, an algorithm
based on successive convex approximation methods is proposed. Thereafter, the
results are extended to the case where all the wardens are equipped with
multiple antennas. After analyzing the DEP in this scenario, a tractable lower
bound of the DEP is obtained by utilizing Pinsker's inequality. Subsequently,
the non-convex optimization problem was established and efficiently coped by
utilizing a similar algorithm as in the single-antenna scenario. Numerical
results indicate the effectiveness of our proposed algorithm.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13933" title="Abstract">arXiv:2310.13933</a> [<a href="/pdf/2310.13933" title="Download PDF">pdf</a>, <a href="/format/2310.13933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wideband Beamforming for STAR-RIS-assisted THz Communications with  Three-Side Beam Split
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wencai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Wanming Hao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Gangcan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we consider the simultaneously transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS)-assisted THz communications with
three-side beam split. Except for the beam split at the base station (BS), we
analyze the double-side beam split at the STAR-RIS for the first time. To
relieve the double-side beam split effect, we propose a time delayer (TD)-based
fully-connected structure at the STAR-RIS. As a further advance, a low-hardware
complexity and low-power consumption sub-connected structure is developed,
where multiple STAR-RIS elements share one TD. Meanwhile, considering the
practical scenario, we investigate a multi-STAR-RIS and multi-user
communication system, and a sum rate maximization problem is formulated by
jointly optimizing the hybrid analog/digital beamforming, time delays at the BS
as well as the double-layer phase-shift coefficients, time delays and amplitude
coefficients at the STAR-RISs. Based on this, we first allocate users for each
STAR-RIS, and then derive the analog beamforming, time delays at the BS, and
the double-layer phase-shift coefficients, time delays at each STAR-RIS. Next,
we develop an alternative optimization algorithm to calculate the digital
beamforming at the BS and amplitude coefficients at the STAR-RISs. Finally, the
numerical results verify the effectiveness of the proposed schemes.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13935" title="Abstract">arXiv:2310.13935</a> [<a href="/pdf/2310.13935" title="Download PDF">pdf</a>, <a href="/format/2310.13935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Generative Data Augmentation for Traffic Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Finamore%2C+A">Alessandro Finamore</a>, 
<a href="/search/cs?searchtype=author&query=Michiardi%2C+P">Pietro Michiardi</a>, 
<a href="/search/cs?searchtype=author&query=Gallo%2C+M">Massimo Gallo</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+D">Dario Rossi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear at CoNEXT Student Workshop, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Data Augmentation (DA)-augmenting training data with synthetic samples-is
wildly adopted in Computer Vision (CV) to improve models performance.
Conversely, DA has not been yet popularized in networking use cases, including
Traffic Classification (TC). In this work, we present a preliminary study of 14
hand-crafted DAs applied on the MIRAGE19 dataset. Our results (i) show that DA
can reap benefits previously unexplored in TC and (ii) foster a research agenda
on the use of generative models to automate DA design.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13937" title="Abstract">arXiv:2310.13937</a> [<a href="/pdf/2310.13937" title="Download PDF">pdf</a>, <a href="/format/2310.13937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed Neural Network Modelling and Predictive Control of  District Heating Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=de+Giuli%2C+L+B">Laura Boca de Giuli</a>, 
<a href="/search/eess?searchtype=author&query=La+Bella%2C+A">Alessio La Bella</a>, 
<a href="/search/eess?searchtype=author&query=Scattolini%2C+R">Riccardo Scattolini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper addresses the data-based modelling and optimal control of District
Heating Systems (DHSs). Physical models of such large-scale networked systems
are governed by complex nonlinear equations that require a large amount of
parameters, leading to potential computational issues in optimizing their
operation. A novel methodology is hence proposed, exploiting operational data
and available physical knowledge to attain accurate and computationally
efficient DHSs dynamic models. The proposed idea consists in leveraging
multiple Recurrent Neural Networks (RNNs) and in embedding the physical
topology of the DHS network in their interconnections. With respect to standard
RNN approaches, the resulting modelling methodology, denoted as
Physics-Informed RNN (PI-RNN), enables to achieve faster training procedures
and higher modelling accuracy, even when reduced-dimension models are
exploited. The developed PI-RNN modelling technique paves the way for the
design of a Nonlinear Model Predictive Control (NMPC) regulation strategy,
enabling, with limited computational time, to minimize production costs, to
increase system efficiency and to respect operative constraints over the whole
DHS network. The proposed methods are tested in simulation on a DHS benchmark
referenced in the literature, showing promising results from the modelling and
control perspective.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13942" title="Abstract">arXiv:2310.13942</a> [<a href="/pdf/2310.13942" title="Download PDF">pdf</a>, <a href="/format/2310.13942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Implication for Probabilistic Graphical Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenig%2C+B">Batya Kenig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2105.14463">arXiv:2105.14463</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The graphical structure of Probabilistic Graphical Models (PGMs) represents
the conditional independence (CI) relations that hold in the modeled
distribution. Every separator in the graph represents a conditional
independence relation in the distribution, making them the vehicle through
which new conditional independencies are inferred and verified. The notion of
separation in graphs depends on whether the graph is directed (i.e., a Bayesian
Network), or undirected (i.e., a Markov Network).
<br />The premise of all current systems-of-inference for deriving CIs in PGMs, is
that the set of CIs used for the construction of the PGM hold exactly. In
practice, algorithms for extracting the structure of PGMs from data discover
approximate CIs that do not hold exactly in the distribution. In this paper, we
ask how the error in this set propagates to the inferred CIs read off the
graphical structure. More precisely, what guarantee can we provide on the
inferred CI when the set of CIs that entailed it hold only approximately? It
has recently been shown that in the general case, no such guarantee can be
provided.
<br />In this work, we prove new negative and positive results concerning this
problem. We prove that separators in undirected PGMs do not necessarily
represent approximate CIs. That is, no guarantee can be provided for CIs
inferred from the structure of undirected graphs. We prove that such a
guarantee exists for the set of CIs inferred in directed graphical models,
making the $d$-separation algorithm a sound and complete system for inferring
approximate CIs. We also establish improved approximation guarantees for
independence relations derived from marginal and saturated CIs.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13943" title="Abstract">arXiv:2310.13943</a> [<a href="/pdf/2310.13943" title="Download PDF">pdf</a>, <a href="/format/2310.13943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heat diffusion blurs photothermal images with increasing depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Burgholzer%2C+P">Peter Burgholzer</a>, 
<a href="/search/math?searchtype=author&query=Mayr%2C+G">G&#xfc;nther Mayr</a>, 
<a href="/search/math?searchtype=author&query=Thummerer%2C+G">Gregor Thummerer</a>, 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this tutorial, we aim to directly recreate some of our "aha" moments when
exploring the impact of heat diffusion on the spatial resolution limit of
photothermal imaging. Our objective is also to communicate how this physical
limit can nevertheless be overcome and include some concrete technological
applications. Describing diffusion as a random walk, one insight is that such a
stochastic process involves not only a Gaussian spread of the mean values in
space, with the variance proportional to the diffusion time, but also temporal
and spatial fluctuations around these mean values. All these fluctuations
strongly influence the image reconstruction immediately after the short heating
pulse. The Gaussian spread of the mean values in space increases the entropy,
while the fluctuations lead to a loss of information that blurs the
reconstruction of the initial temperature distribution and can be described
mathematically by a spatial convolution with a Gaussian thermal
point-spread-function (PSF). The information loss turns out to be equal to the
mean entropy increase and limits the spatial resolution proportional to the
depth of the imaged subsurface structures. This principal resolution limit can
only be overcome by including additional information such as sparsity or
positivity. Prior information can be also included by using a deep neural
network with a finite degrees of freedom and trained on a specific class of
image examples for image reconstruction.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13947" title="Abstract">arXiv:2310.13947</a> [<a href="/pdf/2310.13947" title="Download PDF">pdf</a>, <a href="/format/2310.13947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme learning machine to solve a class of biharmonic equation based  on its coupled scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+X">Xi&#x27;an Li</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+J">Jinran Wu</a>, 
<a href="/search/math?searchtype=author&query=Deng%2C+J">Jiaxin Deng</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">You-Gan Wang</a>, 
<a href="/search/math?searchtype=author&query=Tai%2C+X">Xin Tai</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+J">Jianhua Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">Obtaining the solutions of partial differential equations based on machine
learning methods has drawn more and more attention in the fields of scientific
computation and engineering applications. In this work, we first propose a
coupled Extreme Learning Machine(called CELM) method incorporated with the
physical laws to solve a class of fourth-order biharmonic equations by
reformulating it into two well-posed Poisson problems. In addition, some
activation functions including tangent, gaussian, sine, and trigonometric
functions are introduced to assess our CELM method. Furthermore, we introduce
several activation functions, such as tangent, Gaussian, sine, and
trigonometric functions, to evaluate the performance of our CELM method.
Notably, the sine and trigonometric functions demonstrate a remarkable ability
to effectively minimize the approximation error of the CELM model. In the end,
several numerical experiments are performed to study the initializing ways for
both the weights and biases of the hidden units in our CELM model and explore
the required number of hidden units. Numerical results show the proposed CELM
algorithm is high-precision and efficient to address the biharmonic equations
on both regular and irregular domains.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13949" title="Abstract">arXiv:2310.13949</a> [<a href="/pdf/2310.13949" title="Download PDF">pdf</a>, <a href="/format/2310.13949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DER Pricing Power in the Presence of Multi-Location Consumers with Load  Migration Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mollaeivaneghi%2C+S">Sara Mollaeivaneghi</a>, 
<a href="/search/cs?searchtype=author&query=Barbosa%2C+J">Julia Barbosa</a>, 
<a href="/search/cs?searchtype=author&query=Steinke%2C+F">Florian Steinke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Renewable distributed energy resources (DERs) have the potential to provide
multi-location electricity consumers (MLECs) with electricity at prices lower
than those offered by the grid using behind-the-meter advantages. This study
examines the pricing power of such DER owners in a local environment with few
competitors and how it depends on the MLEC's ability to migrate a portion of
the load between locations. We simulate a dynamic game between an MLEC and the
local DER owners, where the MLEC is modeled as a cost-minimizer and the DER
owners as strategic profit maximizers. We show that, when the MLEC is
inflexible, the DER owners' optimal behavior is to offer their electricity
close to maximal prices, that is, at the grid price level. However, when the
MLEC can migrate a fraction of the load to the other locations, the prices
offered by the DER owners quickly decrease to the minimum level, that is, the
DERs' grid feed-in tariffs quickly decrease to a lower level, depending on the
load migration capability.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13950" title="Abstract">arXiv:2310.13950</a> [<a href="/pdf/2310.13950" title="Download PDF">pdf</a>, <a href="/format/2310.13950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Image Generation by Spatial Transformation in Perceptual  Colorspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aydin%2C+A">Ayberk Aydin</a>, 
<a href="/search/cs?searchtype=author&query=Temizel%2C+A">Alptekin Temizel</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Pattern Recognition Letters, Volume 174, October 2023, Pages 92-98
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Deep neural networks are known to be vulnerable to adversarial perturbations.
The amount of these perturbations are generally quantified using $L_p$ metrics,
such as $L_0$, $L_2$ and $L_\infty$. However, even when the measured
perturbations are small, they tend to be noticeable by human observers since
$L_p$ distance metrics are not representative of human perception. On the other
hand, humans are less sensitive to changes in colorspace. In addition, pixel
shifts in a constrained neighborhood are hard to notice. Motivated by these
observations, we propose a method that creates adversarial examples by applying
spatial transformations, which creates adversarial examples by changing the
pixel locations independently to chrominance channels of perceptual colorspaces
such as $YC_{b}C_{r}$ and $CIELAB$, instead of making an additive perturbation
or manipulating pixel values directly. In a targeted white-box attack setting,
the proposed method is able to obtain competitive fooling rates with very high
confidence. The experimental evaluations show that the proposed method has
favorable results in terms of approximate perceptual distance between benign
and adversarially generated images. The source code is publicly available at
https://github.com/ayberkydn/stadv-torch
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13951" title="Abstract">arXiv:2310.13951</a> [<a href="/pdf/2310.13951" title="Download PDF">pdf</a>, <a href="/format/2310.13951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuzzy-NMS: Improving 3D Object Detection with Fuzzy Classification in  NMS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fachuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuze Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Ziying Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Non-maximum suppression (NMS) is an essential post-processing module used in
many 3D object detection frameworks to remove overlapping candidate bounding
boxes. However, an overreliance on classification scores and difficulties in
determining appropriate thresholds can affect the resulting accuracy directly.
To address these issues, we introduce fuzzy learning into NMS and propose a
novel generalized Fuzzy-NMS module to achieve finer candidate bounding box
filtering. The proposed Fuzzy-NMS module combines the volume and clustering
density of candidate bounding boxes, refining them with a fuzzy classification
method and optimizing the appropriate suppression thresholds to reduce
uncertainty in the NMS process. Adequate validation experiments are conducted
using the mainstream KITTI and large-scale Waymo 3D object detection
benchmarks. The results of these tests demonstrate the proposed Fuzzy-NMS
module can improve the accuracy of numerous recently NMS-based detectors
significantly, including PointPillars, PV-RCNN, and IA-SSD, etc. This effect is
particularly evident for small objects such as pedestrians and bicycles. As a
plug-and-play module, Fuzzy-NMS does not need to be retrained and produces no
obvious increases in inference time.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13952" title="Abstract">arXiv:2310.13952</a> [<a href="/pdf/2310.13952" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Resolution limit in Photoacoustic Imaging using Positivity  and Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Burgholzer%2C+P">Peter Burgholzer</a>, 
<a href="/search/math?searchtype=author&query=Bauer-Marschallinger%2C+J">Johannes Bauer-Marschallinger</a>, 
<a href="/search/math?searchtype=author&query=Hettich%2C+M">Mike Hettich</a>, 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this tutorial, we aim to directly recreate some of our "aha" moments when
exploring the impact of heat diffusion on the spatial resolution limit of
photothermal imaging. Our objective is also to communicate how this physical
limit can nevertheless be overcome and include some concrete technological
applications. Describing diffusion as a random walk, one insight is that such a
stochastic process involves not only a Gaussian spread of the mean values in
space, with the variance proportional to the diffusion time, but also temporal
and spatial fluctuations around these mean values. All these fluctuations
strongly influence the image reconstruction immediately after the short heating
pulse. The Gaussian spread of the mean values in space increases the entropy,
while the fluctuations lead to a loss of information that blurs the
reconstruction of the initial temperature distribution and can be described
mathematically by a spatial convolution with a Gaussian thermal
point-spread-function (PSF). The information loss turns out to be equal to the
mean entropy increase and limits the spatial resolution proportional to the
depth of the imaged subsurface structures. This principal resolution limit can
only be overcome by including additional information such as sparsity or
positivity. Prior information can be also included by using a deep neural
network with a finite degrees of freedom and trained on a specific class of
image examples for image reconstruction
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13953" title="Abstract">arXiv:2310.13953</a> [<a href="/pdf/2310.13953" title="Download PDF">pdf</a>, <a href="/format/2310.13953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards dialogue based, computer aided software requirements elicitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seibert%2C+V">Vasiliy Seibert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Several approaches have been presented, which aim to extract models from
natural language specifications. These approaches have inherent weaknesses for
they assume an initial problem understanding that is perfect, and they leave no
room for feedback. Motivated by real-world collaboration settings between
requirements engineers and customers, this paper proposes an interaction
blueprint that aims for dialogue based, computer aided software requirements
analysis. Compared to mere model extraction approaches, this interaction
blueprint encourages individuality, creativity and genuine compromise. A
simplistic Experiment was conducted to showcase the general idea. This paper
discusses the experiment as well as the proposed interaction blueprint and
argues, that advancements in natural language processing and generative AI
might lead to significant progress in a foreseeable future. However, for that,
there is a need to move away from a magical black box expectation and instead
moving towards a dialogue based approach that recognizes the individuality that
is an undeniable part of requirements engineering.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13954" title="Abstract">arXiv:2310.13954</a> [<a href="/pdf/2310.13954" title="Download PDF">pdf</a>, <a href="/format/2310.13954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth Number Message Authentication Code in the IoT Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Constantinescu%2C+E">Eduard-Matei Constantinescu</a>, 
<a href="/search/cs?searchtype=author&query=Elhajj%2C+M">Mohammed Elhajj</a>, 
<a href="/search/cs?searchtype=author&query=Mariot%2C+L">Luca Mariot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper presents the Smooth Number Message Authentication Code (SNMAC) for
the context of lightweight IoT devices. The proposal is based on the use of
smooth numbers in the field of cryptography, and investigates how one can use
them to improve the security and performance of various algorithms or security
constructs. The literature findings suggest that current IoT solutions are
viable and promising, yet they should explore the potential usage of smooth
numbers. The methodology involves several processes, including the design,
implementation, and results evaluation. After introducing the algorithm,
provides a detailed account of the experimental performance analysis of the
SNMAC solution, showcasing its efficiency in real-world scenarios. Furthermore,
the paper also explores the security aspects of the proposed SNMAC algorithm,
offering valuable insights into its robustness and applicability for ensuring
secure communication within IoT environments.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13955" title="Abstract">arXiv:2310.13955</a> [<a href="/pdf/2310.13955" title="Download PDF">pdf</a>, <a href="/format/2310.13955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Competitive Ensembling Teacher-Student Framework for Semi-Supervised  Left Atrium MRI Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuyan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shasha Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepeted for BIBM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semi-supervised learning has greatly advanced medical image segmentation
since it effectively alleviates the need of acquiring abundant annotations from
experts and utilizes unlabeled data which is much easier to acquire. Among
existing perturbed consistency learning methods, mean-teacher model serves as a
standard baseline for semi-supervised medical image segmentation. In this
paper, we present a simple yet efficient competitive ensembling teacher student
framework for semi-supervised for left atrium segmentation from 3D MR images,
in which two student models with different task-level disturbances are
introduced to learn mutually, while a competitive ensembling strategy is
performed to ensemble more reliable information to teacher model. Different
from the one-way transfer between teacher and student models, our framework
facilitates the collaborative learning procedure of different student models
with the guidance of teacher model and motivates different training networks
for a competitive learning and ensembling procedure to achieve better
performance. We evaluate our proposed method on the public Left Atrium (LA)
dataset and it obtains impressive performance gains by exploiting the unlabeled
data effectively and outperforms several existing semi-supervised methods.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13959" title="Abstract">arXiv:2310.13959</a> [<a href="/pdf/2310.13959" title="Download PDF">pdf</a>, <a href="/format/2310.13959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-discriminator Domain Adversarial Neural Networks with Class-Level  Gradient Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hongke Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hengshu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+N">Nan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised domain adaptation aims to transfer rich knowledge from the
annotated source domain to the unlabeled target domain with the same label
space. One prevalent solution is the bi-discriminator domain adversarial
network, which strives to identify target domain samples outside the support of
the source domain distribution and enforces their classification to be
consistent on both discriminators. Despite being effective, agnostic accuracy
and overconfident estimation for out-of-distribution samples hinder its further
performance improvement. To address the above challenges, we propose a novel
bi-discriminator domain adversarial neural network with class-level gradient
alignment, i.e. BACG. BACG resorts to gradient signals and second-order
probability estimation for better alignment of domain distributions.
Specifically, for accuracy-awareness, we first design an optimizable nearest
neighbor algorithm to obtain pseudo-labels of samples in the target domain, and
then enforce the backward gradient approximation of the two discriminators at
the class level. Furthermore, following evidential learning theory, we
transform the traditional softmax-based optimization method into a Multinomial
Dirichlet hierarchical model to infer the class probability distribution as
well as samples uncertainty, thereby alleviating misestimation of
out-of-distribution samples and guaranteeing high-quality classes alignment. In
addition, inspired by contrastive learning, we develop a memory bank-based
variant, i.e. Fast-BACG, which can greatly shorten the training process at the
cost of a minor decrease in accuracy. Extensive experiments and detailed
theoretical analysis on four benchmark data sets validate the effectiveness and
robustness of our algorithm.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13960" title="Abstract">arXiv:2310.13960</a> [<a href="/pdf/2310.13960" title="Download PDF">pdf</a>, <a href="/format/2310.13960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linguistically Motivated Sign Language Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moryossef%2C+A">Amit Moryossef</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M">Mathias M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Ebling%2C+S">Sarah Ebling</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Sign language segmentation is a crucial task in sign language processing
systems. It enables downstream tasks such as sign recognition, transcription,
and machine translation. In this work, we consider two kinds of segmentation:
segmentation into individual signs and segmentation into \textit{phrases},
larger units comprising several signs. We propose a novel approach to jointly
model these two tasks.
<br />Our method is motivated by linguistic cues observed in sign language corpora.
We replace the predominant IO tagging scheme with BIO tagging to account for
continuous signing. Given that prosody plays a significant role in phrase
boundaries, we explore the use of optical flow features. We also provide an
extensive analysis of hand shapes and 3D hand normalization.
<br />We find that introducing BIO tagging is necessary to model sign boundaries.
Explicitly encoding prosody by optical flow improves segmentation in shallow
models, but its contribution is negligible in deeper models. Careful tuning of
the decoding algorithm atop the models further improves the segmentation
quality.
<br />We demonstrate that our final models generalize to out-of-domain video
content in a different signed language, even under a zero-shot setting. We
observe that including optical flow and 3D hand normalization enhances the
robustness of the model in this context.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13961" title="Abstract">arXiv:2310.13961</a> [<a href="/pdf/2310.13961" title="Download PDF">pdf</a>, <a href="/format/2310.13961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble-Instruct: Generating Instruction-Tuning Data with a  Heterogeneous Mixture of LMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Young-Suk Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>, 
<a href="/search/cs?searchtype=author&query=El-Kurdi%2C+Y">Yousef El-Kurdi</a>, 
<a href="/search/cs?searchtype=author&query=Munawar%2C+T+N+A">Tahira Naseem Asim Munawar</a>, 
<a href="/search/cs?searchtype=author&query=Florian%2C+R">Radu Florian</a>, 
<a href="/search/cs?searchtype=author&query=Roukos%2C+S">Salim Roukos</a>, 
<a href="/search/cs?searchtype=author&query=Astudillo%2C+R+F">Ram&#xf3;n Fernandez Astudillo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Using in-context learning (ICL) for data generation, techniques such as
Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023)
can train strong conversational agents with only a small amount of human
supervision. One limitation of these approaches is that they resort to very
large language models (around 175B parameters) that are also proprietary and
non-public. Here we explore the application of such techniques to language
models that are much smaller (around 10B--40B parameters) and have permissive
licenses. We find the Self-Instruct approach to be less effective at these
sizes and propose new ICL methods that draw on two main ideas: (a)
Categorization and simplification of the ICL templates to make prompt learning
easier for the LM, and (b) Ensembling over multiple LM outputs to help select
high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct
seed tasks and employs separate pipelines for instructions that require an
input and instructions that do not. Empirical investigations with different LMs
show that: (1) Our proposed method yields higher-quality instruction tuning
data than Self-Instruct, (2) It improves performances of both vanilla and
instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned
LMs generate more useful outputs than their larger un-tuned counterparts. Our
codebase is available at https://github.com/IBM/ensemble-instruct.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13971" title="Abstract">arXiv:2310.13971</a> [<a href="/pdf/2310.13971" title="Download PDF">pdf</a>, <a href="/format/2310.13971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A well-balanced second-order finite volume approximation for a coupled  system of granular flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aggarwal%2C+A">Aekta Aggarwal</a>, 
<a href="/search/math?searchtype=author&query=D.%2C+V+G+G">Veerappa Gowda G.D.</a>, 
<a href="/search/math?searchtype=author&query=K%2C+S+K">Sudarshan Kumar K</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A second-order finite volume scheme is proposed and analyzed for a 2X2 system
of non-linear partial differential equations. These equations model the
dynamics of growing sandpiles created by a vertical source on a flat, bounded
rectangular table in multiple dimensions. The well-balancedness of the scheme
is ensured through a modified limitation approach allowing the scheme to reduce
to well-balanced first-order scheme near the steady state while maintaining the
second-order accuracy away from it. The well-balanced property of the scheme is
proven analytically in one dimension and demonstrated numerically in two
dimensions. It is also shown through the numerical experiments that the
second-order scheme reduces the finite time oscillations, takes fewer time
iterations for achieving the steady state and gives sharper resolutions of the
physical structure of the sandpile, as compared to the first-order schemes
existing in the literature.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13974" title="Abstract">arXiv:2310.13974</a> [<a href="/pdf/2310.13974" title="Download PDF">pdf</a>, <a href="/format/2310.13974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Pronunciation Assessment -- A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kheir%2C+Y+E">Yassine El Kheir</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A">Ahmed Ali</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+A">Shammur Absar Chowdhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, accepted to EMNLP Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Pronunciation assessment and its application in computer-aided pronunciation
training (CAPT) have seen impressive progress in recent years. With the rapid
growth in language processing and deep learning over the past few years, there
is a need for an updated review. In this paper, we review methods employed in
pronunciation assessment for both phonemic and prosodic. We categorize the main
challenges observed in prominent research trends, and highlight existing
limitations, and available resources. This is followed by a discussion of the
remaining challenges and possible directions for future work.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13976" title="Abstract">arXiv:2310.13976</a> [<a href="/pdf/2310.13976" title="Download PDF">pdf</a>, <a href="/format/2310.13976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Requirements Engineering through Generative AI: Assessing the  Role of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Chetan Arora</a>, 
<a href="/search/cs?searchtype=author&query=Grundy%2C+J">John Grundy</a>, 
<a href="/search/cs?searchtype=author&query=Abdelrazek%2C+M">Mohamed Abdelrazek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Requirements Engineering (RE) is a critical phase in software development
including the elicitation, analysis, specification, and validation of software
requirements. Despite the importance of RE, it remains a challenging process
due to the complexities of communication, uncertainty in the early stages and
inadequate automation support. In recent years, large-language models (LLMs)
have shown significant promise in diverse domains, including natural language
processing, code generation, and program understanding. This chapter explores
the potential of LLMs in driving RE processes, aiming to improve the efficiency
and accuracy of requirements-related tasks. We propose key directions and SWOT
analysis for research and development in using LLMs for RE, focusing on the
potential for requirements elicitation, analysis, specification, and
validation. We further present the results from a preliminary evaluation, in
this context.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13977" title="Abstract">arXiv:2310.13977</a> [<a href="/pdf/2310.13977" title="Download PDF">pdf</a>, <a href="/format/2310.13977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Invariant Risk Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alesiani%2C+F">Francesco Alesiani</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shujian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Niepert%2C+M">Mathias Niepert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Shorter version of this paper was presented at RobustML workshop of ICLR 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Empirical risk minimization can lead to poor generalization behavior on
unseen environments if the learned model does not capture invariant feature
representations. Invariant risk minimization (IRM) is a recent proposal for
discovering environment-invariant representations. IRM was introduced by
Arjovsky et al. (2019) and extended by Ahuja et al. (2020). IRM assumes that
all environments are available to the learning system at the same time. With
this work, we generalize the concept of IRM to scenarios where environments are
observed sequentially. We show that existing approaches, including those
designed for continual learning, fail to identify the invariant features and
models across sequentially presented environments. We extend IRM under a
variational Bayesian and bilevel framework, creating a general approach to
continual invariant risk minimization. We also describe a strategy to solve the
optimization problems using a variant of the alternating direction method of
multiplier (ADMM). We show empirically using multiple datasets and with
multiple sequential environments that the proposed methods outperform or is
competitive with prior approaches.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13981" title="Abstract">arXiv:2310.13981</a> [<a href="/pdf/2310.13981" title="Download PDF">pdf</a>, <a href="/ps/2310.13981" title="Download PostScript">ps</a>, <a href="/format/2310.13981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filling the Missing: Exploring Generative AI for Enhanced Federated  Learning over Heterogeneous Mobile Edge Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peichun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+L">Liping Qian</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Xuemin">Xuemin</a> (Sherman)
<a href="/search/cs?searchtype=author&query=Shen">Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures. Submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Distributed Artificial Intelligence (AI) model training over mobile edge
networks encounters significant challenges due to the data and resource
heterogeneity of edge devices. The former hampers the convergence rate of the
global model, while the latter diminishes the devices' resource utilization
efficiency. In this paper, we propose a generative AI-empowered federated
learning to address these challenges by leveraging the idea of FIlling the
MIssing (FIMI) portion of local data. Specifically, FIMI can be considered as a
resource-aware data augmentation method that effectively mitigates the data
heterogeneity while ensuring efficient FL training. We first quantify the
relationship between the training data amount and the learning performance. We
then study the FIMI optimization problem with the objective of minimizing the
device-side overall energy consumption subject to required learning performance
constraints. The decomposition-based analysis and the cross-entropy searching
method are leveraged to derive the solution, where each device is assigned
suitable AI-synthesized data and resource utilization policy. Experiment
results demonstrate that FIMI can save up to 50% of the device-side energy to
achieve the target global test accuracy in comparison with the existing
methods. Meanwhile, FIMI can significantly enhance the converged global
accuracy under the non-independently-and-identically distribution (non-IID)
data.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13985" title="Abstract">arXiv:2310.13985</a> [<a href="/pdf/2310.13985" title="Download PDF">pdf</a>, <a href="/ps/2310.13985" title="Download PostScript">ps</a>, <a href="/format/2310.13985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online  Posts using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+V">Vibhor Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+N">Nishanth Sastry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Hate speech has become pervasive in today's digital age. Although there has
been considerable research to detect hate speech or generate counter speech to
combat hateful views, these approaches still cannot completely eliminate the
potential harmful societal consequences of hate speech -- hate speech, even
when detected, can often not be taken down or is often not taken down enough;
and hate speech unfortunately spreads quickly, often much faster than any
generated counter speech.
<br />This paper investigates a relatively new yet simple and effective approach of
suggesting a rephrasing of potential hate speech content even before the post
is made. We show that Large Language Models (LLMs) perform well on this task,
outperforming state-of-the-art baselines such as BART-Detox. We develop 4
different prompts based on task description, hate definition, few-shot
demonstrations and chain-of-thoughts for comprehensive experiments and conduct
experiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well
as OpenAI's GPT-3.5. We propose various evaluation metrics to measure the
efficacy of the generated text and ensure the generated text has reduced hate
intensity without drastically changing the semantic meaning of the original
text.
<br />We find that LLMs with a few-shot demonstrations prompt work the best in
generating acceptable hate-rephrased text with semantic meaning similar to the
original text. Overall, we find that GPT-3.5 outperforms the baseline and
open-source models for all the different kinds of prompts. We also perform
human evaluations and interestingly, find that the rephrasings generated by
GPT-3.5 outperform even the human-generated ground-truth rephrasings in the
dataset. We also conduct detailed ablation studies to investigate why LLMs work
satisfactorily on this task and conduct a failure analysis to understand the
gaps.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13988" title="Abstract">arXiv:2310.13988</a> [<a href="/pdf/2310.13988" title="Download PDF">pdf</a>, <a href="/format/2310.13988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kocmi%2C+T">Tom Kocmi</a>, 
<a href="/search/cs?searchtype=author&query=Federmann%2C+C">Christian Federmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WMT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to
detect translation quality errors, specifically for the quality estimation
setting without the need for human reference translations. Based on the power
of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting
technique, querying the GPT-4 model to mark error quality spans. Compared to
previous works, our method has language-agnostic prompts, thus avoiding the
need for manual prompt preparation for new languages.
<br />While preliminary results indicate that GEMBA-MQM achieves state-of-the-art
accuracy for system ranking, we advise caution when using it in academic works
to demonstrate improvements over other methods due to its dependence on the
proprietary, black-box GPT model.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13989" title="Abstract">arXiv:2310.13989</a> [<a href="/pdf/2310.13989" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CONFIGURE: An Optimisation Framework for the Cost-Effective Spatial  Configuration of Blue-Green Infrastructure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rehman%2C+A+U">Asid Ur Rehman</a>, 
<a href="/search/cs?searchtype=author&query=Glenis%2C+V">Vassilis Glenis</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+E">Elizabeth Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Kilsby%2C+C">Chris Kilsby</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper submitted for publication in Environmental Modelling and Software. 26 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">This paper develops a Blue-Green Infrastructure (BGI) performance evaluation
approach by integrating a Non-dominated Sorting Genetic Algorithm II (NSGA-II)
with a detailed hydrodynamic model. The proposed Cost OptimisatioN Framework
for Implementing blue-Green infrastructURE (CONFIGURE), with a simplified
problem-framing process and efficient genetic operations, can be connected to
any flood simulation model. In this study, CONFIGURE is integrated with the
CityCAT hydrodynamic model to optimise the locations and combinations of
permeable surfaces. Permeable zones with four different levels of spatial
discretisation are designed to evaluate their efficiency for 100-year and
30-year return period rainstorms. Overall, the framework performs effectively
for the given scenarios. The application of the detailed hydrodynamic model
explicitly captures the functioning of permeable features to provide the
optimal locations for their deployment. Moreover, the size and the location of
the permeable surfaces and the intensity of the rainstorm events are the
critical performance parameters for economical BGI deployment.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13990" title="Abstract">arXiv:2310.13990</a> [<a href="/pdf/2310.13990" title="Download PDF">pdf</a>, <a href="/format/2310.13990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Information-Theoretic Objective to Disentangle Representations  for Fair Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>, 
<a href="/search/cs?searchtype=author&query=Noiry%2C+N">Nathan Noiry</a>, 
<a href="/search/cs?searchtype=author&query=Staerman%2C+G">Guillaume Staerman</a>, 
<a href="/search/cs?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">One of the pursued objectives of deep learning is to provide tools that learn
abstract representations of reality from the observation of multiple contextual
situations. More precisely, one wishes to extract disentangled representations
which are (i) low dimensional and (ii) whose components are independent and
correspond to concepts capturing the essence of the objects under consideration
(Locatello et al., 2019b). One step towards this ambitious project consists in
learning disentangled representations with respect to a predefined (sensitive)
attribute, e.g., the gender or age of the writer. Perhaps one of the main
application for such disentangled representations is fair classification.
Existing methods extract the last layer of a neural network trained with a loss
that is composed of a cross-entropy objective and a disentanglement
regularizer. In this work, we adopt an information-theoretic view of this
problem which motivates a novel family of regularizers that minimizes the
mutual information between the latent representation and the sensitive
attribute conditional to the target. The resulting set of losses, called
CLINIC, is parameter free and thus, it is easier and faster to train. CLINIC
losses are studied through extensive numerical experiments by training over 2k
neural networks. We demonstrate that our methods offer a better
disentanglement/accuracy trade-off than previous techniques, and generalize
better than training with cross-entropy loss solely provided that the
disentanglement task is not too constraining.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13991" title="Abstract">arXiv:2310.13991</a> [<a href="/pdf/2310.13991" title="Download PDF">pdf</a>, <a href="/format/2310.13991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An $\textit{M}$-ary Concentration Shift Keying With Common Detection  Thresholds For Multi-Transmitter Molecular Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shitiri%2C+E">Ethungshan Shitiri</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Ho-Shin Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Concentration shift keying (CSK) is a widely adopted modulation technique for
molecular communication-based nanonetworks, which is a key enabler for the
Internet of Bio-NanoThings (IoBNT). However, existing methods provide optimal
error performance at the cost of high operational complexity that scales poorly
as the number of transmitters, $K$, increases. This paper proposes a novel
$M$-ary CSK method termed CSK with Common detection Thresholds (CSK-CT). CSK-CT
uses $\textit{common}$ thresholds that are sufficiently low to ensure the
reliable detection of symbols transmitted by every transmitter, regardless of
their distance. We derive closed-form expressions to obtain the common
thresholds and release concentrations. To enhance the error performance, we
optimize the release concentration using a scaling exponent that further
optimizes the common thresholds. We evaluate the performance of CSK-CT in
comparison to the benchmark CSK for varying values of $K$ and $M$. In terms of
the error probability, CSK-CT offers between $10^{-7}$ and $10^{-4}$, which are
a substantial improvement from the $10^{-4}$ to $10^{-3}$ offered by the
benchmark. In terms of complexity, CSK-CT is $\textit{O}\big(n\big)$ and does
not scale with $K$ but $M$ ($M\ll K$), while the benchmark is
$\textit{O}\big(n^2\big)$. Furthermore, CSK-CT showcased the ability to
mitigate inter-symbol interference, although this facet warrants further
investigation. Due to its low error probability, improved scalability, low
complexity, and potential ISI mitigation features, CSK-CT demonstrates benefits
in applications of IoBNT focused on data-gathering. Specifically, its utility
is well-noted in settings where a computationally strained receiver collects
sensitive health-related data from multiple transmitters.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13992" title="Abstract">arXiv:2310.13992</a> [<a href="/pdf/2310.13992" title="Download PDF">pdf</a>, <a href="/format/2310.13992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pure Bayesian Nash equilibrium for Bayesian games with multidimensional  vector Types and linear payoffs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huot%2C+S">S&#xe9;bastien Huot</a>, 
<a href="/search/cs?searchtype=author&query=Edalat%2C+A">Abbas Edalat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study $n$-agent Bayesian Games with $m$-dimensional vector types and
linear payoffs, also called Linear Multidimensional Bayesian Games. This class
of games is equivalent with $n$-agent, $m$-game Uniform Multigames. We
distinguish between games that have a discrete type space and those with a
continuous type space. More specifically, we are interested in the existence of
pure Bayesian Nash Equilibrium for such games and efficient algorithms to find
them. For continuous priors we suggest a methodology to perform Nash
Equilibrium search in simple cases. For discrete priors we present algorithms
that can handle two actions and two players games efficiently. We introduce the
core concept of threshold strategy and, under some mild conditions, we show
that these games have at least one pure Bayesian Nash Equilibrium. We
illustrate our results with several examples like Double Game Prisoner Dilemna
(DGPD), Chicken Game and Sustainable Adoption Decision Problem (SADP).
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13995" title="Abstract">arXiv:2310.13995</a> [<a href="/pdf/2310.13995" title="Download PDF">pdf</a>, <a href="/format/2310.13995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Bilingual Lexicon Induction with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaoyiran Li</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that
still, to a large extent, relies on calculating cross-lingual word
representations. Inspired by the global paradigm shift in NLP towards Large
Language Models (LLMs), we examine the potential of the latest generation of
LLMs for the development of bilingual lexicons. We ask the following research
question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for
BLI, and how does this approach compare against and complement current BLI
approaches? To this end, we systematically study 1) zero-shot prompting for
unsupervised BLI and 2) few-shot in-context prompting with a set of seed
translation pairs, both without any LLM fine-tuning, as well as 3) standard
BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source
text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two
standard BLI benchmarks covering a range of typologically diverse languages.
Our work is the first to demonstrate strong BLI capabilities of text-to-text
mLLMs. The results reveal that few-shot prompting with in-context examples from
nearest neighbours achieves the best performance, establishing new
state-of-the-art BLI scores for many language pairs. We also conduct a series
of in-depth analyses and ablation studies, providing more insights on BLI with
(m)LLMs, also along with their limitations.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13996" title="Abstract">arXiv:2310.13996</a> [<a href="/pdf/2310.13996" title="Download PDF">pdf</a>, <a href="/format/2310.13996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emulating the Human Mind: A Neural-symbolic Link Prediction Model with  Fast and Slow Reasoning and Filtered Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khojasteh%2C+M+H">Mohammad Hossein Khojasteh</a>, 
<a href="/search/cs?searchtype=author&query=Torabian%2C+N">Najmeh Torabian</a>, 
<a href="/search/cs?searchtype=author&query=Farjami%2C+A">Ali Farjami</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S">Saeid Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Minaei-Bidgoli%2C+B">Behrouz Minaei-Bidgoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Link prediction is an important task in addressing the incompleteness problem
of knowledge graphs (KG). Previous link prediction models suffer from issues
related to either performance or explanatory capability. Furthermore, models
that are capable of generating explanations, often struggle with erroneous
paths or reasoning leading to the correct answer. To address these challenges,
we introduce a novel Neural-Symbolic model named FaSt-FLiP (stands for Fast and
Slow Thinking with Filtered rules for Link Prediction task), inspired by two
distinct aspects of human cognition: "commonsense reasoning" and "thinking,
fast and slow." Our objective is to combine a logical and neural model for
enhanced link prediction. To tackle the challenge of dealing with incorrect
paths or rules generated by the logical model, we propose a semi-supervised
method to convert rules into sentences. These sentences are then subjected to
assessment and removal of incorrect rules using an NLI (Natural Language
Inference) model. Our approach to combining logical and neural models involves
first obtaining answers from both the logical and neural models. These answers
are subsequently unified using an Inference Engine module, which has been
realized through both algorithmic implementation and a novel neural model
architecture. To validate the efficacy of our model, we conducted a series of
experiments. The results demonstrate the superior performance of our model in
both link prediction metrics and the generation of more reliable explanations.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13998" title="Abstract">arXiv:2310.13998</a> [<a href="/pdf/2310.13998" title="Download PDF">pdf</a>, <a href="/format/2310.13998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transductive Learning for Textual Few-Shot Classification in API-based  Embedding Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrain%2C+V">Victor Pellegrain</a>, 
<a href="/search/cs?searchtype=author&query=Boudiaf%2C+M">Malik Boudiaf</a>, 
<a href="/search/cs?searchtype=author&query=Storchan%2C+V">Victor Storchan</a>, 
<a href="/search/cs?searchtype=author&query=Tami%2C+M">Myriam Tami</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I+B">Ismail Ben Ayed</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">Celine Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Proprietary and closed APIs are becoming increasingly common to process
natural language, and are impacting the practical applications of natural
language processing, including few-shot classification. Few-shot classification
involves training a model to perform a new classification task with a handful
of labeled data. This paper presents three contributions. First, we introduce a
scenario where the embedding of a pre-trained model is served through a gated
API with compute-cost and data-privacy constraints. Second, we propose a
transductive inference, a learning paradigm that has been overlooked by the NLP
community. Transductive inference, unlike traditional inductive learning,
leverages the statistics of unlabeled data. We also introduce a new
parameter-free transductive regularizer based on the Fisher-Rao loss, which can
be used on top of the gated API embeddings. This method fully utilizes
unlabeled data, does not share any label with the third-party API provider and
could serve as a baseline for future research. Third, we propose an improved
experimental setting and compile a benchmark of eight datasets involving
multiclass classification in four different languages, with up to 151 classes.
We evaluate our methods using eight backbone models, along with an episodic
evaluation over 1,000 episodes, which demonstrate the superiority of
transductive inference over the standard inductive setting.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14000" title="Abstract">arXiv:2310.14000</a> [<a href="/pdf/2310.14000" title="Download PDF">pdf</a>, <a href="/format/2310.14000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Differential Privacy for Number of Paths and Katz Centrality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Betzer%2C+L">Louis Betzer</a>, 
<a href="/search/cs?searchtype=author&query=Suppakitpaisarn%2C+V">Vorapong Suppakitpaisarn</a>, 
<a href="/search/cs?searchtype=author&query=Hillebrand%2C+Q">Quentin Hillebrand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In this paper, we give an algorithm to publish the number of paths and Katz
centrality under the local differential privacy (LDP), providing a thorough
theoretical analysis. Although various works have already introduced subgraph
counting algorithms under LDP, they have primarily concentrated on subgraphs of
up to five nodes. The challenge in extending this to larger subgraphs is the
cumulative and exponential growth of noise as the subgraph size increases in
any publication under LDP. We address this issue by proposing an algorithm to
publish the number of paths that start at every node in the graph, leading to
an algorithm that publishes the Katz centrality of all nodes. This algorithm
employs multiple rounds of communication and the clipping technique. Both our
theoretical and experimental assessments indicate that our algorithm exhibits
acceptable bias and variance, considerably less than an algorithm that bypasses
clipping. Furthermore, our Katz centrality estimation is able to recall up to
90% of the nodes with the highest Katz centrality.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14001" title="Abstract">arXiv:2310.14001</a> [<a href="/pdf/2310.14001" title="Download PDF">pdf</a>, <a href="/format/2310.14001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Stronger Textual Attack Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>, 
<a href="/search/cs?searchtype=author&query=Picot%2C+M">Marine Picot</a>, 
<a href="/search/cs?searchtype=author&query=Noiry%2C+N">Nathan Noiry</a>, 
<a href="/search/cs?searchtype=author&query=Staerman%2C+G">Guillaume Staerman</a>, 
<a href="/search/cs?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The landscape of available textual adversarial attacks keeps growing, posing
severe threats and raising concerns regarding the deep NLP system's integrity.
However, the crucial problem of defending against malicious attacks has only
drawn the attention of the NLP community. The latter is nonetheless
instrumental in developing robust and trustworthy systems. This paper makes two
important contributions in this line of search: (i) we introduce LAROUSSE, a
new framework to detect textual adversarial attacks and (ii) we introduce
STAKEOUT, a new benchmark composed of nine popular attack methods, three
datasets, and two pre-trained models. LAROUSSE is ready-to-use in production as
it is unsupervised, hyperparameter-free, and non-differentiable, protecting it
against gradient-based methods. Our new benchmark STAKEOUT allows for a robust
evaluation framework: we conduct extensive numerical experiments which
demonstrate that LAROUSSE outperforms previous methods, and which allows to
identify interesting factors of detection rate variations.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14009" title="Abstract">arXiv:2310.14009</a> [<a href="/pdf/2310.14009" title="Download PDF">pdf</a>, <a href="/format/2310.14009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One is More: Diverse Perspectives within a Single Network for Efficient  DRL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Yiqin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Longbo Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep reinforcement learning has achieved remarkable performance in various
domains by leveraging deep neural networks for approximating value functions
and policies. However, using neural networks to approximate value functions or
policy functions still faces challenges, including low sample efficiency and
overfitting. In this paper, we introduce OMNet, a novel learning paradigm
utilizing multiple subnetworks within a single network, offering diverse
outputs efficiently. We provide a systematic pipeline, including
initialization, training, and sampling with OMNet. OMNet can be easily applied
to various deep reinforcement learning algorithms with minimal additional
overhead. Through comprehensive evaluations conducted on MuJoCo benchmark, our
findings highlight OMNet's ability to strike an effective balance between
performance and computational cost.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14017" title="Abstract">arXiv:2310.14017</a> [<a href="/pdf/2310.14017" title="Download PDF">pdf</a>, <a href="/format/2310.14017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrast Everything: A Hierarchical Contrastive Framework for Medical  Time-Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yu Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haishuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeruIPS 2023; 24pages (13 pages main paper + 11 pages supplementary materials)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeruIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Contrastive representation learning is crucial in medical time series
analysis as it alleviates dependency on labor-intensive, domain-specific, and
scarce expert annotations. However, existing contrastive learning methods
primarily focus on one single data level, which fails to fully exploit the
intricate nature of medical time series. To address this issue, we present
COMET, an innovative hierarchical framework that leverages data consistencies
at all inherent levels in medical time series. Our meticulously designed model
systematically captures data consistency from four potential levels:
observation, sample, trial, and patient levels. By developing contrastive loss
at multiple levels, we can learn effective representations that preserve
comprehensive data consistency, maximizing information utilization in a
self-supervised manner. We conduct experiments in the challenging
patient-independent setting. We compare COMET against six baselines using three
diverse datasets, which include ECG signals for myocardial infarction and EEG
signals for Alzheimer's and Parkinson's diseases. The results demonstrate that
COMET consistently outperforms all baselines, particularly in setup with 10%
and 1% labeled data fractions across all datasets. These results underscore the
significant impact of our framework in advancing contrastive representation
learning techniques for medical time series. The source code is available at
https://github.com/DL4mHealth/COMET.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14018" title="Abstract">arXiv:2310.14018</a> [<a href="/pdf/2310.14018" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal convolutional neural networks to generate a head-related  impulse response from one direction to another
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+T">Tatsuki Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Maruyama%2C+Y">Yoshiko Maruyama</a>, 
<a href="/search/cs?searchtype=author&query=Nambu%2C+I">Isao Nambu</a>, 
<a href="/search/cs?searchtype=author&query=Yano%2C+S">Shohei Yano</a>, 
<a href="/search/cs?searchtype=author&query=Wada%2C+Y">Yasuhiro Wada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Virtual sound synthesis is a technology that allows users to perceive spatial
sound through headphones or earphones. However, accurate virtual sound requires
an individual head-related transfer function (HRTF), which can be difficult to
measure due to the need for a specialized environment. In this study, we
proposed a method to generate HRTFs from one direction to the other. To this
end, we used temporal convolutional neural networks (TCNs) to generate
head-related impulse responses (HRIRs). To train the TCNs, publicly available
datasets in the horizontal plane were used. Using the trained networks, we
successfully generated HRIRs for directions other than the front direction in
the dataset. We found that the proposed method successfully generated HRIRs for
publicly available datasets. To test the generalization of the method, we
measured the HRIRs of a new dataset and tested whether the trained networks
could be used for this new dataset. Although the similarity evaluated by
spectral distortion was slightly degraded, behavioral experiments with human
participants showed that the generated HRIRs were equivalent to the measured
ones. These results suggest that the proposed TCNs can be used to generate
personalized HRIRs from one direction to another, which could contribute to the
personalization of virtual sound.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14019" title="Abstract">arXiv:2310.14019</a> [<a href="/pdf/2310.14019" title="Download PDF">pdf</a>, <a href="/format/2310.14019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Only Condense Once: Two Rules for Pruning Condensed Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yang He</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Lingao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Dataset condensation is a crucial tool for enhancing training efficiency by
reducing the size of the training dataset, particularly in on-device scenarios.
However, these scenarios have two significant challenges: 1) the varying
computational resources available on the devices require a dataset size
different from the pre-defined condensed dataset, and 2) the limited
computational resources often preclude the possibility of conducting additional
condensation processes. We introduce You Only Condense Once (YOCO) to overcome
these limitations. On top of one condensed dataset, YOCO produces smaller
condensed datasets with two embarrassingly simple dataset pruning rules: Low
LBPE Score and Balanced Construction. YOCO offers two key advantages: 1) it can
flexibly resize the dataset to fit varying computational constraints, and 2) it
eliminates the need for extra condensation processes, which can be
computationally prohibitive. Experiments validate our findings on networks
including ConvNet, ResNet and DenseNet, and datasets including CIFAR-10,
CIFAR-100 and ImageNet. For example, our YOCO surpassed various dataset
condensation and dataset pruning methods on CIFAR-10 with ten Images Per Class
(IPC), achieving 6.98-8.89% and 6.31-23.92% accuracy gains, respectively. The
code is available at: https://github.com/he-y/you-only-condense-once.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14021" title="Abstract">arXiv:2310.14021</a> [<a href="/pdf/2310.14021" title="Download PDF">pdf</a>, <a href="/format/2310.14021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey of Vector Database Management Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+J">James Jie Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianguo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoliang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">There are now over 20 commercial vector database management systems (VDBMSs),
all produced within the past five years. But embedding-based retrieval has been
studied for over ten years, and similarity search a staggering half century and
more. Driving this shift from algorithms to systems are new data intensive
applications, notably large language models, that demand vast stores of
unstructured data coupled with reliable, secure, fast, and scalable query
processing capability. A variety of new data management techniques now exist
for addressing these needs, however there is no comprehensive survey to
thoroughly review these techniques and systems. We start by identifying five
main obstacles to vector data management, namely vagueness of semantic
similarity, large size of vectors, high cost of similarity comparison, lack of
natural partitioning that can be used for indexing, and difficulty of
efficiently answering hybrid queries that require both attributes and vectors.
Overcoming these obstacles has led to new approaches to query processing,
storage and indexing, and query optimization and execution. For query
processing, a variety of similarity scores and query types are now well
understood; for storage and indexing, techniques include vector compression,
namely quantization, and partitioning based on randomization, learning
partitioning, and navigable partitioning; for query optimization and execution,
we describe new operators for hybrid queries, as well as techniques for plan
enumeration, plan selection, and hardware accelerated execution. These
techniques lead to a variety of VDBMSs across a spectrum of design and runtime
characteristics, including native systems specialized for vectors and extended
systems that incorporate vector capabilities into existing systems. We then
discuss benchmarks, and finally we outline research challenges and point the
direction for future work.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14022" title="Abstract">arXiv:2310.14022</a> [<a href="/pdf/2310.14022" title="Download PDF">pdf</a>, <a href="/format/2310.14022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trocq: Proof Transfer for Free, With or Without Univalence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+C">Cyril Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Crance%2C+E">Enzo Crance</a>, 
<a href="/search/cs?searchtype=author&query=Mahboubi%2C+A">Assia Mahboubi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Libraries of formalized mathematics use a possibly broad range of different
representations for a same mathematical concept. Yet light to major manual
input from users remains most often required for obtaining the corresponding
variants of theorems, when such obvious replacements are typically left
implicit on paper. This article presents Trocq, a new proof transfer framework
for dependent type theory. Trocq is based on a novel formulation of type
equivalence, used to generalize the univalent parametricity translation. This
framework takes care of avoiding dependency on the axiom of univalence when
possible, and may be used with more relations than just equivalences. We have
implemented a corresponding plugin for the Coq proof assistant, in the CoqElpi
meta-language. We use this plugin on a gallery of representative examples of
proof transfer issues in interactive theorem proving, and illustrate how Trocq
covers the spectrum of several existing tools, used in program verification as
well as in formalized mathematics in the broad sense.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14025" title="Abstract">arXiv:2310.14025</a> [<a href="/pdf/2310.14025" title="Download PDF">pdf</a>, <a href="/format/2310.14025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models and Multimodal Retrieval for Visual Word Sense  Disambiguation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kritharoula%2C+A">Anastasia Kritharoula</a>, 
<a href="/search/cs?searchtype=author&query=Lymperaiou%2C+M">Maria Lymperaiou</a>, 
<a href="/search/cs?searchtype=author&query=Stamou%2C+G">Giorgos Stamou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Empirical Methods in Natural Language Processing (EMNLP) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the
goal of retrieving an image among a set of candidates, which better represents
the meaning of an ambiguous word within a given context. In this paper, we make
a substantial step towards unveiling this interesting task by applying a
varying set of approaches. Since VWSD is primarily a text-image retrieval task,
we explore the latest transformer-based methods for multimodal retrieval.
Additionally, we utilize Large Language Models (LLMs) as knowledge bases to
enhance the given phrases and resolve ambiguity related to the target word. We
also study VWSD as a unimodal problem by converting to text-to-text and
image-to-image retrieval, as well as question-answering (QA), to fully explore
the capabilities of relevant models. To tap into the implicit knowledge of
LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable
answer generation. On top of all, we train a learn to rank (LTR) model in order
to combine our different modules, achieving competitive ranking results.
Extensive experiments on VWSD demonstrate valuable insights to effectively
drive future directions.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14028" title="Abstract">arXiv:2310.14028</a> [<a href="/pdf/2310.14028" title="Download PDF">pdf</a>, <a href="/format/2310.14028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GASCOM: Graph-based Attentive Semantic Context Modeling for Online  Conversation Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+V">Vibhor Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+N">Nishanth Sastry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Online conversation understanding is an important yet challenging NLP problem
which has many useful applications (e.g., hate speech detection). However,
online conversations typically unfold over a series of posts and replies to
those posts, forming a tree structure within which individual posts may refer
to semantic context from higher up the tree. Such semantic cross-referencing
makes it difficult to understand a single post by itself; yet considering the
entire conversation tree is not only difficult to scale but can also be
misleading as a single conversation may have several distinct threads or
points, not all of which are relevant to the post being considered. In this
paper, we propose a Graph-based Attentive Semantic COntext Modeling (GASCOM)
framework for online conversation understanding. Specifically, we design two
novel algorithms that utilise both the graph structure of the online
conversation as well as the semantic information from individual posts for
retrieving relevant context nodes from the whole conversation. We further
design a token-level multi-head graph attention mechanism to pay different
attentions to different tokens from different selected context utterances for
fine-grained conversation context modeling. Using this semantic conversational
context, we re-examine two well-studied problems: polarity prediction and hate
speech detection. Our proposed framework significantly outperforms
state-of-the-art methods on both tasks, improving macro-F1 scores by 4.5% for
polarity prediction and by 5% for hate speech detection. The GASCOM context
weights also enhance interpretability.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14029" title="Abstract">arXiv:2310.14029</a> [<a href="/pdf/2310.14029" title="Download PDF">pdf</a>, <a href="/format/2310.14029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline  Solids From Their Text Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rubungo%2C+A+N">Andre Niyongabo Rubungo</a>, 
<a href="/search/cs?searchtype=author&query=Arnold%2C+C">Craig Arnold</a>, 
<a href="/search/cs?searchtype=author&query=Rand%2C+B+P">Barry P. Rand</a>, 
<a href="/search/cs?searchtype=author&query=Dieng%2C+A+B">Adji Bousso Dieng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code for LLM-Prop can be found at: <a href="https://github.com/vertaix/LLM-Prop">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">The prediction of crystal properties plays a crucial role in the crystal
design process. Current methods for predicting crystal properties focus on
modeling crystal structures using graph neural networks (GNNs). Although GNNs
are powerful, accurately modeling the complex interactions between atoms and
molecules within a crystal remains a challenge. Surprisingly, predicting
crystal properties from crystal text descriptions is understudied, despite the
rich information and expressiveness that text data offer. One of the main
reasons is the lack of publicly available data for this task. In this paper, we
develop and make public a benchmark dataset (called TextEdge) that contains
text descriptions of crystal structures with their properties. We then propose
LLM-Prop, a method that leverages the general-purpose learning capabilities of
large language models (LLMs) to predict the physical and electronic properties
of crystals from their text descriptions. LLM-Prop outperforms the current
state-of-the-art GNN-based crystal property predictor by about 4% in predicting
band gap, 3% in classifying whether the band gap is direct or indirect, and 66%
in predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT,
a domain-specific pre-trained BERT model, despite having 3 times fewer
parameters. Our empirical results may highlight the current inability of GNNs
to capture information pertaining to space group symmetry and Wyckoff sites for
accurate crystal property prediction.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14030" title="Abstract">arXiv:2310.14030</a> [<a href="/pdf/2310.14030" title="Download PDF">pdf</a>, <a href="/format/2310.14030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Tracking Nonlinear Model Predictive Control Method for Autonomous  Wind Turbine Inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amer%2C+A">Abdelhakim Amer</a>, 
<a href="/search/cs?searchtype=author&query=Mehndiratta%2C+M">Mohit Mehndiratta</a>, 
<a href="/search/cs?searchtype=author&query=Sejersen%2C+J+l+F">Jonas le Fevre Sejersen</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H+X">Huy Xuan Pham</a>, 
<a href="/search/cs?searchtype=author&query=Kayacan%2C+E">Erdal Kayacan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, accepted for publication at ICAR conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Automated visual inspection of on-and offshore wind turbines using aerial
robots provides several benefits, namely, a safe working environment by
circumventing the need for workers to be suspended high above the ground,
reduced inspection time, preventive maintenance, and access to hard-to-reach
areas. A novel nonlinear model predictive control (NMPC) framework alongside a
global wind turbine path planner is proposed to achieve distance-optimal
coverage for wind turbine inspection. Unlike traditional MPC formulations,
visual tracking NMPC (VT-NMPC) is designed to track an inspection surface,
instead of a position and heading trajectory, thereby circumventing the need to
provide an accurate predefined trajectory for the drone. An additional
capability of the proposed VT-NMPC method is that by incorporating inspection
requirements as visual tracking costs to minimize, it naturally achieves the
inspection task successfully while respecting the physical constraints of the
drone. Multiple simulation runs and real-world tests demonstrate the efficiency
and efficacy of the proposed automated inspection framework, which outperforms
the traditional MPC designs, by providing full coverage of the target wind
turbine blades as well as its robustness to changing wind conditions. The
implementation codes are open-sourced.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14032" title="Abstract">arXiv:2310.14032</a> [<a href="/pdf/2310.14032" title="Download PDF">pdf</a>, <a href="/format/2310.14032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic  Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heppell%2C+F">Freddy Heppell</a>, 
<a href="/search/cs?searchtype=author&query=Bontcheva%2C+K">Kalina Bontcheva</a>, 
<a href="/search/cs?searchtype=author&query=Scarton%2C+C">Carolina Scarton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper analyses two hitherto unstudied sites sharing state-backed
disinformation, Reliable Recent News (rrn.world) and WarOnFakes
(waronfakes.com), which publish content in Arabic, Chinese, English, French,
German, and Spanish. We describe our content acquisition methodology and
perform cross-site unsupervised topic clustering on the resulting multilingual
dataset. We also perform linguistic and temporal analysis of the web page
translations and topics over time, and investigate articles with false
publication dates. We make publicly available this new dataset of 14,053
articles, annotated with each language version, and additional metadata such as
links and images. The main contribution of this paper for the NLP community is
in the novel dataset which enables studies of disinformation networks, and the
training of NLP tools for disinformation detection.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14034" title="Abstract">arXiv:2310.14034</a> [<a href="/pdf/2310.14034" title="Download PDF">pdf</a>, <a href="/format/2310.14034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree Prompting: Efficient Task Adaptation without Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morris%2C+J+X">John X. Morris</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+C">Chandan Singh</a>, 
<a href="/search/cs?searchtype=author&query=Rush%2C+A+M">Alexander M. Rush</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuntian Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Both first authors contributed equally; accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompting language models (LMs) is the main interface for applying them to
new tasks. However, for smaller LMs, prompting provides low accuracy compared
to gradient-based finetuning. Tree Prompting is an approach to prompting which
builds a decision tree of prompts, linking multiple LM calls together to solve
a task. At inference time, each call to the LM is determined by efficiently
routing the outcome of the previous call using the tree. Experiments on
classification datasets show that Tree Prompting improves accuracy over
competing methods and is competitive with fine-tuning. We also show that
variants of Tree Prompting allow inspection of a model's decision-making
process.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14037" title="Abstract">arXiv:2310.14037</a> [<a href="/pdf/2310.14037" title="Download PDF">pdf</a>, <a href="/format/2310.14037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlock Multi-Modal Capability of Dense Retrieval via Visual Module  Plugin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianshuo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+S">Sen Mei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinze Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Chenyan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Ge Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin
(MARVEL) to learn an embedding space for queries and multi-modal documents to
conduct retrieval. MARVEL encodes queries and multi-modal documents with a
unified encoder model, which helps to alleviate the modality gap between images
and texts. Specifically, we enable the image understanding ability of a
well-trained dense retriever, T5-ANCE, by incorporating the image features
encoded by the visual module as its inputs. To facilitate the multi-modal
retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22
dataset, which regards anchor texts as queries, and exact the related texts and
image documents from anchor linked web pages. Our experiments show that MARVEL
significantly outperforms the state-of-the-art methods on the multi-modal
retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the
visual module plugin method is tailored to enable the image understanding
ability for an existing dense retrieval model. Besides, we also show that the
language model has the ability to extract image semantics from image encoders
and adapt the image features in the input space of language models. All codes
are available at https://github.com/OpenMatch/MARVEL.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14038" title="Abstract">arXiv:2310.14038</a> [<a href="/pdf/2310.14038" title="Download PDF">pdf</a>, <a href="/format/2310.14038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Aware Wasserstein Distributionally Robust Control of Vessels in  Natural Waterways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nadales%2C+J+M">Juan Moreno Nadales</a>, 
<a href="/search/cs?searchtype=author&query=Hakobyan%2C+A">Astghik Hakobyan</a>, 
<a href="/search/cs?searchtype=author&query=de+la+Pe%C3%B1a%2C+D+M">David Mu&#xf1;oz de la Pe&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Limon%2C+D">Daniel Limon</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+I">Insoon Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In the realm of maritime transportation, autonomous vessel navigation in
natural inland waterways faces persistent challenges due to unpredictable
natural factors. Existing scheduling algorithms fall short in handling these
uncertainties, compromising both safety and efficiency. Moreover, these
algorithms are primarily designed for non-autonomous vessels, leading to
labor-intensive operations vulnerable to human error. To address these issues,
this study proposes a risk-aware motion control approach for vessels that
accounts for the dynamic and uncertain nature of tide islands in a
distributionally robust manner. Specifically, a model predictive control method
is employed to follow the reference trajectory in the time-space map while
incorporating a risk constraint to prevent grounding accidents. To address
uncertainties in tide islands, a novel modeling technique represents them as
stochastic polytopes. Additionally, potential inaccuracies in waterway depth
are addressed through a risk constraint that considers the worst-case
uncertainty distribution within a Wasserstein ambiguity set around the
empirical distribution. Using sensor data collected in the Guadalquivir River,
we empirically demonstrate the performance of the proposed method through
simulations on a vessel. As a result, the vessel successfully navigates the
waterway while avoiding grounding accidents, even with a limited dataset of
observations. This stands in contrast to existing non-robust controllers,
highlighting the robustness and practical applicability of the proposed
approach.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14040" title="Abstract">arXiv:2310.14040</a> [<a href="/pdf/2310.14040" title="Download PDF">pdf</a>, <a href="/format/2310.14040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Diffusion GAN Model for Symbolic Music Generation Controlled by  Emotions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jincheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fazekas%2C+G">Gy&#xf6;rgy Fazekas</a>, 
<a href="/search/cs?searchtype=author&query=Saitis%2C+C">Charalampos Saitis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Diffusion models have shown promising results for a wide range of generative
tasks with continuous data, such as image and audio synthesis. However, little
progress has been made on using diffusion models to generate discrete symbolic
music because this new class of generative models are not well suited for
discrete data while its iterative sampling process is computationally
expensive. In this work, we propose a diffusion model combined with a
Generative Adversarial Network, aiming to (i) alleviate one of the remaining
challenges in algorithmic music generation which is the control of generation
towards a target emotion, and (ii) mitigate the slow sampling drawback of
diffusion models applied to symbolic music generation. We first used a trained
Variational Autoencoder to obtain embeddings of a symbolic music dataset with
emotion labels and then used those to train a diffusion model. Our results
demonstrate the successful control of our diffusion model to generate symbolic
music with a desired emotion. Our model achieves several orders of magnitude
improvement in computational cost, requiring merely four time steps to denoise
while the steps required by current state-of-the-art diffusion models for
symbolic music generation is in the order of thousands.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14044" title="Abstract">arXiv:2310.14044</a> [<a href="/pdf/2310.14044" title="Download PDF">pdf</a>, <a href="/format/2310.14044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composer Style-specific Symbolic Music Generation Using Vector Quantized  Discrete Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jincheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jingjing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Saitis%2C+C">Charalampos Saitis</a>, 
<a href="/search/cs?searchtype=author&query=Fazekas%2C+G">Gy&#xf6;rgy Fazekas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Emerging Denoising Diffusion Probabilistic Models (DDPM) have become
increasingly utilised because of promising results they have achieved in
diverse generative tasks with continuous data, such as image and sound
synthesis. Nonetheless, the success of diffusion models has not been fully
extended to discrete symbolic music. We propose to combine a vector quantized
variational autoencoder (VQ-VAE) and discrete diffusion models for the
generation of symbolic music with desired composer styles. The trained VQ-VAE
can represent symbolic music as a sequence of indexes that correspond to
specific entries in a learned codebook. Subsequently, a discrete diffusion
model is used to model the VQ-VAE's discrete latent space. The diffusion model
is trained to generate intermediate music sequences consisting of codebook
indexes, which are then decoded to symbolic music using the VQ-VAE's decoder.
The results demonstrate our model can generate symbolic music with target
composer styles that meet the given conditions with a high accuracy of 72.36%.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14045" title="Abstract">arXiv:2310.14045</a> [<a href="/pdf/2310.14045" title="Download PDF">pdf</a>, <a href="/format/2310.14045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Image Derivatives: Increased Accuracy and Universal Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avrutskiy%2C+V+I">Vsevolod I. Avrutskiy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Derivative training is a well-known method to improve the accuracy of neural
networks. In the forward pass, not only the output values are computed, but
also their derivatives, and their deviations from the target derivatives are
included in the cost function, which is minimized with respect to the weights
by a gradient-based algorithm. So far, this method has been implemented for
relatively low-dimensional tasks. In this study, we apply the approach to the
problem of image analysis. We consider the task of reconstructing the vertices
of a cube based on its image. By training the derivatives with respect to the 6
degrees of freedom of the cube, we obtain 25 times more accurate results for
noiseless inputs. The derivatives also provide important insights into the
robustness problem, which is currently understood in terms of two types of
network vulnerabilities. The first type is small perturbations that
dramatically change the output, and the second type is substantial image
changes that the network erroneously ignores. They are currently considered as
conflicting goals, since conventional training methods produce a trade-off. The
first type can be analyzed via the gradient of the network, but the second type
requires human evaluation of the inputs, which is an oracle substitute. For the
task at hand, the nearest neighbor oracle can be defined, and the knowledge of
derivatives allows it to be expanded into Taylor series. This allows to perform
the first-order robustness analysis that unifies both types of vulnerabilities,
and to implement robust training that eliminates any trade-offs, so that
accuracy and robustness are limited only by network capacity.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14047" title="Abstract">arXiv:2310.14047</a> [<a href="/pdf/2310.14047" title="Download PDF">pdf</a>, <a href="/format/2310.14047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeaeQ: Mount Model Extraction Attacks with Efficient Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+C">Chengwei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+M">Minxuan Lv</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We study model extraction attacks in natural language processing (NLP) where
attackers aim to steal victim models by repeatedly querying the open
Application Programming Interfaces (APIs). Recent works focus on limited-query
budget settings and adopt random sampling or active learning-based sampling
strategies on publicly available, unannotated data sources. However, these
methods often result in selected queries that lack task relevance and data
diversity, leading to limited success in achieving satisfactory results with
low query costs. In this paper, we propose MeaeQ (Model extraction attack with
efficient Queries), a straightforward yet effective method to address these
issues. Specifically, we initially utilize a zero-shot sequence inference
classifier, combined with API service information, to filter task-relevant data
from a public text corpus instead of a problem domain-specific dataset.
Furthermore, we employ a clustering-based data reduction technique to obtain
representative data as queries for the attack. Extensive experiments conducted
on four benchmark datasets demonstrate that MeaeQ achieves higher functional
similarity to the victim model than baselines while requiring fewer queries.
Our code is available at https://github.com/C-W-D/MeaeQ.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14049" title="Abstract">arXiv:2310.14049</a> [<a href="/pdf/2310.14049" title="Download PDF">pdf</a>, <a href="/format/2310.14049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Post-Layout Simulation Driven Analog Circuit Sizing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiaohan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Siyuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+D+Z">David Z. Pan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Linxiao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yibo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ru Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Post-layout simulation provides accurate guidance for analog circuit design,
but post-layout performance is hard to be directly optimized at early design
stages. Prior work on analog circuit sizing often utilizes pre-layout
simulation results as the optimization objective. In this work, we propose a
post-layout-simulation-driven (post-simulation-driven for short) analog circuit
sizing framework that directly optimizes the post-layout simulation
performance. The framework integrates automated layout generation into the
optimization loop of transistor sizing and leverages a coupled Bayesian
optimization algorithm to search for the best post-simulation performance.
Experimental results demonstrate that our framework can achieve over 20% better
post-layout performance in competitive time than manual design and the method
that only considers pre-layout optimization.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14050" title="Abstract">arXiv:2310.14050</a> [<a href="/pdf/2310.14050" title="Download PDF">pdf</a>, <a href="/format/2310.14050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code-Switching with Word Senses for Pretraining in Neural Machine  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iyer%2C+V">Vivek Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Barba%2C+E">Edoardo Barba</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>, 
<a href="/search/cs?searchtype=author&query=Navigli%2C+R">Roberto Navigli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP (Findings) 2023 Long Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Lexical ambiguity is a significant and pervasive challenge in Neural Machine
Translation (NMT), with many state-of-the-art (SOTA) NMT systems struggling to
handle polysemous words (Campolungo et al., 2022). The same holds for the NMT
pretraining paradigm of denoising synthetic "code-switched" text (Pan et al.,
2021; Iyer et al., 2023), where word senses are ignored in the noising stage --
leading to harmful sense biases in the pretraining data that are subsequently
inherited by the resulting models. In this work, we introduce Word Sense
Pretraining for Neural Machine Translation (WSP-NMT) - an end-to-end approach
for pretraining multilingual NMT models leveraging word sense-specific
information from Knowledge Bases. Our experiments show significant improvements
in overall translation quality. Then, we show the robustness of our approach to
scale to various challenging data and resource-scarce scenarios and, finally,
report fine-grained accuracy improvements on the DiBiMT disambiguation
benchmark. Our studies yield interesting and novel insights into the merits and
challenges of integrating word sense information and structured knowledge in
multilingual pretraining for NMT.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14052" title="Abstract">arXiv:2310.14052</a> [<a href="/pdf/2310.14052" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CTMaaS: An innovative platform for C-ITS-enabled dynamic Traffic and  Fleet Management as a Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotsi%2C+A">Areti Kotsi</a>, 
<a href="/search/cs?searchtype=author&query=Klimi%2C+V">Vasileia Klimi</a>, 
<a href="/search/cs?searchtype=author&query=Mitsakis%2C+E">Evangelos Mitsakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
<p class="mathjax">Fleet management systems have been one of the most important research fields
in transportation science. Nowadays the enhancement of fleet management systems
with technologies such as the Cooperative Intelligent Transport System (CITS)
that allows fleets to communicate with their environment, with other vehicles
or with the road infrastructure, resulting in safer and more efficient road
travel. This paper aims to present the CTMaaS platform, a tool which integrates
CITS services and traffic management processes to manage vehicle fleets.
Starting with a literature review, the paper presents various fleet management
systems, that have been developed in the last years, and the most typical CITS
services. The next chapters present the CTMaaS platform, use cases, and
methodology.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14053" title="Abstract">arXiv:2310.14053</a> [<a href="/pdf/2310.14053" title="Download PDF">pdf</a>, <a href="/format/2310.14053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Accuracy: Evaluating Self-Consistency of Code Large Language  Models with IdentityChain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+M+J">Marcus J. Min</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yangruibo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Buratti%2C+L">Luca Buratti</a>, 
<a href="/search/cs?searchtype=author&query=Pujar%2C+S">Saurabh Pujar</a>, 
<a href="/search/cs?searchtype=author&query=Kaiser%2C+G">Gail Kaiser</a>, 
<a href="/search/cs?searchtype=author&query=Jana%2C+S">Suman Jana</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+B">Baishakhi Ray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/marcusm117/IdentityChain">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Software Engineering (cs.SE)

</div>
<p class="mathjax">Code Large Language Models (Code LLMs) are being increasingly employed in
real-life applications, so evaluating them is critical. While the general
accuracy of Code LLMs on individual tasks has been extensively evaluated, their
self-consistency across different tasks is overlooked. Intuitively, a
trustworthy model should be self-consistent when generating natural language
specifications for its own code and generating code for its own specifications.
Failure to preserve self-consistency reveals a lack of understanding of the
shared semantics underlying natural language and programming language, and
therefore undermines the trustworthiness of a model. In this paper, we first
formally define the self-consistency of Code LLMs and then design a framework,
IdentityChain, which effectively and efficiently evaluates the self-consistency
and general accuracy of a model at the same time. We study eleven Code LLMs and
show that they fail to preserve self-consistency, which is indeed a distinct
aspect from general accuracy. Furthermore, we show that IdentityChain can be
used as a model debugging tool to expose weaknesses of Code LLMs by
demonstrating three major weaknesses that we identify in current models using
IdentityChain. Our code is available at
https://github.com/marcusm117/IdentityChain.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14054" title="Abstract">arXiv:2310.14054</a> [<a href="/pdf/2310.14054" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All you need is data: the added value of National Access Points as  backbone European ITS data exchange infrastructures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mylonas%2C+C">Chrysostomos Mylonas</a>, 
<a href="/search/cs?searchtype=author&query=Stavara%2C+M">Maria Stavara</a>, 
<a href="/search/cs?searchtype=author&query=Mitsakis%2C+E">Evangelos Mitsakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>

</div>
<p class="mathjax">Intelligent Transport Systems are crucial in the digital transformation of
transportation. The EC mandates the establishment of National Access Points
(NAPs) in each Member State, serving as common national interfaces for ITS data
exchange. While progress has been made in standardizing NAP data, integration
with operational ITS practices remain limited. This paper presents five NAP use
cases from the NAPCORE (National Access Point Coordination Organization for
Europe) CEF funded project. The first one outlines a National Virtual Traffic
Management Center offering real time visualized KPIs supporting motorway
traffic operations. The second focuses on NAP enabled Cooperative ITS and
dynamic traffic management services. Next use case involves a Pan European
interface, providing visualizations of data availability. The fourth use case
enhances the digitization of traffic management plans, among different TMC.
Finally, the fifth use case demonstrates a technical interface combining NAP
traffic data with meteorological information for KPIs on extreme weather
impacts on traffic.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14056" title="Abstract">arXiv:2310.14056</a> [<a href="/pdf/2310.14056" title="Download PDF">pdf</a>, <a href="/format/2310.14056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> With a Few Square Roots, Quantum Computing is as Easy as &#x3a0;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carette%2C+J">Jacques Carette</a>, 
<a href="/search/cs?searchtype=author&query=Heunen%2C+C">Chris Heunen</a>, 
<a href="/search/cs?searchtype=author&query=Kaarsgaard%2C+R">Robin Kaarsgaard</a>, 
<a href="/search/cs?searchtype=author&query=Sabry%2C+A">Amr Sabry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Rig groupoids provide a semantic model of \PiLang, a universal classical
reversible programming language over finite types. We prove that extending rig
groupoids with just two maps and three equations about them results in a model
of quantum computing that is computationally universal and equationally sound
and complete for a variety of gate sets. The first map corresponds to an
$8^{\text{th}}$ root of the identity morphism on the unit $1$. The second map
corresponds to a square root of the symmetry on $1+1$. As square roots are
generally not unique and can sometimes even be trivial, the maps are
constrained to satisfy a nondegeneracy axiom, which we relate to the Euler
decomposition of the Hadamard gate. The semantic construction is turned into an
extension of \PiLang, called \SPiLang, that is a computationally universal
quantum programming language equipped with an equational theory that is sound
and complete with respect to the Clifford gate set, the standard gate set of
Clifford+T restricted to $\le 2$ qubits, and the computationally universal
Gaussian Clifford+T gate set.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14058" title="Abstract">arXiv:2310.14058</a> [<a href="/pdf/2310.14058" title="Download PDF">pdf</a>, <a href="/format/2310.14058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Caching Connections in Matchings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadeh%2C+Y">Yaniv Sadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+H">Haim Kaplan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Motivated by the desire to utilize a limited number of configurable optical
switches by recent advances in Software Defined Networks (SDNs), we define an
online problem which we call the Caching in Matchings problem. This problem has
a natural combinatorial structure and therefore may find additional
applications in theory and practice.
<br />In the Caching in Matchings problem our cache consists of $k$ matchings of
connections between servers that form a bipartite graph. To cache a connection
we insert it into one of the $k$ matchings possibly evicting at most two other
connections from this matching. This problem resembles the problem known as
Connection Caching, where we also cache connections but our only restriction is
that they form a graph with bounded degree $k$. Our results show a somewhat
surprising qualitative separation between the problems: The competitive ratio
of any online algorithm for caching in matchings must depend on the size of the
graph.
<br />Specifically, we give a deterministic $O(nk)$ competitive and randomized $O(n
\log k)$ competitive algorithms for caching in matchings, where $n$ is the
number of servers and $k$ is the number of matchings. We also show that the
competitive ratio of any deterministic algorithm is
$\Omega(\max(\frac{n}{k},k))$ and of any randomized algorithm is $\Omega(\log
\frac{n}{k^2 \log k} \cdot \log k)$. In particular, the lower bound for
randomized algorithms is $\Omega(\log n)$ regardless of $k$, and can be as high
as $\Omega(\log^2 n)$ if $k=n^{1/3}$, for example. We also show that if we
allow the algorithm to use at least $2k-1$ matchings compared to $k$ used by
the optimum then we match the competitive ratios of connection catching which
are independent of $n$. Interestingly, we also show that even a single extra
matching for the algorithm allows to get substantially better bounds.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14060" title="Abstract">arXiv:2310.14060</a> [<a href="/pdf/2310.14060" title="Download PDF">pdf</a>, <a href="/format/2310.14060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Offer you Cannot Refuse? Trends in the Coerciveness of Amazon Book  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rystr%C3%B8m%2C+J+H">Jonathan H. Rystr&#xf8;m</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Recommender systems can be a helpful tool for recommending content but they
can also influence users' preferences. One sociological theory for this
influence is that companies are incentivised to influence preferences to make
users easier to predict and thus more profitable by making it harder to change
preferences. This paper seeks to test that theory empirically. We use
\textit{Barrier-to-Exit}, a metric for how difficult it is for users to change
preferences, to analyse a large dataset of Amazon Book Ratings from 1998 to
2018. We focus the analysis on users who have changed preferences according to
Barrier-to-Exit. To assess the growth of Barrier-to-Exit over time, we
developed a linear mixed-effects model with crossed random effects for users
and categories. Our findings indicate a highly significant growth of
Barrier-to-Exit over time, suggesting that it has become more difficult for the
analysed subset of users to change their preferences. However, it should be
noted that these findings come with several statistical and methodological
caveats including sample bias and construct validity issues related to
Barrier-to-Exit. We discuss the strengths and limitations of our approach and
its implications. Additionally, we highlight the challenges of creating
context-sensitive and generalisable measures for complex socio-technical
concepts such as "difficulty to change preferences." We conclude with a call
for further research: to curb the potential threats of preference manipulation,
we need more measures that allow us to compare commercial as well as
non-commercial systems.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14062" title="Abstract">arXiv:2310.14062</a> [<a href="/pdf/2310.14062" title="Download PDF">pdf</a>, <a href="/format/2310.14062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Neural Tangent Kernel of Equilibrium Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhili Feng</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J.Zico Kolter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This work studies the neural tangent kernel (NTK) of the deep equilibrium
(DEQ) model, a practical ``infinite-depth'' architecture which directly
computes the infinite-depth limit of a weight-tied network via root-finding.
Even though the NTK of a fully-connected neural network can be stochastic if
its width and depth both tend to infinity simultaneously, we show that
contrarily a DEQ model still enjoys a deterministic NTK despite its width and
depth going to infinity at the same time under mild conditions. Moreover, this
deterministic NTK can be found efficiently via root-finding.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14063" title="Abstract">arXiv:2310.14063</a> [<a href="/pdf/2310.14063" title="Download PDF">pdf</a>, <a href="/format/2310.14063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept-based Anomaly Detection in Retail Stores for Automatic  Correction using Mobile Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+A">Aditya Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Sengar%2C+V">Vartika Sengar</a>, 
<a href="/search/cs?searchtype=author&query=George%2C+N">Nijil George</a>, 
<a href="/search/cs?searchtype=author&query=Vatsal%2C+V">Vighnesh Vatsal</a>, 
<a href="/search/cs?searchtype=author&query=Gubbi%2C+J">Jayavardhana Gubbi</a>, 
<a href="/search/cs?searchtype=author&query=P%2C+B">Balamuralidhar P</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Arpan Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, 2 tables, IEEE Transactions on Systems, Man and Cybernetics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Tracking of inventory and rearrangement of misplaced items are some of the
most labor-intensive tasks in a retail environment. While there have been
attempts at using vision-based techniques for these tasks, they mostly use
planogram compliance for detection of any anomalies, a technique that has been
found lacking in robustness and scalability. Moreover, existing systems rely on
human intervention to perform corrective actions after detection. In this
paper, we present Co-AD, a Concept-based Anomaly Detection approach using a
Vision Transformer (ViT) that is able to flag misplaced objects without using a
prior knowledge base such as a planogram. It uses an auto-encoder architecture
followed by outlier detection in the latent space. Co-AD has a peak success
rate of 89.90% on anomaly detection image sets of retail objects drawn from the
RP2K dataset, compared to 80.81% on the best-performing baseline of a standard
ViT auto-encoder. To demonstrate its utility, we describe a robotic mobile
manipulation pipeline to autonomously correct the anomalies flagged by Co-AD.
This work is ultimately aimed towards developing autonomous mobile robot
solutions that reduce the need for human intervention in retail store
management.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14064" title="Abstract">arXiv:2310.14064</a> [<a href="/pdf/2310.14064" title="Download PDF">pdf</a>, <a href="/format/2310.14064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Prediction Under Selective Confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiani%2C+S">Sohaib Kiani</a>, 
<a href="/search/cs?searchtype=author&query=Barton%2C+J">Jared Barton</a>, 
<a href="/search/cs?searchtype=author&query=Sushinsky%2C+J">Jon Sushinsky</a>, 
<a href="/search/cs?searchtype=author&query=Heimbach%2C+L">Lynda Heimbach</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bo Luo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IOS Press Ebooks pp 1256-1263. Volume 372: ECAI'23 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This research addresses the challenge of conducting interpretable causal
inference between a binary treatment and its resulting outcome when not all
confounders are known. Confounders are factors that have an influence on both
the treatment and the outcome. We relax the requirement of knowing all
confounders under desired treatment, which we refer to as Selective
Confounding, to enable causal inference in diverse real-world scenarios. Our
proposed scheme is designed to work in situations where multiple
decision-makers with different policies are involved and where there is a
re-evaluation mechanism after the initial decision to ensure consistency. These
assumptions are more practical to fulfill compared to the availability of all
confounders under all treatments. To tackle the issue of Selective Confounding,
we propose the use of dual-treatment samples. These samples allow us to employ
two-step procedures, such as Regression Adjustment or Doubly-Robust, to learn
counterfactual predictors. We provide both theoretical error bounds and
empirical evidence of the effectiveness of our proposed scheme using synthetic
and real-world child placement data. Furthermore, we introduce three evaluation
methods specifically tailored to assess the performance in child placement
scenarios. By emphasizing transparency and interpretability, our approach aims
to provide decision-makers with a valuable tool. The source code repository of
this work is located at https://github.com/sohaib730/CausalML.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14065" title="Abstract">arXiv:2310.14065</a> [<a href="/pdf/2310.14065" title="Download PDF">pdf</a>, <a href="/format/2310.14065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POVNav: A Pareto-Optimal Mapless Visual Navigator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pushp%2C+D">Durgakant Pushp</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chaomin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gregory%2C+J+M">Jason M. Gregory</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lantao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Mapless navigation has emerged as a promising approach for enabling
autonomous robots to navigate in environments where pre-existing maps may be
inaccurate, outdated, or unavailable. In this work, we propose an image-based
local representation of the environment immediately around a robot to parse
navigability. We further develop a local planning and control framework, a
Pareto-optimal mapless visual navigator (POVNav), to use this representation
and enable autonomous navigation in various challenging and real-world
environments. In POVNav, we choose a Pareto-optimal sub-goal in the image by
evaluating all the navigable pixels, finding a safe visual path, and generating
actions to follow the path using visual servo control. In addition to providing
collision-free motion, our approach enables selective navigation behavior, such
as restricting navigation to select terrain types, by only changing the
navigability definition in the local representation. The ability of POVNav to
navigate a robot to the goal using only a monocular camera without relying on a
map makes it computationally light and easy to implement on various robotic
platforms. Real-world experiments in diverse challenging environments, ranging
from structured indoor environments to unstructured outdoor environments such
as forest trails and roads after a heavy snowfall, using various image
segmentation techniques demonstrate the remarkable efficacy of our proposed
framework.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14069" title="Abstract">arXiv:2310.14069</a> [<a href="/pdf/2310.14069" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Bidirectional Variational Autoencoder for Image Domain  Translation of Dotted Arabic Expiration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zidane%2C+A">Ahmed Zidane</a>, 
<a href="/search/cs?searchtype=author&query=Soliman%2C+G">Ghada Soliman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 Pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">THIS paper proposes an approach of Ladder Bottom-up Convolutional
Bidirectional Variational Autoencoder (LCBVAE) architecture for the encoder and
decoder, which is trained on the image translation of the dotted Arabic
expiration dates by reconstructing the Arabic dotted expiration dates into
filled-in expiration dates. We employed a customized and adapted version of
Convolutional Recurrent Neural Network CRNN model to meet our specific
requirements and enhance its performance in our context, and then trained the
custom CRNN model with the filled-in images from the year of 2019 to 2027 to
extract the expiration dates and assess the model performance of LCBVAE on the
expiration date recognition. The pipeline of (LCBVAE+CRNN) can be then
integrated into an automated sorting systems for extracting the expiry dates
and sorting the products accordingly during the manufacture stage.
Additionally, it can overcome the manual entry of expiration dates that can be
time-consuming and inefficient at the merchants. Due to the lack of the
availability of the dotted Arabic expiration date images, we created an Arabic
dot-matrix True Type Font (TTF) for the generation of the synthetic images. We
trained the model with unrealistic synthetic dates of 59902 images and
performed the testing on a realistic synthetic date of 3287 images from the
year of 2019 to 2027, represented as yyyy/mm/dd. In our study, we demonstrated
the significance of latent bottleneck layer with improving the generalization
when the size is increased up to 1024 in downstream transfer learning tasks as
for image translation. The proposed approach achieved an accuracy of 97% on the
image translation with using the LCBVAE architecture that can be generalized
for any downstream learning tasks as for image translation and reconstruction.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14073" title="Abstract">arXiv:2310.14073</a> [<a href="/pdf/2310.14073" title="Download PDF">pdf</a>, <a href="/format/2310.14073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact asymptotic estimation of unknown parameters of regression  equations with additive perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Glushchenko%2C+A">Anton Glushchenko</a>, 
<a href="/search/eess?searchtype=author&query=Lastochkin%2C+K">Konstantin Lastochkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Most identification methods of unknown parameters of linear regression
equations (LRE) ensure only boundedness of a parametric error in the presence
of additive perturbations, which is almost always unacceptable for practical
scenarios. In this paper, a new identification law is proposed to overcome this
drawback and guarantee asymptotic convergence of the unknown parameters
estimation error to zero in case the mentioned additive perturbation meets
special averaging conditions. Theoretical results are illustrated by numerical
simulations.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14075" title="Abstract">arXiv:2310.14075</a> [<a href="/pdf/2310.14075" title="Download PDF">pdf</a>, <a href="/format/2310.14075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Sim-to-Real Adaptation of Soft Robot Proprioception using a  Dual Cross-modal Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chaeree Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyunkyu Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jung Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Soft robotics is a modern robotic paradigm for performing dexterous
interactions with the surroundings via morphological flexibility. The desire
for autonomous operation requires soft robots to be capable of proprioception
and makes it necessary to devise a calibration process. These requirements can
be greatly benefited by adopting numerical simulation for computational
efficiency. However, the gap between the simulated and real domains limits the
accurate, generalized application of the approach. Herein, we propose an
unsupervised domain adaptation framework as a data-efficient, generalized
alignment of these heterogeneous sensor domains. A dual cross-modal autoencoder
was designed to match the sensor domains at a feature level without any
extensive labeling process, facilitating the computationally efficient
transferability to various tasks. As a proof-of-concept, the methodology was
adopted to the famous soft robot design, a multigait soft robot, and two
fundamental perception tasks for autonomous robot operation, involving
high-fidelity shape estimation and collision detection. The resulting
perception demonstrates the digital-twinned calibration process in both the
simulated and real domains. The proposed design outperforms the existing
prevalent benchmarks for both perception tasks. This unsupervised framework
envisions a new approach to imparting embodied intelligence to soft robotic
systems via blending simulation.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14076" title="Abstract">arXiv:2310.14076</a> [<a href="/pdf/2310.14076" title="Download PDF">pdf</a>, <a href="/format/2310.14076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Relationship Between Relevance and Conflict in Online Social Link  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanbang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">In an online social network, link recommendations are a way for users to
discover relevant links to people they may know, thereby potentially increasing
their engagement on the platform. However, the addition of links to a social
network can also have an effect on the level of conflict in the network --
expressed in terms of polarization and disagreement. To this date, however, we
have very little understanding of how these two implications of link formation
relate to each other: are the goals of high relevance and conflict reduction
aligned, or are the links that users are most likely to accept fundamentally
different from the ones with the greatest potential for reducing conflict? Here
we provide the first analysis of this question, using the recently popular
Friedkin-Johnsen model of opinion dynamics. We first present a surprising
result on how link additions shift the level of opinion conflict, followed by
explanation work that relates the amount of shift to structural features of the
added links. We then characterize the gap in conflict reduction between the set
of links achieving the largest reduction and the set of links achieving the
highest relevance. The gap is measured on real-world data, based on
instantiations of relevance defined by 13 link recommendation algorithms. We
find that some, but not all, of the more accurate algorithms actually lead to
better reduction of conflict. Our work suggests that social links recommended
for increasing user engagement may not be as conflict-provoking as people might
have thought.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14078" title="Abstract">arXiv:2310.14078</a> [<a href="/pdf/2310.14078" title="Download PDF">pdf</a>, <a href="/format/2310.14078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Duet between Metric Embeddings and Minimum-Weight Perfect  Matchings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhore%2C+S">Sujoy Bhore</a>, 
<a href="/search/cs?searchtype=author&query=Filtser%2C+A">Arnold Filtser</a>, 
<a href="/search/cs?searchtype=author&query=T%C3%B3th%2C+C+D">Csaba D. T&#xf3;th</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages, 8 figures, to be presented at the ACM-SIAM Symposium on Discrete Algorithms (SODA24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Low-distortional metric embeddings are a crucial component in the modern
algorithmic toolkit. In an online metric embedding, points arrive sequentially
and the goal is to embed them into a simple space irrevocably, while minimizing
the distortion. Our first result is a deterministic online embedding of a
general metric into Euclidean space with distortion $O(\log
n)\cdot\min\{\sqrt{\log\Phi},\sqrt{n}\}$ (or,
$O(d)\cdot\min\{\sqrt{\log\Phi},\sqrt{n}\}$ if the metric has doubling
dimension $d$), solving a conjecture by Newman and Rabinovich (2020), and
quadratically improving the dependence on the aspect ratio $\Phi$ from Indyk et
al.\ (2010). Our second result is a stochastic embedding of a metric space into
trees with expected distortion $O(d\cdot \log\Phi)$, generalizing previous
results (Indyk et al.\ (2010), Bartal et al.\ (2020)).
<br />Next, we study the \emph{online minimum-weight perfect matching} problem,
where a sequence of $2n$ metric points arrive in pairs, and one has to maintain
a perfect matching at all times. We allow recourse (as otherwise the order of
arrival determines the matching). The goal is to return a perfect matching that
approximates the \emph{minimum-weight} perfect matching at all times, while
minimizing the recourse. Our third result is a randomized algorithm with
competitive ratio $O(d\cdot \log \Phi)$ and recourse $O(\log \Phi)$ against an
oblivious adversary, this result is obtained via our new stochastic online
embedding. Our fourth result is a deterministic algorithm against an adaptive
adversary, using $O(\log^2 n)$ recourse, that maintains a matching of weight at
most $O(\log n)$ times the weight of the MST, i.e., a matching of lightness
$O(\log n)$. We complement our upper bounds with a strategy for an oblivious
adversary that, with recourse $r$, establishes a lower bound of
$\Omega(\frac{\log n}{r \log r})$ for both competitive ratio and lightness.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14079" title="Abstract">arXiv:2310.14079</a> [<a href="/pdf/2310.14079" title="Download PDF">pdf</a>, <a href="/format/2310.14079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Copy, or not to Copy; That is a Critical Issue of the Output Softmax  Layer in Neural Sequential Recommenders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Haw-Shiuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+N">Nikhil Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent studies suggest that the existing neural models have difficulty
handling repeated items in sequential recommendation tasks. However, our
understanding of this difficulty is still limited. In this study, we
substantially advance this field by identifying a major source of the problem:
the single hidden state embedding and static item embeddings in the output
softmax layer. Specifically, the similarity structure of the global item
embeddings in the softmax layer sometimes forces the single hidden state
embedding to be close to new items when copying is a better choice, while
sometimes forcing the hidden state to be close to the items from the input
inappropriately. To alleviate the problem, we adapt the recently-proposed
softmax alternatives such as softmax-CPR to sequential recommendation tasks and
demonstrate that the new softmax architectures unleash the capability of the
neural encoder on learning when to copy and when to exclude the items from the
input sequence. By only making some simple modifications on the output softmax
layer for SASRec and GRU4Rec, softmax-CPR achieves consistent improvement in 12
datasets. With almost the same model size, our best method not only improves
the average NDCG@10 of GRU4Rec in 5 datasets with duplicated items by 10%
(4%-17% individually) but also improves 7 datasets without duplicated items by
24% (8%-39%)!
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14080" title="Abstract">arXiv:2310.14080</a> [<a href="/pdf/2310.14080" title="Download PDF">pdf</a>, <a href="/ps/2310.14080" title="Download PostScript">ps</a>, <a href="/format/2310.14080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New extremal Type II $\mathbb{Z}_4$-codes of length 64 by the doubling  method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ban%2C+S">Sara Ban</a>, 
<a href="/search/cs?searchtype=author&query=Rukavina%2C+S">Sanja Rukavina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Extremal Type II $\mathbb{Z}_4$-codes are a class of self-dual
$\mathbb{Z}_4$-codes with Euclidean weights divisible by eight and the largest
possible minimum Euclidean weight for a given length. A small number of such
codes is known for lengths greater than or equal to $48.$ The doubling method
is a method for constructing Type II $\mathbb{Z}_4$-codes from a given Type II
$\mathbb{Z}_4$-code. Based on the doubling method, in this paper we develop a
method to construct new extremal Type II $\mathbb{Z}_4$-codes starting from an
extremal Type II $\mathbb{Z}_4$-code of type $4^k$ with an extremal residue
code and length $48, 56$ or $64$. Using this method, we construct three new
extremal Type II $\mathbb{Z}_4$-codes of length $64$ and type $4^{31}2^2$.
Extremal Type II $\mathbb{Z}_4$-codes of length $64$ of this type were not
known before. Moreover, the residue codes of the constructed extremal
$\mathbb{Z}_4$-codes are new best known $[64,31]$ binary codes and the supports
of the minimum weight codewords of the residue code and the torsion code of one
of these codes form self-orthogonal $1$-designs.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14084" title="Abstract">arXiv:2310.14084</a> [<a href="/pdf/2310.14084" title="Download PDF">pdf</a>, <a href="/format/2310.14084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks and Applied Linear Algebra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Moore%2C+N+S">Nicholas S. Moore</a>, 
<a href="/search/math?searchtype=author&query=Cyr%2C+E+C">Eric C. Cyr</a>, 
<a href="/search/math?searchtype=author&query=Ohm%2C+P">Peter Ohm</a>, 
<a href="/search/math?searchtype=author&query=Siefert%2C+C+M">Christopher M. Siefert</a>, 
<a href="/search/math?searchtype=author&query=Tuminaro%2C+R+S">Raymond S. Tuminaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Sparse matrix computations are ubiquitous in scientific computing. With the
recent interest in scientific machine learning, it is natural to ask how sparse
matrix computations can leverage neural networks (NN). Unfortunately,
multi-layer perceptron (MLP) neural networks are typically not natural for
either graph or sparse matrix computations. The issue lies with the fact that
MLPs require fixed-sized inputs while scientific applications generally
generate sparse matrices with arbitrary dimensions and a wide range of nonzero
patterns (or matrix graph vertex interconnections). While convolutional NNs
could possibly address matrix graphs where all vertices have the same number of
nearest neighbors, a more general approach is needed for arbitrary sparse
matrices, e.g. arising from discretized partial differential equations on
unstructured meshes. Graph neural networks (GNNs) are one approach suitable to
sparse matrices. GNNs define aggregation functions (e.g., summations) that
operate on variable size input data to produce data of a fixed output size so
that MLPs can be applied. The goal of this paper is to provide an introduction
to GNNs for a numerical linear algebra audience. Concrete examples are provided
to illustrate how many common linear algebra tasks can be accomplished using
GNNs. We focus on iterative methods that employ computational kernels such as
matrix-vector products, interpolation, relaxation methods, and
strength-of-connection measures. Our GNN examples include cases where
parameters are determined a-priori as well as cases where parameters must be
learned. The intent with this article is to help computational scientists
understand how GNNs can be used to adapt machine learning concepts to
computational tasks associated with sparse matrices. It is hoped that this
understanding will stimulate data-driven extensions of classical sparse linear
algebra tasks.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14085" title="Abstract">arXiv:2310.14085</a> [<a href="/pdf/2310.14085" title="Download PDF">pdf</a>, <a href="/ps/2310.14085" title="Download PostScript">ps</a>, <a href="/format/2310.14085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and  Exp-Concave Games with Gradient Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tianyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhengyuan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Operations Research; 47 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Online gradient descent (OGD) is well known to be doubly optimal under strong
convexity or monotonicity assumptions: (1) in the single-agent setting, it
achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost
functions; and (2) in the multi-agent setting of strongly monotone games, with
each agent employing OGD, we obtain last-iterate convergence of the joint
action to a unique Nash equilibrium at an optimal rate of
$\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits,
OGD has the drawback that it requires knowing the strong convexity/monotonicity
parameters. In this paper, we design a fully adaptive OGD algorithm,
\textsf{AdaOGD}, that does not require a priori knowledge of these parameters.
In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under
strong convexity, which is optimal up to a log factor. Further, if each agent
employs \textsf{AdaOGD} in strongly monotone games, the joint action converges
in a last-iterate sense to a unique Nash equilibrium at a rate of
$O(\frac{\log^3 T}{T})$, again optimal up to log factors. We illustrate our
algorithms in a learning version of the classical newsvendor problem, where due
to lost sales, only (noisy) gradient feedback can be observed. Our results
immediately yield the first feasible and near-optimal algorithm for both the
single-retailer and multi-retailer settings. We also extend our results to the
more general setting of exp-concave cost functions and games, using the online
Newton step (ONS) algorithm.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14087" title="Abstract">arXiv:2310.14087</a> [<a href="/pdf/2310.14087" title="Download PDF">pdf</a>, <a href="/format/2310.14087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Specialized Semismooth Newton Method for Kernel-Based Optimal  Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tianyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cuturi%2C+M">Marco Cuturi</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 36 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Kernel-based optimal transport (OT) estimators offer an alternative,
functional estimation procedure to address OT problems from samples. Recent
works suggest that these estimators are more statistically efficient than
plug-in (linear programming-based) OT estimators when comparing probability
measures in high-dimensions~\citep{Vacher-2021-Dimension}. Unfortunately, that
statistical benefit comes at a very steep computational price: because their
computation relies on the short-step interior-point method (SSIPM), which comes
with a large iteration count in practice, these estimators quickly become
intractable w.r.t. sample size $n$. To scale these estimators to larger $n$, we
propose a nonsmooth fixed-point model for the kernel-based OT problem, and show
that it can be efficiently solved via a specialized semismooth Newton (SSN)
method: We show, exploring the problem's structure, that the per-iteration cost
of performing one SSN step can be significantly reduced in practice. We prove
that our SSN method achieves a global convergence rate of $O(1/\sqrt{k})$, and
a local quadratic convergence rate under standard regularity conditions. We
show substantial speedups over SSIPM on both synthetic and real datasets.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14088" title="Abstract">arXiv:2310.14088</a> [<a href="/pdf/2310.14088" title="Download PDF">pdf</a>, <a href="/format/2310.14088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark  for Language Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+A">An Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E+Y">Eric Y. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Gentili%2C+A">Amilcare Gentili</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chun-Nan Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Curated datasets for healthcare are often limited due to the need of human
annotations from experts. In this paper, we present MedEval, a multi-level,
multi-task, and multi-domain medical benchmark to facilitate the development of
language models for healthcare. MedEval is comprehensive and consists of data
from several healthcare systems and spans 35 human body regions from 8
examination modalities. With 22,779 collected sentences and 21,228 reports, we
provide expert annotations at multiple levels, offering a granular potential
usage of the data and supporting a wide range of tasks. Moreover, we
systematically evaluated 10 generic and domain-specific language models under
zero-shot and finetuning settings, from domain-adapted baselines in healthcare
to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our
evaluations reveal varying effectiveness of the two categories of language
models across different tasks, from which we notice the importance of
instruction tuning for few-shot usage of large language models. Our
investigation paves the way toward benchmarking language models for healthcare
and provides valuable insights into the strengths and limitations of adopting
large language models in medical domains, informing their practical
applications and future advancements.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14090" title="Abstract">arXiv:2310.14090</a> [<a href="/pdf/2310.14090" title="Download PDF">pdf</a>, <a href="/ps/2310.14090" title="Download PostScript">ps</a>, <a href="/format/2310.14090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A case study on latency, bandwidth and energy efficiency of mobile 5G  and YouTube Edge service in London. Why the 5G ecosystem and energy  efficiency matter?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+P">Peixuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">JunKyu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Mukhanov%2C+L">Lev Mukhanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The advancements in 5G mobile networks and Edge computing offer great
potential for services like augmented reality and Cloud gaming, thanks to their
low latency and high bandwidth capabilities. However, the practical limitations
of achieving optimal latency on real applications remain uncertain. This paper
aims to investigate the actual latency and bandwidth provided by 5G Networks
and YouTube Edge service in London, UK. We analyze how latency and bandwidth
differ between 4G LTE and 5G networks and how the location of YouTube Edge
servers impacts these metrics. Our research reveals over 10 significant
observations and implications, indicating that the primary constraints on 4G
LTE and 5G capabilities are the ecosystem and energy efficiency of mobile
devices down-streaming data. Our study demonstrates that to fully unlock the
potential of 5G and it's applications, it is crucial to prioritize efforts
aimed at improving the ecosystem and enhancing the energy efficiency.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14092" title="Abstract">arXiv:2310.14092</a> [<a href="/pdf/2310.14092" title="Download PDF">pdf</a>, <a href="/format/2310.14092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Reward for Physical Skills using Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yuwei Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yiqing Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoRL 2023, LangRob workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning reward functions for physical skills are challenging due to the vast
spectrum of skills, the high-dimensionality of state and action space, and
nuanced sensory feedback. The complexity of these tasks makes acquiring expert
demonstration data both costly and time-consuming. Large Language Models (LLMs)
contain valuable task-related knowledge that can aid in learning these reward
functions. However, the direct application of LLMs for proposing reward
functions has its limitations such as numerical instability and inability to
incorporate the environment feedback. We aim to extract task knowledge from
LLMs using environment feedback to create efficient reward functions for
physical skills. Our approach consists of two components. We first use the LLM
to propose features and parameterization of the reward function. Next, we
update the parameters of this proposed reward function through an iterative
self-alignment process. In particular, this process minimizes the ranking
inconsistency between the LLM and our learned reward functions based on the new
observations. We validated our method by testing it on three simulated physical
skill learning tasks, demonstrating effective support for our design choices.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14093" title="Abstract">arXiv:2310.14093</a> [<a href="/pdf/2310.14093" title="Download PDF">pdf</a>, <a href="/format/2310.14093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Knowledge Graphs for Orphan Entity Allocation in Resume  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakliwal%2C+A">Aagam Bakliwal</a>, 
<a href="/search/cs?searchtype=author&query=Gandhi%2C+S+M">Shubham Manish Gandhi</a>, 
<a href="/search/cs?searchtype=author&query=Haribhakta%2C+Y">Yashodhara Haribhakta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 2023 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Significant challenges are posed in talent acquisition and recruitment by
processing and analyzing unstructured data, particularly resumes. This research
presents a novel approach for orphan entity allocation in resume processing
using knowledge graphs. Techniques of association mining, concept extraction,
external knowledge linking, named entity recognition, and knowledge graph
construction are integrated into our pipeline. By leveraging these techniques,
the aim is to automate and enhance the efficiency of the job screening process
by successfully bucketing orphan entities within resumes. This allows for more
effective matching between candidates and job positions, streamlining the
resume screening process, and enhancing the accuracy of candidate-job matching.
The approach's exceptional effectiveness and resilience are highlighted through
extensive experimentation and evaluation, ensuring that alternative measures
can be relied upon for seamless processing and orphan entity allocation in case
of any component failure. The capabilities of knowledge graphs in generating
valuable insights through intelligent information extraction and
representation, specifically in the domain of categorizing orphan entities, are
highlighted by the results of our research.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14098" title="Abstract">arXiv:2310.14098</a> [<a href="/pdf/2310.14098" title="Download PDF">pdf</a>, <a href="/format/2310.14098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilizing reinforcement learning control: A modular framework for  optimizing over all stable behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+N+P">Nathan P. Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=Loewen%2C+P+D">Philip D. Loewen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Forbes%2C+M+G">Michael G. Forbes</a>, 
<a href="/search/cs?searchtype=author&query=Gopaluni%2C+R+B">R. Bhushan Gopaluni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; 18 pages. arXiv admin note: text overlap with <a href="/abs/2304.03422">arXiv:2304.03422</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a framework for the design of feedback controllers that combines
the optimization-driven and model-free advantages of deep reinforcement
learning with the stability guarantees provided by using the Youla-Kucera
parameterization to define the search domain. Recent advances in behavioral
systems allow us to construct a data-driven internal model; this enables an
alternative realization of the Youla-Kucera parameterization based entirely on
input-output exploration data. Perhaps of independent interest, we formulate
and analyze the stability of such data-driven models in the presence of noise.
The Youla-Kucera approach requires a stable "parameter" for controller design.
For the training of reinforcement learning agents, the set of all stable linear
operators is given explicitly through a matrix factorization approach.
Moreover, a nonlinear extension is given using a neural network to express a
parameterized set of stable operators, which enables seamless integration with
standard deep learning libraries. Finally, we show how these ideas can also be
applied to tune fixed-structure controllers.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14101" title="Abstract">arXiv:2310.14101</a> [<a href="/pdf/2310.14101" title="Download PDF">pdf</a>, <a href="/format/2310.14101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel and Distributed Data Series Processing on Modern and Emerging  Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatourou%2C+P">Panagiota Fatourou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper will appear in the Proceedings of the 15th International Conference on Management of Digital EcoSystems (MEDES'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Databases (cs.DB)

</div>
<p class="mathjax">This paper summarizes state-of-the-art results on data series processing with
the emphasis on parallel and distributed data series indexes that exploit the
computational power of modern computing platforms. The paper comprises a
summary of the tutorial the author delivered at the 15th International
Conference on Management of Digital EcoSystems (MEDES'23).
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14103" title="Abstract">arXiv:2310.14103</a> [<a href="/pdf/2310.14103" title="Download PDF">pdf</a>, <a href="/format/2310.14103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faysse%2C+M">Manuel Faysse</a>, 
<a href="/search/cs?searchtype=author&query=Viaud%2C+G">Gautier Viaud</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short paper accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the
zero-shot capabilities of Large Language Models (LLMs), but in doing so induces
new evaluation metric requirements. We show LLM-based metrics to be well
adapted to these requirements, and leverage them to conduct an investigation of
task-specialization strategies, quantifying the trade-offs that emerge in
practical industrial settings. Our findings offer practitioners actionable
insights for real-world IFT model deployment.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14105" title="Abstract">arXiv:2310.14105</a> [<a href="/pdf/2310.14105" title="Download PDF">pdf</a>, <a href="/format/2310.14105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Learning of Individualized Task Contrast Prediction from  Resting-state Functional Connectomes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M">Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+G+H">Gia H. Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Sabuncu%2C+M+R">Mert R. Sabuncu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at DALI@MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Given sufficient pairs of resting-state and task-evoked fMRI scans from
subjects, it is possible to train ML models to predict subject-specific
task-evoked activity using resting-state functional MRI (rsfMRI) scans.
However, while rsfMRI scans are relatively easy to collect, obtaining
sufficient task fMRI scans is much harder as it involves more complex
experimental designs and procedures. Thus, the reliance on scarce paired data
limits the application of current techniques to only tasks seen during
training. We show that this reliance can be reduced by leveraging group-average
contrasts, enabling zero-shot predictions for novel tasks. Our approach, named
OPIC (short for Omni-Task Prediction of Individual Contrasts), takes as input a
subject's rsfMRI-derived connectome and a group-average contrast, to produce a
prediction of the subject-specific contrast. Similar to zero-shot learning in
large language models using special inputs to obtain answers for novel natural
language processing tasks, inputting group-average contrasts guides the OPIC
model to generalize to novel tasks unseen in training. Experimental results
show that OPIC's predictions for novel tasks are not only better than simple
group-averages, but are also competitive with a state-of-the-art model's
in-domain predictions that was trained using in-domain tasks' data.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14107" title="Abstract">arXiv:2310.14107</a> [<a href="/pdf/2310.14107" title="Download PDF">pdf</a>, <a href="/format/2310.14107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Transferability of Visually Grounded PCFGs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanpeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP Findings 2023. Our code is available at <a href="https://github.com/zhaoyanpeng/cpcfg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">There has been a significant surge of interest in visually grounded grammar
induction in recent times. While a variety of models have been developed for
the task and have demonstrated impressive performance, they have not been
evaluated on text domains that are different from the training domain, so it is
unclear if the improvements brought by visual groundings are transferable. Our
study aims to fill this gap and assess the degree of transferability. We start
by extending VC-PCFG (short for Visually-grounded Compound
PCFG~\citep{zhao-titov-2020-visually}) in such a way that it can transfer
across text domains. We consider a zero-shot transfer learning setting where a
model is trained on the source domain and is directly applied to target
domains, without any further training. Our experimental results suggest that:
the benefits from using visual groundings transfer to text in a domain similar
to the training domain but fail to transfer to remote domains. Further, we
conduct data and result analysis; we find that the lexicon overlap between the
source domain and the target domain is the most important factor in the
transferability of VC-PCFG.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14108" title="Abstract">arXiv:2310.14108</a> [<a href="/pdf/2310.14108" title="Download PDF">pdf</a>, <a href="/format/2310.14108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salehi%2C+M">Mohammadreza Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Farajtabar%2C+M">Mehrdad Farajtabar</a>, 
<a href="/search/cs?searchtype=author&query=Horton%2C+M">Maxwell Horton</a>, 
<a href="/search/cs?searchtype=author&query=Faghri%2C+F">Fartash Faghri</a>, 
<a href="/search/cs?searchtype=author&query=Pouransari%2C+H">Hadi Pouransari</a>, 
<a href="/search/cs?searchtype=author&query=Vemulapalli%2C+R">Raviteja Vemulapalli</a>, 
<a href="/search/cs?searchtype=author&query=Tuzel%2C+O">Oncel Tuzel</a>, 
<a href="/search/cs?searchtype=author&query=Farhadi%2C+A">Ali Farhadi</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Sachin Mehta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Contrastive language image pretraining (CLIP) is a standard method for
training vision-language models. While CLIP is scalable, promptable, and robust
to distribution shifts on image classification tasks, it lacks object
localization capabilities. This paper studies the following question: Can we
augment CLIP training with task-specific vision models from model zoos to
improve its visual representations? Towards this end, we leverage open-source
task-specific vision models to generate pseudo-labels for an uncurated and
noisy image-text dataset. Subsequently, we train CLIP models on these
pseudo-labels in addition to the contrastive training on image and text pairs.
This simple setup shows substantial improvements of up to 16.3% across
different vision tasks, including segmentation, detection, depth estimation,
and surface normal estimation. Importantly, these enhancements are achieved
without compromising CLIP's existing capabilities, including its proficiency in
promptable zero-shot classification.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14110" title="Abstract">arXiv:2310.14110</a> [<a href="/pdf/2310.14110" title="Download PDF">pdf</a>, <a href="/format/2310.14110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-context Indexing of Restricted Output Space for NLP Models Facing  Noisy Input
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M">Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IJCNLP-AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">NLP models excel on tasks with clean inputs, but are less accurate with noisy
inputs. In particular, character-level noise such as human-written typos and
adversarially-engineered realistic-looking misspellings often appears in text
and can easily trip up NLP models. Prior solutions to address character-level
noise often alter the content of the inputs (low fidelity), thus inadvertently
lowering model accuracy on clean inputs. We proposed FiRo, an approach to boost
NLP model performance on noisy inputs without sacrificing performance on clean
inputs. FiRo sanitizes the input text while preserving its fidelity by
inferring the noise-free form for each token in the input. FiRo uses
finite-context aggregation to obtain contextual embeddings which is then used
to find the noise-free form within a restricted output space. The output space
is restricted to a small cluster of probable candidates in order to predict the
noise-free tokens more accurately. Although the clusters are small, FiRo's
effective vocabulary (union of all clusters) can be scaled up to better
preserve the input content. Experimental results show NLP models that use FiRo
outperforming baselines on six classification tasks and one sequence labeling
task at various degrees of noise.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14111" title="Abstract">arXiv:2310.14111</a> [<a href="/pdf/2310.14111" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Student-Dominant View of the Readiness to use Metaverse in Education:  The TRI-F Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garbutt%2C+M">Malcolm Garbutt</a>, 
<a href="/search/cs?searchtype=author&query=Ismail%2C+I">Ilhaam Ismail</a>, 
<a href="/search/cs?searchtype=author&query=Juries%2C+C">Calvineo Juries</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+R">Raeez Adams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This paper reports on students' readiness for using Metaverse for education
in a university in a developing country facing infrastructure and poverty
challenges. Covid-19 forced many universities to adopt a hybrid approach to
teaching and supervision. While online meeting technologies have become
commonplace, there is a lack of the connectedness of face-to-face meetings, for
which Metaverse is promoted as a solution. We pose the question as to the level
of readiness of students to use Metaverse technologies. Thematic analysis of
students' self-reflections on their experience of supervision in a 2D virtual
world revealed the usefulness of the technology readiness index model, from
which an extension to the model was proposed to include facilitators for the
application of the technology that may mediate the motivators and inhibitors
when assessing readiness to use Metaverse in education settings.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14113" title="Abstract">arXiv:2310.14113</a> [<a href="/pdf/2310.14113" title="Download PDF">pdf</a>, <a href="/format/2310.14113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sorting from Crowdsourced Comparisons using Expert Verifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vitercik%2C+E">Ellen Vitercik</a>, 
<a href="/search/cs?searchtype=author&query=Zampetakis%2C+M">Manolis Zampetakis</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">David Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We introduce a novel noisy sorting model motivated by the Just Noticeable
Difference (JND) model from experimental psychology. The goal of our model is
to capture the low quality of the data that are collected from crowdsourcing
environments. Compared to other celebrated models of noisy sorting, our model
does not rely on precise data-generation assumptions and captures crowdsourced
tasks' varying levels of difficulty that can lead to different amounts of noise
in the data. To handle this challenging task, we assume we can verify some of
the collected data using expert advice. This verification procedure is costly;
hence, we aim to minimize the number of verifications we use.
<br />We propose a new efficient algorithm called CandidateSort, which we prove
uses the optimal number of verifications in the noisy sorting models we
consider. We characterize this optimal number of verifications by showing that
it is linear in a parameter $k$, which intuitively measures the maximum number
of comparisons that are wrong but not inconsistent in the crowdsourcing data.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14114" title="Abstract">arXiv:2310.14114</a> [<a href="/pdf/2310.14114" title="Download PDF">pdf</a>, <a href="/ps/2310.14114" title="Download PostScript">ps</a>, <a href="/format/2310.14114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Note on dissecting power of regular languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rukavicka%2C+J">Josef Rukavicka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Let $c&gt;1$ be a real constant. We say that a language $L$ is
$c$-\emph{constantly growing} if for every word $u\in L$ there is a word $v\in
L$ with $\vert u\vert&lt;\vert v\vert\leq c+\vert u\vert$. We say that a language
$L$ is $c$-\emph{geometrically growing} if for every word $u\in L$ there is a
word $v\in L$ with $\vert u\vert&lt;\vert v\vert\leq c\vert u\vert$. Given a
language $L$, we say that $L$ is $REG$-\emph{dissectible} if there is a regular
language $R$ such that $\vert L\setminus R\vert=\infty$ and $\vert L\cap
R\vert=\infty$. In 2013, it was shown that every $c$-constantly growing
language $L$ is $REG$-dissectible. In 2023, the following open question has
been presented: "Is the family of geometrically growing languages
$REG$-dissectible?" We construct a $c$-geometrically growing language $L$ that
is not $REG$-dissectible. Hence we answer negatively to the open question.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14116" title="Abstract">arXiv:2310.14116</a> [<a href="/pdf/2310.14116" title="Download PDF">pdf</a>, <a href="/format/2310.14116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feasibility-Guided Safety-Aware Model Predictive Control for Jump Markov  Linear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Laouar%2C+Z">Zakariya Laouar</a>, 
<a href="/search/eess?searchtype=author&query=Mazouz%2C+R">Rayan Mazouz</a>, 
<a href="/search/eess?searchtype=author&query=Becker%2C+T">Tyler Becker</a>, 
<a href="/search/eess?searchtype=author&query=Ho%2C+Q+H">Qi Heng Ho</a>, 
<a href="/search/eess?searchtype=author&query=Sunberg%2C+Z+N">Zachary N. Sunberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we present a framework that synthesizes maximally safe control
policies for Jump Markov Linear Systems subject to stochastic mode switches.
Our approach builds on safe and robust methods for Model Predictive Control
(MPC), but in contrast to existing approaches that either optimize without
regard to feasibility or utilize soft constraints that increase computational
requirements, we employ a safe and robust control approach informed by the
feasibility of the optimization problem. When subject to inaccurate hybrid
state estimation, our feasibility-guided MPC algorithm generates a control
policy that is maximally robust to uncertainty in the system's modes.
Additionally, we formulate the notion of safety guarantees for multiple-model
receding horizon control using Control Barrier Functions (CBF) to enforce
forward invariance in safety-critical settings. We simulate our approach on a
six degree-of-freedom hexacopter under several scenarios to demonstrate the
utility of the framework. Results illustrate that the proposed technique of
maximizing the robustness horizon, and the use of CBFs for forward-invariance,
improve the overall safety and performance of Jump Markov Linear Systems.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14117" title="Abstract">arXiv:2310.14117</a> [<a href="/pdf/2310.14117" title="Download PDF">pdf</a>, <a href="/format/2310.14117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preventing Supply Chain Vulnerabilities in Java with a Fine-Grained  Permission Manager
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amusuo%2C+P+C">Paschal C. Amusuo</a> (1), 
<a href="/search/cs?searchtype=author&query=Robinson%2C+K+A">Kyle A. Robinson</a> (1), 
<a href="/search/cs?searchtype=author&query=Torres-Arias%2C+S">Santiago Torres-Arias</a> (1), 
<a href="/search/cs?searchtype=author&query=Simon%2C+L">Laurent Simon</a> (2), 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C. Davis</a> (1) ((1) Purdue University, (2) Google)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Integrating third-party packages accelerates modern software engineering, but
introduces the risk of software supply chain vulnerabilities. Vulnerabilities
in applications' dependencies are being exploited worldwide. Often, these
exploits leverage features that are present in a package, yet unneeded by an
application. Unfortunately, the current generation of permission managers, such
as SELinux, Docker containers, and the Java Security Manager, are too
coarse-grained to usefully support engineers and operators in mitigating these
vulnerabilities. Current approaches offer permissions only at the application's
granularity, lumping legitimate operations made by safe packages with
illegitimate operations made by exploited packages. This strategy does not
reflect modern engineering practice. we need a permission manager capable of
distinguishing between actions taken by different packages in an application's
supply chain.
<br />In this paper, we describe Next-JSM, the first fine-grained ("supply chain
aware") permission manager for Java applications. Next-JSM supports permission
management at package-level granularity. Next-JSM faces three key challenges:
operating on existing JVMs and without access to application or package source
code, minimizing performance overhead in applications with many packages, and
helping operators manage finer-grained permissions. We show that these
challenges can be addressed through bytecode rewriting; appropriate data
structures and algorithms; and an expressive permission notation plus automated
tooling to establish default permission. In our evaluation, we report that
Next-JSM mitigates 11 of the 12 package vulnerabilities we evaluated and incurs
an average 2.72% overhead on the Dacapobench benchmark. Qualitatively, we argue
that Next-JSM addresses the shortcomings of the (recently deprecated) Java
Security Manager (JSM).
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14119" title="Abstract">arXiv:2310.14119</a> [<a href="/pdf/2310.14119" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Aquatic Soft Robots with Elastic Instability Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zechen Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+H">Jeong Hun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lipson%2C+H">Hod Lipson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Sinusoidal undulation has long been considered the most successful swimming
pattern for fish and bionic aquatic robots [1]. However, a swimming pattern
generated by the hair clip mechanism (HCM, part iii, Figure 1A) [2]~[5] may
challenge this knowledge. HCM is an in-plane prestressed bi-stable mechanism
that stores elastic energy and releases the stored energy quickly via its
snap-through buckling. When used for fish robots, the HCM functions as the fish
body and creates unique swimming patterns that we term HCM undulation. With the
same energy consumption [3], HCM fish outperforms the traditionally designed
soft fish with a two-fold increase in cruising speed. We reproduce this
phenomenon in a single-link simulation with Aquarium [6]. HCM undulation
generates an average propulsion of 16.7 N/m, 2-3 times larger than the
reference undulation (6.78 N/m), sine pattern (5.34 N/m/s), and cambering sine
pattern (6.36 N/m), and achieves an efficiency close to the sine pattern. These
results can aid in developing fish robots and faster swimming patterns.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14120" title="Abstract">arXiv:2310.14120</a> [<a href="/pdf/2310.14120" title="Download PDF">pdf</a>, <a href="/format/2310.14120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment Analysis Across Multiple African Languages: A Current  Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aryal%2C+S+K">Saurav K. Aryal</a>, 
<a href="/search/cs?searchtype=author&query=Prioleau%2C+H">Howard Prioleau</a>, 
<a href="/search/cs?searchtype=author&query=Aryal%2C+S">Surakshya Aryal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be published as part of SIAIA @ AAAI 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AAAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sentiment analysis is a fundamental and valuable task in NLP. However, due to
limitations in data and technological availability, research into sentiment
analysis of African languages has been fragmented and lacking. With the recent
release of the AfriSenti-SemEval Shared Task 12, hosted as a part of The 17th
International Workshop on Semantic Evaluation, an annotated sentiment analysis
of 14 African languages was made available. We benchmarked and compared current
state-of-art transformer models across 12 languages and compared the
performance of training one-model-per-language versus
single-model-all-languages. We also evaluated the performance of standard
multilingual models and their ability to learn and transfer cross-lingual
representation from non-African to African languages. Our results show that
despite work in low resource modeling, more data still produces better models
on a per-language basis. Models explicitly developed for African languages
outperform other models on all tasks. Additionally, no one-model-fits-all
solution exists for a per-language evaluation of the models evaluated.
Moreover, for some languages with a smaller sample size, a larger multilingual
model may perform better than a dedicated per-language model for sentiment
classification.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14122" title="Abstract">arXiv:2310.14122</a> [<a href="/pdf/2310.14122" title="Download PDF">pdf</a>, <a href="/format/2310.14122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring  Fine-Grained Relevance Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Honglei Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+K">Kai Hui</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junru Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Le Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuanhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Berdersky%2C+M">Michael Berdersky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Zero-shot text rankers powered by recent LLMs achieve remarkable ranking
performance by simply prompting. Existing prompts for pointwise LLM rankers
mostly ask the model to choose from binary relevance labels like "Yes" and
"No". However, the lack of intermediate relevance label options may cause the
LLM to provide noisy or biased answers for documents that are partially
relevant to the query. We propose to incorporate fine-grained relevance labels
into the prompt for LLM rankers, enabling them to better differentiate among
documents with different levels of relevance to the query and thus derive a
more accurate ranking. We study two variants of the prompt template, coupled
with different numbers of relevance levels. Our experiments on 8 BEIR data sets
show that adding fine-grained relevance labels significantly improves the
performance of LLM rankers.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14124" title="Abstract">arXiv:2310.14124</a> [<a href="/pdf/2310.14124" title="Download PDF">pdf</a>, <a href="/format/2310.14124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structural generalization in COGS: Supertagging is (almost) all you need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petit%2C+A">Alban Petit</a>, 
<a href="/search/cs?searchtype=author&query=Corro%2C+C">Caio Corro</a>, 
<a href="/search/cs?searchtype=author&query=Yvon%2C+F">Fran&#xe7;ois Yvon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In many Natural Language Processing applications, neural networks have been
found to fail to generalize on out-of-distribution examples. In particular,
several recent semantic parsing datasets have put forward important limitations
of neural networks in cases where compositional generalization is required. In
this work, we extend a neural graph-based semantic parsing framework in several
ways to alleviate this issue. Notably, we propose: (1) the introduction of a
supertagging step with valency constraints, expressed as an integer linear
program; (2) a reduction of the graph prediction problem to the maximum
matching problem; (3) the design of an incremental early-stopping training
strategy to prevent overfitting. Experimentally, our approach significantly
improves results on examples that require structural generalization in the COGS
dataset, a known challenging benchmark for compositional generalization.
Overall, our results confirm that structural constraints are important for
generalization in semantic parsing.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14125" title="Abstract">arXiv:2310.14125</a> [<a href="/pdf/2310.14125" title="Download PDF">pdf</a>, <a href="/format/2310.14125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting IoT Device Provisioning Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fezeu%2C+R+A+K">Rostand A. K. Fezeu</a>, 
<a href="/search/cs?searchtype=author&query=Salo%2C+T+J">Timothy J. Salo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhi-Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">We examine in detail the provisioning process used by many common,
consumer-grade Internet of Things (IoT) devices. We find that this provisioning
process involves the IoT device, the vendor's cloud-based server, and a
vendor-provided mobile app. In order to better understand this process, we
develop two toolkits. IoT-Dissect I enables us to decrypt and examine the
messages exchanged between the IoT device and the vendor's server, and between
the vendor's server and a vendor-provided mobile app. IoT-Dissect II permits us
to reverse engineer the vendor's mobile app and observe its operation in
detail. We find several potential security issues with the provisioning process
and recommend ways to mitigate these potential problems. Further, based on
these observations, we conclude that it is likely feasible to construct a
vendor-agnostic IoT home gateway that will automate this largely manual
provisioning process, isolate IoT devices on their own network, and perhaps
open the tight association between an IoT device and the vendor's server.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14126" title="Abstract">arXiv:2310.14126</a> [<a href="/pdf/2310.14126" title="Download PDF">pdf</a>, <a href="/format/2310.14126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask To The Point: Open-Domain Entity-Centric Question Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023. Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a new task called *entity-centric question generation* (ECQG),
motivated by real-world applications such as topic-specific learning, assisted
reading, and fact-checking. The task aims to generate questions from an entity
perspective. To solve ECQG, we propose a coherent PLM-based framework GenCONE
with two novel modules: content focusing and question verification. The content
focusing module first identifies a focus as "what to ask" to form draft
questions, and the question verification module refines the questions
afterwards by verifying the answerability. We also construct a large-scale
open-domain dataset from SQuAD to support this task. Our extensive experiments
demonstrate that GenCONE significantly and consistently outperforms various
baselines, and two modules are effective and complementary in generating
high-quality questions.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14128" title="Abstract">arXiv:2310.14128</a> [<a href="/pdf/2310.14128" title="Download PDF">pdf</a>, <a href="/format/2310.14128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Smooth Sliding Control Applied to UAV Trajectory Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peixoto%2C+A+J">Alessandro Jacoud Peixoto</a>, 
<a href="/search/eess?searchtype=author&query=Serrantola%2C+W+G">Wenderson G. Serrantola</a>, 
<a href="/search/eess?searchtype=author&query=Lizarralde%2C+F">Fernando Lizarralde</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes a sliding mode controller with smooth control effort for
a class of nonlinear plants. The proposed controller is created by allowing
some constant parameters of the earlier smooth sliding control (SSC) to vary as
a function of the output tracking error, improving the control chattering
alleviation in practical implementations. Furthermore, during the sliding mode,
the new scheme can synthesize a range of controllers, such as fixed gain PI
controllers and approximations of the standard Super-Twisting Algorithm (STA),
as well as, the variable gain Super-Twisting Algorithm (VGSTA). A complete
closed-loop stability analysis is provided. In addition, realistic simulation
results with an unmanned aerial vehicle (UAV) model, incorporating aerodynamic
effects and internal closed-loop controllers, are obtained and validated via
experiments with a commercial hexacopter.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14129" title="Abstract">arXiv:2310.14129</a> [<a href="/pdf/2310.14129" title="Download PDF">pdf</a>, <a href="/format/2310.14129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Batched Best Arm Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tianyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiaokui Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 1 figure, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the batched best arm identification (BBAI) problem, where the
learner's goal is to identify the best arm while switching the policy as less
as possible. In particular, we aim to find the best arm with probability
$1-\delta$ for some small constant $\delta&gt;0$ while minimizing both the sample
complexity (total number of arm pulls) and the batch complexity (total number
of batches). We propose the three-batch best arm identification (Tri-BBAI)
algorithm, which is the first batched algorithm that achieves the optimal
sample complexity in the asymptotic setting (i.e., $\delta\rightarrow 0$) and
runs only in at most $3$ batches. Based on Tri-BBAI, we further propose the
almost optimal batched best arm identification (Opt-BBAI) algorithm, which is
the first algorithm that achieves the near-optimal sample and batch complexity
in the non-asymptotic setting (i.e., $\delta&gt;0$ is arbitrarily fixed), while
enjoying the same batch and sample complexity as Tri-BBAI when $\delta$ tends
to zero. Moreover, in the non-asymptotic setting, the complexity of previous
batch algorithms is usually conditioned on the event that the best arm is
returned (with a probability of at least $1-\delta$), which is potentially
unbounded in cases where a sub-optimal arm is returned. In contrast, the
complexity of Opt-BBAI does not rely on such an event. This is achieved through
a novel procedure that we design for checking whether the best arm is
eliminated, which is of independent interest.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14133" title="Abstract">arXiv:2310.14133</a> [<a href="/pdf/2310.14133" title="Download PDF">pdf</a>, <a href="/format/2310.14133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Quality Metric Oriented Error-bounded Lossy Compression for  Scientific Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+S">Sheng Di</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zizhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cappello%2C+F">Franck Cappello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">With the ever-increasing execution scale of high performance computing (HPC)
applications, vast amounts of data are being produced by scientific research
every day. Error-bounded lossy compression has been considered a very promising
solution to address the big-data issue for scientific applications because it
can significantly reduce the data volume with low time cost meanwhile allowing
users to control the compression errors with a specified error bound. The
existing error-bounded lossy compressors, however, are all developed based on
inflexible designs or compression pipelines, which cannot adapt to diverse
compression quality requirements/metrics favored by different application
users. In this paper, we propose a novel dynamic quality metric oriented
error-bounded lossy compression framework, namely QoZ. The detailed
contribution is three-fold. (1) We design a novel highly-parameterized
multi-level interpolation-based data predictor, which can significantly improve
the overall compression quality with the same compressed size. (2) We design
the error-bounded lossy compression framework QoZ based on the adaptive
predictor, which can auto-tune the critical parameters and optimize the
compression result according to user-specified quality metrics during online
compression. (3) We evaluate QoZ carefully by comparing its compression quality
with multiple state-of-the-arts on various real-world scientific application
datasets. Experiments show that, compared with the second-best lossy
compressor, QoZ can achieve up to 70% compression ratio improvement under the
same error bound, up to 150% compression ratio improvement under the same PSNR,
or up to 270% compression ratio improvement under the same SSIM.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14135" title="Abstract">arXiv:2310.14135</a> [<a href="/pdf/2310.14135" title="Download PDF">pdf</a>, <a href="/format/2310.14135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational Approaches for Modeling Power Consumption on an Underwater  Flapping Fin Propulsion System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Brian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Geder%2C+J">Jason Geder</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Alisha Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Julian Lee</a>, 
<a href="/search/cs?searchtype=author&query=Pruessner%2C+M">Marius Pruessner</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurti%2C+R">Ravi Ramamurti</a>, 
<a href="/search/cs?searchtype=author&query=Viswanath%2C+K">Kamal Viswanath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The last few decades have led to the rise of research focused on propulsion
and control systems for bio-inspired unmanned underwater vehicles (UUVs), which
provide more maneuverable alternatives to traditional UUVs in underwater
missions. Propulsive efficiency is of utmost importance for flapping-fin UUVs
in order to extend their range and endurance for essential operations. To
optimize for different gait performance metrics, we develop a non-dimensional
figure of merit (FOM), derived from measures of propulsive efficiency, that is
able to evaluate different fin designs and kinematics, and allow for comparison
with other bio-inspired platforms. We create and train computational models
using experimental data, and use these models to predict thrust and power under
different fin operating states, providing efficiency profiles. We then use the
developed FOM to analyze optimal gaits and compare the performance between
different fin materials. These comparisons provide a better understanding of
how fin materials affect our thrust generation and propulsive efficiency,
allowing us to inform control systems and weight for efficiency on an inverse
gait-selector model.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14137" title="Abstract">arXiv:2310.14137</a> [<a href="/pdf/2310.14137" title="Download PDF">pdf</a>, <a href="/format/2310.14137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Vulnerabilities in Mobile Application APIs: A Modular  Programmatic Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haris%2C+N">Nate Haris</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kendree Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+A">Ann Song</a>, 
<a href="/search/cs?searchtype=author&query=Pou%2C+B">Benjamin Pou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Currently, Application Programming Interfaces (APIs) are becoming
increasingly popular to facilitate data transfer in a variety of mobile
applications. These APIs often process sensitive user information through their
endpoints, which are potentially exploitable due to developer
misimplementation. In this paper, a custom, modular endpoint vulnerability
detection tool was created and implemented to present current statistics on the
degree of information leakage in various mobile Android applications. Our
endpoint vulnerability detection tool provided an automated approach to API
testing, programmatically modifying requests multiple times using specific
information attack methods (IAMs) and heuristically analyzing responses for
potentially vulnerable endpoints (PVEs). After analysis of API requests in an
encompassing range of applications, findings showed that easily exploitable
Broken Access Control (BAC) vulnerabilities of varying severity were common in
over 50% of applications. These vulnerabilities ranged from small data leakages
due to unintended API use, to full disclosure of sensitive user data, including
passwords, names, addresses, and SSNs. This investigation aims to demonstrate
the necessity of complete API endpoint security within Android applications, as
well as provide an open source example of a modular program which developers
could use to test for endpoint vulnerabilities.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14139" title="Abstract">arXiv:2310.14139</a> [<a href="/pdf/2310.14139" title="Download PDF">pdf</a>, <a href="/format/2310.14139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are LSTMs Good Few-Shot Learners?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huisman%2C+M">Mike Huisman</a>, 
<a href="/search/cs?searchtype=author&query=Moerland%2C+T+M">Thomas M. Moerland</a>, 
<a href="/search/cs?searchtype=author&query=Plaat%2C+A">Aske Plaat</a>, 
<a href="/search/cs?searchtype=author&query=van+Rijn%2C+J+N">Jan N. van Rijn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Machine Learning Journal, Special Issue of the ECML PKDD 2023 Journal Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep learning requires large amounts of data to learn new tasks well,
limiting its applicability to domains where such data is available.
Meta-learning overcomes this limitation by learning how to learn. In 2001,
Hochreiter et al. showed that an LSTM trained with backpropagation across
different tasks is capable of meta-learning. Despite promising results of this
approach on small problems, and more recently, also on reinforcement learning
problems, the approach has received little attention in the supervised few-shot
learning setting. We revisit this approach and test it on modern few-shot
learning benchmarks. We find that LSTM, surprisingly, outperform the popular
meta-learning technique MAML on a simple few-shot sine wave regression
benchmark, but that LSTM, expectedly, fall short on more complex few-shot image
classification benchmarks. We identify two potential causes and propose a new
method called Outer Product LSTM (OP-LSTM) that resolves these issues and
displays substantial performance gains over the plain LSTM. Compared to popular
meta-learning baselines, OP-LSTM yields competitive performance on
within-domain few-shot image classification, and performs better in
cross-domain settings by 0.5% to 1.9% in accuracy score. While these results
alone do not set a new state-of-the-art, the advances of OP-LSTM are orthogonal
to other advances in the field of meta-learning, yield new insights in how LSTM
work in image classification, allowing for a whole range of new research
directions. For reproducibility purposes, we publish all our research code
publicly.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14143" title="Abstract">arXiv:2310.14143</a> [<a href="/pdf/2310.14143" title="Download PDF">pdf</a>, <a href="/format/2310.14143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMTF-DES: A Fusion of Multimodal Transformer Models for Desire, Emotion,  and Sentiment Analysis of Social Media Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aziz%2C+A">Abdul Aziz</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+N+K">Nihad Karim Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+A">Muhammad Ashad Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Chy%2C+A+N">Abu Nowshed Chy</a>, 
<a href="/search/cs?searchtype=author&query=Siddique%2C+M+J">Md. Jawad Siddique</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Desire is a set of human aspirations and wishes that comprise verbal and
cognitive aspects that drive human feelings and behaviors, distinguishing
humans from other animals. Understanding human desire has the potential to be
one of the most fascinating and challenging research domains. It is tightly
coupled with sentiment analysis and emotion recognition tasks. It is beneficial
for increasing human-computer interactions, recognizing human emotional
intelligence, understanding interpersonal relationships, and making decisions.
However, understanding human desire is challenging and under-explored because
ways of eliciting desire might be different among humans. The task gets more
difficult due to the diverse cultures, countries, and languages. Prior studies
overlooked the use of image-text pairwise feature representation, which is
crucial for the task of human desire understanding. In this research, we have
proposed a unified multimodal transformer-based framework with image-text pair
settings to identify human desire, sentiment, and emotion. The core of our
proposed method lies in the encoder module, which is built using two
state-of-the-art multimodal transformer models. These models allow us to
extract diverse features. To effectively extract visual and contextualized
embedding features from social media image and text pairs, we conducted joint
fine-tuning of two pre-trained multimodal transformer models:
Vision-and-Language Transformer (ViLT) and Vision-and-Augmented-Language
Transformer (VAuLT). Subsequently, we use an early fusion strategy on these
embedding features to obtain combined diverse feature representations of the
image-text pair. This consolidation incorporates diverse information about this
task, enabling us to robustly perceive the context and image pair from multiple
perspectives.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14151" title="Abstract">arXiv:2310.14151</a> [<a href="/pdf/2310.14151" title="Download PDF">pdf</a>, <a href="/format/2310.14151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Huanran Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mosha Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Buzhou Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Biomedical language understanding benchmarks are the driving forces for
artificial intelligence applications with large language model (LLM) back-ends.
However, most current benchmarks: (a) are limited to English which makes it
challenging to replicate many of the successes in English for other languages,
or (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs
apply these knowledge to perform on a wide range of bio-medical tasks, or (c)
have become a publicly available corpus and are leaked to LLMs during
pre-training. To facilitate the research in medical LLMs, we re-build the
Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a
large scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable
test-bed and an online platform for evaluating Chinese LLMs' multi-task
capabilities on a wide range bio-medical tasks including medical entity
recognition, medical text classification, medical natural language inference,
medical dialogue understanding and medical content/dialogue generation. To
establish evaluation on these tasks, we have experimented and report the
results with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning
techniques.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14152" title="Abstract">arXiv:2310.14152</a> [<a href="/pdf/2310.14152" title="Download PDF">pdf</a>, <a href="/format/2310.14152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orthogonal Subspace Learning for Language Model Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Q">Qiming Ge</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Han Xia</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Rong Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Benefiting from massive corpora and advanced hardware, large language models
(LLMs) exhibit remarkable capabilities in language understanding and
generation. However, their performance degrades in scenarios where multiple
tasks are encountered sequentially, also known as catastrophic forgetting. In
this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and
efficient approach for continual learning in language models, effectively
mitigating catastrophic forgetting while learning new tasks. Specifically,
O-LoRA learns tasks in different (low-rank) vector subspaces that are kept
orthogonal to each other in order to minimize interference. Our method induces
only marginal additional parameter costs and requires no user data storage for
replay. Experimental results on continual learning benchmarks show that our
method outperforms state-of-the-art methods. Furthermore, compared to previous
approaches, our method excels in preserving the generalization ability of LLMs
on unseen tasks.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14154" title="Abstract">arXiv:2310.14154</a> [<a href="/pdf/2310.14154" title="Download PDF">pdf</a>, <a href="/format/2310.14154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affine-Consistent Transformer for Multi-Class Cell Nuclei Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haofeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, released code: <a href="https://github.com/lhaof/ACFormer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-class cell nuclei detection is a fundamental prerequisite in the
diagnosis of histopathology. It is critical to efficiently locate and identify
cells with diverse morphology and distributions in digital pathological images.
Most existing methods take complex intermediate representations as learning
targets and rely on inflexible post-refinements while paying less attention to
various cell density and fields of view. In this paper, we propose a novel
Affine-Consistent Transformer (AC-Former), which directly yields a sequence of
nucleus positions and is trained collaboratively through two sub-networks, a
global and a local network. The local branch learns to infer distorted input
images of smaller scales while the global network outputs the large-scale
predictions as extra supervision signals. We further introduce an Adaptive
Affine Transformer (AAT) module, which can automatically learn the key spatial
transformations to warp original images for local network training. The AAT
module works by learning to capture the transformed image regions that are more
valuable for training the model. Experimental results demonstrate that the
proposed method significantly outperforms existing state-of-the-art algorithms
on various benchmarks.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14157" title="Abstract">arXiv:2310.14157</a> [<a href="/pdf/2310.14157" title="Download PDF">pdf</a>, <a href="/format/2310.14157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Genetic Algorithms with Neural Cost Predictor for Solving Hierarchical  Vehicle Routing Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sobhanan%2C+A">Abhay Sobhanan</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+C">Changhyun Kwon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">When vehicle routing decisions are intertwined with higher-level decisions,
the resulting optimization problems pose significant challenges for
computation. Examples are the multi-depot vehicle routing problem (MDVRP),
where customers are assigned to depots before delivery, and the capacitated
location routing problem (CLRP), where the locations of depots should be
determined first. A simple and straightforward approach for such hierarchical
problems would be to separate the higher-level decisions from the complicated
vehicle routing decisions. For each higher-level decision candidate, we may
evaluate the underlying vehicle routing problems to assess the candidate. As
this approach requires solving vehicle routing problems multiple times, it has
been regarded as impractical in most cases. We propose a novel
deep-learning-based approach called Genetic Algorithm with Neural Cost
Predictor (GANCP) to tackle the challenge and simplify algorithm developments.
For each higher-level decision candidate, we predict the objective function
values of the underlying vehicle routing problems using a pre-trained graph
neural network without actually solving the routing problems. In particular,
our proposed neural network learns the objective values of the HGS-CVRP
open-source package that solves capacitated vehicle routing problems. Our
numerical experiments show that this simplified approach is effective and
efficient in generating high-quality solutions for both MDVRP and CLRP and has
the potential to expedite algorithm developments for complicated hierarchical
problems. We provide computational results evaluated in the standard benchmark
instances used in the literature.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14158" title="Abstract">arXiv:2310.14158</a> [<a href="/pdf/2310.14158" title="Download PDF">pdf</a>, <a href="/format/2310.14158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual-Attribute Prompt Learning for Progressive Mild Cognitive  Impairment Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+L">Luoyao Kang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Haifan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haofeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023, released code: <a href="https://github.com/lhaof/VAPL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning (DL) has been used in the automatic diagnosis of Mild Cognitive
Impairment (MCI) and Alzheimer's Disease (AD) with brain imaging data. However,
previous methods have not fully exploited the relation between brain image and
clinical information that is widely adopted by experts in practice. To exploit
the heterogeneous features from imaging and tabular data simultaneously, we
propose the Visual-Attribute Prompt Learning-based Transformer (VAP-Former), a
transformer-based network that efficiently extracts and fuses the multi-modal
features with prompt fine-tuning. Furthermore, we propose a Prompt fine-Tuning
(PT) scheme to transfer the knowledge from AD prediction task for progressive
MCI (pMCI) diagnosis. In details, we first pre-train the VAP-Former without
prompts on the AD diagnosis task and then fine-tune the model on the pMCI
detection task with PT, which only needs to optimize a small amount of
parameters while keeping the backbone frozen. Next, we propose a novel global
prompt token for the visual prompts to provide global guidance to the
multi-modal representations. Extensive experiments not only show the
superiority of our method compared with the state-of-the-art methods in pMCI
prediction but also demonstrate that the global prompt can make the prompt
learning process more effective and stable. Interestingly, the proposed prompt
learning model even outperforms the fully fine-tuning baseline on transferring
the knowledge from AD to pMCI.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14159" title="Abstract">arXiv:2310.14159</a> [<a href="/pdf/2310.14159" title="Download PDF">pdf</a>, <a href="/format/2310.14159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models Laugh at YouTube Short-form Videos?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+D">Dayoon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">As short-form funny videos on social networks are gaining popularity, it
becomes demanding for AI models to understand them for better communication
with humans. Unfortunately, previous video humor datasets target specific
domains, such as speeches or sitcoms, and mostly focus on verbal cues. We
curate a user-generated dataset of 10K multimodal funny videos from YouTube,
called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both
verbal and visual elements contributing to humor. After filtering, we annotate
each video with timestamps and text explanations for funny moments. Our
ExFunTube is unique over existing datasets in that our videos cover a wide
range of domains with various types of humor that necessitate a multimodal
understanding of the content. Also, we develop a zero-shot video-to-text
prompting to maximize video humor understanding of large language models
(LLMs). With three different evaluation methods using automatic scores,
rationale quality experiments, and human evaluations, we show that our
prompting significantly improves LLMs' ability for humor explanation.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14160" title="Abstract">arXiv:2310.14160</a> [<a href="/pdf/2310.14160" title="Download PDF">pdf</a>, <a href="/format/2310.14160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gap Amplification for Reconfiguration Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ohsaka%2C+N">Naoto Ohsaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, to appear in Proc. 35th Annu. ACM-SIAM Symp. Discrete Algorithms (SODA), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">In this paper, we demonstrate gap amplification for reconfiguration problems.
In particular, we prove an explicit factor of PSPACE-hardness of approximation
for three popular reconfiguration problems only assuming the Reconfiguration
Inapproximability Hypothesis (RIH) due to Ohsaka (STACS 2023). Our main result
is that under RIH, Maxmin Binary CSP Reconfiguration is PSPACE-hard to
approximate within a factor of $0.9942$. Moreover, the same result holds even
if the constraint graph is restricted to $(d,\lambda)$-expander for arbitrarily
small $\frac{\lambda}{d}$. The crux of its proof is an alteration of the gap
amplification technique due to Dinur (J. ACM, 2007), which amplifies the $1$
vs. $1-\epsilon$ gap for arbitrarily small $\epsilon &gt; 0$ up to the $1$ vs.
$1-0.0058$ gap. As an application of the main result, we demonstrate that
Minmax Set Cover Reconfiguration and Minmax Dominating Set Reconfiguratio} are
PSPACE-hard to approximate within a factor of $1.0029$ under RIH. Our proof is
based on a gap-preserving reduction from Label Cover to Set Cover due to Lund
and Yannakakis (J. ACM, 1994). However, unlike Lund--Yannakakis' reduction, the
expander mixing lemma is essential to use. We highlight that all results hold
unconditionally as long as "PSPACE-hard" is replaced by "NP-hard," and are the
first explicit inapproximability results for reconfiguration problems without
resorting to the parallel repetition theorem. We finally complement the main
result by showing that it is NP-hard to approximate Maxmin Binary CSP
Reconfiguration within a factor better than $\frac{3}{4}$.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14161" title="Abstract">arXiv:2310.14161</a> [<a href="/pdf/2310.14161" title="Download PDF">pdf</a>, <a href="/format/2310.14161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promoting Generalization for Exact Solvers via Adversarial Instance  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Y">Yufei Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Feng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning has been successfully applied to improve the efficiency of
Mixed-Integer Linear Programming (MILP) solvers. However, the learning-based
solvers often suffer from severe performance degradation on unseen MILP
instances -- especially on large-scale instances from a perturbed environment
-- due to the limited diversity of training distributions. To tackle this
problem, we propose a novel approach, which is called Adversarial Instance
Augmentation and does not require to know the problem type for new instance
generation, to promote data diversity for learning-based branching modules in
the branch-and-bound (B&amp;B) Solvers (AdaSolver). We use the bipartite graph
representations for MILP instances and obtain various perturbed instances to
regularize the solver by augmenting the graph structures with a learned
augmentation policy. The major technical contribution of AdaSolver is that we
formulate the non-differentiable instance augmentation as a contextual bandit
problem and adversarially train the learning-based solver and augmentation
policy, enabling efficient gradient-based training of the augmentation policy.
To the best of our knowledge, AdaSolver is the first general and effective
framework for understanding and improving the generalization of both
imitation-learning-based (IL-based) and reinforcement-learning-based (RL-based)
B&amp;B solvers. Extensive experiments demonstrate that by producing various
augmented instances, AdaSolver leads to a remarkable efficiency improvement
across various distributions.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14162" title="Abstract">arXiv:2310.14162</a> [<a href="/pdf/2310.14162" title="Download PDF">pdf</a>, <a href="/format/2310.14162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting End-to-End Steering Angle Prediction with CAN Bus Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rohan Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, end to end steering prediction for autonomous vehicles has
become a major area of research. The primary method for achieving end to end
steering was to use computer vision models on a live feed of video data.
However, to further increase accuracy, many companies have added data from
light detection and ranging (LiDAR) and or radar sensors through sensor fusion.
However, the addition of lasers and sensors comes at a high financial cost. In
this paper, I address both of these issues by increasing the accuracy of the
computer vision models without the increased cost of using LiDAR and or
sensors. I achieved this by improving the accuracy of computer vision models by
sensor fusing CAN bus data, a vehicle protocol, with video data. CAN bus data
is a rich source of information about the vehicle's state, including its speed,
steering angle, and acceleration. By fusing this data with video data, the
accuracy of the computer vision model's predictions can be improved. When I
trained the model without CAN bus data, I obtained an RMSE of 0.02492, while
the model trained with the CAN bus data achieved an RMSE of 0.01970. This
finding indicates that fusing CAN Bus data with video data can reduce the
computer vision model's prediction error by 20% with some models decreasing the
error by 80%.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14163" title="Abstract">arXiv:2310.14163</a> [<a href="/pdf/2310.14163" title="Download PDF">pdf</a>, <a href="/format/2310.14163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FGO-ILNS: Tightly Coupled Multi-Sensor Integrated Navigation System  Based on Factor Graph Optimization for Autonomous Underwater Vehicle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiangbo Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wanqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruofan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiangwei Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Multi-sensor fusion is an effective way to enhance the positioning
performance of autonomous underwater vehicles (AUVs). However, underwater
multi-sensor fusion faces challenges such as heterogeneous frequency and
dynamic availability of sensors. Traditional filter-based algorithms suffer
from low accuracy and robustness when sensors become unavailable. The factor
graph optimization (FGO) can enable multi-sensor plug-and-play despite data
frequency. Therefore, we present an FGO-based strapdown inertial navigation
system (SINS) and long baseline location (LBL) system tightly coupled
navigation system (FGO-ILNS). Sensors such as Doppler velocity log (DVL),
magnetic compass pilot (MCP), pressure sensor (PS), and global navigation
satellite system (GNSS) can be tightly coupled with FGO-ILNS to satisfy
different navigation scenarios. In this system, we propose a floating LBL slant
range difference factor model tightly coupled with IMU preintegration factor to
achieve unification of global position above and below water. Furthermore, to
address the issue of sensor measurements not being synchronized with the LBL
during fusion, we employ forward-backward IMU preintegration to construct
sensor factors such as GNSS and DVL. Moreover, we utilize the marginalization
method to reduce the computational load of factor graph optimization.
Simulation and public KAIST dataset experiments have verified that, compared to
filter-based algorithms like the extended Kalman filter and federal Kalman
filter, as well as the state-of-the-art optimization-based algorithm ORB-SLAM3,
our proposed FGO-ILNS leads in accuracy and robustness.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14164" title="Abstract">arXiv:2310.14164</a> [<a href="/pdf/2310.14164" title="Download PDF">pdf</a>, <a href="/format/2310.14164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3b1;$-Fair Contextual Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+S">Siddhant Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Abhishek Sinha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Contextual bandit algorithms are at the core of many applications, including
recommender systems, clinical trials, and optimal portfolio selection. One of
the most popular problems studied in the contextual bandit literature is to
maximize the sum of the rewards in each round by ensuring a sublinear regret
against the best-fixed context-dependent policy. However, in many applications,
the cumulative reward is not the right objective - the bandit algorithm must be
fair in order to avoid the echo-chamber effect and comply with the regulatory
requirements. In this paper, we consider the $\alpha$-Fair Contextual Bandits
problem, where the objective is to maximize the global $\alpha$-fair utility
function - a non-decreasing concave function of the cumulative rewards in the
adversarial setting. The problem is challenging due to the non-separability of
the objective across rounds. We design an efficient algorithm that guarantees
an approximately sublinear regret in the full-information and bandit feedback
settings.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14165" title="Abstract">arXiv:2310.14165</a> [<a href="/pdf/2310.14165" title="Download PDF">pdf</a>, <a href="/format/2310.14165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Convolutional Network with Connectivity Uncertainty for EEG-based  Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongxiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiangyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhipeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lulu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengyu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Automatic emotion recognition based on multichannel Electroencephalography
(EEG) holds great potential in advancing human-computer interaction. However,
several significant challenges persist in existing research on algorithmic
emotion recognition. These challenges include the need for a robust model to
effectively learn discriminative node attributes over long paths, the
exploration of ambiguous topological information in EEG channels and effective
frequency bands, and the mapping between intrinsic data qualities and provided
labels. To address these challenges, this study introduces the
distribution-based uncertainty method to represent spatial dependencies and
temporal-spectral relativeness in EEG signals based on Graph Convolutional
Network (GCN) architecture that adaptively assigns weights to functional
aggregate node features, enabling effective long-path capturing while
mitigating over-smoothing phenomena. Moreover, the graph mixup technique is
employed to enhance latent connected edges and mitigate noisy label issues.
Furthermore, we integrate the uncertainty learning method with deep GCN weights
in a one-way learning fashion, termed Connectivity Uncertainty GCN (CU-GCN). We
evaluate our approach on two widely used datasets, namely SEED and SEEDIV, for
emotion recognition tasks. The experimental results demonstrate the superiority
of our methodology over previous methods, yielding positive and significant
improvements. Ablation studies confirm the substantial contributions of each
component to the overall performance.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14166" title="Abstract">arXiv:2310.14166</a> [<a href="/pdf/2310.14166" title="Download PDF">pdf</a>, <a href="/format/2310.14166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble Learning for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+Z+H">Zhen Hao Wong</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+L">Ling Yue</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Q">Quanming Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have shown success in various fields for
learning from graph-structured data. This paper investigates the application of
ensemble learning techniques to improve the performance and robustness of Graph
Neural Networks (GNNs). By training multiple GNN models with diverse
initializations or architectures, we create an ensemble model named ELGNN that
captures various aspects of the data and uses the Tree-Structured Parzen
Estimator algorithm to determine the ensemble weights. Combining the
predictions of these models enhances overall accuracy, reduces bias and
variance, and mitigates the impact of noisy data. Our findings demonstrate the
efficacy of ensemble learning in enhancing GNN capabilities for analyzing
complex graph-structured data. The code is public at
https://github.com/wongzhenhao/ELGNN.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14167" title="Abstract">arXiv:2310.14167</a> [<a href="/pdf/2310.14167" title="Download PDF">pdf</a>, <a href="/format/2310.14167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factor Graph Processing for Dual-Blind Deconvolution at ISAC Receiver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacome%2C+R">Roman Jacome</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+E">Edwin Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+K+V">Kumar Vijay Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Sadler%2C+B+M">Brian M. Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Arguello%2C+H">Henry Arguello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Integrated sensing and communications (ISAC) systems have gained significant
interest because of their ability to jointly and efficiently access, utilize,
and manage the scarce electromagnetic spectrum. The co-existence approach
toward ISAC focuses on the receiver processing of overlaid radar and
communications signals coming from independent transmitters. A specific ISAC
coexistence problem is dual-blind deconvolution (DBD), wherein the transmit
signals and channels of both radar and communications are unknown to the
receiver. Prior DBD works ignore the evolution of the signal model over time.
In this work, we consider a dynamic DBD scenario using a linear state space
model (LSSM) such that, apart from the transmit signals and channels of both
systems, the LSSM parameters are also unknown. We employ a factor graph
representation to model these unknown variables. We avoid the conventional
matrix inversion approach to estimate the unknown variables by using an
efficient expectation-maximization algorithm, where each iteration employs a
Gaussian message passing over the factor graph structure. Numerical experiments
demonstrate the accurate estimation of radar and communications channels,
including in the presence of noise.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14170" title="Abstract">arXiv:2310.14170</a> [<a href="/pdf/2310.14170" title="Download PDF">pdf</a>, <a href="/format/2310.14170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Invariant Molecular Representation in Latent Discrete Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+X">Xiang Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Keyan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jingsong Lv</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Molecular representation learning lays the foundation for drug discovery.
However, existing methods suffer from poor out-of-distribution (OOD)
generalization, particularly when data for training and testing originate from
different environments. To address this issue, we propose a new framework for
learning molecular representations that exhibit invariance and robustness
against distribution shifts. Specifically, we propose a strategy called
``first-encoding-then-separation'' to identify invariant molecule features in
the latent space, which deviates from conventional practices. Prior to the
separation step, we introduce a residual vector quantization module that
mitigates the over-fitting to training data distributions while preserving the
expressivity of encoders. Furthermore, we design a task-agnostic
self-supervised learning objective to encourage precise invariance
identification, which enables our method widely applicable to a variety of
tasks, such as regression and multi-label classification. Extensive experiments
on 18 real-world molecular datasets demonstrate that our model achieves
stronger generalization against state-of-the-art baselines in the presence of
various distribution shifts. Our code is available at
https://github.com/HICAI-ZJU/iMoLD.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14171" title="Abstract">arXiv:2310.14171</a> [<a href="/pdf/2310.14171" title="Download PDF">pdf</a>, <a href="/format/2310.14171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable Data Transmission through Private CBRS Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuo%2C+H">Hsun-Yu Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Szu-Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chin-Ya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu-Chi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+M">Meng-Hua Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">We consider the use of a domain proxy assisted private citizen broadband
radio service (CBRS) network and propose a Maximum Transmission Continuity
(MTC) scheme to transmit Internet of Things (IoT) data reliably. MTC
dynamically allocates available CBRS channels to sustain the continuity of data
transmission without violating the channel access requirements. MTC allocates
the granted CBRS channels according to the priority of each user, the instant
channel access status, interference among users, and the fairness. The
simulation results demonstrate the improvement in managing reliable IoT data
transmission in the private CBRS network.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14173" title="Abstract">arXiv:2310.14173</a> [<a href="/pdf/2310.14173" title="Download PDF">pdf</a>, <a href="/format/2310.14173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> First-Shot Unsupervised Anomalous Sound Detection With Unknown Anomalies  Estimated by Metadata-Assisted Audio Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hejing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiaoxi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+J">Jian Guan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haohe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+F">Feiyang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jiantong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+X">Xinhao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xubo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenwu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">First-shot (FS) unsupervised anomalous sound detection (ASD) is a brand-new
task introduced in DCASE 2023 Challenge Task 2, where the anomalous sounds for
the target machine types are unseen in training. Existing methods often rely on
the availability of normal and abnormal sound data from the target machines.
However, due to the lack of anomalous sound data for the target machine types,
it becomes challenging when adapting the existing ASD methods to the first-shot
task. In this paper, we propose a new framework for the first-shot unsupervised
ASD, where metadata-assisted audio generation is used to estimate unknown
anomalies, by utilising the available machine information (i.e., metadata and
sound data) to fine-tune a text-to-audio generation model for generating the
anomalous sounds that contain unique acoustic characteristics accounting for
each different machine types. We then use the method of Time-Weighted Frequency
domain audio Representation with Gaussian Mixture Model (TWFR-GMM) as the
backbone to achieve the first-shot unsupervised ASD. Our proposed FS-TWFR-GMM
method achieves competitive performance amongst top systems in DCASE 2023
Challenge Task 2, while requiring only 1% model parameters for detection, as
validated in our experiments.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14174" title="Abstract">arXiv:2310.14174</a> [<a href="/pdf/2310.14174" title="Download PDF">pdf</a>, <a href="/format/2310.14174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-Context Schema Understanding Method for Knowledge Base Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yantao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaolong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Long Bai</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+S">Saiping Guan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The Knowledge Base Question Answering (KBQA) task aims to answer natural
language questions based on a given knowledge base. As a kind of common method
for this task, semantic parsing-based ones first convert natural language
questions to logical forms (e.g., SPARQL queries) and then execute them on
knowledge bases to get answers. Recently, Large Language Models (LLMs) have
shown strong abilities in language understanding and may be adopted as semantic
parsers in such kinds of methods. However, in doing so, a great challenge for
LLMs is to understand the schema of knowledge bases. Therefore, in this paper,
we propose an In-Context Schema Understanding (ICSU) method for facilitating
LLMs to be used as a semantic parser in KBQA. Specifically, ICSU adopts the
In-context Learning mechanism to instruct LLMs to generate SPARQL queries with
examples. In order to retrieve appropriate examples from annotated
question-query pairs, which contain comprehensive schema information related to
questions, ICSU explores four different retrieval strategies. Experimental
results on the largest KBQA benchmark, KQA Pro, show that ICSU with all these
strategies outperforms that with a random retrieval strategy significantly
(from 12\% to 78.76\% in accuracy).
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14175" title="Abstract">arXiv:2310.14175</a> [<a href="/pdf/2310.14175" title="Download PDF">pdf</a>, <a href="/format/2310.14175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A semi-randomized and augmented Kaczmarz method with simple random  sampling for large-scale inconsistent linear systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+S">Shunchang Li</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+G">Gang Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A greedy randomized augmented Kaczmarz (GRAK) method was proposed in [Z.-Z.
Bai and W.-T. WU, SIAM J. Sci. Comput., 43 (2021), pp. A3892-A3911] for large
and sparse inconsistent linear systems. However, one has to construct two new
index sets via computing residual vector with respect to the augmented linear
system in each iteration. Thus, the computational overhead of this method is
large for extremely large-scale problems. Moreover, there is no reliable
stopping criterion for this method. In this work, we are interested in solving
large-scale sparse or dense inconsistent linear systems, and try to enhance the
numerical performance of the GRAK method. First, we propose an accelerated
greedy randomized augmented Kaczmarz method. Theoretical analysis indicates
that it converges faster than the GRAK method under very weak assumptions.
Second, in order to further release the overhead, we propose a semi-randomized
augmented Kaczmarz method with simple random sampling. Third, to the best of
our knowledge, there are no practical stopping criteria for all the randomized
Kaczmarz-type methods till now. To fill-in this gap, we introduce a practical
stopping criterion for Kaczmarz-type methods, and show its rationality from a
theoretical point of view. Numerical experiments are performed on both
real-world and synthetic data sets, which demonstrate the efficiency of the
proposed methods and the effectiveness of our stopping criterion.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14176" title="Abstract">arXiv:2310.14176</a> [<a href="/pdf/2310.14176" title="Download PDF">pdf</a>, <a href="/format/2310.14176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-based Grouping Transformer for Nucleus Detection and  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haofeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weijun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023, released code: <a href="https://github.com/lhaof/PGT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Automatic nuclei detection and classification can produce effective
information for disease diagnosis. Most existing methods classify nuclei
independently or do not make full use of the semantic similarity between nuclei
and their grouping features. In this paper, we propose a novel end-to-end
nuclei detection and classification framework based on a grouping
transformer-based classifier. The nuclei classifier learns and updates the
representations of nuclei groups and categories via hierarchically grouping the
nucleus embeddings. Then the cell types are predicted with the pairwise
correlations between categorical embeddings and nucleus features. For the
efficiency of the fully transformer-based framework, we take the nucleus group
embeddings as the input prompts of backbone, which helps harvest grouping
guided features by tuning only the prompts instead of the whole backbone.
Experimental results show that the proposed method significantly outperforms
the existing models on three datasets.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14184" title="Abstract">arXiv:2310.14184</a> [<a href="/pdf/2310.14184" title="Download PDF">pdf</a>, <a href="/format/2310.14184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partition Speeds Up Learning Implicit Neural Representations Based on  Exponential-Increase Hypothesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Ke Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haishuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Ning Ma</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+J">Jiajun Bu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">$\textit{Implicit neural representations}$ (INRs) aim to learn a
$\textit{continuous function}$ (i.e., a neural network) to represent an image,
where the input and output of the function are pixel coordinates and RGB/Gray
values, respectively. However, images tend to consist of many objects whose
colors are not perfectly consistent, resulting in the challenge that image is
actually a $\textit{discontinuous piecewise function}$ and cannot be well
estimated by a continuous function. In this paper, we empirically investigate
that if a neural network is enforced to fit a discontinuous piecewise function
to reach a fixed small error, the time costs will increase exponentially with
respect to the boundaries in the spatial domain of the target signal. We name
this phenomenon the $\textit{exponential-increase}$ hypothesis. Under the
$\textit{exponential-increase}$ hypothesis, learning INRs for images with many
objects will converge very slowly. To address this issue, we first prove that
partitioning a complex signal into several sub-regions and utilizing piecewise
INRs to fit that signal can significantly speed up the convergence. Based on
this fact, we introduce a simple partition mechanism to boost the performance
of two INR methods for image reconstruction: one for learning INRs, and the
other for learning-to-learn INRs. In both cases, we partition an image into
different sub-regions and dedicate smaller networks for each part. In addition,
we further propose two partition rules based on regular grids and semantic
segmentation maps, respectively. Extensive experiments validate the
effectiveness of the proposed partitioning methods in terms of learning INR for
a single image (ordinary learning framework) and the learning-to-learn
framework.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14185" title="Abstract">arXiv:2310.14185</a> [<a href="/pdf/2310.14185" title="Download PDF">pdf</a>, <a href="/format/2310.14185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Space Complexity of Generating Tent Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okada%2C+N">Naoaki Okada</a>, 
<a href="/search/cs?searchtype=author&query=Kijima%2C+S">Shuji Kijima</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">This paper is motivated by a question whether it is possible to calculate a
chaotic sequence efficiently, e.g., is it possible to get the $n$-th bit of a
bit sequence generated by a chaotic map, such as $\beta$-expansion, tent map
and logistic map in $o(n)$ time/space? This paper gives an affirmative answer
to the question about the space complexity of a tent map. We prove that a tent
code of $n$-bits with an initial condition uniformly at random is exactly
generated in $O(\log^2 n)$ space in expectation.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14189" title="Abstract">arXiv:2310.14189</a> [<a href="/pdf/2310.14189" title="Download PDF">pdf</a>, <a href="/format/2310.14189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Techniques for Training Consistency Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=Dhariwal%2C+P">Prafulla Dhariwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Consistency models are a nascent family of generative models that can sample
high quality data in one step without the need for adversarial training.
Current consistency models achieve optimal sample quality by distilling from
pre-trained diffusion models and employing learned metrics such as LPIPS.
However, distillation limits the quality of consistency models to that of the
pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation.
To tackle these challenges, we present improved techniques for consistency
training, where consistency models learn directly from data without
distillation. We delve into the theory behind consistency training and identify
a previously overlooked flaw, which we address by eliminating Exponential
Moving Average from the teacher consistency model. To replace learned metrics
like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally,
we introduce a lognormal noise schedule for the consistency training objective,
and propose to double total discretization steps every set number of training
iterations. Combined with better hyperparameter tuning, these modifications
enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10
and ImageNet $64\times 64$ respectively in a single sampling step. These scores
mark a 3.5$\times$ and 4$\times$ improvement compared to prior consistency
training approaches. Through two-step sampling, we further reduce FID scores to
2.24 and 2.77 on these two datasets, surpassing those obtained via distillation
in both one-step and two-step settings, while narrowing the gap between
consistency models and other state-of-the-art generative models.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14192" title="Abstract">arXiv:2310.14192</a> [<a href="/pdf/2310.14192" title="Download PDF">pdf</a>, <a href="/format/2310.14192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptMix: A Class Boundary Augmentation Method for Large Language Model  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+G">Gaurav Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Vechtomova%2C+O">Olga Vechtomova</a>, 
<a href="/search/cs?searchtype=author&query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
<a href="/search/cs?searchtype=author&query=Laradji%2C+I+H">Issam H. Laradji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data augmentation is a widely used technique to address the problem of text
classification when there is a limited amount of training data. Recent work
often tackles this problem using large language models (LLMs) like GPT3 that
can generate new examples given already available ones. In this work, we
propose a method to generate more helpful augmented data by utilizing the LLM's
abilities to follow instructions and perform few-shot classifications. Our
specific PromptMix method consists of two steps: 1) generate challenging text
augmentations near class boundaries; however, generating borderline examples
increases the risk of false positives in the dataset, so we 2) relabel the text
augmentations using a prompting-based LLM classifier to enhance the correctness
of labels in the generated data. We evaluate the proposed method in challenging
2-shot and zero-shot settings on four text classification datasets: Banking77,
TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that
generating and, crucially, relabeling borderline examples facilitates the
transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and
cheaper classifiers like DistilBERT$_{base}$ and BERT$_{base}$. Furthermore,
2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the
four datasets. Our code is available at
https://github.com/ServiceNow/PromptMix-EMNLP-2023.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14194" title="Abstract">arXiv:2310.14194</a> [<a href="/pdf/2310.14194" title="Download PDF">pdf</a>, <a href="/format/2310.14194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distractor-aware Event-based Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yingkai Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baocai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiaopeng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Event cameras, or dynamic vision sensors, have recently achieved success from
fundamental vision tasks to high-level vision researches. Due to its ability to
asynchronously capture light intensity changes, event camera has an inherent
advantage to capture moving objects in challenging scenarios including objects
under low light, high dynamic range, or fast moving objects. Thus event camera
are natural for visual object tracking. However, the current event-based
trackers derived from RGB trackers simply modify the input images to event
frames and still follow conventional tracking pipeline that mainly focus on
object texture for target distinction. As a result, the trackers may not be
robust dealing with challenging scenarios such as moving cameras and cluttered
foreground. In this paper, we propose a distractor-aware event-based tracker
that introduces transformer modules into Siamese network architecture (named
DANet). Specifically, our model is mainly composed of a motion-aware network
and a target-aware network, which simultaneously exploits both motion cues and
object contours from event data, so as to discover motion objects and identify
the target object by removing dynamic distractors. Our DANet can be trained in
an end-to-end manner without any post-processing and can run at over 80 FPS on
a single V100. We conduct comprehensive experiments on two large event tracking
datasets to validate the proposed model. We demonstrate that our tracker has
superior performance against the state-of-the-art trackers in terms of both
accuracy and efficiency.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14196" title="Abstract">arXiv:2310.14196</a> [<a href="/pdf/2310.14196" title="Download PDF">pdf</a>, <a href="/format/2310.14196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Discern: Imitating Heterogeneous Human Demonstrations with  Preference and Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuhar%2C+S">Sachit Kuhar</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shuo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Shivang Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Bronars%2C+M">Matthew Bronars</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Danfei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at the 7th Annual Conference on Robot Learning (CoRL) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Practical Imitation Learning (IL) systems rely on large human demonstration
datasets for successful policy learning. However, challenges lie in maintaining
the quality of collected data and addressing the suboptimal nature of some
demonstrations, which can compromise the overall dataset quality and hence the
learning outcome. Furthermore, the intrinsic heterogeneity in human behavior
can produce equally successful but disparate demonstrations, further
exacerbating the challenge of discerning demonstration quality. To address
these challenges, this paper introduces Learning to Discern (L2D), an offline
imitation learning framework for learning from demonstrations with diverse
quality and style. Given a small batch of demonstrations with sparse quality
labels, we learn a latent representation for temporally embedded trajectory
segments. Preference learning in this latent space trains a quality evaluator
that generalizes to new demonstrators exhibiting different styles. Empirically,
we show that L2D can effectively assess and learn from varying demonstrations,
thereby leading to improved policy performance across a range of tasks in both
simulations and on a physical robot.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14198" title="Abstract">arXiv:2310.14198</a> [<a href="/pdf/2310.14198" title="Download PDF">pdf</a>, <a href="/format/2310.14198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QA-NatVer: Question Answering for Natural Logic-based Fact Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aly%2C+R">Rami Aly</a>, 
<a href="/search/cs?searchtype=author&query=Strong%2C+M">Marek Strong</a>, 
<a href="/search/cs?searchtype=author&query=Vlachos%2C+A">Andreas Vlachos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Fact verification systems assess a claim's veracity based on evidence. An
important consideration in designing them is faithfulness, i.e. generating
explanations that accurately reflect the reasoning of the model. Recent works
have focused on natural logic, which operates directly on natural language by
capturing the semantic relation of spans between an aligned claim with its
evidence via set-theoretic operators. However, these approaches rely on
substantial resources for training, which are only available for high-resource
languages. To this end, we propose to use question answering to predict natural
logic operators, taking advantage of the generalization capabilities of
instruction-tuned language models. Thus, we obviate the need for annotated
training data while still relying on a deterministic inference system. In a
few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$
accuracy points, including a state-of-the-art pre-trained seq2seq natural logic
system, as well as a state-of-the-art prompt-based classifier. Our system
demonstrates its robustness and portability, achieving competitive performance
on a counterfactual dataset and surpassing all approaches without further
annotation on a Danish verification dataset. A human evaluation indicates that
our approach produces more plausible proofs with fewer erroneous natural logic
operators than previous natural logic-based systems.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14199" title="Abstract">arXiv:2310.14199</a> [<a href="/pdf/2310.14199" title="Download PDF">pdf</a>, <a href="/ps/2310.14199" title="Download PostScript">ps</a>, <a href="/format/2310.14199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partially Explicit Generalized Multiscale Method for Poroelasticity  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Su%2C+X">Xin Su</a>, 
<a href="/search/math?searchtype=author&query=Leung%2C+W+T">Wing Tat Leung</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wenyuan Li</a>, 
<a href="/search/math?searchtype=author&query=Pun%2C+S">Sai-Mang Pun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 Pages,61 figures. arXiv admin note: text overlap with <a href="/abs/2208.05542">arXiv:2208.05542</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We develop a partially explicit time discretization based on the framework of
constraint energy minimizing generalized multiscale finite element method
(CEM-GMsFEM) for the problem of linear poroelasticity with high contrast.
Firstly, dominant basis functions generated by the CEM-GMsFEM approach are used
to capture important degrees of freedom and it is known to give
contrast-independent convergence that scales with the mesh size. In typical
situation, one has very few degrees of freedom in dominant basis functions.
This part is treated implicitly. Secondly, we design and introduce an
additional space in the complement space and these degrees are treated
explicitly. We also investigate the CFL-type stability restriction for this
problem, and the restriction for the time step is contrast independent.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14200" title="Abstract">arXiv:2310.14200</a> [<a href="/pdf/2310.14200" title="Download PDF">pdf</a>, <a href="/ps/2310.14200" title="Download PostScript">ps</a>, <a href="/format/2310.14200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Resource Management in CDRT Systems through Adaptive NOMA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Hongjiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingxu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Ki-Hong Park</a>, 
<a href="/search/cs?searchtype=author&query=Saeed%2C+N">Nasir Saeed</a>, 
<a href="/search/cs?searchtype=author&query=She%2C+X">Xusheng She</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jianling Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, submitted to IEEE journal for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper introduces a novel adaptive transmission scheme to amplify the
prowess of coordinated direct and relay transmission (CDRT) systems rooted in
non-orthogonal multiple access principles. Leveraging the maximum ratio
transmission scheme, we seamlessly meet the prerequisites of CDRT while
harnessing the potential of dynamic power allocation and directional antennas
to elevate the system's operational efficiency. Through meticulous derivations,
we unveil closed-form expressions depicting the exact effective sum throughput.
Our simulation results adeptly validate the theoretical analysis and vividly
showcase the effectiveness of the proposed scheme.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14201" title="Abstract">arXiv:2310.14201</a> [<a href="/pdf/2310.14201" title="Download PDF">pdf</a>, <a href="/format/2310.14201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Engineering Through the Lens of Optimal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yifan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chengfeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhennan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bin Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Prompt Engineering (PE) has emerged as a critical technique for guiding Large
Language Models (LLMs) in solving intricate tasks. Its importance is
highlighted by its potential to significantly enhance the efficiency and
effectiveness of human-machine interaction. As tasks grow increasingly complex,
recent advanced PE methods have extended beyond the limitations of single-round
interactions to embrace multi-round interactions, which allows for a deeper and
more nuanced engagement with LLMs. In this paper, we propose an optimal control
framework tailored for multi-round interactions with LLMs. This framework
provides a unified mathematical structure that not only systematizes the
existing PE methods but also sets the stage for rigorous analytical
improvements. Furthermore, we extend this framework to include PE via ensemble
methods and multi-agent collaboration, thereby enlarging the scope of
applicability. By adopting an optimal control perspective, we offer fresh
insights into existing PE methods and highlight theoretical challenges that
warrant future research. Besides, our work lays a foundation for the
development of more effective and interpretable PE methods.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14206" title="Abstract">arXiv:2310.14206</a> [<a href="/pdf/2310.14206" title="Download PDF">pdf</a>, <a href="/format/2310.14206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manifold-Preserving Transformers are Effective for Short-Long Range  Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+A">Ayan Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M+S">Md Shad Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, 5 tables, Findings of the Association for Computational Linguistics: EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multi-head self-attention-based Transformers have shown promise in different
learning tasks. Albeit these models exhibit significant improvement in
understanding short-term and long-term contexts from sequences, encoders of
Transformers and their variants fail to preserve layer-wise contextual
information. Transformers usually project tokens onto sparse manifolds and fail
to preserve mathematical equivalence among the token representations. In this
work, we propose TransJect, an encoder model that guarantees a theoretical
bound for layer-wise distance preservation between a pair of tokens. We propose
a simple alternative to dot-product attention to ensure Lipschitz continuity.
This allows TransJect to learn injective mappings to transform token
representations to different manifolds with similar topology and preserve
Euclidean distance between every pair of tokens in subsequent layers.
Evaluations across multiple benchmark short- and long-sequence classification
tasks show maximum improvements of 6.8% and 5.9%, respectively, over the
variants of Transformers. Additionally, TransJect displays 79% better
performance than Transformer on the language modeling task. We further
highlight the shortcomings of multi-head self-attention from the statistical
physics viewpoint. Although multi-head self-attention was incepted to learn
different abstraction levels within the networks, our empirical analyses
suggest that different attention heads learn randomly and unorderly. In
contrast, TransJect adapts a mixture of experts for regularization; these
experts are more orderly and balanced and learn different sparse
representations from the input sequences. TransJect exhibits very low entropy
and can be efficiently scaled to larger depths.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14209" title="Abstract">arXiv:2310.14209</a> [<a href="/pdf/2310.14209" title="Download PDF">pdf</a>, <a href="/format/2310.14209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SUT: Active Defects Probing for Transcompiler Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+M">Mengnan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Maoquan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yongqiang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Clement%2C+C">Colin Clement</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+N">Neel Sundaresan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic Program translation has enormous application value and hence has
been attracting significant interest from AI researchers. However, we observe
that current program translation models still make elementary syntax errors,
particularly, when the target language does not have syntax elements in the
source language. Metrics like BLUE, CodeBLUE and computation accuracy may not
expose these issues. In this paper we introduce a new metrics for programming
language translation and these metrics address these basic syntax errors. We
develop a novel active defects probing suite called Syntactic Unit Tests (SUT)
which includes a highly interpretable evaluation harness for accuracy and test
scoring. Experiments have shown that even powerful models like ChatGPT still
make mistakes on these basic unit tests. Specifically, compared to previous
program translation task evaluation dataset, its pass rate on our unit tests
has decreased by 26.15%. Further our evaluation harness reveal syntactic
element errors in which these models exhibit deficiencies.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14211" title="Abstract">arXiv:2310.14211</a> [<a href="/pdf/2310.14211" title="Download PDF">pdf</a>, <a href="/format/2310.14211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LUNA: A Model-Based Universal Analysis Framework for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Da Song</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiayang Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Derui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Juefei-Xu%2C+F">Felix Juefei-Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Software Engineering (cs.SE)

</div>
<p class="mathjax">Over the past decade, Artificial Intelligence (AI) has had great success
recently and is being used in a wide range of academic and industrial fields.
More recently, LLMs have made rapid advancements that have propelled AI to a
new level, enabling even more diverse applications and industrial domains with
intelligence, particularly in areas like software engineering and natural
language processing. Nevertheless, a number of emerging trustworthiness
concerns and issues exhibited in LLMs have already recently received much
attention, without properly solving which the widespread adoption of LLMs could
be greatly hindered in practice. The distinctive characteristics of LLMs, such
as the self-attention mechanism, extremely large model scale, and
autoregressive generation schema, differ from classic AI software based on CNNs
and RNNs and present new challenges for quality analysis. Up to the present, it
still lacks universal and systematic analysis techniques for LLMs despite the
urgent industrial demand. Towards bridging this gap, we initiate an early
exploratory study and propose a universal analysis framework for LLMs, LUNA,
designed to be general and extensible, to enable versatile analysis of LLMs
from multiple quality perspectives in a human-interpretable manner. In
particular, we first leverage the data from desired trustworthiness
perspectives to construct an abstract model as an auxiliary analysis asset,
which is empowered by various abstract model construction methods. To assess
the quality of the abstract model, we collect and define a number of evaluation
metrics, aiming at both abstract model level and the semantics level. Then, the
semantics, which is the degree of satisfaction of the LLM w.r.t. the
trustworthiness perspective, is bound to and enriches the abstract model with
semantics, which enables more detailed analysis applications for diverse
purposes.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14214" title="Abstract">arXiv:2310.14214</a> [<a href="/pdf/2310.14214" title="Download PDF">pdf</a>, <a href="/format/2310.14214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransY-Net:Learning Fully Transformer Networks for Change Detection of  Remote Sensing Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tianyu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zifu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pingping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Gong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is accepted by TGRS2023. It is an extension of our ACCV2022 paper and <a href="/abs/2210.00757">arXiv:2210.00757</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">In the remote sensing field, Change Detection (CD) aims to identify and
localize the changed regions from dual-phase images over the same places.
Recently, it has achieved great progress with the advances of deep learning.
However, current methods generally deliver incomplete CD regions and irregular
CD boundaries due to the limited representation ability of the extracted visual
features. To relieve these issues, in this work we propose a novel
Transformer-based learning framework named TransY-Net for remote sensing image
CD, which improves the feature extraction from a global view and combines
multi-level visual features in a pyramid manner. More specifically, the
proposed framework first utilizes the advantages of Transformers in long-range
dependency modeling. It can help to learn more discriminative global-level
features and obtain complete CD regions. Then, we introduce a novel pyramid
structure to aggregate multi-level visual features from Transformers for
feature enhancement. The pyramid structure grafted with a Progressive Attention
Module (PAM) can improve the feature representation ability with additional
inter-dependencies through spatial and channel attentions. Finally, to better
train the whole framework, we utilize the deeply-supervised learning with
multiple boundary-aware loss functions. Extensive experiments demonstrate that
our proposed method achieves a new state-of-the-art performance on four optical
and two SAR image CD benchmarks. The source code is released at
https://github.com/Drchip61/TransYNet.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14215" title="Abstract">arXiv:2310.14215</a> [<a href="/pdf/2310.14215" title="Download PDF">pdf</a>, <a href="/format/2310.14215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Item-Graph2vec: a Efficient and Effective Approach using Item  Co-occurrence Graph Embedding for Collaborative Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruilin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Leya Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuanzhe Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current item-item collaborative filtering algorithms based on artificial
neural network, such as Item2vec, have become ubiquitous and are widely applied
in the modern recommender system. However, these approaches do not apply to the
large-scale item-based recommendation system because of their extremely long
training time. To overcome the shortcoming that current algorithms have high
training time costs and poor stability when dealing with large-scale data sets,
the item graph embedding algorithm Item-Graph2vec is described here. This
algorithm transforms the users' shopping list into a item co-occurrence graph,
obtains item sequences through randomly travelling on this co-occurrence graph
and finally trains item vectors through sequence samples. We posit that because
of the stable size of item, the size and density of the item co-occurrence
graph change slightly with the increase in the training corpus. Therefore,
Item-Graph2vec has a stable runtime on the large scale data set, and its
performance advantage becomes more and more obvious with the growth of the
training corpus. Extensive experiments conducted on real-world data sets
demonstrate that Item-Graph2vec outperforms Item2vec by 3 times in terms of
efficiency on douban data set, while the error generated by the random walk
sampling is small.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14216" title="Abstract">arXiv:2310.14216</a> [<a href="/pdf/2310.14216" title="Download PDF">pdf</a>, <a href="/format/2310.14216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniMAP: Universal SMILES-Graph Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shikun Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lixin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weiying Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yanyan Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Molecular representation learning is fundamental for many drug related
applications. Most existing molecular pre-training models are limited in using
single molecular modality, either SMILES or graph representation. To
effectively leverage both modalities, we argue that it is critical to capture
the fine-grained 'semantics' between SMILES and graph, because subtle
sequence/graph differences may lead to contrary molecular properties. In this
paper, we propose a universal SMILE-graph representation learning model, namely
UniMAP. Firstly, an embedding layer is employed to obtain the token and
node/edge representation in SMILES and graph, respectively. A multi-layer
Transformer is then utilized to conduct deep cross-modality fusion. Specially,
four kinds of pre-training tasks are designed for UniMAP, including Multi-Level
Cross-Modality Masking (CMM), SMILES-Graph Matching (SGM), Fragment-Level
Alignment (FLA), and Domain Knowledge Learning (DKL). In this way, both global
(i.e. SGM and DKL) and local (i.e. CMM and FLA) alignments are integrated to
achieve comprehensive cross-modality fusion. We evaluate UniMAP on various
downstream tasks, i.e. molecular property prediction, drug-target affinity
prediction and drug-drug interaction. Experimental results show that UniMAP
outperforms current state-of-the-art pre-training methods.We also visualize the
learned representations to demonstrate the effect of multi-modality
integration.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14217" title="Abstract">arXiv:2310.14217</a> [<a href="/pdf/2310.14217" title="Download PDF">pdf</a>, <a href="/ps/2310.14217" title="Download PostScript">ps</a>, <a href="/format/2310.14217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Sum Secrecy Rate of Multi-User Holographic MIMO Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Sena%2C+A+S">Arthur S. de Sena</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiguang He</a>, 
<a href="/search/cs?searchtype=author&query=Hammadi%2C+A+A">Ahmed Al Hammadi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bader%2C+F">Faouzi Bader</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Fink%2C+M">Mathias Fink</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, submitted to IEEE ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The emerging concept of extremely-large holographic multiple-input
multiple-output (HMIMO), beneficial from compactly and densely packed
cost-efficient radiating meta-atoms, has been demonstrated for enhanced degrees
of freedom even in pure line-of-sight conditions, enabling tremendous
multiplexing gain for the next-generation communication systems. Most of the
reported works focus on energy and spectrum efficiency, path loss analyses, and
channel modeling. The extension to secure communications remains unexplored. In
this paper, we theoretically characterize the secrecy capacity of the HMIMO
network with multiple legitimate users and one eavesdropper while taking into
consideration artificial noise and max-min fairness. We formulate the power
allocation (PA) problem and address it by following successive convex
approximation and Taylor expansion. We further study the effect of fixed PA
coefficients, imperfect channel state information, inter-element spacing, and
the number of Eve's antennas on the sum secrecy rate. Simulation results show
that significant performance gain with more than 100\% increment in the high
signal-to-noise ratio (SNR) regime for the two-user case is obtained by
exploiting adaptive/flexible PA compared to the case with fixed PA
coefficients.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14219" title="Abstract">arXiv:2310.14219</a> [<a href="/pdf/2310.14219" title="Download PDF">pdf</a>, <a href="/ps/2310.14219" title="Download PostScript">ps</a>, <a href="/format/2310.14219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Codes with Low Redundancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+C">Chaoping Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Determining the largest size, or equivalently finding the lowest redundancy,
of q-ary codes for given length and minimum distance is one of the central and
fundamental problems in coding theory. Inspired by the construction of
Varshamov-Tenengolts (VT for short) codes via check-sums, we provide an
explicit construction of nonlinear codes with lower redundancy than linear
codes under the same length and minimum distance. Similar to the VT codes, our
construction works well for small distance (or even constant distance).
Furthermore, we design quasi-linear time decoding algorithms for both erasure
and adversary errors.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14221" title="Abstract">arXiv:2310.14221</a> [<a href="/pdf/2310.14221" title="Download PDF">pdf</a>, <a href="/ps/2310.14221" title="Download PostScript">ps</a>, <a href="/format/2310.14221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Importance of Anti-Aliasing in Tiny Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+J">Jinlai Ning</a>, 
<a href="/search/cs?searchtype=author&query=Spratling%2C+M">Michael Spratling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Tiny object detection has gained considerable attention in the research
community owing to the frequent occurrence of tiny objects in numerous critical
real-world scenarios. However, convolutional neural networks (CNNs) used as the
backbone for object detection architectures typically neglect Nyquist's
sampling theorem during down-sampling operations, resulting in aliasing and
degraded performance. This is likely to be a particular issue for tiny objects
that occupy very few pixels and therefore have high spatial frequency features.
This paper applied an existing approach WaveCNet for anti-aliasing to tiny
object detection. WaveCNet addresses aliasing by replacing standard
down-sampling processes in CNNs with Wavelet Pooling (WaveletPool) layers,
effectively suppressing aliasing. We modify the original WaveCNet to apply
WaveletPool in a consistent way in both pathways of the residual blocks in
ResNets. Additionally, we also propose a bottom-heavy version of the backbone,
which further improves the performance of tiny object detection while also
reducing the required number of parameters by almost half. Experimental results
on the TinyPerson, WiderFace, and DOTA datasets demonstrate the importance of
anti-aliasing in tiny object detection and the effectiveness of the proposed
method which achieves new state-of-the-art results on all three datasets. Codes
and experiment results are released at
https://github.com/freshn/Anti-aliasing-Tiny-Object-Detection.git.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14222" title="Abstract">arXiv:2310.14222</a> [<a href="/pdf/2310.14222" title="Download PDF">pdf</a>, <a href="/format/2310.14222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-for-All: Towards Universal Domain Translation with a Single StyleGAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yong Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+J">Jiahui Zhan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shengfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose a novel translation model, UniTranslator, for
transforming representations between visually distinct domains under conditions
of limited training data and significant visual differences. The main idea
behind our approach is leveraging the domain-neutral capabilities of CLIP as a
bridging mechanism, while utilizing a separate module to extract abstract,
domain-agnostic semantics from the embeddings of both the source and target
realms. Fusing these abstract semantics with target-specific semantics results
in a transformed embedding within the CLIP space. To bridge the gap between the
disparate worlds of CLIP and StyleGAN, we introduce a new non-linear mapper,
the CLIP2P mapper. Utilizing CLIP embeddings, this module is tailored to
approximate the latent distribution in the P space, effectively acting as a
connector between these two spaces. The proposed UniTranslator is versatile and
capable of performing various tasks, including style mixing, stylization, and
translations, even in visually challenging scenarios across different visual
domains. Notably, UniTranslator generates high-quality translations that
showcase domain relevance, diversity, and improved image quality. UniTranslator
surpasses the performance of existing general-purpose models and performs well
against specialized models in representative tasks. The source code and trained
models will be released to the public.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14224" title="Abstract">arXiv:2310.14224</a> [<a href="/pdf/2310.14224" title="Download PDF">pdf</a>, <a href="/format/2310.14224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detrive: Imitation Learning with Transformer Detection for End-to-End  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Daoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Ning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pipe%2C+T">Tony Pipe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, DISA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This Paper proposes a novel Transformer-based end-to-end autonomous driving
model named Detrive. This model solves the problem that the past end-to-end
models cannot detect the position and size of traffic participants. Detrive
uses an end-to-end transformer based detection model as its perception module;
a multi-layer perceptron as its feature fusion network; a recurrent neural
network with gate recurrent unit for path planning; and two controllers for the
vehicle's forward speed and turning angle. The model is trained with an on-line
imitation learning method. In order to obtain a better training set, a
reinforcement learning agent that can directly obtain a ground truth bird's-eye
view map from the Carla simulator as a perceptual output, is used as teacher
for the imitation learning. The trained model is tested on the Carla's
autonomous driving benchmark. The results show that the Transformer detector
based end-to-end model has obvious advantages in dynamic obstacle avoidance
compared with the traditional classifier based end-to-end model.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14225" title="Abstract">arXiv:2310.14225</a> [<a href="/pdf/2310.14225" title="Download PDF">pdf</a>, <a href="/format/2310.14225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Customising General Large Language Models for Specialised Emotion  Recognition Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Liyizhe Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tao Pang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jing Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Schuller%2C+B+W">Bj&#xf6;rn W. Schuller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The advent of large language models (LLMs) has gained tremendous attention
over the past year. Previous studies have shown the astonishing performance of
LLMs not only in other tasks but also in emotion recognition in terms of
accuracy, universality, explanation, robustness, few/zero-shot learning, and
others. Leveraging the capability of LLMs inevitably becomes an essential
solution for emotion recognition. To this end, we further comprehensively
investigate how LLMs perform in linguistic emotion recognition if we
concentrate on this specific task. Specifically, we exemplify a publicly
available and widely used LLM -- Chat General Language Model, and customise it
for our target by using two different modal adaptation techniques, i.e., deep
prompt tuning and low-rank adaptation. The experimental results obtained on six
widely used datasets present that the adapted LLM can easily outperform other
state-of-the-art but specialised deep models. This indicates the strong
transferability and feasibility of LLMs in the field of emotion recognition.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14226" title="Abstract">arXiv:2310.14226</a> [<a href="/pdf/2310.14226" title="Download PDF">pdf</a>, <a href="/format/2310.14226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-stream Cell Segmentation with Low-level Cues for Multi-modality  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+W">Wei Lou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xinyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haofeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The second place in NeurIPS 2022 cell segmentation challenge (<a href="https://neurips22-cellseg.grand-challenge.org/">this https URL</a>), released code: <a href="https://github.com/lhaof/CellSeg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Cell segmentation for multi-modal microscopy images remains a challenge due
to the complex textures, patterns, and cell shapes in these images. To tackle
the problem, we first develop an automatic cell classification pipeline to
label the microscopy images based on their low-level image characteristics, and
then train a classification model based on the category labels. Afterward, we
train a separate segmentation model for each category using the images in the
corresponding category. Besides, we further deploy two types of segmentation
models to segment cells with roundish and irregular shapes respectively.
Moreover, an efficient and powerful backbone model is utilized to enhance the
efficiency of our segmentation model. Evaluated on the Tuning Set of NeurIPS
2022 Cell Segmentation Challenge, our method achieves an F1-score of 0.8795 and
the running time for all cases is within the time tolerance.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14227" title="Abstract">arXiv:2310.14227</a> [<a href="/pdf/2310.14227" title="Download PDF">pdf</a>, <a href="/format/2310.14227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Deep Ensemble for Out-of-Distribution Detection: A Loss  Landscape Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+K">Kun Fang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Q">Qinghua Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Existing Out-of-Distribution (OoD) detection methods address to detect OoD
samples from In-Distribution data (InD) mainly by exploring differences in
features, logits and gradients in Deep Neural Networks (DNNs). We in this work
propose a new perspective upon loss landscape and mode ensemble to investigate
OoD detection. In the optimization of DNNs, there exist many local optima in
the parameter space, or namely modes. Interestingly, we observe that these
independent modes, which all reach low-loss regions with InD data (training and
test data), yet yield significantly different loss landscapes with OoD data.
Such an observation provides a novel view to investigate the OoD detection from
the loss landscape and further suggests significantly fluctuating OoD detection
performance across these modes. For instance, FPR values of the RankFeat method
can range from 46.58% to 84.70% among 5 modes, showing uncertain detection
performance evaluations across independent modes. Motivated by such diversities
on OoD loss landscape across modes, we revisit the deep ensemble method for OoD
detection through mode ensemble, leading to improved performance and benefiting
the OoD detector with reduced variances. Extensive experiments covering varied
OoD detectors and network structures illustrate high variances across modes and
also validate the superiority of mode ensemble in boosting OoD detection. We
hope this work could attract attention in the view of independent modes in the
OoD loss landscape and more reliable evaluations on OoD detectors.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14228" title="Abstract">arXiv:2310.14228</a> [<a href="/pdf/2310.14228" title="Download PDF">pdf</a>, <a href="/format/2310.14228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Vector Quantized Transformer for Multi-class Unsupervised  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Ruiying Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">YuJie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+L">Long Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dongsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruimin Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised image Anomaly Detection (UAD) aims to learn robust and
discriminative representations of normal samples. While separate solutions per
class endow expensive computation and limited generalizability, this paper
focuses on building a unified framework for multiple classes. Under such a
challenging setting, popular reconstruction-based networks with continuous
latent representation assumption always suffer from the "identical shortcut"
issue, where both normal and abnormal samples can be well recovered and
difficult to distinguish. To address this pivotal issue, we propose a
hierarchical vector quantized prototype-oriented Transformer under a
probabilistic framework. First, instead of learning the continuous
representations, we preserve the typical normal patterns as discrete iconic
prototypes, and confirm the importance of Vector Quantization in preventing the
model from falling into the shortcut. The vector quantized iconic prototype is
integrated into the Transformer for reconstruction, such that the abnormal data
point is flipped to a normal data point.Second, we investigate an exquisite
hierarchical framework to relieve the codebook collapse issue and replenish
frail normal patterns. Third, a prototype-oriented optimal transport method is
proposed to better regulate the prototypes and hierarchically evaluate the
abnormal score. By evaluating on MVTec-AD and VisA datasets, our model
surpasses the state-of-the-art alternatives and possesses good
interpretability. The code is available at
https://github.com/RuiyingLu/HVQ-Trans.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14230" title="Abstract">arXiv:2310.14230</a> [<a href="/pdf/2310.14230" title="Download PDF">pdf</a>, <a href="/format/2310.14230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comprehensive survey on deep active learning and its applications in  medical image analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wanga%2C+H">Haoran Wanga</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiuye Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiman Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Manning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhijian Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Github Page: <a href="https://github.com/LightersWang/Awesome-Active-Learning-for-Medical-Image-Analysis">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning has achieved widespread success in medical image analysis,
leading to an increasing demand for large-scale expert-annotated medical image
datasets. Yet, the high cost of annotating medical images severely hampers the
development of deep learning in this field. To reduce annotation costs, active
learning aims to select the most informative samples for annotation and train
high-performance models with as few labeled samples as possible. In this
survey, we review the core methods of active learning, including the evaluation
of informativeness and sampling strategy. For the first time, we provide a
detailed summary of the integration of active learning with other
label-efficient techniques, such as semi-supervised, self-supervised learning,
and so on. Additionally, we also highlight active learning works that are
specifically tailored to medical image analysis. In the end, we offer our
perspectives on the future trends and challenges of active learning and its
applications in medical image analysis.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14237" title="Abstract">arXiv:2310.14237</a> [<a href="/pdf/2310.14237" title="Download PDF">pdf</a>, <a href="/format/2310.14237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Quality 3D Face Reconstruction with Affine Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiqian Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiangke Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lincheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhengxia Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
<p class="mathjax">Recent works based on convolutional encoder-decoder architecture and 3DMM
parameterization have shown great potential for canonical view reconstruction
from a single input image. Conventional CNN architectures benefit from
exploiting the spatial correspondence between the input and output pixels.
However, in 3D face reconstruction, the spatial misalignment between the input
image (e.g. face) and the canonical/UV output makes the feature
encoding-decoding process quite challenging. In this paper, to tackle this
problem, we propose a new network architecture, namely the Affine Convolution
Networks, which enables CNN based approaches to handle spatially
non-corresponding input and output images and maintain high-fidelity quality
output at the same time. In our method, an affine transformation matrix is
learned from the affine convolution layer for each spatial location of the
feature maps. In addition, we represent 3D human heads in UV space with
multiple components, including diffuse maps for texture representation,
position maps for geometry representation, and light maps for recovering more
complex lighting conditions in the real world. All the components can be
trained without any manual annotations. Our method is parametric-free and can
generate high-quality UV maps at resolution of 512 x 512 pixels, while previous
approaches normally generate 256 x 256 pixels or smaller. Our code will be
released once the paper got accepted.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14239" title="Abstract">arXiv:2310.14239</a> [<a href="/pdf/2310.14239" title="Download PDF">pdf</a>, <a href="/format/2310.14239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guidance system for Visually Impaired Persons using Deep Learning and  Optical flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dubey%2C+S">Shwetang Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+A+R">Alok Ranjan Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+P">Pavan Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Visually impaired persons find it difficult to know about their surroundings
while walking on a road. Walking sticks used by them can only give them
information about the obstacles in the stick's proximity. Moreover, it is
mostly effective in static or very slow-paced environments. Hence, this paper
introduces a method to guide them in a busy street. To create such a system it
is very important to know about the approaching object and its direction of
approach. To achieve this objective we created a method in which the image
frame received from the video is divided into three parts i.e. center, left,
and right to know the direction of approach of the approaching object. Object
detection is done using YOLOv3. Lucas Kanade's optical flow estimation method
is used for the optical flow estimation and Depth-net is used for depth
estimation. Using the depth information, object motion trajectory, and object
category information, the model provides necessary information/warning to the
person. This model has been tested in the real world to show its effectiveness.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14248" title="Abstract">arXiv:2310.14248</a> [<a href="/pdf/2310.14248" title="Download PDF">pdf</a>, <a href="/format/2310.14248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Static to Dynamic: A Continual Learning Framework for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mingzhe Du</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+B">Bin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-kiong Ng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The vast number of parameters in large language models (LLMs) endows them
with remarkable capabilities, allowing them to excel in a variety of natural
language processing tasks. However, this complexity also presents challenges,
making LLMs difficult to train and inhibiting their ability to continuously
assimilate new knowledge, which may lead to inaccuracies in their outputs. To
mitigate these issues, this paper presents DynaMind, a novel continual learning
framework designed for LLMs. DynaMind incorporates memory mechanisms to
assimilate new knowledge and modular operators to enhance the model inference
process with the newly assimilated knowledge, consequently improving the
accuracies of LLMs' outputs. Benchmark experiments demonstrate DynaMind's
effectiveness in overcoming these challenges. The code and demo of DynaMind are
available on GitHub: https://github.com/Elfsong/DynaMind.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14251" title="Abstract">arXiv:2310.14251</a> [<a href="/pdf/2310.14251" title="Download PDF">pdf</a>, <a href="/format/2310.14251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAS-assisted NOMA Short-Packet Communication Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jianchao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tuo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+X">Xiazhi Lai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Cunhua Pan</a>, 
<a href="/search/cs?searchtype=author&query=Elkashlan%2C+M">Maged Elkashlan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we investigate a fluid antenna system (FAS)-assisted downlink
non-orthogonal multiple access (NOMA) for short-packet communications. The base
station (BS) adopts a single fixed antenna, while both the central user (CU)
and the cell-edge user (CEU) are equipped with a FAS. Each FAS comprises $N$
flexible positions (also known as ports), linked to $N$ arbitrarily correlated
Rayleigh fading channels. We derive expressions for the average block error
rate (BLER) of the FAS-assisted NOMA system and provide asymptotic BLER
expressions. We determine that the diversity order for CU and CEU is $N$,
indicating that the system performance can be considerably improved by
increasing $N$. Simulation results validate the great performance of FAS.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14257" title="Abstract">arXiv:2310.14257</a> [<a href="/pdf/2310.14257" title="Download PDF">pdf</a>, <a href="/ps/2310.14257" title="Download PostScript">ps</a>, <a href="/format/2310.14257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimizing Age of Information under Latency and Throughput Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saurav%2C+K">Kumar Saurav</a>, 
<a href="/search/cs?searchtype=author&query=Chavva%2C+A+K+R">Ashok Kumar Reddy Chavva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We consider a scheduling problem pertinent to a base station (BS) that serves
heterogeneous users (UEs). We focus on a downlink model, with users that are
either interested in large throughput, low latency (the time difference between
the packet arrival at the BS, and reception at the UE), or low
age-of-information (AoI, the difference between the current time and the
arrival time of the latest packet at the BS that has been received at the UE).
In each time step, the BS may serve a limited number of UEs. The objective is
to find a scheduling algorithm that meets the individual service requirements
of all the UEs. In this paper, we formulate the aforementioned objective as an
optimization problem, and propose candidate solutions (algorithms) that
rationalize the tradeoff between throughput, latency and the AoI. Then, we
analyze the performance of the proposed algorithms both analytically and
numerically, and discuss techniques to extend their applicability to other
general setups.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14258" title="Abstract">arXiv:2310.14258</a> [<a href="/pdf/2310.14258" title="Download PDF">pdf</a>, <a href="/format/2310.14258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructive barrier feedback for collision avoidance in leader-follower  formation control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tang%2C+Z">Zhiqi Tang</a>, 
<a href="/search/eess?searchtype=author&query=Cunha%2C+R">Rita Cunha</a>, 
<a href="/search/eess?searchtype=author&query=Hamel%2C+T">Tarek Hamel</a>, 
<a href="/search/eess?searchtype=author&query=Silvestre%2C+C">Carlos Silvestre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes a novel constructive barrier feedback for reactive
collision avoidance between two agents. It incorporates this feature in a
formation tracking control strategy for a group of 2nd-order dynamic robots
defined in three-dimensional space. Using only relative measurements between
neighboring agents, we propose an elegant decentralized controller as the sum
of a nominal tracking controller and the constructive barrier feedback for
leader-follower formations under a directed single-spanning tree graph
topology. The key ingredient is the use of divergent flow as a dissipative
term, which slows down the relative velocity in the direction of the
neighboring robots without compromising the nominal controller's performance.
Compared to traditional barrier function-based optimization controllers, the
proposed constructive barrier feedback avoids feasibility issues and results in
more computationally efficient control algorithms with systematic equilibrium
analysis.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14261" title="Abstract">arXiv:2310.14261</a> [<a href="/pdf/2310.14261" title="Download PDF">pdf</a>, <a href="/format/2310.14261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and  Majority Voted Fine-Tuned Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seth%2C+P">Pratinav Seth</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+R">Rashi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+K">Komal Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Vemulapalli%2C+S">Swetha Vemulapalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at The 1st Workshop on Bangla Language Processing (BLP 2023), EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper describes our approach to submissions made at Shared Task 2 at BLP
Workshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis
is an action research area in the digital age. With the rapid and constant
growth of online social media sites and services and the increasing amount of
textual data, the application of automatic Sentiment Analysis is on the rise.
However, most of the research in this domain is based on the English language.
Despite being the world's sixth most widely spoken language, little work has
been done in Bangla. This task aims to promote work on Bangla Sentiment
Analysis while identifying the polarity of social media content by determining
whether the sentiment expressed in the text is Positive, Negative, or Neutral.
Our approach consists of experimenting and finetuning various multilingual and
pre-trained BERT-based models on our downstream tasks and using a Majority
Voting and Weighted ensemble model that outperforms individual baseline model
scores. Our system scored 0.711 for the multiclass classification task and
scored 10th place among the participants on the leaderboard for the shared
task. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14262" title="Abstract">arXiv:2310.14262</a> [<a href="/pdf/2310.14262" title="Download PDF">pdf</a>, <a href="/format/2310.14262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Unsupervised Machine Translation with Pseudo-Parallel Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kvapil%C3%ADkov%C3%A1%2C+I">Ivana Kvapil&#xed;kov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Bojar%2C+O">Ond&#x159;ej Bojar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MT Summit 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ivana Kvapil\'ikov\'a, Ond\v{r}ej Bojar (2023): Boosting
  Unsupervised Machine Translation with Pseudo-Parallel Data. In: Proceedings
  of Machine Translation Summit XIX vol. 1: Research Track, pp. 135-147, AAMT,
  Kyoto, Japan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Even with the latest developments in deep learning and large-scale language
modeling, the task of machine translation (MT) of low-resource languages
remains a challenge. Neural MT systems can be trained in an unsupervised way
without any translation resources but the quality lags behind, especially in
truly low-resource conditions. We propose a training strategy that relies on
pseudo-parallel sentence pairs mined from monolingual corpora in addition to
synthetic sentence pairs back-translated from monolingual corpora. We
experiment with different training schedules and reach an improvement of up to
14.5 BLEU points (English to Ukrainian) over a baseline trained on
back-translated data only.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14265" title="Abstract">arXiv:2310.14265</a> [<a href="/pdf/2310.14265" title="Download PDF">pdf</a>, <a href="/format/2310.14265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CT-GAT: Cross-Task Generative Adversarial Attack based on  Transferability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+M">Minxuan Lv</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+C">Chengwei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Neural network models are vulnerable to adversarial examples, and adversarial
transferability further increases the risk of adversarial attacks. Current
methods based on transferability often rely on substitute models, which can be
impractical and costly in real-world scenarios due to the unavailability of
training data and the victim model's structural details. In this paper, we
propose a novel approach that directly constructs adversarial examples by
extracting transferable features across various tasks. Our key insight is that
adversarial transferability can extend across different tasks. Specifically, we
train a sequence-to-sequence generative model named CT-GAT using adversarial
sample data collected from multiple tasks to acquire universal adversarial
features and generate adversarial examples for different tasks. We conduct
experiments on ten distinct datasets, and the results demonstrate that our
method achieves superior attack performance with small cost.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14266" title="Abstract">arXiv:2310.14266</a> [<a href="/pdf/2310.14266" title="Download PDF">pdf</a>, <a href="/ps/2310.14266" title="Download PostScript">ps</a>, <a href="/format/2310.14266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Universal Eigenvalues and Eigenvectors of Hypermatrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheng%2C+D">Daizhan Cheng</a>, 
<a href="/search/math?searchtype=author&query=Ji%2C+Z">Zhengping Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Rings and Algebras (math.RA)

</div>
<p class="mathjax">A cubic hypermatrix of order $d$ can be considered as a structure matrix of a
tensor with covariant order $r$ and contra-variant order $s=d-r$. Corresponding
to this matrix expression of the hypermatrix, an eigenvector $x$ with respect
to an eigenvalue $\lambda$ is proposed, called the universal eigenvector and
eigenvalue of the hypermatrix. According to the action of tensors, if $x$ is
decomposable, it is called a universal hyper-(UH-)eigenvector. Particularly, if
all decomposed components are the same, $x$ is called a universal diagonal
hyper (UDH-)eigenvector, which covers most of existing definitions of
eigenvalue/eigenvector of hypermatrices. Using Semi-tensor product (STP) of
matrices, the properties of universal eigenvalues/eigenvectors are
investigated. Algorithms are developed to calculate universal
eigenvalues/eigenvectors for hypermatrices. Particular efforts have been put on
UDH- eigenvalues/eigenvectors, because they cover most of the existing
eigenvalues/eigenvectors for hypermatrices. Some numerical examples are
presented to illustrate that the proposed technique is universal and efficient.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14274" title="Abstract">arXiv:2310.14274</a> [<a href="/pdf/2310.14274" title="Download PDF">pdf</a>, <a href="/format/2310.14274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Visual Imitation Learning with Inverse Dynamics Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+R">Rongchang Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kewu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Lingfei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jishiyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhe Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Imitation learning (IL) has achieved considerable success in solving complex
sequential decision-making problems. However, current IL methods mainly assume
that the environment for learning policies is the same as the environment for
collecting expert datasets. Therefore, these methods may fail to work when
there are slight differences between the learning and expert environments,
especially for challenging problems with high-dimensional image observations.
However, in real-world scenarios, it is rare to have the chance to collect
expert trajectories precisely in the target learning environment. To address
this challenge, we propose a novel robust imitation learning approach, where we
develop an inverse dynamics state representation learning objective to align
the expert environment and the learning environment. With the abstract state
representation, we design an effective reward function, which thoroughly
measures the similarity between behavior data and expert data not only
element-wise, but also from the trajectory level. We conduct extensive
experiments to evaluate the proposed approach under various visual
perturbations and in diverse visual control tasks. Our approach can achieve a
near-expert performance in most environments, and significantly outperforms the
state-of-the-art visual IL methods and robust IL methods.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14276" title="Abstract">arXiv:2310.14276</a> [<a href="/pdf/2310.14276" title="Download PDF">pdf</a>, <a href="/ps/2310.14276" title="Download PostScript">ps</a>, <a href="/format/2310.14276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothed projections over manifolds in finite element exterior calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Licht%2C+M+W">Martin W. Licht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted. 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We develop commuting finite element projections over smooth Riemannian
manifolds. This extension of finite element exterior calculus establishes the
stability and convergence of finite element methods for the Hodge-Laplace
equation on manifolds. The commuting projections use localized mollification
operators, building upon a classical construction by de Rham. These projections
are uniformly bounded on Lebesgue spaces of differential forms and map onto
intrinsic finite element spaces defined with respect to an intrinsic smooth
triangulation of the manifold. We analyze the Galerkin approximation error.
Since practical computations use extrinsic finite element methods over
approximate computational manifolds, we also analyze the geometric error
incurred.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14277" title="Abstract">arXiv:2310.14277</a> [<a href="/pdf/2310.14277" title="Download PDF">pdf</a>, <a href="/format/2310.14277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Continual Semantic Segmentation: Theory, Challenge, Method  and Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Bo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Danpei Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures. Undergoing Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Continual learning, also known as incremental learning or life-long learning,
stands at the forefront of deep learning and AI systems. It breaks through the
obstacle of one-way training on close sets and enables continuous adaptive
learning on open-set conditions. In the recent decade, continual learning has
been explored and applied in multiple fields especially in computer vision
covering classification, detection and segmentation tasks. Continual semantic
segmentation (CSS), of which the dense prediction peculiarity makes it a
challenging, intricate and burgeoning task. In this paper, we present a review
of CSS, committing to building a comprehensive survey on problem formulations,
primary challenges, universal datasets, neoteric theories and multifarious
applications. Concretely, we begin by elucidating the problem definitions and
primary challenges. Based on an in-depth investigation of relevant approaches,
we sort out and categorize current CSS models into two main branches including
\textit{data-replay} and \textit{data-free} sets. In each branch, the
corresponding approaches are similarity-based clustered and thoroughly
analyzed, following qualitative comparison and quantitative reproductions on
relevant datasets. Besides, we also introduce four CSS specialities with
diverse application scenarios and development tendencies. Furthermore, we
develop a benchmark for CSS encompassing representative references, evaluation
results and reproductions, which is available
at~\url{https://github.com/YBIO/SurveyCSS}. We hope this survey can serve as a
reference-worthy and stimulating contribution to the advancement of the
life-long learning field, while also providing valuable perspectives for
related fields.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14278" title="Abstract">arXiv:2310.14278</a> [<a href="/pdf/2310.14278" title="Download PDF">pdf</a>, <a href="/format/2310.14278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Speech Recognition by Learning Audio-textual Cross-modal  Contextual Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Kun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Hang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Quan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Ning Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lei Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to TASLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Automatic Speech Recognition (ASR) in conversational settings presents unique
challenges, including extracting relevant contextual information from previous
conversational turns. Due to irrelevant content, error propagation, and
redundancy, existing methods struggle to extract longer and more effective
contexts. To address this issue, we introduce a novel Conversational ASR
system, extending the Conformer encoder-decoder model with cross-modal
conversational representation. Our approach leverages a cross-modal extractor
that combines pre-trained speech and text models through a specialized encoder
and a modal-level mask input. This enables the extraction of richer historical
speech context without explicit error propagation. We also incorporate
conditional latent variational modules to learn conversational level attributes
such as role preference and topic coherence. By introducing both cross-modal
and conversational representations into the decoder, our model retains context
over longer sentences without information loss, achieving relative accuracy
improvements of 8.8% and 23% on Mandarin conversation datasets HKUST and
MagicData-RAMC, respectively, compared to the standard Conformer model.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14282" title="Abstract">arXiv:2310.14282</a> [<a href="/pdf/2310.14282" title="Download PDF">pdf</a>, <a href="/format/2310.14282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NERetrieve: Dataset for Next Generation Named Entity Recognition and  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katz%2C+U">Uri Katz</a>, 
<a href="/search/cs?searchtype=author&query=Vetzler%2C+M">Matan Vetzler</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A+D">Amir DN Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Recognizing entities in texts is a central need in many information-seeking
scenarios, and indeed, Named Entity Recognition (NER) is arguably one of the
most successful examples of a widely adopted NLP task and corresponding NLP
technology. Recent advances in large language models (LLMs) appear to provide
effective solutions (also) for NER tasks that were traditionally handled with
dedicated models, often matching or surpassing the abilities of the dedicated
models. Should NER be considered a solved problem? We argue to the contrary:
the capabilities provided by LLMs are not the end of NER research, but rather
an exciting beginning. They allow taking NER to the next level, tackling
increasingly more useful, and increasingly more challenging, variants. We
present three variants of the NER task, together with a dataset to support
them. The first is a move towards more fine-grained -- and intersectional --
entity types. The second is a move towards zero-shot recognition and extraction
of these fine-grained types based on entity-type labels. The third, and most
challenging, is the move from the recognition setup to a novel retrieval setup,
where the query is a zero-shot entity type, and the expected result is all the
sentences from a large, pre-indexed corpus that contain entities of these
types, and their corresponding spans. We show that all of these are far from
being solved. We provide a large, silver-annotated corpus of 4 million
paragraphs covering 500 entity types, to facilitate research towards all of
these three goals.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14283" title="Abstract">arXiv:2310.14283</a> [<a href="/pdf/2310.14283" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandwidth Efficient Livestreaming in Mobile Wireless Networks: A  Peer-to-Peer ACIDE Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Negulescu%2C+A">Andrei Negulescu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+W">Weijia Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, Conference Submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, a media distribution model, Active Control in an Intelligent
and Distributed Environment (ACIDE), and solutions are proposed for video and
audio livestreaming in mobile wireless networks. A base station and a cluster
formed by a number of users are the essential components. Inside a cluster,
users can establish peer to peer communications. The users that are members of
a cluster are considered peers. This paper addresses the problem of minimizing
the bandwidth allocated to a cluster of n peers such that a continuous media
play of all the peers is guaranteed. The basic idea is to send the livestream
media in packages. A media package is divided into n blocks. The distribution
of blocks to the peers of a cluster follows a two-phase, multi-step approach.
In phase 1 each peer receives one block with the optimal size from the base
station. In phase 2, peers exchange their media blocks simultaneously in a few
steps. Then the media package can be reconstructed and a live media can be
played continuously. Allocated bandwidth, the amount of bandwidth the base
station has to allocate to this cluster in order to play live streaming media
without interruptions, is a function of many parameters such as the block
sizes, download and upload bandwidth values of peers. This problem is
formulated as an optimization problem. A solution is proposed to find the
optimal block sizes such that the allocated bandwidth is minimized. Both
theoretical model and simulations show that when the number of peers is large,
the optimal allocated bandwidth approaches the lower bound that is the
bandwidth required for multicasting. In other words, the allocated bandwidth
may be reduced n times.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14288" title="Abstract">arXiv:2310.14288</a> [<a href="/pdf/2310.14288" title="Download PDF">pdf</a>, <a href="/format/2310.14288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Popular Matchings with Uncertain, Multilayer and Aggregated Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cs%C3%A1ji%2C+G">Gergely Cs&#xe1;ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the Popular Matching problem in multiple models, where the
preferences of the agents in the instance may change or may be
unknown/uncertain. In particular, we study an Uncertainty model, where each
agent has a possible set of preferences, a Multilayer model, where there are
layers of preference profiles, a Robust model, where any agent may move some
other agents up or down some places in his preference list and an Aggregated
Preference model, where votes are summed over multiple instances with different
preferences.
<br />We study both one-sided and two-sided preferences in bipartite graphs. In the
one-sided model, we show that all our problems can be solved in polynomial time
by utilizing the structure of popular matchings. We also obtain nice structural
results. With two-sided preferences, we show that all four above models lead to
NP-hard questions. Except for the Robust model, these hardness results hold
even if one side of the agents has always the same preferences.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14289" title="Abstract">arXiv:2310.14289</a> [<a href="/pdf/2310.14289" title="Download PDF">pdf</a>, <a href="/ps/2310.14289" title="Download PostScript">ps</a>, <a href="/format/2310.14289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separating multiscale Battery dynamics and predicting multi-step ahead  voltage simultaneously through a data-driven approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Desai%2C+T">Tushar Desai</a>, 
<a href="/search/eess?searchtype=author&query=Ferrari%2C+R+M+G">Riccardo M.G. Ferrari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 10 figures, IEEE Vehicle Power and Propulsion confernce(IEEE VPPC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Accurate prediction of battery performance under various ageing conditions is
necessary for reliable and stable battery operations. Due to complex battery
degradation mechanisms, estimating the accurate ageing level and
ageing-dependent battery dynamics is difficult. This work presents a
health-aware battery model that is capable of separating fast dynamics from
slowly varying states of degradation and state of charge (SOC). The method is
based on a sequence-to-sequence learning-based encoder-decoder model, where the
encoder infers the slowly varying states as the latent space variables in an
unsupervised way, and the decoder provides health-aware multi-step ahead
prediction conditioned on slowly varying states from the encoder. The proposed
approach is verified on a Lithium-ion battery ageing dataset based on real
driving profiles of electric vehicles.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14290" title="Abstract">arXiv:2310.14290</a> [<a href="/pdf/2310.14290" title="Download PDF">pdf</a>, <a href="/format/2310.14290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Morozov regularization of inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>, 
<a href="/search/math?searchtype=author&query=Kowar%2C+R">Richard Kowar</a>, 
<a href="/search/math?searchtype=author&query=Tiefentaler%2C+M">Markus Tiefentaler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The solution of inverse problems is central to a wide range of applications
including medicine, biology, and engineering. These problems require finding a
desired solution in the presence of noisy observations. A key feature of
inverse problems is their ill-posedness, which leads to unstable behavior under
noise when standard solution methods are used. For this reason, regularization
methods have been developed that compromise between data fitting and prior
structure. Recently, data-driven variational regularization methods have been
introduced, where the prior in the form of a regularizer is derived from
provided ground truth data. However, these methods have mainly been analyzed
for Tikhonov regularization, referred to as Network Tikhonov Regularization
(NETT). In this paper, we propose and analyze Morozov regularization in
combination with a learned regularizer. The regularizers, which can be adapted
to the training data, are defined by neural networks and are therefore
non-convex. We give a convergence analysis in the non-convex setting allowing
noise-dependent regularizers, and propose a possible training strategy. We
present numerical results for attenuation correction in the context of
photoacoustic tomography.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14294" title="Abstract">arXiv:2310.14294</a> [<a href="/pdf/2310.14294" title="Download PDF">pdf</a>, <a href="/format/2310.14294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep MDP: A Modular Framework for Multi-Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Abhineet Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a fast and modular framework for Multi-Object Tracking
(MOT) based on the Markov descision process (MDP) tracking-by-detection
paradigm. It is designed to allow its various functional components to be
replaced by custom-designed alternatives to suit a given application. An
interactive GUI with integrated object detection, segmentation, MOT and
semi-automated labeling is also provided to help make it easier to get started
with this framework. Though not breaking new ground in terms of performance,
Deep MDP has a large code-base that should be useful for the community to try
out new ideas or simply to have an easy-to-use and easy-to-adapt system for any
MOT application. Deep MDP is available at
https://github.com/abhineet123/deep_mdp.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14296" title="Abstract">arXiv:2310.14296</a> [<a href="/pdf/2310.14296" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research on Key Technologies of Infrastructure Digitalization based on  Multimodal Spatial Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhanyuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianrui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zerui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, in Chinese language, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Since NASA put forward the concept of the digital twin in 2010, many
industries have put forward the dynamic goal of digital development, and the
transportation industry is also among them. With more and more companies laying
out on this virgin land, the digital twin transportation industry has grown
rapidly and gradually formed a complete scientific research system. However,
under the largely mature framework, there are still many loophole problems that
need to be solved. In the process of constructing a road network with point
cloud information, we summarize several major features of the point cloud
collected by laser scanners and analyze the potential problems of constructing
the network, such as misjudging the feature points as ground points and grid
voids. On this basis, we reviewed relevant literature and proposed targeted
solutions, such as building a point cloud pyramid modeled after the image
pyramid, expanding the virtual grid, etc., applying CSF for ground-point cloud
extraction, and constructing a road network model using the PTD (progressive
density-based filter) algorithm. For the problem of road sign detection, we
optimize the remote sensing data in the ground point cloud by enhancing the
information density using edge detection, improving the data quality by
removing the low intensity points, and achieving 90% accuracy of road text
recognition using PaddleOCR and Densenet. As for the real-time digital twin
traffic, we design the P2PRN network using the backbone of MPR-GAN for 2D
feature generation and SuperGlue for 2D feature matching, rendering the
viewpoints according to the matching optimization points, completing the
multimodal matching task after several iterations, and successfully calculating
the road camera position with 10{\deg} and 15m accuracy.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14297" title="Abstract">arXiv:2310.14297</a> [<a href="/pdf/2310.14297" title="Download PDF">pdf</a>, <a href="/format/2310.14297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APP-RUSS: Automated Path Planning for Robotic Ultrasound System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">David Liu</a>, 
<a href="/search/cs?searchtype=author&query=Charton%2C+J">Jerome Charton</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Quanzheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous robotic ultrasound System (RUSS) has been extensively studied.
However, fully automated ultrasound image acquisition is still challenging,
partly due to the lack of study in combining two phases of path planning:
guiding the ultrasound probe to the scan target and covering the scan surface
or volume. This paper presents a system of Automated Path Planning for RUSS
(APP-RUSS). Our focus is on the first phase of automation, which emphasizes
directing the ultrasound probe's path toward the target over extended
distances. Specifically, our APP-RUSS system consists of a RealSense D405 RGB-D
camera that is employed for visual guidance of the UR5e robotic arm and a cubic
Bezier curve path planning model that is customized for delivering the probe to
the recognized target. APP-RUSS can contribute to understanding the integration
of the two phases of path planning in robotic ultrasound imaging, paving the
way for its clinical adoption.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14303" title="Abstract">arXiv:2310.14303</a> [<a href="/pdf/2310.14303" title="Download PDF">pdf</a>, <a href="/format/2310.14303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Unalignment: Parametric Red-Teaming to Expose Hidden  Harms and Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+R">Rishabh Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Red-teaming has been a widely adopted way to evaluate the harmfulness of
Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to
make it act as a helpful agent disregarding the harmfulness of the query.
Existing methods are primarily based on input text-based red-teaming such as
adversarial prompts, low-resource prompts, or contextualized prompts to
condition the model in a way to bypass its safe behavior. Bypassing the
guardrails uncovers hidden harmful information and biases in the model that are
left untreated or newly introduced by its safety training. However,
prompt-based attacks fail to provide such a diagnosis owing to their low attack
success rate, and applicability to specific models. In this paper, we present a
new perspective on LLM safety research i.e., parametric red-teaming through
Unalignment. It simply (instruction) tunes the model parameters to break model
guardrails that are not deeply rooted in the model's behavior. Unalignment
using as few as 100 examples can significantly bypass commonly referred to as
CHATGPT, to the point where it responds with an 88% success rate to harmful
queries on two safety benchmark datasets. On open-source models such as
VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more
than 91%. On bias evaluations, Unalignment exposes inherent biases in
safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's
responses are strongly biased and opinionated 64% of the time.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14304" title="Abstract">arXiv:2310.14304</a> [<a href="/pdf/2310.14304" title="Download PDF">pdf</a>, <a href="/format/2310.14304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Model for All: Large Language Models are Domain-Agnostic  Recommendation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zuoli Tang</a>, 
<a href="/search/cs?searchtype=author&query=Huan%2C+Z">Zhaoxin Huan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaolu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chilin Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The purpose of sequential recommendation is to utilize the interaction
history of a user and predict the next item that the user is most likely to
interact with. While data sparsity and cold start are two challenges that most
recommender systems are still facing, many efforts are devoted to utilizing
data from other domains, called cross-domain methods. However, general
cross-domain methods explore the relationship between two domains by designing
complex model architecture, making it difficult to scale to multiple domains
and utilize more data. Moreover, existing recommendation systems use IDs to
represent item, which carry less transferable signals in cross-domain
scenarios, and user cross-domain behaviors are also sparse, making it
challenging to learn item relationship from different domains. These problems
hinder the application of multi-domain methods to sequential recommendation.
Recently, large language models (LLMs) exhibit outstanding performance in world
knowledge learning from text corpora and general-purpose question answering.
Inspired by these successes, we propose a simple but effective framework for
domain-agnostic recommendation by exploiting the pre-trained LLMs (namely
LLM-Rec). We mix the user's behavior across different domains, and then
concatenate the title information of these items into a sentence and model the
user's behaviors with a pre-trained language model. We expect that by mixing
the user's behaviors across different domains, we can exploit the common
knowledge encoded in the pre-trained language model to alleviate the problems
of data sparsity and cold start problems. Furthermore, we are curious about
whether the latest technical advances in nature language processing (NLP) can
transfer to the recommendation scenarios.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14307" title="Abstract">arXiv:2310.14307</a> [<a href="/pdf/2310.14307" title="Download PDF">pdf</a>, <a href="/format/2310.14307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Media Perceptions of 51% Attacks on Proof-of-Work  Cryptocurrencies: A Natural Language Processing Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baruwa%2C+Z">Zsofia Baruwa</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacherjee%2C+S">Sanjay Bhattacherjee</a>, 
<a href="/search/cs?searchtype=author&query=Chandnani%2C+S+R">Sahil Rey Chandnani</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhen Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">This work is the first study on the effects of 51% attacks on proof-of-work
(PoW) cryptocurrencies as expressed in the sentiments and emotions of social
media users. Our goals are to design the methodologies for the study including
data collection, conduct volumetric and temporal analyses of the data, and
profile the sentiments and emotions that emerge from the data. As a first step,
we have identified 31 events of 51% attacks on various PoW cryptocurrencies. We
have designed the methodologies and gathered Twitter data on the events as well
as benchmark data during normal times for comparison. We have defined
parameters for profiling the datasets based on their sentiments and emotions.
We have studied the variation of these sentiment and emotion profiles when a
cryptocurrency is under attack and the benchmark otherwise, between multiple
attack events of the same cryptocurrency, and between different
cryptocurrencies. Our results confirm some expected overall behaviour and
reactions while providing nuanced insights that may not be obvious or may even
be considered surprising. Our code and datasets are publicly accessible.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14312" title="Abstract">arXiv:2310.14312</a> [<a href="/pdf/2310.14312" title="Download PDF">pdf</a>, <a href="/format/2310.14312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Text Sanitization with Privacy Risk Indicators: An Empirical  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadopoulou%2C+A">Anthi Papadopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Lison%2C+P">Pierre Lison</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+M">Mark Anderson</a>, 
<a href="/search/cs?searchtype=author&query=%C3%98vrelid%2C+L">Lilja &#xd8;vrelid</a>, 
<a href="/search/cs?searchtype=author&query=Pil%C3%A1n%2C+I">Ildik&#xf3; Pil&#xe1;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text sanitization is the task of redacting a document to mask all occurrences
of (direct or indirect) personal identifiers, with the goal of concealing the
identity of the individual(s) referred in it. In this paper, we consider a
two-step approach to text sanitization and provide a detailed analysis of its
empirical performance on two recently published datasets: the Text
Anonymization Benchmark (Pil\'an et al., 2022) and a collection of Wikipedia
biographies (Papadopoulou et al., 2022). The text sanitization process starts
with a privacy-oriented entity recognizer that seeks to determine the text
spans expressing identifiable personal information. This privacy-oriented
entity recognizer is trained by combining a standard named entity recognition
model with a gazetteer populated by person-related terms extracted from
Wikidata. The second step of the text sanitization process consists in
assessing the privacy risk associated with each detected text span, either
isolated or in combination with other text spans. We present five distinct
indicators of the re-identification risk, respectively based on language model
probabilities, text span classification, sequence labelling, perturbations, and
web search. We provide a contrastive analysis of each privacy indicator and
highlight their benefits and limitations, notably in relation to the available
labeled data.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14313" title="Abstract">arXiv:2310.14313</a> [<a href="/pdf/2310.14313" title="Download PDF">pdf</a>, <a href="/format/2310.14313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arbitrary order spline representation of cohomology generators for  isogeometric analysis of eddy current problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kapidani%2C+B">Bernard Kapidani</a>, 
<a href="/search/math?searchtype=author&query=Merkel%2C+M">Melina Merkel</a>, 
<a href="/search/math?searchtype=author&query=Sch%C3%B6ps%2C+S">Sebastian Sch&#xf6;ps</a>, 
<a href="/search/math?searchtype=author&query=V%C3%A1zquez%2C+R">Rafael V&#xe1;zquez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The eddy current problem has many relevant practical applications in science,
ranging from non-destructive testing to magnetic confinement of plasma in
fusion reactors. It arises when electrical conductors are immersed in an
external time-varying magnetic field operating at frequencies for which
electromagnetic wave propagation effects can be neglected.
<br />Popular formulations of the eddy current problem either use the magnetic
vector potential or the magnetic scalar potential. They have individual
advantages and disadvantages. One challenge is related to differential
geometry: Scalar potential based formulations run into trouble when conductors
are present in non-trivial topology, as approximation spaces must be then
augmented with generators of the first cohomology group of the non-conducting
domain.
<br />For all existing algorithms based on lowest order methods it is assumed that
the extension of the graph-based algorithms to high-order approximations
requires hierarchical bases for the curl-conforming discrete spaces. However,
building on insight on de Rham complexes approximation with splines, we will
show in the present submission that the hierarchical basis condition is not
necessary. Algorithms based on spanning tree techniques can instead be adapted
to work on an underlying hexahedral mesh arising from isomorphisms between
spline spaces of differential forms and de Rham complexes on an auxiliary
control mesh.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14318" title="Abstract">arXiv:2310.14318</a> [<a href="/pdf/2310.14318" title="Download PDF">pdf</a>, <a href="/format/2310.14318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent Contrastive Learning with Cross Subsequences for Sequential  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+X">Xiuyuan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Huanhuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Pengpeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guanfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+F">Fuzhen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+V+S">Victor S. Sheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10pages, 5figures, WSDM2024. arXiv admin note: text overlap with <a href="/abs/2304.07763">arXiv:2304.07763</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The user purchase behaviors are mainly influenced by their intentions (e.g.,
buying clothes for decoration, buying brushes for painting, etc.). Modeling a
user's latent intention can significantly improve the performance of
recommendations. Previous works model users' intentions by considering the
predefined label in auxiliary information or introducing stochastic data
augmentation to learn purposes in the latent space. However, the auxiliary
information is sparse and not always available for recommender systems, and
introducing stochastic data augmentation may introduce noise and thus change
the intentions hidden in the sequence. Therefore, leveraging user intentions
for sequential recommendation (SR) can be challenging because they are
frequently varied and unobserved. In this paper, Intent contrastive learning
with Cross Subsequences for sequential Recommendation (ICSRec) is proposed to
model users' latent intentions. Specifically, ICSRec first segments a user's
sequential behaviors into multiple subsequences by using a dynamic sliding
operation and takes these subsequences into the encoder to generate the
representations for the user's intentions. To tackle the problem of no explicit
labels for purposes, ICSRec assumes different subsequences with the same target
item may represent the same intention and proposes a coarse-grain intent
contrastive learning to push these subsequences closer. Then, fine-grain intent
contrastive learning is mentioned to capture the fine-grain intentions of
subsequences in sequential behaviors. Extensive experiments conducted on four
real-world datasets demonstrate the superior performance of the proposed ICSRec
model compared with baseline methods.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14319" title="Abstract">arXiv:2310.14319</a> [<a href="/pdf/2310.14319" title="Download PDF">pdf</a>, <a href="/format/2310.14319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Roca%2C+D">Diego Roca</a>, 
<a href="/search/cs?searchtype=author&query=Vilares%2C+D">David Vilares</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">We introduce an encoding for parsing as sequence labeling that can represent
any projective dependency tree as a sequence of 4-bit labels, one per word. The
bits in each word's label represent (1) whether it is a right or left
dependent, (2) whether it is the outermost (left/right) dependent of its
parent, (3) whether it has any left children and (4) whether it has any right
children. We show that this provides an injective mapping from trees to labels
that can be encoded and decoded in linear time. We then define a 7-bit
extension that represents an extra plane of arcs, extending the coverage to
almost full non-projectivity (over 99.9% empirical arc coverage). Results on a
set of diverse treebanks show that our 7-bit encoding obtains substantial
accuracy gains over the previously best-performing sequence labeling encodings.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14320" title="Abstract">arXiv:2310.14320</a> [<a href="/pdf/2310.14320" title="Download PDF">pdf</a>, <a href="/format/2310.14320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of the RMM-01 Market Maker
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jepsen%2C+W">Waylon Jepsen</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+C">Colin Roberts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Finance (q-fin.CP); Portfolio Management (q-fin.PM); Trading and Market Microstructure (q-fin.TR)

</div>
<p class="mathjax">Constant function market makers(CFMMS) are a popular market design for
decentralized exchanges(DEX). Liquidity providers(LPs) supply the CFMMs with
assets to enable trades. In exchange for providing this liquidity, an LP
receives a token that replicates a payoff determined by the trading function
used by the CFMM. In this paper, we study a time-dependent CFMM called RMM-01.
The trading function for RMM-01 is chosen such that LPs recover the payoff of a
Black--Scholes priced covered call. First, we introduce the general framework
for CFMMs. After, we analyze the pricing properties of RMM-01. This includes
the cost of price manipulation and the corresponding implications on arbitrage.
Our first primary contribution is from examining the time-varying price
properties of RMM-01 and determining parameter bounds when RMM-01 has a more
stable price than Uniswap. Finally, we discuss combining lending protocols with
RMM-01 to achieve other option payoffs which is our other primary contribution.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14325" title="Abstract">arXiv:2310.14325</a> [<a href="/pdf/2310.14325" title="Download PDF">pdf</a>, <a href="/format/2310.14325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Harmful Erotic Content Detection through Coreference-Driven  Contextual Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okulska%2C+I">Inez Okulska</a>, 
<a href="/search/cs?searchtype=author&query=Wi%C5%9Bnios%2C+E">Emilia Wi&#x15b;nios</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for 6th Workshop on Computational Models of Reference, Anaphora and Coreference at EMNLP 2023 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Adult content detection still poses a great challenge for automation.
Existing classifiers primarily focus on distinguishing between erotic and
non-erotic texts. However, they often need more nuance in assessing the
potential harm. Unfortunately, the content of this nature falls beyond the
reach of generative models due to its potentially harmful nature. Ethical
restrictions prohibit large language models (LLMs) from analyzing and
classifying harmful erotics, let alone generating them to create synthetic
datasets for other neural models. In such instances where data is scarce and
challenging, a thorough analysis of the structure of such texts rather than a
large model may offer a viable solution. Especially given that harmful erotic
narratives, despite appearing similar to harmless ones, usually reveal their
harmful nature first through contextual information hidden in the non-sexual
parts of the narrative.
<br />This paper introduces a hybrid neural and rule-based context-aware system
that leverages coreference resolution to identify harmful contextual cues in
erotic content. Collaborating with professional moderators, we compiled a
dataset and developed a classifier capable of distinguishing harmful from
non-harmful erotic content. Our hybrid model, tested on Polish text,
demonstrates a promising accuracy of 84% and a recall of 80%. Models based on
RoBERTa and Longformer without explicit usage of coreference chains achieved
significantly weaker results, underscoring the importance of coreference
resolution in detecting such nuanced content as harmful erotics. This approach
also offers the potential for enhanced visual explainability, supporting
moderators in evaluating predictions and taking necessary actions to address
harmful content.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14326" title="Abstract">arXiv:2310.14326</a> [<a href="/pdf/2310.14326" title="Download PDF">pdf</a>, <a href="/format/2310.14326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural  Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nandy%2C+A">Abhilash Nandy</a>, 
<a href="/search/cs?searchtype=author&query=Kapadnis%2C+M+N">Manav Nitin Kapadnis</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+N">Niloy Ganguly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP Findings 2023, 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we propose CLMSM, a domain-specific, continual pre-training
framework, that learns from a large set of procedural recipes. CLMSM uses a
Multi-Task Learning Framework to optimize two objectives - a) Contrastive
Learning using hard triplets to learn fine-grained differences across entities
in the procedures, and b) a novel Mask-Step Modelling objective to learn
step-wise context of a procedure. We test the performance of CLMSM on the
downstream tasks of tracking entities and aligning actions between two
procedures on three datasets, one of which is an open-domain dataset not
conforming with the pre-training dataset. We show that CLMSM not only
outperforms baselines on recipes (in-domain) but is also able to generalize to
open-domain procedural NLP tasks.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14328" title="Abstract">arXiv:2310.14328</a> [<a href="/pdf/2310.14328" title="Download PDF">pdf</a>, <a href="/format/2310.14328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Key Leasing for PKE and FHE with a Classical Lessor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chardouvelis%2C+O">Orestis Chardouvelis</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+V">Vipul Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Aayush Jain</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahui Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">In this work, we consider the problem of secure key leasing, also known as
revocable cryptography (Agarwal et. al. Eurocrypt' 23, Ananth et. al. TCC' 23),
as a strengthened security notion of its predecessor put forward in Ananth et.
al. Eurocrypt' 21. This problem aims to leverage unclonable nature of quantum
information to allow a lessor to lease a quantum key with reusability for
evaluating a classical functionality. Later, the lessor can request the lessee
to provably delete the key and then the lessee will be completely deprived of
the capability to evaluate.
<br />In this work, we construct a secure key leasing scheme to lease a decryption
key of a (classical) public-key, homomorphic encryption scheme from standard
lattice assumptions. We achieve strong form of security where:
<br />* The entire protocol uses only classical communication between a classical
leaser (client) and a quantum lessee (server).
<br />* Assuming standard assumptions, our security definition ensures that every
computationally bounded quantum adversary could not simultaneously provide a
valid classical deletion certificate and yet distinguish ciphertexts.
<br />Our security relies on the hardness of learning with errors assumption. Our
scheme is the first scheme to be based on a standard assumption and satisfying
the two properties above.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14329" title="Abstract">arXiv:2310.14329</a> [<a href="/pdf/2310.14329" title="Download PDF">pdf</a>, <a href="/format/2310.14329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and  Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zakizadeh%2C+M">Mahdi Zakizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Miandoab%2C+K+E">Kaveh Eskandari Miandoab</a>, 
<a href="/search/cs?searchtype=author&query=Pilehvar%2C+M+T">Mohammad Taher Pilehvar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Numerous debiasing techniques have been proposed to mitigate the gender bias
that is prevalent in pretrained language models. These are often evaluated on
datasets that check the extent to which the model is gender-neutral in its
predictions. Importantly, this evaluation protocol overlooks the possible
adverse impact of bias mitigation on useful gender knowledge. To fill this gap,
we propose DiFair, a manually curated dataset based on masked language modeling
objectives. DiFair allows us to introduce a unified metric, gender invariance
score, that not only quantifies a model's biased behavior, but also checks if
useful gender knowledge is preserved. We use DiFair as a benchmark for a number
of widely-used pretained language models and debiasing techniques. Experimental
results corroborate previous findings on the existing gender biases, while also
demonstrating that although debiasing techniques ameliorate the issue of gender
bias, this improvement usually comes at the price of lowering useful gender
knowledge of the model.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14332" title="Abstract">arXiv:2310.14332</a> [<a href="/pdf/2310.14332" title="Download PDF">pdf</a>, <a href="/ps/2310.14332" title="Download PostScript">ps</a>, <a href="/format/2310.14332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Battery State of Charge and parameters estimation through  Multi-Rate Moving Horizon Estimator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Desai%2C+T">Tushar Desai</a>, 
<a href="/search/eess?searchtype=author&query=Oliva%2C+F">Federico Oliva</a>, 
<a href="/search/eess?searchtype=author&query=Ferrari%2C+R+M+G">Riccardo M.G. Ferrari</a>, 
<a href="/search/eess?searchtype=author&query=Carnevale%2C+D">Daniele Carnevale</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 10 figures, IFAC World Congress 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">For reliable and safe battery operations, accurate and robust State of Charge
(SOC) and model parameters estimation are vital. However, the nonlinear
dependency of the model parameters on battery states makes the problem
challenging. We propose a Moving-Horizon Estimation (MHE)-based robust approach
for joint state and parameters estimation. Due to all the time scales involved
in the model dynamics, a multi-rate MHE is designed to improve the estimation
performance. Moreover, a parallelized structure for the observer is exploited
to reduce the computational burden, combining both multi-rate and a
reduced-order MHEs. Results show that the battery SOC and parameters can be
effectively estimated. The proposed MHE observers are verified on a
Simulink-based battery equivalent circuit model.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14333" title="Abstract">arXiv:2310.14333</a> [<a href="/pdf/2310.14333" title="Download PDF">pdf</a>, <a href="/ps/2310.14333" title="Download PostScript">ps</a>, <a href="/format/2310.14333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Iterative Methods for the Linear Boltzmann Transport  Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Houston%2C+P">Paul Houston</a>, 
<a href="/search/math?searchtype=author&query=Hubbard%2C+M+E">Matthew E. Hubbard</a>, 
<a href="/search/math?searchtype=author&query=Radley%2C+T+J">Thomas J. Radley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this article we consider the iterative solution of the linear system of
equations arising from the discretisation of the poly-energetic linear
Boltzmann transport equation using a discontinuous Galerkin finite element
approximation in space, angle, and energy. In particular, we develop
preconditioned Richardson iterations which may be understood as generalisations
of source iteration in the mono-energetic setting, and derive computable a
posteriori bounds for the solver error incurred due to inexact linear algebra,
measured in a relevant problem-specific norm. We prove that the convergence of
the resulting schemes and a posteriori solver error estimates are independent
of the discretisation parameters. We also discuss how the poly-energetic
Richardson iteration may be employed as a preconditioner for the generalised
minimal residual (GMRES) method. Furthermore, we show that standard
implementations of GMRES based on minimising the Euclidean norm of the residual
vector can be utilized to yield computable a posteriori solver error estimates
at each iteration, through judicious selections of left- and
right-preconditioners for the original linear system. The effectiveness of
poly-energetic source iteration and preconditioned GMRES, as well as their
respective a posteriori solver error estimates, is demonstrated through
numerical examples arising in the modelling of photon transport.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14336" title="Abstract">arXiv:2310.14336</a> [<a href="/pdf/2310.14336" title="Download PDF">pdf</a>, <a href="/format/2310.14336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interpretable Rules for Scalable Data Representation and  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages. arXiv admin note: substantial text overlap with <a href="/abs/2109.15103">arXiv:2109.15103</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Rule-based models, e.g., decision trees, are widely used in scenarios
demanding high model interpretability for their transparent inner structures
and good model expressivity. However, rule-based models are hard to optimize,
especially on large data sets, due to their discrete parameters and structures.
Ensemble methods and fuzzy/soft rules are commonly used to improve performance,
but they sacrifice the model interpretability. To obtain both good scalability
and interpretability, we propose a new classifier, named Rule-based
Representation Learner (RRL), that automatically learns interpretable non-fuzzy
rules for data representation and classification. To train the
non-differentiable RRL effectively, we project it to a continuous space and
propose a novel training method, called Gradient Grafting, that can directly
optimize the discrete model using gradient descent. A novel design of logical
activation functions is also devised to increase the scalability of RRL and
enable it to discretize the continuous features end-to-end. Exhaustive
experiments on ten small and four large data sets show that RRL outperforms the
competitive interpretable approaches and can be easily adjusted to obtain a
trade-off between classification accuracy and model complexity for different
scenarios. Our code is available at: https://github.com/12wang3/rrl.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14337" title="Abstract">arXiv:2310.14337</a> [<a href="/pdf/2310.14337" title="Download PDF">pdf</a>, <a href="/format/2310.14337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PPFL: A Personalized Federated Learning Framework for Heterogeneous  Population
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di%2C+H">Hao Di</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haishan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiangyu Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Personalization aims to characterize individual preferences and is widely
applied across many fields. However, conventional personalized methods operate
in a centralized manner and potentially expose the raw data when pooling
individual information. In this paper, with privacy considerations, we develop
a flexible and interpretable personalized framework within the paradigm of
Federated Learning, called PPFL (Population Personalized Federated Learning).
By leveraging canonical models to capture fundamental characteristics among the
heterogeneous population and employing membership vectors to reveal clients'
preferences, it models the heterogeneity as clients' varying preferences for
these characteristics and provides substantial insights into client
characteristics, which is lacking in existing Personalized Federated Learning
(PFL) methods. Furthermore, we explore the relationship between our method and
three main branches of PFL methods: multi-task PFL, clustered FL, and
decoupling PFL, and demonstrate the advantages of PPFL. To solve PPFL (a
non-convex constrained optimization problem), we propose a novel random block
coordinate descent algorithm and present the convergence property. We conduct
experiments on both pathological and practical datasets, and the results
validate the effectiveness of PPFL.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14338" title="Abstract">arXiv:2310.14338</a> [<a href="/pdf/2310.14338" title="Download PDF">pdf</a>, <a href="/format/2310.14338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Chaos to Clarity: Claim Normalization to Empower Fact-Checking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sundriyal%2C+M">Megha Sundriyal</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the proliferation of social media platforms, users are exposed to vast
information, including posts containing misleading claims. However, the
pervasive noise inherent in these posts presents a challenge in identifying
precise and prominent claims that require verification. Extracting the core
assertions from such posts is arduous and time-consuming. We introduce a novel
task called Claim Normalization (aka ClaimNorm) that aims to decompose complex
and noisy social media posts into more straightforward and understandable
forms, termed normalized claims. We propose CACN, a pioneering approach that
leverages chain-of-thought and claim check-worthiness estimation, mimicking
human reasoning processes, to comprehend intricate claims. Moreover, we
capitalize on large language models' powerful in-context learning abilities to
provide guidance and improve the claim normalization process. To evaluate the
effectiveness of our proposed model, we meticulously compile a comprehensive
real-world dataset, CLAN, comprising more than 6k instances of social media
posts alongside their respective normalized claims. Experimentation
demonstrates that CACN outperforms several baselines across various evaluation
measures. A rigorous error analysis validates CACN's capabilities and pitfalls.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14339" title="Abstract">arXiv:2310.14339</a> [<a href="/pdf/2310.14339" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion Planning for Autonomous Ground Vehicles Using Artificial  Potential Fields: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rehman%2C+A+u">Aziz ur Rehman</a>, 
<a href="/search/cs?searchtype=author&query=Tanveer%2C+A">Ahsan Tanveer</a>, 
<a href="/search/cs?searchtype=author&query=Ashraf%2C+M+T">M. Touseef Ashraf</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+U">Umer Khan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 1st International Conference on Women Development in Engineering
  Science &amp; Technology, Jamshoro, Pakistan, October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Autonomous ground vehicle systems have found extensive potential and
practical applications in the modern world. The development of an autonomous
ground vehicle poses a significant challenge, particularly in identifying the
best path plan, based on defined performance metrics such as safety margin,
shortest time, and energy consumption. Various techniques for motion planning
have been proposed by researchers, one of which is the use of artificial
potential fields. Several authors in the past two decades have proposed various
modified versions of the artificial potential field algorithms. The variations
of the traditional APF approach have given an answer to prior shortcomings.
This gives potential rise to a strategic survey on the improved versions of
this algorithm. This study presents a review of motion planning for autonomous
ground vehicles using artificial potential fields. Each article is evaluated
based on criteria that involve the environment type, which may be either static
or dynamic, the evaluation scenario, which may be real-time or simulated, and
the method used for improving the search performance of the algorithm. All the
customized designs of planning models are analyzed and evaluated. At the end,
the results of the review are discussed, and future works are proposed.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14340" title="Abstract">arXiv:2310.14340</a> [<a href="/pdf/2310.14340" title="Download PDF">pdf</a>, <a href="/format/2310.14340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Commonsense-Guided Search Query Generation for Open-Domain  Knowledge-Powered Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reddy%2C+R+G">Revanth Gangi Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">Hao Bai</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wentao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+S+C+E">Sharath Chandra Etagi Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C">ChengXiang Zhai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Open-domain dialog involves generating search queries that help obtain
relevant knowledge for holding informative conversations. However, it can be
challenging to determine what information to retrieve when the user is passive
and does not express a clear need or request. To tackle this issue, we present
a novel approach that focuses on generating internet search queries that are
guided by social commonsense. Specifically, we leverage a commonsense dialog
system to establish connections related to the conversation topic, which
subsequently guides our query generation. Our proposed framework addresses
passive user interactions by integrating topic tracking, commonsense response
generation and instruction-driven query generation. Through extensive
evaluations, we show that our approach overcomes limitations of existing query
generation techniques that rely solely on explicit dialog information, and
produces search queries that are more relevant, specific, and compelling,
ultimately resulting in more engaging responses.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14341" title="Abstract">arXiv:2310.14341</a> [<a href="/pdf/2310.14341" title="Download PDF">pdf</a>, <a href="/format/2310.14341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">YeXin Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Raw vision with fews of mistakes. Paper 6 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The Hidden Markov Model (HMM) can predict the future value of a time series
based on its current and previous values, making it a powerful algorithm for
handling various types of time series. Numerous studies have explored the
improvement of HMM using advanced techniques, leading to the development of
several variations of HMM. Despite these studies indicating the increased
competitiveness of HMM compared to other advanced algorithms, few have
recognized the significance and impact of incorporating multistep stochastic
states into its performance. In this work, we propose a Pyramidal Hidden Markov
Model (PHMM) that can capture multiple multistep stochastic states. Initially,
a multistep HMM is designed for extracting short multistep stochastic states.
Next, a novel time series forecasting structure is proposed based on PHMM,
which utilizes pyramid-like stacking to adaptively identify long multistep
stochastic states. By employing these two schemes, our model can effectively
handle non-stationary and noisy data, while also establishing long-term
dependencies for more accurate and comprehensive forecasting. The experimental
results on diverse multivariate time series datasets convincingly demonstrate
the superior performance of our proposed PHMM compared to its competitive peers
in time series forecasting.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14342" title="Abstract">arXiv:2310.14342</a> [<a href="/pdf/2310.14342" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PulmoBell: Home-based Pulmonary Rehabilitation Assistive Technology for  People with COPD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuanxiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Polydorides%2C+A">Andreas Polydorides</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+J">Jitesh Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Youngjun Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short Technical Report: Best student-led project in COMP0145: Research Methods and Making Skills (2022/23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Chronic Obstructive Pulmonary Disease (COPD) can be fatal and is challenging
to live with due to its severe symptoms. Pulmonary rehabilitation (PR) is one
of the managements means to maintain COPD in a stable status. However,
implementation of PR in the UK has been challenging due to the environmental
and personal barriers faced by patients, which hinder their uptake, adherence,
and completion of the programmes. Moreover, increased exercise capacity
following PR does not always translate into physical activity (PA) and
unfortunately, can lead back to exercise capacity seen prior to PR. Current
alternative solutions using telerehabilitation methods have limitations on
addressing these accessibility problems, and no clear conclusion can be drawn
on the efficacy of telerehabilitation in enhancing the sustainability of PR
outcomes via promoting PA in patients' everyday life. In this work, the authors
propose a novel design of sensor-based assistive product with the aim of
facilitating PR and promoting PA maintenance in a home-based setting.
Prototypes of different levels of fidelity are presented, followed by an
evaluation plan for future research directions.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14343" title="Abstract">arXiv:2310.14343</a> [<a href="/pdf/2310.14343" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Artistic Visualization of Physiological Signals for  Mindfulness and Relaxation: A Pilot Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zihan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Youngjun Cho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Mindfulness and relaxation techniques for mental health are increasingly
being explored in the human-computer interaction community. Physiological
signals and their visualization have often been exploited together in a form of
biofeedback with other intervention methods. Here, we aim to contribute to the
body of existing work on biofeedback interfaces for mindfulness, with a
particular focus on incorporating artistic effects into physiological signal
visualization. With an implemented artistic biofeedback interface, we conduct a
pilot study where 10 participants attend stress-induction sessions followed by
two biofeedback mindfulness sessions: classic biofeedback and artistic
visualization. The result demonstrates that artistic visualization-driven
biofeedback significantly improves the effectiveness of biofeedback in helping
users feel relaxed in comparison with a classic graphical form of biofeedback.
Also, it shows that the artistic effect makes it easy to understand what
biofeedback represents. Future work includes exploring how advanced
physiological computing methods can improve its efficiency and performance.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14344" title="Abstract">arXiv:2310.14344</a> [<a href="/pdf/2310.14344" title="Download PDF">pdf</a>, <a href="/format/2310.14344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s in a Prior? Learned Proximal Networks for Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhenghan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Buchanan%2C+S">Sam Buchanan</a>, 
<a href="/search/cs?searchtype=author&query=Sulam%2C+J">Jeremias Sulam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Proximal operators are ubiquitous in inverse problems, commonly appearing as
part of algorithmic strategies to regularize problems that are otherwise
ill-posed. Modern deep learning models have been brought to bear for these
tasks too, as in the framework of plug-and-play or deep unrolling, where they
loosely resemble proximal operators. Yet, something essential is lost in
employing these purely data-driven approaches: there is no guarantee that a
general deep network represents the proximal operator of any function, nor is
there any characterization of the function for which the network might provide
some approximate proximal. This not only makes guaranteeing convergence of
iterative schemes challenging but, more fundamentally, complicates the analysis
of what has been learned by these networks about their training data. Herein we
provide a framework to develop learned proximal networks (LPN), prove that they
provide exact proximal operators for a data-driven nonconvex regularizer, and
show how a new training strategy, dubbed proximal matching, provably promotes
the recovery of the log-prior of the true data distribution. Such LPN provide
general, unsupervised, expressive proximal operators that can be used for
general inverse problems with convergence guarantees. We illustrate our results
in a series of cases of increasing complexity, demonstrating that these models
not only result in state-of-the-art performance, but provide a window into the
resulting priors learned from data.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14346" title="Abstract">arXiv:2310.14346</a> [<a href="/pdf/2310.14346" title="Download PDF">pdf</a>, <a href="/format/2310.14346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Law and NLP: Bridging Disciplinary Disconnects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahari%2C+R">Robert Mahari</a>, 
<a href="/search/cs?searchtype=author&query=Stammbach%2C+D">Dominik Stammbach</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Pentland%2C+A+%27">Alex &#x27;Sandy&#x27; Pentland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Legal practice is intrinsically rooted in the fabric of language, yet legal
practitioners and scholars have been slow to adopt tools from natural language
processing (NLP). At the same time, the legal system is experiencing an access
to justice crisis, which could be partially alleviated with NLP. In this
position paper, we argue that the slow uptake of NLP in legal practice is
exacerbated by a disconnect between the needs of the legal community and the
focus of NLP researchers. In a review of recent trends in the legal NLP
literature, we find limited overlap between the legal NLP community and legal
academia. Our interpretation is that some of the most popular legal NLP tasks
fail to address the needs of legal practitioners. We discuss examples of legal
NLP tasks that promise to bridge disciplinary disconnects and highlight
interesting areas for legal NLP research that remain underexplored.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14347" title="Abstract">arXiv:2310.14347</a> [<a href="/pdf/2310.14347" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Assistant: Portable Progressive Muscle Relaxation Training  Interface for Anxiety Reduction in Office Workers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lingqian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Katherine Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Youngjun Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short Technical Report: Best student-led project in COMP0145: Research Methods and Making Skills (2022/23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Workload often triggers anxiety for office workers. While a variety of stress
intervention and management techniques have been explored, there exist only a
few of portable tangible interfaces for anxiety reduction. Contributing to the
body of work, we introduce Self-Assistant, a portable anxiety intervention
training interface. This is based on progressive muscle relaxation training
which is an effective way for reducing stress and raise awareness of tension,
and provide feelings of deep relaxation. This interface incorporates a stress
ball with a pressure sensor and visual indicators which help track the user's
anxiety level through squeeze actions. This is designed to help improve
self-awareness of anxiety and stress, in turn self-regulating bodily functions,
offering rapid and low-sensory stimulus training for anxiety relief. Finally,
we discuss application scenarios and future work with Self-Assistant.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14348" title="Abstract">arXiv:2310.14348</a> [<a href="/pdf/2310.14348" title="Download PDF">pdf</a>, <a href="/format/2310.14348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DePAint: A Decentralized Safe Multi-Agent Reinforcement Learning  Algorithm considering Peak and Average Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassan%2C+R">Raheeb Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Wadith%2C+K+M+S">K.M. Shadman Wadith</a>, 
<a href="/search/cs?searchtype=author&query=Rashid%2C+M+M+o">Md. Mamun or Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+M">Md. Mosaddek Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The field of safe multi-agent reinforcement learning, despite its potential
applications in various domains such as drone delivery and vehicle automation,
remains relatively unexplored. Training agents to learn optimal policies that
maximize rewards while considering specific constraints can be challenging,
particularly in scenarios where having a central controller to coordinate the
agents during the training process is not feasible. In this paper, we address
the problem of multi-agent policy optimization in a decentralized setting,
where agents communicate with their neighbors to maximize the sum of their
cumulative rewards while also satisfying each agent's safety constraints. We
consider both peak and average constraints. In this scenario, there is no
central controller coordinating the agents and both the rewards and constraints
are only known to each agent locally/privately. We formulate the problem as a
decentralized constrained multi-agent Markov Decision Problem and propose a
momentum-based decentralized policy gradient method, DePAint, to solve it. To
the best of our knowledge, this is the first privacy-preserving fully
decentralized multi-agent reinforcement learning algorithm that considers both
peak and average constraints. We also provide theoretical analysis and
empirical evaluation of our algorithm in various scenarios and compare its
performance to centralized algorithms that consider similar constraints.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14351" title="Abstract">arXiv:2310.14351</a> [<a href="/pdf/2310.14351" title="Download PDF">pdf</a>, <a href="/format/2310.14351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonasymptotic Convergence Rate of Quasi-Monte Carlo: Applications to  Linear Elliptic PDEs with Lognormal Coefficients and Importance Samplings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/math?searchtype=author&query=Tempone%2C+R">Ra&#xfa;l Tempone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This study analyzes the nonasymptotic convergence behavior of the quasi-Monte
Carlo (QMC) method with applications to linear elliptic partial differential
equations (PDEs) with lognormal coefficients. Building upon the error analysis
presented in (Owen, 2006), we derive a nonasymptotic convergence estimate
depending on the specific integrands, the input dimensionality, and the finite
number of samples used in the QMC quadrature. We discuss the effects of the
variance and dimensionality of the input random variable. Then, we apply the
QMC method with importance sampling (IS) to approximate deterministic,
real-valued, bounded linear functionals that depend on the solution of a linear
elliptic PDE with a lognormal diffusivity coefficient in bounded domains of
$\mathbb{R}^d$, where the random coefficient is modeled as a stationary
Gaussian random field parameterized by the trigonometric and wavelet-type
basis. We propose two types of IS distributions, analyze their effects on the
QMC convergence rate, and observe the improvements.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14355" title="Abstract">arXiv:2310.14355</a> [<a href="/pdf/2310.14355" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A global product of fine-scale urban building height based on spaceborne  lidar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Moskal%2C+L+M">L. Monika Moskal</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+P">Peng Gong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qinghua Guo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huabing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuecao Li</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Huan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bailang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuyu Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Characterizing urban environments with broad coverages and high precision is
more important than ever for achieving the UN's Sustainable Development Goals
(SDGs) as half of the world's populations are living in cities. Urban building
height as a fundamental 3D urban structural feature has far-reaching
applications. However, so far, producing readily available datasets of recent
urban building heights with fine spatial resolutions and global coverages
remains a challenging task. Here, we provide an up-to-date global product of
urban building heights based on a fine grid size of 150 m around 2020 by
combining the spaceborne lidar instrument of GEDI and multi-sourced data
including remotely sensed images (i.e., Landsat-8, Sentinel-2, and Sentinel-1)
and topographic data. Our results revealed that the estimated method of
building height samples based on the GEDI data was effective with 0.78 of
Pearson's r and 3.67 m of RMSE in comparison to the reference data. The mapping
product also demonstrated good performance as indicated by its strong
correlation with the reference data (i.e., Pearson's r = 0.71, RMSE = 4.60 m).
Compared with the currently existing products, our global urban building height
map holds the ability to provide a higher spatial resolution (i.e., 150 m) with
a great level of inherent details about the spatial heterogeneity and
flexibility of updating using the GEDI samples as inputs. This work will boost
future urban studies across many fields including climate, environmental,
ecological, and social sciences.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14356" title="Abstract">arXiv:2310.14356</a> [<a href="/pdf/2310.14356" title="Download PDF">pdf</a>, <a href="/format/2310.14356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cultural and Linguistic Diversity Improves Visual Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+A">Andre Ye</a>, 
<a href="/search/cs?searchtype=author&query=Santy%2C+S">Sebastin Santy</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J+D">Jena D. Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A+X">Amy X. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Computer vision often treats perception as objective, and this assumption
gets reflected in the way that datasets are collected and models are trained.
For instance, image descriptions in different languages are typically assumed
to be translations of the same semantic content. However, work in
cross-cultural psychology and linguistics has shown that individuals differ in
their visual perception depending on their cultural background and the language
they speak. In this paper, we demonstrate significant differences in semantic
content across languages in both dataset and model-produced captions. When data
is multilingual as opposed to monolingual, captions have higher semantic
coverage on average, as measured by scene graph, embedding, and linguistic
complexity. For example, multilingual captions have on average 21.8% more
objects, 24.5% more relations, and 27.1% more attributes than a set of
monolingual captions. Moreover, models trained on content from different
languages perform best against test data from those languages, while those
trained on multilingual content perform consistently well across all evaluation
data compositions. Our research provides implications for how diverse modes of
perception can improve image understanding.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14358" title="Abstract">arXiv:2310.14358</a> [<a href="/pdf/2310.14358" title="Download PDF">pdf</a>, <a href="/format/2310.14358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Right, No Matter Why: AI Fact-checking and AI Authority in  Health-related Inquiry Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sergeeva%2C+E">Elena Sergeeva</a>, 
<a href="/search/cs?searchtype=author&query=Sergeeva%2C+A">Anastasia Sergeeva</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huiyun Tang</a>, 
<a href="/search/cs?searchtype=author&query=Bongard-Blanchy%2C+K">Kerstin Bongard-Blanchy</a>, 
<a href="/search/cs?searchtype=author&query=Szolovits%2C+P">Peter Szolovits</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Previous research on expert advice-taking shows that humans exhibit two
contradictory behaviors: on the one hand, people tend to overvalue their own
opinions undervaluing the expert opinion, and on the other, people often defer
to other people's advice even if the advice itself is rather obviously wrong.
In our study, we conduct an exploratory evaluation of users' AI-advice
accepting behavior when evaluating the truthfulness of a health-related
statement in different "advice quality" settings. We find that even feedback
that is confined to just stating that "the AI thinks that the statement is
false/true" results in more than half of people moving their statement veracity
assessment towards the AI suggestion. The different types of advice given
influence the acceptance rates, but the sheer effect of getting a suggestion is
often bigger than the suggestion-type effect.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14360" title="Abstract">arXiv:2310.14360</a> [<a href="/pdf/2310.14360" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT a game changer for geocoding -- a benchmark for geocoding  address parsing techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhengcong Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Diya Li</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+D+W">Daniel W. Goldberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The remarkable success of GPT models across various tasks, including toponymy
recognition motivates us to assess the performance of the GPT-3 model in the
geocoding address parsing task. To ensure that the evaluation more accurately
mirrors performance in real-world scenarios with diverse user input qualities
and resolve the pressing need for a 'gold standard' evaluation dataset for
geocoding systems, we introduce a benchmark dataset of low-quality address
descriptions synthesized based on human input patterns mining from actual input
logs of a geocoding system in production. This dataset has 21 different input
errors and variations; contains over 239,000 address records that are uniquely
selected from streets across all U.S. 50 states and D.C.; and consists of three
subsets to be used as training, validation, and testing sets. Building on this,
we train and gauge the performance of the GPT-3 model in extracting address
components, contrasting its performance with transformer-based and LSTM-based
models. The evaluation results indicate that Bidirectional LSTM-CRF model has
achieved the best performance over these transformer-based models and GPT-3
model. Transformer-based models demonstrate very comparable results compared to
the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in
performance, showcases potential in the address parsing task with few-shot
examples, exhibiting room for improvement with additional fine-tuning. We open
source the code and data of this presented benchmark so that researchers can
utilize it for future model development or extend it to evaluate similar tasks,
such as document geocoding.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14364" title="Abstract">arXiv:2310.14364</a> [<a href="/pdf/2310.14364" title="Download PDF">pdf</a>, <a href="/format/2310.14364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantitative Evaluation of Dense 3D Reconstruction of Sinus Anatomy  from Monocular Endoscopic Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mangulabnan%2C+J+E">Jan Emily Mangulabnan</a>, 
<a href="/search/cs?searchtype=author&query=Soberanis-Mukul%2C+R+D">Roger D. Soberanis-Mukul</a>, 
<a href="/search/cs?searchtype=author&query=Teufel%2C+T">Timo Teufel</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez%2C+I">Isabela Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Winter%2C+J">Jonas Winter</a>, 
<a href="/search/cs?searchtype=author&query=Sahu%2C+M">Manish Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Porras%2C+J+L">Jose L. Porras</a>, 
<a href="/search/cs?searchtype=author&query=Vedula%2C+S+S">S. Swaroop Vedula</a>, 
<a href="/search/cs?searchtype=author&query=Ishii%2C+M">Masaru Ishii</a>, 
<a href="/search/cs?searchtype=author&query=Hager%2C+G">Gregory Hager</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+R+H">Russell H. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Unberath%2C+M">Mathias Unberath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Generating accurate 3D reconstructions from endoscopic video is a promising
avenue for longitudinal radiation-free analysis of sinus anatomy and surgical
outcomes. Several methods for monocular reconstruction have been proposed,
yielding visually pleasant 3D anatomical structures by retrieving relative
camera poses with structure-from-motion-type algorithms and fusion of monocular
depth estimates. However, due to the complex properties of the underlying
algorithms and endoscopic scenes, the reconstruction pipeline may perform
poorly or fail unexpectedly. Further, acquiring medical data conveys additional
challenges, presenting difficulties in quantitatively benchmarking these
models, understanding failure cases, and identifying critical components that
contribute to their precision. In this work, we perform a quantitative analysis
of a self-supervised approach for sinus reconstruction using endoscopic
sequences paired with optical tracking and high-resolution computed tomography
acquired from nine ex-vivo specimens. Our results show that the generated
reconstructions are in high agreement with the anatomy, yielding an average
point-to-mesh error of 0.91 mm between reconstructions and CT segmentations.
However, in a point-to-point matching scenario, relevant for endoscope tracking
and navigation, we found average target registration errors of 6.58 mm. We
identified that pose and depth estimation inaccuracies contribute equally to
this error and that locally consistent sequences with shorter trajectories
generate more accurate reconstructions. These results suggest that achieving
global consistency between relative camera poses and estimated depths with the
anatomy is essential. In doing so, we can ensure proper synergy between all
components of the pipeline for improved reconstructions that will facilitate
clinical application of this innovative technology.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14366" title="Abstract">arXiv:2310.14366</a> [<a href="/pdf/2310.14366" title="Download PDF">pdf</a>, <a href="/format/2310.14366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-Encoders based Species Normalization -- Pairwise Sentence Learning to  Rank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Awan%2C+Z">Zainab Awan</a>, 
<a href="/search/cs?searchtype=author&query=Kahlke%2C+T">Tim Kahlke</a>, 
<a href="/search/cs?searchtype=author&query=Ralph%2C+P">Peter Ralph</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+P">Paul Kennedy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Motivation: Biomedical named-entity normalization involves connecting
biomedical entities with distinct database identifiers in order to facilitate
data integration across various fields of biology. Existing systems for
biomedical named entity normalization heavily rely on dictionaries, manually
created rules, and high-quality representative features such as lexical or
morphological characteristics. However, recent research has investigated the
use of neural network-based models to reduce dependence on dictionaries,
manually crafted rules, and features. Despite these advancements, the
performance of these models is still limited due to the lack of sufficiently
large training datasets. These models have a tendency to overfit small training
corpora and exhibit poor generalization when faced with previously unseen
entities, necessitating the redesign of rules and features. Contribution: We
present a novel deep learning approach for named entity normalization, treating
it as a pair-wise learning to rank problem. Our method utilizes the widely-used
information retrieval algorithm Best Matching 25 to generate candidate
concepts, followed by the application of bi-directional encoder representation
from the encoder (BERT) to re-rank the candidate list. Notably, our approach
eliminates the need for feature-engineering or rule creation. We conduct
experiments on species entity types and evaluate our method against
state-of-the-art techniques using LINNAEUS and S800 biomedical corpora. Our
proposed approach surpasses existing methods in linking entities to the NCBI
taxonomy. To the best of our knowledge, there is no existing neural
network-based approach for species normalization in the literature.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14369" title="Abstract">arXiv:2310.14369</a> [<a href="/pdf/2310.14369" title="Download PDF">pdf</a>, <a href="/format/2310.14369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoPe: Model Perturbation-based Privacy Attacks on Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Marvin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jason Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jeffrey Wang</a>, 
<a href="/search/cs?searchtype=author&query=Neel%2C+S">Seth Neel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent work has shown that Large Language Models (LLMs) can unintentionally
leak sensitive information present in their training data. In this paper, we
present Model Perturbations (MoPe), a new method to identify with high
confidence if a given text is in the training data of a pre-trained language
model, given white-box access to the models parameters. MoPe adds noise to the
model in parameter space and measures the drop in log-likelihood at a given
point $x$, a statistic we show approximates the trace of the Hessian matrix
with respect to model parameters. Across language models ranging from $70$M to
$12$B parameters, we show that MoPe is more effective than existing loss-based
attacks and recently proposed perturbation-based methods. We also examine the
role of training point order and model size in attack success, and empirically
demonstrate that MoPe accurately approximate the trace of the Hessian in
practice. Our results show that the loss of a point alone is insufficient to
determine extractability -- there are training points we can recover using our
method that have average loss. This casts some doubt on prior works that use
the loss of a point as evidence of memorization or unlearning.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14374" title="Abstract">arXiv:2310.14374</a> [<a href="/pdf/2310.14374" title="Download PDF">pdf</a>, <a href="/format/2310.14374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OV-VG: A Benchmark for Open-Vocabulary Visual Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunlei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wenquan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangtai Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Guangliang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Shuchang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Binghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lijiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Open-vocabulary learning has emerged as a cutting-edge research area,
particularly in light of the widespread adoption of vision-based foundational
models. Its primary objective is to comprehend novel concepts that are not
encompassed within a predefined vocabulary. One key facet of this endeavor is
Visual Grounding, which entails locating a specific region within an image
based on a corresponding language description. While current foundational
models excel at various visual language tasks, there's a noticeable absence of
models specifically tailored for open-vocabulary visual grounding. This
research endeavor introduces novel and challenging OV tasks, namely
Open-Vocabulary Visual Grounding and Open-Vocabulary Phrase Localization. The
overarching aim is to establish connections between language descriptions and
the localization of novel objects. To facilitate this, we have curated a
comprehensive annotated benchmark, encompassing 7,272 OV-VG images and 1,000
OV-PL images. In our pursuit of addressing these challenges, we delved into
various baseline methodologies rooted in existing open-vocabulary object
detection, VG, and phrase localization frameworks. Surprisingly, we discovered
that state-of-the-art methods often falter in diverse scenarios. Consequently,
we developed a novel framework that integrates two critical components:
Text-Image Query Selection and Language-Guided Feature Attention. These modules
are designed to bolster the recognition of novel categories and enhance the
alignment between visual and linguistic information. Extensive experiments
demonstrate the efficacy of our proposed framework, which consistently attains
SOTA performance across the OV-VG task. Additionally, ablation studies provide
further evidence of the effectiveness of our innovative models. Codes and
datasets will be made publicly available at https://github.com/cv516Buaa/OV-VG.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14379" title="Abstract">arXiv:2310.14379</a> [<a href="/pdf/2310.14379" title="Download PDF">pdf</a>, <a href="/ps/2310.14379" title="Download PostScript">ps</a>, <a href="/format/2310.14379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Metrics for Evaluating Explanation Goals in Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zanon%2C+A+L">Andr&#xe9; Levi Zanon</a>, 
<a href="/search/cs?searchtype=author&query=Manzato%2C+M+G">Marcelo Garcia Manzato</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+L">Leonardo Rocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Explanations are crucial for improving users' transparency, persuasiveness,
engagement, and trust in Recommender Systems (RSs). However, evaluating the
effectiveness of explanation algorithms regarding those goals remains
challenging due to existing offline metrics' limitations. This paper introduces
new metrics for the evaluation and validation of explanation algorithms based
on the items and properties used to form the sentence of an explanation.
Towards validating the metrics, the results of three state-of-the-art post-hoc
explanation algorithms were evaluated for six RSs, comparing the offline
metrics results with those of an online user study. The findings show the
proposed offline metrics can effectively measure the performance of explanation
algorithms and highlight a trade-off between the goals of transparency and
trust, which are related to popular properties, and the goals of engagement and
persuasiveness, which are associated with the diversification of properties
displayed to users. Furthermore, the study contributes to the development of
more robust evaluation methods for explanation algorithms in RSs.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14384" title="Abstract">arXiv:2310.14384</a> [<a href="/pdf/2310.14384" title="Download PDF">pdf</a>, <a href="/format/2310.14384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARCOQ: Arabic Closest Opposite Questions Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rizkallah%2C+S">Sandra Rizkallah</a>, 
<a href="/search/cs?searchtype=author&query=Atiya%2C+A+F">Amir F. Atiya</a>, 
<a href="/search/cs?searchtype=author&query=Shaheen%2C+S">Samir Shaheen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a dataset for closest opposite questions in Arabic
language. The dataset is the first of its kind for the Arabic language. It is
beneficial for the assessment of systems on the aspect of antonymy detection.
The structure is similar to that of the Graduate Record Examination (GRE)
closest opposite questions dataset for the English language. The introduced
dataset consists of 500 questions, each contains a query word for which the
closest opposite needs to be determined from among a set of candidate words.
Each question is also associated with the correct answer. We publish the
dataset publicly in addition to providing standard splits of the dataset into
development and test sets. Moreover, the paper provides a benchmark for the
performance of different Arabic word embedding models on the introduced
dataset.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14386" title="Abstract">arXiv:2310.14386</a> [<a href="/pdf/2310.14386" title="Download PDF">pdf</a>, <a href="/format/2310.14386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Generalizable Manipulation Policies with Object-Centric 3D  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhenyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 7th Annual Conference on Robot Learning (CoRL), 2023 in Atlanta, US
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce GROOT, an imitation learning method for learning robust policies
with object-centric and 3D priors. GROOT builds policies that generalize beyond
their initial training conditions for vision-based manipulation. It constructs
object-centric 3D representations that are robust toward background changes and
camera views and reason over these representations using a transformer-based
policy. Furthermore, we introduce a segmentation correspondence model that
allows policies to generalize to new objects at test time. Through
comprehensive experiments, we validate the robustness of GROOT policies against
perceptual variations in simulated and real-world environments. GROOT's
performance excels in generalization over background changes, camera viewpoint
shifts, and the presence of new object instances, whereas both state-of-the-art
end-to-end learning methods and object proposal-based approaches fall short. We
also extensively evaluate GROOT policies on real robots, where we demonstrate
the efficacy under very wild changes in setup. More videos and model details
can be found in the appendix and the project website:
https://ut-austin-rpl.github.io/GROOT .
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14389" title="Abstract">arXiv:2310.14389</a> [<a href="/pdf/2310.14389" title="Download PDF">pdf</a>, <a href="/format/2310.14389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Subjective Cognitive Appraisals of Emotions from Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Hongli Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+D+C">Desmond C. Ong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings) Camera-Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The emotions we experience involve complex processes; besides physiological
aspects, research in psychology has studied cognitive appraisals where people
assess their situations subjectively, according to their own values (Scherer,
2005). Thus, the same situation can often result in different emotional
experiences. While the detection of emotion is a well-established task, there
is very limited work so far on the automatic prediction of cognitive
appraisals. This work fills the gap by presenting CovidET-Appraisals, the most
comprehensive dataset to-date that assesses 24 appraisal dimensions, each with
a natural language rationale, across 241 Reddit posts. CovidET-Appraisals
presents an ideal testbed to evaluate the ability of large language models --
excelling at a wide range of NLP tasks -- to automatically assess and explain
cognitive appraisals. We found that while the best models are performant,
open-sourced LLMs fall short at this task, presenting a new challenge in the
future development of emotionally intelligent models. We release our dataset at
https://github.com/honglizhan/CovidET-Appraisals-Public.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14390" title="Abstract">arXiv:2310.14390</a> [<a href="/pdf/2310.14390" title="Download PDF">pdf</a>, <a href="/format/2310.14390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Domain HAR: Few Shot Transfer Learning for Human Activity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thukral%2C+M">Megha Thukral</a>, 
<a href="/search/cs?searchtype=author&query=Haresamudram%2C+H">Harish Haresamudram</a>, 
<a href="/search/cs?searchtype=author&query=Ploetz%2C+T">Thomas Ploetz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The ubiquitous availability of smartphones and smartwatches with integrated
inertial measurement units (IMUs) enables straightforward capturing of human
activities. For specific applications of sensor based human activity
recognition (HAR), however, logistical challenges and burgeoning costs render
especially the ground truth annotation of such data a difficult endeavor,
resulting in limited scale and diversity of datasets. Transfer learning, i.e.,
leveraging publicly available labeled datasets to first learn useful
representations that can then be fine-tuned using limited amounts of labeled
data from a target domain, can alleviate some of the performance issues of
contemporary HAR systems. Yet they can fail when the differences between source
and target conditions are too large and/ or only few samples from a target
application domain are available, each of which are typical challenges in
real-world human activity recognition scenarios. In this paper, we present an
approach for economic use of publicly available labeled HAR datasets for
effective transfer learning. We introduce a novel transfer learning framework,
Cross-Domain HAR, which follows the teacher-student self-training paradigm to
more effectively recognize activities with very limited label information. It
bridges conceptual gaps between source and target domains, including sensor
locations and type of activities. Through our extensive experimental evaluation
on a range of benchmark datasets, we demonstrate the effectiveness of our
approach for practically relevant few shot activity recognition scenarios. We
also present a detailed analysis into how the individual components of our
framework affect downstream performance.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14391" title="Abstract">arXiv:2310.14391</a> [<a href="/pdf/2310.14391" title="Download PDF">pdf</a>, <a href="/ps/2310.14391" title="Download PostScript">ps</a>, <a href="/format/2310.14391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance bounds for Reduced Order Models with Application to  Parametric Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rim%2C+D">D. Rim</a>, 
<a href="/search/math?searchtype=author&query=Welper%2C+G">G. Welper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The Kolmogorov $n$-width is an established benchmark to judge the performance
of reduced basis and similar methods that produce linear reduced spaces.
Although immensely successful in the elliptic regime, this width, shows
unsatisfactory slow convergence rates for transport dominated problems. While
this has triggered a large amount of work on nonlinear model reduction
techniques, we are lacking a benchmark to evaluate their optimal performance.
<br />Nonlinear benchmarks like manifold/stable/Lipschitz width applied to the
solution manifold are often trivial if the degrees of freedom exceed the
parameter dimension and ignore desirable structure as offline/online
decompositions. In this paper, we show that the same benchmarks applied to the
full reduced order model pipeline from PDE to parametric quantity of interest
provide non-trivial benchmarks and we prove lower bounds for transport
equations.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14393" title="Abstract">arXiv:2310.14393</a> [<a href="/pdf/2310.14393" title="Download PDF">pdf</a>, <a href="/format/2310.14393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Merging Generated and Retrieved Knowledge for Open-Domain QA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khalifa%2C+M">Muhammad Khalifa</a>, 
<a href="/search/cs?searchtype=author&query=Logeswaran%2C+L">Lajanugen Logeswaran</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Moontae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 - Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Open-domain question answering (QA) systems are often built with retrieval
modules. However, retrieving passages from a given source is known to suffer
from insufficient knowledge coverage. Alternatively, prompting large language
models (LLMs) to generate contextual passages based on their parametric
knowledge has been shown to improve QA performance. Yet, LLMs tend to
"hallucinate" content that conflicts with the retrieved knowledge. Based on the
intuition that answers supported by both sources are more likely to be correct,
we propose COMBO, a Compatibility-Oriented knowledge Merging for Better
Open-domain QA framework, to effectively leverage the two sources of
information. Concretely, we match LLM-generated passages with retrieved
counterparts into compatible pairs, based on discriminators trained with silver
compatibility labels. Then a Fusion-in-Decoder-based reader model handles
passage pairs to arrive at the final answer. Experiments show that COMBO
outperforms competitive baselines on three out of four tested open-domain QA
benchmarks. Further analysis reveals that our proposed framework demonstrates
greater efficacy in scenarios with a higher degree of knowledge conflicts.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14394" title="Abstract">arXiv:2310.14394</a> [<a href="/pdf/2310.14394" title="Download PDF">pdf</a>, <a href="/format/2310.14394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks are Integrable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yucong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this study, we explore the integration of neural networks, a potent class
of functions known for their exceptional approximation capabilities. Our
primary emphasis is on the integration of multi-layer neural networks, a
challenging task within this domain. To tackle this challenge, we introduce a
novel numerical method that combines the forward algorithm with a corrective
procedure. Our experimental results demonstrate the accuracy achieved through
our integration approach.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14398" title="Abstract">arXiv:2310.14398</a> [<a href="/pdf/2310.14398" title="Download PDF">pdf</a>, <a href="/format/2310.14398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to bag with a simulation-free reinforcement learning framework  for robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Munguia-Galeano%2C+F">Francisco Munguia-Galeano</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jihong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez%2C+J+D">Juan David Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ze Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IET Cyber-Systems and Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Bagging is an essential skill that humans perform in their daily activities.
However, deformable objects, such as bags, are complex for robots to
manipulate. This paper presents an efficient learning-based framework that
enables robots to learn bagging. The novelty of this framework is its ability
to perform bagging without relying on simulations. The learning process is
accomplished through a reinforcement learning algorithm introduced in this
work, designed to find the best grasping points of the bag based on a set of
compact state representations. The framework utilizes a set of primitive
actions and represents the task in five states. In our experiments, the
framework reaches a 60 % and 80 % of success rate after around three hours of
training in the real world when starting the bagging task from folded and
unfolded, respectively. Finally, we test the trained model with two more bags
of different sizes to evaluate its generalizability.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14400" title="Abstract">arXiv:2310.14400</a> [<a href="/pdf/2310.14400" title="Download PDF">pdf</a>, <a href="/format/2310.14400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Pytorch Reproduction of Masked Generative Image Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Besnier%2C+V">Victor Besnier</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mickael Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this technical report, we present a reproduction of MaskGIT: Masked
Generative Image Transformer, using PyTorch. The approach involves leveraging a
masked bidirectional transformer architecture, enabling image generation with
only few steps (8~16 steps) for 512 x 512 resolution images, i.e., ~64x faster
than an auto-regressive approach. Through rigorous experimentation and
optimization, we achieved results that closely align with the findings
presented in the original paper. We match the reported FID of 7.32 with our
replication and obtain 7.59 with similar hyperparameters on ImageNet at
resolution 512 x 512. Moreover, we improve over the official implementation
with some minor hyperparameter tweaking, achieving FID of 7.26. At the lower
resolution of 256 x 256 pixels, our reimplementation scores 6.80, in comparison
to the original paper's 6.18. To promote further research on Masked Generative
Models and facilitate their reproducibility, we released our code and
pre-trained weights openly at https://github.com/valeoai/MaskGIT-pytorch/
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14402" title="Abstract">arXiv:2310.14402</a> [<a href="/pdf/2310.14402" title="Download PDF">pdf</a>, <a href="/format/2310.14402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Value of Assistance for Grasping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masarwy%2C+M">Mohammad Masarwy</a>, 
<a href="/search/cs?searchtype=author&query=Goshen%2C+Y">Yuval Goshen</a>, 
<a href="/search/cs?searchtype=author&query=Dovrat%2C+D">David Dovrat</a>, 
<a href="/search/cs?searchtype=author&query=Keren%2C+S">Sarah Keren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In many realistic settings, a robot is tasked with grasping an object without
knowing its exact pose. Instead, the robot relies on a probabilistic estimation
of the pose to decide how to attempt the grasp. We offer a novel Value of
Assistance (VOA) measure for assessing the expected effect a specific
observation will have on the robot's ability to successfully complete the
grasp. Thus, VOA supports the decision of which sensing action would be most
beneficial to the grasping task. We evaluate our suggested measures in both
simulated and real-world robotic settings.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14403" title="Abstract">arXiv:2310.14403</a> [<a href="/pdf/2310.14403" title="Download PDF">pdf</a>, <a href="/format/2310.14403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> O3D: Offline Data-driven Discovery and Distillation for Sequential  Decision-Making with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yuchen Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Madhushani%2C+U">Udari Madhushani</a>, 
<a href="/search/cs?searchtype=author&query=Vann%2C+J">Jared Vann</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+D">Deepeka Garg</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+S">Sumitra Ganesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent advancements in large language models (LLMs) have exhibited promising
performance in solving sequential decision-making problems. By imitating
few-shot examples provided in the prompts (i.e., in-context learning), an LLM
agent can interact with an external environment and complete given tasks
without additional training. However, such few-shot examples are often
insufficient to generate high-quality solutions for complex and long-horizon
tasks, while the limited context length cannot consume larger-scale
demonstrations. To this end, we propose an offline learning framework that
utilizes offline data at scale (e.g, logs of human interactions) to facilitate
the in-context learning performance of LLM agents. We formally define
LLM-powered policies with both text-based approaches and code-based approaches.
We then introduce an Offline Data-driven Discovery and Distillation (O3D)
framework to improve LLM-powered policies without finetuning. O3D automatically
discovers reusable skills and distills generalizable knowledge across multiple
tasks based on offline interaction data, advancing the capability of solving
downstream tasks. Empirical results under two interactive decision-making
benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the
decision-making capabilities of LLMs through the offline discovery and
distillation process, and consistently outperform baselines across various LLMs
with both text-based-policy and code-based-policy.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14404" title="Abstract">arXiv:2310.14404</a> [<a href="/pdf/2310.14404" title="Download PDF">pdf</a>, <a href="/format/2310.14404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Be Selfish, But Wisely: Investigating the Impact of Agent Personality in  Mixed-Motive Human-Agent Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chawla%2C+K">Kushal Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+I">Ian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yu Rong</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+G+M">Gale M. Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Gratch%2C+J">Jonathan Gratch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A natural way to design a negotiation dialogue system is via self-play RL:
train an agent that learns to maximize its performance by interacting with a
simulated user that has been designed to imitate human-human dialogue data.
Although this procedure has been adopted in prior work, we find that it results
in a fundamentally flawed system that fails to learn the value of compromise in
a negotiation, which can often lead to no agreements (i.e., the partner walking
away without a deal), ultimately hurting the model's overall performance. We
investigate this observation in the context of the DealOrNoDeal task, a
multi-issue negotiation over books, hats, and balls. Grounded in negotiation
theory from Economics, we modify the training procedure in two novel ways to
design agents with diverse personalities and analyze their performance with
human partners. We find that although both techniques show promise, a selfish
agent, which maximizes its own performance while also avoiding walkaways,
performs superior to other variants by implicitly learning to generate value
for both itself and the negotiation partner. We discuss the implications of our
findings for what it means to be a successful negotiation dialogue system and
how these systems should be designed in the future.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14406" title="Abstract">arXiv:2310.14406</a> [<a href="/pdf/2310.14406" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Urban wireless traffic evolution: the role of new devices and the effect  of policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benseny%2C+J">Jaume Benseny</a>, 
<a href="/search/cs?searchtype=author&query=Lahteenmaki%2C+J">Jarno Lahteenmaki</a>, 
<a href="/search/cs?searchtype=author&query=Toyli%2C+J">Juuso Toyli</a>, 
<a href="/search/cs?searchtype=author&query=Hammainen%2C+H">Heikki Hammainen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Telecommunications Policy, 47(7), 102595 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">The emergence of new wireless technologies, such as the Internet of Things,
allows digitalizing new and diverse urban activities. Thus, wireless traffic
grows in volume and complexity, making prediction, investment planning, and
regulation increasingly difficult. This article characterizes urban wireless
traffic evolution, supporting operators to drive mobile network evolution and
policymakers to increase national and local competitiveness. We propose a
holistic method that widens previous research scope, including new devices and
the effect of policy from multiple government levels. We provide an analytical
formulation that combines existing complementary methods on traffic evolution
research and diverse data sources. Results for a centric area of Helsinki
during 2020-2030 indicate that daily volumes increase, albeit a surprisingly
large part of the traffic continues to be generated by smartphones. Machine
traffic gains importance, driven by surveillance video cameras and connected
cars. While camera traffic is sensitive to law enforcement policies and data
regulation, car traffic is less affected by transport electrification policy.
High-priority traffic remains small, even under encouraging autonomous vehicle
policies. We suggest that 5G small cells might be needed around 2025, albeit
the utilization of novel radio technology and additional mid-band spectrum
could delay this need until 2029. We argue that mobile network operators
inevitably need to cooperate in constructing a single, shared small cell
network to mitigate the high deployment costs of massively deploying small
cells. We also provide guidance to local and national policymakers for
IoT-enabled competitive gains via the mitigation of five bottlenecks. For
example, local monopolies for mmWave connectivity should be facilitated on
space-limited urban furniture or risk an eventual capacity crunch, slowing down
digitalization.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14408" title="Abstract">arXiv:2310.14408</a> [<a href="/pdf/2310.14408" title="Download PDF">pdf</a>, <a href="/format/2310.14408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaRaDe: Passage Ranking using Demonstrations with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drozdov%2C+A">Andrew Drozdov</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Honglei Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Z">Zhuyun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+R">Razieh Rahimi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuanhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+D">Dana Alon</a>, 
<a href="/search/cs?searchtype=author&query=Iyyer%2C+M">Mohit Iyyer</a>, 
<a href="/search/cs?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>, 
<a href="/search/cs?searchtype=author&query=Metzler%2C+D">Donald Metzler</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+K">Kai Hui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recent studies show that large language models (LLMs) can be instructed to
effectively perform zero-shot passage re-ranking, in which the results of a
first stage retrieval method, such as BM25, are rated and reordered to improve
relevance. In this work, we improve LLM-based re-ranking by algorithmically
selecting few-shot demonstrations to include in the prompt. Our analysis
investigates the conditions where demonstrations are most helpful, and shows
that adding even one demonstration is significantly beneficial. We propose a
novel demonstration selection strategy based on difficulty rather than the
commonly used semantic similarity. Furthermore, we find that demonstrations
helpful for ranking are also effective at question generation. We hope our work
will spur more principled research into question generation and passage
ranking.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14413" title="Abstract">arXiv:2310.14413</a> [<a href="/pdf/2310.14413" title="Download PDF">pdf</a>, <a href="/format/2310.14413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation: a Combined Inductive-Deductive Approach featuring  Answer Set Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruno%2C+P">Pierangela Bruno</a>, 
<a href="/search/cs?searchtype=author&query=Calimeri%2C+F">Francesco Calimeri</a>, 
<a href="/search/cs?searchtype=author&query=Marte%2C+C">Cinzia Marte</a>, 
<a href="/search/cs?searchtype=author&query=Perri%2C+S">Simona Perri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Although the availability of a large amount of data is usually given for
granted, there are relevant scenarios where this is not the case; for instance,
in the biomedical/healthcare domain, some applications require to build huge
datasets of proper images, but the acquisition of such images is often hard for
different reasons (e.g., accessibility, costs, pathology-related variability),
thus causing limited and usually imbalanced datasets. Hence, the need for
synthesizing photo-realistic images via advanced Data Augmentation techniques
is crucial. In this paper we propose a hybrid inductive-deductive approach to
the problem; in particular, starting from a limited set of real labeled images,
the proposed framework makes use of logic programs for declaratively specifying
the structure of new images, that is guaranteed to comply with both a set of
constraints coming from the domain knowledge and some specific desiderata. The
resulting labeled images undergo a dedicated process based on Deep Learning in
charge of creating photo-realistic images that comply with the generated label.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14414" title="Abstract">arXiv:2310.14414</a> [<a href="/pdf/2310.14414" title="Download PDF">pdf</a>, <a href="/format/2310.14414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision Language Models in Autonomous Driving and Intelligent  Transportation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xingcheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zagar%2C+B+L">Bare Luka Zagar</a>, 
<a href="/search/cs?searchtype=author&query=Yurtsever%2C+E">Ekim Yurtsever</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A+C">Alois C. Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The applications of Vision-Language Models (VLMs) in the fields of Autonomous
Driving (AD) and Intelligent Transportation Systems (ITS) have attracted
widespread attention due to their outstanding performance and the ability to
leverage Large Language Models (LLMs). By integrating language data, the
vehicles, and transportation systems are able to deeply understand real-world
environments, improving driving safety and efficiency. In this work, we present
a comprehensive survey of the advances in language models in this domain,
encompassing current models and datasets. Additionally, we explore the
potential applications and emerging research directions. Finally, we thoroughly
discuss the challenges and research gap. The paper aims to provide researchers
with the current work and future trends of VLMs in AD and ITS.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14416" title="Abstract">arXiv:2310.14416</a> [<a href="/pdf/2310.14416" title="Download PDF">pdf</a>, <a href="/format/2310.14416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConViViT -- A Deep Neural Network Combining Convolutions and Factorized  Self-Attention for Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dokkar%2C+R+R">Rachid Reda Dokkar</a>, 
<a href="/search/cs?searchtype=author&query=Chaieb%2C+F">Faten Chaieb</a>, 
<a href="/search/cs?searchtype=author&query=Drira%2C+H">Hassen Drira</a>, 
<a href="/search/cs?searchtype=author&query=Aberkane%2C+A">Arezki Aberkane</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Workshop on MultiMedia Signal Processing (MMSP
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Transformer architecture has gained significant popularity in computer
vision tasks due to its capacity to generalize and capture long-range
dependencies. This characteristic makes it well-suited for generating
spatiotemporal tokens from videos. On the other hand, convolutions serve as the
fundamental backbone for processing images and videos, as they efficiently
aggregate information within small local neighborhoods to create spatial tokens
that describe the spatial dimension of a video. While both CNN-based
architectures and pure transformer architectures are extensively studied and
utilized by researchers, the effective combination of these two backbones has
not received comparable attention in the field of activity recognition. In this
research, we propose a novel approach that leverages the strengths of both CNNs
and Transformers in an hybrid architecture for performing activity recognition
using RGB videos. Specifically, we suggest employing a CNN network to enhance
the video representation by generating a 128-channel video that effectively
separates the human performing the activity from the background. Subsequently,
the output of the CNN module is fed into a transformer to extract
spatiotemporal tokens, which are then used for classification purposes. Our
architecture has achieved new SOTA results with 90.05 \%, 99.6\%, and 95.09\%
on HMDB51, UCF101, and ETRI-Activity3D respectively.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14418" title="Abstract">arXiv:2310.14418</a> [<a href="/pdf/2310.14418" title="Download PDF">pdf</a>, <a href="/format/2310.14418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REFER: An End-to-end Rationale Extraction Framework for Explanation  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madani%2C+M+R+G">Mohammad Reza Ghasemi Madani</a>, 
<a href="/search/cs?searchtype=author&query=Minervini%2C+P">Pasquale Minervini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Human-annotated textual explanations are becoming increasingly important in
Explainable Natural Language Processing. Rationale extraction aims to provide
faithful (i.e., reflective of the behavior of the model) and plausible (i.e.,
convincing to humans) explanations by highlighting the inputs that had the
largest impact on the prediction without compromising the performance of the
task model. In recent works, the focus of training rationale extractors was
primarily on optimizing for plausibility using human highlights, while the task
model was trained on jointly optimizing for task predictive accuracy and
faithfulness. We propose REFER, a framework that employs a differentiable
rationale extractor that allows to back-propagate through the rationale
extraction process. We analyze the impact of using human highlights during
training by jointly training the task model and the rationale extractor. In our
experiments, REFER yields significantly better results in terms of
faithfulness, plausibility, and downstream task accuracy on both
in-distribution and out-of-distribution data. On both e-SNLI and CoS-E, our
best setting produces better results in terms of composite normalized relative
gain than the previous baselines by 11% and 3%, respectively.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14420" title="Abstract">arXiv:2310.14420</a> [<a href="/pdf/2310.14420" title="Download PDF">pdf</a>, <a href="/format/2310.14420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monte Carlo Thought Search: Large Language Model Querying for Complex  Scientific Reasoning in Catalyst Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sprueill%2C+H+W">Henry W. Sprueill</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+C">Carl Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Olarte%2C+M+V">Mariefel V. Olarte</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+U">Udishnu Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Discovering novel catalysts requires complex reasoning involving multiple
chemical properties and resultant trade-offs, leading to a combinatorial growth
in the search space. While large language models (LLM) have demonstrated novel
capabilities for chemistry through complex instruction following capabilities
and high quality reasoning, a goal-driven combinatorial search using LLMs has
not been explored in detail. In this work, we present a Monte Carlo Tree
Search-based approach that improves beyond state-of-the-art chain-of-thought
prompting variants to augment scientific reasoning. We introduce two new
reasoning datasets: 1) a curation of computational chemistry simulations, and
2) diverse questions written by catalysis researchers for reasoning about novel
chemical conversion processes. We improve over the best baseline by 25.8\% and
find that our approach can augment scientist's reasoning and discovery process
with novel insights.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14422" title="Abstract">arXiv:2310.14422</a> [<a href="/pdf/2310.14422" title="Download PDF">pdf</a>, <a href="/format/2310.14422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are biased to overestimate profoundness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herrera-Berg%2C+E">Eugenio Herrera-Berg</a>, 
<a href="/search/cs?searchtype=author&query=Browne%2C+T+V">Tom&#xe1;s Vergara Browne</a>, 
<a href="/search/cs?searchtype=author&query=Le%C3%B3n-Villagr%C3%A1%2C+P">Pablo Le&#xf3;n-Villagr&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Vives%2C+M">Marc-Llu&#xed;s Vives</a>, 
<a href="/search/cs?searchtype=author&query=Calderon%2C+C+B">Cristian Buc Calderon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advancements in natural language processing by large language models
(LLMs), such as GPT-4, have been suggested to approach Artificial General
Intelligence. And yet, it is still under dispute whether LLMs possess similar
reasoning abilities to humans. This study evaluates GPT-4 and various other
LLMs in judging the profoundness of mundane, motivational, and pseudo-profound
statements. We found a significant statement-to-statement correlation between
the LLMs and humans, irrespective of the type of statements and the prompting
technique used. However, LLMs systematically overestimate the profoundness of
nonsensical statements, with the exception of Tk-instruct, which uniquely
underestimates the profoundness of statements. Only few-shot learning prompts,
as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.
Furthermore, this work provides insights into the potential biases induced by
Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the
bias to overestimate the profoundness of statements.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14423" title="Abstract">arXiv:2310.14423</a> [<a href="/pdf/2310.14423" title="Download PDF">pdf</a>, <a href="/format/2310.14423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quadratic Synchronization Rule for Distributed Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xinran Gu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+K">Kaifeng Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Longbo Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In distributed deep learning with data parallelism, synchronizing gradients
at each training step can cause a huge communication overhead, especially when
many nodes work together to train large models. Local gradient methods, such as
Local SGD, address this issue by allowing workers to compute locally for $H$
steps without synchronizing with others, hence reducing communication
frequency. While $H$ has been viewed as a hyperparameter to trade optimization
efficiency for communication cost, recent research indicates that setting a
proper $H$ value can lead to generalization improvement. Yet, selecting a
proper $H$ is elusive. This work proposes a theory-grounded method for
determining $H$, named the Quadratic Synchronization Rule (QSR), which
recommends dynamically setting $H$ in proportion to $\frac{1}{\eta^2}$ as the
learning rate $\eta$ decays over time. Extensive ImageNet experiments on ResNet
and ViT show that local gradient methods with QSR consistently improve the test
accuracy over other synchronization strategies. Compared with the standard data
parallel training, QSR enables Local AdamW on ViT-B to cut the training time on
16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the
same time, achieves $1.16\%$ or $0.84\%$ higher top-1 validation accuracy.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14424" title="Abstract">arXiv:2310.14424</a> [<a href="/pdf/2310.14424" title="Download PDF">pdf</a>, <a href="/format/2310.14424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Which Prompts Make The Difference? Data Prioritization For Efficient  Human LLM Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boubdir%2C+M">Meriem Boubdir</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Edward Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ermis%2C+B">Beyza Ermis</a>, 
<a href="/search/cs?searchtype=author&query=Fadaee%2C+M">Marzieh Fadaee</a>, 
<a href="/search/cs?searchtype=author&query=Hooker%2C+S">Sara Hooker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Human evaluation is increasingly critical for assessing large language
models, capturing linguistic nuances, and reflecting user preferences more
accurately than traditional automated metrics. However, the resource-intensive
nature of this type of annotation process poses significant challenges. The key
question driving our work: "is it feasible to minimize human-in-the-loop
feedback by prioritizing data instances which most effectively distinguish
between models?" We evaluate several metric-based methods and find that these
metrics enhance the efficiency of human evaluations by minimizing the number of
required annotations, thus saving time and cost, while ensuring a robust
performance evaluation. We show that our method is effective across widely used
model families, reducing instances of indecisive (or "tie") outcomes by up to
54% compared to a random sample when focusing on the top-20 percentile of
prioritized instances. This potential reduction in required human effort
positions our approach as a valuable strategy in future large language model
evaluations.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14427" title="Abstract">arXiv:2310.14427</a> [<a href="/pdf/2310.14427" title="Download PDF">pdf</a>, <a href="/format/2310.14427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMDA: a tool for Continuous Monitoring Data Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghalati%2C+P+F">Pejman Farhadi Ghalati</a>, 
<a href="/search/cs?searchtype=author&query=Schuppert%2C+A">Andreas Schuppert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Over the last few years, with the growth of time-series collecting and
storing, there has been a great demand for tools and software for temporal data
engineering and modeling. This paper presents a generic workflow for time
series data research, including temporal data importing, preprocessing, and
feature extraction. This framework is developed and built as a robust and
easy-to-use Python package, called CMDA, with a modular structure that offers
tools to prepare raw data, allowing both scientists and non-experts to analyze
various temporal data structures.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14429" title="Abstract">arXiv:2310.14429</a> [<a href="/pdf/2310.14429" title="Download PDF">pdf</a>, <a href="/format/2310.14429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text generation for dataset augmentation in security classification  tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Welsh%2C+A+P">Alexander P. Welsh</a>, 
<a href="/search/cs?searchtype=author&query=Edwards%2C+M">Matthew Edwards</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Security classifiers, designed to detect malicious content in computer
systems and communications, can underperform when provided with insufficient
training data. In the security domain, it is often easy to find samples of the
negative (benign) class, and challenging to find enough samples of the positive
(malicious) class to train an effective classifier. This study evaluates the
application of natural language text generators to fill this data gap in
multiple security-related text classification tasks. We describe a variety of
previously-unexamined language-model fine-tuning approaches for this purpose
and consider in particular the impact of disproportionate class-imbalances in
the training set. Across our evaluation using three state-of-the-art
classifiers designed for offensive language detection, review fraud detection,
and SMS spam detection, we find that models trained with GPT-3 data
augmentation strategies outperform both models trained without augmentation and
models trained using basic data augmentation strategies already in common
usage. In particular, we find substantial benefits for GPT-3 data augmentation
strategies in situations with severe limitations on known positive-class
samples.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14430" title="Abstract">arXiv:2310.14430</a> [<a href="/pdf/2310.14430" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Students Based on Gamification User Types and Learning Styles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arslan%2C+E">Emre Arslan</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zkaymak%2C+A">Atilla &#xd6;zkaymak</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%B6nmez%2C+N+%C3%96">Nesrin &#xd6;zdener D&#xf6;nmez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The aim of this study is clustering students according to their gamification
user types and learning styles with the purpose of providing instructors with a
new perspective of grouping students in case of clustering which cannot be done
by hand when there are multiple scales in data. The data used consists of 251
students who were enrolled at a Turkish state university. When grouping
students, K-means algorithm has been utilized as clustering algorithm. As for
determining the gamification user types and learning styles of students,
Gamification User Type Hexad Scale and Grasha-Riechmann Student Learning Style
Scale have been used respectively. Silhouette coefficient is utilized as
clustering quality measure. After fitting the algorithm in several ways,
highest Silhouette coefficient obtained was 0.12 meaning that results are
neutral but not satisfactory. All the statistical operations and data
visualizations were made using Python programming language.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14432" title="Abstract">arXiv:2310.14432</a> [<a href="/pdf/2310.14432" title="Download PDF">pdf</a>, <a href="/format/2310.14432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness-aware Optimal Graph Filter Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kose%2C+O+D">O. Deniz Kose</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yanning Shen</a>, 
<a href="/search/cs?searchtype=author&query=Mateos%2C+G">Gonzalo Mateos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures, 9 tables. arXiv admin note: text overlap with <a href="/abs/2303.11459">arXiv:2303.11459</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Graphs are mathematical tools that can be used to represent complex
real-world interconnected systems, such as financial markets and social
networks. Hence, machine learning (ML) over graphs has attracted significant
attention recently. However, it has been demonstrated that ML over graphs
amplifies the already existing bias towards certain under-represented groups in
various decision-making problems due to the information aggregation over biased
graph structures. Faced with this challenge, here we take a fresh look at the
problem of bias mitigation in graph-based learning by borrowing insights from
graph signal processing. Our idea is to introduce predesigned graph filters
within an ML pipeline to reduce a novel unsupervised bias measure, namely the
correlation between sensitive attributes and the underlying graph connectivity.
We show that the optimal design of said filters can be cast as a convex problem
in the graph spectral domain. We also formulate a linear programming (LP)
problem informed by a theoretical bias analysis, which attains a closed-form
solution and leads to a more efficient fairness-aware graph filter. Finally,
for a design whose degrees of freedom are independent of the input graph size,
we minimize the bias metric over the family of polynomial graph convolutional
filters. Our optimal filter designs offer complementary strengths to explore
favorable fairness-utility-complexity tradeoffs. For performance evaluation, we
conduct extensive and reproducible node classification experiments over
real-world networks. Our results show that the proposed framework leads to
better fairness measures together with similar utility compared to
state-of-the-art fairness-aware baselines.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14433" title="Abstract">arXiv:2310.14433</a> [<a href="/pdf/2310.14433" title="Download PDF">pdf</a>, <a href="/format/2310.14433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigate how developers and managers view security design in software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imran%2C+A">Asif Imran</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 Evaluation of Novel Approaches to Software Engineering
  (ENASE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
<p class="mathjax">Software security requirements have been traditionally considered as a
non-functional attribute of the software. However, as more software started to
provide services online, existing mechanisms of using firewalls and other
hardware to secure software have lost their applicability. At the same time,
under the current world circumstances, the increase of cyber-attacks on
software is ever increasing. As a result, it is important to consider the
security requirements of software during its design. To design security in the
software, it is important to obtain the views of the developers and managers of
the software. Also, it is important to evaluate if their viewpoints match or
differ regarding the security. Conducting this communication through a specific
model will enable the developers and managers to eliminate any doubts on
security design and adopt an effective strategy to build security into the
software. In this paper, we analyzed the viewpoints of developers and managers
regarding their views on security design. We interviewed a team of 7 developers
and 2 managers, who worked in two teams to build a real-life software product
that was recently compromised by a cyber-attack. We obtained their views on the
reasons for the successful attack by the malware and took their recommendations
on the important aspects to consider regarding security. Based on their
feedback, we coded their open-ended responses into 4 codes, which we
recommended using for other real-life software as well.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14434" title="Abstract">arXiv:2310.14434</a> [<a href="/pdf/2310.14434" title="Download PDF">pdf</a>, <a href="/format/2310.14434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Accuracy-Privacy Trade-off in Differentially Private Split  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+N+D">Ngoc Duy Pham</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+K+T">Khoa Tran Phan</a>, 
<a href="/search/cs?searchtype=author&query=Chilamkurti%2C+N">Naveen Chilamkurti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Split learning (SL) aims to protect user data privacy by distributing deep
models between client-server and keeping private data locally. Only processed
or `smashed' data can be transmitted from the clients to the server during the
SL process. However, recently proposed model inversion attacks can recover the
original data from the smashed data. In order to enhance privacy protection
against such attacks, a strategy is to adopt differential privacy (DP), which
involves safeguarding the smashed data at the expense of some accuracy loss.
This paper presents the first investigation into the impact on accuracy when
training multiple clients in SL with various privacy requirements.
Subsequently, we propose an approach that reviews the DP noise distributions of
other clients during client training to address the identified accuracy
degradation. We also examine the application of DP to the local model of SL to
gain insights into the trade-off between accuracy and privacy. Specifically,
findings reveal that introducing noise in the later local layers offers the
most favorable balance between accuracy and privacy. Drawing from our insights
in the shallower layers, we propose an approach to reduce the size of smashed
data to minimize data leakage while maintaining higher accuracy, optimizing the
accuracy-privacy trade-off. Additionally, a smaller size of smashed data
reduces communication overhead on the client side, mitigating one of the
notable drawbacks of SL. Experiments with popular datasets demonstrate that our
proposed approaches provide an optimal trade-off for incorporating DP into SL,
ultimately enhancing training accuracy for multi-client SL with varying privacy
requirements.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14435" title="Abstract">arXiv:2310.14435</a> [<a href="/pdf/2310.14435" title="Download PDF">pdf</a>, <a href="/format/2310.14435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Augmented Chain-of-Thought in Semi-structured Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mavi%2C+V">Vaibhav Mavi</a>, 
<a href="/search/cs?searchtype=author&query=Saparov%2C+A">Abulhair Saparov</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in NLLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Applying existing question answering (QA) systems to specialized domains like
law and finance presents challenges that necessitate domain expertise. Although
large language models (LLMs) have shown impressive language comprehension and
in-context learning capabilities, their inability to handle very long
inputs/contexts is well known. Tasks specific to these domains need significant
background knowledge, leading to contexts that can often exceed the maximum
length that existing LLMs can process. This study explores leveraging the
semi-structured nature of legal and financial data to efficiently retrieve
relevant context, enabling the use of LLMs for domain-specialized QA. The
resulting system outperforms contemporary models and also provides useful
explanations for the answers, encouraging the integration of LLMs into legal
and financial NLP systems for future research.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14437" title="Abstract">arXiv:2310.14437</a> [<a href="/pdf/2310.14437" title="Download PDF">pdf</a>, <a href="/format/2310.14437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mobile AR Depth Estimation: Challenges &amp; Prospects -- Extended Version
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganj%2C+A">Ashkan Ganj</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiqin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tian Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Metric depth estimation plays an important role in mobile augmented reality
(AR). With accurate metric depth, we can achieve more realistic user
interactions such as object placement and occlusion detection. While
specialized hardware like LiDAR demonstrates its promise, its restricted
availability, i.e., only on selected high-end mobile devices, and performance
limitations such as range and sensitivity to the environment, make it less
ideal. Monocular depth estimation, on the other hand, relies solely on mobile
cameras, which are ubiquitous, making it a promising alternative for mobile AR.
<br />In this paper, we investigate the challenges and opportunities of achieving
accurate metric depth estimation in mobile AR. We tested four different
state-of-the-art monocular depth estimation models on a newly introduced
dataset (ARKitScenes) and identified three types of challenges: hard-ware,
data, and model related challenges. Furthermore, our research provides
promising future directions to explore and solve those challenges. These
directions include (i) using more hardware-related information from the mobile
device's camera and other available sensors, (ii) capturing high-quality data
to reflect real-world AR scenarios, and (iii) designing a model architecture to
utilize the new information.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14439" title="Abstract">arXiv:2310.14439</a> [<a href="/pdf/2310.14439" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the automation of book typesetting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rebelo%2C+S+M">S&#xe9;rgio M. Rebelo</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+T">Tiago Martins</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+D">Diogo Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Rebelo%2C+A">Artur Rebelo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 5 figures. Revised version published at Visual Informatics, 7(2), pp. 1\textendash{}12
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Visual Informatics, (2023) 7(2), 1--12
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR); Multimedia (cs.MM)

</div>
<p class="mathjax">This paper proposes a generative approach for the automatic typesetting of
books in desktop publishing. The presented system consists in a computer script
that operates inside a widely used design software tool and implements a
generative process based on several typographic rules, styles and principles
which have been identified in the literature. The performance of the proposed
system is tested through an experiment which included the evaluation of its
outputs with people. The results reveal the ability of the system to
consistently create varied book designs from the same input content as well as
visually coherent book designs with different contents while complying with
fundamental typographic principles.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14441" title="Abstract">arXiv:2310.14441</a> [<a href="/pdf/2310.14441" title="Download PDF">pdf</a>, <a href="/format/2310.14441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDGE++: Improved Training and Sampling of EDGE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mingyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liping Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently developed deep neural models like NetGAN, CELL, and Variational
Graph Autoencoders have made progress but face limitations in replicating key
graph statistics on generating large graphs. Diffusion-based methods have
emerged as promising alternatives, however, most of them present challenges in
computational efficiency and generative performance. EDGE is effective at
modeling large networks, but its current denoising approach can be inefficient,
often leading to wasted computational resources and potential mismatches in its
generation process. In this paper, we propose enhancements to the EDGE model to
address these issues. Specifically, we introduce a degree-specific noise
schedule that optimizes the number of active nodes at each timestep,
significantly reducing memory consumption. Additionally, we present an improved
sampling scheme that fine-tunes the generative process, allowing for better
control over the similarity between the synthesized and the true network. Our
experimental results demonstrate that the proposed modifications not only
improve the efficiency but also enhance the accuracy of the generated graphs,
offering a robust and scalable solution for graph generation tasks.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14444" title="Abstract">arXiv:2310.14444</a> [<a href="/pdf/2310.14444" title="Download PDF">pdf</a>, <a href="/format/2310.14444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> URegM: a unified prediction model of resource consumption for  refactoring software smells in open source cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imran%2C+A">Asif Imran</a>, 
<a href="/search/cs?searchtype=author&query=Kosar%2C+T">Tevfik Kosar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 The 3rd European Symposium on Software Engineering (ESSE
  2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The low cost and rapid provisioning capabilities have made the cloud a
desirable platform to launch complex scientific applications. However, resource
utilization optimization is a significant challenge for cloud service
providers, since the earlier focus is provided on optimizing resources for the
applications that run on the cloud, with a low emphasis being provided on
optimizing resource utilization of the cloud computing internal processes. Code
refactoring has been associated with improving the maintenance and
understanding of software code. However, analyzing the impact of the
refactoring source code of the cloud and studying its impact on cloud resource
usage require further analysis. In this paper, we propose a framework called
Unified Regression Modelling (URegM) which predicts the impact of code smell
refactoring on cloud resource usage. We test our experiments in a real-life
cloud environment using a complex scientific application as a workload. Results
show that URegM is capable of accurately predicting resource consumption due to
code smell refactoring. This will permit cloud service providers with advanced
knowledge about the impact of refactoring code smells on resource consumption,
thus allowing them to plan their resource provisioning and code refactoring
more effectively.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14449" title="Abstract">arXiv:2310.14449</a> [<a href="/pdf/2310.14449" title="Download PDF">pdf</a>, <a href="/format/2310.14449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qualitative analysis of the relationship between design smells and  software engineering challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imran%2C+A">Asif Imran</a>, 
<a href="/search/cs?searchtype=author&query=Kosar%2C+T">Tevfik Kosar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/1910.05428">arXiv:1910.05428</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 The 3rd European Symposium on Software Engineering (ESSE
  2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Software design debt aims to elucidate the rectification attempts of the
present design flaws and studies the influence of those to the cost and time of
the software. Design smells are a key cause of incurring design debt. Although
the impact of design smells on design debt have been predominantly considered
in current literature, how design smells are caused due to not following
software engineering best practices require more exploration. This research
provides a tool which is used for design smell detection in Java software by
analyzing large volume of source codes. More specifically, 409,539 Lines of
Code (LoC) and 17,760 class files of open source Java software are analyzed
here. Obtained results show desirable precision values ranging from 81.01\% to
93.43\%. Based on the output of the tool, a study is conducted to relate the
cause of the detected design smells to two software engineering challenges
namely "irregular team meetings" and "scope creep". As a result, the gained
information will provide insight to the software engineers to take necessary
steps of design remediation actions.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14450" title="Abstract">arXiv:2310.14450</a> [<a href="/pdf/2310.14450" title="Download PDF">pdf</a>, <a href="/format/2310.14450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanley%2C+H+W+A">Hans W. A. Hanley</a>, 
<a href="/search/cs?searchtype=author&query=Durumeric%2C+Z">Zakir Durumeric</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Stance detection is important for understanding different attitudes and
beliefs on the Internet. However, given that a passage's stance toward a given
topic is often highly dependent on that topic, building a stance detection
model that generalizes to unseen topics is difficult. In this work, we propose
using contrastive learning as well as an unlabeled dataset of news articles
that cover a variety of different topics to train topic-agnostic/TAG and
topic-aware/TAW embeddings for use in downstream stance detection. Combining
these embeddings in our full TATA model, we achieve state-of-the-art
performance across several public stance detection datasets (0.771 $F_1$-score
on the Zero-shot VAST dataset). We release our code and data at
https://github.com/hanshanley/tata.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14451" title="Abstract">arXiv:2310.14451</a> [<a href="/pdf/2310.14451" title="Download PDF">pdf</a>, <a href="/format/2310.14451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Terminology Integration into Machine Translation: Leveraging  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moslem">Moslem</a>, 
<a href="/search/cs?searchtype=author&query=Yasmin">Yasmin</a>, 
<a href="/search/cs?searchtype=author&query=Romani">Romani</a>, 
<a href="/search/cs?searchtype=author&query=Gianfranco">Gianfranco</a>, 
<a href="/search/cs?searchtype=author&query=Molaei">Molaei</a>, 
<a href="/search/cs?searchtype=author&query=Mahdi">Mahdi</a>, 
<a href="/search/cs?searchtype=author&query=Haque">Haque</a>, 
<a href="/search/cs?searchtype=author&query=Rejwanul">Rejwanul</a>, 
<a href="/search/cs?searchtype=author&query=Kelleher%2C+D">D. Kelleher</a>, 
<a href="/search/cs?searchtype=author&query=John">John</a>, Way, 
<a href="/search/cs?searchtype=author&query=Andy">Andy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WMT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper discusses the methods that we used for our submissions to the WMT
2023 Terminology Shared Task for German-to-English (DE-EN), English-to-Czech
(EN-CS), and Chinese-to-English (ZH-EN) language pairs. The task aims to
advance machine translation (MT) by challenging participants to develop systems
that accurately translate technical terms, ultimately enhancing communication
and understanding in specialised domains. To this end, we conduct experiments
that utilise large language models (LLMs) for two purposes: generating
synthetic bilingual terminology-based data, and post-editing translations
generated by an MT model through incorporating pre-approved terms. Our system
employs a four-step process: (i) using an LLM to generate bilingual synthetic
data based on the provided terminology, (ii) fine-tuning a generic
encoder-decoder MT model, with a mix of the terminology-based synthetic data
generated in the first step and a randomly sampled portion of the original
generic training data, (iii) generating translations with the fine-tuned MT
model, and (iv) finally, leveraging an LLM for terminology-constrained
automatic post-editing of the translations that do not include the required
terms. The results demonstrate the effectiveness of our proposed approach in
improving the integration of pre-approved terms into translations. The number
of terms incorporated into the translations of the blind dataset increases from
an average of 36.67% with the generic model to an average of 72.88% by the end
of the process. In other words, successful utilisation of terms nearly doubles
across the three language pairs.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14453" title="Abstract">arXiv:2310.14453</a> [<a href="/pdf/2310.14453" title="Download PDF">pdf</a>, <a href="/format/2310.14453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skipped Feature Pyramid Network with Grid Anchor for Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pengfei%2C+L">Li Pengfei</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+Z">Zhu Rong</a>, 
<a href="/search/cs?searchtype=author&query=Liguo%2C+Z">Zhou Liguo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">CNN-based object detection methods have achieved significant progress in
recent years. The classic structures of CNNs produce pyramid-like feature maps
due to the pooling or other re-scale operations. The feature maps in different
levels of the feature pyramid are used to detect objects with different scales.
For more accurate object detection, the highest-level feature, which has the
lowest resolution and contains the strongest semantics, is up-scaled and
connected with the lower-level features to enhance the semantics in the
lower-level features. However, the classic mode of feature connection combines
the feature of lower-level with all the features above it, which may result in
semantics degradation. In this paper, we propose a skipped connection to obtain
stronger semantics at each level of the feature pyramid. In our method, the
lower-level feature only connects with the feature at the highest level, making
it more reasonable that each level is responsible for detecting objects with
fixed scales. In addition, we simplify the generation of anchor for bounding
box regression, which can further improve the accuracy of object detection. The
experiments on the MS COCO and Wider Face demonstrate that our method
outperforms the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14455" title="Abstract">arXiv:2310.14455</a> [<a href="/pdf/2310.14455" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An International Consortium for Evaluations of Societal-Scale Risks from  Advanced AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruetzemacher%2C+R">Ross Gruetzemacher</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Alan Chan</a>, 
<a href="/search/cs?searchtype=author&query=Frazier%2C+K">Kevin Frazier</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C">Christy Manning</a>, 
<a href="/search/cs?searchtype=author&query=Los%2C+%C5%A0">&#x160;t&#x11b;p&#xe1;n Los</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+J">James Fox</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Orallo%2C+J">Jos&#xe9; Hern&#xe1;ndez-Orallo</a>, 
<a href="/search/cs?searchtype=author&query=Burden%2C+J">John Burden</a>, 
<a href="/search/cs?searchtype=author&query=Franklin%2C+M">Matija Franklin</a>, 
<a href="/search/cs?searchtype=author&query=Ghuidhir%2C+C+N">Cl&#xed;odhna N&#xed; Ghuidhir</a>, 
<a href="/search/cs?searchtype=author&query=Bailey%2C+M">Mark Bailey</a>, 
<a href="/search/cs?searchtype=author&query=Pilditch%2C+T">Toby Pilditch</a>, 
<a href="/search/cs?searchtype=author&query=Kilian%2C+K">Kyle Kilian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Given rapid progress toward advanced AI and risks from frontier AI systems
(advanced AI systems pushing the boundaries of the AI capabilities frontier),
the creation and implementation of AI governance and regulatory schemes
deserves prioritization and substantial investment. However, the status quo is
untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to
conduct research, development, and deployment activities with minimal
oversight. In response, frontier AI system evaluations have been proposed as a
way of assessing risks from the development and deployment of frontier AI
systems. Yet, the budding AI risk evaluation ecosystem faces significant
coordination challenges, such as a limited diversity of evaluators, suboptimal
allocation of effort, and perverse incentives. This paper proposes a solution
in the form of an international consortium for AI risk evaluations, comprising
both AI developers and third-party AI risk evaluators. Such a consortium could
play a critical role in international efforts to mitigate societal-scale risks
from advanced AI, including in managing responsible scaling policies and
coordinated evaluation-based risk response. In this paper, we discuss the
current evaluation ecosystem and its shortcomings, propose an international
consortium for advanced AI risk evaluations, discuss issues regarding its
implementation, discuss lessons that can be learnt from previous international
institutions and existing proposals for international AI governance
institutions, and, finally, we recommend concrete steps to advance the
establishment of the proposed consortium: (i) solicit feedback from
stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for
stakeholders, (iv) analyze feedback and create final proposal, (v) solicit
funding, and (vi) create a consortium.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14456" title="Abstract">arXiv:2310.14456</a> [<a href="/pdf/2310.14456" title="Download PDF">pdf</a>, <a href="/format/2310.14456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mobile Traffic Prediction at the Edge through Distributed and Transfer  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrella%2C+A">Alfredo Petrella</a>, 
<a href="/search/cs?searchtype=author&query=Miozzo%2C+M">Marco Miozzo</a>, 
<a href="/search/cs?searchtype=author&query=Dini%2C+P">Paolo Dini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traffic prediction represents one of the crucial tasks for smartly optimizing
the mobile network. The research in this topic concentrated in making
predictions in a centralized fashion, i.e., by collecting data from the
different network elements. This translates to a considerable amount of energy
for data transmission and processing. In this work, we propose a novel
prediction framework based on edge computing which uses datasets obtained on
the edge through a large measurement campaign. Two main Deep Learning
architectures are designed, based on Convolutional Neural Networks (CNNs) and
Recurrent Neural Networks (RNNs), and tested under different training
conditions. In addition, Knowledge Transfer Learning (KTL) techniques are
employed to improve the performance of the models while reducing the required
computational resources. Simulation results show that the CNN architectures
outperform the RNNs. An estimation for the needed training energy is provided,
highlighting KTL ability to reduce the energy footprint of the models of 60%
and 90% for CNNs and RNNs, respectively. Finally, two cutting-edge explainable
Artificial Intelligence techniques are employed to interpret the derived
learning models.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14457" title="Abstract">arXiv:2310.14457</a> [<a href="/pdf/2310.14457" title="Download PDF">pdf</a>, <a href="/format/2310.14457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A generalized likelihood-weighted optimal sampling algorithm for  rare-event probability quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xianliang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yulin Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">In this work, we introduce a new acquisition function for sequential sampling
to efficiently quantify rare-event statistics of an input-to-response (ItR)
system with given input probability and expensive function evaluations. Our
acquisition is a generalization of the likelihood-weighted (LW) acquisition
that was initially designed for the same purpose and then extended to many
other applications. The improvement in our acquisition comes from the
generalized form with two additional parameters, by varying which one can
target and address two weaknesses of the original LW acquisition: (1) that the
input space associated with rare-event responses is not sufficiently stressed
in sampling; (2) that the surrogate model (generated from samples) may have
significant deviation from the true ItR function, especially for cases with
complex ItR function and limited number of samples. In addition, we develop a
critical procedure in Monte-Carlo discrete optimization of the acquisition
function, which achieves orders of magnitude acceleration compared to existing
approaches for such type of problems. The superior performance of our new
acquisition to the original LW acquisition is demonstrated in a number of test
cases, including some cases that were designed to show the effectiveness of the
original LW acquisition. We finally apply our method to an engineering example
to quantify the rare-event roll-motion statistics of a ship in a random sea.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14458" title="Abstract">arXiv:2310.14458</a> [<a href="/pdf/2310.14458" title="Download PDF">pdf</a>, <a href="/format/2310.14458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion-Model-Assisted Supervised Learning of Generative Models for  Density Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanfang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Minglei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zezhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+F">Feng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yanzhao Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guannan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We present a supervised learning framework of training generative models for
density estimation. Generative models, including generative adversarial
networks, normalizing flows, variational auto-encoders, are usually considered
as unsupervised learning models, because labeled data are usually unavailable
for training. Despite the success of the generative models, there are several
issues with the unsupervised training, e.g., requirement of reversible
architectures, vanishing gradients, and training instability. To enable
supervised learning in generative models, we utilize the score-based diffusion
model to generate labeled data. Unlike existing diffusion models that train
neural networks to learn the score function, we develop a training-free score
estimation method. This approach uses mini-batch-based Monte Carlo estimators
to directly approximate the score function at any spatial-temporal location in
solving an ordinary differential equation (ODE), corresponding to the
reverse-time stochastic differential equation (SDE). This approach can offer
both high accuracy and substantial time savings in neural network training.
Once the labeled data are generated, we can train a simple fully connected
neural network to learn the generative model in the supervised manner. Compared
with existing normalizing flow models, our method does not require to use
reversible neural networks and avoids the computation of the Jacobian matrix.
Compared with existing diffusion models, our method does not need to solve the
reverse-time SDE to generate new samples. As a result, the sampling efficiency
is significantly improved. We demonstrate the performance of our method by
applying it to a set of 2D datasets as well as real data from the UCI
repository.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14459" title="Abstract">arXiv:2310.14459</a> [<a href="/pdf/2310.14459" title="Download PDF">pdf</a>, <a href="/format/2310.14459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ANN-MoC Method for Inverse Transient Transport Problems in  One-Dimensional Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Roman%2C+N+G">Nelson Garcia Roman</a>, 
<a href="/search/math?searchtype=author&query=Santos%2C+P+C+d">Pedro Costa dos Santos</a>, 
<a href="/search/math?searchtype=author&query=de+Almeida+Konzen%2C+P+H">Pedro Henrique de Almeida Konzen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures, conference paper XXVI ENMC/XIV ECTM, Juiz de Fora, Brazil
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
<p class="mathjax">The inverse problems of particle neutral transport models have many important
engineering and medical applications. Safety protocols, quality control
procedures, and optical medical solutions can be developed based on inverse
transport solutions. In this work, we propose the ANN-MoC method to solve the
inverse transient transport problem of estimating the absorption coefficient
from measurements of the scalar flux at the boundaries of the model domain. The
main idea is to train an Artificial Neural Network (ANN) from data generated by
direct solutions computed by a Method of Characteristics (MoC) solver. The
direct solver is tested on a problem with a manufactured solution. And, the
proposed ANN-MoC method is tested on two inverse problems. In the first, the
medium is homogeneous and has a constant absorption coefficient. In the second,
a heterogeneous medium is considered, with the absorption coefficient constant
by parts. Very accurate ANN estimations have been achieved for these two
problems, indicating that the quality of the results relies on the accuracy of
the direct solver solutions. The results show the potential of the proposed
approach to be applied to more realistic inverse transport problems.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14462" title="Abstract">arXiv:2310.14462</a> [<a href="/pdf/2310.14462" title="Download PDF">pdf</a>, <a href="/ps/2310.14462" title="Download PostScript">ps</a>, <a href="/format/2310.14462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Fourier transform via automorphism groups of rational function  fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Songsong Li</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+C">Chaoping Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The Fast Fourier Transform (FFT) over a finite field $\mathbb{F}_q$ computes
evaluations of a given polynomial of degree less than $n$ at a specifically
chosen set of $n$ distinct evaluation points in $\mathbb{F}_q$. If $q$ or $q-1$
is a smooth number, then the divide-and-conquer approach leads to the fastest
known FFT algorithms. Depending on the type of group that the set of evaluation
points forms, these algorithms are classified as multiplicative (Math of Comp.
1965) and additive (FOCS 2014) FFT algorithms. In this work, we provide a
unified framework for FFT algorithms that include both multiplicative and
additive FFT algorithms as special cases, and beyond: our framework also works
when $q+1$ is smooth, while all known results require $q$ or $q-1$ to be
smooth. For the new case where $q+1$ is smooth (this new case was not
considered before in literature as far as we know), we show that if $n$ is a
divisor of $q+1$ that is $B$-smooth for a real $B&gt;0$, then our FFT needs
$O(Bn\log n)$ arithmetic operations in $\mathbb{F}_q$. Our unified framework is
a natural consequence of introducing the algebraic function fields into the
study of FFT.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14466" title="Abstract">arXiv:2310.14466</a> [<a href="/pdf/2310.14466" title="Download PDF">pdf</a>, <a href="/format/2310.14466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring Relational Potentials in Interacting Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Comas-Massagu%C3%A9%2C+A">Armand Comas-Massagu&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+C">Christian Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Ghimire%2C+S">Sandesh Ghimire</a>, 
<a href="/search/cs?searchtype=author&query=Sznaier%2C+M">Mario Sznaier</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Camps%2C+O">Octavia Camps</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICML 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Systems consisting of interacting agents are prevalent in the world, ranging
from dynamical systems in physics to complex biological networks. To build
systems which can interact robustly in the real world, it is thus important to
be able to infer the precise interactions governing such systems. Existing
approaches typically discover such interactions by explicitly modeling the
feed-forward dynamics of the trajectories. In this work, we propose Neural
Interaction Inference with Potentials (NIIP) as an alternative approach to
discover such interactions that enables greater flexibility in trajectory
modeling: it discovers a set of relational potentials, represented as energy
functions, which when minimized reconstruct the original trajectory. NIIP
assigns low energy to the subset of trajectories which respect the relational
constraints observed. We illustrate that with these representations NIIP
displays unique capabilities in test-time. First, it allows trajectory
manipulation, such as interchanging interaction types across separately trained
models, as well as trajectory forecasting. Additionally, it allows adding
external hand-crafted potentials at test-time. Finally, NIIP enables the
detection of out-of-distribution samples and anomalies without explicit
training. Website: https://energy-based-model.github.io/interaction-potentials.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14468" title="Abstract">arXiv:2310.14468</a> [<a href="/pdf/2310.14468" title="Download PDF">pdf</a>, <a href="/format/2310.14468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Implicit Differentiation for Learning Problems in Optimal  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Ming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Molloy%2C+T">Timothy Molloy</a>, 
<a href="/search/cs?searchtype=author&query=Gould%2C+S">Stephen Gould</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper proposes a new method for differentiating through optimal
trajectories arising from non-convex, constrained discrete-time optimal control
(COC) problems using the implicit function theorem (IFT). Previous works solve
a differential Karush-Kuhn-Tucker (KKT) system for the trajectory derivative,
and achieve this efficiently by solving an auxiliary Linear Quadratic Regulator
(LQR) problem. In contrast, we directly evaluate the matrix equations which
arise from applying variable elimination on the Lagrange multiplier terms in
the (differential) KKT system. By appropriately accounting for the structure of
the terms within the resulting equations, we show that the trajectory
derivatives scale linearly with the number of timesteps. Furthermore, our
approach allows for easy parallelization, significantly improved scalability
with model size, direct computation of vector-Jacobian products and improved
numerical stability compared to prior works. As an additional contribution, we
unify prior works, addressing claims that computing trajectory derivatives
using IFT scales quadratically with the number of timesteps. We evaluate our
method on a both synthetic benchmark and four challenging, learning from
demonstration benchmarks including a 6-DoF maneuvering quadrotor and 6-DoF
rocket powered landing.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14469" title="Abstract">arXiv:2310.14469</a> [<a href="/pdf/2310.14469" title="Download PDF">pdf</a>, <a href="/format/2310.14469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Player Re-Identification Using Body Part Appearences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhosale%2C+M">Mahesh Bhosale</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhishek Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Doermann%2C+D">David Doermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a neural network architecture that learns body part appearances
for soccer player re-identification. Our model consists of a two-stream network
(one stream for appearance map extraction and the other for body part map
extraction) and a bilinear-pooling layer that generates and spatially pools the
body part map. Each local feature of the body part map is obtained by a
bilinear mapping of the corresponding local appearance and body part
descriptors. Our novel representation yields a robust image-matching feature
map, which results from combining the local similarities of the relevant body
parts with the weighted appearance similarity. Our model does not require any
part annotation on the SoccerNet-V3 re-identification dataset to train the
network. Instead, we use a sub-network of an existing pose estimation network
(OpenPose) to initialize the part substream and then train the entire network
to minimize the triplet loss. The appearance stream is pre-trained on the
ImageNet dataset, and the part stream is trained from scratch for the
SoccerNet-V3 dataset. We demonstrate the validity of our model by showing that
it outperforms state-of-the-art models such as OsNet and InceptionNet.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14470" title="Abstract">arXiv:2310.14470</a> [<a href="/pdf/2310.14470" title="Download PDF">pdf</a>, <a href="/format/2310.14470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized Stokeslet Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ferranti%2C+D">Dana Ferranti</a>, 
<a href="/search/math?searchtype=author&query=Cortez%2C+R">Ricardo Cortez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">An extension of the Method of Regularized Stokeslets (MRS) in three
dimensions is developed for triangulated surfaces with a piecewise linear force
distribution. The method extends the regularized Stokeslet segment methodology
used for piecewise linear curves. By using analytic integration of the
regularized Stokeslet kernel over the triangles, the regularization parameter
$\epsilon$ is effectively decoupled from the spatial discretization of the
surface. This is in contrast to the usual implementation of the method in which
the regularization parameter is chosen for accuracy reasons to be about the
same size as the spatial discretization. The validity of the method is
demonstrated through several examples, including the flow around a rigidly
translating/rotating sphere, a rotating spheroid, and the squirmer model for
self-propulsion. Notably, second order convergence in the spatial
discretization for fixed $\epsilon$ is demonstrated. Considerations of mesh
design and choice of regularization parameter are discussed, and the
performance of the method is compared with existing quadrature-based
implementations.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14471" title="Abstract">arXiv:2310.14471</a> [<a href="/pdf/2310.14471" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Electric Vehicle Control Strategy to Mitigate Load Altering Attacks  Against Power Grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sayed%2C+M+A">Mohammad Ali Sayed</a>, 
<a href="/search/eess?searchtype=author&query=Ghafouri%2C+M">Mohsen Ghafouri</a>, 
<a href="/search/eess?searchtype=author&query=Atallah%2C+R">Ribal Atallah</a>, 
<a href="/search/eess?searchtype=author&query=Debbabi%2C+M">Mourad Debbabi</a>, 
<a href="/search/eess?searchtype=author&query=Assi%2C+C">Chadi Assi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the 2023 8th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE)-ICRAIE 2023. arXiv admin note: substantial text overlap with <a href="/abs/2308.07526">arXiv:2308.07526</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Due to growing environmental concerns, the world's governments have been
encouraging the shift of the transportation sector towards the adoption of
Electric Vehicles (EVs). As a result, EV numbers have been growing
exponentially and are expected to continue growing further which will add a
large EV charging load to the power grid. To this end, this paper presents an
EV-based defense mechanism against Load-Altering (LA) attacks targeting the
grid. The developed mechanism utilizes H-infinity controllers and Linear Matrix
Inequalities (LMIs) to mitigate LA attacks. After the controller synthesis and
presentation of the attack scenarios, we demonstrate the effectiveness and
success of our defense mechanism against the three known types of LA attacks.
The scenarios include three 800 MW LA attacks against the New England 39-bus
grid. The results demonstrate how our EV-based mitigation scheme eliminates the
attack impacts and maintains the grid's stability in face of an unknown
persisting attack.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14478" title="Abstract">arXiv:2310.14478</a> [<a href="/pdf/2310.14478" title="Download PDF">pdf</a>, <a href="/format/2310.14478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoLM: Empowering Language Models for Geospatially Grounded Language  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenxuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+Y">Yao-Yi Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP23 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Humans subconsciously engage in geospatial reasoning when reading articles.
We recognize place names and their spatial relations in text and mentally
associate them with their physical locations on Earth. Although pretrained
language models can mimic this cognitive process using linguistic context, they
do not utilize valuable geospatial information in large, widely available
geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a
geospatially grounded language model that enhances the understanding of
geo-entities in natural language. GeoLM leverages geo-entity mentions as
anchors to connect linguistic information in text corpora with geospatial
information extracted from geographical databases. GeoLM connects the two types
of context through contrastive learning and masked language modeling. It also
incorporates a spatial coordinate embedding mechanism to encode distance and
direction relations to capture geospatial context. In the experiment, we
demonstrate that GeoLM exhibits promising capabilities in supporting toponym
recognition, toponym linking, relation extraction, and geo-entity typing, which
bridge the gap between natural language processing and geospatial sciences. The
code is publicly available at https://github.com/knowledge-computing/geolm.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14479" title="Abstract">arXiv:2310.14479</a> [<a href="/pdf/2310.14479" title="Download PDF">pdf</a>, <a href="/format/2310.14479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DetectGPT-SC: Improving Detection of Text Generated by Large Language  Models through Self-Consistency with Masked Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Sihong Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">General large language models (LLMs) such as ChatGPT have shown remarkable
success, but it has also raised concerns among people about the misuse of
AI-generated texts. Therefore, an important question is how to detect whether
the texts are generated by ChatGPT or by humans. Existing detectors are built
on the assumption that there is a distribution gap between human-generated and
AI-generated texts. These gaps are typically identified using statistical
information or classifiers. In contrast to prior research methods, we find that
large language models such as ChatGPT exhibit strong self-consistency in text
generation and continuation. Self-consistency capitalizes on the intuition that
AI-generated texts can still be reasoned with by large language models using
the same logical reasoning when portions of the texts are masked, which differs
from human-generated texts. Using this observation, we subsequently proposed a
new method for AI-generated texts detection based on self-consistency with
masked predictions to determine whether a text is generated by LLMs. This
method, which we call DetectGPT-SC. We conducted a series of experiments to
evaluate the performance of DetectGPT-SC. In these experiments, we employed
various mask scheme, zero-shot, and simple prompt for completing masked texts
and self-consistency predictions. The results indicate that DetectGPT-SC
outperforms the current state-of-the-art across different tasks.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14480" title="Abstract">arXiv:2310.14480</a> [<a href="/pdf/2310.14480" title="Download PDF">pdf</a>, <a href="/format/2310.14480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-Enhancing Backdoor Attacks Against BERT-based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+W">Weimin Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Songzhu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Lu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haibin Ling</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent studies have revealed that \textit{Backdoor Attacks} can threaten the
safety of natural language processing (NLP) models. Investigating the
strategies of backdoor attacks will help to understand the model's
vulnerability. Most existing textual backdoor attacks focus on generating
stealthy triggers or modifying model weights. In this paper, we directly target
the interior structure of neural networks and the backdoor mechanism. We
propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior
by directly manipulating the attention patterns. Our loss can be applied to
different attacking methods to boost their attack efficacy in terms of attack
successful rates and poisoning rates. It applies to not only traditional
dirty-label attacks, but also the more challenging clean-label attacks. We
validate our method on different backbone models (BERT, RoBERTa, and
DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic
Classification).
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14481" title="Abstract">arXiv:2310.14481</a> [<a href="/pdf/2310.14481" title="Download PDF">pdf</a>, <a href="/format/2310.14481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Heterogeneous Graph Learning via Random Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Heterogeneous Graph Neural Networks (HGNNs) are powerful tools for deep
learning on heterogeneous graphs. Typical HGNNs require repetitive message
passing during training, limiting efficiency for large-scale real-world graphs.
Recent pre-computation-based HGNNs use one-time message passing to transform a
heterogeneous graph into regular-shaped tensors, enabling efficient mini-batch
training. Existing pre-computation-based HGNNs can be mainly categorized into
two styles, which differ in how much information loss is allowed and
efficiency. We propose a hybrid pre-computation-based HGNN, named Random
Projection Heterogeneous Graph Neural Network (RpHGNN), which combines the
benefits of one style's efficiency with the low information loss of the other
style. To achieve efficiency, the main framework of RpHGNN consists of
propagate-then-update iterations, where we introduce a Random Projection
Squashing step to ensure that complexity increases only linearly. To achieve
low information loss, we introduce a Relation-wise Neighbor Collection
component with an Even-odd Propagation Scheme, which aims to collect
information from neighbors in a finer-grained way. Experimental results
indicate that our approach achieves state-of-the-art results on seven small and
large benchmark datasets while also being 230% faster compared to the most
effective baseline. Surprisingly, our approach not only surpasses
pre-processing-based baselines but also outperforms end-to-end methods.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14483" title="Abstract">arXiv:2310.14483</a> [<a href="/pdf/2310.14483" title="Download PDF">pdf</a>, <a href="/format/2310.14483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Why Should I Review This Paper?&quot; Unifying Semantic, Topic, and Citation  Factors for Paper-Reviewer Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yanzhen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiusi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bowen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Digital Libraries (cs.DL); Machine Learning (cs.LG)

</div>
<p class="mathjax">As many academic conferences are overwhelmed by a rapidly increasing number
of paper submissions, automatically finding appropriate reviewers for each
submission becomes a more urgent need than ever. Various factors have been
considered by previous attempts on this task to measure the expertise relevance
between a paper and a reviewer, including whether the paper is semantically
close to, shares topics with, and cites previous papers of the reviewer.
However, the majority of previous studies take only one of these factors into
account, leading to an incomprehensive evaluation of paper-reviewer relevance.
To bridge this gap, in this paper, we propose a unified model for
paper-reviewer matching that jointly captures semantic, topic, and citation
factors. In the unified model, a contextualized language model backbone is
shared by all factors to learn common knowledge, while instruction tuning is
introduced to characterize the uniqueness of each factor by producing
factor-aware paper embeddings. Experiments on four datasets (one of which is
newly contributed by us) across different fields, including machine learning,
computer vision, information retrieval, and data mining, consistently validate
the effectiveness of our proposed UniPR model in comparison with
state-of-the-art paper-reviewer matching methods and scientific pre-trained
language models.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14484" title="Abstract">arXiv:2310.14484</a> [<a href="/pdf/2310.14484" title="Download PDF">pdf</a>, <a href="/format/2310.14484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlipDyn with Control: Resource Takeover Games with Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Banik%2C+S">Sandeep Banik</a>, 
<a href="/search/eess?searchtype=author&query=Bopardikar%2C+S+D">Shaunak D. Bopardikar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 Pages, 2 figures. Under review at IEEE TAC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We present the FlipDyn, a dynamic game in which two opponents (a defender and
an adversary) choose strategies to optimally takeover a resource that involves
a dynamical system. At any time instant, each player can take over the resource
and thereby control the dynamical system after incurring a state-dependent and
a control-dependent costs. The resulting model becomes a hybrid dynamical
system where the discrete state (FlipDyn state) determines which player is in
control of the resource. Our objective is to compute the Nash equilibria of
this dynamic zero-sum game. Our contributions are four-fold. First, for any
non-negative costs, we present analytical expressions for the saddle-point
value of the FlipDyn game, along with the corresponding Nash equilibrium (NE)
takeover strategies. Second, for continuous state, linear dynamical systems
with quadratic costs, we establish sufficient conditions under which the game
admits a NE in the space of linear state-feedback policies. Third, for scalar
dynamical systems with quadratic costs, we derive the NE takeover strategies
and saddle-point values independent of the continuous state of the dynamical
system. Fourth and finally, for higher dimensional linear dynamical systems
with quadratic costs, we derive approximate NE takeover strategies and control
policies which enable the computation of bounds on the value functions of the
game in each takeover state. We illustrate our findings through a numerical
study involving the control of a linear dynamical system in the presence of an
adversary.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14485" title="Abstract">arXiv:2310.14485</a> [<a href="/pdf/2310.14485" title="Download PDF">pdf</a>, <a href="/ps/2310.14485" title="Download PostScript">ps</a>, <a href="/format/2310.14485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Escape of Robotic Systems: A Survey of Methodologies,  Applications, and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S+X">Simon X. Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by Journal of Intelligent and Robotic Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Intelligent escape is an interdisciplinary field that employs artificial
intelligence (AI) techniques to enable robots with the capacity to
intelligently react to potential dangers in dynamic, intricate, and
unpredictable scenarios. As the emphasis on safety becomes increasingly
paramount and advancements in robotic technologies continue to advance, a wide
range of intelligent escape methodologies has been developed in recent years.
This paper presents a comprehensive survey of state-of-the-art research work on
intelligent escape of robotic systems. Four main methods of intelligent escape
are reviewed, including planning-based methodologies, partitioning-based
methodologies, learning-based methodologies, and bio-inspired methodologies.
The strengths and limitations of existing methods are summarized. In addition,
potential applications of intelligent escape are discussed in various domains,
such as search and rescue, evacuation, military security, and healthcare. In an
effort to develop new approaches to intelligent escape, this survey identifies
current research challenges and provides insights into future research trends
in intelligent escape.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14486" title="Abstract">arXiv:2310.14486</a> [<a href="/pdf/2310.14486" title="Download PDF">pdf</a>, <a href="/format/2310.14486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Fact Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balepur%2C+N">Nishant Balepur</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text style transfer is a prominent task that aims to control the style of
text without inherently changing its factual content. To cover more text
modification applications, such as adapting past news for current events and
repurposing educational materials, we propose the task of text fact transfer,
which seeks to transfer the factual content of a source text between topics
without modifying its style. We find that existing language models struggle
with text fact transfer, due to their inability to preserve the specificity and
phrasing of the source text, and tendency to hallucinate errors. To address
these issues, we design ModQGA, a framework that minimally modifies a source
text with a novel combination of end-to-end question generation and
specificity-aware question answering. Through experiments on four existing
datasets adapted for text fact transfer, we show that ModQGA can accurately
transfer factual content without sacrificing the style of the source text.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14487" title="Abstract">arXiv:2310.14487</a> [<a href="/pdf/2310.14487" title="Download PDF">pdf</a>, <a href="/format/2310.14487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VQ-NeRF: Vector Quantization Enhances Implicit Neural Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiying Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Fukun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Gang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jiayuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 38th Annual AAAI Conference on Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in implicit neural representations have contributed to
high-fidelity surface reconstruction and photorealistic novel view synthesis.
However, the computational complexity inherent in these methodologies presents
a substantial impediment, constraining the attainable frame rates and
resolutions in practical applications. In response to this predicament, we
propose VQ-NeRF, an effective and efficient pipeline for enhancing implicit
neural representations via vector quantization. The essence of our method
involves reducing the sampling space of NeRF to a lower resolution and
subsequently reinstating it to the original size utilizing a pre-trained VAE
decoder, thereby effectively mitigating the sampling time bottleneck
encountered during rendering. Although the codebook furnishes representative
features, reconstructing fine texture details of the scene remains challenging
due to high compression rates. To overcome this constraint, we design an
innovative multi-scale NeRF sampling scheme that concurrently optimizes the
NeRF model at both compressed and original scales to enhance the network's
ability to preserve fine details. Furthermore, we incorporate a semantic loss
function to improve the geometric fidelity and semantic coherence of our 3D
reconstructions. Extensive experiments demonstrate the effectiveness of our
model in achieving the optimal trade-off between rendering quality and
efficiency. Evaluation on the DTU, BlendMVS, and H3DS datasets confirms the
superior performance of our approach.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14489" title="Abstract">arXiv:2310.14489</a> [<a href="/pdf/2310.14489" title="Download PDF">pdf</a>, <a href="/format/2310.14489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MSFormer: A Skeleton-multiview Fusion Method For Tooth Instance  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yubo Tao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangyang He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaohu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hai Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Recently, deep learning-based tooth segmentation methods have been limited by
the expensive and time-consuming processes of data collection and labeling.
Achieving high-precision segmentation with limited datasets is critical. A
viable solution to this entails fine-tuning pre-trained multiview-based models,
thereby enhancing performance with limited data. However, relying solely on
two-dimensional (2D) images for three-dimensional (3D) tooth segmentation can
produce suboptimal outcomes because of occlusion and deformation, i.e.,
incomplete and distorted shape perception. To improve this fine-tuning-based
solution, this paper advocates 2D-3D joint perception. The fundamental
challenge in employing 2D-3D joint perception with limited data is that the
3D-related inputs and modules must follow a lightweight policy instead of using
huge 3D data and parameter-rich modules that require extensive training data.
Following this lightweight policy, this paper selects skeletons as the 3D
inputs and introduces MSFormer, a novel method for tooth segmentation. MSFormer
incorporates two lightweight modules into existing multiview-based models: a
3D-skeleton perception module to extract 3D perception from skeletons and a
skeleton-image contrastive learning module to obtain the 2D-3D joint perception
by fusing both multiview and skeleton perceptions. The experimental results
reveal that MSFormer paired with large pre-trained multiview models achieves
state-of-the-art performance, requiring only 100 training meshes. Furthermore,
the segmentation accuracy is improved by 2.4%-5.5% with the increasing volume
of training data.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14490" title="Abstract">arXiv:2310.14490</a> [<a href="/pdf/2310.14490" title="Download PDF">pdf</a>, <a href="/format/2310.14490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What is in Your App? Uncovering Privacy Risks of Female Health  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassan%2C+M">Muhammad Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Jameel%2C+M">Mahnoor Jameel</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+M">Masooda Bashir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">FemTech or Female Technology, is an expanding field dedicated to providing
affordable and accessible healthcare solutions for women, prominently through
Female Health Applications that monitor health and reproductive data. With the
leading app exceeding 1 billion downloads, these applications are gaining
widespread popularity. However, amidst contemporary challenges to women's
reproductive rights and privacy, there is a noticeable lack of comprehensive
studies on the security and privacy aspects of these applications. This
exploratory study delves into the privacy risks associated with seven popular
applications. Our initial quantitative static analysis reveals varied and
potentially risky permissions and numerous third-party trackers. Additionally,
a preliminary examination of privacy policies indicates non-compliance with
fundamental data privacy principles. These early findings highlight a critical
gap in establishing robust privacy and security safeguards for FemTech apps,
especially significant in a climate where women's reproductive rights face
escalating threats.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14491" title="Abstract">arXiv:2310.14491</a> [<a href="/pdf/2310.14491" title="Download PDF">pdf</a>, <a href="/format/2310.14491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Mechanistic Interpretation of Multi-Step Reasoning  Capabilities of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yifan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaoda Li</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Y">Yu Fei</a>, 
<a href="/search/cs?searchtype=author&query=Stolfo%2C+A">Alessandro Stolfo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+G">Guangtao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is published in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent work has shown that language models (LMs) have strong multi-step
(i.e., procedural) reasoning capabilities. However, it is unclear whether LMs
perform these tasks by cheating with answers memorized from pretraining corpus,
or, via a multi-step reasoning mechanism. In this paper, we try to answer this
question by exploring a mechanistic interpretation of LMs for multi-step
reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a
reasoning tree resembling the correct reasoning process within it. We test this
hypothesis by introducing a new probing approach (called MechanisticProbe) that
recovers the reasoning tree from the model's attention patterns. We use our
probe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element),
and LLaMA on two simple language-based reasoning tasks (ProofWriter &amp; AI2
Reasoning Challenge). We show that MechanisticProbe is able to detect the
information of the reasoning tree from the model's attentions for most
examples, suggesting that the LM indeed is going through a process of
multi-step reasoning within its architecture in many cases.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14492" title="Abstract">arXiv:2310.14492</a> [<a href="/pdf/2310.14492" title="Download PDF">pdf</a>, <a href="/format/2310.14492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robotic Arm Manipulation to Perform Rock Skipping in Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramirez%2C+N">Nicholas Ramirez</a>, 
<a href="/search/cs?searchtype=author&query=Burgess%2C+M">Michael Burgess</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Rock skipping is a highly dynamic and relatively complex task that can easily
be performed by humans. This project aims to bring rock skipping into a robotic
setting, utilizing the lessons we learned in Robotic Manipulation.
Specifically, this project implements a system consisting of a robotic arm and
dynamic environment to perform rock skipping in simulation. By varying
important parameters such as release velocity, we hope to use our system to
gain insight into the most important factors for maximizing the total number of
skips. In addition, by implementing the system in simulation, we have a more
rigorous and precise testing approach over these varied test parameters.
However, this project experienced some limitations due to gripping
inefficiencies and problems with release height trajectories which is further
discussed in our report.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14495" title="Abstract">arXiv:2310.14495</a> [<a href="/pdf/2310.14495" title="Download PDF">pdf</a>, <a href="/format/2310.14495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructExcel: A Benchmark for Natural Language Instruction in Excel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Payan%2C+J">Justin Payan</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Poelitz%2C+C">Christian Poelitz</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Subhro Roy</a>, 
<a href="/search/cs?searchtype=author&query=Chakravarthy%2C+R">Rasika Chakravarthy</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+E">Elnaz Nouri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023, 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the evolution of Large Language Models (LLMs) we can solve increasingly
more complex NLP tasks across various domains, including spreadsheets. This
work investigates whether LLMs can generate code (Excel OfficeScripts, a
TypeScript API for executing many tasks in Excel) that solves Excel specific
tasks provided via natural language user instructions. To do so we introduce a
new large-scale benchmark, InstructExcel, created by leveraging the 'Automate'
feature in Excel to automatically generate OfficeScripts from users' actions.
Our benchmark includes over 10k samples covering 170+ Excel operations across
2,000 publicly available Excel spreadsheets. Experiments across various
zero-shot and few-shot settings show that InstructExcel is a hard benchmark for
state of the art models like GPT-4. We observe that (1) using GPT-4 over
GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can
help improve performance on this benchmark.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14496" title="Abstract">arXiv:2310.14496</a> [<a href="/pdf/2310.14496" title="Download PDF">pdf</a>, <a href="/format/2310.14496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redundancy-Adaptive Multimodal Learning for Imperfect Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiangchao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+L">Linyu Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Multimodal models trained on complete modality data often exhibit a
substantial decrease in performance when faced with imperfect data containing
corruptions or missing modalities. To address this robustness challenge, prior
methods have explored various approaches from aspects of augmentation,
consistency or uncertainty, but these approaches come with associated drawbacks
related to data complexity, representation, and learning, potentially
diminishing their overall effectiveness. In response to these challenges, this
study introduces a novel approach known as the Redundancy-Adaptive Multimodal
Learning (RAML). RAML efficiently harnesses information redundancy across
multiple modalities to combat the issues posed by imperfect data while
remaining compatible with the complete modality. Specifically, RAML achieves
redundancy-lossless information extraction through separate unimodal
discriminative tasks and enforces a proper norm constraint on each unimodal
feature representation. Furthermore, RAML explicitly enhances multimodal fusion
by leveraging fine-grained redundancy among unimodal features to learn
correspondences between corrupted and untainted information. Extensive
experiments on various benchmark datasets under diverse conditions have
consistently demonstrated that RAML outperforms state-of-the-art methods by a
significant margin.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14497" title="Abstract">arXiv:2310.14497</a> [<a href="/pdf/2310.14497" title="Download PDF">pdf</a>, <a href="/format/2310.14497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Explanation Generation with s(CASP)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+S">Sopam Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Shakerin%2C+F">Farhad Shakerin</a>, 
<a href="/search/cs?searchtype=author&query=Arias%2C+J">Joaqu&#xed;n Arias</a>, 
<a href="/search/cs?searchtype=author&query=Salazar%2C+E">Elmer Salazar</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+G">Gopal Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Machine learning models that automate decision-making are increasingly being
used in consequential areas such as loan approvals, pretrial bail, hiring, and
many more. Unfortunately, most of these models are black-boxes, i.e., they are
unable to reveal how they reach these prediction decisions. A need for
transparency demands justification for such predictions. An affected individual
might desire explanations to understand why a decision was made. Ethical and
legal considerations may further require informing the individual of changes in
the input attribute that could be made to produce a desirable outcome. This
paper focuses on the latter problem of automatically generating counterfactual
explanations. Our approach utilizes answer set programming and the s(CASP)
goal-directed ASP system. Answer Set Programming (ASP) is a well-known
knowledge representation and reasoning paradigm. s(CASP) is a goal-directed ASP
system that executes answer-set programs top-down without grounding them. The
query-driven nature of s(CASP) allows us to provide justifications as proof
trees, which makes it possible to analyze the generated counterfactual
explanations. We show how counterfactual explanations are computed and
justified by imagining multiple possible worlds where some or all factual
assumptions are untrue and, more importantly, how we can navigate between these
worlds. We also show how our algorithm can be used to find the Craig
Interpolant for a class of answer set programs for a failing query.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14500" title="Abstract">arXiv:2310.14500</a> [<a href="/pdf/2310.14500" title="Download PDF">pdf</a>, <a href="/format/2310.14500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coyote C++: An Industrial-Strength Fully Automated Unit Testing Tool
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rho%2C+S">Sanghoon Rho</a>, 
<a href="/search/cs?searchtype=author&query=Martens%2C+P">Philipp Martens</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Seungcheol Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yeoneo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Heo%2C+H">Hoon Heo</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">SeungHyun Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Coyote C++ is an automated testing tool that uses a sophisticated
concolic-execution-based approach to realize fully automated unit testing for C
and C++. While concolic testing has proven effective for languages such as C
and Java, tools have struggled to achieve a practical level of automation for
C++ due to its many syntactical intricacies and overall complexity. Coyote C++
is the first automated testing tool to breach the barrier and bring automated
unit testing for C++ to a practical level suitable for industrial adoption,
consistently reaching around 90% code coverage. Notably, this testing process
requires no user involvement and performs test harness generation, test case
generation and test execution with "one-click" automation. In this paper, we
introduce Coyote C++ by outlining its high-level structure and discussing the
core design decisions that shaped the implementation of its concolic execution
engine. Finally, we demonstrate that Coyote C++ is capable of achieving high
coverage results within a reasonable timespan by presenting the results from
experiments on both open-source and industrial software.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14503" title="Abstract">arXiv:2310.14503</a> [<a href="/pdf/2310.14503" title="Download PDF">pdf</a>, <a href="/format/2310.14503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversify Question Generation with Retrieval-Augmented Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Q">Qi Gou</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zehua Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Cam-Tu%2C+N">Nguyen Cam-Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Given a textual passage and an answer, humans are able to ask questions with
various expressions, but this ability is still challenging for most question
generation (QG) systems. Existing solutions mainly focus on the internal
knowledge within the given passage or the semantic word space for diverse
content planning. These methods, however, have not considered the potential of
external knowledge for expression diversity. To bridge this gap, we propose
RAST, a framework for Retrieval-Augmented Style Transfer, where the objective
is to utilize the style of diverse templates for question generation. For
training RAST, we develop a novel Reinforcement Learning (RL) based approach
that maximizes a weighted combination of diversity reward and consistency
reward. Here, the consistency reward is computed by a Question-Answering (QA)
model, whereas the diversity reward measures how much the final output mimics
the retrieved template. Experimental results show that our method outperforms
previous diversity-driven baselines on diversity while being comparable in
terms of consistency scores. Our code is available at
https://github.com/gouqi666/RAST.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14504" title="Abstract">arXiv:2310.14504</a> [<a href="/pdf/2310.14504" title="Download PDF">pdf</a>, <a href="/format/2310.14504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADoPT: LiDAR Spoofing Attack Detection Based on Point-Level Temporal  Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minkyoung Cho</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yulong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zixiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z+M">Z. Morley Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BMVC 2023 (17 pages, 13 figures, and 1 table)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks (DNNs) are increasingly integrated into LiDAR (Light
Detection and Ranging)-based perception systems for autonomous vehicles (AVs),
requiring robust performance under adversarial conditions. We aim to address
the challenge of LiDAR spoofing attacks, where attackers inject fake objects
into LiDAR data and fool AVs to misinterpret their environment and make
erroneous decisions. However, current defense algorithms predominantly depend
on perception outputs (i.e., bounding boxes) thus face limitations in detecting
attackers given the bounding boxes are generated by imperfect perception models
processing limited points, acquired based on the ego vehicle's viewpoint. To
overcome these limitations, we propose a novel framework, named ADoPT (Anomaly
Detection based on Point-level Temporal consistency), which quantitatively
measures temporal consistency across consecutive frames and identifies abnormal
objects based on the coherency of point clusters. In our evaluation using the
nuScenes dataset, our algorithm effectively counters various LiDAR spoofing
attacks, achieving a low (&lt; 10%) false positive ratio (FPR) and high (&gt; 85%)
true positive ratio (TPR), outperforming existing state-of-the-art defense
methods, CARLO and 3D-TC2. Furthermore, our evaluation demonstrates the
promising potential for accurate attack detection across various road
environments.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14505" title="Abstract">arXiv:2310.14505</a> [<a href="/pdf/2310.14505" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment analysis with adaptive multi-head attention in Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanfei Meng</a>, 
<a href="/search/cs?searchtype=author&query=Demeter%2C+D">David Demeter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 4th International Conference on Signal Processing and Machine Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose a novel framework based on the attention mechanism to identify the
sentiment of a movie review document. Previous efforts on deep neural networks
with attention mechanisms focus on encoder and decoder with fixed numbers of
multi-head attention. Therefore, we need a mechanism to stop the attention
process automatically if no more useful information can be read from the
memory.In this paper, we propose an adaptive multi-head attention architecture
(AdaptAttn) which varies the number of attention heads based on length of
sentences. AdaptAttn has a data preprocessing step where each document is
classified into any one of the three bins small, medium or large based on
length of the sentence. The document classified as small goes through two heads
in each layer, the medium group passes four heads and the large group is
processed by eight heads. We examine the merit of our model on the Stanford
large movie review dataset. The experimental results show that the F1 score
from our model is on par with the baseline model.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14507" title="Abstract">arXiv:2310.14507</a> [<a href="/pdf/2310.14507" title="Download PDF">pdf</a>, <a href="/format/2310.14507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Marching based Rendezvous Path Planning for a Team of Heterogeneous  Vehicle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaekwang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyung-Jun Park</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jaejeong Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">A formulation is developed for deterministically calculating the optimized
paths for a multi-agent system consisting of heterogeneous vehicles. The
essence of this formulation is the calculation of the shortest time for each
agent to reach every grid point from its known initial position. Such arrival
time map can be readily assessed using the Fast Marching Method (FMM), a
computational algorithm originally designed for solving boundary value problems
of the Eikonal equation. Leveraging the FMM method, we demonstrate that the
minimal time rendezvous point and paths for all member vehicles can be uniquely
determined with minimal computational concerns. To showcase the potential of
our method, we use an example of a virtual rendezvous scenario that entails the
coordination of a ship, an underwater vehicle, an aerial vehicle, and a ground
vehicle to converge at the optimal location within the Tampa Bay area in
minimal time. It illustrates the value of the developed framework in
efficiently constructing continuous path planning, while accommodating
different operational constraints of heterogeneous member vehicles.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14508" title="Abstract">arXiv:2310.14508</a> [<a href="/pdf/2310.14508" title="Download PDF">pdf</a>, <a href="/format/2310.14508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data  Augmentation for Multi-hop Fact Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yingjie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+J">Jiasheng Si</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yibo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haiyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Deyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic multi-hop fact verification task has gained significant attention
in recent years. Despite impressive results, these well-designed models perform
poorly on out-of-domain data. One possible solution is to augment the training
data with counterfactuals, which are generated by minimally altering the causal
features of the original data. However, current counterfactual data
augmentation techniques fail to handle multi-hop fact verification due to their
incapability to preserve the complex logical relationships within multiple
correlated texts. In this paper, we overcome this limitation by developing a
rationale-sensitive method to generate linguistically diverse and
label-flipping counterfactuals while preserving logical relationships. In
specific, the diverse and fluent counterfactuals are generated via an
Explain-Edit-Generate architecture. Moreover, the checking and filtering
modules are proposed to regularize the counterfactual data with logical
relations and flipped labels. Experimental results show that the proposed
approach outperforms the SOTA baselines and can generate linguistically diverse
counterfactual data without disrupting their logical relationships.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14509" title="Abstract">arXiv:2310.14509</a> [<a href="/pdf/2310.14509" title="Download PDF">pdf</a>, <a href="/format/2310.14509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iteratively Learn Diverse Strategies with State Distance Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+W">Wei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Weihua Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sunli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In complex reinforcement learning (RL) problems, policies with similar
rewards may have substantially different behaviors. It remains a fundamental
challenge to optimize rewards while also discovering as many diverse strategies
as possible, which can be crucial in many practical applications. Our study
examines two design choices for tackling this challenge, i.e., diversity
measure and computation framework. First, we find that with existing diversity
measures, visually indistinguishable policies can still yield high diversity
scores. To accurately capture the behavioral difference, we propose to
incorporate the state-space distance information into the diversity measure. In
addition, we examine two common computation frameworks for this problem, i.e.,
population-based training (PBT) and iterative learning (ITR). We show that
although PBT is the precise problem formulation, ITR can achieve comparable
diversity scores with higher computation efficiency, leading to improved
solution quality in practice. Based on our analysis, we further combine ITR
with two tractable realizations of the state-distance-based diversity measures
and develop a novel diversity-driven RL algorithm, State-based Intrinsic-reward
Policy Optimization (SIPO), with provable convergence properties. We
empirically examine SIPO across three domains from robot locomotion to
multi-agent games. In all of our testing environments, SIPO consistently
produces strategically diverse and human-interpretable policies that cannot be
discovered by existing baselines.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14510" title="Abstract">arXiv:2310.14510</a> [<a href="/pdf/2310.14510" title="Download PDF">pdf</a>, <a href="/format/2310.14510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CITB: A Benchmark for Continual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Namazi-Rad%2C+M">Mohammad-Reza Namazi-Rad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Continual learning (CL) is a paradigm that aims to replicate the human
ability to learn and accumulate knowledge continually without forgetting
previous knowledge and transferring it to new tasks. Recent instruction tuning
(IT) involves fine-tuning models to make them more adaptable to solving NLP
tasks in general. However, it is still uncertain how instruction tuning works
in the context of CL tasks. This challenging yet practical problem is
formulated as Continual Instruction Tuning (CIT). In this work, we establish a
CIT benchmark consisting of learning and evaluation protocols. We curate two
long dialogue task streams of different types, InstrDialog and InstrDialog++,
to study various CL methods systematically. Our experiments show that existing
CL methods do not effectively leverage the rich natural language instructions,
and fine-tuning an instruction-tuned model sequentially can yield similar or
better results. We further explore different aspects that might affect the
learning of CIT. We hope this benchmark will facilitate more research in this
direction.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14511" title="Abstract">arXiv:2310.14511</a> [<a href="/pdf/2310.14511" title="Download PDF">pdf</a>, <a href="/format/2310.14511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poster: Real-Time Object Substitution for Mobile Diminished Reality with  Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+H">Hongyu Ke</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxin Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SEC '2023 The Eighth ACM/IEEE Symposium on Edge Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Diminished Reality (DR) is considered as the conceptual counterpart to
Augmented Reality (AR), and has recently gained increasing attention from both
industry and academia. Unlike AR which adds virtual objects to the real world,
DR allows users to remove physical content from the real world. When combined
with object replacement technology, it presents an further exciting avenue for
exploration within the metaverse. Although a few researches have been conducted
on the intersection of object substitution and DR, there is no real-time object
substitution for mobile diminished reality architecture with high quality. In
this paper, we propose an end-to-end architecture to facilitate immersive and
real-time scene construction for mobile devices with edge computing.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14512" title="Abstract">arXiv:2310.14512</a> [<a href="/pdf/2310.14512" title="Download PDF">pdf</a>, <a href="/format/2310.14512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CorefPrompt: Prompt-based Event Coreference Resolution by Measuring  Event Type and Argument Compatibilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Sheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiaoming Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Event coreference resolution (ECR) aims to group event mentions referring to
the same real-world event into clusters. Most previous studies adopt the
"encoding first, then scoring" framework, making the coreference judgment rely
on event encoding. Furthermore, current methods struggle to leverage
human-summarized ECR rules, e.g., coreferential events should have the same
event type, to guide the model. To address these two issues, we propose a
prompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM
(masked language model) task. This allows for simultaneous event modeling and
coreference discrimination within a single template, with a fully shared
context. In addition, we introduce two auxiliary prompt tasks, event-type
compatibility and argument compatibility, to explicitly demonstrate the
reasoning process of ECR, which helps the model make final predictions.
Experimental results show that our method CorefPrompt performs well in a
state-of-the-art (SOTA) benchmark.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14513" title="Abstract">arXiv:2310.14513</a> [<a href="/pdf/2310.14513" title="Download PDF">pdf</a>, <a href="/format/2310.14513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turn-Level Active Learning for Dialogue State Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Fanghua Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Namazi-Rad%2C+M">Mohammad-Reza Namazi-Rad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dialogue state tracking (DST) plays an important role in task-oriented
dialogue systems. However, collecting a large amount of turn-by-turn annotated
dialogue data is costly and inefficient. In this paper, we propose a novel
turn-level active learning framework for DST to actively select turns in
dialogues to annotate. Given the limited labelling budget, experimental results
demonstrate the effectiveness of selective annotation of dialogue turns.
Additionally, our approach can effectively achieve comparable DST performance
to traditional training approaches with significantly less annotated data,
which provides a more efficient way to annotate new dialogue data.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14516" title="Abstract">arXiv:2310.14516</a> [<a href="/pdf/2310.14516" title="Download PDF">pdf</a>, <a href="/format/2310.14516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundations of Quantum Federated Learning Over Classical and Quantum  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chehimi%2C+M">Mahdi Chehimi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S+Y">Samuel Yen-Chi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Saad%2C+W">Walid Saad</a>, 
<a href="/search/cs?searchtype=author&query=Towsley%2C+D">Don Towsley</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">M&#xe9;rouane Debbah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Quantum federated learning (QFL) is a novel framework that integrates the
advantages of classical federated learning (FL) with the computational power of
quantum technologies. This includes quantum computing and quantum machine
learning (QML), enabling QFL to handle high-dimensional complex data. QFL can
be deployed over both classical and quantum communication networks in order to
benefit from information-theoretic security levels surpassing traditional FL
frameworks. In this paper, we provide the first comprehensive investigation of
the challenges and opportunities of QFL. We particularly examine the key
components of QFL and identify the unique challenges that arise when deploying
it over both classical and quantum networks. We then develop novel solutions
and articulate promising research directions that can help address the
identified challenges. We also provide actionable recommendations to advance
the practical realization of QFL.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14520" title="Abstract">arXiv:2310.14520</a> [<a href="/pdf/2310.14520" title="Download PDF">pdf</a>, <a href="/format/2310.14520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yating Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mangla%2C+R">Ritika Mangla</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready for EMNLP Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Questions Under Discussion (QUD) is a versatile linguistic framework in which
discourse progresses as continuously asking questions and answering them.
Automatic parsing of a discourse to produce a QUD structure thus entails a
complex question generation task: given a document and an answer sentence,
generate a question that satisfies linguistic constraints of QUD and can be
grounded in an anchor sentence in prior context. These questions are known to
be curiosity-driven and open-ended. This work introduces the first framework
for the automatic evaluation of QUD parsing, instantiating the theoretical
constraints of QUD in a concrete protocol. We present QUDeval, a dataset of
fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned
systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD
is still challenging for modern LLMs, and that existing evaluation metrics
poorly approximate parser quality. Encouragingly, human-authored QUDs are
scored highly by our human evaluators, suggesting that there is headroom for
further progress on language modeling to improve both QUD parsing and QUD
evaluation.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14523" title="Abstract">arXiv:2310.14523</a> [<a href="/pdf/2310.14523" title="Download PDF">pdf</a>, <a href="/format/2310.14523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Word-Level Auto-Completion in Computer-Aided Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guoping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhirui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Word-Level Auto-Completion (WLAC) plays a crucial role in Computer-Assisted
Translation. It aims at providing word-level auto-completion suggestions for
human translators. While previous studies have primarily focused on designing
complex model architectures, this paper takes a different perspective by
rethinking the fundamental question: what kind of words are good
auto-completions? We introduce a measurable criterion to answer this question
and discover that existing WLAC models often fail to meet this criterion.
Building upon this observation, we propose an effective approach to enhance
WLAC performance by promoting adherence to the criterion. Notably, the proposed
approach is general and can be applied to various encoder-based architectures.
Through extensive experiments, we demonstrate that our approach outperforms the
top-performing system submitted to the WLAC shared tasks in WMT2022, while
utilizing significantly smaller model sizes.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14525" title="Abstract">arXiv:2310.14525</a> [<a href="/pdf/2310.14525" title="Download PDF">pdf</a>, <a href="/format/2310.14525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do We Really Need Contrastive Learning for Graph Representation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yulan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Sheng Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Ge Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhirui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Junchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, contrastive learning has emerged as a dominant
self-supervised paradigm, attracting numerous research interests in the field
of graph learning. Graph contrastive learning (GCL) aims to embed augmented
anchor samples close to each other while pushing the embeddings of other
samples (negative samples) apart. However, existing GCL methods require large
and diverse negative samples to ensure the quality of embeddings, and recent
studies typically leverage samples excluding the anchor and positive samples as
negative samples, potentially introducing false negative samples (negatives
that share the same class as the anchor). Additionally, this practice can
result in heavy computational burden and high time complexity of $O(N^2)$,
which is particularly unaffordable for large graphs. To address these
deficiencies, we leverage rank learning and propose a simple yet effective
model, GraphRank. Specifically, we first generate two graph views through
corruption. Then, we compute the similarity of pairwise nodes (anchor node and
positive node) in both views, an arbitrary node in the latter view is selected
as a negative node, and its similarity with the anchor node is computed. Based
on this, we introduce rank-based learning to measure similarity scores which
successfully relieve the false negative provlem and decreases the time
complexity from $O(N^2)$ to $O(N)$. Moreover, we conducted extensive
experiments across multiple graph tasks, demonstrating that GraphRank performs
favorably against other cutting-edge GCL methods in various tasks.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14526" title="Abstract">arXiv:2310.14526</a> [<a href="/pdf/2310.14526" title="Download PDF">pdf</a>, <a href="/format/2310.14526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Zero Shot Learning in Restless Multi-armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunfan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Behari%2C+N">Nikhil Behari</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+E">Edward Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Edwin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nagaraj%2C+D">Dheeraj Nagaraj</a>, 
<a href="/search/cs?searchtype=author&query=Tuyls%2C+K">Karl Tuyls</a>, 
<a href="/search/cs?searchtype=author&query=Taneja%2C+A">Aparna Taneja</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Restless multi-arm bandits (RMABs), a class of resource allocation problems
with broad application in areas such as healthcare, online advertising, and
anti-poaching, have recently been studied from a multi-agent reinforcement
learning perspective. Prior RMAB research suffers from several limitations,
e.g., it fails to adequately address continuous states, and requires retraining
from scratch when arms opt-in and opt-out over time, a common challenge in many
real world applications. We address these limitations by developing a neural
network-based pre-trained model (PreFeRMAB) that has general zero-shot ability
on a wide range of previously unseen RMABs, and which can be fine-tuned on
specific instances in a more sample-efficient way than retraining from scratch.
Our model also accommodates general multi-action settings and discrete or
continuous state spaces. To enable fast generalization, we learn a novel single
policy network model that utilizes feature information and employs a training
procedure in which arms opt-in and out over time. We derive a new update rule
for a crucial $\lambda$-network with theoretical convergence guarantees and
empirically demonstrate the advantages of our approach on several challenging,
real-world inspired problems.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14527" title="Abstract">arXiv:2310.14527</a> [<a href="/pdf/2310.14527" title="Download PDF">pdf</a>, <a href="/format/2310.14527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Marginal Nodes Matter: Towards Structure Fairness in Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaotian Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaixiong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting-Hsiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jundong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+N">Na Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGKDD Explorations (To Appear)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In social network, a person located at the periphery region (marginal node)
is likely to be treated unfairly when compared with the persons at the center.
While existing fairness works on graphs mainly focus on protecting sensitive
attributes (e.g., age and gender), the fairness incurred by the graph structure
should also be given attention. On the other hand, the information aggregation
mechanism of graph neural networks amplifies such structure unfairness, as
marginal nodes are often far away from other nodes. In this paper, we focus on
novel fairness incurred by the graph structure on graph neural networks, named
\emph{structure fairness}. Specifically, we first analyzed multiple graphs and
observed that marginal nodes in graphs have a worse performance of downstream
tasks than others in graph neural networks. Motivated by the observation, we
propose \textbf{S}tructural \textbf{Fair} \textbf{G}raph \textbf{N}eural
\textbf{N}etwork (SFairGNN), which combines neighborhood expansion based
structure debiasing with hop-aware attentive information aggregation to achieve
structure fairness. Our experiments show \SFairGNN can significantly improve
structure fairness while maintaining overall performance in the downstream
tasks.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14528" title="Abstract">arXiv:2310.14528</a> [<a href="/pdf/2310.14528" title="Download PDF">pdf</a>, <a href="/format/2310.14528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Tianyuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zijian Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Efficient knowledge retrieval plays a pivotal role in ensuring the success of
end-to-end task-oriented dialogue systems by facilitating the selection of
relevant information necessary to fulfill user requests. However, current
approaches generally integrate knowledge retrieval and response generation,
which poses scalability challenges when dealing with extensive knowledge bases.
Taking inspiration from open-domain question answering, we propose a
retriever-generator architecture that harnesses a retriever to retrieve
pertinent knowledge and a generator to generate system responses.~Due to the
lack of retriever training labels, we propose relying on feedback from the
generator as pseudo-labels to train the retriever. To achieve this, we
introduce a dual-feedback mechanism that generates both positive and negative
feedback based on the output of the generator. Our method demonstrates superior
performance in task-oriented dialogue tasks, as evidenced by experimental
results on three benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14532" title="Abstract">arXiv:2310.14532</a> [<a href="/pdf/2310.14532" title="Download PDF">pdf</a>, <a href="/format/2310.14532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Deep Dispersed Watermarking with Synchronization and Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hengchang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qilong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Junwei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+F">Feng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenbin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiaodong Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minglei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpeted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep learning based blind watermarking works have gradually emerged and
achieved impressive performance. However, previous deep watermarking studies
mainly focus on fixed low-resolution images while paying less attention to
arbitrary resolution images, especially widespread high-resolution images
nowadays. Moreover, most works usually demonstrate robustness against typical
non-geometric attacks (\textit{e.g.}, JPEG compression) but ignore common
geometric attacks (\textit{e.g.}, Rotate) and more challenging combined
attacks. To overcome the above limitations, we propose a practical deep
\textbf{D}ispersed \textbf{W}atermarking with \textbf{S}ynchronization and
\textbf{F}usion, called \textbf{\proposed}. Specifically, given an
arbitrary-resolution cover image, we adopt a dispersed embedding scheme which
sparsely and randomly selects several fixed small-size cover blocks to embed a
consistent watermark message by a well-trained encoder. In the extraction
stage, we first design a watermark synchronization module to locate and rectify
the encoded blocks in the noised watermarked image. We then utilize a decoder
to obtain messages embedded in these blocks, and propose a message fusion
strategy based on similarity to make full use of the consistency among
messages, thus determining a reliable message. Extensive experiments conducted
on different datasets convincingly demonstrate the effectiveness of our
proposed {\proposed}. Compared with state-of-the-art approaches, our blind
watermarking can achieve better performance: averagely improve the bit accuracy
by 5.28\% and 5.93\% against single and combined attacks, respectively, and
show less file size increment and better visual quality. Our code is available
at https://github.com/bytedance/DWSF.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14533" title="Abstract">arXiv:2310.14533</a> [<a href="/pdf/2310.14533" title="Download PDF">pdf</a>, <a href="/format/2310.14533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Aware Prediction of User Engagement on Online Social Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peters%2C+H">Heinrich Peters</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yozen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Barbieri%2C+F">Francesco Barbieri</a>, 
<a href="/search/cs?searchtype=author&query=Baten%2C+R+A">Raiyan A. Baten</a>, 
<a href="/search/cs?searchtype=author&query=Matz%2C+S+C">Sandra C. Matz</a>, 
<a href="/search/cs?searchtype=author&query=Bos%2C+M+W">Maarten W. Bos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">The success of online social platforms hinges on their ability to predict and
understand user behavior at scale. Here, we present data suggesting that
context-aware modeling approaches may offer a holistic yet lightweight and
potentially privacy-preserving representation of user engagement on online
social platforms. Leveraging deep LSTM neural networks to analyze more than 100
million Snapchat sessions from almost 80.000 users, we demonstrate that
patterns of active and passive use are predictable from past behavior
(R2=0.345) and that the integration of context information substantially
improves predictive performance compared to the behavioral baseline model
(R2=0.522). Features related to smartphone connectivity status, location,
temporal context, and weather were found to capture non-redundant variance in
user engagement relative to features derived from histories of in-app
behaviors. Further, we show that a large proportion of variance can be
accounted for with minimal behavioral histories if momentary context
information is considered (R2=0.44). These results indicate the potential of
context-aware approaches for making models more efficient and
privacy-preserving by reducing the need for long data histories. Finally, we
employ model explainability techniques to glean preliminary insights into the
underlying behavioral mechanisms. Our findings are consistent with the notion
of context-contingent, habit-driven patterns of active and passive use,
underscoring the value of contextualized representations of user behavior for
predicting user engagement on social platforms.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14534" title="Abstract">arXiv:2310.14534</a> [<a href="/pdf/2310.14534" title="Download PDF">pdf</a>, <a href="/format/2310.14534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Seq2Seq Grammatical Error Correction via Decoding  Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Houquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yumeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenghua Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The sequence-to-sequence (Seq2Seq) approach has recently been widely used in
grammatical error correction (GEC) and shows promising performance. However,
the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC
model can only be trained on parallel data, which, in GEC task, is often noisy
and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an
explicit awareness of the correctness of the token being generated. In this
paper, we propose a unified decoding intervention framework that employs an
external critic to assess the appropriateness of the token to be generated
incrementally, and then dynamically influence the choice of the next token. We
discover and investigate two types of critics: a pre-trained left-to-right
language model critic and an incremental target-side grammatical error detector
critic. Through extensive experiments on English and Chinese datasets, our
framework consistently outperforms strong baselines and achieves results
competitive with state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14536" title="Abstract">arXiv:2310.14536</a> [<a href="/pdf/2310.14536" title="Download PDF">pdf</a>, <a href="/format/2310.14536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Training Realized Volatility Prediction Model with Neural  Distributional Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xin Du</a>, 
<a href="/search/cs?searchtype=author&query=Moriyama%2C+K">Kai Moriyama</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka-Ishii%2C+K">Kumiko Tanaka-Ishii</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICAIF'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">This paper shows a novel machine learning model for realized volatility (RV)
prediction using a normalizing flow, an invertible neural network. Since RV is
known to be skewed and have a fat tail, previous methods transform RV into
values that follow a latent distribution with an explicit shape and then apply
a prediction model. However, knowing that shape is non-trivial, and the
transformation result influences the prediction model. This paper proposes to
jointly train the transformation and the prediction model. The training process
follows a maximum-likelihood objective function that is derived from the
assumption that the prediction residuals on the transformed RV time series are
homogeneously Gaussian. The objective function is further approximated using an
expectation-maximum algorithm. On a dataset of 100 stocks, our method
significantly outperforms other methods using analytical or naive
neural-network transformations.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14538" title="Abstract">arXiv:2310.14538</a> [<a href="/pdf/2310.14538" title="Download PDF">pdf</a>, <a href="/ps/2310.14538" title="Download PostScript">ps</a>, <a href="/format/2310.14538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Complex Channel Estimation in Extra-Large Scale MIMO with the  Spherical Wave Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+X">Xumin Pu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhinan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qianbin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages with 3 figures, accepted by Physical Communication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper investigates the low-complex linear minimum mean squared error
(LMMSE) channel estimation in an extra-large scale MIMO system with the
spherical wave model (SWM). We model the extra-large scale MIMO channels using
the SWM in the terahertz (THz) line-of-sight propagation, in which the
transceiver is a uniform circular antenna array. On this basis, for the known
channel covariance matrix (CCM), a low-complex LMMSE channel estimation
algorithm is proposed by exploiting the spherical wave properties (SWP).
Meanwhile, for the unknown CCM, a similar low-complex LMMSE channel estimation
algorithm is also proposed. Both theoretical and simulation results show that
the proposed algorithm has lower complexity without reducing the accuracy of
channel estimation.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14540" title="Abstract">arXiv:2310.14540</a> [<a href="/pdf/2310.14540" title="Download PDF">pdf</a>, <a href="/format/2310.14540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Spatial Understanding of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamada%2C+Y">Yutaro Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yihan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Lampinen%2C+A+K">Andrew K. Lampinen</a>, 
<a href="/search/cs?searchtype=author&query=Kasai%2C+J">Jungo Kasai</a>, 
<a href="/search/cs?searchtype=author&query=Yildirim%2C+I">Ilker Yildirim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) show remarkable capabilities across a variety of
tasks. Despite the models only seeing text in training, several recent studies
suggest that LLM representations implicitly capture aspects of the underlying
grounded concepts. Here, we explore LLM representations of a particularly
salient kind of grounded knowledge -- spatial relationships. We design
natural-language navigation tasks and evaluate the ability of LLMs, in
particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and
reason about spatial structures, and compare these abilities to human
performance on the same tasks. These tasks reveal substantial variability in
LLM performance across different spatial structures, including square,
hexagonal, and triangular grids, rings, and trees. We also discover that,
similar to humans, LLMs utilize object names as landmarks for maintaining
spatial maps. Finally, in extensive error analysis, we find that LLMs' mistakes
reflect both spatial and non-spatial factors. These findings suggest that LLMs
appear to capture certain aspects of spatial structure implicitly, but room for
improvement remains.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14541" title="Abstract">arXiv:2310.14541</a> [<a href="/pdf/2310.14541" title="Download PDF">pdf</a>, <a href="/format/2310.14541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Named Entity Recognition without Catastrophic Forgetting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Duzhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+W">Wei Cong</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiahua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yahan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhen Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023 main conference as a long paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Continual Named Entity Recognition (CNER) is a burgeoning area, which
involves updating an existing model by incorporating new entity types
sequentially. Nevertheless, continual learning approaches are often severely
afflicted by catastrophic forgetting. This issue is intensified in CNER due to
the consolidation of old entity types from previous steps into the non-entity
type at each step, leading to what is known as the semantic shift problem of
the non-entity type. In this paper, we introduce a pooled feature distillation
loss that skillfully navigates the trade-off between retaining knowledge of old
entity types and acquiring new ones, thereby more effectively mitigating the
problem of catastrophic forgetting. Additionally, we develop a confidence-based
pseudo-labeling for the non-entity type, \emph{i.e.,} predicting entity types
using the old model to handle the semantic shift of the non-entity type.
Following the pseudo-labeling process, we suggest an adaptive re-weighting
type-balanced learning strategy to handle the issue of biased type
distribution. We carried out comprehensive experiments on ten CNER settings
using three different datasets. The results illustrate that our method
significantly outperforms prior state-of-the-art approaches, registering an
average improvement of $6.3$\% and $8.0$\% in Micro and Macro F1 scores,
respectively.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14542" title="Abstract">arXiv:2310.14542</a> [<a href="/pdf/2310.14542" title="Download PDF">pdf</a>, <a href="/format/2310.14542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Large Language Models on Controlled Generation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yufei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rahul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Wieting%2C+J+F">John Frederick Wieting</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuezhe Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While recent studies have looked into the abilities of large language models
in various benchmark tasks, including question generation, reading
comprehension, multilingual and etc, there have been few studies looking into
the controllability of large language models on generation tasks. We present an
extensive analysis of various benchmarks including a sentence planning
benchmark with different granularities. After comparing large language models
against state-of-the-start finetuned smaller models, we present a spectrum
showing large language models falling behind, are comparable, or exceed the
ability of smaller models. We conclude that **large language models struggle at
meeting fine-grained hard constraints**.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14545" title="Abstract">arXiv:2310.14545</a> [<a href="/pdf/2310.14545" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing ChatGPT for thematic analysis: Are we ready?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+V+V">V Vien Lee</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Lubbe%2C+S+C+C">Stephanie C. C. van der Lubbe</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+L+H">Lay Hoon Goh</a>, 
<a href="/search/cs?searchtype=author&query=Valderas%2C+J+M">Jose M. Valderas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures, 3 tables, 1 textbox
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">ChatGPT is an advanced natural language processing tool with growing
applications across various disciplines in medical research. Thematic analysis,
a qualitative research method to identify and interpret patterns in data, is
one application that stands to benefit from this technology. This viewpoint
explores the utilization of ChatGPT in three core phases of thematic analysis
within a medical context: 1) direct coding of transcripts, 2) generating themes
from a predefined list of codes, and 3) preprocessing quotes for manuscript
inclusion. Additionally, we explore the potential of ChatGPT to generate
interview transcripts, which may be used for training purposes. We assess the
strengths and limitations of using ChatGPT in these roles, highlighting areas
where human intervention remains necessary. Overall, we argue that ChatGPT can
function as a valuable tool during analysis, enhancing the efficiency of the
thematic analysis and offering additional insights into the qualitative data.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14548" title="Abstract">arXiv:2310.14548</a> [<a href="/pdf/2310.14548" title="Download PDF">pdf</a>, <a href="/format/2310.14548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test Smell: A Parasitic Energy Consumer in Software Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Misu%2C+M+R+H">Md Rakib Hossain Misu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Bhattiprolu%2C+A">Adithya Bhattiprolu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Almeida%2C+E">Eduardo Almeida</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+I">Iftekhar Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Traditionally, energy efficiency research has focused on reducing energy
consumption at the hardware level and, more recently, in the design and coding
phases of the software development life cycle. However, software testing's
impact on energy consumption did not receive attention from the research
community. Specifically, how test code design quality and test smell (e.g.,
sub-optimal design and bad practices in test code) impact energy consumption
has not been investigated yet. This study examined 12 Apache projects to
analyze the association between test smell and its effects on energy
consumption in software testing. We conducted a mixed-method empirical analysis
from two dimensions; software (data mining in Apache projects) and developers'
views (a survey of 62 software practitioners). Our findings show that: 1) test
smell is associated with energy consumption in software testing. Specifically
smelly part of a test case consumes 10.92\% more energy compared to the
non-smelly part. 2) certain test smells are more energy-hungry than others, 3)
refactored test cases tend to consume less energy than their smelly
counterparts, and 4) most developers lack knowledge about test smells' impact
on energy consumption. We conclude the paper with several observations that can
direct future research and developments.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14549" title="Abstract">arXiv:2310.14549</a> [<a href="/pdf/2310.14549" title="Download PDF">pdf</a>, <a href="/format/2310.14549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Graph Learning for Modeling Emerging Pandemics with Big Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+K">Khanh-Tung Tran</a>, 
<a href="/search/cs?searchtype=author&query=Hy%2C+T+S">Truong Son Hy</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Lili Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+X">Xuan-Son Vu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Accurate forecasting and analysis of emerging pandemics play a crucial role
in effective public health management and decision-making. Traditional
approaches primarily rely on epidemiological data, overlooking other valuable
sources of information that could act as sensors or indicators of pandemic
patterns. In this paper, we propose a novel framework called MGL4MEP that
integrates temporal graph neural networks and multi-modal data for learning and
forecasting. We incorporate big data sources, including social media content,
by utilizing specific pre-trained language models and discovering the
underlying graph structure among users. This integration provides rich
indicators of pandemic dynamics through learning with temporal graph neural
networks. Extensive experiments demonstrate the effectiveness of our framework
in pandemic forecasting and analysis, outperforming baseline methods across
different areas, pandemic situations, and prediction horizons. The fusion of
temporal graph learning and multi-modal data enables a comprehensive
understanding of the pandemic landscape with less time lag, cheap cost, and
more potential information indicators.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14550" title="Abstract">arXiv:2310.14550</a> [<a href="/pdf/2310.14550" title="Download PDF">pdf</a>, <a href="/format/2310.14550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-Robust Offline Reinforcement Learning with General Function  Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We investigate the problem of corruption robustness in offline reinforcement
learning (RL) with general function approximation, where an adversary can
corrupt each sample in the offline dataset, and the corruption level
$\zeta\geq0$ quantifies the cumulative corruption amount over $n$ episodes and
$H$ steps. Our goal is to find a policy that is robust to such corruption and
minimizes the suboptimality gap with respect to the optimal policy for the
uncorrupted Markov decision processes (MDPs). Drawing inspiration from the
uncertainty-weighting technique from the robust online RL setting
\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight
iteration procedure to efficiently compute on batched samples and propose a
corruption-robust algorithm for offline RL. Notably, under the assumption of
single policy coverage and the knowledge of $\zeta$, our proposed algorithm
achieves a suboptimality bound that is worsened by an additive factor of
$\mathcal O(\zeta \cdot (\text{CC}(\lambda,\hat{\mathcal F},\mathcal
Z_n^H))^{1/2} (C(\hat{\mathcal F},\mu))^{-1/2} n^{-1})$ due to the corruption.
Here $\text{CC}(\lambda,\hat{\mathcal F},\mathcal Z_n^H)$ is the coverage
coefficient that depends on the regularization parameter $\lambda$, the
confidence set $\hat{\mathcal F}$, and the dataset $\mathcal Z_n^H$, and
$C(\hat{\mathcal F},\mu)$ is a coefficient that depends on $\hat{\mathcal F}$
and the underlying data distribution $\mu$. When specialized to linear MDPs,
the corruption-dependent error term reduces to $\mathcal O(\zeta d n^{-1})$
with $d$ being the dimension of the feature map, which matches the existing
lower bound for corrupted linear MDPs. This suggests that our analysis is tight
in terms of the corruption-dependent term.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14552" title="Abstract">arXiv:2310.14552</a> [<a href="/pdf/2310.14552" title="Download PDF">pdf</a>, <a href="/format/2310.14552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KindMed: Knowledge-Induced Medicine Prescribing Network for Medication  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mulyadi%2C+A+W">Ahmad Wisnu Mulyadi</a>, 
<a href="/search/cs?searchtype=author&query=Suk%2C+H">Heung-Il Suk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Extensive adoption of electronic health records (EHRs) offers opportunities
for its use in various clinical analyses. We could acquire more comprehensive
insights by enriching an EHR cohort with external knowledge (e.g., standardized
medical ontology and wealthy semantics curated on the web) as it divulges a
spectrum of informative relations between observed medical codes. This paper
proposes a novel Knowledge-Induced Medicine Prescribing Network (KindMed)
framework to recommend medicines by inducing knowledge from myriad
medical-related external sources upon the EHR cohort, rendering them as medical
knowledge graphs (KGs). On top of relation-aware graph representation learning
to unravel an adequate embedding of such KGs, we leverage hierarchical sequence
learning to discover and fuse clinical and medicine temporal dynamics across
patients' historical admissions for encouraging personalized recommendations.
In predicting safe, precise, and personalized medicines, we devise an attentive
prescribing that accounts for and associates three essential aspects, i.e., a
summary of joint historical medical records, clinical condition progression,
and the current clinical state of patients. We exhibited the effectiveness of
our KindMed on the augmented real-world EHR cohorts, etching leading
performances against graph-driven competing baselines.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14553" title="Abstract">arXiv:2310.14553</a> [<a href="/pdf/2310.14553" title="Download PDF">pdf</a>, <a href="/format/2310.14553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Opponents Position in Partial Observation Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayareh%2C+A">Aref Sayareh</a>, 
<a href="/search/cs?searchtype=author&query=Sardari%2C+A">Aria Sardari</a>, 
<a href="/search/cs?searchtype=author&query=Khoddami%2C+V">Vahid Khoddami</a>, 
<a href="/search/cs?searchtype=author&query=Zare%2C+N">Nader Zare</a>, 
<a href="/search/cs?searchtype=author&query=da+Fonseca%2C+V+P">Vinicius Prado da Fonseca</a>, 
<a href="/search/cs?searchtype=author&query=Soares%2C+A">Amilcar Soares</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">The RoboCup competitions hold various leagues, and the Soccer Simulation 2D
League is a major among them. Soccer Simulation 2D (SS2D) match involves two
teams, including 11 players and a coach for each team, competing against each
other. The players can only communicate with the Soccer Simulation Server
during the game. Several code bases are released publicly to simplify team
development. So researchers can easily focus on decision-making and
implementing machine learning methods. SS2D actions and behaviors are only
partially accurate due to different challenges, such as noise and partial
observation. Therefore, one strategy is to implement alternative denoising
methods to tackle observation inaccuracy. Our idea is to predict opponent
positions while they have yet to be seen in a finite number of cycles using
machine learning methods to make more accurate actions such as pass. We will
explain our position prediction idea powered by Long Short-Term Memory models
(LSTM) and Deep Neural Networks (DNN). The results show that the LSTM and DNN
predict the opponents' position more accurately than the standard algorithm,
such as the last-seen method.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14554" title="Abstract">arXiv:2310.14554</a> [<a href="/pdf/2310.14554" title="Download PDF">pdf</a>, <a href="/ps/2310.14554" title="Download PostScript">ps</a>, <a href="/format/2310.14554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making RL with Preference-based Feedback Efficient via Randomization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Runzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Reinforcement Learning algorithms that learn from human feedback (RLHF) need
to be efficient in terms of statistical complexity, computational complexity,
and query complexity. In this work, we consider the RLHF setting where the
feedback is given in the format of preferences over pairs of trajectories. In
the linear MDP model, by using randomization in algorithm design, we present an
algorithm that is sample efficient (i.e., has near-optimal worst-case regret
bounds) and has polynomial running time (i.e., computational complexity is
polynomial with respect to relevant parameters). Our algorithm further
minimizes the query complexity through a novel randomized active learning
procedure. In particular, our algorithm demonstrates a near-optimal tradeoff
between the regret bound and the query complexity. To extend the results to
more general nonlinear function approximation, we design a model-based
randomized algorithm inspired by the idea of Thompson sampling. Our algorithm
minimizes Bayesian regret bound and query complexity, again achieving a
near-optimal tradeoff between these two quantities. Computation-wise, similar
to the prior Thompson sampling algorithms under the regular RL setting, the
main computation primitives of our algorithm are Bayesian supervised learning
oracles which have been heavily investigated on the empirical side when
applying Thompson sampling algorithms to RL benchmark problems.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14556" title="Abstract">arXiv:2310.14556</a> [<a href="/pdf/2310.14556" title="Download PDF">pdf</a>, <a href="/format/2310.14556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S3Aug: Segmentation, Sampling, and Shift for Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sugiura%2C+T">Taiki Sugiura</a>, 
<a href="/search/cs?searchtype=author&query=Tamaki%2C+T">Toru Tamaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Action recognition is a well-established area of research in computer vision.
In this paper, we propose S3Aug, a video data augmenatation for action
recognition. Unlike conventional video data augmentation methods that involve
cutting and pasting regions from two videos, the proposed method generates new
videos from a single training video through segmentation and label-to-image
transformation. Furthermore, the proposed method modifies certain categories of
label images by sampling to generate a variety of videos, and shifts
intermediate features to enhance the temporal coherency between frames of the
generate videos. Experimental results on the UCF101, HMDB51, and Mimetics
datasets demonstrate the effectiveness of the proposed method, paricularlly for
out-of-context videos of the Mimetics dataset.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14557" title="Abstract">arXiv:2310.14557</a> [<a href="/pdf/2310.14557" title="Download PDF">pdf</a>, <a href="/format/2310.14557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Doan%2C+K+D">Khai Duy Doan</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q">Qisheng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate
remarkable performance in a wide range of tasks. Despite numerous recent
studies that examine the performance of instruction-tuned LLMs on various NLP
benchmarks, there remains a lack of comprehensive investigation into their
ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning
embedded within social and interactive contexts. This deficiency arises partly
from SM not being adequately represented in any of the existing benchmarks. To
address this gap, we present SPARROW, an extensive multilingual benchmark
specifically designed for SM understanding. SPARROW comprises 169 datasets
covering 13 task types across six primary categories (e.g., anti-social
language detection, emotion recognition). SPARROW datasets encompass 64
different languages originating from 12 language families representing 16
writing scripts. We evaluate the performance of various multilingual pretrained
language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)
on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our
comprehensive analysis reveals that existing open-source instruction tuned LLMs
still struggle to understand SM across various languages, performing close to a
random baseline in some cases. We also find that although ChatGPT outperforms
many LLMs, it still falls behind task-specific finetuned models with a gap of
12.19 SPARROW score. Our benchmark is available at:
https://github.com/UBC-NLP/SPARROW
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14558" title="Abstract">arXiv:2310.14558</a> [<a href="/pdf/2310.14558" title="Download PDF">pdf</a>, <a href="/format/2310.14558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlpaCare:Instruction-tuned Large Language Models for Medical Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinlu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C">Chenxin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Petzold%2C+L+R">Linda Ruth Petzold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated significant enhancements in
instruction-following abilities through instruction tuning, achieving notable
performances across various tasks. Previous research has focused on fine-tuning
medical domain-specific LLMs using an extensive array of medical-specific data,
incorporating millions of pieces of biomedical literature to augment their
medical capabilities. However, existing medical instruction-tuned LLMs have
been constrained by the limited scope of tasks and instructions available,
restricting the efficacy of instruction tuning and adversely affecting
performance in the general domain. In this paper, we fine-tune LLaMA-series
models using 52k diverse, machine-generated, medical instruction-following
data, MedInstruct-52k, resulting in the model AlpaCare. Comprehensive
experimental results on both general and medical-specific domain free-form
instruction evaluations showcase AlpaCare's strong medical proficiency and
generalizability compared to previous instruction-tuned models in both medical
and general domains. We provide public access to our MedInstruct-52k dataset
and a clinician-crafted free-form instruction test set, MedInstruct-test, along
with our codebase, to foster further research and development. Our project page
is available at https://github.com/XZhang97666/AlpaCare.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14560" title="Abstract">arXiv:2310.14560</a> [<a href="/pdf/2310.14560" title="Download PDF">pdf</a>, <a href="/format/2310.14560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polyhedral Surface: Self-supervised Point Cloud Reconstruction Based on  Polyhedral Surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Point cloud reconstruction from raw point cloud has been an important topic
in computer graphics for decades, especially due to its high demand in modeling
and rendering applications. An important way to solve this problem is
establishing a local geometry to fit the local curve. However, previous methods
build either a local plane or polynomial curve. Local plane brings the loss of
sharp feature and the boundary artefacts on open surface. Polynomial curve is
hard to combine with neural network due to the local coordinate consistent
problem. To address this, we propose a novel polyhedral surface to represent
local surface. This method provides more flexible to represent sharp feature
and surface boundary on open surface. It does not require any local coordinate
system, which is important when introducing neural networks. Specifically, we
use normals to construct the polyhedral surface, including both dihedral and
trihedral surfaces using 2 and 3 normals, respectively. Our method achieves
state-of-the-art results on three commonly used datasets (ShapeNetCore, ABC,
and ScanNet). Code will be released upon acceptance.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14561" title="Abstract">arXiv:2310.14561</a> [<a href="/pdf/2310.14561" title="Download PDF">pdf</a>, <a href="/format/2310.14561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> F$^2$AT: Feature-Focusing Adversarial Training via Disentanglement of  Natural and Perturbed Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yaguan Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Z">Zhaoquan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shouling Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Boyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by
well-designed perturbations. This could lead to disastrous results on critical
applications such as self-driving cars, surveillance security, and medical
diagnosis. At present, adversarial training is one of the most effective
defenses against adversarial examples. However, traditional adversarial
training makes it difficult to achieve a good trade-off between clean accuracy
and robustness since spurious features are still learned by DNNs. The intrinsic
reason is that traditional adversarial training makes it difficult to fully
learn core features from adversarial examples when adversarial noise and clean
examples cannot be disentangled. In this paper, we disentangle the adversarial
examples into natural and perturbed patterns by bit-plane slicing. We assume
the higher bit-planes represent natural patterns and the lower bit-planes
represent perturbed patterns, respectively. We propose a Feature-Focusing
Adversarial Training (F$^2$AT), which differs from previous work in that it
enforces the model to focus on the core features from natural patterns and
reduce the impact of spurious features from perturbed patterns. The
experimental results demonstrated that F$^2$AT outperforms state-of-the-art
methods in clean accuracy and adversarial robustness.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14563" title="Abstract">arXiv:2310.14563</a> [<a href="/pdf/2310.14563" title="Download PDF">pdf</a>, <a href="/format/2310.14563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling  Social Norm Adherence and Violation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+O">Oliver Li</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+M">Mallika Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Saakyan%2C+A">Arkadiy Saakyan</a>, 
<a href="/search/cs?searchtype=author&query=CH-Wang%2C+S">Sky CH-Wang</a>, 
<a href="/search/cs?searchtype=author&query=Muresan%2C+S">Smaranda Muresan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2022 Main Conference, Short Paper; Data at <a href="https://github.com/Aochong-Li/NormDial">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Social norms fundamentally shape interpersonal communication. We present
NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations
of social norm adherences and violations for Chinese and American cultures.
Introducing the task of social norm observance detection, our dataset is
synthetically generated in both Chinese and English using a human-in-the-loop
pipeline by prompting large language models with a small collection of
expert-annotated social norms. We show that our generated dialogues are of high
quality through human evaluation and further evaluate the performance of
existing large language models on this task. Our findings point towards new
directions for understanding the nuances of social norms as they manifest in
conversational contexts that span across languages and cultures.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14564" title="Abstract">arXiv:2310.14564</a> [<a href="/pdf/2310.14564" title="Download PDF">pdf</a>, <a href="/format/2310.14564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models Hallucinate, but May Excel at Fact Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+J">Jian Guan</a>, 
<a href="/search/cs?searchtype=author&query=Dodge%2C+J">Jesse Dodge</a>, 
<a href="/search/cs?searchtype=author&query=Wadden%2C+D">David Wadden</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent progress in natural language processing (NLP) owes much to remarkable
advances in large language models (LLMs). Nevertheless, LLMs frequently
"hallucinate," resulting in non-factual outputs. Our carefully designed human
evaluation substantiates the serious hallucination issue, revealing that even
GPT-3.5 produces factual outputs less than 25% of the time. This underscores
the importance of fact verifiers in order to measure and incentivize progress.
Our systematic investigation affirms that LLMs can be repurposed as effective
fact verifiers with strong correlations with human judgments, at least in the
Wikipedia domain. Surprisingly, FLAN-T5-11B, the least factual generator in our
study, performs the best as a fact verifier, even outperforming more capable
LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these
LLMs on high-quality evidence, as well as their deficiencies in robustness and
generalization ability. Our study presents insights for developing trustworthy
generation models.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14565" title="Abstract">arXiv:2310.14565</a> [<a href="/pdf/2310.14565" title="Download PDF">pdf</a>, <a href="/format/2310.14565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEPSI: Practically Efficient Private Set Intersection in the Unbalanced  Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahdavi%2C+R+A">Rasoul Akhavan Mahdavi</a>, 
<a href="/search/cs?searchtype=author&query=Lukas%2C+N">Nils Lukas</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimianghazani%2C+F">Faezeh Ebrahimianghazani</a>, 
<a href="/search/cs?searchtype=author&query=Humphries%2C+T">Thomas Humphries</a>, 
<a href="/search/cs?searchtype=author&query=Kacsmar%2C+B">Bailey Kacsmar</a>, 
<a href="/search/cs?searchtype=author&query=Premkumar%2C+J">John Premkumar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinda Li</a>, 
<a href="/search/cs?searchtype=author&query=Oya%2C+S">Simon Oya</a>, 
<a href="/search/cs?searchtype=author&query=Amjadian%2C+E">Ehsan Amjadian</a>, 
<a href="/search/cs?searchtype=author&query=Kerschbaum%2C+F">Florian Kerschbaum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Two parties with private data sets can find shared elements using a Private
Set Intersection (PSI) protocol without revealing any information beyond the
intersection. Circuit PSI protocols privately compute an arbitrary function of
the intersection - such as its cardinality, and are often employed in an
unbalanced setting where one party has more data than the other. Existing
protocols are either computationally inefficient or require extensive
server-client communication on the order of the larger set. We introduce
Practically Efficient PSI or PEPSI, a non-interactive solution where only the
client sends its encrypted data. PEPSI can process an intersection of 1024
client items with a million server items in under a second, using less than 5
MB of communication. Our work is over 4 orders of magnitude faster than an
existing non-interactive circuit PSI protocol and requires only 10% of the
communication. It is also up to 20 times faster than the work of Ion et al.,
which computes a limited set of functions and has communication costs
proportional to the larger set. Our work is the first to demonstrate that
non-interactive circuit PSI can be practically applied in an unbalanced
setting.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14566" title="Abstract">arXiv:2310.14566</a> [<a href="/pdf/2310.14566" title="Download PDF">pdf</a>, <a href="/format/2310.14566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HallusionBench: You See What You Think? Or You Think What You See? An  Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5,  and Other Multi-modality Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fuxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+T">Tianrui Guan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yacoob%2C+Y">Yaser Yacoob</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs), after being aligned with vision models and
integrated into vision-language models (VLMs), can bring impressive improvement
in image reasoning tasks. This was shown by the recently released GPT-4V(ison),
LLaVA-1.5, etc. However, the strong language prior in these SOTA LVLMs can be a
double-edged sword: they may ignore the image context and solely rely on the
(even contradictory) language prior for reasoning. In contrast, the vision
modules in VLMs are weaker than LLMs and may result in misleading visual
representations, which are then translated to confident mistakes by LLMs. To
study these two types of VLM mistakes, i.e., language hallucination and visual
illusion, we curated HallusionBench, an image-context reasoning benchmark that
is still challenging to even GPT-4V and LLaVA-1.5. We provide a detailed
analysis of examples in HallusionBench, which sheds novel insights on the
illusion or hallucination of VLMs and how to improve them in the future. The
benchmark and codebase will be released at
https://github.com/tianyi-lab/HallusionBench.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14570" title="Abstract">arXiv:2310.14570</a> [<a href="/pdf/2310.14570" title="Download PDF">pdf</a>, <a href="/format/2310.14570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DICE: Diverse Diffusion Model with Scoring for Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Younwoo Choi</a>, 
<a href="/search/cs?searchtype=author&query=Mercurius%2C+R+C">Ray Coden Mercurius</a>, 
<a href="/search/cs?searchtype=author&query=Shabestary%2C+S+M+A">Soheil Mohamad Alizadeh Shabestary</a>, 
<a href="/search/cs?searchtype=author&query=Rasouli%2C+A">Amir Rasouli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Road user trajectory prediction in dynamic environments is a challenging but
crucial task for various applications, such as autonomous driving. One of the
main challenges in this domain is the multimodal nature of future trajectories
stemming from the unknown yet diverse intentions of the agents. Diffusion
models have shown to be very effective in capturing such stochasticity in
prediction tasks. However, these models involve many computationally expensive
denoising steps and sampling operations that make them a less desirable option
for real-time safety-critical applications. To this end, we present a novel
framework that leverages diffusion models for predicting future trajectories in
a computationally efficient manner. To minimize the computational bottlenecks
in iterative sampling, we employ an efficient sampling mechanism that allows us
to maximize the number of sampled trajectories for improved accuracy while
maintaining inference time in real time. Moreover, we propose a scoring
mechanism to select the most plausible trajectories by assigning relative
ranks. We show the effectiveness of our approach by conducting empirical
evaluations on common pedestrian (UCY/ETH) and autonomous driving (nuScenes)
benchmark datasets on which our model achieves state-of-the-art performance on
several subsets and metrics.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14572" title="Abstract">arXiv:2310.14572</a> [<a href="/pdf/2310.14572" title="Download PDF">pdf</a>, <a href="/format/2310.14572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Multi-Annotation Process: Examining the Influence of  Annotation Quantity and Instance Difficulty on Model Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadasi%2C+P">Pritam Kadasi</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mayank Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The NLP community has long advocated for the construction of multi-annotator
datasets to better capture the nuances of language interpretation,
subjectivity, and ambiguity. This paper conducts a retrospective study to show
how performance scores can vary when a dataset expands from a single annotation
per instance to multiple annotations. We propose a novel multi-annotator
simulation process to generate datasets with varying annotation budgets. We
show that similar datasets with the same annotation budget can lead to varying
performance gains. Our findings challenge the popular belief that models
trained on multi-annotation examples always lead to better performance than
models trained on single or few-annotation examples.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14573" title="Abstract">arXiv:2310.14573</a> [<a href="/pdf/2310.14573" title="Download PDF">pdf</a>, <a href="/format/2310.14573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Boundaries of GPT-4 in Radiology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qianchu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hyland%2C+S">Stephanie Hyland</a>, 
<a href="/search/cs?searchtype=author&query=Bannur%2C+S">Shruthi Bannur</a>, 
<a href="/search/cs?searchtype=author&query=Bouzid%2C+K">Kenza Bouzid</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+D+C">Daniel C. Castro</a>, 
<a href="/search/cs?searchtype=author&query=Wetscherek%2C+M+T">Maria Teodora Wetscherek</a>, 
<a href="/search/cs?searchtype=author&query=Tinn%2C+R">Robert Tinn</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+H">Harshita Sharma</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Garc%C3%ADa%2C+F">Fernando P&#xe9;rez-Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Schwaighofer%2C+A">Anton Schwaighofer</a>, 
<a href="/search/cs?searchtype=author&query=Rajpurkar%2C+P">Pranav Rajpurkar</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S+T">Sameer Tajdin Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+H">Hoifung Poon</a>, 
<a href="/search/cs?searchtype=author&query=Usuyama%2C+N">Naoto Usuyama</a>, 
<a href="/search/cs?searchtype=author&query=Thieme%2C+A">Anja Thieme</a>, 
<a href="/search/cs?searchtype=author&query=Nori%2C+A+V">Aditya V. Nori</a>, 
<a href="/search/cs?searchtype=author&query=Lungren%2C+M+P">Matthew P. Lungren</a>, 
<a href="/search/cs?searchtype=author&query=Oktay%2C+O">Ozan Oktay</a>, 
<a href="/search/cs?searchtype=author&query=Alvarez-Valle%2C+J">Javier Alvarez-Valle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The recent success of general-domain large language models (LLMs) has
significantly changed the natural language processing paradigm towards a
unified foundation model across domains and applications. In this paper, we
focus on assessing the performance of GPT-4, the most capable LLM so far, on
the text-based applications for radiology reports, comparing against
state-of-the-art (SOTA) radiology-specific models. Exploring various prompting
strategies, we evaluated GPT-4 on a diverse range of common radiology tasks and
we found GPT-4 either outperforms or is on par with current SOTA radiology
models. With zero-shot prompting, GPT-4 already obtains substantial gains
($\approx$ 10% absolute improvement) over radiology models in temporal sentence
similarity classification (accuracy) and natural language inference ($F_1$).
For tasks that require learning dataset-specific style or schema (e.g. findings
summarisation), GPT-4 improves with example-based prompting and matches
supervised SOTA. Our extensive error analysis with a board-certified
radiologist shows GPT-4 has a sufficient level of radiology knowledge with only
occasional errors in complex context that require nuanced domain knowledge. For
findings summarisation, GPT-4 outputs are found to be overall comparable with
existing manually-written impressions.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14575" title="Abstract">arXiv:2310.14575</a> [<a href="/pdf/2310.14575" title="Download PDF">pdf</a>, <a href="/ps/2310.14575" title="Download PostScript">ps</a>, <a href="/format/2310.14575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listing 6-Cycles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Ce Jin</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+V+V">Virginia Vassilevska Williams</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Renfei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages; SOSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Listing copies of small subgraphs (such as triangles, $4$-cycles, small
cliques) in the input graph is an important and well-studied problem in
algorithmic graph theory. In this paper, we give a simple algorithm that lists
$t$ (non-induced) $6$-cycles in an $n$-node undirected graph in $\tilde
O(n^2+t)$ time. This nearly matches the fastest known algorithm for detecting a
$6$-cycle in $O(n^2)$ time by Yuster and Zwick (1997). Previously, a folklore
$O(n^2+t)$-time algorithm was known for the task of listing $4$-cycles.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14576" title="Abstract">arXiv:2310.14576</a> [<a href="/pdf/2310.14576" title="Download PDF">pdf</a>, <a href="/format/2310.14576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Decomposition Based Attention Module for Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haoyu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Ruijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xuerui Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yule Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Malu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+L">Liangjian Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The attention mechanism has been proven to be an effective way to improve
spiking neural network (SNN). However, based on the fact that the current SNN
input data flow is split into tensors to process on GPUs, none of the previous
works consider the properties of tensors to implement an attention module. This
inspires us to rethink current SNN from the perspective of tensor-relevant
theories. Using tensor decomposition, we design the \textit{projected full
attention} (PFA) module, which demonstrates excellent results with linearly
growing parameters. Specifically, PFA is composed by the \textit{linear
projection of spike tensor} (LPST) module and \textit{attention map composing}
(AMC) module. In LPST, we start by compressing the original spike tensor into
three projected tensors using a single property-preserving strategy with
learnable parameters for each dimension. Then, in AMC, we exploit the inverse
procedure of the tensor decomposition process to combine the three tensors into
the attention map using a so-called connecting factor. To validate the
effectiveness of the proposed PFA module, we integrate it into the widely used
VGG and ResNet architectures for classification tasks. Our method achieves
state-of-the-art performance on both static and dynamic benchmark datasets,
surpassing the existing SNN models with Transformer-based and CNN-based
backbones.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14577" title="Abstract">arXiv:2310.14577</a> [<a href="/pdf/2310.14577" title="Download PDF">pdf</a>, <a href="/format/2310.14577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeCrisisMB: Debiased Semi-Supervised Learning for Crisis Tweet  Classification via Memory Bank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+H+P">Henry Peng Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Caragea%2C+C">Cornelia Caragea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">During crisis events, people often use social media platforms such as Twitter
to disseminate information about the situation, warnings, advice, and support.
Emergency relief organizations leverage such information to acquire timely
crisis circumstances and expedite rescue operations. While existing works
utilize such information to build models for crisis event analysis,
fully-supervised approaches require annotating vast amounts of data and are
impractical due to limited response time. On the other hand, semi-supervised
models can be biased, performing moderately well for certain classes while
performing extremely poorly for others, resulting in substantially negative
effects on disaster monitoring and rescue. In this paper, we first study two
recent debiasing methods on semi-supervised crisis tweet classification. Then
we propose a simple but effective debiasing method, DeCrisisMB, that utilizes a
Memory Bank to store and perform equal sampling for generated pseudo-labels
from each class at each training iteration. Extensive experiments are conducted
to compare different debiasing methods' performance and generalization ability
in both in-distribution and out-of-distribution settings. The results
demonstrate the superior performance of our proposed method. Our code is
available at https://github.com/HenryPengZou/DeCrisisMB.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14579" title="Abstract">arXiv:2310.14579</a> [<a href="/pdf/2310.14579" title="Download PDF">pdf</a>, <a href="/format/2310.14579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedSplitX: Federated Split Learning for Computationally-Constrained  Heterogeneous Clients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jiyun Shin</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J">Jinhyun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Honggu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Joonhyuk Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Foundation models (FMs) have demonstrated remarkable performance in machine
learning but demand extensive training data and computational resources.
Federated learning (FL) addresses the challenges posed by FMs, especially
related to data privacy and computational burdens. However, FL on FMs faces
challenges in situations with heterogeneous clients possessing varying
computing capabilities, as clients with limited capabilities may struggle to
train the computationally intensive FMs. To address these challenges, we
propose FedSplitX, a novel FL framework that tackles system heterogeneity.
FedSplitX splits a large model into client-side and server-side components at
multiple partition points to accommodate diverse client capabilities. This
approach enables clients to collaborate while leveraging the server's
computational power, leading to improved model performance compared to
baselines that limit model size to meet the requirement of the poorest client.
Furthermore, FedSplitX incorporates auxiliary networks at each partition point
to reduce communication costs and delays while enhancing model performance. Our
experiments demonstrate that FedSplitX effectively utilizes server capabilities
to train large models, outperforming baseline approaches.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14580" title="Abstract">arXiv:2310.14580</a> [<a href="/pdf/2310.14580" title="Download PDF">pdf</a>, <a href="/format/2310.14580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acoustic BPE for Speech Generation with Discrete Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+F">Feiyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chenpeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures; submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Discrete audio tokens derived from self-supervised learning models have
gained widespread usage in speech generation. However, current practice of
directly utilizing audio tokens poses challenges for sequence modeling due to
the length of the token sequence. Additionally, this approach places the burden
on the model to establish correlations between tokens, further complicating the
modeling process. To address this issue, we propose acoustic BPE which encodes
frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE
effectively reduces the sequence length and leverages the prior morphological
information present in token sequence, which alleviates the modeling challenges
of token correlation. Through comprehensive investigations on a speech language
model trained with acoustic BPE, we confirm the notable advantages it offers,
including faster inference and improved syntax capturing capabilities. In
addition, we propose a novel rescore method to select the optimal synthetic
speech among multiple candidates generated by rich-diversity TTS system.
Experiments prove that rescore selection aligns closely with human preference,
which highlights acoustic BPE's potential to other speech generation tasks.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14581" title="Abstract">arXiv:2310.14581</a> [<a href="/pdf/2310.14581" title="Download PDF">pdf</a>, <a href="/format/2310.14581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Image-Text Similarity and Caption Modification for the  DataComp Challenge: Filtering Track and BYOD Track
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yokoo%2C+S">Shuhei Yokoo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+P">Peifei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+Y">Yuchi Ishikawa</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+M">Mikihiro Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Kondo%2C+M">Masayoshi Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Kataoka%2C+H">Hirokatsu Kataoka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the ICCV 2023 Workshop on Towards the Next Generation of Computer Vision Datasets: DataComp Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large web crawl datasets have already played an important role in learning
multimodal features with high generalization capabilities. However, there are
still very limited studies investigating the details or improvements of data
design. Recently, a DataComp challenge has been designed to propose the best
training data with the fixed models. This paper presents our solution to both
filtering track and BYOD track of the DataComp challenge. Our solution adopts
large multimodal models CLIP and BLIP-2 to filter and modify web crawl data,
and utilize external datasets along with a bag of tricks to improve the data
quality. Experiments show our solution significantly outperforms DataComp
baselines (filtering track: 6.6% improvement, BYOD track: 48.5% improvement).
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14583" title="Abstract">arXiv:2310.14583</a> [<a href="/pdf/2310.14583" title="Download PDF">pdf</a>, <a href="/format/2310.14583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JointMatch: A Unified Approach for Diverse and Collaborative  Pseudo-Labeling to Semi-Supervised Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+H+P">Henry Peng Zou</a>, 
<a href="/search/cs?searchtype=author&query=Caragea%2C+C">Cornelia Caragea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Semi-supervised text classification (SSTC) has gained increasing attention
due to its ability to leverage unlabeled data. However, existing approaches
based on pseudo-labeling suffer from the issues of pseudo-label bias and error
accumulation. In this paper, we propose JointMatch, a holistic approach for
SSTC that addresses these challenges by unifying ideas from recent
semi-supervised learning and the task of learning with noise. JointMatch
adaptively adjusts classwise thresholds based on the learning status of
different classes to mitigate model bias towards current easy classes.
Additionally, JointMatch alleviates error accumulation by utilizing two
differently initialized networks to teach each other in a cross-labeling
manner. To maintain divergence between the two networks for mutual learning, we
introduce a strategy that weighs more disagreement data while also allowing the
utilization of high-quality agreement data for training. Experimental results
on benchmark datasets demonstrate the superior performance of JointMatch,
achieving a significant 5.13% improvement on average. Notably, JointMatch
delivers impressive results even in the extremely-scarce-label setting,
obtaining 86% accuracy on AG News with only 5 labels per class. We make our
code available at https://github.com/HenryPengZou/JointMatch.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14586" title="Abstract">arXiv:2310.14586</a> [<a href="/pdf/2310.14586" title="Download PDF">pdf</a>, <a href="/format/2310.14586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Molaei%2C+S">Soheila Molaei</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Evaluating the performance of graph neural networks (GNNs) is an essential
task for practical GNN model deployment and serving, as deployed GNNs face
significant performance uncertainty when inferring on unseen and unlabeled test
graphs, due to mismatched training-test graph distributions. In this paper, we
study a new problem, GNN model evaluation, that aims to assess the performance
of a specific GNN model trained on labeled and observed graphs, by precisely
estimating its performance (e.g., node classification accuracy) on unseen
graphs without labels. Concretely, we propose a two-stage GNN model evaluation
framework, including (1) DiscGraph set construction and (2) GNNEvaluator
training and inference. The DiscGraph set captures wide-range and diverse graph
data distribution discrepancies through a discrepancy measurement function,
which exploits the outputs of GNNs related to latent node embeddings and node
class predictions. Under the effective training supervision from the DiscGraph
set, GNNEvaluator learns to precisely estimate node classification accuracy of
the to-be-evaluated GNN model and makes an accurate inference for evaluating
GNN model performance. Extensive experiments on real-world unseen and unlabeled
test graphs demonstrate the effectiveness of our proposed method for GNN model
evaluation.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14587" title="Abstract">arXiv:2310.14587</a> [<a href="/pdf/2310.14587" title="Download PDF">pdf</a>, <a href="/format/2310.14587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Search Model: Redefining Search Stack in the Era of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+R">Rangan Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Modern search engines are built on a stack of different components, including
query understanding, retrieval, multi-stage ranking, and question answering,
among others. These components are often optimized and deployed independently.
In this paper, we introduce a novel conceptual framework called large search
model, which redefines the conventional search stack by unifying search tasks
with one large language model (LLM). All tasks are formulated as autoregressive
text generation problems, allowing for the customization of tasks through the
use of natural language prompts. This proposed framework capitalizes on the
strong language understanding and reasoning capabilities of LLMs, offering the
potential to enhance search result quality while simultaneously simplifying the
existing cumbersome search stack. To substantiate the feasibility of this
framework, we present a series of proof-of-concept experiments and discuss the
potential challenges associated with implementing this approach within
real-world search systems.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14592" title="Abstract">arXiv:2310.14592</a> [<a href="/pdf/2310.14592" title="Download PDF">pdf</a>, <a href="/format/2310.14592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Training LiDAR-Based 3D Object Detectors Through Colorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+T">Tai-Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianle Chen</a>, 
<a href="/search/cs?searchtype=author&query=Phoo%2C+C+P">Cheng Perng Phoo</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K+Z">Katie Z Luo</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yurong You</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+M">Mark Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Hariharan%2C+B">Bharath Hariharan</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-Lun Chao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate 3D object detection and understanding for self-driving cars heavily
relies on LiDAR point clouds, necessitating large amounts of labeled data to
train. In this work, we introduce an innovative pre-training approach, Grounded
Point Colorization (GPC), to bridge the gap between data and labels by teaching
the model to colorize LiDAR point clouds, equipping it with valuable semantic
cues. To tackle challenges arising from color variations and selection bias, we
incorporate color as "context" by providing ground-truth colors as hints during
colorization. Experimental results on the KITTI and Waymo datasets demonstrate
GPC's remarkable effectiveness. Even with limited labeled data, GPC
significantly improves fine-tuning performance; notably, on just 20% of the
KITTI dataset, GPC outperforms training from scratch with the entire dataset.
In sum, we introduce a fresh perspective on pre-training for 3D object
detection, aligning the objective with the model's intended role and ultimately
advancing the accuracy and efficiency of 3D object detection for autonomous
vehicles.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14595" title="Abstract">arXiv:2310.14595</a> [<a href="/pdf/2310.14595" title="Download PDF">pdf</a>, <a href="/ps/2310.14595" title="Download PostScript">ps</a>, <a href="/format/2310.14595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Auditing of Information Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oren-Loberman%2C+M">Mor Oren-Loberman</a>, 
<a href="/search/cs?searchtype=author&query=Azar%2C+V">Vered Azar</a>, 
<a href="/search/cs?searchtype=author&query=Huleihel%2C+W">Wasim Huleihel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Modern social media platforms play an important role in facilitating rapid
dissemination of information through their massive user networks. Fake news,
misinformation, and unverifiable facts on social media platforms propagate
disharmony and affect society. In this paper, we consider the problem of online
auditing of information flow/propagation with the goal of classifying news
items as fake or genuine. Specifically, driven by experiential studies on
real-world social media platforms, we propose a probabilistic Markovian
information spread model over networks modeled by graphs. We then formulate our
inference task as a certain sequential detection problem with the goal of
minimizing the combination of the error probability and the time it takes to
achieve correct decision. For this model, we find the optimal detection
algorithm minimizing the aforementioned risk and prove several statistical
guarantees. We then test our algorithm over real-world datasets. To that end,
we first construct an offline algorithm for learning the probabilistic
information spreading model, and then apply our optimal detection algorithm.
Experimental study show that our algorithm outperforms state-of-the-art
misinformation detection algorithms in terms of accuracy and detection time.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14596" title="Abstract">arXiv:2310.14596</a> [<a href="/pdf/2310.14596" title="Download PDF">pdf</a>, <a href="/format/2310.14596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Correct Noisy Labels for Fine-Grained Entity Typing via  Co-Prediction Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Minghao Tang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yongquan He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yongxiu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongbo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP 2023, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fine-grained entity typing (FET) is an essential task in natural language
processing that aims to assign semantic types to entities in text. However, FET
poses a major challenge known as the noise labeling problem, whereby current
methods rely on estimating noise distribution to identify noisy labels but are
confused by diverse noise distribution deviation. To address this limitation,
we introduce Co-Prediction Prompt Tuning for noise correction in FET, which
leverages multiple prediction results to identify and correct noisy labels.
Specifically, we integrate prediction results to recall labeled labels and
utilize a differentiated margin to identify inaccurate labels. Moreover, we
design an optimization objective concerning divergent co-predictions during
fine-tuning, ensuring that the model captures sufficient information and
maintains robustness in noise identification. Experimental results on three
widely-used FET datasets demonstrate that our noise correction approach
significantly enhances the quality of various types of training samples,
including those annotated using distant supervision, ChatGPT, and
crowdsourcing.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14599" title="Abstract">arXiv:2310.14599</a> [<a href="/pdf/2310.14599" title="Download PDF">pdf</a>, <a href="/format/2310.14599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prefix-Tuning Based Unsupervised Text Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mai%2C+H">Huiyu Mai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhihong Deng</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unsupervised text style transfer aims at training a generative model that can
alter the style of the input sentence while preserving its content without
using any parallel data. In this paper, we employ powerful pre-trained large
language models and present a new prefix-tuning-based method for unsupervised
text style transfer. We construct three different kinds of prefixes, i.e.,
\textit{shared prefix, style prefix}, and \textit{content prefix}, to encode
task-specific information, target style, and the content information of the
input sentence, respectively. Compared to embeddings used by previous works,
the proposed prefixes can provide richer information for the model.
Furthermore, we adopt a recursive way of using language models in the process
of style transfer. This strategy provides a more effective way for the
interactions between the input sentence and GPT-2, helps the model construct
more informative prefixes, and thus, helps improve the performance. Evaluations
on the well-known datasets show that our method outperforms the
state-of-the-art baselines. Results, analysis of ablation studies, and
subjective evaluations from humans are also provided for a deeper understanding
of the proposed method.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14600" title="Abstract">arXiv:2310.14600</a> [<a href="/pdf/2310.14600" title="Download PDF">pdf</a>, <a href="/ps/2310.14600" title="Download PostScript">ps</a>, <a href="/format/2310.14600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NFT formalised
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamkuemah%2C+M+N">Martha N. Kamkuemah</a>, 
<a href="/search/cs?searchtype=author&query=Sanders%2C+J+W">J. W. Sanders</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, formulae
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">Non-fungible tokens, NFT, have been used to record ownership of real estate,
art, digital assets, and more recently to serve legal notice. They provide an
important and accessible non-financial use of cryptocurrency's blockchain but
are peculiar because ownership by NFT confers no rights over the asset. This
work shows that it is possible to specify that peculiar property by combining
functional and epistemic properties. Suitability of the specification is
evaluated by proof that the blockchain implementation conforms to it, and by
its use in an analysis of serving legal notice.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14602" title="Abstract">arXiv:2310.14602</a> [<a href="/pdf/2310.14602" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Pre-trained Transformer for Vietnamese Community-based  COVID-19 Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vo%2C+T+M">Tam Minh Vo</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+K+V">Khiem Vinh Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent studies have provided empirical evidence of the wide-ranging potential
of Generative Pre-trained Transformer (GPT), a pretrained language model, in
the field of natural language processing. GPT has been effectively employed as
a decoder within state-of-the-art (SOTA) question answering systems, yielding
exceptional performance across various tasks. However, the current research
landscape concerning GPT's application in Vietnamese remains limited. This
paper aims to address this gap by presenting an implementation of GPT-2 for
community-based question answering specifically focused on COVID-19 related
queries in Vietnamese. We introduce a novel approach by conducting a
comparative analysis of different Transformers vs SOTA models in the
community-based COVID-19 question answering dataset. The experimental findings
demonstrate that the GPT-2 models exhibit highly promising outcomes,
outperforming other SOTA models as well as previous community-based COVID-19
question answering models developed for Vietnamese.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14605" title="Abstract">arXiv:2310.14605</a> [<a href="/pdf/2310.14605" title="Download PDF">pdf</a>, <a href="/format/2310.14605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal  Aspect-based Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Y">Yawen Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianbing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinyu Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained
Sentiment Analysis task, which has attracted growing research interests
recently. Existing work mainly utilizes image information to improve the
performance of MABSA task. However, most of the studies overestimate the
importance of images since there are many noise images unrelated to the text in
the dataset, which will have a negative impact on model learning. Although some
work attempts to filter low-quality noise images by setting thresholds, relying
on thresholds will inevitably filter out a lot of useful image information.
Therefore, in this work, we focus on whether the negative impact of noisy
images can be reduced without modifying the data. To achieve this goal, we
borrow the idea of Curriculum Learning and propose a Multi-grained
Multi-curriculum Denoising Framework (M2DF), which can achieve denoising by
adjusting the order of training data. Extensive experimental results show that
our framework consistently outperforms state-of-the-art work on three sub-tasks
of MABSA.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14607" title="Abstract">arXiv:2310.14607</a> [<a href="/pdf/2310.14607" title="Download PDF">pdf</a>, <a href="/ps/2310.14607" title="Download PostScript">ps</a>, <a href="/format/2310.14607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Fairness of Large Language Models for Predictions on  Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+S">Srishti Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent literature has suggested the potential of using large language models
(LLMs) to make predictions for tabular tasks. However, LLMs have been shown to
exhibit harmful social biases that reflect the stereotypes and inequalities
present in the society. To this end, as well as the widespread use of tabular
data in many high-stake applications, it is imperative to explore the following
questions: what sources of information do LLMs draw upon when making
predictions for tabular tasks; whether and to what extent are LLM predictions
for tabular tasks influenced by social biases and stereotypes; and what are the
consequential implications for fairness? Through a series of experiments, we
delve into these questions and show that LLMs tend to inherit social biases
from their training data which significantly impact their fairness in tabular
prediction tasks. Furthermore, our investigations show that in the context of
bias mitigation, though in-context learning and fine-tuning have a moderate
effect, the fairness metric gap between different subgroups is still larger
than that in traditional machine learning models, such as Random Forest and
shallow Neural Networks. This observation emphasizes that the social biases are
inherent within the LLMs themselves and inherited from their pre-training
corpus, not only from the downstream task datasets. Besides, we demonstrate
that label-flipping of in-context examples can significantly reduce biases,
further highlighting the presence of inherent bias within LLMs.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14609" title="Abstract">arXiv:2310.14609</a> [<a href="/pdf/2310.14609" title="Download PDF">pdf</a>, <a href="/format/2310.14609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long Short-Term Planning for Conversational Recommendation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xian Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hongguang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yeqin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xubin Li</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C">Cam-Tu Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures. Accepted by ICONIP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In Conversational Recommendation Systems (CRS), the central question is how
the conversational agent can naturally ask for user preferences and provide
suitable recommendations. Existing works mainly follow the hierarchical
architecture, where a higher policy decides whether to invoke the conversation
module (to ask questions) or the recommendation module (to make
recommendations). This architecture prevents these two components from fully
interacting with each other. In contrast, this paper proposes a novel
architecture, the long short-term feedback architecture, to connect these two
essential components in CRS. Specifically, the recommendation predicts the
long-term recommendation target based on the conversational context and the
user history. Driven by the targeted recommendation, the conversational model
predicts the next topic or attribute to verify if the user preference matches
the target. The balance feedback loop continues until the short-term planner
output matches the long-term planner output, that is when the system should
make the recommendation.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14610" title="Abstract">arXiv:2310.14610</a> [<a href="/pdf/2310.14610" title="Download PDF">pdf</a>, <a href="/format/2310.14610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> That was the last straw, we need more: Are Translation Systems Sensitive  to Disambiguating Context?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaechan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Alisa Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ahia%2C+O">Orevaoghene Ahia</a>, 
<a href="/search/cs?searchtype=author&query=Gonen%2C+H">Hila Gonen</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The translation of ambiguous text presents a challenge for translation
systems, as it requires using the surrounding context to disambiguate the
intended meaning as much as possible. While prior work has studied ambiguities
that result from different grammatical features of the source and target
language, we study semantic ambiguities that exist in the source (English in
this work) itself. In particular, we focus on idioms that are open to both
literal and figurative interpretations (e.g., goose egg), and collect TIDE, a
dataset of 512 pairs of English sentences containing idioms with disambiguating
context such that one is literal (it laid a goose egg) and another is
figurative (they scored a goose egg, as in a score of zero). In experiments, we
compare MT-specific models and language models for (i) their preference when
given an ambiguous subsentence, (ii) their sensitivity to disambiguating
context, and (iii) the performance disparity between figurative and literal
source sentences. We find that current MT models consistently translate English
idioms literally, even when the context suggests a figurative interpretation.
On the other hand, LMs are far more context-aware, although there remain
disparities across target languages. Our findings underline the potential of
LMs as a strong backbone for context-aware translation.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14611" title="Abstract">arXiv:2310.14611</a> [<a href="/pdf/2310.14611" title="Download PDF">pdf</a>, <a href="/format/2310.14611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Monitoring against Pattern Regular Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ang%2C+Z">Zhendong Ang</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+U">Umang Mathur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">In this paper, we focus on the problem of dynamically analysing concurrent
software against high-level temporal specifications. Existing techniques for
runtime monitoring against such specifications are primarily designed for
sequential software and remain inadequate in the presence of concurrency --
violations may be observed only in intricate thread interleavings, requiring
many re-runs of the underlying software. Towards this, we study the problem of
predictive runtime monitoring, inspired by the analogous problem of predictive
data race detection studied extensively recently. The predictive runtime
monitoring question asks, given an execution $\sigma$, if it can be soundly
reordered to expose violations of a specification.
<br />In this paper, we focus on specifications that are given in regular
languages. Our notion of reorderings is trace equivalence, where an execution
is considered a reordering of another if it can be obtained from the latter by
successively commuting adjacent independent actions. We first show that the
problem of predictive admits a super-linear lower bound of $O(n^\alpha)$, where
$n$ is the number of events in the execution, and $\alpha$ is a parameter
describing the degree of commutativity. As a result, predictive runtime
monitoring even in this setting is unlikely to be efficiently solvable.
<br />Towards this, we identify a sub-class of regular languages, called pattern
languages (and their extension generalized pattern languages). Pattern
languages can naturally express specific ordering of some number of (labelled)
events, and have been inspired by popular empirical hypotheses, the `small bug
depth' hypothesis. More importantly, we show that for pattern (and generalized
pattern) languages, the predictive monitoring problem can be solved using a
constant-space streaming linear-time algorithm.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14613" title="Abstract">arXiv:2310.14613</a> [<a href="/pdf/2310.14613" title="Download PDF">pdf</a>, <a href="/format/2310.14613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Modulation Current for Gain-Switching Lasers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Borisevich%2C+A">Alex Borisevich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC); Optics (physics.optics)

</div>
<p class="mathjax">This paper formally shows that an exponentially rising current is optimal in
terms of resistive ohmic loss for driving a semiconductor laser into the
gain-switching mode. A metric to quantify the quality of laser operation that
measures the similarity of a generated optical pulse to the delta function is
proposed. Several circuit implementations to approximate exponentially rising
current are developed, including using a driver circuit with BJT output stage,
a network of RLC circuits, and a saturating inductor. An experimental
comparison between a state-of-the-art sinewave resonant driver circuit and a
directly driven laser is performed that favors the latest variant of the
driver.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14614" title="Abstract">arXiv:2310.14614</a> [<a href="/pdf/2310.14614" title="Download PDF">pdf</a>, <a href="/format/2310.14614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yige Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhiwei Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqi Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Emotion Recognition in Conversation (ERC) has been widely studied due to its
importance in developing emotion-aware empathetic machines. The rise of
pre-trained language models (PLMs) has further pushed the limit of ERC
performance. However, most recent works on ERC using PLMs are heavily
data-driven, and requires fine-tuning the entire PLMs. To improve both sample
and computational efficiency, we propose a derivative-free optimization method
called Cross-Task Prompt Tuning (CTPT) for few-shot conversational emotion
recognition. Unlike existing methods that learn independent knowledge from
individual tasks, CTPT leverages sharable cross-task knowledge by exploiting
external knowledge from other source tasks to improve learning performance
under the few-shot setting. Moreover, CTPT only needs to optimize a vector
under the low intrinsic dimensionality without gradient, which is highly
parameter-efficient compared with existing approaches. Experiments on five
different contextual conversation datasets demonstrate that our CTPT method has
superior results on both few-shot scenarios and zero-shot transfers.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14616" title="Abstract">arXiv:2310.14616</a> [<a href="/pdf/2310.14616" title="Download PDF">pdf</a>, <a href="/format/2310.14616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking SIGN Training: Provable Nonconvex Acceleration without First-  and Second-Order Gradient Lipschitz
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Congliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+P">Peng Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sign-based stochastic methods have gained attention due to their ability to
achieve robust performance despite using only the sign information for
parameter updates. However, the current convergence analysis of sign-based
methods relies on the strong assumptions of first-order gradient Lipschitz and
second-order gradient Lipschitz, which may not hold in practical tasks like
deep neural network training that involve high non-smoothness. In this paper,
we revisit sign-based methods and analyze their convergence under more
realistic assumptions of first- and second-order smoothness. We first establish
the convergence of the sign-based method under weak first-order Lipschitz.
Motivated by the weak first-order Lipschitz, we propose a relaxed second-order
condition that still allows for nonconvex acceleration in sign-based methods.
Based on our theoretical results, we gain insights into the computational
advantages of the recently developed LION algorithm. In distributed settings,
we prove that this nonconvex acceleration persists with linear speedup in the
number of nodes, when utilizing fast communication compression gossip
protocols. The novelty of our theoretical results lies in that they are derived
under much weaker assumptions, thereby expanding the provable applicability of
sign-based algorithms to a wider range of problems.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14622" title="Abstract">arXiv:2310.14622</a> [<a href="/pdf/2310.14622" title="Download PDF">pdf</a>, <a href="/format/2310.14622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High order asymptotic preserving and well-balanced schemes for the  shallow water equations with source terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+G">Guanlan Huang</a>, 
<a href="/search/math?searchtype=author&query=Boscarino%2C+S">Sebastiano Boscarino</a>, 
<a href="/search/math?searchtype=author&query=Xiong%2C+T">Tao Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this study, we investigate the Shallow Water Equations incorporating
source terms accounting for Manning friction and a non-flat bottom topology.
Our primary focus is on developing and validating numerical schemes that serve
a dual purpose: firstly, preserving all steady states within the model, and
secondly, maintaining the late-time asymptotic behavior of solutions, which is
governed by a diffusion equation and coincides with a long time and stiff
friction limit. Our proposed approach draws inspiration from a penalization
technique adopted in {\it{[Boscarino et. al, SIAM Journal on Scientific
Computing, 2014]}}. By employing an additive implicit-explicit Runge-Kutta
method, the scheme can ensure a correct asymptotic behavior for the limiting
diffusion equation, without suffering from a parabolic-type time step
restriction which often afflicts multiscale problems in the diffusive limit.
Numerical experiments are performed to illustrate high order accuracy,
asymptotic preserving, and asymptotically accurate properties of the designed
schemes.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14623" title="Abstract">arXiv:2310.14623</a> [<a href="/pdf/2310.14623" title="Download PDF">pdf</a>, <a href="/format/2310.14623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine  Chain-of-Thought Prompting for Multi-domain NLU Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H+H">Hoang H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While Chain-of-Thought prompting is popular in reasoning tasks, its
application to Large Language Models (LLMs) in Natural Language Understanding
(NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose
Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks
into multiple reasoning steps where LLMs can learn to acquire and leverage
essential concepts to solve tasks from different granularities. Moreover, we
propose leveraging semantic-based Abstract Meaning Representation (AMR)
structured knowledge as an intermediate step to capture the nuances and diverse
structures of utterances, and to understand connections between their varying
levels of granularity. Our proposed approach is demonstrated effective in
assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot
and few-shot multi-domain settings.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14624" title="Abstract">arXiv:2310.14624</a> [<a href="/pdf/2310.14624" title="Download PDF">pdf</a>, <a href="/format/2310.14624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint secure communication and sensing in 6G networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitev%2C+M">Miroslav Mitev</a>, 
<a href="/search/cs?searchtype=author&query=Mayya%2C+A">Amitha Mayya</a>, 
<a href="/search/cs?searchtype=author&query=Chorti%2C+A">Arsenia Chorti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Joint communication and sensing is expected to be one of the features
introduced by the sixth-generation (6G) wireless systems. This will enable a
huge variety of new applications, hence, it is important to find suitable
approaches to secure the exchanged information. Conventional security
mechanisms may not be able to meet the stringent delay, power, and complexity
requirements which opens the challenge of finding new lightweight security
solutions. A promising approach coming from the physical layer is the secret
key generation (SKG) from channel fading. While SKG has been investigated for
several decades, practical implementations of its full protocol are still
scarce. The aim of this chapter is to evaluate the SKG rates in real-life
setups under a set of different scenarios. We consider a typical radar waveform
and present a full implementation of the SKG protocol. Each step is evaluated
to demonstrate that generating keys from the physical layer can be a viable
solution for future networks. However, we show that there is not a single
solution that can be generalized for all cases, instead, parameters should be
chosen according to the context.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14626" title="Abstract">arXiv:2310.14626</a> [<a href="/pdf/2310.14626" title="Download PDF">pdf</a>, <a href="/format/2310.14626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Recommender System and Large Language Model Are Made for  Each Other in E-commerce Pre-sales Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanxing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei-Nan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">Haopeng Bai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hengbin Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">E-commerce pre-sales dialogue aims to understand and elicit user needs and
preferences for the items they are seeking so as to provide appropriate
recommendations. Conversational recommender systems (CRSs) learn user
representation and provide accurate recommendations based on dialogue context,
but rely on external knowledge. Large language models (LLMs) generate responses
that mimic pre-sales dialogues after fine-tuning, but lack domain-specific
knowledge for accurate recommendations. Intuitively, the strengths of LLM and
CRS in E-commerce pre-sales dialogues are complementary, yet no previous work
has explored this. This paper investigates the effectiveness of combining LLM
and CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:
CRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a
real-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of
two collaborative approaches with two CRSs and two LLMs on four tasks of
Ecommerce pre-sales dialogue. We find that collaborations between CRS and LLM
can be very effective in some cases.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14627" title="Abstract">arXiv:2310.14627</a> [<a href="/pdf/2310.14627" title="Download PDF">pdf</a>, <a href="/format/2310.14627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrisisMatch: Semi-Supervised Few-Shot Learning for Fine-Grained Disaster  Tweet Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+H+P">Henry Peng Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Caragea%2C+C">Cornelia Caragea</a>, 
<a href="/search/cs?searchtype=author&query=Caragea%2C+D">Doina Caragea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ISCRAM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The shared real-time information about natural disasters on social media
platforms like Twitter and Facebook plays a critical role in informing
volunteers, emergency managers, and response organizations. However, supervised
learning models for monitoring disaster events require large amounts of
annotated data, making them unrealistic for real-time use in disaster events.
To address this challenge, we present a fine-grained disaster tweet
classification model under the semi-supervised, few-shot learning setting where
only a small number of annotated data is required. Our model, CrisisMatch,
effectively classifies tweets into fine-grained classes of interest using few
labeled data and large amounts of unlabeled data, mimicking the early stage of
a disaster. Through integrating effective semi-supervised learning ideas and
incorporating TextMixUp, CrisisMatch achieves performance improvement on two
disaster datasets of 11.2\% on average. Further analyses are also provided for
the influence of the number of labeled data and out-of-domain results.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14628" title="Abstract">arXiv:2310.14628</a> [<a href="/pdf/2310.14628" title="Download PDF">pdf</a>, <a href="/format/2310.14628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tengxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiangkun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As large language models (LLMs) have shown effectiveness with different
prompting methods, such as Chain of Thought, Program of Thought, we find that
these methods have formed a great complementarity to each other on math
reasoning tasks. In this work, we propose XoT, an integrated problem solving
framework by prompting LLMs with diverse reasoning thoughts. For each question,
XoT always begins with selecting the most suitable method then executes each
method iteratively. Within each iteration, XoT actively checks the validity of
the generated answer and incorporates the feedback from external executors,
allowing it to dynamically switch among different prompting methods. Through
extensive experiments on 10 popular math reasoning datasets, we demonstrate the
effectiveness of our proposed approach and thoroughly analyze the strengths of
each module. Moreover, empirical results suggest that our framework is
orthogonal to recent work that makes improvements on single reasoning methods
and can further generalise to logical reasoning domain. By allowing method
switching, XoT provides a fresh perspective on the collaborative integration of
diverse reasoning thoughts in a unified framework.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14629" title="Abstract">arXiv:2310.14629</a> [<a href="/pdf/2310.14629" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making informed decisions in cutting tool maintenance in milling: A KNN  based model agnostic approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahalkar%2C+A+M">Aditya M. Rahalkar</a>, 
<a href="/search/cs?searchtype=author&query=Khare%2C+O+M">Om M. Khare</a>, 
<a href="/search/cs?searchtype=author&query=Patange%2C+A+D">Abhishek D. Patange</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In machining processes, monitoring the condition of the tool is a crucial
aspect to ensure high productivity and quality of the product. Using different
machine learning techniques in Tool Condition Monitoring TCM enables a better
analysis of the large amount of data of different signals acquired during the
machining processes. The real time force signals encountered during the process
were acquired by performing numerous experiments. Different tool wear
conditions were considered during the experimentation. A comprehensive
statistical analysis of the data and feature selection using decision trees was
conducted, and the KNN algorithm was used to perform classification.
Hyperparameter tuning of the model was done to improve the models performance.
Much research has been done to employ machine learning approaches in tool
condition monitoring systems, however, a model agnostic approach to increase
the interpretability of the process and get an in depth understanding of how
the decision making is done is not implemented by many. This research paper
presents a KNN based white box model, which allows us to dive deep into how the
model performs the classification and how it prioritizes the different features
included. This approach helps in detecting why the tool is in a certain
condition and allows the manufacturer to make an informed decision about the
tools maintenance.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14631" title="Abstract">arXiv:2310.14631</a> [<a href="/pdf/2310.14631" title="Download PDF">pdf</a>, <a href="/format/2310.14631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Edge Caching For Individualized Demand Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quan%2C+G">Guocong Quan</a>, 
<a href="/search/cs?searchtype=author&query=Eryilmaz%2C+A">Atilla Eryilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Shroff%2C+N">Ness Shroff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The ever-growing end user data demands, and the simultaneous reductions in
memory costs are fueling edge-caching deployments. Caching at the edge is
substantially different from that at the core and needs to take into account
the nature of individual data demands. For example, an individual user may not
be interested in requesting the same data item again, if it has recently
requested it. Such individual dynamics are not apparent in the aggregated data
requests at the core and have not been considered in popularity-driven caching
designs for the core. Hence, these traditional caching policies could induce
significant inefficiencies when applied at the edges. To address this issue, we
develop new edge caching policies optimized for the individual demands that
also leverage overhearing opportunities at the wireless edge. With the
objective of maximizing the hit ratio, the proposed policies will actively
evict the data items that are not likely to be requested in the near future,
and strategically bring them back into the cache through overhearing when they
are likely to be popular again. Both theoretical analysis and numerical
simulations demonstrate that the proposed edge caching policies could
outperform the popularity-driven policies that are optimal at the core.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14633" title="Abstract">arXiv:2310.14633</a> [<a href="/pdf/2310.14633" title="Download PDF">pdf</a>, <a href="/format/2310.14633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending Input Contexts of Language Models through Training on  Segmented Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karypis%2C+P">Petros Karypis</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Effectively training language models on long inputs poses many technical
challenges. As a cost consideration, languages models are pretrained on a fixed
sequence length before being adapted to longer sequences. We explore various
methods for adapting models to longer inputs by training on segmented sequences
and an interpolation-based method for extending absolute positional embeddings.
We develop a training procedure to extend the input context size of pretrained
models with no architectural changes and no additional memory costs than
training on the original input lengths. By sub-sampling segments from long
inputs while maintaining their original position the model is able to learn new
positional interactions. Our method benefits both models trained with absolute
positional embeddings, by extending their input contexts, as well as popular
relative positional embedding methods showing a reduced perplexity on sequences
longer than they were trained on. We demonstrate our method can extend input
contexts by a factor of 4x while improving perplexity.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14637" title="Abstract">arXiv:2310.14637</a> [<a href="/pdf/2310.14637" title="Download PDF">pdf</a>, <a href="/format/2310.14637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-Aware Adversarial Training for Reliable Deep Hashing Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xunguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lin Wu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Transactions on Information Forensics and Security, vol.
  18, pp. 4681-4694, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Deep hashing has been intensively studied and successfully applied in
large-scale image retrieval systems due to its efficiency and effectiveness.
Recent studies have recognized that the existence of adversarial examples poses
a security threat to deep hashing models, that is, adversarial vulnerability.
Notably, it is challenging to efficiently distill reliable semantic
representatives for deep hashing to guide adversarial learning, and thereby it
hinders the enhancement of adversarial robustness of deep hashing-based
retrieval models. Moreover, current researches on adversarial training for deep
hashing are hard to be formalized into a unified minimax structure. In this
paper, we explore Semantic-Aware Adversarial Training (SAAT) for improving the
adversarial robustness of deep hashing models. Specifically, we conceive a
discriminative mainstay features learning (DMFL) scheme to construct semantic
representatives for guiding adversarial learning in deep hashing. Particularly,
our DMFL with the strict theoretical guarantee is adaptively optimized in a
discriminative learning manner, where both discriminative and semantic
properties are jointly considered. Moreover, adversarial examples are
fabricated by maximizing the Hamming distance between the hash codes of
adversarial samples and mainstay features, the efficacy of which is validated
in the adversarial attack trials. Further, we, for the first time, formulate
the formalized adversarial training of deep hashing into a unified minimax
optimization under the guidance of the generated mainstay codes. Extensive
experiments on benchmark datasets show superb attack performance against the
state-of-the-art algorithms, meanwhile, the proposed adversarial training can
effectively eliminate adversarial perturbations for trustworthy deep
hashing-based retrieval. Our code is available at
https://github.com/xandery-geek/SAAT.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14642" title="Abstract">arXiv:2310.14642</a> [<a href="/pdf/2310.14642" title="Download PDF">pdf</a>, <a href="/format/2310.14642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relit-NeuLF: Efficient Relighting and Novel View Synthesis via Neural 4D  Light Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Liangchen Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xiangyu Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lele Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Junsong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yi Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we address the problem of simultaneous relighting and novel
view synthesis of a complex scene from multi-view images with a limited number
of light sources. We propose an analysis-synthesis approach called Relit-NeuLF.
Following the recent neural 4D light field network (NeuLF), Relit-NeuLF first
leverages a two-plane light field representation to parameterize each ray in a
4D coordinate system, enabling efficient learning and inference. Then, we
recover the spatially-varying bidirectional reflectance distribution function
(SVBRDF) of a 3D scene in a self-supervised manner. A DecomposeNet learns to
map each ray to its SVBRDF components: albedo, normal, and roughness. Based on
the decomposed BRDF components and conditioning light directions, a RenderNet
learns to synthesize the color of the ray. To self-supervise the SVBRDF
decomposition, we encourage the predicted ray color to be close to the
physically-based rendering result using the microfacet model. Comprehensive
experiments demonstrate that the proposed method is efficient and effective on
both synthetic data and real-world human face data, and outperforms the
state-of-the-art results. We publicly released our code on GitHub. You can find
it here: https://github.com/oppo-us-research/RelitNeuLF
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14644" title="Abstract">arXiv:2310.14644</a> [<a href="/pdf/2310.14644" title="Download PDF">pdf</a>, <a href="/format/2310.14644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual k-Nearest-Neighbor Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stap%2C+D">David Stap</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">k-nearest-neighbor machine translation has demonstrated remarkable
improvements in machine translation quality by creating a datastore of cached
examples. However, these improvements have been limited to high-resource
language pairs, with large datastores, and remain a challenge for low-resource
languages. In this paper, we address this issue by combining representations
from multiple languages into a single datastore. Our results consistently
demonstrate substantial improvements not only in low-resource translation
quality (up to +3.6 BLEU), but also for high-resource translation quality (up
to +0.5 BLEU). Our experiments show that it is possible to create multilingual
datastores that are a quarter of the size, achieving a 5.3x speed improvement,
by using linguistic similarities for datastore creation.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14649" title="Abstract">arXiv:2310.14649</a> [<a href="/pdf/2310.14649" title="Download PDF">pdf</a>, <a href="/format/2310.14649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Scalable Solvers for Stochastic Linear and Nonlinear Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Sudhi Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Jolivet%2C+P">Pierre Jolivet</a>, 
<a href="/search/cs?searchtype=author&query=Dolean%2C+V">Victorita Dolean</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Abhijit Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">This article discusses the uncertainty quantification (UQ) for
time-independent linear and nonlinear partial differential equation (PDE)-based
systems with random model parameters carried out using sampling-free intrusive
stochastic Galerkin method leveraging multilevel scalable solvers constructed
combining two-grid Schwarz method and AMG. High-resolution spatial meshes along
with a large number of stochastic expansion terms increase the system size
leading to significant memory consumption and computational costs. Domain
decomposition (DD)-based parallel scalable solvers are developed to this end
for linear and nonlinear stochastic PDEs. A generalized minimum residual
(GMRES) iterative solver equipped with a multilevel preconditioner consisting
of restricted additive Schwarz (RAS) for the fine grid and algebraic multigrid
(AMG) for the coarse grid is constructed to improve scalability. Numerical
experiments illustrate the scalabilities of the proposed solver for stochastic
linear and nonlinear Poisson problems.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14650" title="Abstract">arXiv:2310.14650</a> [<a href="/pdf/2310.14650" title="Download PDF">pdf</a>, <a href="/format/2310.14650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic expansions for the solution of a linear PDE with a  multifrequency highly oscillatory potential
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Perczy%C5%84ski%2C+R">Rafa&#x142; Perczy&#x144;ski</a>, 
<a href="/search/math?searchtype=author&query=Augustynowicz%2C+A">Antoni Augustynowicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Highly oscillatory differential equations present significant challenges in
numerical treatments. The Modulated Fourier Expansion (MFE), used as an ansatz,
is a commonly employed tool as a numerical approximation method. In this
article, the Modulated Fourier Expansion is analytically derived for a linear
partial differential equation with a multifrequency highly oscillatory
potential. The solution of the equation is expressed as a convergent Neumann
series within the appropriate Sobolev space. The proposed approach enables,
firstly, to derive a general formula for the error associated with the
approximation of the solution by MFE, and secondly, to determine the
coefficients for this expansion -- without the need to solve numerically the
system of differential equations to find the coefficients of MFE. Numerical
experiments illustrate the theoretical investigations.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14651" title="Abstract">arXiv:2310.14651</a> [<a href="/pdf/2310.14651" title="Download PDF">pdf</a>, <a href="/format/2310.14651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x39b;$-Split: A Privacy-Preserving Split Computing Framework for  Cloud-Powered Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ohta%2C+S">Shoki Ohta</a>, 
<a href="/search/cs?searchtype=author&query=Nishio%2C+T">Takayuki Nishio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In the wake of the burgeoning expansion of generative artificial intelligence
(AI) services, the computational demands inherent to these technologies
frequently necessitate cloud-powered computational offloading, particularly for
resource-constrained mobile devices. These services commonly employ prompts to
steer the generative process, and both the prompts and the resultant content,
such as text and images, may harbor privacy-sensitive or confidential
information, thereby elevating security and privacy risks. To mitigate these
concerns, we introduce $\Lambda$-Split, a split computing framework to
facilitate computational offloading while simultaneously fortifying data
privacy against risks such as eavesdropping and unauthorized access. In
$\Lambda$-Split, a generative model, usually a deep neural network (DNN), is
partitioned into three sub-models and distributed across the user's local
device and a cloud server: the input-side and output-side sub-models are
allocated to the local, while the intermediate, computationally-intensive
sub-model resides on the cloud server. This architecture ensures that only the
hidden layer outputs are transmitted, thereby preventing the external
transmission of privacy-sensitive raw input and output data. Given the
black-box nature of DNNs, estimating the original input or output from
intercepted hidden layer outputs poses a significant challenge for malicious
eavesdroppers. Moreover, $\Lambda$-Split is orthogonal to traditional
encryption-based security mechanisms, offering enhanced security when deployed
in conjunction. We empirically validate the efficacy of the $\Lambda$-Split
framework using Llama 2 and Stable Diffusion XL, representative large language
and diffusion models developed by Meta and Stability AI, respectively. Our
$\Lambda$-Split implementation is publicly accessible at
https://github.com/nishio-laboratory/lambda_split.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14652" title="Abstract">arXiv:2310.14652</a> [<a href="/pdf/2310.14652" title="Download PDF">pdf</a>, <a href="/format/2310.14652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariant Feature Regularization for Fair Face Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiali Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Z">Zhongqi Yue</a>, 
<a href="/search/cs?searchtype=author&query=Tomoyuki%2C+K">Kagaya Tomoyuki</a>, 
<a href="/search/cs?searchtype=author&query=Tomoki%2C+S">Suzuki Tomoki</a>, 
<a href="/search/cs?searchtype=author&query=Jayashree%2C+K">Karlekar Jayashree</a>, 
<a href="/search/cs?searchtype=author&query=Pranata%2C+S">Sugiri Pranata</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Conference on Computer Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fair face recognition is all about learning invariant feature that
generalizes to unseen faces in any demographic group. Unfortunately, face
datasets inevitably capture the imbalanced demographic attributes that are
ubiquitous in real-world observations, and the model learns biased feature that
generalizes poorly in the minority group. We point out that the bias arises due
to the confounding demographic attributes, which mislead the model to capture
the spurious demographic-specific feature. The confounding effect can only be
removed by causal intervention, which requires the confounder annotations.
However, such annotations can be prohibitively expensive due to the diversity
of the demographic attributes. To tackle this, we propose to generate diverse
data partitions iteratively in an unsupervised fashion. Each data partition
acts as a self-annotated confounder, enabling our Invariant Feature
Regularization (INV-REG) to deconfound. INV-REG is orthogonal to existing
methods, and combining INV-REG with two strong baselines (Arcface and CIFP)
leads to new state-of-the-art that improves face recognition on a variety of
demographic groups. Code is available at
https://github.com/PanasonicConnect/InvReg.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14654" title="Abstract">arXiv:2310.14654</a> [<a href="/pdf/2310.14654" title="Download PDF">pdf</a>, <a href="/ps/2310.14654" title="Download PostScript">ps</a>, <a href="/format/2310.14654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab,  IIT Madras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%2C+N">Nithya R</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+M">Malavika S</a>, 
<a href="/search/cs?searchtype=author&query=F%2C+J">Jordan F</a>, 
<a href="/search/cs?searchtype=author&query=Gangwar%2C+A">Arjun Gangwar</a>, 
<a href="/search/cs?searchtype=author&query=J%2C+M+N">Metilda N J</a>, 
<a href="/search/cs?searchtype=author&query=Umesh%2C+S">S Umesh</a>, 
<a href="/search/cs?searchtype=author&query=Sarab%2C+R">Rithik Sarab</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+A+K">Akhilesh Kumar Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Divakaran%2C+G">Govind Divakaran</a>, 
<a href="/search/cs?searchtype=author&query=K%2C+S+V">Samudra Vijaya K</a>, 
<a href="/search/cs?searchtype=author&query=Gangashetty%2C+S+V">Suryakanth V Gangashetty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, About SPRING-INX Data
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">India is home to a multitude of languages of which 22 languages are
recognised by the Indian Constitution as official. Building speech based
applications for the Indian population is a difficult problem owing to limited
data and the number of languages and accents to accommodate. To encourage the
language technology community to build speech based applications in Indian
languages, we are open sourcing SPRING-INX data which has about 2000 hours of
legally sourced and manually transcribed speech data for ASR system building in
Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi
and Tamil. This endeavor is by SPRING Lab , Indian Institute of Technology
Madras and is a part of National Language Translation Mission (NLTM), funded by
the Indian Ministry of Electronics and Information Technology (MeitY),
Government of India. We describe the data collection and data cleaning process
along with the data statistics in this paper.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14657" title="Abstract">arXiv:2310.14657</a> [<a href="/pdf/2310.14657" title="Download PDF">pdf</a>, <a href="/format/2310.14657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning about Ambiguous Definite Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schouten%2C+S+F">Stefan F. Schouten</a>, 
<a href="/search/cs?searchtype=author&query=Bloem%2C+P">Peter Bloem</a>, 
<a href="/search/cs?searchtype=author&query=Markov%2C+I">Ilia Markov</a>, 
<a href="/search/cs?searchtype=author&query=Vossen%2C+P">Piek Vossen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Natural language reasoning plays an increasingly important role in improving
language models' ability to solve complex language understanding tasks. An
interesting use case for reasoning is the resolution of context-dependent
ambiguity. But no resources exist to evaluate how well Large Language Models
can use explicit reasoning to resolve ambiguity in language. We propose to use
ambiguous definite descriptions for this purpose and create and publish the
first benchmark dataset consisting of such phrases. Our method includes all
information required to resolve the ambiguity in the prompt, which means a
model does not require anything but reasoning to do well. We find this to be a
challenging task for recent LLMs. Code and data available at:
https://github.com/sfschouten/exploiting-ambiguity
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14658" title="Abstract">arXiv:2310.14658</a> [<a href="/pdf/2310.14658" title="Download PDF">pdf</a>, <a href="/format/2310.14658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CryptoVerif: a Computationally-Sound Security Protocol Verifier (Initial  Version with Communications on Channels)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanchet%2C+B">Bruno Blanchet</a> (PROSECCO)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This document presents the security protocol verifier CryptoVerif.CryptoVerif
does not rely on the symbolic, Dolev-Yao model, but on the computational model.
It can verify secrecy, correspondence (which include authentication), and
indistinguishability properties. It produces proofs presented as sequences of
games, like those manually written by cryptographers; these games are
formalized in aprobabilistic process calculus. CryptoVerif provides a generic
method for specifying security properties of the cryptographic primitives.It
produces proofs valid for any number of sessions of the protocol, and provides
an upper bound on the probability of success of an attack against the protocol
as a function of the probability of breaking each primitive and of the number
of sessions. It can work automatically, or the user can guide it with manual
proof indications.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14659" title="Abstract">arXiv:2310.14659</a> [<a href="/pdf/2310.14659" title="Download PDF">pdf</a>, <a href="/ps/2310.14659" title="Download PostScript">ps</a>, <a href="/format/2310.14659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Accurate Lagrangian Multipliers for Mixed Integer Linear  Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Demelas%2C+F">Francesco Demelas</a>, 
<a href="/search/cs?searchtype=author&query=Roux%2C+J+L">Joseph Le Roux</a>, 
<a href="/search/cs?searchtype=author&query=Lacroix%2C+M">Mathieu Lacroix</a>, 
<a href="/search/cs?searchtype=author&query=Parmentier%2C+A">Axel Parmentier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Lagrangian relaxation stands among the most efficient approaches for solving
a Mixed Integer Linear Programs (MILP) with difficult constraints. Given any
duals for these constraints, called Lagrangian Multipliers (LMs), it returns a
bound on the optimal value of the MILP, and Lagrangian methods seek the LMs
giving the best such bound. But these methods generally rely on iterative
algorithms resembling gradient descent to maximize the concave piecewise linear
dual function: the computational burden grows quickly with the number of
relaxed constraints. We introduce a deep learning approach that bypasses the
descent, effectively amortizing the local, per instance, optimization. A
probabilistic encoder based on a graph convolutional network computes
high-dimensional representations of relaxed constraints in MILP instances. A
decoder then turns these representations into LMs. We train the encoder and
decoder jointly by directly optimizing the bound obtained from the predicted
multipliers. Numerical experiments show that our approach closes up to 85~\% of
the gap between the continuous relaxation and the best Lagrangian bound, and
provides a high quality warm-start for descent based Lagrangian methods.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14661" title="Abstract">arXiv:2310.14661</a> [<a href="/pdf/2310.14661" title="Download PDF">pdf</a>, <a href="/ps/2310.14661" title="Download PostScript">ps</a>, <a href="/format/2310.14661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tractable MCMC for Private Learning with Pure and Gaussian Differential  Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yingyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Redberg%2C+R">Rachel Redberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Posterior sampling, i.e., exponential mechanism to sample from the posterior
distribution, provides $\varepsilon$-pure differential privacy (DP) guarantees
and does not suffer from potentially unbounded privacy breach introduced by
$(\varepsilon,\delta)$-approximate DP. In practice, however, one needs to apply
approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus
re-introducing the unappealing $\delta$-approximation error into the privacy
guarantees. To bridge this gap, we propose the Approximate SAample Perturbation
(abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to
its Wasserstein-infinity ($W_\infty$) distance from a reference distribution
that satisfies pure DP or pure Gaussian DP (i.e., $\delta=0$). We then leverage
a Metropolis-Hastings algorithm to generate the sample and prove that the
algorithm converges in W$_\infty$ distance. We show that by combining our new
techniques with a careful localization step, we obtain the first nearly
linear-time algorithm that achieves the optimal rates in the DP-ERM problem
with strongly convex and smooth losses.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14664" title="Abstract">arXiv:2310.14664</a> [<a href="/pdf/2310.14664" title="Download PDF">pdf</a>, <a href="/format/2310.14664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Pruning via Moving-one-Sample-out
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haoru Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Sitong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+F">Fei Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yukang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhibin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we propose a novel data-pruning approach called
moving-one-sample-out (MoSo), which aims to identify and remove the least
informative samples from the training set. The core insight behind MoSo is to
determine the importance of each sample by assessing its impact on the optimal
empirical risk. This is achieved by measuring the extent to which the empirical
risk changes when a particular sample is excluded from the training set.
Instead of using the computationally expensive leaving-one-out-retraining
procedure, we propose an efficient first-order approximator that only requires
gradient information from different training stages. The key idea behind our
approximation is that samples with gradients that are consistently aligned with
the average gradient of the training set are more informative and should
receive higher scores, which could be intuitively understood as follows: if the
gradient from a specific sample is consistent with the average gradient vector,
it implies that optimizing the network using the sample will yield a similar
effect on all remaining samples. Experimental results demonstrate that MoSo
effectively mitigates severe performance degradation at high pruning ratios and
achieves satisfactory performance across various settings.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14665" title="Abstract">arXiv:2310.14665</a> [<a href="/pdf/2310.14665" title="Download PDF">pdf</a>, <a href="/format/2310.14665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Read Disturbance in High Bandwidth Memory: An Experimental  Analysis of Real HBM2 DRAM Chips
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olgun%2C+A">Ataberk Olgun</a>, 
<a href="/search/cs?searchtype=author&query=Osseiran%2C+M">Majd Osseiran</a>, 
<a href="/search/cs?searchtype=author&query=Yaglikci%2C+A+G">Abdullah Giray Yaglikci</a>, 
<a href="/search/cs?searchtype=author&query=Tugrul%2C+Y+C">Yahya Can Tugrul</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haocong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Rhyner%2C+S">Steve Rhyner</a>, 
<a href="/search/cs?searchtype=author&query=Salami%2C+B">Behzad Salami</a>, 
<a href="/search/cs?searchtype=author&query=Luna%2C+J+G">Juan Gomez Luna</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of an earlier DSN Disrupt 2023 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">DRAM read disturbance is a significant and worsening safety, security, and
reliability issue of modern DRAM chips that can be exploited to break memory
isolation. Two prominent examples of read-disturb phenomena are RowHammer and
RowPress. However, no prior work extensively studies read-disturb phenomena in
modern high-bandwidth memory (HBM) chips. In this work, we experimentally
demonstrate the effects of read disturbance and uncover the inner workings of
undocumented in-DRAM read disturbance mitigation mechanisms in HBM.
<br />Our characterization of six real HBM2 DRAM chips shows that (1) the number of
read disturbance bitflips and the number of row activations needed to induce
the first read disturbance bitflip significantly varies between different HBM2
chips and different 3D-stacked channels, pseudo channels, banks, and rows
inside an HBM2 chip. (2) The DRAM rows at the end and in the middle of a DRAM
bank exhibit significantly fewer read disturbance bitflips than the rest of the
rows. (3) It takes fewer additional activations to induce more read disturbance
bitflips in a DRAM row if the row exhibits the first bitflip already at a
relatively high activation count. (4) HBM2 chips exhibit read disturbance
bitflips with only two row activations when rows are kept active for an
extremely long time.
<br />We show that a modern HBM2 DRAM chip implements undocumented read disturbance
defenses that can track potential aggressor rows based on how many times they
are activated, and refresh their victim rows with every 17 periodic refresh
operations. We draw key takeaways from our observations and discuss their
implications for future read disturbance attacks and defenses. We explain how
our findings could be leveraged to develop both i) more powerful read
disturbance attacks and ii) more efficient read disturbance defense mechanisms.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14666" title="Abstract">arXiv:2310.14666</a> [<a href="/pdf/2310.14666" title="Download PDF">pdf</a>, <a href="/format/2310.14666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeLeP: Learning Based Semantic Prefetching for Exploratory Database  Workloads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zirak%2C+F">Farzaneh Zirak</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+F">Farhana Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Borovica-Gajic%2C+R">Renata Borovica-Gajic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Prefetching is a crucial technique employed in traditional databases to
enhance interactivity, particularly in the context of data exploitation. Data
exploration is a query processing paradigm in which users search for insights
buried in the data, often not knowing what exactly they are looking for. Data
exploratory tools deal with multiple challenges such as the need for
interactivity with no a priori knowledge being present to help with the system
tuning. The state-of-the-art prefetchers are specifically designed for
navigational workloads only, where the number of possible actions is limited.
The prefetchers that work with SQL-based workloads, on the other hand, mainly
rely on data logical addresses rather than the data semantics. They fail to
predict complex access patterns in cases where the database size is
substantial, resulting in an extensive address space, or when there is frequent
co-accessing of data. In this paper, we propose SeLeP, a semantic prefetcher
that makes prefetching decisions for both types of workloads, based on the
encoding of the data values contained inside the accessed blocks. Following the
popular path of using machine learning approaches to automatically learn the
hidden patterns, we formulate the prefetching task as a time-series forecasting
problem and use an encoder-decoder LSTM architecture to learn the data access
pattern. Our extensive experiments, across real-life exploratory workloads,
demonstrate that SeLeP improves the hit ratio up to 40% and reduces I/O time up
to 45% compared to the state-of-the-art, attaining impressive 95% hit ratio and
80% I/O reduction on average.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14669" title="Abstract">arXiv:2310.14669</a> [<a href="/pdf/2310.14669" title="Download PDF">pdf</a>, <a href="/format/2310.14669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> B^2SFL: A Bi-level Blockchained Architecture for Secure Federated  Learning-based Traffic Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Meese%2C+C">Collin Meese</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wanxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chien-Chung Shen</a>, 
<a href="/search/cs?searchtype=author&query=Nejad%2C+M">Mark Nejad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted for publication in IEEE Transactions on Services Computing (TSC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) is a privacy-preserving machine learning (ML)
technology that enables collaborative training and learning of a global ML
model based on aggregating distributed local model updates. However, security
and privacy guarantees could be compromised due to malicious participants and
the centralized FL server. This article proposed a bi-level blockchained
architecture for secure federated learning-based traffic prediction. The bottom
and top layer blockchain store the local model and global aggregated parameters
accordingly, and the distributed homomorphic-encrypted federated averaging
(DHFA) scheme addresses the secure computation problems. We propose the partial
private key distribution protocol and a partially homomorphic
encryption/decryption scheme to achieve the distributed privacy-preserving
federated averaging model. We conduct extensive experiments to measure the
running time of DHFA operations, quantify the read and write performance of the
blockchain network, and elucidate the impacts of varying regional group sizes
and model complexities on the resulting prediction accuracy for the online
traffic flow prediction task. The results indicate that the proposed system can
facilitate secure and decentralized federated learning for real-world traffic
prediction tasks.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14670" title="Abstract">arXiv:2310.14670</a> [<a href="/pdf/2310.14670" title="Download PDF">pdf</a>, <a href="/format/2310.14670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and  Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhecan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+H">Haoxuan You</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Keyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yicheng He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Codella%2C+N">Noal Codella</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shih-Fu Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Vision-language (VL) understanding tasks evaluate models' comprehension of
complex visual scenes through multiple-choice questions. However, we have
identified two dataset biases that models can exploit as shortcuts to resolve
various VL tasks correctly without proper understanding. The first type of
dataset bias is \emph{Unbalanced Matching} bias, where the correct answer
overlaps the question and image more than the incorrect answers. The second
type of dataset bias is \emph{Distractor Similarity} bias, where incorrect
answers are overly dissimilar to the correct answer but significantly similar
to other incorrect answers within the same sample. To address these dataset
biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic
training and debiased evaluation data. We then introduce Intra-sample
Counterfactual Training (ICT) to assist models in utilizing the synthesized
training data, particularly the counterfactual data, via focusing on
intra-sample differentiation. Extensive experiments demonstrate the
effectiveness of ADS and ICT in consistently improving model performance across
different benchmarks, even in domain-shifted scenarios.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14671" title="Abstract">arXiv:2310.14671</a> [<a href="/pdf/2310.14671" title="Download PDF">pdf</a>, <a href="/format/2310.14671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Population Descent: A Natural-Selection Based Hyper-Parameter Tuning  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pomalapally%2C+A">Abhinav Pomalapally</a>, 
<a href="/search/cs?searchtype=author&query=Mabsout%2C+B+E">Bassel El Mabsout</a>, 
<a href="/search/cs?searchtype=author&query=Mansuco%2C+R">Renato Mansuco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">First-order gradient descent has been the base of the most successful
optimization algorithms ever implemented. On supervised learning problems with
very high dimensionality, such as neural network optimization, it is almost
always the algorithm of choice, mainly due to its memory and computational
efficiency. However, it is a classical result in optimization that gradient
descent converges to local minima on non-convex functions. Even more
importantly, in certain high-dimensional cases, escaping the plateaus of large
saddle points becomes intractable. On the other hand, black-box optimization
methods are not sensitive to the local structure of a loss function's landscape
but suffer the curse of dimensionality. Instead, memetic algorithms aim to
combine the benefits of both. Inspired by this, we present Population Descent,
a memetic algorithm focused on hyperparameter optimization. We show that an
adaptive m-elitist selection approach combined with a normalized-fitness-based
randomization scheme outperforms more complex state-of-the-art algorithms by up
to 13% on common benchmark tasks.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14672" title="Abstract">arXiv:2310.14672</a> [<a href="/pdf/2310.14672" title="Download PDF">pdf</a>, <a href="/format/2310.14672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integration of Independent Heat Transfer Mechanisms for Non-Contact Cold  Sensation Presentation With Low Residual Heat
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiayi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hasegawa%2C+S">Shoichi Hasegawa</a>, 
<a href="/search/cs?searchtype=author&query=Kiyokawa%2C+K">Kiyoshi Kiyokawa</a>, 
<a href="/search/cs?searchtype=author&query=Ienaga%2C+N">Naoto Ienaga</a>, 
<a href="/search/cs?searchtype=author&query=Kuroda%2C+Y">Yoshihiro Kuroda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Thermal sensation is crucial to enhancing our comprehension of the world and
enhancing our ability to interact with it. Therefore, the development of
thermal sensation presentation technologies holds significant potential,
providing a novel method of interaction. Traditional technologies often leave
residual heat in the system or the skin, affecting subsequent presentations.
Our study focuses on presenting thermal sensations with low residual heat,
especially cold sensations. To mitigate the impact of residual heat in the
presentation system, we opted for a non-contact method, and to address the
influence of residual heat on the skin, we present thermal sensations without
significantly altering skin temperature. Specifically, we integrated two highly
responsive and independent heat transfer mechanisms: convection via cold air
and radiation via visible light, providing non-contact thermal stimuli. By
rapidly alternating between perceptible decreases and imperceptible increases
in temperature on the same skin area, we maintained near-constant skin
temperature while presenting continuous cold sensations. In our experiments
involving 15 participants, we observed that when the cooling rate was -0.2 to
-0.24 degree celsius per second and the cooling time ratio was 30 to 50 %, more
than 86.67 % of the participants perceived only persistent cold without any
warmth.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14673" title="Abstract">arXiv:2310.14673</a> [<a href="/pdf/2310.14673" title="Download PDF">pdf</a>, <a href="/format/2310.14673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-contact Cold Thermal Display by Controlling Low-temperature Air Flow  Generated with Vortex Tube
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiayi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kuroda%2C+Y">Yoshihiro Kuroda</a>, 
<a href="/search/cs?searchtype=author&query=Yoshimoto%2C+S">Shunsuke Yoshimoto</a>, 
<a href="/search/cs?searchtype=author&query=Oshiro%2C+O">Osamu Oshiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In recent years, thermal display has been studied intensively in order to
represent a more realistic tactile quality of the object. Since human feels the
temperature of the air without touching other objects, it is necessary to
present thermal sensation in a non-contact manner. Studies on non-contact heat
display have been explored; however, few studies have reported on a device that
can display cold in a non-contact manner. In this study, we propose a
non-contact cold thermal display using a low-temperature heat source-vortex
tube, which can generate ultra-low air temperature when supplied with
compressed air. We developed a cooling model that relates the flow velocity of
cold air with the absorbed heat from skin; we implemented a prototype system
that can control the flow velocity of the generated air; and we conducted an
experiment to examine the cold sensation that the system can present. Our
results revealed that various cold sensations can be generated so that the
faster the flow velocity, the colder a user would feel.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14675" title="Abstract">arXiv:2310.14675</a> [<a href="/pdf/2310.14675" title="Download PDF">pdf</a>, <a href="/format/2310.14675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Out-of-Domain Detection for Automated Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A4mann%2C+T">Timo S&#xe4;mann</a>, 
<a href="/search/cs?searchtype=author&query=Gro%C3%9F%2C+H">Horst-Michael Gro&#xdf;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Machine Learning in Certified Systems (MLCS) Workshop, 14.-15.01.2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Ensuring safety in automated driving is a major challenge for the automotive
industry. Special attention is paid to artificial intelligence, in particular
to Deep Neural Networks (DNNs), which is considered a key technology in the
realization of highly automated driving. DNNs learn from training data, which
means that they only achieve good accuracy within the underlying data
distribution of the training data. When leaving the training domain, a
distributional shift is caused, which can lead to a drastic reduction of
accuracy. In this work, we present a proof of concept for a safety mechanism
that can detect the leaving of the domain online, i.e. at runtime. In our
experiments with the Synthia data set we can show that a 100 % correct
detection of whether the input data is inside or outside the domain is
achieved. The ability to detect when the vehicle leaves the domain can be an
important requirement for certification.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14676" title="Abstract">arXiv:2310.14676</a> [<a href="/pdf/2310.14676" title="Download PDF">pdf</a>, <a href="/format/2310.14676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-Trained Language Models Augmented with Synthetic Scanpaths for  Natural Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shuwen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Prasse%2C+P">Paul Prasse</a>, 
<a href="/search/cs?searchtype=author&query=Reich%2C+D+R">David R. Reich</a>, 
<a href="/search/cs?searchtype=author&query=Scheffer%2C+T">Tobias Scheffer</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A4ger%2C+L+A">Lena A. J&#xe4;ger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Human gaze data offer cognitive information that reflects natural language
comprehension. Indeed, augmenting language models with human scanpaths has
proven beneficial for a range of NLP tasks, including language understanding.
However, the applicability of this approach is hampered because the abundance
of text corpora is contrasted by a scarcity of gaze data. Although models for
the generation of human-like scanpaths during reading have been developed, the
potential of synthetic gaze data across NLP tasks remains largely unexplored.
We develop a model that integrates synthetic scanpath generation with a
scanpath-augmented language model, eliminating the need for human gaze data.
Since the model's error gradient can be propagated throughout all parts of the
model, the scanpath generator can be fine-tuned to downstream tasks. We find
that the proposed model not only outperforms the underlying language model, but
achieves a performance that is comparable to a language model augmented with
real human gaze data. Our code is publicly available.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14681" title="Abstract">arXiv:2310.14681</a> [<a href="/pdf/2310.14681" title="Download PDF">pdf</a>, <a href="/format/2310.14681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latency and Power Consumption in 2.4 GHz IoT Wireless Mesh Nodes: An  Experimental Evaluation of Bluetooth Mesh and Wirepas Mesh
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cortesi%2C+S">Silvano Cortesi</a>, 
<a href="/search/cs?searchtype=author&query=Vogt%2C+C">Christian Vogt</a>, 
<a href="/search/cs?searchtype=author&query=Reinschmidt%2C+E">Elio Reinschmidt</a>, 
<a href="/search/cs?searchtype=author&query=Magno%2C+M">Michele Magno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been accepted for publication in the proceedings of the 2023 IEEE International Conference on Wireless and Mobile Computing, Networking And Communications (WiMob). DOI: 10.1109/WiMob58348.2023.10187799
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The rapid growth of the Internet of Things paradigm is pushing the need to
connect billions of batteryoperated devices to the internet and among them. To
address this need, the introduction of energy-efficient wireless mesh networks
based on Bluetooth provides an effective solution. This paper proposes a
testbed setup to accurately evaluate and compare the standard Bluetooth Mesh
5.0 and the emerging energy-efficient Wirepas protocol that promises better
performance. The paper presents the evaluation in terms of power consumption,
energy efficiency, and transmission latency which are the most crucial
features, in a controlled and reproducible test setup consisting of 10 nodes.
Experimental results demonstrated that Wirepas has a median latency of 2.83 ms
in Low-Latency mode respectively around 2 s in the Low-Energy mode. The
corresponding power consumption is 6.2 mA in Low-Latency mode and 38.9 uA in
Low-Energy mode. For Bluetooth Mesh the median latency is 4.54 ms with a power
consumption of 6.2 mA at 3.3 V. Based on this comparison, conclusions about the
advantages and disadvantages of both technologies can be drawn.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14684" title="Abstract">arXiv:2310.14684</a> [<a href="/pdf/2310.14684" title="Download PDF">pdf</a>, <a href="/format/2310.14684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpEL: Structured Prediction for Entity Linking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shavarani%2C+H+S">Hassan S. Shavarani</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Anoop Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Entity linking is a prominent thread of research focused on structured data
creation by linking spans of text to an ontology or knowledge source. We
revisit the use of structured prediction for entity linking which classifies
each individual input token as an entity, and aggregates the token predictions.
Our system, called SpEL (Structured prediction for Entity Linking) is a
state-of-the-art entity linking system that uses some new ideas to apply
structured prediction to the task of entity linking including: two refined
fine-tuning steps; a context sensitive prediction aggregation strategy;
reduction of the size of the model's output vocabulary, and; we address a
common problem in entity-linking systems where there is a training vs.
inference tokenization mismatch. Our experiments show that we can outperform
the state-of-the-art on the commonly used AIDA benchmark dataset for entity
linking to Wikipedia. Our method is also very compute efficient in terms of
number of parameters and speed of inference.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14685" title="Abstract">arXiv:2310.14685</a> [<a href="/pdf/2310.14685" title="Download PDF">pdf</a>, <a href="/format/2310.14685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Learning in Contextual Games under Unknown Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maddux%2C+A+M">Anna M. Maddux</a>, 
<a href="/search/cs?searchtype=author&query=Kamgarpour%2C+M">Maryam Kamgarpour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We consider the problem of learning to play a repeated contextual game with
unknown reward and unknown constraints functions. Such games arise in
applications where each agent's action needs to belong to a feasible set, but
the feasible set is a priori unknown. For example, in constrained multi-agent
reinforcement learning, the constraints on the agents' policies are a function
of the unknown dynamics and hence, are themselves unknown. Under kernel-based
regularity assumptions on the unknown functions, we develop a no-regret,
no-violation approach which exploits similarities among different reward and
constraint outcomes. The no-violation property ensures that the time-averaged
sum of constraint violations converges to zero as the game is repeated. We show
that our algorithm, referred to as c.z.AdaNormalGP, obtains kernel-dependent
regret bounds and that the cumulative constraint violations have sublinear
kernel-dependent upper bounds. In addition we introduce the notion of
constrained contextual coarse correlated equilibria (c.z.CCE) and show that
$\epsilon$-c.z.CCEs can be approached whenever players' follow a no-regret
no-violation strategy. Finally, we experimentally demonstrate the effectiveness
of c.z.AdaNormalGP on an instance of multi-agent reinforcement learning.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14687" title="Abstract">arXiv:2310.14687</a> [<a href="/pdf/2310.14687" title="Download PDF">pdf</a>, <a href="/format/2310.14687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> API-Assisted Code Generation for Question Answering on Varied Table  Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yihan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ryan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiruo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera ready, 13 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A persistent challenge to table question answering (TableQA) by generating
executable programs has been adapting to varied table structures, typically
requiring domain-specific logical forms. In response, this paper introduces a
unified TableQA framework that: (1) provides a unified representation for
structured tables as multi-index Pandas data frames, (2) uses Python as a
powerful querying language, and (3) uses few-shot prompting to translate NL
questions into Python programs, which are executable on Pandas data frames.
Furthermore, to answer complex relational questions with extended program
functionality and external knowledge, our framework allows customized APIs that
Python programs can call. We experiment with four TableQA datasets that involve
tables of different structures -- relational, multi-table, and hierarchical
matrix shapes -- and achieve prominent improvements over past state-of-the-art
systems. In ablation studies, we (1) show benefits from our multi-index
representation and APIs over baselines that use only an LLM, and (2)
demonstrate that our approach is modular and can incorporate additional APIs.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14692" title="Abstract">arXiv:2310.14692</a> [<a href="/pdf/2310.14692" title="Download PDF">pdf</a>, <a href="/format/2310.14692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Partial Shape Correspondence and Functional Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bracha%2C+A">Amit Bracha</a>, 
<a href="/search/cs?searchtype=author&query=Dag%C3%A8s%2C+T">Thomas Dag&#xe8;s</a>, 
<a href="/search/cs?searchtype=author&query=Kimmel%2C+R">Ron Kimmel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While dealing with matching shapes to their parts, we often utilize an
instrument known as functional maps. The idea is to translate the shape
matching problem into ``convenient'' spaces by which matching is performed
algebraically by solving a least squares problem. Here, we argue that such
formulations, though popular in this field, introduce errors in the estimated
match when partiality is invoked. Such errors are unavoidable even when
considering advanced feature extraction networks, and they can be shown to
escalate with increasing degrees of shape partiality, adversely affecting the
learning capability of such systems. To circumvent these limitations, we
propose a novel approach for partial shape matching.
<br />Our study of functional maps led us to a novel method that establishes direct
correspondence between partial and full shapes through feature matching
bypassing the need for functional map intermediate spaces. The Gromov distance
between metric spaces leads to the construction of the first part of our loss
functions. For regularization we use two options: a term based on the area
preserving property of the mapping, and a relaxed version of it without the
need to compute a functional map.
<br />The proposed approach shows superior performance on the SHREC'16 dataset,
outperforming existing unsupervised methods for partial shape matching. In
particular, it achieves state-of-the-art result on the SHREC'16 HOLES
benchmark, superior also compared to supervised methods.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14693" title="Abstract">arXiv:2310.14693</a> [<a href="/pdf/2310.14693" title="Download PDF">pdf</a>, <a href="/format/2310.14693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated learning compression designed for lightweight communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+L+G">Lucas Grativol Ribeiro</a> (IMT Atlantique - MEE, Lab_STICC_BRAIn, Lab-STICC_2AI, LHC), 
<a href="/search/cs?searchtype=author&query=Leonardon%2C+M">Mathieu Leonardon</a> (IMT Atlantique - MEE, Lab_STICC_BRAIn), 
<a href="/search/cs?searchtype=author&query=Muller%2C+G">Guillaume Muller</a>, 
<a href="/search/cs?searchtype=author&query=Fresse%2C+V">Virginie Fresse</a>, 
<a href="/search/cs?searchtype=author&query=Arzel%2C+M">Matthieu Arzel</a> (IMT Atlantique - MEE, Lab-STICC_2AI)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE 30th International Conference on Electronics, Circuits and
  Systems, Dec 2023, Istanbul, Turkey
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Federated Learning (FL) is a promising distributed method for edge-level
machine learning, particularly for privacysensitive applications such as those
in military and medical domains, where client data cannot be shared or
transferred to a cloud computing server. In many use-cases, communication cost
is a major challenge in FL due to its natural intensive network usage. Client
devices, such as smartphones or Internet of Things (IoT) nodes, have limited
resources in terms of energy, computation, and memory. To address these
hardware constraints, lightweight models and compression techniques such as
pruning and quantization are commonly adopted in centralised paradigms. In this
paper, we investigate the impact of compression techniques on FL for a typical
image classification task. Going further, we demonstrate that a straightforward
method can compresses messages up to 50% while having less than 1% of accuracy
loss, competing with state-of-the-art techniques.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14695" title="Abstract">arXiv:2310.14695</a> [<a href="/pdf/2310.14695" title="Download PDF">pdf</a>, <a href="/format/2310.14695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAwa-NeRF: Instant Learning of Compression-Aware NeRF Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmoud%2C+O">Omnia Mahmoud</a>, 
<a href="/search/cs?searchtype=author&query=Ladune%2C+T">Th&#xe9;o Ladune</a>, 
<a href="/search/cs?searchtype=author&query=Gendrin%2C+M">Matthieu Gendrin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Modeling 3D scenes by volumetric feature grids is one of the promising
directions of neural approximations to improve Neural Radiance Fields (NeRF).
Instant-NGP (INGP) introduced multi-resolution hash encoding from a lookup
table of trainable feature grids which enabled learning high-quality neural
graphics primitives in a matter of seconds. However, this improvement came at
the cost of higher storage size. In this paper, we address this challenge by
introducing instant learning of compression-aware NeRF features (CAwa-NeRF),
that allows exporting the zip compressed feature grids at the end of the model
training with a negligible extra time overhead without changing neither the
storage architecture nor the parameters used in the original INGP paper.
Nonetheless, the proposed method is not limited to INGP but could also be
adapted to any model. By means of extensive simulations, our proposed instant
learning pipeline can achieve impressive results on different kinds of static
scenes such as single object masked background scenes and real-life scenes
captured in our studio. In particular, for single object masked background
scenes CAwa-NeRF compresses the feature grids down to 6% (1.2 MB) of the
original size without any loss in the PSNR (33 dB) or down to 2.4% (0.53 MB)
with a slight virtual loss (32.31 dB).
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14696" title="Abstract">arXiv:2310.14696</a> [<a href="/pdf/2310.14696" title="Download PDF">pdf</a>, <a href="/format/2310.14696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree of Clarifications: Answering Ambiguous Questions with  Retrieval-Augmented Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gangwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+B">Byeongguk Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Joonsuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jaewoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Questions in open-domain question answering are often ambiguous, allowing
multiple interpretations. One approach to handling them is to identify all
possible interpretations of the ambiguous question (AQ) and to generate a
long-form answer addressing them all, as suggested by Stelmakh et al., (2022).
While it provides a comprehensive response without bothering the user for
clarification, considering multiple dimensions of ambiguity and gathering
corresponding knowledge remains a challenge. To cope with the challenge, we
propose a novel framework, Tree of Clarifications (ToC): It recursively
constructs a tree of disambiguations for the AQ -- via few-shot prompting
leveraging external knowledge -- and uses it to generate a long-form answer.
ToC outperforms existing baselines on ASQA in a few-shot setup across the
metrics, while surpassing fully-supervised baselines trained on the whole
training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at
https://github.com/gankim/tree-of-clarifications.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14700" title="Abstract">arXiv:2310.14700</a> [<a href="/pdf/2310.14700" title="Download PDF">pdf</a>, <a href="/format/2310.14700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interaction-Driven Active 3D Reconstruction with Object Interiors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zihao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+F">Fubao Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruizhen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hui Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SIGGRAPH Asia 2023, project page at <a href="https://vcc.tech/research/2023/InterRecon">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce an active 3D reconstruction method which integrates visual
perception, robot-object interaction, and 3D scanning to recover both the
exterior and interior, i.e., unexposed, geometries of a target 3D object.
Unlike other works in active vision which focus on optimizing camera viewpoints
to better investigate the environment, the primary feature of our
reconstruction is an analysis of the interactability of various parts of the
target object and the ensuing part manipulation by a robot to enable scanning
of occluded regions. As a result, an understanding of part articulations of the
target object is obtained on top of complete geometry acquisition. Our method
operates fully automatically by a Fetch robot with built-in RGBD sensors. It
iterates between interaction analysis and interaction-driven reconstruction,
scanning and reconstructing detected moveable parts one at a time, where both
the articulated part detection and mesh reconstruction are carried out by
neural networks. In the final step, all the remaining, non-articulated parts,
including all the interior structures that had been exposed by prior part
manipulations and subsequently scanned, are reconstructed to complete the
acquisition. We demonstrate the performance of our method via qualitative and
quantitative evaluation, ablation studies, comparisons to alternatives, as well
as experiments in a real environment.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14702" title="Abstract">arXiv:2310.14702</a> [<a href="/pdf/2310.14702" title="Download PDF">pdf</a>, <a href="/format/2310.14702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BM2CP: Efficient Collaborative Perception with LiDAR-Camera Modalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Binyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhaonian Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures. Accepted by CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Collaborative perception enables agents to share complementary perceptual
information with nearby agents. This would improve the perception performance
and alleviate the issues of single-view perception, such as occlusion and
sparsity. Most existing approaches mainly focus on single modality (especially
LiDAR), and not fully exploit the superiority of multi-modal perception. We
propose a collaborative perception paradigm, BM2CP, which employs LiDAR and
camera to achieve efficient multi-modal perception. It utilizes LiDAR-guided
modal fusion, cooperative depth generation and modality-guided intermediate
fusion to acquire deep interactions among modalities of different agents,
Moreover, it is capable to cope with the special case where one of the sensors,
same or different type, of any agent is missing. Extensive experiments validate
that our approach outperforms the state-of-the-art methods with 50X lower
communication volumes in both simulated and real-world autonomous driving
scenarios. Our code is available at https://github.com/byzhaoAI/BM2CP.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14703" title="Abstract">arXiv:2310.14703</a> [<a href="/pdf/2310.14703" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The continued usefulness of vocabulary tests for evaluating large  language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+G">Gonzalo Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Conde%2C+J">Javier Conde</a>, 
<a href="/search/cs?searchtype=author&query=Merino-G%C3%B3mez%2C+E">Elena Merino-G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Berm%C3%BAdez-Margaretto%2C+B">Beatriz Berm&#xfa;dez-Margaretto</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez%2C+J+A">Jos&#xe9; Alberto Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Reviriego%2C+P">Pedro Reviriego</a>, 
<a href="/search/cs?searchtype=author&query=Brysbaert%2C+M">Marc Brysbaert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In their seminal article on semantic vectors, Landauer and Dumain (1997)
proposed testing the quality of AI language models with a challenging
vocabulary test. We show that their Test of English as a Foreign Language
(TOEFL) test remains informative for contemporary major language models, since
none of the models was perfect and made errors on divergent items. The TOEFL
test consists of target words with four alternatives to choose from. We further
tested the models on a Yes/No test that requires distinguishing between
existing words and made-up nonwords. The models performed significantly worse
on the nonword items, in line with other observations that current major
language models provide non-existent information. The situation was worse when
we generalized the tests to Spanish. Here, most models gave
meanings/translations for the majority of random letter sequences. On the plus
side, the best models began to perform quite well, and they also pointed to
nonwords that were unknown to the test participants but can be found in
dictionaries.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14705" title="Abstract">arXiv:2310.14705</a> [<a href="/pdf/2310.14705" title="Download PDF">pdf</a>, <a href="/format/2310.14705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot-Assisted Navigation for Visually Impaired through Adaptive  Impedance and Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balatti%2C+P">Pietro Balatti</a> (1), 
<a href="/search/cs?searchtype=author&query=Ozdamar%2C+I">Idil Ozdamar</a> (1,2), 
<a href="/search/cs?searchtype=author&query=Sirintuna%2C+D">Doganay Sirintuna</a> (1,2), 
<a href="/search/cs?searchtype=author&query=Fortini%2C+L">Luca Fortini</a> (1), 
<a href="/search/cs?searchtype=author&query=Leonori%2C+M">Mattia Leonori</a> (1), 
<a href="/search/cs?searchtype=author&query=Gandarias%2C+J+M">Juan M. Gandarias</a> (3), 
<a href="/search/cs?searchtype=author&query=Ajoudani%2C+A">Arash Ajoudani</a> (1) ((1) HRI2 Lab, Istituto Italiano di Tecnologia, Genoa, Italy, (2) Dept. of Informatics, Bioengineering, Robotics, and System Engineering. University of Genoa, Genoa, Italy, (3) Robotics and Mechatronics lab, Systems Engineering and Automation Department, University of Malaga, Malaga, Spain)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, submitted to IEEE International Conference on Robotics and Automation, for associated video, see <a href="https://youtu.be/B94n3QjdnJE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a framework to navigate visually impaired people through
unfamiliar environments by means of a mobile manipulator. The Human-Robot
system consists of three key components: a mobile base, a robotic arm, and the
human subject who gets guided by the robotic arm via physically coupling their
hand with the cobot's end-effector. These components, receiving a goal from the
user, traverse a collision-free set of waypoints in a coordinated manner, while
avoiding static and dynamic obstacles through an obstacle avoidance unit and a
novel human guidance planner. With this aim, we also present a legs tracking
algorithm that utilizes 2D LiDAR sensors integrated into the mobile base to
monitor the human pose. Additionally, we introduce an adaptive pulling planner
responsible for guiding the individual back to the intended path if they veer
off course. This is achieved by establishing a target arm end-effector position
and dynamically adjusting the impedance parameters in real-time through a
impedance tuning unit. To validate the framework we present a set of
experiments both in laboratory settings with 12 healthy blindfolded subjects
and a proof-of-concept demonstration in a real-world scenario.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14707" title="Abstract">arXiv:2310.14707</a> [<a href="/pdf/2310.14707" title="Download PDF">pdf</a>, <a href="/format/2310.14707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid GNN approach for predicting node data for 3D meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salimath%2C+S">Shwetha Salimath</a>, 
<a href="/search/cs?searchtype=author&query=Bugiotti%2C+F">Francesca Bugiotti</a>, 
<a href="/search/cs?searchtype=author&query=Magoules%2C+F">Frederic Magoules</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Metal forging is used to manufacture dies. We require the best set of input
parameters for the process to be efficient. Currently, we predict the best
parameters using the finite element method by generating simulations for the
different initial conditions, which is a time-consuming process. In this paper,
introduce a hybrid approach that helps in processing and generating new data
simulations using a surrogate graph neural network model based on graph
convolutions, having a cheaper time cost. We also introduce a hybrid approach
that helps in processing and generating new data simulations using the model.
Given a dataset representing meshes, our focus is on the conversion of the
available information into a graph or point cloud structure. This new
representation enables deep learning. The predicted result is similar, with a
low error when compared to that produced using the finite element method. The
new models have outperformed existing PointNet and simple graph neural network
models when applied to produce the simulations.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14708" title="Abstract">arXiv:2310.14708</a> [<a href="/pdf/2310.14708" title="Download PDF">pdf</a>, <a href="/format/2310.14708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong and Efficient Baselines for Open Domain Conversational Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coman%2C+A+C">Andrei C. Coman</a>, 
<a href="/search/cs?searchtype=author&query=Barlacchi%2C+G">Gianni Barlacchi</a>, 
<a href="/search/cs?searchtype=author&query=de+Gispert%2C+A">Adri&#xe0; de Gispert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Unlike the Open Domain Question Answering (ODQA) setting, the conversational
(ODConvQA) domain has received limited attention when it comes to reevaluating
baselines for both efficiency and effectiveness. In this paper, we study the
State-of-the-Art (SotA) Dense Passage Retrieval (DPR) retriever and
Fusion-in-Decoder (FiD) reader pipeline, and show that it significantly
underperforms when applied to ODConvQA tasks due to various limitations. We
then propose and evaluate strong yet simple and efficient baselines, by
introducing a fast reranking component between the retriever and the reader,
and by performing targeted finetuning steps. Experiments on two ODConvQA tasks,
namely TopiOCQA and OR-QuAC, show that our method improves the SotA results,
while reducing reader's latency by 60%. Finally, we provide new and valuable
insights into the development of challenging baselines that serve as a
reference for future, more intricate approaches, including those that leverage
Large Language Models (LLMs).
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14709" title="Abstract">arXiv:2310.14709</a> [<a href="/pdf/2310.14709" title="Download PDF">pdf</a>, <a href="/format/2310.14709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Once Upon a $\textit{Time}$ in $\textit{Graph}$: Relative-Time  Pretraining for Complex Temporal Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Our physical world is constantly evolving over time, rendering challenges for
pre-trained language models to understand and reason over the temporal contexts
of texts. Existing work focuses on strengthening the direct association between
a piece of text and its time-stamp. However, the knowledge-time association is
usually insufficient for the downstream tasks that require reasoning over
temporal dependencies between knowledge. In this work, we make use of the
underlying nature of time, all temporally-scoped sentences are strung together
through a one-dimensional time axis, and suggest creating a graph structure
based on the relative placements of events along the time axis. Inspired by the
graph view, we propose RemeMo ($\underline{Re}$lative Ti$\underline{me}$
$\underline{Mo}$deling), which explicitly connects all temporally-scoped facts
by modeling the time relations between any two sentences. Experimental results
show that RemeMo outperforms the baseline T5 on multiple temporal question
answering datasets under various settings. Further analysis suggests that
RemeMo is especially good at modeling long-range complex temporal dependencies.
We release our code and pre-trained checkpoints at
$\href{https://github.com/DAMO-NLP-SG/RemeMo}{\text{this url}}$.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14712" title="Abstract">arXiv:2310.14712</a> [<a href="/pdf/2310.14712" title="Download PDF">pdf</a>, <a href="/format/2310.14712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit-Explicit Time Integration for the Immersed Wave Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fa%C3%9Fbender%2C+C">Christian Fa&#xdf;bender</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCrchner%2C+T">Tim B&#xfc;rchner</a>, 
<a href="/search/cs?searchtype=author&query=Kopp%2C+P">Philipp Kopp</a>, 
<a href="/search/cs?searchtype=author&query=Rank%2C+E">Ernst Rank</a>, 
<a href="/search/cs?searchtype=author&query=Kollmannsberger%2C+S">Stefan Kollmannsberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Immersed boundary methods simplify mesh generation by embedding the domain of
interest into an extended domain that is easy to mesh, introducing the
challenge of dealing with cells that intersect the domain boundary. Combined
with explicit time integration schemes, the finite cell method introduces a
lower bound for the critical time step size. Explicit transient analyses
commonly use the spectral element method due to its natural way of obtaining
diagonal mass matrices through nodal lumping. Its combination with the finite
cell method is called the spectral cell method. Unfortunately, a direct
application of nodal lumping in the spectral cell method is impossible due to
the special quadrature necessary to treat the discontinuous integrand inside
the cut cells. We analyze an implicit-explicit (IMEX) time integration method
to exploit the advantages of the nodal lumping scheme for uncut cells on one
side and the unconditional stability of implicit time integration schemes for
cut cells on the other. In this hybrid, immersed Newmark IMEX approach, we use
explicit second-order central differences to integrate the uncut degrees of
freedom that lead to a diagonal block in the mass matrix and an implicit
trapezoidal Newmark method to integrate the remaining degrees of freedom (those
supported by at least one cut cell). The immersed Newmark IMEX approach
preserves the high-order convergence rates and the geometric flexibility of the
finite cell method. We analyze a simple system of spring-coupled masses to
highlight some of the essential characteristics of Newmark IMEX time
integration. We then solve the scalar wave equation on two- and
three-dimensional examples with significant geometric complexity to show that
our approach is more efficient than state-of-the-art time integration schemes
when comparing accuracy and runtime.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14713" title="Abstract">arXiv:2310.14713</a> [<a href="/pdf/2310.14713" title="Download PDF">pdf</a>, <a href="/format/2310.14713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A self-adaptive genetic algorithm for the flying sidekick travelling  salesman problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pilcher%2C+T">Ted Pilcher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">This paper presents a novel approach to solving the Flying Sidekick
Travelling Salesman Problem (FSTSP) using a state-of-the-art self-adaptive
genetic algorithm. The Flying Sidekick Travelling Salesman Problem is a
combinatorial optimisation problem that extends the Travelling Salesman Problem
(TSP) by introducing the use of drones. In FSTSP, the objective is to minimise
the total time to visit all locations while strategically deploying a drone to
serve hard-to-reach customer locations. Also, to the best of my knowledge, this
is the first time a self-adaptive genetic algorithm (GA) has been used to solve
the FSTSP problem. Experimental results on smaller-sized problem instances
demonstrate that this algorithm can find a higher quantity of optimal solutions
and a lower percentage gap to the optimal solution compared to rival
algorithms. Moreover, on larger-sized problem instances, this algorithm
outperforms all rival algorithms on each problem size while maintaining a
reasonably low computation time.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14714" title="Abstract">arXiv:2310.14714</a> [<a href="/pdf/2310.14714" title="Download PDF">pdf</a>, <a href="/format/2310.14714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BatteryML:An Open-source platform for Machine Learning on Battery  Degradation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+X">Xiaofan Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Ziheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Battery degradation remains a pivotal concern in the energy storage domain,
with machine learning emerging as a potent tool to drive forward insights and
solutions. However, this intersection of electrochemical science and machine
learning poses complex challenges. Machine learning experts often grapple with
the intricacies of battery science, while battery researchers face hurdles in
adapting intricate models tailored to specific datasets. Beyond this, a
cohesive standard for battery degradation modeling, inclusive of data formats
and evaluative benchmarks, is conspicuously absent. Recognizing these
impediments, we present BatteryML - a one-step, all-encompass, and open-source
platform designed to unify data preprocessing, feature extraction, and the
implementation of both traditional and state-of-the-art models. This
streamlined approach promises to enhance the practicality and efficiency of
research applications. BatteryML seeks to fill this void, fostering an
environment where experts from diverse specializations can collaboratively
contribute, thus elevating the collective understanding and advancement of
battery research.The code for our project is publicly available on GitHub at
https://github.com/microsoft/BatteryML.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14718" title="Abstract">arXiv:2310.14718</a> [<a href="/pdf/2310.14718" title="Download PDF">pdf</a>, <a href="/format/2310.14718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Scale Imbalance in Semi-supervised Object Detection for  Aerial Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruixiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wen Yang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guangjun He</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Huai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gui-Song Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper focuses on the scale imbalance problem of semi-supervised object
detection(SSOD) in aerial images. Compared to natural images, objects in aerial
images show smaller sizes and larger quantities per image, increasing the
difficulty of manual annotation. Meanwhile, the advanced SSOD technique can
train superior detectors by leveraging limited labeled data and massive
unlabeled data, saving annotation costs. However, as an understudied task in
aerial images, SSOD suffers from a drastic performance drop when facing a large
proportion of small objects. By analyzing the predictions between small and
large objects, we identify three imbalance issues caused by the scale bias,
i.e., pseudo-label imbalance, label assignment imbalance, and negative learning
imbalance. To tackle these issues, we propose a novel Scale-discriminative
Semi-Supervised Object Detection (S^3OD) learning pipeline for aerial images.
In our S^3OD, three key components, Size-aware Adaptive Thresholding (SAT),
Size-rebalanced Label Assignment (SLA), and Teacher-guided Negative Learning
(TNL), are proposed to warrant scale unbiased learning. Specifically, SAT
adaptively selects appropriate thresholds to filter pseudo-labels for objects
at different scales. SLA balances positive samples of objects at different
scales through resampling and reweighting. TNL alleviates the imbalance in
negative samples by leveraging information generated by a teacher model.
Extensive experiments conducted on the DOTA-v1.5 benchmark demonstrate the
superiority of our proposed methods over state-of-the-art competitors. Codes
will be released soon.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14719" title="Abstract">arXiv:2310.14719</a> [<a href="/pdf/2310.14719" title="Download PDF">pdf</a>, <a href="/ps/2310.14719" title="Download PostScript">ps</a>, <a href="/format/2310.14719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Reusable Machine-Calculus for Automated Resource Analyses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suzanne%2C+H">Hector Suzanne</a> (APR), 
<a href="/search/cs?searchtype=author&query=Chailloux%2C+E">Emmanuel Chailloux</a> (APR)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Logic-Based Program Synthesis and Transformation, Oct 2023,
  Cascais, Portugal. pp.61-79
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">An automated resource analysis technique is introduced, targeting a
Call-By-Push-Value abstract machine, with memory prediction as a practical
goal. The machine has a polymorphic and linear type system enhanced with a
first-order logical fragment, which encodes both low-level operational
semantics of resource manipulations and high-level synthesis of algorithmic
complexity. Resource analysis must involve a diversity of static analysis, for
escape, aliasing, algorithmic invariants, and more. Knowing this, we implement
the Automated Amortized Resource Analysis framework (AARA) from scratch in our
generic system. In this setting, access to resources is a state-passing effect
which produces a compile-time approximation of runtime resource usage. We
implemented type inference constraint generation for our calculus, accompanied
with an elaboration of bounds for iterators on algebraic datatypes, for minimal
ML-style programming languages with Call-by-Value and Call-By-Push-Value
semantics. The closed-formed bounds are derived as multivariate polynomials
over the integers. This now serves as a base for the development of an
experimental toolkit for automated memory analysis of functional languages.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14720" title="Abstract">arXiv:2310.14720</a> [<a href="/pdf/2310.14720" title="Download PDF">pdf</a>, <a href="/format/2310.14720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extended Deep Adaptive Input Normalization for Preprocessing Time Series  Data for Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=September%2C+M+A+K">Marcus A. K. September</a>, 
<a href="/search/cs?searchtype=author&query=Passino%2C+F+S">Francesco Sanna Passino</a>, 
<a href="/search/cs?searchtype=author&query=Goldmann%2C+L">Leonie Goldmann</a>, 
<a href="/search/cs?searchtype=author&query=Hinel%2C+A">Anton Hinel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Data preprocessing is a crucial part of any machine learning pipeline, and it
can have a significant impact on both performance and training efficiency. This
is especially evident when using deep neural networks for time series
prediction and classification: real-world time series data often exhibit
irregularities such as multi-modality, skewness and outliers, and the model
performance can degrade rapidly if these characteristics are not adequately
addressed. In this work, we propose the EDAIN (Extended Deep Adaptive Input
Normalization) layer, a novel adaptive neural layer that learns how to
appropriately normalize irregular time series data for a given task in an
end-to-end fashion, instead of using a fixed normalization scheme. This is
achieved by optimizing its unknown parameters simultaneously with the deep
neural network using back-propagation. Our experiments, conducted using
synthetic data, a credit default prediction dataset, and a large-scale limit
order book benchmark dataset, demonstrate the superior performance of the EDAIN
layer when compared to conventional normalization methods and existing adaptive
time series preprocessing layers.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14724" title="Abstract">arXiv:2310.14724</a> [<a href="/pdf/2310.14724" title="Download PDF">pdf</a>, <a href="/format/2310.14724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on LLM-gernerated Text Detection: Necessity, Methods, and  Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junchao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+R">Runzhe Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yulin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D+F">Derek F. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+L+S">Lidia S. Chao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The powerful ability to understand, follow, and generate complex language
emerging from large language models (LLMs) makes LLM-generated text flood many
areas of our daily lives at an incredible speed and is widely accepted by
humans. As LLMs continue to expand, there is an imperative need to develop
detectors that can detect LLM-generated text. This is crucial to mitigate
potential misuse of LLMs and safeguard realms like artistic expression and
social networks from harmful influence of LLM-generated content. The
LLM-generated text detection aims to discern if a piece of text was produced by
an LLM, which is essentially a binary classification task. The detector
techniques have witnessed notable advancements recently, propelled by
innovations in watermarking techniques, zero-shot methods, fine-turning LMs
methods, adversarial learning methods, LLMs as detectors, and human-assisted
methods. In this survey, we collate recent research breakthroughs in this area
and underscore the pressing need to bolster detector research. We also delve
into prevalent datasets, elucidating their limitations and developmental
requirements. Furthermore, we analyze various LLM-generated text detection
paradigms, shedding light on challenges like out-of-distribution problems,
potential attacks, and data ambiguity. Conclusively, we highlight interesting
directions for future research in LLM-generated text detection to advance the
implementation of responsible artificial intelligence (AI). Our aim with this
survey is to provide a clear and comprehensive introduction for newcomers while
also offering seasoned researchers a valuable update in the field of
LLM-generated text detection.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14725" title="Abstract">arXiv:2310.14725</a> [<a href="/pdf/2310.14725" title="Download PDF">pdf</a>, <a href="/ps/2310.14725" title="Download PostScript">ps</a>, <a href="/format/2310.14725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Learning Polynomial Recursive Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buna-Marginean%2C+A">Alex Buna-Marginean</a>, 
<a href="/search/cs?searchtype=author&query=Cheval%2C+V">Vincent Cheval</a>, 
<a href="/search/cs?searchtype=author&query=Shirmohammadi%2C+M">Mahsa Shirmohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Worrell%2C+J">James Worrell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL); Programming Languages (cs.PL)

</div>
<p class="mathjax">We introduce the class of P-finite automata. These are a generalisation of
weighted automata, in which the weights of transitions can depend polynomially
on the length of the input word. P-finite automata can also be viewed as simple
tail-recursive programs in which the arguments of recursive calls can
non-linearly refer to a variable that counts the number of recursive calls. The
nomenclature is motivated by the fact that over a unary alphabet P-finite
automata compute so-called P-finite sequences, that is, sequences that satisfy
a linear recurrence with polynomial coefficients. Our main result shows that
P-finite automata can be learned in polynomial time in Angluin's MAT exact
learning model. This generalises the classical results that deterministic
finite automata and weighted automata over a field are respectively
polynomial-time learnable in the MAT model.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14729" title="Abstract">arXiv:2310.14729</a> [<a href="/pdf/2310.14729" title="Download PDF">pdf</a>, <a href="/format/2310.14729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D  diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapon%2C+R">Roy Kapon</a>, 
<a href="/search/cs?searchtype=author&query=Tevet%2C+G">Guy Tevet</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Or%2C+D">Daniel Cohen-Or</a>, 
<a href="/search/cs?searchtype=author&query=Bermano%2C+A+H">Amit H. Bermano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We introduce Multi-view Ancestral Sampling (MAS), a method for generating
consistent multi-view 2D samples of a motion sequence, enabling the creation of
its 3D counterpart. MAS leverages a diffusion model trained solely on 2D data,
opening opportunities to exciting and diverse fields of motion previously
under-explored as 3D data is scarce and hard to collect. MAS works by
simultaneously denoising multiple 2D motion sequences representing the same
motion from different angles. Our consistency block ensures consistency across
all views at each diffusion step by combining the individual generations into a
unified 3D sequence, and projecting it back to the original views for the next
iteration. We demonstrate MAS on 2D pose data acquired from videos depicting
professional basketball maneuvers, rhythmic gymnastic performances featuring a
ball apparatus, and horse obstacle course races. In each of these domains, 3D
motion capture is arduous, and yet, MAS generates diverse and realistic 3D
sequences without textual conditioning. As we demonstrate, our ancestral
sampling-based approach offers a more natural integration with the diffusion
framework compared to popular denoising optimization-based approaches, and
avoids common issues such as out-of-domain sampling, lack of details and
mode-collapse. https://guytevet.github.io/mas-page/
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14732" title="Abstract">arXiv:2310.14732</a> [<a href="/pdf/2310.14732" title="Download PDF">pdf</a>, <a href="/format/2310.14732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Prototypes for Contradiction Detection Using Large Language  Models and Linguistic Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pielka%2C+M">Maren Pielka</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+S">Svetlana Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Sifa%2C+R">Rafet Sifa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a novel data generation method for contradiction detection,
which leverages the generative power of large language models as well as
linguistic rules. Our vision is to provide a condensed corpus of prototypical
contradictions, allowing for in-depth linguistic analysis as well as efficient
language model fine-tuning. To this end, we instruct the generative models to
create contradicting statements with respect to descriptions of specific
contradiction types. In addition, the model is also instructed to come up with
completely new contradiction typologies. As an auxiliary approach, we use
linguistic rules to construct simple contradictions such as those arising from
negation, antonymy and numeric mismatch. We find that our methods yield
promising results in terms of coherence and variety of the data. Further
studies, as well as manual refinement are necessary to make use of this data in
a machine learning setup.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14735" title="Abstract">arXiv:2310.14735</a> [<a href="/pdf/2310.14735" title="Download PDF">pdf</a>, <a href="/format/2310.14735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the potential of prompt engineering in Large Language Models:  a comprehensive review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Banghao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Langren%C3%A9%2C+N">Nicolas Langren&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shengxin Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper delves into the pivotal role of prompt engineering in unleashing
the capabilities of Large Language Models (LLMs). Prompt engineering is the
process of structuring input text for LLMs and is a technique integral to
optimizing the efficacy of LLMs. This survey elucidates foundational principles
of prompt engineering, such as role-prompting, one-shot, and few-shot
prompting, as well as more advanced methodologies such as the chain-of-thought
and tree-of-thoughts prompting. The paper sheds light on how external
assistance in the form of plugins can assist in this task, and reduce machine
hallucination by retrieving external knowledge. We subsequently delineate
prospective directions in prompt engineering research, emphasizing the need for
a deeper understanding of structures and the role of agents in Artificial
Intelligence-Generated Content (AIGC) tools. We discuss how to assess the
efficacy of prompt methods from different perspectives and using different
methods. Finally, we gather information about the application of prompt
engineering in such fields as education and programming, showing its
transformative potential. This comprehensive survey aims to serve as a friendly
guide for anyone venturing through the big world of LLMs and prompt
engineering.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14736" title="Abstract">arXiv:2310.14736</a> [<a href="/pdf/2310.14736" title="Download PDF">pdf</a>, <a href="/format/2310.14736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAMCLR: Contrastive pre-training on complex scenes using SAM for view  sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Missaoui%2C+B">Benjamin Missaoui</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chongbin Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In Computer Vision, self-supervised contrastive learning enforces similar
representations between different views of the same image. The pre-training is
most often performed on image classification datasets, like ImageNet, where
images mainly contain a single class of objects. However, when dealing with
complex scenes with multiple items, it becomes very unlikely for several views
of the same image to represent the same object category. In this setting, we
propose SAMCLR, an add-on to SimCLR which uses SAM to segment the image into
semantic regions, then sample the two views from the same region. Preliminary
results show empirically that when pre-training on Cityscapes and ADE20K, then
evaluating on classification on CIFAR-10, STL10 and ImageNette, SAMCLR performs
at least on par with, and most often significantly outperforms not only SimCLR,
but also DINO and MoCo.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14741" title="Abstract">arXiv:2310.14741</a> [<a href="/pdf/2310.14741" title="Download PDF">pdf</a>, <a href="/format/2310.14741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive CPU Resource Allocation for Emulator in Kernel-based Virtual  Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yecheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+P">Pu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiawen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Quan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Minyi Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">The technologies of heterogeneous multi-core architectures, co-location, and
virtualization can be used to reduce server power consumption and improve
system utilization, which are three important technologies for data centers.
This article explores the scheduling strategy of Emulator threads within
virtual machine processes in a scenario of co-location of multiple virtual
machines on heterogeneous multi-core architectures. In this co-location
scenario, the scheduling strategy for Emulator threads significantly affects
the performance of virtual machines. This article focuses on this thread for
the first time in the relevant field. This article found that the scheduling
latency metric can well indicate the running status of the vCPU threads and
Emulator threads in the virtualization environment, and applied this metric to
the design of the scheduling strategy. This article designed an Emulator thread
scheduler based on heuristic rules, which, in coordination with the host
operating system's scheduler, dynamically adjusts the scheduling scope of
Emulator threads to improve the overall performance of virtual machines. The
article found that in real application scenarios, the scheduler effectively
improved the performance of applications within virtual machines, with a
maximum performance improvement of 40.7%.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14743" title="Abstract">arXiv:2310.14743</a> [<a href="/pdf/2310.14743" title="Download PDF">pdf</a>, <a href="/format/2310.14743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Safety Challenges of Deep Learning in Real-World Type 1 Diabetes  Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emerson%2C+H">Harry Emerson</a>, 
<a href="/search/cs?searchtype=author&query=McConville%2C+R">Ryan McConville</a>, 
<a href="/search/cs?searchtype=author&query=Guy%2C+M">Matthew Guy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Blood glucose simulation allows the effectiveness of type 1 diabetes (T1D)
management strategies to be evaluated without patient harm. Deep learning
algorithms provide a promising avenue for extending simulator capabilities;
however, these algorithms are limited in that they do not necessarily learn
physiologically correct glucose dynamics and can learn incorrect and
potentially dangerous relationships from confounders in training data. This is
likely to be more important in real-world scenarios, as data is not collected
under strict research protocol. This work explores the implications of using
deep learning algorithms trained on real-world data to model glucose dynamics.
Free-living data was processed from the OpenAPS Data Commons and supplemented
with patient-reported tags of challenging diabetes events, constituting one of
the most detailed real-world T1D datasets. This dataset was used to train and
evaluate state-of-the-art glucose simulators, comparing their prediction error
across safety critical scenarios and assessing the physiological
appropriateness of the learned dynamics using Shapley Additive Explanations
(SHAP). While deep learning prediction accuracy surpassed the widely-used
mathematical simulator approach, the model deteriorated in safety critical
scenarios and struggled to leverage self-reported meal and exercise
information. SHAP value analysis also indicated the model had fundamentally
confused the roles of insulin and carbohydrates, which is one of the most basic
T1D management principles. This work highlights the importance of considering
physiological appropriateness when using deep learning to model real-world
systems in T1D and healthcare more broadly, and provides recommendations for
building models that are robust to real-world data constraints.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14744" title="Abstract">arXiv:2310.14744</a> [<a href="/pdf/2310.14744" title="Download PDF">pdf</a>, <a href="/format/2310.14744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-timescale and Chance-Constrained Energy Dispatching Strategy of  Integrated Heat-Power Community with Shared Hybrid Energy Storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wenyi Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yue Chen</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+R">Rui Xie</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yunjian Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The community in the future may develop into an integrated heat-power system,
which includes a high proportion of renewable energy, power generator units,
heat generator units, and shared hybrid energy storage. In the integrated
heat-power system with coupling heat-power generators and demands, the key
challenges lie in the interaction between heat and power, the inherent
uncertainty of renewable energy and consumers' demands, and the multi-timescale
scheduling of heat and power. In this paper, we propose a game theoretic model
of the integrated heat-power system. For the welfare-maximizing community
operator, its energy dispatch strategy is under chance constraints, where the
day-ahead scheduling determines the scheduled energy dispatching strategies,
and the real-time dispatch considers the adjustment of generators. For
utility-maximizing consumers, their demands are sensitive to the preference
parameters. Taking into account the uncertainty in both renewable energy and
consumer demand, we prove the existence and uniqueness of the Stackelberg game
equilibrium and develop a fixed point algorithm to find the market equilibrium
between the community operator and community consumers. Numerical simulations
on integrated heat-power system validate the effectiveness of the proposed
multi-timescale integrated heat and power model.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14745" title="Abstract">arXiv:2310.14745</a> [<a href="/pdf/2310.14745" title="Download PDF">pdf</a>, <a href="/ps/2310.14745" title="Download PostScript">ps</a>, <a href="/format/2310.14745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Domain Channel Estimation for Extremely Large MIMO THz  Communications with Beam Squint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vlachos%2C+E">Evangelos Vlachos</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+A">Aryan Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Eldar%2C+Y+C">Yonina C. Eldar</a>, 
<a href="/search/cs?searchtype=author&query=Alexandropoulos%2C+G+C">George C. Alexandropoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we study the problem of extremely large (XL) multiple-input
multiple-output (MIMO) channel estimation in the Terahertz (THz) frequency
band, considering the presence of propagation delays across the entire array
apertures, which leads to frequency selectivity, a problem known as beam
squint. Multi-carrier transmission schemes which are usually deployed to
address this problem, suffer from high peak-to-average power ratio, which is
specifically dominant in THz communications where low transmit power is
realized. Diverging from the usual approach, we devise a novel channel
estimation problem formulation in the time domain for single-carrier (SC)
modulation, which favors transmissions in THz, and incorporate the beam-squint
effect in a sparse vector recovery problem that is solved via sparse
optimization tools. In particular, the beam squint and the sparse MIMO channel
are jointly tracked by using an alternating minimization approach that
decomposes the two estimation problems. The presented performance evaluation
results validate that the proposed SC technique exhibits superior performance
than the conventional one as well as than state-of-the-art multi-carrier
approaches.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14746" title="Abstract">arXiv:2310.14746</a> [<a href="/pdf/2310.14746" title="Download PDF">pdf</a>, <a href="/format/2310.14746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homogenized lattice Boltzmann methods for fluid flow through porous  media -- part I: kinetic model derivation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Simonis%2C+S">Stephan Simonis</a>, 
<a href="/search/math?searchtype=author&query=Hafen%2C+N">Nicolas Hafen</a>, 
<a href="/search/math?searchtype=author&query=Je%C3%9Fberger%2C+J">Julius Je&#xdf;berger</a>, 
<a href="/search/math?searchtype=author&query=Dapelo%2C+D">Davide Dapelo</a>, 
<a href="/search/math?searchtype=author&query=Th%C3%A4ter%2C+G">Gudrun Th&#xe4;ter</a>, 
<a href="/search/math?searchtype=author&query=Krause%2C+M+J">Mathias J. Krause</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">In this series of works we establish homogenized lattice Boltzmann methods
(HLBM) for the simulation of fluid flow through porous media. Our contributions
in part I are twofold. First, we assemble the targeted partial differential
equation system by formally unifying the governing equations for nonstationary
fluid flow in porous media. To this end, a matrix of regularly arranged
obstacles of equal size is placed into the domain to model fluid flow through
structures of different porosities that is governed by the incompressible
nonstationary Navier--Stokes equations. Depending on the ratio of geometric
parameters in the matrix arrangement, several cases of homogenized equations
are obtained. We review the existing methods to homogenize the nonstationary
Navier--Stokes equations for specific porosities and interpret connections
between the resulting model equations from the perspective of applicability.
Consequently, the homogenized Navier--Stokes equations are formulated as
targeted partial differential equations which jointly incorporate the derived
aspects. Second, we propose a kinetic model, named homogenized
Bhatnagar--Gross--Krook Boltzmann equation, which approximates the homogenized
nonstationary Navier--Stokes equations. We formally prove that the zeroth and
first order moments of the kinetic model provide solutions to the mass and
momentum balance variables of the macrocopic model up to specific orders in the
scaling parameter. Based on the present contributions, in the sequel (part II)
the homogenized Navier--Stokes equations are consistently approximated by
deriving a limit consistent HLBM discretization of the homogenized
Bhatnagar--Gross--Krook Boltzmann equation.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14747" title="Abstract">arXiv:2310.14747</a> [<a href="/pdf/2310.14747" title="Download PDF">pdf</a>, <a href="/format/2310.14747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCC-KD: Multi-CoT Consistent Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongzhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ENMLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have showcased remarkable capabilities in
complex reasoning through chain of thought (CoT) prompting.~Recently, there has
been a growing interest in transferring these reasoning abilities from LLMs to
smaller models.~However, achieving both the diversity and consistency in
rationales presents a challenge.~In this paper, we focus on enhancing these two
aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to
efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple
rationales for each question and enforce consistency among the corresponding
predictions by minimizing the bidirectional KL-divergence between the answer
distributions.~We investigate the effectiveness of MCC-KD with different model
architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both
mathematical reasoning and commonsense reasoning benchmarks. The empirical
results not only confirm MCC-KD's superior performance on in-distribution
datasets but also highlight its robust generalization ability on
out-of-distribution datasets.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14749" title="Abstract">arXiv:2310.14749</a> [<a href="/pdf/2310.14749" title="Download PDF">pdf</a>, <a href="/format/2310.14749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring hierarchical framework of nonlinear sparse Bayesian learning  algorithm through numerical investigations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dabiran%2C+N">Nastaran Dabiran</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+B">Brandon Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Sandhu%2C+R">Rimple Sandhu</a>, 
<a href="/search/cs?searchtype=author&query=Khalil%2C+M">Mohammad Khalil</a>, 
<a href="/search/cs?searchtype=author&query=Pettit%2C+C+L">Chris L. Pettit</a>, 
<a href="/search/cs?searchtype=author&query=Poirel%2C+D">Dominique Poirel</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Abhijit Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Sparse Bayesian learning (SBL) has been extensively utilized in data-driven
modeling to combat the issue of overfitting. While SBL excels in
linear-in-parameter models, its direct applicability is limited in models where
observations possess nonlinear relationships with unknown parameters. Recently,
a semi-analytical Bayesian framework known as nonlinear sparse Bayesian
learning (NSBL) was introduced by the authors to induce sparsity among model
parameters during the Bayesian inversion of nonlinear-in-parameter models. NSBL
relies on optimally selecting the hyperparameters of sparsity-inducing Gaussian
priors. It is inherently an approximate method since the uncertainty in the
hyperparameter posterior is disregarded as we instead seek the maximum a
posteriori (MAP) estimate of the hyperparameters (type-II MAP estimate). This
paper aims to investigate the hierarchical structure that forms the basis of
NSBL and validate its accuracy through a comparison with a one-level
hierarchical Bayesian inference as a benchmark in the context of three
numerical experiments: (i) a benchmark linear regression example with Gaussian
prior and Gaussian likelihood, (ii) the same regression problem with a highly
non-Gaussian prior, and (iii) an example of a dynamical system with a
non-Gaussian prior and a highly non-Gaussian likelihood function, to explore
the performance of the algorithm in these new settings. Through these numerical
examples, it can be shown that NSBL is well-suited for physics-based models as
it can be readily applied to models with non-Gaussian prior distributions and
non-Gaussian likelihood functions. Moreover, we illustrate the accuracy of the
NSBL algorithm as an approximation to the one-level hierarchical Bayesian
inference and its ability to reduce the computational cost while adequately
exploring the parameter posteriors.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14751" title="Abstract">arXiv:2310.14751</a> [<a href="/pdf/2310.14751" title="Download PDF">pdf</a>, <a href="/format/2310.14751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Interpretable Bandit Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhojyoti Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Ruihao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kveton%2C+B">Branislav Kveton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Motivated by the importance of explainability in modern machine learning, we
design bandit algorithms that are \emph{efficient} and \emph{interpretable}. A
bandit algorithm is interpretable if it explores with the objective of reducing
uncertainty in the unknown model parameter. To quantify the interpretability,
we introduce a novel metric of \textit{uncertainty loss}, which compares the
rate of the uncertainty reduction to the theoretical optimum. We propose CODE,
a bandit algorithm based on a \textbf{C}onstrained \textbf{O}ptimal
\textbf{DE}sign, that is interpretable and maximally reduces the uncertainty.
The key idea in \code is to explore among all plausible actions, determined by
a statistical constraint, to achieve interpretability. We implement CODE
efficiently in both multi-armed and linear bandits and derive near-optimal
regret bounds by leveraging the optimality criteria of the approximate optimal
design. CODE can be also viewed as removing phases in conventional phased
elimination, which makes it more practical and general. We demonstrate the
advantage of \code by numerical experiments on both synthetic and real-world
problems. CODE outperforms other state-of-the-art interpretable designs while
matching the performance of popular but uninterpretable designs, such as upper
confidence bound algorithms.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14753" title="Abstract">arXiv:2310.14753</a> [<a href="/pdf/2310.14753" title="Download PDF">pdf</a>, <a href="/format/2310.14753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yaorui Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">An Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Enzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Masked graph modeling excels in the self-supervised representation learning
of molecular graphs. Scrutinizing previous studies, we can reveal a common
scheme consisting of three key components: (1) graph tokenizer, which breaks a
molecular graph into smaller fragments (i.e., subgraphs) and converts them into
tokens; (2) graph masking, which corrupts the graph with masks; (3) graph
autoencoder, which first applies an encoder on the masked graph to generate the
representations, and then employs a decoder on the representations to recover
the tokens of the original graph. However, the previous MGM studies focus
extensively on graph masking and encoder, while there is limited understanding
of tokenizer and decoder. To bridge the gap, we first summarize popular
molecule tokenizers at the granularity of node, edge, motif, and Graph Neural
Networks (GNNs), and then examine their roles as the MGM's reconstruction
targets. Further, we explore the potential of adopting an expressive decoder in
MGM. Our results show that a subgraph-level tokenizer and a sufficiently
expressive decoder with remask decoding have a large impact on the encoder's
representation learning. Finally, we propose a novel MGM method SimSGT,
featuring a Simple GNN-based Tokenizer (SGT) and an effective decoding
strategy. We empirically validate that our method outperforms the existing
molecule self-supervised learning methods. Our codes and checkpoints are
available at https://github.com/syr-cn/SimSGT.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14754" title="Abstract">arXiv:2310.14754</a> [<a href="/pdf/2310.14754" title="Download PDF">pdf</a>, <a href="/format/2310.14754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximately well-balanced Discontinuous Galerkin methods using bases  enriched with Physics-Informed Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Franck%2C+E">Emmanuel Franck</a>, 
<a href="/search/math?searchtype=author&query=Michel-Dansac%2C+V">Victor Michel-Dansac</a>, 
<a href="/search/math?searchtype=author&query=Navoret%2C+L">Laurent Navoret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work concerns the enrichment of Discontinuous Galerkin (DG) bases, so
that the resulting scheme provides a much better approximation of steady
solutions to hyperbolic systems of balance laws. The basis enrichment leverages
a prior -- an approximation of the steady solution -- which we propose to
compute using a Physics-Informed Neural Network (PINN). To that end, after
presenting the classical DG scheme, we show how to enrich its basis with a
prior. Convergence results and error estimates follow, in which we prove that
the basis with prior does not change the order of convergence, and that the
error constant is improved. To construct the prior, we elect to use parametric
PINNs, which we introduce, as well as the algorithms to construct a prior from
PINNs. We finally perform several validation experiments on four different
hyperbolic balance laws to highlight the properties of the scheme. Namely, we
show that the DG scheme with prior is much more accurate on steady solutions
than the DG scheme without prior, while retaining the same approximation
quality on unsteady solutions.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14757" title="Abstract">arXiv:2310.14757</a> [<a href="/pdf/2310.14757" title="Download PDF">pdf</a>, <a href="/format/2310.14757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for  Social Media NLP Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antypas%2C+D">Dimosthenis Antypas</a>, 
<a href="/search/cs?searchtype=author&query=Ushio%2C+A">Asahi Ushio</a>, 
<a href="/search/cs?searchtype=author&query=Barbieri%2C+F">Francesco Barbieri</a>, 
<a href="/search/cs?searchtype=author&query=Neves%2C+L">Leonardo Neves</a>, 
<a href="/search/cs?searchtype=author&query=Rezaee%2C+K">Kiamehr Rezaee</a>, 
<a href="/search/cs?searchtype=author&query=Espinosa-Anke%2C+L">Luis Espinosa-Anke</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+J">Jiaxin Pei</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite its relevance, the maturity of NLP for social media pales in
comparison with general-purpose models, metrics and benchmarks. This fragmented
landscape makes it hard for the community to know, for instance, given a task,
which is the best performing model and how it compares with others. To
alleviate this issue, we introduce a unified benchmark for NLP evaluation in
social media, SuperTweetEval, which includes a heterogeneous set of tasks and
datasets combined, adapted and constructed from scratch. We benchmarked the
performance of a wide range of models on SuperTweetEval and our results suggest
that, despite the recent advances in language modelling, social media remains
challenging.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14764" title="Abstract">arXiv:2310.14764</a> [<a href="/pdf/2310.14764" title="Download PDF">pdf</a>, <a href="/format/2310.14764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved K-mer Based Prediction of Protein-Protein Interactions With  Chaos Game Representation, Deep Learning and Reduced Representation Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veevers%2C+R">Ruth Veevers</a>, 
<a href="/search/cs?searchtype=author&query=MacLean%2C+D">Dan MacLean</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Protein-protein interactions drive many biological processes, including the
detection of phytopathogens by plants' R-Proteins and cell surface receptors.
Many machine learning studies have attempted to predict protein-protein
interactions but performance is highly dependent on training data; models have
been shown to accurately predict interactions when the proteins involved are
included in the training data, but achieve consistently poorer results when
applied to previously unseen proteins. In addition, models that are trained
using proteins that take part in multiple interactions can suffer from
representation bias, where predictions are driven not by learned biological
features but by learning of the structure of the interaction dataset.
<br />We present a method for extracting unique pairs from an interaction dataset,
generating non-redundant paired data for unbiased machine learning. After
applying the method to datasets containing _Arabidopsis thaliana_ and pathogen
effector interations, we developed a convolutional neural network model capable
of learning and predicting interactions from Chaos Game Representations of
proteins' coding genes.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14766" title="Abstract">arXiv:2310.14766</a> [<a href="/pdf/2310.14766" title="Download PDF">pdf</a>, <a href="/format/2310.14766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Learning of Behavioural Inputs for Autonomous Driving in  Dense Traffic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+J">Jatan Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Idoko%2C+S">Simon Idoko</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+B">Basant Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Arun Kumar Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IROS 2023. arXiv admin note: text overlap with <a href="/abs/2212.02224">arXiv:2212.02224</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Trajectory sampling in the Frenet(road-aligned) frame, is one of the most
popular methods for motion planning of autonomous vehicles. It operates by
sampling a set of behavioural inputs, such as lane offset and forward speed,
before solving a trajectory optimization problem conditioned on the sampled
inputs. The sampling is handcrafted based on simple heuristics, does not adapt
to driving scenarios, and is oblivious to the capabilities of downstream
trajectory planners. In this paper, we propose an end-to-end learning of
behavioural input distribution from expert demonstrations or in a
self-supervised manner. Our core novelty lies in embedding a custom
differentiable trajectory optimizer as a layer in neural networks, allowing us
to update behavioural inputs by considering the optimizer's feedback. Moreover,
our end-to-end approach also ensures that the learned behavioural inputs aid
the convergence of the optimizer. We improve the state-of-the-art in the
following aspects. First, we show that learned behavioural inputs substantially
decrease collision rate while improving driving efficiency over handcrafted
approaches. Second, our approach outperforms model predictive control methods
based on sampling-based optimization.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14767" title="Abstract">arXiv:2310.14767</a> [<a href="/pdf/2310.14767" title="Download PDF">pdf</a>, <a href="/format/2310.14767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting COVID-19 Infections Using Multi-layer Centrality Measures in  Population-scale Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Westernhagen%2C+C+H">Christine Hedde-von Westernhagen</a>, 
<a href="/search/cs?searchtype=author&query=Garcia-Bernardo%2C+J">Javier Garcia-Bernardo</a>, 
<a href="/search/cs?searchtype=author&query=Bagheri%2C+A">Ayoub Bagheri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph); Applications (stat.AP)

</div>
<p class="mathjax">Understanding the spread of SARS-CoV-2 has been one of the most pressing
problems of the recent past. Network models present a potent approach to
studying such spreading phenomena because of their ability to represent complex
social interactions. While previous studies have shown that network centrality
measures are generally able to identify influential spreaders in a susceptible
population, it is not yet known if they can also be used to predict infection
risks. However, information about infection risks at the individual level is
vital for the design of targeted interventions. Here, we use large-scale
administrative data from the Netherlands to study whether centrality measures
can predict the risk and timing of infections with COVID-19-like diseases. We
investigate this issue leveraging the framework of multi-layer networks, which
accounts for interactions taking place in different contexts, such as
workplaces, households and schools. In epidemic models simulated on real-world
network data from over one million individuals, we find that existing
centrality measures offer good predictions of relative infection risks, and are
correlated with the timing of individual infections. We however find no
association between centrality measures and real SARS-CoV-2 test data, which
indicates that population-scale network data alone cannot aid predictions of
virus transmission.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14768" title="Abstract">arXiv:2310.14768</a> [<a href="/pdf/2310.14768" title="Download PDF">pdf</a>, <a href="/format/2310.14768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Gradient with Kernel Quadrature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayakawa%2C+S">Satoshi Hayakawa</a>, 
<a href="/search/cs?searchtype=author&query=Morimura%2C+T">Tetsuro Morimura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reward evaluation of episodes becomes a bottleneck in a broad range of
reinforcement learning tasks. Our aim in this paper is to select a small but
representative subset of a large batch of episodes, only on which we actually
compute rewards for more efficient policy gradient iterations. We build a
Gaussian process modeling of discounted returns or rewards to derive a positive
definite kernel on the space of episodes, run an "episodic" kernel quadrature
method to compress the information of sample episodes, and pass the reduced
episodes to the policy network for gradient updates. We present the theoretical
background of this procedure as well as its numerical illustrations in MuJoCo
and causal discovery tasks.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14770" title="Abstract">arXiv:2310.14770</a> [<a href="/pdf/2310.14770" title="Download PDF">pdf</a>, <a href="/ps/2310.14770" title="Download PostScript">ps</a>, <a href="/format/2310.14770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretically Grounded Loss Functions and Algorithms for Score-Based  Multi-Class Abstention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+A">Anqi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Mohri%2C+M">Mehryar Mohri</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yutao Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Learning with abstention is a key scenario where the learner can abstain from
making a prediction at some cost. In this paper, we analyze the score-based
formulation of learning with abstention in the multi-class classification
setting. We introduce new families of surrogate losses for the abstention loss
function, which include the state-of-the-art surrogate losses in the
single-stage setting and a novel family of loss functions in the two-stage
setting. We prove strong non-asymptotic and hypothesis set-specific consistency
guarantees for these surrogate losses, which upper-bound the estimation error
of the abstention loss function in terms of the estimation error of the
surrogate loss. Our bounds can help compare different score-based surrogates
and guide the design of novel abstention algorithms by minimizing the proposed
surrogate losses. We experimentally evaluate our new algorithms on CIFAR-10,
CIFAR-100, and SVHN datasets and the practical significance of our new
surrogate losses and two-stage abstention algorithms. Our results also show
that the relative performance of the state-of-the-art score-based surrogate
losses can vary across datasets.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14771" title="Abstract">arXiv:2310.14771</a> [<a href="/pdf/2310.14771" title="Download PDF">pdf</a>, <a href="/format/2310.14771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Knowledge Base Completion Potential of GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veseli%2C+B">Blerta Veseli</a>, 
<a href="/search/cs?searchtype=author&query=Razniewski%2C+S">Simon Razniewski</a>, 
<a href="/search/cs?searchtype=author&query=Kalo%2C+J">Jan-Christoph Kalo</a>, 
<a href="/search/cs?searchtype=author&query=Weikum%2C+G">Gerhard Weikum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages 4 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Structured knowledge bases (KBs) are an asset for search engines and other
applications, but are inevitably incomplete. Language models (LMs) have been
proposed for unsupervised knowledge base completion (KBC), yet, their ability
to do this at scale and with high accuracy remains an open question. Prior
experimental studies mostly fall short because they only evaluate on popular
subjects, or sample already existing facts from KBs. In this work, we perform a
careful evaluation of GPT's potential to complete the largest public KB:
Wikidata. We find that, despite their size and capabilities, models like GPT-3,
ChatGPT and GPT-4 do not achieve fully convincing results on this task.
Nonetheless, they provide solid improvements over earlier approaches with
smaller LMs. In particular, we show that, with proper thresholding, GPT-3
enables to extend Wikidata by 27M facts at 90% precision.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14772" title="Abstract">arXiv:2310.14772</a> [<a href="/pdf/2310.14772" title="Download PDF">pdf</a>, <a href="/format/2310.14772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+A">Anqi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Mohri%2C+M">Mehryar Mohri</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yutao Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the key framework of learning with abstention in the multi-class
classification setting. In this setting, the learner can choose to abstain from
making a prediction with some pre-defined cost. We present a series of new
theoretical and algorithmic results for this learning problem in the
predictor-rejector framework. We introduce several new families of surrogate
losses for which we prove strong non-asymptotic and hypothesis set-specific
consistency guarantees, thereby resolving positively two existing open
questions. These guarantees provide upper bounds on the estimation error of the
abstention loss function in terms of that of the surrogate loss. We analyze
both a single-stage setting where the predictor and rejector are learned
simultaneously and a two-stage setting crucial in applications, where the
predictor is learned in a first stage using a standard surrogate loss such as
cross-entropy. These guarantees suggest new multi-class abstention algorithms
based on minimizing these surrogate losses. We also report the results of
extensive experiments comparing these algorithms to the current
state-of-the-art algorithms on CIFAR-10, CIFAR-100 and SVHN datasets. Our
results demonstrate empirically the benefit of our new surrogate losses and
show the remarkable performance of our broadly applicable two-stage abstention
algorithm.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14774" title="Abstract">arXiv:2310.14774</a> [<a href="/pdf/2310.14774" title="Download PDF">pdf</a>, <a href="/ps/2310.14774" title="Download PostScript">ps</a>, <a href="/format/2310.14774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principled Approaches for Learning to Defer with Multiple Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+A">Anqi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Mohri%2C+M">Mehryar Mohri</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yutao Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a study of surrogate losses and algorithms for the general problem
of learning to defer with multiple experts. We first introduce a new family of
surrogate losses specifically tailored for the multiple-expert setting, where
the prediction and deferral functions are learned simultaneously. We then prove
that these surrogate losses benefit from strong $H$-consistency bounds. We
illustrate the application of our analysis through several examples of
practical surrogate losses, for which we give explicit guarantees. These loss
functions readily lead to the design of new learning to defer algorithms based
on their minimization. While the main focus of this work is a theoretical
analysis, we also report the results of several experiments on SVHN and
CIFAR-10 datasets.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14777" title="Abstract">arXiv:2310.14777</a> [<a href="/pdf/2310.14777" title="Download PDF">pdf</a>, <a href="/format/2310.14777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geographical Erasure in Language Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schw%C3%B6bel%2C+P">Pola Schw&#xf6;bel</a>, 
<a href="/search/cs?searchtype=author&query=Golebiowski%2C+J">Jacek Golebiowski</a>, 
<a href="/search/cs?searchtype=author&query=Donini%2C+M">Michele Donini</a>, 
<a href="/search/cs?searchtype=author&query=Archambeau%2C+C">C&#xe9;dric Archambeau</a>, 
<a href="/search/cs?searchtype=author&query=Pruthi%2C+D">Danish Pruthi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) encode vast amounts of world knowledge. However,
since these models are trained on large swaths of internet data, they are at
risk of inordinately capturing information about dominant groups. This
imbalance can propagate into generated language. In this work, we study and
operationalise a form of geographical erasure, wherein language models
underpredict certain countries. We demonstrate consistent instances of erasure
across a range of LLMs. We discover that erasure strongly correlates with low
frequencies of country mentions in the training corpus. Lastly, we mitigate
erasure by finetuning using a custom objective.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14778" title="Abstract">arXiv:2310.14778</a> [<a href="/pdf/2310.14778" title="Download PDF">pdf</a>, <a href="/format/2310.14778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-Visual Speaker Tracking: Progress, Challenges, and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinzheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xinyuan Qian</a>, 
<a href="/search/cs?searchtype=author&query=Berghi%2C+D">Davide Berghi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peipei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+M">Meng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+P+J+B">Philip J.B. Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenwu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio-visual speaker tracking has drawn increasing attention over the past
few years due to its academic values and wide application. Audio and visual
modalities can provide complementary information for localization and tracking.
With audio and visual information, the Bayesian-based filter can solve the
problem of data association, audio-visual fusion and track management. In this
paper, we conduct a comprehensive overview of audio-visual speaker tracking. To
our knowledge, this is the first extensive survey over the past five years. We
introduce the family of Bayesian filters and summarize the methods for
obtaining audio-visual measurements. In addition, the existing trackers and
their performance on AV16.3 dataset are summarized. In the past few years, deep
learning techniques have thrived, which also boosts the development of audio
visual speaker tracking. The influence of deep learning techniques in terms of
measurement extraction and state estimation is also discussed. At last, we
discuss the connections between audio-visual speaker tracking and other areas
such as speech separation and distributed speaker tracking.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14780" title="Abstract">arXiv:2310.14780</a> [<a href="/pdf/2310.14780" title="Download PDF">pdf</a>, <a href="/format/2310.14780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dance Your Latents: Consistent Dance Generation through Spatial-temporal  Subspace Attention Guided by Motion Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Haipeng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhihao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziyao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+F">Fan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Juan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Sheng Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The advancement of generative AI has extended to the realm of Human Dance
Generation, demonstrating superior generative capacities. However, current
methods still exhibit deficiencies in achieving spatiotemporal consistency,
resulting in artifacts like ghosting, flickering, and incoherent motions. In
this paper, we present Dance-Your-Latents, a framework that makes latents dance
coherently following motion flow to generate consistent dance videos. Firstly,
considering that each constituent element moves within a confined space, we
introduce spatial-temporal subspace-attention blocks that decompose the global
space into a combination of regular subspaces and efficiently model the
spatiotemporal consistency within these subspaces. This module enables each
patch pay attention to adjacent areas, mitigating the excessive dispersion of
long-range attention. Furthermore, observing that body part's movement is
guided by pose control, we design motion flow guided subspace align &amp; restore.
This method enables the attention to be computed on the irregular subspace
along the motion flow. Experimental results in TikTok dataset demonstrate that
our approach significantly enhances spatiotemporal consistency of the generated
videos.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14782" title="Abstract">arXiv:2310.14782</a> [<a href="/pdf/2310.14782" title="Download PDF">pdf</a>, <a href="/format/2310.14782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards equilibrium molecular conformation generation with GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Volokhova%2C+A">Alexandra Volokhova</a>, 
<a href="/search/cs?searchtype=author&query=Koziarski%2C+M">Micha&#x142; Koziarski</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Garc%C3%ADa%2C+A">Alex Hern&#xe1;ndez-Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cheng-Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cs?searchtype=author&query=Lemos%2C+P">Pablo Lemos</a>, 
<a href="/search/cs?searchtype=author&query=Thiede%2C+L">Luca Thiede</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zichao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Aspuru-Guzik%2C+A">Al&#xe1;n Aspuru-Guzik</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sampling diverse, thermodynamically feasible molecular conformations plays a
crucial role in predicting properties of a molecule. In this paper we propose
to use GFlowNet for sampling conformations of small molecules from the
Boltzmann distribution, as determined by the molecule's energy. The proposed
approach can be used in combination with energy estimation methods of different
fidelity and discovers a diverse set of low-energy conformations for highly
flexible drug-like molecules. We demonstrate that GFlowNet can reproduce
molecular potential energy surfaces by sampling proportionally to the Boltzmann
distribution.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14783" title="Abstract">arXiv:2310.14783</a> [<a href="/pdf/2310.14783" title="Download PDF">pdf</a>, <a href="/format/2310.14783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Deep Reinforcement Learning for Optimizing Heterogeneous  Energy Storage Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+L">Luolin Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chensheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shuai Mao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+K">Ke Meng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhaoyang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+F">Feng Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Energy storage systems (ESS) are pivotal component in the energy market,
serving as both energy suppliers and consumers. ESS operators can reap benefits
from energy arbitrage by optimizing operations of storage equipment. To further
enhance ESS flexibility within the energy market and improve renewable energy
utilization, a heterogeneous photovoltaic-ESS (PV-ESS) is proposed, which
leverages the unique characteristics of battery energy storage (BES) and
hydrogen energy storage (HES). For scheduling tasks of the heterogeneous
PV-ESS, cost description plays a crucial role in guiding operator's strategies
to maximize benefits. We develop a comprehensive cost function that takes into
account degradation, capital, and operation/maintenance costs to reflect
real-world scenarios. Moreover, while numerous methods excel in optimizing ESS
energy arbitrage, they often rely on black-box models with opaque
decision-making processes, limiting practical applicability. To overcome this
limitation and enable transparent scheduling strategies, a prototype-based
policy network with inherent interpretability is introduced. This network
employs human-designed prototypes to guide decision-making by comparing
similarities between prototypical situations and encountered situations, which
allows for naturally explained scheduling strategies. Comparative results
across four distinct cases underscore the effectiveness and practicality of our
proposed pre-hoc interpretable optimization method when contrasted with
black-box models.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14784" title="Abstract">arXiv:2310.14784</a> [<a href="/pdf/2310.14784" title="Download PDF">pdf</a>, <a href="/ps/2310.14784" title="Download PostScript">ps</a>, <a href="/format/2310.14784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Imbalance-Aware Federated Learning Approach for Wearable  Healthcare with Autoregressive Ratio Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wenhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">He Li</a>, 
<a href="/search/cs?searchtype=author&query=Ota%2C+K">Kaoru Ota</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+M">Mianxiong Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE OJCS in Oct. 2023 (under review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Widely available healthcare services are now getting popular because of
advancements in wearable sensing techniques and mobile edge computing. People's
health information is collected by edge devices such as smartphones and
wearable bands for further analysis on servers, then send back suggestions and
alerts for abnormal conditions. The recent emergence of federated learning
allows users to train private data on local devices while updating models
collaboratively. However, the heterogeneous distribution of the health
condition data may lead to significant risks to model performance due to class
imbalance. Meanwhile, as FL training is powered by sharing gradients only with
the server, training data is almost inaccessible. The conventional solutions to
class imbalance do not work for federated learning. In this work, we propose a
new federated learning framework FedImT, dedicated to addressing the challenges
of class imbalance in federated learning scenarios. FedImT contains an online
scheme that can estimate the data composition during each round of aggregation,
then introduces a self-attenuating iterative equivalent to track variations of
multiple estimations and promptly tweak the balance of the loss computing for
minority classes. Experiments demonstrate the effectiveness of FedImT in
solving the imbalance problem without extra energy consumption and avoiding
privacy risks.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14785" title="Abstract">arXiv:2310.14785</a> [<a href="/pdf/2310.14785" title="Download PDF">pdf</a>, <a href="/format/2310.14785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Enhanced Semantic Entity Recognition in Document Images via  Visually-Asymmetric Consistency Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiahua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Chenhui Chu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Extracting meaningful entities belonging to predefined categories from
Visually-rich Form-like Documents (VFDs) is a challenging task. Visual and
layout features such as font, background, color, and bounding box location and
size provide important cues for identifying entities of the same type. However,
existing models commonly train a visual encoder with weak cross-modal
supervision signals, resulting in a limited capacity to capture these
non-textual features and suboptimal performance. In this paper, we propose a
novel \textbf{V}isually-\textbf{A}symmetric co\textbf{N}sisten\textbf{C}y
\textbf{L}earning (\textsc{Vancl}) approach that addresses the above limitation
by enhancing the model's ability to capture fine-grained visual and layout
features through the incorporation of color priors. Experimental results on
benchmark datasets show that our approach substantially outperforms the strong
LayoutLM series baseline, demonstrating the effectiveness of our approach.
Additionally, we investigate the effects of different color schemes on our
approach, providing insights for optimizing model performance. We believe our
work will inspire future research on multimodal information extraction.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14787" title="Abstract">arXiv:2310.14787</a> [<a href="/pdf/2310.14787" title="Download PDF">pdf</a>, <a href="/format/2310.14787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Algorithm for Approximating Implicit Functions by Polynomials without  Higher-Order Differentiability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rim%2C+K+S">Kyung Soo Rim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">We consider an equation of multiple variables in which a partial derivative
does not vanish at a point. The implicit function theorem provides a local
existence and uniqueness of the function for the equation. In this paper, we
propose an algorithm to approximate the function by a polynomial without using
higher-order differentiability, which depends essentially on integrability.
Moreover, we extend the method to a system of equations if the Jacobian
determinant does not vanish. This is a robust method for implicit functions
that are not differentiable to higher-order. Additionally, we present two
numerical experiments to verify the theoretical results.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14788" title="Abstract">arXiv:2310.14788</a> [<a href="/pdf/2310.14788" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specialized Deep Residual Policy Safe Reinforcement Learning-Based  Controller for Complex and Continuous State-Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A+N">Ammar N. Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Chasparis%2C+G+C">Georgios C. Chasparis</a>, 
<a href="/search/cs?searchtype=author&query=Kelleher%2C+J+D">John D. Kelleher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Traditional controllers have limitations as they rely on prior knowledge
about the physics of the problem, require modeling of dynamics, and struggle to
adapt to abnormal situations. Deep reinforcement learning has the potential to
address these problems by learning optimal control policies through exploration
in an environment. For safety-critical environments, it is impractical to
explore randomly, and replacing conventional controllers with black-box models
is also undesirable. Also, it is expensive in continuous state and action
spaces, unless the search space is constrained. To address these challenges we
propose a specialized deep residual policy safe reinforcement learning with a
cycle of learning approach adapted for complex and continuous state-action
spaces. Residual policy learning allows learning a hybrid control architecture
where the reinforcement learning agent acts in synchronous collaboration with
the conventional controller. The cycle of learning initiates the policy through
the expert trajectory and guides the exploration around it. Further, the
specialization through the input-output hidden Markov model helps to optimize
policy that lies within the region of interest (such as abnormality), where the
reinforcement learning agent is required and is activated. The proposed
solution is validated on the Tennessee Eastman process control.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14790" title="Abstract">arXiv:2310.14790</a> [<a href="/pdf/2310.14790" title="Download PDF">pdf</a>, <a href="/format/2310.14790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Joint Maximum Mean Discrepancy Enabled  Multi-Source-Multi-Target Unsupervised Domain Adaptation Fault Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Haoran Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bo Qin</a>, 
<a href="/search/cs?searchtype=author&query=Butala%2C+M+D">Mark D. Butala</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Weiming Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongwei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite the remarkable results that can be achieved by data-driven
intelligent fault diagnosis techniques, they presuppose the same distribution
of training and test data as well as sufficient labeled data. Various operating
states often exist in practical scenarios, leading to the problem of domain
shift that hinders the effectiveness of fault diagnosis. While recent
unsupervised domain adaptation methods enable cross-domain fault diagnosis,
they struggle to effectively utilize information from multiple source domains
and achieve effective diagnosis faults in multiple target domains
simultaneously. In this paper, we innovatively proposed a weighted joint
maximum mean discrepancy enabled multi-source-multi-target unsupervised domain
adaptation (WJMMD-MDA), which realizes domain adaptation under
multi-source-multi-target scenarios in the field of fault diagnosis for the
first time. The proposed method extracts sufficient information from multiple
labeled source domains and achieves domain alignment between source and target
domains through an improved weighted distance loss. As a result,
domain-invariant and discriminative features between multiple source and target
domains are learned with cross-domain fault diagnosis realized. The performance
of the proposed method is evaluated in comprehensive comparative experiments on
three datasets, and the experimental results demonstrate the superiority of
this method.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14793" title="Abstract">arXiv:2310.14793</a> [<a href="/pdf/2310.14793" title="Download PDF">pdf</a>, <a href="/format/2310.14793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared  Properties in Large Concept Vocabularies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gajbhiye%2C+A">Amit Gajbhiye</a>, 
<a href="/search/cs?searchtype=author&query=Bouraoui%2C+Z">Zied Bouraoui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Na Li</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+U">Usashi Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Anke%2C+L+E">Luis Espinosa Anke</a>, 
<a href="/search/cs?searchtype=author&query=Schockaert%2C+S">Steven Schockaert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Concepts play a central role in many applications. This includes settings
where concepts have to be modelled in the absence of sentence context. Previous
work has therefore focused on distilling decontextualised concept embeddings
from language models. But concepts can be modelled from different perspectives,
whereas concept embeddings typically mostly capture taxonomic structure. To
address this issue, we propose a strategy for identifying what different
concepts, from a potentially large concept vocabulary, have in common with
others. We then represent concepts in terms of the properties they share with
the other concepts. To demonstrate the practical usefulness of this way of
modelling concepts, we consider the task of ultra-fine entity typing, which is
a challenging multi-label classification problem. We show that by augmenting
the label set with shared properties, we can improve the performance of the
state-of-the-art models for this task.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14796" title="Abstract">arXiv:2310.14796</a> [<a href="/pdf/2310.14796" title="Download PDF">pdf</a>, <a href="/format/2310.14796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Transfer Learning Method Utilizing Acoustic and Vibration  Signals for Rotating Machinery Fault Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhongliang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhuofei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wenxiong Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Fault diagnosis of rotating machinery plays a important role for the safety
and stability of modern industrial systems. However, there is a distribution
discrepancy between training data and data of real-world operation scenarios,
which causing the decrease of performance of existing systems. This paper
proposed a transfer learning based method utilizing acoustic and vibration
signal to address this distribution discrepancy. We designed the acoustic and
vibration feature fusion MAVgram to offer richer and more reliable information
of faults, coordinating with a DNN-based classifier to obtain more effective
diagnosis representation. The backbone was pre-trained and then fine-tuned to
obtained excellent performance of the target task. Experimental results
demonstrate the effectiveness of the proposed method, and achieved improved
performance compared to STgram-MFN.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14799" title="Abstract">arXiv:2310.14799</a> [<a href="/pdf/2310.14799" title="Download PDF">pdf</a>, <a href="/format/2310.14799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning  across Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Libo Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fuxuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shijue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Chain-of-thought (CoT) is capable of eliciting models to explicitly generate
reasoning paths, thus promoting reasoning accuracy and attracting increasing
attention. Specifically, zero-shot CoT achieves remarkable improvements in a
wide range of reasoning tasks by simply instructing the LLM with the prompt
"Let's think step by step!". Despite the success of zero-shot CoT, the existing
zero-shot prompting techniques remain limited to a single language, making it
challenging to generalize to other languages and hindering global development.
In this work, we introduce cross-lingual prompting (CLP), aiming to improve
zero-shot CoT reasoning across languages. Specifically, CLP consists of two
main components: (1) cross-lingual alignment prompting and (2) task-specific
solver prompting. The cross-lingual alignment prompting is responsible for
aligning representations across different languages, whereas the task-specific
solver prompting is used to generate the final chain of thoughts and results
for the reasoning task. In addition, we further introduce cross-lingual
self-consistent prompting (CLSP) to ensemble different reasoning paths across
languages. Our experimental evaluations on several benchmarks demonstrate that
CLP and CLSP significantly outperform the existing prompting methods and
achieve state-of-the-art performance. We hope this work will inspire further
breakthroughs in cross-lingual CoT.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14802" title="Abstract">arXiv:2310.14802</a> [<a href="/pdf/2310.14802" title="Download PDF">pdf</a>, <a href="/format/2310.14802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye  Movement for Machine Reading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yue Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Chenhui Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, Accepted by Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The use of visually-rich documents (VRDs) in various fields has created a
demand for Document AI models that can read and comprehend documents like
humans, which requires the overcoming of technical, linguistic, and cognitive
barriers. Unfortunately, the lack of appropriate datasets has significantly
hindered advancements in the field. To address this issue, we introduce
\textsc{DocTrack}, a VRD dataset really aligned with human eye-movement
information using eye-tracking technology. This dataset can be used to
investigate the challenges mentioned above. Additionally, we explore the impact
of human reading order on document understanding tasks and examine what would
happen if a machine reads in the same order as a human. Our results suggest
that although Document AI models have made significant progress, they still
have a long way to go before they can read VRDs as accurately, continuously,
and flexibly as humans do. These findings have potential implications for
future research and development of Document AI models. The data is available at
\url{https://github.com/hint-lab/doctrack}.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14804" title="Abstract">arXiv:2310.14804</a> [<a href="/pdf/2310.14804" title="Download PDF">pdf</a>, <a href="/format/2310.14804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models can Share Images, Too!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Young-Jun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hyeon%2C+J">Jonghwan Hyeon</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Ho-Jin Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This paper explores the image-sharing capability of Large Language Models
(LLMs), such as InstructGPT, ChatGPT, and GPT-4, in a zero-shot setting,
without the help of visual foundation models. Inspired by the two-stage process
of image-sharing in human dialogues, we propose a two-stage framework that
allows LLMs to predict potential image-sharing turns and generate related image
descriptions using our effective restriction-based prompt template. With
extensive experiments, we unlock the \textit{image-sharing} capability of LLMs
in zero-shot prompting, with GPT-4 achieving the best performance.
Additionally, we uncover the emergent \textit{image-sharing} ability in
zero-shot prompting, demonstrating the effectiveness of restriction-based
prompts in both stages of our framework. Based on this framework, we augment
the PhotoChat dataset with images generated by Stable Diffusion at predicted
turns, namely PhotoChat++. To our knowledge, this is the first study to assess
the image-sharing ability of LLMs in a zero-shot setting without visual
foundation models. The source code and the dataset will be released after
publication.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14805" title="Abstract">arXiv:2310.14805</a> [<a href="/pdf/2310.14805" title="Download PDF">pdf</a>, <a href="/format/2310.14805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Modal Conceptualization in Bottleneck Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alukaev%2C+D">Danis Alukaev</a>, 
<a href="/search/cs?searchtype=author&query=Kiselev%2C+S">Semen Kiselev</a>, 
<a href="/search/cs?searchtype=author&query=Pershin%2C+I">Ilya Pershin</a>, 
<a href="/search/cs?searchtype=author&query=Ibragimov%2C+B">Bulat Ibragimov</a>, 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+V">Vladimir Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Kornaev%2C+A">Alexey Kornaev</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Concept Bottleneck Models (CBMs) assume that training examples (e.g., x-ray
images) are annotated with high-level concepts (e.g., types of abnormalities),
and perform classification by first predicting the concepts, followed by
predicting the label relying on these concepts. The main difficulty in using
CBMs comes from having to choose concepts that are predictive of the label and
then having to label training examples with these concepts. In our approach, we
adopt a more moderate assumption and instead use text descriptions (e.g.,
radiology reports), accompanying the images in training, to guide the induction
of concepts. Our cross-modal approach treats concepts as discrete latent
variables and promotes concepts that (1) are predictive of the label, and (2)
can be predicted reliably from both the image and text. Through experiments
conducted on datasets ranging from synthetic datasets (e.g., synthetic images
with generated descriptions) to realistic medical imaging datasets, we
demonstrate that cross-modal learning encourages the induction of interpretable
concepts while also facilitating disentanglement. Our results also suggest that
this guidance leads to increased robustness by suppressing the reliance on
shortcut features.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14806" title="Abstract">arXiv:2310.14806</a> [<a href="/pdf/2310.14806" title="Download PDF">pdf</a>, <a href="/ps/2310.14806" title="Download PostScript">ps</a>, <a href="/format/2310.14806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Timestamp Information for Serialized Joint Streaming  Recognition and Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papi%2C+S">Sara Papi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junkun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jian Xue</a>, 
<a href="/search/cs?searchtype=author&query=Kanda%2C+N">Naoyuki Kanda</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+Y">Yashesh Gaur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \c{opyright} 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The growing need for instant spoken language transcription and translation is
driven by increased global communication and cross-lingual interactions. This
has made offering translations in multiple languages essential for user
applications. Traditional approaches to automatic speech recognition (ASR) and
speech translation (ST) have often relied on separate systems, leading to
inefficiencies in computational resources, and increased synchronization
complexity in real time. In this paper, we propose a streaming
Transformer-Transducer (T-T) model able to jointly produce many-to-one and
one-to-many transcription and translation using a single decoder. We introduce
a novel method for joint token-level serialized output training based on
timestamp information to effectively produce ASR and ST outputs in the
streaming setting. Experiments on {it,es,de}-&gt;en prove the effectiveness of our
approach, enabling the generation of one-to-many joint outputs with a single
decoder for the first time.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14811" title="Abstract">arXiv:2310.14811</a> [<a href="/pdf/2310.14811" title="Download PDF">pdf</a>, <a href="/format/2310.14811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-criteria Optimization of Workflow-Based Assembly Tasks in  Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holzinger%2C+F">Florian Holzinger</a>, 
<a href="/search/cs?searchtype=author&query=Beham%2C+A">Andreas Beham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in Computer Aided Systems Theory - EUROCAST 2022 and is available online at this URL <a href="https://doi.org/10.1007/978-3-031-25312-6_5">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Moreno-D\'iaz, R., Pichler, F., Quesada-Arencibia, A. (eds)
  Computer Aided Systems Theory - EUROCAST 2022. EUROCAST 2022. Lecture Notes
  in Computer Science, vol 13789. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Industrial manufacturing is currently amidst it's fourth great revolution,
pushing towards the digital transformation of production processes. One key
element of this transformation is the formalization and digitization of
processes, creating an increased potential to monitor, understand and optimize
existing processes. However, one major obstacle in this process is the
increased diversification and specialisation, resulting in the dependency on
multiple experts, which are rarely amalgamated in small to medium sized
companies. To mitigate this issue, this paper presents a novel approach for
multi-criteria optimization of workflow-based assembly tasks in manufacturing
by combining a workflow modeling framework and the HeuristicLab optimization
framework. For this endeavour, a new generic problem definition is implemented
in HeuristicLab, enabling the optimization of arbitrary workflows represented
with the modeling framework. The resulting Pareto front of the multi-criteria
optimization provides the decision makers a set of optimal workflows from which
they can choose to optimally fit the current demands. The advantages of the
herein presented approach are highlighted with a real world use case from an
ongoing research project.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14814" title="Abstract">arXiv:2310.14814</a> [<a href="/pdf/2310.14814" title="Download PDF">pdf</a>, <a href="/format/2310.14814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Ensemble Diversity for Robust Self-Training in the Presence  of Sample Selection Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Odonnat%2C+A">Ambroise Odonnat</a>, 
<a href="/search/cs?searchtype=author&query=Feofanov%2C+V">Vasilii Feofanov</a>, 
<a href="/search/cs?searchtype=author&query=Redko%2C+I">Ievgen Redko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-training is a well-known approach for semi-supervised learning. It
consists of iteratively assigning pseudo-labels to unlabeled data for which the
model is confident and treating them as labeled examples. For neural networks,
softmax prediction probabilities are often used as a confidence measure,
despite the fact that they are known to be overconfident, even for wrong
predictions. This phenomenon is particularly intensified in the presence of
sample selection bias, i.e., when data labeling is subject to some constraint.
To address this issue, we propose a novel confidence measure, called
$\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of
linear classifiers. We provide the theoretical analysis of our approach by
studying stationary points and describing the relationship between the
diversity of the individual members and their performance. We empirically
demonstrate the benefit of our confidence measure for three different
pseudo-labeling policies on classification datasets of various data modalities.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14815" title="Abstract">arXiv:2310.14815</a> [<a href="/pdf/2310.14815" title="Download PDF">pdf</a>, <a href="/format/2310.14815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep learning denoiser assisted roughness measurements extraction from  thin resists with low Signal-to-Noise Ratio(SNR) SEM images: analysis with  SMILE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sacchi%2C+S">Sara Sacchi</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+B">Bappaditya Dey</a>, 
<a href="/search/cs?searchtype=author&query=Mochi%2C+I">Iacopo Mochi</a>, 
<a href="/search/cs?searchtype=author&query=Halder%2C+S">Sandip Halder</a>, 
<a href="/search/cs?searchtype=author&query=Leray%2C+P">Philippe Leray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The technological advance of High Numerical Aperture Extreme Ultraviolet
Lithography (High NA EUVL) has opened the gates to extensive researches on
thinner photoresists (below 30nm), necessary for the industrial implementation
of High NA EUVL. Consequently, images from Scanning Electron Microscopy (SEM)
suffer from reduced imaging contrast and low Signal-to-Noise Ratio (SNR),
impacting the measurement of unbiased Line Edge Roughness (uLER) and Line Width
Roughness (uLWR). Thus, the aim of this work is to enhance the SNR of SEM
images by using a Deep Learning denoiser and enable robust roughness extraction
of the thin resist. For this study, we acquired SEM images of Line-Space (L/S)
patterns with a Chemically Amplified Resist (CAR) with different thicknesses
(15nm, 20nm, 25nm, 30nm), underlayers (Spin-On-Glass-SOG, Organic
Underlayer-OUL) and frames of averaging (4, 8, 16, 32, and 64 Fr). After
denoising, a systematic analysis has been carried out on both noisy and
denoised images using an open-source metrology software, SMILE 2.3.2, for
investigating mean CD, SNR improvement factor, biased and unbiased LWR/LER
Power Spectral Density (PSD). Denoised images with lower number of frames
present unaltered Critical Dimensions (CDs), enhanced SNR (especially for low
number of integration frames), and accurate measurements of uLER and uLWR, with
the same accuracy as for noisy images with a consistent higher number of
frames. Therefore, images with a small number of integration frames and with
SNR &lt; 2 can be successfully denoised, and advantageously used in improving
metrology throughput while maintaining reliable roughness measurements for the
thin resist.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14816" title="Abstract">arXiv:2310.14816</a> [<a href="/pdf/2310.14816" title="Download PDF">pdf</a>, <a href="/format/2310.14816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Multi-Level Replanning TAMP Framework for Dynamic  Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+C">Chengfei Yue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xibin Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Task and Motion Planning (TAMP) algorithms can generate plans that combine
logic and motion aspects for robots. However, these plans are sensitive to
interference and control errors. To make TAMP more applicable in real-world, we
propose the generalized multi-level replanning TAMP framework(GMRF), blending
the probabilistic completeness of sampling-based TAMP algorithm with the
robustness of reactive replanning. GMRF generates an nominal plan from the
initial state, then dynamically reconstructs this nominal plan in real-time,
reorders robot manipulations. Following the logic-level adjustment, GMRF will
try to replan a new motion path to ensure the updated plan is feasible at the
motion level. Finally, we conducted real-world experiments involving stack and
rearrange task domains. The result demonstrate GMRF's ability to swiftly
complete tasks in scenarios with varying degrees of interference.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14817" title="Abstract">arXiv:2310.14817</a> [<a href="/pdf/2310.14817" title="Download PDF">pdf</a>, <a href="/format/2310.14817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text2Topic: Multi-Label Text Classification System for Efficient Topic  Detection in User Generated Content with Zero-Shot Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fengjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Beladev%2C+M">Moran Beladev</a>, 
<a href="/search/cs?searchtype=author&query=Kleinfeld%2C+O">Ofri Kleinfeld</a>, 
<a href="/search/cs?searchtype=author&query=Frayerman%2C+E">Elina Frayerman</a>, 
<a href="/search/cs?searchtype=author&query=Shachar%2C+T">Tal Shachar</a>, 
<a href="/search/cs?searchtype=author&query=Fainman%2C+E">Eran Fainman</a>, 
<a href="/search/cs?searchtype=author&query=Assaraf%2C+K+L">Karen Lastmann Assaraf</a>, 
<a href="/search/cs?searchtype=author&query=Mizrachi%2C+S">Sarai Mizrachi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benjamin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Multi-label text classification is a critical task in the industry. It helps
to extract structured information from large amount of textual data. We propose
Text to Topic (Text2Topic), which achieves high multi-label classification
performance by employing a Bi-Encoder Transformer architecture that utilizes
concatenation, subtraction, and multiplication of embeddings on both text and
topic. Text2Topic also supports zero-shot predictions, produces domain-specific
text embeddings, and enables production-scale batch-inference with high
throughput. The final model achieves accurate and comprehensive results
compared to state-of-the-art baselines, including large language models (LLMs).
<br />In this study, a total of 239 topics are defined, and around 1.6 million
text-topic pairs annotations (in which 200K are positive) are collected on
approximately 120K texts from 3 main data sources on Booking.com. The data is
collected with optimized smart sampling and partial labeling. The final
Text2Topic model is deployed on a real-world stream processing platform, and it
outperforms other models with 92.9% micro mAP, as well as a 75.8% macro mAP
score. We summarize the modeling choices which are extensively tested through
ablation studies, and share detailed in-production decision-making steps.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14819" title="Abstract">arXiv:2310.14819</a> [<a href="/pdf/2310.14819" title="Download PDF">pdf</a>, <a href="/format/2310.14819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction  Following: A Case Study of Arabic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boughorbel%2C+S">Sabri Boughorbel</a>, 
<a href="/search/cs?searchtype=author&query=Hawasly%2C+M">Majd Hawasly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SIGARAB ArabicNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While significant progress has been made in benchmarking Large Language
Models (LLMs) across various tasks, there is a lack of comprehensive evaluation
of their abilities in responding to multi-turn instructions in less-commonly
tested languages like Arabic. Our paper offers a detailed examination of the
proficiency of open LLMs in such scenarios in Arabic. Utilizing a customized
Arabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a
uniform evaluator for both English and Arabic queries to assess and compare the
performance of the LLMs on various open-ended tasks. Our findings reveal
variations in model responses on different task categories, e.g., logic vs.
literacy, when instructed in English or Arabic. We find that fine-tuned base
models using multilingual and multi-turn datasets could be competitive to
models trained from scratch on multilingual data. Finally, we hypothesize that
an ensemble of small, open LLMs could perform competitively to proprietary LLMs
on the benchmark.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14820" title="Abstract">arXiv:2310.14820</a> [<a href="/pdf/2310.14820" title="Download PDF">pdf</a>, <a href="/format/2310.14820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALCUNA: Large Language Models Meet New Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xunjian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baizhou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the rapid development of NLP, large-scale language models (LLMs) excel
in various tasks across multiple domains now. However, existing benchmarks may
not adequately measure these models' capabilities, especially when faced with
new knowledge. In this paper, we address the lack of benchmarks to evaluate
LLMs' ability to handle new knowledge, an important and challenging aspect in
the rapidly evolving world. We propose an approach called KnowGen that
generates new knowledge by altering existing entity attributes and
relationships, resulting in artificial entities that are distinct from
real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to
assess LLMs' abilities in knowledge understanding, differentiation, and
association. We benchmark several LLMs, reveals that their performance in face
of new knowledge is not satisfactory, particularly in reasoning between new and
internal knowledge. We also explore the impact of entity similarity on the
model's understanding of entity knowledge and the influence of contextual
entities. We appeal to the need for caution when using LLMs in new scenarios or
with new knowledge, and hope that our benchmarks can help drive the development
of LLMs in face of new knowledge.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14821" title="Abstract">arXiv:2310.14821</a> [<a href="/pdf/2310.14821" title="Download PDF">pdf</a>, <a href="/format/2310.14821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mysticeti: Low-Latency DAG Consensus with Fast Commit Path
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babel%2C+K">Kushal Babel</a>, 
<a href="/search/cs?searchtype=author&query=Chursin%2C+A">Andrey Chursin</a>, 
<a href="/search/cs?searchtype=author&query=Danezis%2C+G">George Danezis</a>, 
<a href="/search/cs?searchtype=author&query=Kokoris-Kogias%2C+L">Lefteris Kokoris-Kogias</a>, 
<a href="/search/cs?searchtype=author&query=Sonnino%2C+A">Alberto Sonnino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We introduce Mysticeti-C a byzantine consensus protocol with low-latency and
high resource efficiency. It leverages a DAG based on Threshold Clocks and
incorporates innovations in pipelining and multiple leaders to reduce latency
in the steady state and under crash failures. Mysticeti-FPC incorporates a fast
commit path that has even lower latency. We prove the safety and liveness of
the protocols in a byzantine context. We evaluate Mysticeti and compare it with
state-of-the-art consensus and fast path protocols to demonstrate its low
latency and resource efficiency, as well as more graceful degradation under
crash failures. Mysticeti is the first byzantine protocol to achieve WAN
latency of 0.5s for consensus commit, at a throughput of over 50k TPS that
matches the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14825" title="Abstract">arXiv:2310.14825</a> [<a href="/pdf/2310.14825" title="Download PDF">pdf</a>, <a href="/format/2310.14825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed interval scheduling problem with minimal idle time with an  application to music arrangement problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Botelho%2C+L">Ludmila Botelho</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+%C3%96">&#xd6;zlem Salehi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
<p class="mathjax">The Operational Fixed Interval Scheduling Problem aims to find an assignment
of jobs to machines that maximizes the total weight of the completed jobs. We
introduce a new variant of the problem where we consider the additional goal of
minimizing the idle time, the total duration during which the machines are
idle. The problem is expressed using quadratic unconstrained binary
optimization (QUBO) formulation, taking into account soft and hard constraints
required to ensure that the number of jobs running at a time point is desirably
equal to the number of machines. Our choice of QUBO representation is motivated
by the increasing popularity of new computational architectures such as
neuromorphic processors, coherent Ising machines, and quantum and
quantum-inspired digital annealers for which QUBO is a natural input. An
optimization problem that can be solved using the presented QUBO formulation is
the music reduction problem, the process of reducing a given music piece for a
smaller number of instruments. We use two music compositions to test the QUBO
formulation and compare the performance of simulated, quantum, and hybrid
annealing algorithms.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14829" title="Abstract">arXiv:2310.14829</a> [<a href="/pdf/2310.14829" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing how &#x27;distributional&#x27; NLP corpora distance metrics are
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ackerman%2C+S">Samuel Ackerman</a>, 
<a href="/search/cs?searchtype=author&query=Kour%2C+G">George Kour</a>, 
<a href="/search/cs?searchtype=author&query=Farchi%2C+E">Eitan Farchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the August 2023 Joint Statistical Meetings proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Applications (stat.AP)

</div>
<p class="mathjax">A corpus of vector-embedded text documents has some empirical distribution.
Given two corpora, we want to calculate a single metric of distance (e.g.,
Mauve, Frechet Inception) between them. We describe an abstract quality, called
`distributionality', of such metrics. A non-distributional metric tends to use
very local measurements, or uses global measurements in a way that does not
fully reflect the distributions' true distance. For example, if individual
pairwise nearest-neighbor distances are low, it may judge the two corpora to
have low distance, even if their two distributions are in fact far from each
other. A more distributional metric will, in contrast, better capture the
distributions' overall distance. We quantify this quality by constructing a
Known-Similarity Corpora set from two paraphrase corpora and calculating the
distance between paired corpora from it. The distances' trend shape as set
element separation increases should quantify the distributionality of the
metric. We propose that Average Hausdorff Distance and energy distance between
corpora are representative examples of non-distributional and distributional
distance metrics, to which other metrics can be compared, to evaluate how
distributional they are.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14837" title="Abstract">arXiv:2310.14837</a> [<a href="/pdf/2310.14837" title="Download PDF">pdf</a>, <a href="/ps/2310.14837" title="Download PostScript">ps</a>, <a href="/format/2310.14837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Attention Mechanisms: Efficient Sequence Reduction using  Attention-based Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biermann%2C+D">Daniel Biermann</a>, 
<a href="/search/cs?searchtype=author&query=Palumbo%2C+F">Fabrizio Palumbo</a>, 
<a href="/search/cs?searchtype=author&query=Goodwin%2C+M">Morten Goodwin</a>, 
<a href="/search/cs?searchtype=author&query=Granmo%2C+O">Ole-Christoffer Granmo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 images, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many machine learning models use the manipulation of dimensions as a driving
force to enable models to identify and learn important features in data. In the
case of sequential data this manipulation usually happens on the token
dimension level. Despite the fact that many tasks require a change in sequence
length itself, the step of sequence length reduction usually happens out of
necessity and in a single step. As far as we are aware, no model uses the
sequence length reduction step as an additional opportunity to tune the models
performance. In fact, sequence length manipulation as a whole seems to be an
overlooked direction. In this study we introduce a novel attention-based method
that allows for the direct manipulation of sequence lengths. To explore the
method's capabilities, we employ it in an autoencoder model. The autoencoder
reduces the input sequence to a smaller sequence in latent space. It then aims
to reproduce the original sequence from this reduced form. In this setting, we
explore the methods reduction performance for different input and latent
sequence lengths. We are able to show that the autoencoder retains all the
significant information when reducing the original sequence to half its
original size. When reducing down to as low as a quarter of its original size,
the autoencoder is still able to reproduce the original sequence with an
accuracy of around 90%.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14838" title="Abstract">arXiv:2310.14838</a> [<a href="/pdf/2310.14838" title="Download PDF">pdf</a>, <a href="/format/2310.14838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibration of Time-Series Forecasting Transformers: Detecting and  Adapting Context-Driven Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mouxiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lefei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Han Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuo Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianling Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent years have witnessed the success of introducing Transformers to time
series forecasting. From a data generation perspective, we illustrate that
existing Transformers are susceptible to distribution shifts driven by temporal
contexts, whether observed or unobserved. Such context-driven distribution
shift (CDS) introduces biases in predictions within specific contexts and poses
challenges for conventional training paradigm. In this paper, we introduce a
universal calibration methodology for the detection and adaptation of CDS with
a trained Transformer model. To this end, we propose a novel CDS detector,
termed the "residual-based CDS detector" or "Reconditionor", which quantifies
the model's vulnerability to CDS by evaluating the mutual information between
prediction residuals and their corresponding contexts. A high Reconditionor
score indicates a severe susceptibility, thereby necessitating model
adaptation. In this circumstance, we put forth a straightforward yet potent
adapter framework for model calibration, termed the "sample-level
contextualized adapter" or "SOLID". This framework involves the curation of a
contextually similar dataset to the provided test sample and the subsequent
fine-tuning of the model's prediction layer with a limited number of steps. Our
theoretical analysis demonstrates that this adaptation strategy is able to
achieve an optimal equilibrium between bias and variance. Notably, our proposed
Reconditionor and SOLID are model-agnostic and readily adaptable to a wide
range of Transformers. Extensive experiments show that SOLID consistently
enhances the performance of current SOTA Transformers on real-world datasets,
especially on cases with substantial CDS detected by the proposed
Reconditionor, thus validate the effectiveness of the calibration approach.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14839" title="Abstract">arXiv:2310.14839</a> [<a href="/pdf/2310.14839" title="Download PDF">pdf</a>, <a href="/format/2310.14839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ESVAE: An Efficient Spiking Variational Autoencoder with  Reparameterizable Poisson Spiking Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Q">Qiugang Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiurui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guisong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Malu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In recent years, studies on image generation models of spiking neural
networks (SNNs) have gained the attention of many researchers. Variational
autoencoders (VAEs), as one of the most popular image generation models, have
attracted a lot of work exploring their SNN implementation. Due to the
constrained binary representation in SNNs, existing SNN VAE methods implicitly
construct the latent space by an elaborated autoregressive network and use the
network outputs as the sampling variables. However, this unspecified implicit
representation of the latent space will increase the difficulty of generating
high-quality images and introduces additional network parameters. In this
paper, we propose an efficient spiking variational autoencoder (ESVAE) that
constructs an interpretable latent space distribution and design a
reparameterizable spiking sampling method. Specifically, we construct the prior
and posterior of the latent space as a Poisson distribution using the firing
rate of the spiking neurons. Subsequently, we propose a reparameterizable
Poisson spiking sampling method, which is free from the additional network.
Comprehensive experiments have been conducted, and the experimental results
show that the proposed ESVAE outperforms previous SNN VAE methods in
reconstructed &amp; generated images quality. In addition, experiments demonstrate
that ESVAE's encoder is able to retain the original image information more
efficiently, and the decoder is more robust. The source code is available at
https://github.com/QgZhan/ESVAE.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14840" title="Abstract">arXiv:2310.14840</a> [<a href="/pdf/2310.14840" title="Download PDF">pdf</a>, <a href="/format/2310.14840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transparency at the Source: Evaluating and Interpreting Language Models  With Access to the True Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jumelet%2C+J">Jaap Jumelet</a>, 
<a href="/search/cs?searchtype=author&query=Zuidema%2C+W">Willem Zuidema</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present a setup for training, evaluating and interpreting neural language
models, that uses artificial, language-like data. The data is generated using a
massive probabilistic grammar (based on state-split PCFGs), that is itself
derived from a large natural language corpus, but also provides us complete
control over the generative process. We describe and release both grammar and
corpus, and test for the naturalness of our generated data. This approach
allows us to define closed-form expressions to efficiently compute exact lower
bounds on obtainable perplexity using both causal and masked language
modelling. Our results show striking differences between neural language
modelling architectures and training objectives in how closely they allow
approximating the lower bound on perplexity. Our approach also allows us to
directly compare learned representations to symbolic rules in the underlying
source. We experiment with various techniques for interpreting model behaviour
and learning dynamics. With access to the underlying true source, our results
show striking differences and outcomes in learning dynamics between different
classes of words.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14843" title="Abstract">arXiv:2310.14843</a> [<a href="/pdf/2310.14843" title="Download PDF">pdf</a>, <a href="/format/2310.14843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Software Construction using ChatGPT: An Experience Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monteiro%2C+M">Mauricio Monteiro</a>, 
<a href="/search/cs?searchtype=author&query=Branco%2C+B+C">Bruno Castelo Branco</a>, 
<a href="/search/cs?searchtype=author&query=Silvestre%2C+S">Samuel Silvestre</a>, 
<a href="/search/cs?searchtype=author&query=Avelino%2C+G">Guilherme Avelino</a>, 
<a href="/search/cs?searchtype=author&query=Valente%2C+M+T">Marco Tulio Valente</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In this paper, we explore the application of Large Language Models (LLMs) in
the particular context of end-to-end software construction, i.e., in contexts
where software developers have a set of requirements and have to design,
implement, test, and validate a new software system. Particularly, we report an
experiment where we asked three software developers to use ChatGPT to fully
implement a Web-based application using mainstream software architectures and
technologies. After that, we compare the apps produced by ChatGPT with a
reference implementation that we manually implemented for our research. As a
result, we document four categories of prompts that can be used by developers
in similar contexts, including initialization prompts, feature requests,
bug-fixing, and layout prompts. Additionally, we discuss the advantages and
disadvantages of two prompt construction approaches: top-down (where we start
with a high-level description of the target software, typically in the form of
user stories) and bottom-up (where we request the construction of the system
feature by feature).
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14845" title="Abstract">arXiv:2310.14845</a> [<a href="/pdf/2310.14845" title="Download PDF">pdf</a>, <a href="/format/2310.14845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mouxiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zemin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jundong Li</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qiheng Mao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianling Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent research has demonstrated the efficacy of pre-training graph neural
networks (GNNs) to capture the transferable graph semantics and enhance the
performance of various downstream tasks. However, the semantic knowledge
learned from pretext tasks might be unrelated to the downstream task, leading
to a semantic gap that limits the application of graph pre-training. To reduce
this gap, traditional approaches propose hybrid pre-training to combine various
pretext tasks together in a multi-task learning fashion and learn multi-grained
knowledge, which, however, cannot distinguish tasks and results in some
transferable task-specific knowledge distortion by each other. Moreover, most
GNNs cannot distinguish nodes located in different parts of the graph, making
them fail to learn position-specific knowledge and lead to suboptimal
performance. In this work, inspired by the prompt-based tuning in natural
language processing, we propose a unified framework for graph hybrid
pre-training which injects the task identification and position identification
into GNNs through a prompt mechanism, namely multi-task graph dual prompt
(ULTRA-DP). Based on this framework, we propose a prompt-based transferability
test to find the most relevant pretext task in order to reduce the semantic
gap. To implement the hybrid pre-training tasks, beyond the classical edge
prediction task (node-node level), we further propose a novel pre-training
paradigm based on a group of $k$-nearest neighbors (node-group level). The
combination of them across different scales is able to comprehensively express
more structural semantics and derive richer multi-grained knowledge. Extensive
experiments show that our proposed ULTRA-DP can significantly enhance the
performance of hybrid pre-training methods and show the generalizability to
other pre-training tasks and backbone architectures.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14848" title="Abstract">arXiv:2310.14848</a> [<a href="/pdf/2310.14848" title="Download PDF">pdf</a>, <a href="/format/2310.14848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-knowledge Proof Meets Machine Learning in Verifiability: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Z">Zhibo Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liehuang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Russello%2C+G">Giovanni Russello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">With the rapid advancement of artificial intelligence technology, the usage
of machine learning models is gradually becoming part of our daily lives.
High-quality models rely not only on efficient optimization algorithms but also
on the training and learning processes built upon vast amounts of data and
computational power. However, in practice, due to various challenges such as
limited computational resources and data privacy concerns, users in need of
models often cannot train machine learning models locally. This has led them to
explore alternative approaches such as outsourced learning and federated
learning. While these methods address the feasibility of model training
effectively, they introduce concerns about the trustworthiness of the training
process since computations are not performed locally. Similarly, there are
trustworthiness issues associated with outsourced model inference. These two
problems can be summarized as the trustworthiness problem of model
computations: How can one verify that the results computed by other
participants are derived according to the specified algorithm, model, and input
data? To address this challenge, verifiable machine learning (VML) has emerged.
This paper presents a comprehensive survey of zero-knowledge proof-based
verifiable machine learning (ZKP-VML) technology. We first analyze the
potential verifiability issues that may exist in different machine learning
scenarios. Subsequently, we provide a formal definition of ZKP-VML. We then
conduct a detailed analysis and classification of existing works based on their
technical approaches. Finally, we discuss the key challenges and future
directions in the field of ZKP-based VML.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14849" title="Abstract">arXiv:2310.14849</a> [<a href="/pdf/2310.14849" title="Download PDF">pdf</a>, <a href="/format/2310.14849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Domain Adaptation for Robust Handling of Distributional Shifts  in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyuhng Joon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyunsoo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sang-Woo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junyeob Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Choonghyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sang-goo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taeuk Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">When deploying machine learning systems to the wild, it is highly desirable
for them to effectively leverage prior knowledge to the unfamiliar domain while
also firing alarms to anomalous inputs. In order to address these requirements,
Universal Domain Adaptation (UniDA) has emerged as a novel research area in
computer vision, focusing on achieving both adaptation ability and robustness
(i.e., the ability to detect out-of-distribution samples). While UniDA has led
significant progress in computer vision, its application on language input
still needs to be explored despite its feasibility. In this paper, we propose a
comprehensive benchmark for natural language that offers thorough viewpoints of
the model's generalizability and robustness. Our benchmark encompasses multiple
datasets with varying difficulty levels and characteristics, including temporal
shifts and diverse domains. On top of our testbed, we validate existing UniDA
methods from computer vision and state-of-the-art domain adaptation techniques
from NLP literature, yielding valuable findings: We observe that UniDA methods
originally designed for image input can be effectively transferred to the
natural language domain while also underscoring the effect of adaptation
difficulty in determining the model's performance.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14853" title="Abstract">arXiv:2310.14853</a> [<a href="/pdf/2310.14853" title="Download PDF">pdf</a>, <a href="/format/2310.14853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Policy with Wait-$k$ Model for Simultaneous Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Libo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+K">Kai Fan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shushu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziqian Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhongqiang Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept to EMNLP 2023 main conference. 17 pages, 12 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Simultaneous machine translation (SiMT) requires a robust read/write policy
in conjunction with a high-quality translation model. Traditional methods rely
on either a fixed wait-$k$ policy coupled with a standalone wait-$k$
translation model, or an adaptive policy jointly trained with the translation
model. In this study, we propose a more flexible approach by decoupling the
adaptive policy model from the translation model. Our motivation stems from the
observation that a standalone multi-path wait-$k$ model performs competitively
with adaptive policies utilized in state-of-the-art SiMT approaches.
Specifically, we introduce DaP, a divergence-based adaptive policy, that makes
read/write decisions for any translation model based on the potential
divergence in translation distributions resulting from future information. DaP
extends a frozen wait-$k$ model with lightweight parameters, and is both memory
and computation efficient. Experimental results across various benchmarks
demonstrate that our approach offers an improved trade-off between translation
accuracy and latency, outperforming strong baselines.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14855" title="Abstract">arXiv:2310.14855</a> [<a href="/pdf/2310.14855" title="Download PDF">pdf</a>, <a href="/format/2310.14855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Refinement of Translations: Large Language Models for  Sentence and Document-Level Post-Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koneru%2C+S">Sai Koneru</a>, 
<a href="/search/cs?searchtype=author&query=Exel%2C+M">Miriam Exel</a>, 
<a href="/search/cs?searchtype=author&query=Huck%2C+M">Matthias Huck</a>, 
<a href="/search/cs?searchtype=author&query=Niehues%2C+J">Jan Niehues</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLM's) have demonstrated considerable success in
various Natural Language Processing tasks, but they have yet to attain
state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,
their significant performance in tasks demanding a broad understanding and
contextual processing shows their potential for translation. To exploit these
abilities, we investigate using LLM's for MT and explore recent
parameter-efficient fine-tuning techniques. Surprisingly, our initial
experiments find that fine-tuning for translation purposes even led to
performance degradation. To overcome this, we propose an alternative approach:
adapting LLM's as Automatic Post-Editors (APE) rather than direct translators.
Building on the LLM's exceptional ability to process and generate lengthy
sequences, we also propose extending our approach to document-level
translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can
yield significant improvements across both sentence and document-level metrics
while generalizing to out-of-domain data. Most notably, we achieve a
state-of-the-art accuracy rate of 89\% on the ContraPro test set, which
specifically assesses the model's ability to resolve pronoun ambiguities when
translating from English to German. Lastly, we investigate a practical scenario
involving manual post-editing for document-level translation, where reference
context is made available. Here, we demonstrate that leveraging human
corrections can significantly reduce the number of edits required for
subsequent translations\footnote{Interactive Demo for integrating manual
feedback can be found
\href{https://huggingface.co/spaces/skoneru/contextual_refinement_ende}{here}}
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14858" title="Abstract">arXiv:2310.14858</a> [<a href="/pdf/2310.14858" title="Download PDF">pdf</a>, <a href="/format/2310.14858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamically Weighted Federated k-Means
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holzer%2C+P">Patrick Holzer</a>, 
<a href="/search/cs?searchtype=author&query=Jacob%2C+T">Tania Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Kavane%2C+S">Shubham Kavane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated clustering is an important part of the field of federated machine
learning, that allows multiple data sources to collaboratively cluster their
data while keeping it decentralized and preserving privacy. In this paper, we
introduce a novel federated clustering algorithm, named Dynamically Weighted
Federated k-means (DWF k-means), to address the challenges posed by distributed
data sources and heterogeneous data. Our proposed algorithm combines the
benefits of traditional clustering techniques with the privacy and scalability
advantages of federated learning. It enables multiple data owners to
collaboratively cluster their local data while exchanging minimal information
with a central coordinator. The algorithm optimizes the clustering process by
adaptively aggregating cluster assignments and centroids from each data source,
thereby learning a global clustering solution that reflects the collective
knowledge of the entire federated network. We conduct experiments on multiple
datasets and data distribution settings to evaluate the performance of our
algorithm in terms of clustering score, accuracy, and v-measure. The results
demonstrate that our approach can match the performance of the centralized
classical k-means baseline, and outperform existing federated clustering
methods in realistic scenarios.
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14859" title="Abstract">arXiv:2310.14859</a> [<a href="/pdf/2310.14859" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for  Embodied Turn-Taking Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatan%2C+M">Mehdi Fatan</a>, 
<a href="/search/cs?searchtype=author&query=Mincato%2C+E">Emanuele Mincato</a>, 
<a href="/search/cs?searchtype=author&query=Pintzou%2C+D">Dimitra Pintzou</a>, 
<a href="/search/cs?searchtype=author&query=Dimiccoli%2C+M">Mariella Dimiccoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Predicting turn-taking in multiparty conversations has many practical
applications in human-computer/robot interaction. However, the complexity of
human communication makes it a challenging task. Recent advances have shown
that synchronous multi-perspective egocentric data can significantly improve
turn-taking prediction compared to asynchronous, single-perspective
transcriptions. Building on this research, we propose a new multimodal
transformer-based architecture for predicting turn-taking in embodied,
synchronized multi-perspective data. Our experimental results on the recently
introduced EgoCom dataset show a substantial performance improvement of up to
14.01% on average compared to existing baselines and alternative
transformer-based approaches. The source code, and the pre-trained models of
our 3T-Transformer will be available upon acceptance.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14860" title="Abstract">arXiv:2310.14860</a> [<a href="/pdf/2310.14860" title="Download PDF">pdf</a>, <a href="/format/2310.14860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Tuning of Robotic Polishing Skills based on Force Feedback  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhouyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zezheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhitao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+F">Fangyu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaowei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rong Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Acquiring human skills offers an efficient approach to tackle complex task
planning challenges. When performing a learned skill model for a continuous
contact task, such as robot polishing in an uncertain environment, the robot
needs to be able to adaptively modify the skill model to suit the environment
and perform the desired task. The environmental perturbation of the polishing
task is mainly reflected in the variation of contact force. Therefore,
adjusting the task skill model by providing feedback on the contact force
deviation is an effective way to meet the task requirements. In this study, a
phase-modulated diagonal recurrent neural network (PMDRNN) is proposed for
force feedback model learning in the robotic polishing task. The contact
between the tool and the workpiece in the polishing task can be considered a
dynamic system. In comparison to the existing feedforward neural network
phase-modulated neural network (PMNN), PMDRNN combines the diagonal recurrent
network structure with the phase-modulated neural network layer to improve the
learning performance of the feedback model for dynamic systems. Specifically,
data from real-world robot polishing experiments are used to learn the feedback
model. PMDRNN demonstrates a significant reduction in the training error of the
feedback model when compared to PMNN. Building upon this, the combination of
PMDRNN and dynamic movement primitives (DMPs) can be used for real-time
adjustment of skills for polishing tasks and effectively improve the robustness
of the task skill model. Finally, real-world robotic polishing experiments are
conducted to demonstrate the effectiveness of the approach.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14863" title="Abstract">arXiv:2310.14863</a> [<a href="/pdf/2310.14863" title="Download PDF">pdf</a>, <a href="/format/2310.14863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Paraphrase Types for Generation and Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wahle%2C+J+P">Jan Philip Wahle</a>, 
<a href="/search/cs?searchtype=author&query=Gipp%2C+B">Bela Gipp</a>, 
<a href="/search/cs?searchtype=author&query=Ruas%2C+T">Terry Ruas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Current approaches in paraphrase generation and detection heavily rely on a
single general similarity score, ignoring the intricate linguistic properties
of language. This paper introduces two new tasks to address this shortcoming by
considering paraphrase types - specific linguistic perturbations at particular
text positions. We name these tasks Paraphrase Type Generation and Paraphrase
Type Detection. Our results suggest that while current techniques perform well
in a binary classification scenario, i.e., paraphrased or not, the inclusion of
fine-grained paraphrase types poses a significant challenge. While most
approaches are good at generating and detecting general semantic similar
content, they fail to understand the intrinsic linguistic variables they
manipulate. Models trained in generating and identifying paraphrase types also
show improvements in tasks without them. In addition, scaling these models
further improves their ability to understand paraphrase types. We believe
paraphrase types can unlock a new paradigm for developing paraphrase models and
solving tasks in the future.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14864" title="Abstract">arXiv:2310.14864</a> [<a href="/pdf/2310.14864" title="Download PDF">pdf</a>, <a href="/format/2310.14864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diverse Priors for Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+C">Chenfan Weng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongguo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In Reinforcement Learning (RL), agents aim at maximizing cumulative rewards
in a given environment. During the learning process, RL agents face the dilemma
of exploitation and exploration: leveraging existing knowledge to acquire
rewards or seeking potentially higher ones. Using uncertainty as a guiding
principle provides an active and effective approach to solving this dilemma and
ensemble-based methods are one of the prominent avenues for quantifying
uncertainty. Nevertheless, conventional ensemble-based uncertainty estimation
lacks an explicit prior, deviating from Bayesian principles. Besides, this
method requires diversity among members to generate less biased uncertainty
estimation results. To address the above problems, previous research has
incorporated random functions as priors. Building upon these foundational
efforts, our work introduces an innovative approach with delicately designed
prior NNs, which can incorporate maximal diversity in the initial value
functions of RL. Our method has demonstrated superior performance compared with
the random prior approaches in solving classic control problems and general
exploration tasks, significantly improving sample efficiency.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14866" title="Abstract">arXiv:2310.14866</a> [<a href="/pdf/2310.14866" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study on Knowledge Graph Embeddings and Graph Neural Networks for Web  Of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittakola%2C+R+T">Rohith Teja Mittakola</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+T">Thomas Hassan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph data structures are widely used to store relational information between
several entities. With data being generated worldwide on a large scale, we see
a significant growth in the generation of knowledge graphs. Thing in the future
is Orange's take on a knowledge graph in the domain of the Web Of Things (WoT),
where the main objective of the platform is to provide a digital representation
of the physical world and enable cross-domain applications to be built upon
this massive and highly connected graph of things. In this context, as the
knowledge graph grows in size, it is prone to have noisy and messy data. In
this paper, we explore state-of-the-art knowledge graph embedding (KGE) methods
to learn numerical representations of the graph entities and, subsequently,
explore downstream tasks like link prediction, node classification, and triple
classification. We also investigate Graph neural networks (GNN) alongside KGEs
and compare their performance on the same downstream tasks. Our evaluation
highlights the encouraging performance of both KGE and GNN-based methods on
node classification, and the superiority of GNN approaches in the link
prediction task. Overall, we show that state-of-the-art approaches are relevant
in a WoT context, and this preliminary work provides insights to implement and
evaluate them in this context.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14867" title="Abstract">arXiv:2310.14867</a> [<a href="/pdf/2310.14867" title="Download PDF">pdf</a>, <a href="/format/2310.14867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who&#x27;s Watching Me?: Exploring the Impact of Audience Familiarity on  Player Performance, Experience, and Exertion in Virtual Reality Exergames
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zixuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenge Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jialin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+C">Cheng-Hung Lo</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hai-Ning Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, IEEE International Symposium on Mixed and Augmented Reality (ISMAR) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Familiarity with audiences plays a significant role in shaping individual
performance and experience across various activities in everyday life. This
study delves into the impact of familiarity with non-playable character (NPC)
audiences on player performance and experience in virtual reality (VR)
exergames. By manipulating of NPC appearance (face and body shape) and voice
familiarity, we explored their effect on game performance, experience, and
exertion. The findings reveal that familiar NPC audiences have a positive
impact on performance, creating a more enjoyable gaming experience, and leading
players to perceive less exertion. Moreover, individuals with higher levels of
self-consciousness exhibit heightened sensitivity to the familiarity with NPC
audiences. Our results shed light on the role of familiar NPC audiences in
enhancing player experiences and provide insights for designing more engaging
and personalized VR exergame environments.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14868" title="Abstract">arXiv:2310.14868</a> [<a href="/pdf/2310.14868" title="Download PDF">pdf</a>, <a href="/format/2310.14868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study  on Syllogism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Mengyu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Kuribayashi%2C+T">Tatsuki Kuribayashi</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+J">Jun Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+G">Goro Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Funayama%2C+H">Hiroaki Funayama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) take advantage of step-by-step reasoning
instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their
ability to perform CoT-style reasoning robustly is of interest from a probing
perspective. In this study, we inspect the step-by-step reasoning ability of
LLMs with a focus on negation, which is a core linguistic phenomenon that is
difficult to process. In particular, we introduce several controlled settings
(e.g., reasoning in case of fictional entities) to evaluate the logical
reasoning abilities of the models. We observed that dozens of modern LLMs were
not robust against lexical negation (e.g., plausible -&gt;implausible) when
performing CoT-style reasoning, and the results highlight unique limitations in
each LLM family.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14870" title="Abstract">arXiv:2310.14870</a> [<a href="/pdf/2310.14870" title="Download PDF">pdf</a>, <a href="/format/2310.14870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> We are Who We Cite: Bridges of Influence Between Natural Language  Processing and Other Academic Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wahle%2C+J+P">Jan Philip Wahle</a>, 
<a href="/search/cs?searchtype=author&query=Ruas%2C+T">Terry Ruas</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M">Mohamed Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Gipp%2C+B">Bela Gipp</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Natural Language Processing (NLP) is poised to substantially influence the
world. However, significant progress comes hand-in-hand with substantial risks.
Addressing them requires broad engagement with various fields of study. Yet,
little empirical work examines the state of such engagement (past or current).
In this paper, we quantify the degree of influence between 23 fields of study
and NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP
papers to other papers, and ~1.8m citations from other papers to NLP papers. We
show that, unlike most fields, the cross-field engagement of NLP, measured by
our proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in
1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown
more insular -- citing increasingly more NLP papers and having fewer papers
that act as bridges between fields. NLP citations are dominated by computer
science; Less than 8% of NLP citations are to linguistics, and less than 3% are
to math and psychology. These findings underscore NLP's urgent need to reflect
on its engagement with various fields.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14876" title="Abstract">arXiv:2310.14876</a> [<a href="/pdf/2310.14876" title="Download PDF">pdf</a>, <a href="/format/2310.14876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Martian Lava Tube Exploration Using Jumping Legged Robots: A Concept  Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olsen%2C+J+A">J&#xf8;rgen Anker Olsen</a>, 
<a href="/search/cs?searchtype=author&query=Alexis%2C+K">Kostas Alexis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74rd International Astronautical Congress (IAC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In recent years, robotic exploration has become increasingly important in
planetary exploration. One area of particular interest for exploration is
Martian lava tubes, which have several distinct features of interest. First, it
is theorized that they contain more easily accessible resources such as water
ice, needed for in-situ utilization on Mars. Second, lava tubes of significant
size can provide radiation and impact shelter for possible future human
missions to Mars. Third, lava tubes may offer a protected and preserved view
into Mars' geological and possible biological past. However, exploration of
these lava tubes poses significant challenges due to their sheer size,
geometric complexity, uneven terrain, steep slopes, collapsed sections,
significant obstacles, and unstable surfaces. Such challenges may hinder
traditional wheeled rover exploration. To overcome these challenges, legged
robots and particularly jumping systems have been proposed as potential
solutions. Jumping legged robots utilize legs to both walk and jump. This
allows them to traverse uneven terrain and steep slopes more easily compared to
wheeled or tracked systems. In the context of Martian lava tube exploration,
jumping legged robots would be particularly useful due to their ability to jump
over big boulders, gaps, and obstacles, as well as to descend and climb steep
slopes. This would allow them to explore and map such caves, and possibly
collect samples from areas that may otherwise be inaccessible. This paper
presents the specifications, design, capabilities, and possible mission
profiles for state-of-the-art legged robots tailored to space exploration.
Additionally, it presents the design, capabilities, and possible mission
profiles of a new jumping legged robot for Martian lava tube exploration that
is being developed at the Norwegian University of Science and Technology.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14880" title="Abstract">arXiv:2310.14880</a> [<a href="/pdf/2310.14880" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal  Scenarios Like a Lawyer?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+X">Xiaoxi Kang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lizhen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Soon%2C+L">Lay-Ki Soon</a>, 
<a href="/search/cs?searchtype=author&query=Trakic%2C+A">Adnan Trakic</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+T+Y">Terry Yue Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Emerton%2C+P+C">Patrick Charles Emerton</a>, 
<a href="/search/cs?searchtype=author&query=Grant%2C+G">Genevieve Grant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions
recently in the legal domain due to its emergent ability to tackle a variety of
legal tasks. However, it is still unknown if LLMs are able to analyze a legal
case and perform reasoning in the same manner as lawyers. Therefore, we
constructed a novel corpus consisting of scenarios pertain to Contract Acts
Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to
perform analysis on the corpus using the IRAC method, which is a framework
widely used by legal professionals for organizing legal analysis. Each scenario
in the corpus is annotated with a complete IRAC analysis in a semi-structured
format so that both machines and legal professionals are able to interpret and
understand the annotations. In addition, we conducted the first empirical
assessment of ChatGPT for IRAC analysis in order to understand how well it
aligns with the analysis of legal professionals. Our experimental results shed
lights on possible future research directions to improve alignments between
LLMs and legal experts in terms of legal reasoning.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14883" title="Abstract">arXiv:2310.14883</a> [<a href="/pdf/2310.14883" title="Download PDF">pdf</a>, <a href="/format/2310.14883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-autoregressive Streaming Transformer for Simultaneous Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhengrui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaolei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shoutao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chenze Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference; Source code is available at <a href="https://github.com/ictnlp/NAST">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Simultaneous machine translation (SiMT) models are trained to strike a
balance between latency and translation quality. However, training these models
to achieve high quality while maintaining low latency often leads to a tendency
for aggressive anticipation. We argue that such issue stems from the
autoregressive architecture upon which most existing SiMT models are built. To
address those issues, we propose non-autoregressive streaming Transformer
(NAST) which comprises a unidirectional encoder and a non-autoregressive
decoder with intra-chunk parallelism. We enable NAST to generate the blank
token or repetitive tokens to adjust its READ/WRITE strategy flexibly, and
train it to maximize the non-monotonic latent alignment with an alignment-based
latency loss. Experiments on various SiMT benchmarks demonstrate that NAST
outperforms previous strong autoregressive SiMT baselines.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14884" title="Abstract">arXiv:2310.14884</a> [<a href="/pdf/2310.14884" title="Download PDF">pdf</a>, <a href="/format/2310.14884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Budgeted Embedding Table For Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yunke Qu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q+V+H">Quoc Viet Hung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">At the heart of contemporary recommender systems (RSs) are latent factor
models that provide quality recommendation experience to users. These models
use embedding vectors, which are typically of a uniform and fixed size, to
represent users and items. As the number of users and items continues to grow,
this design becomes inefficient and hard to scale. Recent lightweight embedding
methods have enabled different users and items to have diverse embedding sizes,
but are commonly subject to two major drawbacks. Firstly, they limit the
embedding size search to optimizing a heuristic balancing the recommendation
quality and the memory complexity, where the trade-off coefficient needs to be
manually tuned for every memory budget requested. The implicitly enforced
memory complexity term can even fail to cap the parameter usage, making the
resultant embedding table fail to meet the memory budget strictly. Secondly,
most solutions, especially reinforcement learning based ones derive and
optimize the embedding size for each each user/item on an instance-by-instance
basis, which impedes the search efficiency. In this paper, we propose Budgeted
Embedding Table (BET), a novel method that generates table-level actions (i.e.,
embedding sizes for all users and items) that is guaranteed to meet
pre-specified memory budgets. Furthermore, by leveraging a set-based action
formulation and engaging set representation learning, we present an innovative
action search strategy powered by an action fitness predictor that efficiently
evaluates each table-level action. Experiments have shown state-of-the-art
performance on two real-world datasets when BET is paired with three popular
recommender models under different memory budgets.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14885" title="Abstract">arXiv:2310.14885</a> [<a href="/pdf/2310.14885" title="Download PDF">pdf</a>, <a href="/format/2310.14885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Location Estimation and Recovery using 5G Positioning: Thwarting GNSS  Spoofing Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A+K">Aneet Kumar Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Brandt%2C+S">Sebastian Brandt</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mridula Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The availability of cheap GNSS spoofers can prevent safe navigation and
tracking of road users. It can lead to loss of assets, inaccurate fare
estimation, enforcing the wrong speed limit, miscalculated toll tax, passengers
reaching an incorrect location, etc. The techniques designed to prevent and
detect spoofing by using cryptographic solutions or receivers capable of
differentiating legitimate and attack signals are insufficient in detecting
GNSS spoofing of road users. Recent studies, testbeds, and 3GPP standards are
exploring the possibility of hybrid positioning, where GNSS data will be
combined with the 5G-NR positioning to increase the security and accuracy of
positioning. We design the Location Estimation and Recovery(LER) systems to
estimate the correct absolute position using the combination of GNSS and 5G
positioning with other road users, where a subset of road users can be
malicious and collude to prevent spoofing detection. Our Location Verification
Protocol extends the understanding of Message Time of Arrival Codes (MTAC) to
prevent attacks against malicious provers. The novel Recovery and Meta Protocol
uses road users' dynamic and unpredictable nature to detect GNSS spoofing. This
protocol provides fast detection of GNSS spoofing with a very low rate of false
positives and can be customized to a large family of settings. Even in a
(highly unrealistic) worst-case scenario where each user is malicious with a
probability of as large as 0.3, our protocol detects GNSS spoofing with high
probability after communication and ranging with at most 20 road users, with a
false positive rate close to 0. SUMO simulations for road traffic show that we
can detect GNSS spoofing in 2.6 minutes since its start under moderate traffic
conditions.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14888" title="Abstract">arXiv:2310.14888</a> [<a href="/pdf/2310.14888" title="Download PDF">pdf</a>, <a href="/format/2310.14888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Bayesian Model Averaging over Paths in Probabilistic Programs  with Stochastic Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reichelt%2C+T">Tim Reichelt</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+L">Luke Ong</a>, 
<a href="/search/cs?searchtype=author&query=Rainforth%2C+T">Tom Rainforth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">The posterior in probabilistic programs with stochastic support decomposes as
a weighted sum of the local posterior distributions associated with each
possible program path. We show that making predictions with this full posterior
implicitly performs a Bayesian model averaging (BMA) over paths. This is
potentially problematic, as model misspecification can cause the BMA weights to
prematurely collapse onto a single path, leading to sub-optimal predictions in
turn. To remedy this issue, we propose alternative mechanisms for path
weighting: one based on stacking and one based on ideas from PAC-Bayes. We show
how both can be implemented as a cheap post-processing step on top of existing
inference engines. In our experiments, we find them to be more robust and lead
to better predictions compared to the default BMA weights.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14891" title="Abstract">arXiv:2310.14891</a> [<a href="/pdf/2310.14891" title="Download PDF">pdf</a>, <a href="/format/2310.14891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpeakEasy: A Conversational Intelligence Chatbot for Enhancing College  Students&#x27; Communication Skills
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyunbae Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Ramachandran%2C+R">Rhea Ramachandran</a>, 
<a href="/search/cs?searchtype=author&query=Ploerer%2C+V">Victoria Ploerer</a>, 
<a href="/search/cs?searchtype=author&query=Diekmann%2C+Y">Yella Diekmann</a>, 
<a href="/search/cs?searchtype=author&query=Bagga%2C+M">Max Bagga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Social interactions and conversation skills separate the successful from the
rest and the confident from the shy. For college students in particular, the
ability to converse can be an outlet for the stress and anxiety experienced on
a daily basis along with a foundation for all-important career skills. In light
of this, we designed SpeakEasy: a chatbot with some degree of intelligence that
provides feedback to the user on their ability to engage in free-form
conversations with the chatbot. SpeakEasy attempts to help college students
improve their communication skills by engaging in a seven-minute spoken
conversation with the user, analyzing the user's responses with metrics
designed based on previous psychology and linguistics research, and providing
feedback to the user on how they can improve their conversational ability. To
simulate natural conversation, SpeakEasy converses with the user on a wide
assortment of topics that two people meeting for the first time might discuss:
travel, sports, and entertainment. Unlike most other chatbots with the goal of
improving conversation skills, SpeakEasy actually records the user speaking,
transcribes the audio into tokens, and uses macros-e.g., sequences that
calculate the pace of speech, determine if the user has an over-reliance on
certain words, and identifies awkward transitions-to evaluate the quality of
the conversation. Based on the evaluation, SpeakEasy provides elaborate
feedback on how the user can improve their conversations. In turn, SpeakEasy
updates its algorithms based on a series of questions that the user responds to
regarding SpeakEasy's performance.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14892" title="Abstract">arXiv:2310.14892</a> [<a href="/pdf/2310.14892" title="Download PDF">pdf</a>, <a href="/format/2310.14892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time  Controllable Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+T">Tianqi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Quan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jingxuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhendong Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Controllable text generation (CTG) aims to generate text with desired
attributes, and decoding-time-based methods have shown promising performance on
this task. However, in this paper, we identify the phenomenon of Attribute
Collapse for the first time. It causes the fluency of generated text to rapidly
decrease when the control strength exceeds a critical value, rendering the text
completely unusable. This limitation hinders the effectiveness of decoding
methods in achieving high levels of controllability. To address this problem,
we propose a novel lightweight decoding framework named Air-Decoding. Its main
idea is reconstructing the attribute distributions to balance the weights
between attribute words and non-attribute words to generate more fluent text.
Specifically, we train prefixes by prefix-tuning to obtain attribute
distributions. Then we design a novel attribute distribution reconstruction
method to balance the obtained distributions and use the reconstructed
distributions to guide language models for generation, effectively avoiding the
issue of Attribute Collapse. Experiments on multiple CTG tasks prove that our
method achieves a new state-of-the-art control performance.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14893" title="Abstract">arXiv:2310.14893</a> [<a href="/pdf/2310.14893" title="Download PDF">pdf</a>, <a href="/format/2310.14893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Drift Monitoring for Log Anomaly Detection Pipelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wani%2C+D">Dipak Wani</a>, 
<a href="/search/cs?searchtype=author&query=Ackerman%2C+S">Samuel Ackerman</a>, 
<a href="/search/cs?searchtype=author&query=Farchi%2C+E">Eitan Farchi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaotong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Hau-wen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lalithsena%2C+S">Sarasi Lalithsena</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Applications (stat.AP)

</div>
<p class="mathjax">Logs enable the monitoring of infrastructure status and the performance of
associated applications. Logs are also invaluable for diagnosing the root
causes of any problems that may arise. Log Anomaly Detection (LAD) pipelines
automate the detection of anomalies in logs, providing assistance to site
reliability engineers (SREs) in system diagnosis. Log patterns change over
time, necessitating updates to the LAD model defining the `normal' log activity
profile. In this paper, we introduce a Bayes Factor-based drift detection
method that identifies when intervention, retraining, and updating of the LAD
model are required with human involvement. We illustrate our method using
sequences of log activity, both from unaltered data, and simulated activity
with controlled levels of anomaly contamination, based on real collected log
data.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14894" title="Abstract">arXiv:2310.14894</a> [<a href="/pdf/2310.14894" title="Download PDF">pdf</a>, <a href="/format/2310.14894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Universal Rule-based Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bobek%2C+S">Szymon Bobek</a>, 
<a href="/search/cs?searchtype=author&query=Nalepa%2C+G+J">Grzegorz J. Nalepa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Explainable artificial intelligence (XAI) is one of the most intensively
developed are of AI in recent years. It is also one of the most fragmented one
with multiple methods that focus on different aspects of explanations. This
makes difficult to obtain the full spectrum of explanation at once in a compact
and consistent way. To address this issue, we present Local Universal Explainer
(LUX) that is a rule-based explainer which can generate factual, counterfactual
and visual explanations. It is based on a modified version of decision tree
algorithms that allows for oblique splits and integration with feature
importance XAI methods such as SHAP or LIME. It does not use data generation in
opposite to other algorithms, but is focused on selecting local concepts in a
form of high-density clusters of real data that have the highest impact on
forming the decision boundary of the explained model. We tested our method on
real and synthetic datasets and compared it with state-of-the-art rule-based
explainers such as LORE, EXPLAN and Anchor. Our method outperforms currently
existing approaches in terms of simplicity, global fidelity and
representativeness.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14899" title="Abstract">arXiv:2310.14899</a> [<a href="/pdf/2310.14899" title="Download PDF">pdf</a>, <a href="/ps/2310.14899" title="Download PostScript">ps</a>, <a href="/format/2310.14899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Knowledge Graph Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kouagou%2C+N+J">N&#x27;Dah Jean Kouagou</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+C">Caglar Demir</a>, 
<a href="/search/cs?searchtype=author&query=Zahera%2C+H+M">Hamada M. Zahera</a>, 
<a href="/search/cs?searchtype=author&query=Wilke%2C+A">Adrian Wilke</a>, 
<a href="/search/cs?searchtype=author&query=Heindorf%2C+S">Stefan Heindorf</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiayi Li</a>, 
<a href="/search/cs?searchtype=author&query=Ngomo%2C+A+N">Axel-Cyrille Ngonga Ngomo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A variety of knowledge graph embedding approaches have been developed. Most
of them obtain embeddings by learning the structure of the knowledge graph
within a link prediction setting. As a result, the embeddings reflect only the
semantics of a single knowledge graph, and embeddings for different knowledge
graphs are not aligned, e.g., they cannot be used to find similar entities
across knowledge graphs via nearest neighbor search. However, knowledge graph
embedding applications such as entity disambiguation require a more global
representation, i.e., a representation that is valid across multiple sources.
We propose to learn universal knowledge graph embeddings from large-scale
interlinked knowledge sources. To this end, we fuse large knowledge graphs
based on the owl:sameAs relation such that every entity is represented by a
unique identity. We instantiate our idea by computing universal embeddings
based on DBpedia and Wikidata yielding embeddings for about 180 million
entities, 15 thousand relations, and 1.2 billion triples. Moreover, we develop
a convenient API to provide embeddings as a service. Experiments on link
prediction show that universal knowledge graph embeddings encode better
semantics compared to embeddings computed on a single knowledge graph. For
reproducibility purposes, we provide our source code and datasets open access
at https://github.com/dice-group/Universal_Embeddings
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14901" title="Abstract">arXiv:2310.14901</a> [<a href="/pdf/2310.14901" title="Download PDF">pdf</a>, <a href="/format/2310.14901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Series of Hessian-Vector Products for Tractable Saddle-Free Newton  Optimisation of Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oldewage%2C+E+T">Elre T. Oldewage</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+R+M">Ross M. Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 10 figures, 5 tables. Submitted to TMLR. First two authors' order randomised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Despite their popularity in the field of continuous optimisation,
second-order quasi-Newton methods are challenging to apply in machine learning,
as the Hessian matrix is intractably large. This computational burden is
exacerbated by the need to address non-convexity, for instance by modifying the
Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an
optimisation algorithm which addresses both of these concerns - to our
knowledge, the first efficiently-scalable optimisation algorithm to
asymptotically use the exact (eigenvalue-modified) inverse Hessian. Our method
frames the problem as a series which principally square-roots and inverts the
squared Hessian, then uses it to precondition a gradient vector, all without
explicitly computing or eigendecomposing the Hessian. A truncation of this
infinite series provides a new optimisation algorithm which is scalable and
comparable to other first- and second-order optimisation methods in both
runtime and optimisation performance. We demonstrate this in a variety of
settings, including a ResNet-18 trained on CIFAR-10.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14906" title="Abstract">arXiv:2310.14906</a> [<a href="/pdf/2310.14906" title="Download PDF">pdf</a>, <a href="/format/2310.14906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DYNAMITE: Dynamic Interplay of Mini-Batch Size and Aggregation Frequency  for Federated Learning with Static and Streaming Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weijie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoxi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jingpu Duan</a>, 
<a href="/search/cs?searchtype=author&query=Joe-Wong%2C+C">Carlee Joe-Wong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated Learning (FL) is a distributed learning paradigm that can
coordinate heterogeneous edge devices to perform model training without sharing
private data. While prior works have focused on analyzing FL convergence with
respect to hyperparameters like batch size and aggregation frequency, the joint
effects of adjusting these parameters on model performance, training time, and
resource consumption have been overlooked, especially when facing dynamic data
streams and network characteristics. This paper introduces novel analytical
models and optimization algorithms that leverage the interplay between batch
size and aggregation frequency to navigate the trade-offs among convergence,
cost, and completion time for dynamic FL training. We establish a new
convergence bound for training error considering heterogeneous datasets across
devices and derive closed-form solutions for co-optimized batch size and
aggregation frequency that are consistent across all devices. Additionally, we
design an efficient algorithm for assigning different batch configurations
across devices, improving model accuracy and addressing the heterogeneity of
both data and system characteristics. Further, we propose an adaptive control
algorithm that dynamically estimates network states, efficiently samples
appropriate data batches, and effectively adjusts batch sizes and aggregation
frequency on the fly. Extensive experiments demonstrate the superiority of our
offline optimal solutions and online adaptive algorithm.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14907" title="Abstract">arXiv:2310.14907</a> [<a href="/pdf/2310.14907" title="Download PDF">pdf</a>, <a href="/format/2310.14907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orientation-Aware Leg Movement Learning for Action-Driven Human Motion  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+C">Chunzhi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kuriyama%2C+S">Shigeru Kuriyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">The task of action-driven human motion prediction aims to forecast future
human motion from the observed sequence while respecting the given action
label. It requires modeling not only the stochasticity within human motion but
the smooth yet realistic transition between multiple action labels. However,
the fact that most of the datasets do not contain such transition data
complicates this task. Existing work tackles this issue by learning a
smoothness prior to simply promote smooth transitions, yet doing so can result
in unnatural transitions especially when the history and predicted motions
differ significantly in orientations. In this paper, we argue that valid human
motion transitions should incorporate realistic leg movements to handle
orientation changes, and cast it as an action-conditioned in-betweening (ACB)
learning task to encourage transition naturalness. Because modeling all
possible transitions is virtually unreasonable, our ACB is only performed on
very few selected action classes with active gait motions, such as Walk or Run.
Specifically, we follow a two-stage forecasting strategy by first employing the
motion diffusion model to generate the target motion with a specified future
action, and then producing the in-betweening to smoothly connect the
observation and prediction to eventually address motion prediction. Our method
is completely free from the labeled motion transition data during training. To
show the robustness of our approach, we generalize our trained in-betweening
learning model on one dataset to two unseen large-scale motion datasets to
produce natural transitions. Extensive methods on three benchmark datasets
demonstrate that our method yields the state-of-the-art performance in terms of
visual quality, prediction accuracy, and action faithfulness.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14909" title="Abstract">arXiv:2310.14909</a> [<a href="/pdf/2310.14909" title="Download PDF">pdf</a>, <a href="/format/2310.14909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linking Surface Facts to Large-Scale Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radevski%2C+G">Gorjan Radevski</a>, 
<a href="/search/cs?searchtype=author&query=Gashteovski%2C+K">Kiril Gashteovski</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+C">Chia-Chien Hung</a>, 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+C">Carolin Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=Glava%C5%A1%2C+G">Goran Glava&#x161;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Open Information Extraction (OIE) methods extract facts from natural language
text in the form of ("subject"; "relation"; "object") triples. These facts are,
however, merely surface forms, the ambiguity of which impedes their downstream
usage; e.g., the surface phrase "Michael Jordan" may refer to either the former
basketball player or the university professor. Knowledge Graphs (KGs), on the
other hand, contain facts in a canonical (i.e., unambiguous) form, but their
coverage is limited by a static schema (i.e., a fixed set of entities and
predicates). To bridge this gap, we need the best of both worlds: (i) high
coverage of free-text OIEs, and (ii) semantic precision (i.e., monosemy) of
KGs. In order to achieve this goal, we propose a new benchmark with novel
evaluation protocols that can, for example, measure fact linking performance on
a granular triple slot level, while also measuring if a system has the ability
to recognize that a surface form has no match in the existing KG. Our extensive
evaluation of several baselines show that detection of out-of-KG entities and
predicates is more difficult than accurate linking to existing ones, thus
calling for more research efforts on this difficult task. We publicly release
all resources (data, benchmark and code) on
https://github.com/nec-research/fact-linking.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14910" title="Abstract">arXiv:2310.14910</a> [<a href="/pdf/2310.14910" title="Download PDF">pdf</a>, <a href="/ps/2310.14910" title="Download PostScript">ps</a>, <a href="/format/2310.14910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear matrix inequality based Type-III compensator synthesis for DC-DC  converters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keskin%2C+R">R&#x131;dvan Keskin</a>, 
<a href="/search/cs?searchtype=author&query=Aliskan%2C+I">Ibrahim Aliskan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Boost, buck-boost, and fly-back DC-DC converters which are utilized in power
lines of any electric vehicles, solar energy, and power factor correction
applications require control systems to regulate the output voltage under
mismatched disturbances i.e. load current and input voltage. In continuous
current mode operation, the converters, however, are bandwidth-limited control
systems due to their non-minimum phase nature. Disturbance rejection
performance of such bandwidth-limited control system is an open problem
especially where input voltage and load current disturbances cannot be
measured. A third-order integral-lead (Type-III) compensator with a disturbance
observer (DOB) can suppress the disturbances and unmodeled dynamics of the
converters. However, synthesizing such a fixed-order control system under
performance constraints is generally challenging. This paper proposes a
simultaneous design of a Type-III compensator and a fixed order DOB based on
Hinf control approach using convex optimization. The optimization problem is
formulated in a convex-concave procedure by including the estimated disturbance
and sensor noise functions. We proposed a two-stage iterative algorithm to
solve the problem in a convex optimization framework. Convex programming can
therefore be used to synthesize an optimal fixed-order control system by
removing the non-convex constraints on the parameter space. The approach leads
to an easily resolvable control algorithm with linear matrix inequality
constraints over parameterized controller parameters due to the convexity of
the problem. The proposed control system is implemented on a 200W DC-DC
multi-phase interleaved boost converter prototype using a TMS320F28335 digital
signal processor. The performance of the approach is compared with the
well-known K-factor design approach for the Type-III compensators.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14911" title="Abstract">arXiv:2310.14911</a> [<a href="/pdf/2310.14911" title="Download PDF">pdf</a>, <a href="/format/2310.14911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Fronthaul Load Balancing and Computation Resource Allocation in  Cell-Free User-Centric Massive MIMO Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6ttsch%2C+F">Fabian G&#xf6;ttsch</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Ming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, submitted to IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We consider scalable cell-free massive multiple-input multiple-output
networks under an open radio access network paradigm comprising user equipments
(UEs), radio units (RUs), and decentralized processing units (DUs). UEs are
served by dynamically allocated user-centric clusters of RUs. The corresponding
cluster processors (implementing the physical layer for each user) are hosted
by the DUs as software-defined virtual network functions. Unlike the current
literature, mainly focused on the characterization of the user rates under
unrestricted fronthaul communication and computation, in this work we
explicitly take into account the fronthaul topology, the limited fronthaul
communication capacity, and computation constraints at the DUs. In particular,
we systematically address the new problem of joint fronthaul load balancing and
allocation of the computation resource. As a consequence of our new
optimization framework, we present representative numerical results
highlighting the existence of an optimal number of quantization bits in the
analog-to-digital conversion at the RUs.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14914" title="Abstract">arXiv:2310.14914</a> [<a href="/pdf/2310.14914" title="Download PDF">pdf</a>, <a href="/format/2310.14914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object Pose Estimation Annotation Pipeline for Multi-view Monocular  Camera Systems in Industrial Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Youssef%2C+H">Hazem Youssef</a>, 
<a href="/search/cs?searchtype=author&query=Polachowski%2C+F">Frederik Polachowski</a>, 
<a href="/search/cs?searchtype=author&query=Rutinowski%2C+J">J&#xe9;r&#xf4;me Rutinowski</a>, 
<a href="/search/cs?searchtype=author&query=Roidl%2C+M">Moritz Roidl</a>, 
<a href="/search/cs?searchtype=author&query=Reining%2C+C">Christopher Reining</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Object localization, and more specifically object pose estimation, in large
industrial spaces such as warehouses and production facilities, is essential
for material flow operations. Traditional approaches rely on artificial
artifacts installed in the environment or excessively expensive equipment, that
is not suitable at scale. A more practical approach is to utilize existing
cameras in such spaces in order to address the underlying pose estimation
problem and to localize objects of interest. In order to leverage
state-of-the-art methods in deep learning for object pose estimation, large
amounts of data need to be collected and annotated. In this work, we provide an
approach to the annotation of large datasets of monocular images without the
need for manual labor. Our approach localizes cameras in space, unifies their
location with a motion capture system, and uses a set of linear mappings to
project 3D models of objects of interest at their ground truth 6D pose
locations. We test our pipeline on a custom dataset collected from a system of
eight cameras in an industrial setting that mimics the intended area of
operation. Our approach was able to provide consistent quality annotations for
our dataset with 26, 482 object instances at a fraction of the time required by
human annotators.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14918" title="Abstract">arXiv:2310.14918</a> [<a href="/pdf/2310.14918" title="Download PDF">pdf</a>, <a href="/format/2310.14918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARNIQA: Learning Distortion Manifold for Image Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agnolucci%2C+L">Lorenzo Agnolucci</a>, 
<a href="/search/cs?searchtype=author&query=Galteri%2C+L">Leonardo Galteri</a>, 
<a href="/search/cs?searchtype=author&query=Bertini%2C+M">Marco Bertini</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to
measure image quality in alignment with human perception without the need for a
high-quality reference image. In this work, we propose a self-supervised
approach named ARNIQA (leArning distoRtion maNifold for Image Quality
Assessment) for modeling the image distortion manifold to obtain quality
representations in an intrinsic manner. First, we introduce an image
degradation model that randomly composes ordered sequences of consecutively
applied distortions. In this way, we can synthetically degrade images with a
large variety of degradation patterns. Second, we propose to train our model by
maximizing the similarity between the representations of patches of different
images distorted equally, despite varying content. Therefore, images degraded
in the same manner correspond to neighboring positions within the distortion
manifold. Finally, we map the image representations to the quality scores with
a simple linear regressor, thus without fine-tuning the encoder weights. The
experiments show that our approach achieves state-of-the-art performance on
several datasets. In addition, ARNIQA demonstrates improved data efficiency,
generalization capabilities, and robustness compared to competing methods. The
code and the model are publicly available at
https://github.com/miccunifi/ARNIQA.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14919" title="Abstract">arXiv:2310.14919</a> [<a href="/pdf/2310.14919" title="Download PDF">pdf</a>, <a href="/format/2310.14919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRLib: An Open-Source Hand Gesture Detection and Recognition Python  Library
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Warchocki%2C+J">Jan Warchocki</a>, 
<a href="/search/cs?searchtype=author&query=Vlasenko%2C+M">Mikhail Vlasenko</a>, 
<a href="/search/cs?searchtype=author&query=Eisma%2C+Y+B">Yke Bauke Eisma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Hand gesture recognition systems provide a natural way for humans to interact
with computer systems. Although various algorithms have been designed for this
task, a host of external conditions, such as poor lighting or distance from the
camera, make it difficult to create an algorithm that performs well across a
range of environments. In this work, we present GRLib: an open-source Python
library able to detect and classify static and dynamic hand gestures. Moreover,
the library can be trained on existing data for improved classification
robustness. The proposed solution utilizes a feed from an RGB camera. The
retrieved frames are then subjected to data augmentation and passed on to
MediaPipe Hands to perform hand landmark detection. The landmarks are then
classified into their respective gesture class. The library supports dynamic
hand gestures through trajectories and keyframe extraction. It was found that
the library outperforms another publicly available HGR system - MediaPipe
Solutions, on three diverse, real-world datasets. The library is available at
https://github.com/mikhail-vlasenko/grlib and can be installed with pip.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14921" title="Abstract">arXiv:2310.14921</a> [<a href="/pdf/2310.14921" title="Download PDF">pdf</a>, <a href="/format/2310.14921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PartialFormer: Modeling Part Instead of Whole
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bei Li</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Huiwen Bao</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+W">Weiqiao Shan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jingbo Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The design choices in Transformer feed-forward neural networks have resulted
in significant computational and parameter overhead. In this work, we emphasize
the importance of hidden dimension in designing lightweight FFNs, a factor
often overlooked in previous architectures. Guided by this principle, we
introduce PartialFormer, a parameter-efficient Transformer architecture
utilizing multiple smaller FFNs to reduce parameters and computation while
maintaining essential hidden dimensions. These smaller FFNs are integrated into
a multi-head attention system to enable effective collaboration. We also
propose a tailored head scaling strategy to enhance PartialFormer's
capabilities. Furthermore, we present a residual-like attention calculation to
improve depth scaling within PartialFormer. Extensive experiments on 9
translation tasks and 1 abstractive summarization task validate the
effectiveness of our PartialFormer approach. Our code would be available at:
\url{https://github.com/zhengkid/PartialFormer}.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14924" title="Abstract">arXiv:2310.14924</a> [<a href="/pdf/2310.14924" title="Download PDF">pdf</a>, <a href="/format/2310.14924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Converting Depth Images and Point Clouds for Feature-based Pose  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6sch%2C+R">Robert L&#xf6;sch</a> (1), 
<a href="/search/cs?searchtype=author&query=Sastuba%2C+M">Mark Sastuba</a> (2), 
<a href="/search/cs?searchtype=author&query=Toth%2C+J">Jonas Toth</a> (1), 
<a href="/search/cs?searchtype=author&query=Jung%2C+B">Bernhard Jung</a> (1) ((1) Technical University Bergakademie Freiberg, Germany, (2) German Centre for Rail Traffic Research at the Federal Railway Authority, Germany)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in IROS 2023 conference proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In recent years, depth sensors have become more and more affordable and have
found their way into a growing amount of robotic systems. However, mono- or
multi-modal sensor registration, often a necessary step for further processing,
faces many challenges on raw depth images or point clouds. This paper presents
a method of converting depth data into images capable of visualizing spatial
details that are basically hidden in traditional depth images. After noise
removal, a neighborhood of points forms two normal vectors whose difference is
encoded into this new conversion. Compared to Bearing Angle images, our method
yields brighter, higher-contrast images with more visible contours and more
details. We tested feature-based pose estimation of both conversions in a
visual odometry task and RGB-D SLAM. For all tested features, AKAZE, ORB, SIFT,
and SURF, our new Flexion images yield better results than Bearing Angle images
and show great potential to bridge the gap between depth data and classical
computer vision. Source code is available here:
https://rlsch.github.io/depth-flexion-conversion.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14925" title="Abstract">arXiv:2310.14925</a> [<a href="/pdf/2310.14925" title="Download PDF">pdf</a>, <a href="/format/2310.14925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Causal Discovery for Robotics Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castri%2C+L">Luca Castri</a>, 
<a href="/search/cs?searchtype=author&query=Mghames%2C+S">Sariah Mghames</a>, 
<a href="/search/cs?searchtype=author&query=Bellotto%2C+N">Nicola Bellotto</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published at 5th Italian Conference on Robotics and Intelligent
  Machines (I-RIM 3D 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Using robots for automating tasks in environments shared with humans, such as
warehouses, shopping centres, or hospitals, requires these robots to comprehend
the fundamental physical interactions among nearby agents and objects.
Specifically, creating models to represent cause-and-effect relationships among
these elements can aid in predicting unforeseen human behaviours and anticipate
the outcome of particular robot actions. To be suitable for robots, causal
analysis must be both fast and accurate, meeting real-time demands and the
limited computational resources typical in most robotics applications. In this
paper, we present a practical demonstration of our approach for fast and
accurate causal analysis, known as Filtered~PCMCI~(F-PCMCI), along with a
real-world robotics application. The provided application illustrates how our
F-PCMCI can accurately and promptly reconstruct the causal model of a
human-robot interaction scenario, which can then be leveraged to enhance the
quality of the interaction.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14926" title="Abstract">arXiv:2310.14926</a> [<a href="/pdf/2310.14926" title="Download PDF">pdf</a>, <a href="/format/2310.14926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reference-based Restoration of Digitized Analog Videotapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agnolucci%2C+L">Lorenzo Agnolucci</a>, 
<a href="/search/cs?searchtype=author&query=Galteri%2C+L">Leonardo Galteri</a>, 
<a href="/search/cs?searchtype=author&query=Bertini%2C+M">Marco Bertini</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Analog magnetic tapes have been the main video data storage device for
several decades. Videos stored on analog videotapes exhibit unique degradation
patterns caused by tape aging and reader device malfunctioning that are
different from those observed in film and digital video restoration tasks. In
this work, we present a reference-based approach for the resToration of
digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact
detection to identify the cleanest frames of each video through textual prompts
describing different artifacts. Then, we select the clean frames most similar
to the input ones and employ them as references. We design a transformer-based
Swin-UNet network that exploits both neighboring and reference frames via our
Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on
cross-attention and attention pooling to take advantage of the most useful
parts of each reference frame. To address the absence of ground truth in
real-world videos, we create a synthetic dataset of videos exhibiting artifacts
that closely resemble those commonly found in analog videotapes. Both
quantitative and qualitative experiments show the effectiveness of our approach
compared to other state-of-the-art methods. The code, the model, and the
synthetic dataset are publicly available at https://github.com/miccunifi/TAPE.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14928" title="Abstract">arXiv:2310.14928</a> [<a href="/pdf/2310.14928" title="Download PDF">pdf</a>, <a href="/format/2310.14928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling A Core Linguistic Region in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yide Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Luhui Gao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work on progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Brain localization, which describes the association between specific regions
of the brain and their corresponding functions, is widely accepted in the field
of cognitive science as an objective fact. Today's large language models (LLMs)
possess human-level linguistic competence and can execute complex tasks
requiring abstract knowledge and reasoning. To deeply understand the inherent
mechanisms of intelligence emergence in LLMs, this paper conducts an analogical
research using brain localization as a prototype. We have discovered a core
region in LLMs that corresponds to linguistic competence, accounting for
approximately 1% of the total model parameters. This core region exhibits
significant dimension dependency, and perturbations to even a single parameter
on specific dimensions can lead to a loss of linguistic competence.
Furthermore, we observe that an improvement in linguistic competence does not
necessarily accompany an elevation in the model's knowledge level, which might
imply the existence of regions of domain knowledge that are dissociated from
the linguistic region. Overall, exploring the LLMs' functional regions provides
insights into the foundation of their intelligence. In the future, we will
continue to investigate knowledge regions within LLMs and the interactions
between them.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14929" title="Abstract">arXiv:2310.14929</a> [<a href="/pdf/2310.14929" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Definitional Functoriality for Dependent (Sub)Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laurent%2C+T">Th&#xe9;o Laurent</a>, 
<a href="/search/cs?searchtype=author&query=Lennon-Bertrand%2C+M">Meven Lennon-Bertrand</a>, 
<a href="/search/cs?searchtype=author&query=Maillard%2C+K">Kenji Maillard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Dependently-typed proof assistant rely crucially on definitional equality,
which relates types and terms that are automatically identified in the
underlying type theory. This paper extends type theory with definitional
functor laws, equations satisfied propositionally by a large class of
container-like type constructors $F : \operatorname{Type} \to
\operatorname{Type}$, equipped with a $\operatorname{map}_{F} : (A \to B) \to
F\ A \to F\ B$, such as lists or trees. Promoting these equations to
definitional ones strengthen the theory, enabling slicker proofs and more
automation for functorial type constructors. This extension is used to
modularly justify a structural form of coercive subtyping, propagating
subtyping through type formers in a map-like fashion. We show that the
resulting notion of coercive subtyping, thanks to the extra definitional
equations, is equivalent to a natural and implicit form of subsumptive
subtyping. The key result of decidability of type-checking in a dependent type
system with functor laws for lists has been entirely mechanized in Coq.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14931" title="Abstract">arXiv:2310.14931</a> [<a href="/pdf/2310.14931" title="Download PDF">pdf</a>, <a href="/ps/2310.14931" title="Download PostScript">ps</a>, <a href="/format/2310.14931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Derivative for Shallow Water Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ngom%2C+M+G">Mame Gor Ngom</a>, 
<a href="/search/math?searchtype=author&query=Faye%2C+I">Ibrahima Faye</a>, 
<a href="/search/math?searchtype=author&query=Seck%2C+D">Diaraf Seck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Coastal erosion is a major and growing environmental problem describing the
movement of sand caused by tides, waves or currents. Several phenomena
contribute to the significant advance of the sea. These include climate change,
with rising sea levels due to the melting of ice at the Earth's poles, the
amplification of the tidal effect, leading to the transport of large masses of
sand, storms, etc. We contribute to this problem by using topological shape
optimization techniques applied to an PDE describing coastal erosion. We use
Shallow water equations as a model.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14932" title="Abstract">arXiv:2310.14932</a> [<a href="/pdf/2310.14932" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating the Ocean with DRL: Path following for marine vessels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jose%2C+J">Joel Jose</a>, 
<a href="/search/eess?searchtype=author&query=Alam%2C+M+S">Md Shadab Alam</a>, 
<a href="/search/eess?searchtype=author&query=Somayajula%2C+A+S">Abhilash Sharma Somayajula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the Sixth International Conference in Ocean Engineering (ICOE2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Human error is a substantial factor in marine accidents, accounting for 85%
of all reported incidents. By reducing the need for human intervention in
vessel navigation, AI-based methods can potentially reduce the risk of
accidents. AI techniques, such as Deep Reinforcement Learning (DRL), have the
potential to improve vessel navigation in challenging conditions, such as in
restricted waterways and in the presence of obstacles. This is because DRL
algorithms can optimize multiple objectives, such as path following and
collision avoidance, while being more efficient to implement compared to
traditional methods. In this study, a DRL agent is trained using the Deep
Deterministic Policy Gradient (DDPG) algorithm for path following and waypoint
tracking. Furthermore, the trained agent is evaluated against a traditional PD
controller with an Integral Line of Sight (ILOS) guidance system for the same.
This study uses the Kriso Container Ship (KCS) as a test case for evaluating
the performance of different controllers. The ship's dynamics are modeled using
the maneuvering Modelling Group (MMG) model. This mathematical simulation is
used to train a DRL-based controller and to tune the gains of a traditional PD
controller. The simulation environment is also used to assess the controller's
effectiveness in the presence of wind.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14935" title="Abstract">arXiv:2310.14935</a> [<a href="/pdf/2310.14935" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal machine learning for single-cell genomics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tejada-Lapuerta%2C+A">Alejandro Tejada-Lapuerta</a>, 
<a href="/search/cs?searchtype=author&query=Bertin%2C+P">Paul Bertin</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+S">Stefan Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Aliee%2C+H">Hananeh Aliee</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Theis%2C+F+J">Fabian J. Theis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures, 3 tables, 1 box
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Genomics (q-bio.GN)

</div>
<p class="mathjax">Advances in single-cell omics allow for unprecedented insights into the
transcription profiles of individual cells. When combined with large-scale
perturbation screens, through which specific biological mechanisms can be
targeted, these technologies allow for measuring the effect of targeted
perturbations on the whole transcriptome. These advances provide an opportunity
to better understand the causative role of genes in complex biological
processes such as gene regulation, disease progression or cellular development.
However, the high-dimensional nature of the data, coupled with the intricate
complexity of biological systems renders this task nontrivial. Within the
machine learning community, there has been a recent increase of interest in
causality, with a focus on adapting established causal techniques and
algorithms to handle high-dimensional data. In this perspective, we delineate
the application of these methodologies within the realm of single-cell genomics
and their challenges. We first present the model that underlies most of current
causal approaches to single-cell biology and discuss and challenge the
assumptions it entails from the biological point of view. We then identify open
problems in the application of causal approaches to single-cell data:
generalising to unseen environments, learning interpretable models, and
learning causal models of dynamics. For each problem, we discuss how various
research directions - including the development of computational approaches and
the adaptation of experimental protocols - may offer ways forward, or on the
contrary pose some difficulties. With the advent of single cell atlases and
increasing perturbation data, we expect causal models to become a crucial tool
for informed experimental design.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14938" title="Abstract">arXiv:2310.14938</a> [<a href="/pdf/2310.14938" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI on the Water: Applying DRL to Autonomous Vessel Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Alam%2C+M+S">Md Shadab Alam</a>, 
<a href="/search/eess?searchtype=author&query=Sudha%2C+S+K+R">Sanjeev Kumar Ramkumar Sudha</a>, 
<a href="/search/eess?searchtype=author&query=Somayajula%2C+A">Abhilash Somayajula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the Sixth International Conference in Ocean Engineering (ICOE2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Human decision-making errors cause a majority of globally reported marine
accidents. As a result, automation in the marine industry has been gaining more
attention in recent years. Obstacle avoidance becomes very challenging for an
autonomous surface vehicle in an unknown environment. We explore the
feasibility of using Deep Q-Learning (DQN), a deep reinforcement learning
approach, for controlling an underactuated autonomous surface vehicle to follow
a known path while avoiding collisions with static and dynamic obstacles. The
ship's motion is described using a three-degree-of-freedom (3-DOF) dynamic
model. The KRISO container ship (KCS) is chosen for this study because it is a
benchmark hull used in several studies, and its hydrodynamic coefficients are
readily available for numerical modelling. This study shows that Deep
Reinforcement Learning (DRL) can achieve path following and collision avoidance
successfully and can be a potential candidate that may be investigated further
to achieve human-level or even better decision-making for autonomous marine
vehicles.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14939" title="Abstract">arXiv:2310.14939</a> [<a href="/pdf/2310.14939" title="Download PDF">pdf</a>, <a href="/format/2310.14939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressivity and Complexity of the Conjunctive Core of the SIGNAL  Process Query Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kampik%2C+T">Timotheus Kampik</a>, 
<a href="/search/cs?searchtype=author&query=Okulmus%2C+C">Cem Okulmus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">This paper presents a formalisation and expressivity and complexity analysis
of SIGNAL, an industry-scale query language for analysing business process
event logs. The formal analysis shows that the core capabilities of SIGNAL,
which we refer to as the SIGNAL Conjunctive Core, are more expressive than
relational algebra and thus not captured by standard relational databases. We
do provide an upper-bound on the expressiveness via a reduction to
semi-positive Datalog, which also leads to an upper bound of P-hard for the
data complexity of evaluating SIGNAL Conjunctive Core queries. The findings
provide first insights into how real-world process query languages are
fundamentally different from the more generally prevalent structured query
languages for querying relational databases and provide a rigorous foundation
for extending the existing capabilities of the industry-scale state-of-the-art
of process data querying.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14940" title="Abstract">arXiv:2310.14940</a> [<a href="/pdf/2310.14940" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of path following in ships using modern and traditional  controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sudha%2C+S+K+R">Sanjeev Kumar Ramkumar Sudha</a>, 
<a href="/search/eess?searchtype=author&query=Alam%2C+M+S">Md Shadab Alam</a>, 
<a href="/search/eess?searchtype=author&query=Reddy%2C+B">Bindusara Reddy</a>, 
<a href="/search/eess?searchtype=author&query=Somayajula%2C+A+S">Abhilash Sharma Somayajula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the Sixth International Conference in Ocean Engineering (ICOE2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Vessel navigation is difficult in restricted waterways and in the presence of
static and dynamic obstacles. This difficulty can be attributed to the
high-level decisions taken by humans during these maneuvers, which is evident
from the fact that 85% of the reported marine accidents are traced back to
human errors. Artificial intelligence-based methods offer us a way to eliminate
human intervention in vessel navigation. Newer methods like Deep Reinforcement
Learning (DRL) can optimize multiple objectives like path following and
collision avoidance at the same time while being computationally cheaper to
implement in comparison to traditional approaches. Before addressing the
challenge of collision avoidance along with path following, the performance of
DRL-based controllers on the path following task alone must be established.
Therefore, this study trains a DRL agent using Proximal Policy Optimization
(PPO) algorithm and tests it against a traditional PD controller guided by an
Integral Line of Sight (ILOS) guidance system. The Krisco Container Ship (KCS)
is chosen to test the different controllers. The ship dynamics are
mathematically simulated using the Maneuvering Modelling Group (MMG) model
developed by the Japanese. The simulation environment is used to train the deep
reinforcement learning-based controller and is also used to tune the gains of
the traditional PD controller. The effectiveness of the controllers in the
presence of wind is also investigated.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14941" title="Abstract">arXiv:2310.14941</a> [<a href="/pdf/2310.14941" title="Download PDF">pdf</a>, <a href="/ps/2310.14941" title="Download PostScript">ps</a>, <a href="/format/2310.14941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is dynamic dedicated path protection tractable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Szcze%C5%9Bniak%2C+I">Ireneusz Szcze&#x15b;niak</a>, 
<a href="/search/cs?searchtype=author&query=Olszewski%2C+I">Ireneusz Olszewski</a>, 
<a href="/search/cs?searchtype=author&query=Wo%C5%BAna-Szcze%C5%9Bniak%2C+B">Bo&#x17c;ena Wo&#x17a;na-Szcze&#x15b;niak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Intractable is the problem of finding two link-disjoint paths of minimal cost
if the path cost is limited since it can be a special case of the partition
problem. In optical networks, this limit can be introduced by the signal
modulation reach. Even without this limit, the existing literature suggested
the problem intractable because of the spectrum continuity and contiguity
constraints, but we show that the problem can be solved exactly with the
recently-proposed generic Dijkstra algorithm over a polynomially-bounded search
space, thus proving the problem tractable.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14942" title="Abstract">arXiv:2310.14942</a> [<a href="/pdf/2310.14942" title="Download PDF">pdf</a>, <a href="/format/2310.14942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Watermark: Effective and Harmless Dataset Copyright Protection is  Closed at Hand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Junfeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiming Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lixu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by NeurIPS 2023. The first two authors contributed equally to this work. 30 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The prosperity of deep neural networks (DNNs) is largely benefited from
open-source datasets, based on which users can evaluate and improve their
methods. In this paper, we revisit backdoor-based dataset ownership
verification (DOV), which is currently the only feasible approach to protect
the copyright of open-source datasets. We reveal that these methods are
fundamentally harmful given that they could introduce malicious
misclassification behaviors to watermarked DNNs by the adversaries. In this
paper, we design DOV from another perspective by making watermarked models
(trained on the protected dataset) correctly classify some `hard' samples that
will be misclassified by the benign model. Our method is inspired by the
generalization property of DNNs, where we find a \emph{hardly-generalized
domain} for the original dataset (as its \emph{domain watermark}). It can be
easily learned with the protected dataset containing modified samples.
Specifically, we formulate the domain generation as a bi-level optimization and
propose to optimize a set of visually-indistinguishable clean-label modified
data with similar effects to domain-watermarked samples from the
hardly-generalized domain to ensure watermark stealthiness. We also design a
hypothesis-test-guided ownership verification via our domain watermark and
provide the theoretical analyses of our method. Extensive experiments on three
benchmark datasets are conducted, which verify the effectiveness of our method
and its resistance to potential adaptive methods. The code for reproducing main
experiments is available at
\url{https://github.com/JunfengGo/Domain-Watermark}.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14944" title="Abstract">arXiv:2310.14944</a> [<a href="/pdf/2310.14944" title="Download PDF">pdf</a>, <a href="/format/2310.14944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Event based Prediction Suffix Tree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrew%2C+E">Evie Andrew</a>, 
<a href="/search/cs?searchtype=author&query=Monk%2C+T">Travis Monk</a>, 
<a href="/search/cs?searchtype=author&query=van+Schaik%2C+A">Andr&#xe9; van Schaik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This article introduces the Event based Prediction Suffix Tree (EPST), a
biologically inspired, event-based prediction algorithm. The EPST learns a
model online based on the statistics of an event based input and can make
predictions over multiple overlapping patterns. The EPST uses a representation
specific to event based data, defined as a portion of the power set of event
subsequences within a short context window. It is explainable, and possesses
many promising properties such as fault tolerance, resistance to event noise,
as well as the capability for one-shot learning. The computational features of
the EPST are examined in a synthetic data prediction task with additive event
noise, event jitter, and dropout. The resulting algorithm outputs predicted
projections for the near term future of the signal, which may be applied to
tasks such as event based anomaly detection or pattern recognition.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14945" title="Abstract">arXiv:2310.14945</a> [<a href="/pdf/2310.14945" title="Download PDF">pdf</a>, <a href="/format/2310.14945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computationally Efficient Electromagnetic Transient Power System Studies  using Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Leterme%2C+W">Willem Leterme</a>, 
<a href="/search/eess?searchtype=author&query=Heylen%2C+E">Evelyn Heylen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The power system of the future will be governed by complex interactions and
non-linear phenomena, that should be studied more and more through
computationally expensive software simulations. To solve the abovementioned
problems, power system engineers face problems with following characteristics:
(i) a computationally expensive simulator, (ii) non-linear functions to
optimize and (iii) lack of abundance of data. Existing optimization settings
involving EMT-type simulations have been developed, but mainly use a
deterministic model and optimizer, which may be computationally inefficient and
do not guarantee finding a global optimum. In this paper, an automation
framework based on Bayesian Optimization is introduced, and applied to two case
studies. It is found that the framework has the potential to reduce
computational effort, outperform deterministic optimizers and is applicable to
a multitude of problems. Nevertheless, it was found that the output of the
Bayesian Optimization depends on the number of evaluations used for
initialization, and in addition, careful selection of surrogate models, which
should be subject to future investigation.
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14946" title="Abstract">arXiv:2310.14946</a> [<a href="/pdf/2310.14946" title="Download PDF">pdf</a>, <a href="/format/2310.14946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intuitive Multilingual Audio-Visual Speech Recognition with a  Single-Trained Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Joanna Hong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S+J">Se Jin Park</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+Y+M">Yong Man Ro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present a novel approach to multilingual audio-visual speech recognition
tasks by introducing a single model on a multilingual dataset. Motivated by a
human cognitive system where humans can intuitively distinguish different
languages without any conscious effort or guidance, we propose a model that can
capture which language is given as an input speech by distinguishing the
inherent similarities and differences between languages. To do so, we design a
prompt fine-tuning technique into the largely pre-trained audio-visual
representation model so that the network can recognize the language class as
well as the speech with the corresponding language. Our work contributes to
developing robust and efficient multilingual audio-visual speech recognition
systems, reducing the need for language-specific models.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14947" title="Abstract">arXiv:2310.14947</a> [<a href="/pdf/2310.14947" title="Download PDF">pdf</a>, <a href="/format/2310.14947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System Combination via Quality Estimation for Grammatical Error  Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qorib%2C+M+R">Muhammad Reza Qorib</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+H+T">Hwee Tou Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Quality estimation models have been developed to assess the corrections made
by grammatical error correction (GEC) models when the reference or
gold-standard corrections are not available. An ideal quality estimator can be
utilized to combine the outputs of multiple GEC systems by choosing the best
subset of edits from the union of all edits proposed by the GEC base systems.
However, we found that existing GEC quality estimation models are not good
enough in differentiating good corrections from bad ones, resulting in a low
F0.5 score when used for system combination. In this paper, we propose GRECO, a
new state-of-the-art quality estimation model that gives a better estimate of
the quality of a corrected sentence, as indicated by having a higher
correlation to the F0.5 score of a corrected sentence. It results in a combined
GEC system with a higher F0.5 score. We also propose three methods for
utilizing GEC quality estimation models for system combination with varying
generality: model-agnostic, model-agnostic with voting bias, and
model-dependent method. The combined GEC system outperforms the state of the
art on the CoNLL-2014 test set and the BEA-2019 test set, achieving the highest
F0.5 scores published to date.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14948" title="Abstract">arXiv:2310.14948</a> [<a href="/pdf/2310.14948" title="Download PDF">pdf</a>, <a href="/format/2310.14948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Graph Convolutional Networks: Towards a generalized  framework for complex geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chenaud%2C+M">Marien Chenaud</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+J">Jos&#xe9; Alves</a>, 
<a href="/search/cs?searchtype=author&query=Magoul%C3%A8s%2C+F">Fr&#xe9;d&#xe9;ric Magoul&#xe8;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Civil-Comp Conferences, Volume 5, Paper 4.2, Civil-Comp Press,
  Edinburgh, United Kingdom, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mathematical Physics (math-ph); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">Since the seminal work of [9] and their Physics-Informed neural networks
(PINNs), many efforts have been conducted towards solving partial differential
equations (PDEs) with Deep Learning models. However, some challenges remain,
for instance the extension of such models to complex three-dimensional
geometries, and a study on how such approaches could be combined to classical
numerical solvers. In this work, we justify the use of graph neural networks
for these problems, based on the similarity between these architectures and the
meshes used in traditional numerical techniques for solving partial
differential equations. After proving an issue with the Physics-Informed
framework for complex geometries, during the computation of PDE residuals, an
alternative procedure is proposed, by combining classical numerical solvers and
the Physics-Informed framework. Finally, we propose an implementation of this
approach, that we test on a three-dimensional problem on an irregular geometry.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14949" title="Abstract">arXiv:2310.14949</a> [<a href="/pdf/2310.14949" title="Download PDF">pdf</a>, <a href="/format/2310.14949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Maintenance Model Based on Anomaly Detection in Induction  Motors: A Machine Learning Approach Using Real-Time IoT Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chevtchenko%2C+S+F">Sergio F. Chevtchenko</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+M+C+M+d">Monalisa C. M. dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Vieira%2C+D+M">Diego M. Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Mota%2C+R+L">Ricardo L. Mota</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+E">Elisson Rocha</a>, 
<a href="/search/cs?searchtype=author&query=Cruz%2C+B+V">Bruna V. Cruz</a>, 
<a href="/search/cs?searchtype=author&query=Ara%C3%BAjo%2C+D">Danilo Ara&#xfa;jo</a>, 
<a href="/search/cs?searchtype=author&query=Andrade%2C+E">Ermeson Andrade</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the support of Internet of Things (IoT) devices, it is possible to
acquire data from degradation phenomena and design data-driven models to
perform anomaly detection in industrial equipment. This approach not only
identifies potential anomalies but can also serve as a first step toward
building predictive maintenance policies. In this work, we demonstrate a novel
anomaly detection system on induction motors used in pumps, compressors, fans,
and other industrial machines. This work evaluates a combination of
pre-processing techniques and machine learning (ML) models with a low
computational cost. We use a combination of pre-processing techniques such as
Fast Fourier Transform (FFT), Wavelet Transform (WT), and binning, which are
well-known approaches for extracting features from raw data. We also aim to
guarantee an optimal balance between multiple conflicting parameters, such as
anomaly detection rate, false positive rate, and inference speed of the
solution. To this end, multiobjective optimization and analysis are performed
on the evaluated models. Pareto-optimal solutions are presented to select which
models have the best results regarding classification metrics and computational
effort. Differently from most works in this field that use publicly available
datasets to validate their models, we propose an end-to-end solution combining
low-cost and readily available IoT sensors. The approach is validated by
acquiring a custom dataset from induction motors. Also, we fuse vibration,
temperature, and noise data from these sensors as the input to the proposed ML
model. Therefore, we aim to propose a methodology general enough to be applied
in different industrial contexts in the future.
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14951" title="Abstract">arXiv:2310.14951</a> [<a href="/pdf/2310.14951" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of Caching Mechanisms to Improve Hadoop Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghazali%2C+R">Rana Ghazali</a>, 
<a href="/search/cs?searchtype=author&query=Down%2C+D+G">Douglas G.Down</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Nowadays distributed computing environments, large amounts of data are
generated from different resources with a high velocity, rendering the data
difficult to capture, manage, and process within existing relational databases.
Hadoop is a tool to store and process large datasets in a parallel manner
across a cluster of machines in a distributed environment. Hadoop brings many
benefits like flexibility, scalability, and high fault tolerance; however, it
faces some challenges in terms of data access time, I/O operation, and
duplicate computations resulting in extra overhead, resource wastage, and poor
performance. Many researchers have utilized caching mechanisms to tackle these
challenges. For example, they have presented approaches to improve data access
time, enhance data locality rate, remove repetitive calculations, reduce the
number of I/O operations, decrease the job execution time, and increase
resource efficiency. In the current study, we provide a comprehensive overview
of caching strategies to improve Hadoop performance. Additionally, a novel
classification is introduced based on cache utilization. Using this
classification, we analyze the impact on Hadoop performance and discuss the
advantages and disadvantages of each group. Finally, a novel hybrid approach
called Hybrid Intelligent Cache (HIC) that combines the benefits of two methods
from different groups, H-SVM-LRU and CLQLMRS, is presented. Experimental
results show that our hybrid method achieves an average improvement of 31.2% in
job execution time.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14952" title="Abstract">arXiv:2310.14952</a> [<a href="/pdf/2310.14952" title="Download PDF">pdf</a>, <a href="/format/2310.14952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 8+8=4: Formalizing Time Units to Handle Symbolic Music Durations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karystinaios%2C+E">Emmanouil Karystinaios</a>, 
<a href="/search/cs?searchtype=author&query=Foscarin%2C+F">Francesco Foscarin</a>, 
<a href="/search/cs?searchtype=author&query=Jacquemard%2C+F">Florent Jacquemard</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+M">Masahiko Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Tojo%2C+S">Satoshi Tojo</a>, 
<a href="/search/cs?searchtype=author&query=Widmer%2C+G">Gerhard Widmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the International Symposium on Computer Music Multidisciplinary Research (CMMR 2023), Tokyo, Japan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper focuses on the nominal durations of musical events (notes and
rests) in a symbolic musical score, and on how to conveniently handle these in
computer applications. We propose the usage of a temporal unit that is directly
related to the graphical symbols in musical scores and pair this with a set of
operations that cover typical computations in music applications. We formalize
this time unit and the more commonly used approach in a single mathematical
framework, as semirings, algebraic structures that enable an abstract
description of algorithms/processing pipelines. We then discuss some practical
use cases and highlight when our system can improve such pipelines by making
them more efficient in terms of data type used and the number of computations.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14954" title="Abstract">arXiv:2310.14954</a> [<a href="/pdf/2310.14954" title="Download PDF">pdf</a>, <a href="/format/2310.14954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key Frame Mechanism For Efficient Conformer Based End-to-end Speech  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+P">Peng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+C">Changhao Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Sining Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qing Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been accepted by IEEE Signal Processing Letters for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recently, Conformer as a backbone network for end-to-end automatic speech
recognition achieved state-of-the-art performance. The Conformer block
leverages a self-attention mechanism to capture global information, along with
a convolutional neural network to capture local information, resulting in
improved performance. However, the Conformer-based model encounters an issue
with the self-attention mechanism, as computational complexity grows
quadratically with the length of the input sequence. Inspired by previous
Connectionist Temporal Classification (CTC) guided blank skipping during
decoding, we introduce intermediate CTC outputs as guidance into the
downsampling procedure of the Conformer encoder. We define the frame with
non-blank output as key frame. Specifically, we introduce the key frame-based
self-attention (KFSA) mechanism, a novel method to reduce the computation of
the self-attention mechanism using key frames. The structure of our proposed
approach comprises two encoders. Following the initial encoder, we introduce an
intermediate CTC loss function to compute the label frame, enabling us to
extract the key frames and blank frames for KFSA. Furthermore, we introduce the
key frame-based downsampling (KFDS) mechanism to operate on high-dimensional
acoustic features directly and drop the frames corresponding to blank labels,
which results in new acoustic feature sequences as input to the second encoder.
By using the proposed method, which achieves comparable or higher performance
than vanilla Conformer and other similar work such as Efficient Conformer.
Meantime, our proposed method can discard more than 60\% useless frames during
model training and inference, which will accelerate the inference speed
significantly. This work code is available in
{https://github.com/scufan1990/Key-Frame-Mechanism-For-Efficient-Conformer}
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14957" title="Abstract">arXiv:2310.14957</a> [<a href="/pdf/2310.14957" title="Download PDF">pdf</a>, <a href="/format/2310.14957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XTSC-Bench: Quantitative Benchmarking for Explainers on Time Series  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%B6llig%2C+J">Jacqueline H&#xf6;llig</a>, 
<a href="/search/cs?searchtype=author&query=Thoma%2C+S">Steffen Thoma</a>, 
<a href="/search/cs?searchtype=author&query=Grimm%2C+F">Florian Grimm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICMLA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Despite the growing body of work on explainable machine learning in time
series classification (TSC), it remains unclear how to evaluate different
explainability methods. Resorting to qualitative assessment and user studies to
evaluate explainers for TSC is difficult since humans have difficulties
understanding the underlying information contained in time series data.
Therefore, a systematic review and quantitative comparison of explanation
methods to confirm their correctness becomes crucial. While steps to
standardized evaluations were taken for tabular, image, and textual data,
benchmarking explainability methods on time series is challenging due to a)
traditional metrics not being directly applicable, b) implementation and
adaption of traditional metrics for time series in the literature vary, and c)
varying baseline implementations. This paper proposes XTSC-Bench, a
benchmarking tool providing standardized datasets, models, and metrics for
evaluating explanation methods on TSC. We analyze 3 perturbation-, 6 gradient-
and 2 example-based explanation methods to TSC showing that improvements in the
explainers' robustness and reliability are necessary, especially for
multivariate data.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14958" title="Abstract">arXiv:2310.14958</a> [<a href="/pdf/2310.14958" title="Download PDF">pdf</a>, <a href="/format/2310.14958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Real-World Image De-Weathering with Imperfect Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhilu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaohe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chaoyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaotao Wang</a>, 
<a href="/search/cs?searchtype=author&query=LEI%2C+L">LEI LEI</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Real-world image de-weathering aims at removing various undesirable
weather-related artifacts. Owing to the impossibility of capturing image pairs
concurrently, existing real-world de-weathering datasets often exhibit
inconsistent illumination, position, and textures between the ground-truth
images and the input degraded images, resulting in imperfect supervision. Such
non-ideal supervision negatively affects the training process of learning-based
de-weathering methods. In this work, we attempt to address the problem with a
unified solution for various inconsistencies. Specifically, inspired by
information bottleneck theory, we first develop a Consistent Label Constructor
(CLC) to generate a pseudo-label as consistent as possible with the input
degraded image while removing most weather-related degradations. In particular,
multiple adjacent frames of the current input are also fed into CLC to enhance
the pseudo-label. Then we combine the original imperfect labels and
pseudo-labels to jointly supervise the de-weathering model by the proposed
Information Allocation Strategy (IAS). During testing, only the de-weathering
model is used for inference. Experiments on two real-world de-weathering
datasets show that our method helps existing de-weathering models achieve
better performance. Codes are available at
https://github.<a href="/abs/com/1180300">com/1180300</a>419/imperfect-deweathering.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14959" title="Abstract">arXiv:2310.14959</a> [<a href="/pdf/2310.14959" title="Download PDF">pdf</a>, <a href="/format/2310.14959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bipartite ShockHash: Pruning ShockHash Search for Efficient Perfect  Hashing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+H">Hans-Peter Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Sanders%2C+P">Peter Sanders</a>, 
<a href="/search/cs?searchtype=author&query=Walzer%2C+S">Stefan Walzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">A minimal perfect hash function (MPHF) maps a set of n keys to the first n
integers without collisions. Representing this bijection needs at least
$\log_2(e) \approx 1.443$ bits per key, and there is a wide range of practical
implementations achieving about 2 bits per key. Minimal perfect hashing is a
key ingredient in many compact data structures such as updatable retrieval data
structures and approximate membership data structures.
<br />A simple implementation reaching the space lower bound is to sample random
hash functions using brute-force, which needs about $e^n \approx 2.718^n$ tries
in expectation. ShockHash recently reduced that to about $(e/2)^n \approx
1.359^n$ tries in expectation by sampling random graphs. With bipartite
ShockHash, we now sample random bipartite graphs. In this paper, we describe
the general algorithmic ideas of bipartite ShockHash and give an experimental
evaluation. The key insight is that we can try all combinations of two hash
functions, each mapping into one half of the output range. This reduces the
number of sampled hash functions to only about $(\sqrt{e/2})^n \approx 1.166^n$
in expectation. In itself, this does not reduce the asymptotic running time
much because all combinations still need to be tested. However, by filtering
the candidates before combining them, we can reduce this to less than $1.175^n$
combinations in expectation.
<br />Our implementation of bipartite ShockHash is up to 3 orders of magnitude
faster than original ShockHash. Inside the RecSplit framework, bipartite
ShockHash-RS enables significantly larger base cases, leading to a construction
that is, depending on the allotted space budget, up to 20 times faster. In our
most extreme configuration, ShockHash-RS can build an MPHF for 10 million keys
with 1.489 bits per key (within 3.3% of the lower bound) in about half an hour,
pushing the limits of what is possible.
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14960" title="Abstract">arXiv:2310.14960</a> [<a href="/pdf/2310.14960" title="Download PDF">pdf</a>, <a href="/format/2310.14960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Outlier Detection Method Based on Local Entropy and Global  Density
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaituo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuhua Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">By now, most outlier-detection algorithms struggle to accurately detect both
point anomalies and cluster anomalies simultaneously. Furthermore, a few
K-nearest-neighbor-based anomaly-detection methods exhibit excellent
performance on many datasets, but their sensitivity to the value of K is a
critical issue that needs to be addressed. To address these challenges, we
propose a novel robust anomaly detection method, called Entropy Density Ratio
Outlier Detection (EDROD). This method incorporates the probability density of
each sample as the global feature, and the local entropy around each sample as
the local feature, to obtain a comprehensive indicator of abnormality for each
sample, which is called Entropy Density Ratio (EDR) for short in this paper. By
comparing several competing anomaly detection methods on both synthetic and
real-world datasets, it is found that the EDROD method can detect both point
anomalies and cluster anomalies simultaneously with accurate performance. In
addition, it is also found that the EDROD method exhibits strong robustness to
the number of selected neighboring samples, the dimension of samples in the
dataset, and the size of the dataset. Therefore, the proposed EDROD method can
be applied to a variety of real-world datasets to detect anomalies with
accurate and robust performances.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14963" title="Abstract">arXiv:2310.14963</a> [<a href="/pdf/2310.14963" title="Download PDF">pdf</a>, <a href="/format/2310.14963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adam through a Second-Order Lens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clarke%2C+R+M">Ross M. Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+B">Baiyu Su</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 15 figures, 4 tables. Submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Research into optimisation for deep learning is characterised by a tension
between the computational efficiency of first-order, gradient-based methods
(such as SGD and Adam) and the theoretical efficiency of second-order,
curvature-based methods (such as quasi-Newton methods and K-FAC). We seek to
combine the benefits of both approaches into a single computationally-efficient
algorithm. Noting that second-order methods often depend on stabilising
heuristics (such as Levenberg-Marquardt damping), we propose AdamQLR: an
optimiser combining damping and learning rate selection techniques from K-FAC
(Martens and Grosse, 2015) with the update directions proposed by Adam,
inspired by considering Adam through a second-order lens. We evaluate AdamQLR
on a range of regression and classification tasks at various scales, achieving
competitive generalisation performance vs runtime.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14968" title="Abstract">arXiv:2310.14968</a> [<a href="/pdf/2310.14968" title="Download PDF">pdf</a>, <a href="/format/2310.14968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fundamental Dilemma of Bayesian Active Meta-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sloman%2C+S+J">Sabina J. Sloman</a>, 
<a href="/search/cs?searchtype=author&query=Bharti%2C+A">Ayush Bharti</a>, 
<a href="/search/cs?searchtype=author&query=Kaski%2C+S">Samuel Kaski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Many applications involve estimation of parameters that generalize across
multiple diverse, but related, data-scarce task environments. Bayesian active
meta-learning, a form of sequential optimal experimental design, provides a
framework for solving such problems. The active meta-learner's goal is to gain
transferable knowledge (estimate the transferable parameters) in the presence
of idiosyncratic characteristics of the current task (task-specific
parameters). We show that in such a setting, greedy pursuit of this goal can
actually hurt estimation of the transferable parameters (induce so-called
negative transfer). The learner faces a dilemma akin to but distinct from the
exploration--exploitation dilemma: should they spend their acquisition budget
pursuing transferable knowledge, or identifying the current task-specific
parameters? We show theoretically that some tasks pose an inevitable and
arbitrarily large threat of negative transfer, and that task identification is
critical to reducing this threat. Our results generalize to analysis of prior
misspecification over nuisance parameters. Finally, we empirically illustrate
circumstances that lead to negative transfer.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14970" title="Abstract">arXiv:2310.14970</a> [<a href="/pdf/2310.14970" title="Download PDF">pdf</a>, <a href="/format/2310.14970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards LLM-driven Dialogue State Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yujie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zexin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+L">Liming Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao-Ming Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Dialogue State Tracking (DST) is of paramount importance in ensuring accurate
tracking of user goals and system actions within task-oriented dialogue
systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT
has sparked considerable interest in assessing their efficacy across diverse
applications. In this study, we conduct an initial examination of ChatGPT's
capabilities in DST. Our evaluation uncovers the exceptional performance of
ChatGPT in this task, offering valuable insights to researchers regarding its
capabilities and providing useful directions for designing and enhancing
dialogue systems. Despite its impressive performance, ChatGPT has significant
limitations including its closed-source nature, request restrictions, raising
data privacy concerns, and lacking local deployment capabilities. To address
these concerns, we present LDST, an LLM-driven DST framework based on smaller,
open-source foundation models. By utilizing a novel domain-slot instruction
tuning method, LDST achieves performance on par with ChatGPT. Comprehensive
evaluations across three distinct experimental settings, we find that LDST
exhibits remarkable performance improvements in both zero-shot and few-shot
setting compared to previous SOTA methods. The source code is provided for
reproducibility.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14971" title="Abstract">arXiv:2310.14971</a> [<a href="/pdf/2310.14971" title="Download PDF">pdf</a>, <a href="/format/2310.14971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penalty Decoding: Well Suppress the Self-Reinforcement Effect in  Open-Ended Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+H">Hongkun Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The decoding algorithm is critical for open-ended text generation,
transforming latent representations into coherent and meaningful outputs. This
paper investigates the self-reinforcement effect in text generation and the
effectiveness of a repetition penalty to mitigate it. However, determining the
optimal repetition penalty value is challenging. To tackle this, we propose a
forgetting mechanism that disregards distant tokens, reducing the burden of
penalty selection. In addition, we introduce a length penalty to address overly
short sentences caused by excessive penalties. Our penalty decoding approach
incorporating three strategies helps resolve issues with sampling methods
deviating from factual information. Experimental results demonstrate the
efficacy of our approach in generating high-quality sentences resembling human
output.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14975" title="Abstract">arXiv:2310.14975</a> [<a href="/pdf/2310.14975" title="Download PDF">pdf</a>, <a href="/format/2310.14975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The WHY in Business Processes: Discovery of Causal Execution  Dependencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fournier%2C+F">Fabiana Fournier</a>, 
<a href="/search/cs?searchtype=author&query=Limonad%2C+L">Lior Limonad</a>, 
<a href="/search/cs?searchtype=author&query=Skarbovsky%2C+I">Inna Skarbovsky</a>, 
<a href="/search/cs?searchtype=author&query=David%2C+Y">Yuval David</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A crucial element in predicting the outcomes of process interventions and
making informed decisions about the process is unraveling the genuine
relationships between the execution of process activities. Contemporary process
discovery algorithms exploit time precedence as their main source of model
derivation. Such reliance can sometimes be deceiving from a causal perspective.
This calls for faithful new techniques to discover the true execution
dependencies among the tasks in the process. To this end, our work offers a
systematic approach to the unveiling of the true causal business process by
leveraging an existing causal discovery algorithm over activity timing. In
addition, this work delves into a set of conditions under which process mining
discovery algorithms generate a model that is incongruent with the causal
business process model, and shows how the latter model can be methodologically
employed for a sound analysis of the process. Our methodology searches for such
discrepancies between the two models in the context of three causal patterns,
and derives a new view in which these inconsistencies are annotated over the
mined process model. We demonstrate our methodology employing two open process
mining algorithms, the IBM Process Mining tool, and the LiNGAM causal discovery
technique. We apply it on a synthesized dataset and on two open benchmark data
sets.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14976" title="Abstract">arXiv:2310.14976</a> [<a href="/pdf/2310.14976" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement learning in large, structured action spaces: A simulation  study of decision support for spinal cord injury rehabilitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phelps%2C+N">Nathan Phelps</a>, 
<a href="/search/cs?searchtype=author&query=Marrocco%2C+S">Stephanie Marrocco</a>, 
<a href="/search/cs?searchtype=author&query=Cornell%2C+S">Stephanie Cornell</a>, 
<a href="/search/cs?searchtype=author&query=Wolfe%2C+D+L">Dalton L. Wolfe</a>, 
<a href="/search/cs?searchtype=author&query=Lizotte%2C+D+J">Daniel J. Lizotte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning (RL) has helped improve decision-making in several
applications. However, applying traditional RL is challenging in some
applications, such as rehabilitation of people with a spinal cord injury (SCI).
Among other factors, using RL in this domain is difficult because there are
many possible treatments (i.e., large action space) and few patients (i.e.,
limited training data). Treatments for SCIs have natural groupings, so we
propose two approaches to grouping treatments so that an RL agent can learn
effectively from limited data. One relies on domain knowledge of SCI
rehabilitation and the other learns similarities among treatments using an
embedding technique. We then use Fitted Q Iteration to train an agent that
learns optimal treatments. Through a simulation study designed to reflect the
properties of SCI rehabilitation, we find that both methods can help improve
the treatment decisions of physiotherapists, but the approach based on domain
knowledge offers better performance. Our findings provide a "proof of concept"
that RL can be used to help improve the treatment of those with an SCI and
indicates that continued efforts to gather data and apply RL to this domain are
worthwhile.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14977" title="Abstract">arXiv:2310.14977</a> [<a href="/pdf/2310.14977" title="Download PDF">pdf</a>, <a href="/format/2310.14977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Counting in Generalized Turnstile Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dingyu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Databases (cs.DB); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Traditionally in the turnstile model of data streams, there is a state vector
$x=(x_1,x_2,\ldots,x_n)$ which is updated through a stream of pairs $(i,k)$
where $i\in [n]$ and $k\in \Z$. Upon receiving $(i,k)$, $x_i\gets x_i + k$. A
distinct count algorithm in the turnstile model takes one pass of the stream
and then estimates $\norm{x}_0 = |\{i\in[n]\mid x_i\neq 0\}|$ (aka $L_0$, the
Hamming norm).
<br />In this paper, we define a finite-field version of the turnstile model. Let
$F$ be any finite field. Then in the $F$-turnstile model, for each $i\in [n]$,
$x_i\in F$; for each update $(i,k)$, $k\in F$. The update $x_i\gets x_i+k$ is
then computed in the field $F$. A distinct count algorithm in the $F$-turnstile
model takes one pass of the stream and estimates $\norm{x}_{0;F} =
|\{i\in[n]\mid x_i\neq 0_F\}|$.
<br />We present a simple distinct count algorithm, called $F$-\pcsa{}, in the
$F$-turnstile model for any finite field $F$. The new $F$-\pcsa{} algorithm
takes $m\log(n)\log (|F|)$ bits of memory and estimates $\norm{x}_{0;F}$ with
$O(\frac{1}{\sqrt{m}})$ relative error where the hidden constant depends on the
order of the field.
<br />$F$-\pcsa{} is straightforward to implement and has several applications in
the real world with different choices of $F$. Most notably, it makes distinct
count with deletions as simple as distinct count without deletions.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14978" title="Abstract">arXiv:2310.14978</a> [<a href="/pdf/2310.14978" title="Download PDF">pdf</a>, <a href="/format/2310.14978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks  with TTFS Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Malu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">The biological neurons use precise spike times, in addition to the spike
firing rate, to communicate with each other. The time-to-first-spike (TTFS)
coding is inspired by such biological observation. However, there is a lack of
effective solutions for training TTFS-based spiking neural network (SNN). In
this paper, we put forward a simple yet effective network conversion algorithm,
which is referred to as LC-TTFS, by addressing two main problems that hinder an
effective conversion from a high-performance artificial neural network (ANN) to
a TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping
between the activation values of an ANN and the spike times of an SNN on a
number of challenging AI tasks, including image classification, image
reconstruction, and speech enhancement. With TTFS coding, we can achieve up to
orders of magnitude saving in computation over ANN and other rate-based SNNs.
The study, therefore, paves the way for deploying ultra-low-power TTFS-based
SNNs on power-constrained edge computing platforms.
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14979" title="Abstract">arXiv:2310.14979</a> [<a href="/pdf/2310.14979" title="Download PDF">pdf</a>, <a href="/format/2310.14979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACTOR: Active Learning with Annotator-specific Classification Heads to  Embrace Human Label Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Label aggregation such as majority voting is commonly used to resolve
annotator disagreement in dataset creation. However, this may disregard
minority values and opinions. Recent studies indicate that learning from
individual annotations outperforms learning from aggregated labels, though they
require a considerable amount of annotation. Active learning, as an annotation
cost-saving strategy, has not been fully explored in the context of learning
from disagreement. We show that in the active learning setting, a multi-head
model performs significantly better than a single-head model in terms of
uncertainty estimation. By designing and evaluating acquisition functions with
annotator-specific heads on two datasets, we show that group-level entropy
works generally well on both datasets. Importantly, it achieves performance in
terms of both prediction and uncertainty estimation comparable to full-scale
training from disagreement, while saving up to 70% of the annotation budget.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14981" title="Abstract">arXiv:2310.14981</a> [<a href="/pdf/2310.14981" title="Download PDF">pdf</a>, <a href="/format/2310.14981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fidelity-Enriched Contrastive Search: Reconciling the  Faithfulness-Diversity Trade-Off in Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Cheng-Kuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hsin-Hsi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chung-Chi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a short paper at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we address the hallucination problem commonly found in natural
language generation tasks. Language models often generate fluent and convincing
content but can lack consistency with the provided source, resulting in
potential inaccuracies. We propose a new decoding method called
Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive
search framework with context-aware regularization terms. FECS promotes tokens
that are semantically similar to the provided source while penalizing
repetitiveness in the generated text. We demonstrate its effectiveness across
two tasks prone to hallucination: abstractive summarization and dialogue
generation. Results show that FECS consistently enhances faithfulness across
various language model sizes while maintaining output diversity comparable to
well-performing decoding algorithms.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14982" title="Abstract">arXiv:2310.14982</a> [<a href="/pdf/2310.14982" title="Download PDF">pdf</a>, <a href="/format/2310.14982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delayed Memory Unit: Modelling Temporal Dependency Through Delay Gate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Pengfei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Malu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Devos%2C+P">Paul Devos</a>, 
<a href="/search/cs?searchtype=author&query=Botteldooren%2C+D">Dick Botteldooren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">Recurrent Neural Networks (RNNs) are renowned for their adeptness in modeling
temporal dependencies, a trait that has driven their widespread adoption for
sequential data processing. Nevertheless, vanilla RNNs are confronted with the
well-known issue of gradient vanishing and exploding, posing a significant
challenge for learning and establishing long-range dependencies. Additionally,
gated RNNs tend to be over-parameterized, resulting in poor network
generalization. To address these challenges, we propose a novel Delayed Memory
Unit (DMU) in this paper, wherein a delay line structure, coupled with delay
gates, is introduced to facilitate temporal interaction and temporal credit
assignment, so as to enhance the temporal modeling capabilities of vanilla
RNNs. Particularly, the DMU is designed to directly distribute the input
information to the optimal time instant in the future, rather than aggregating
and redistributing it over time through intricate network dynamics. Our
proposed DMU demonstrates superior temporal modeling capabilities across a
broad range of sequential modeling tasks, utilizing considerably fewer
parameters than other state-of-the-art gated RNN models in applications such as
speech recognition, radar gesture recognition, ECG waveform segmentation, and
permuted sequential image classification.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14984" title="Abstract">arXiv:2310.14984</a> [<a href="/pdf/2310.14984" title="Download PDF">pdf</a>, <a href="/format/2310.14984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> My Lockdown Escape: Sparking Self-Empathy in the Context of the Covid-19  Pandemic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tocchetti%2C+A">Andrea Tocchetti</a>, 
<a href="/search/cs?searchtype=author&query=Talenti%2C+S+M">Silvia Maria Talenti</a>, 
<a href="/search/cs?searchtype=author&query=Brambilla%2C+M">Marco Brambilla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">During the Covid-19 pandemic, research communities focused on collecting and
understanding people's behaviours and feelings to study and tackle the pandemic
indirect effects. Despite its consequences are slowly starting to fade away,
such an interest is still alive. In this article, we propose a hybrid,
gamified, story-driven data collection approach to spark self-empathy, hence
resurfacing people's past feelings. The game is designed to include a physical
board, decks of cards, and a digital application. As the player plays through
the game, they customize and escape from their lockdown room by completing
statements and answering a series of questions that define their story. The
decoration of the lockdown room and the storytelling-driven approach are
targeted at sparking people's emotions and self-empathy towards their past
selves. Ultimately, the proposed approach was proven effective in sparking and
collecting feelings, while a few improvements are still necessary.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14985" title="Abstract">arXiv:2310.14985</a> [<a href="/pdf/2310.14985" title="Download PDF">pdf</a>, <a href="/format/2310.14985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Based Agent Society Investigation: Collaboration and Confrontation  in Avalon Gameplay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yihuai Lan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiqiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Deheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+E">Ee-Peng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper aims to investigate the open research problem of uncovering the
social behaviors of LLM-based agents. To achieve this goal, we adopt Avalon, a
representative communication game, as the environment and use system prompts to
guide LLM agents to play the game. While previous studies have conducted
preliminary investigations into gameplay with LLM agents, there lacks research
on their social behaviors. In this paper, we present a novel framework designed
to seamlessly adapt to Avalon gameplay. The core of our proposed framework is a
multi-agent system that enables efficient communication and interaction among
agents. We evaluate the performance of our framework based on metrics from two
perspectives: winning the game and analyzing the social behaviors of LLM
agents. Our results demonstrate the effectiveness of our framework in
generating adaptive and intelligent agents and highlight the potential of
LLM-based agents in addressing the challenges associated with dynamic social
environment interaction. By analyzing the social behaviors of LLM agents from
the aspects of both collaboration and confrontation, we provide insights into
the research and applications of this domain.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14991" title="Abstract">arXiv:2310.14991</a> [<a href="/pdf/2310.14991" title="Download PDF">pdf</a>, <a href="/format/2310.14991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Impartial Selection with Weights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cembrano%2C+J">Javier Cembrano</a>, 
<a href="/search/cs?searchtype=author&query=Griesbach%2C+S+M">Svenja M. Griesbach</a>, 
<a href="/search/cs?searchtype=author&query=Stahlberg%2C+M+J">Maximilian J. Stahlberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the Proceedings of the 19th Conference on Web and Internet Economics (WINE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">In the impartial selection problem, a subset of agents up to a fixed size $k$
among a group of $n$ is to be chosen based on votes cast by the agents
themselves. A selection mechanism is impartial if no agent can influence its
own chance of being selected by changing its vote. It is $\alpha$-optimal if,
for every instance, the ratio between the votes received by the selected subset
is at least a fraction of $\alpha$ of the votes received by the subset of size
$k$ with the highest number of votes. We study deterministic impartial
mechanisms in a more general setting with arbitrarily weighted votes and
provide the first approximation guarantee, roughly $1/\lceil 2n/k\rceil$. When
the number of agents to select is large enough compared to the total number of
agents, this yields an improvement on the previously best known approximation
ratio of $1/k$ for the unweighted setting. We further show that our mechanism
can be adapted to the impartial assignment problem, in which multiple sets of
up to $k$ agents are to be selected, with a loss in the approximation ratio of
$1/2$.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14992" title="Abstract">arXiv:2310.14992</a> [<a href="/pdf/2310.14992" title="Download PDF">pdf</a>, <a href="/format/2310.14992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Regression Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Falconer%2C+T">Thomas Falconer</a>, 
<a href="/search/cs?searchtype=author&query=Kazempour%2C+J">Jalal Kazempour</a>, 
<a href="/search/cs?searchtype=author&query=Pinson%2C+P">Pierre Pinson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 11 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning tasks are vulnerable to the quality of data used as input.
Yet, it is often challenging for firms to obtain adequate datasets, with them
being naturally distributed amongst owners, that in practice, may be
competitors in a downstream market and reluctant to share information. Focusing
on supervised learning for regression tasks, we develop a \textit{regression
market} to provide a monetary incentive for data sharing. Our proposed
mechanism adopts a Bayesian framework, allowing us to consider a more general
class of regression tasks. We present a thorough exploration of the market
properties, and show that similar proposals in current literature expose the
market agents to sizeable financial risks, which can be mitigated in our
probabilistic setting.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14993" title="Abstract">arXiv:2310.14993</a> [<a href="/pdf/2310.14993" title="Download PDF">pdf</a>, <a href="/format/2310.14993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Inner Workings of Language Models Through  Representation Dissimilarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brown%2C+D">Davis Brown</a>, 
<a href="/search/cs?searchtype=author&query=Godfrey%2C+C">Charles Godfrey</a>, 
<a href="/search/cs?searchtype=author&query=Konz%2C+N">Nicholas Konz</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+J">Jonathan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Kvinge%2C+H">Henry Kvinge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">As language models are applied to an increasing number of real-world
applications, understanding their inner workings has become an important issue
in model trust, interpretability, and transparency. In this work we show that
representation dissimilarity measures, which are functions that measure the
extent to which two model's internal representations differ, can be a valuable
tool for gaining insight into the mechanics of language models. Among our
insights are: (i) an apparent asymmetry in the internal representations of
model using SoLU and GeLU activation functions, (ii) evidence that
dissimilarity measures can identify and locate generalization properties of
models that are invisible via in-distribution test set performance, and (iii)
new evaluations of how language model features vary as width and depth are
increased. Our results suggest that dissimilarity measures are a promising set
of tools for shedding light on the inner workings of language models.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14997" title="Abstract">arXiv:2310.14997</a> [<a href="/pdf/2310.14997" title="Download PDF">pdf</a>, <a href="/format/2310.14997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Hardware-Efficient PCFGs with Independent Left and Right  Productions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Songlin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+K">Kewei Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Scaling dense PCFGs to thousands of nonterminals via a low-rank
parameterization of the rule probability tensor has been shown to be beneficial
for unsupervised parsing. However, PCFGs scaled this way still perform poorly
as a language model, and even underperform similarly-sized HMMs. This work
introduces \emph{SimplePCFG}, a simple PCFG formalism with independent left and
right productions. Despite imposing a stronger independence assumption than the
low-rank approach, we find that this formalism scales more effectively both as
a language model and as an unsupervised parser. As an unsupervised parser, our
simple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language
model, it obtains a perplexity of 119.0, outperforming similarly-sized low-rank
PCFGs. We further introduce \emph{FlashInside}, a hardware IO-aware
implementation of the inside algorithm for efficiently scaling simple PCFGs.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15003" title="Abstract">arXiv:2310.15003</a> [<a href="/pdf/2310.15003" title="Download PDF">pdf</a>, <a href="/format/2310.15003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent  Geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Oc%C3%A1riz+Borde%2C+H+S">Haitz S&#xe1;ez de Oc&#xe1;riz Borde</a>, 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages + Appendix, 2 Figures, 9 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Discrete Mathematics (cs.DM); Neural and Evolutionary Computing (cs.NE); Metric Geometry (math.MG)

</div>
<p class="mathjax">The inductive bias of a graph neural network (GNN) is largely encoded in its
specified graph. Latent graph inference relies on latent geometric
representations to dynamically rewire or infer a GNN's graph to maximize the
GNN's predictive downstream performance, but it lacks solid theoretical
foundations in terms of embedding-based representation guarantees. This paper
addresses this issue by introducing a trainable deep learning architecture,
coined neural snowflake, that can adaptively implement fractal-like metrics on
$\mathbb{R}^d$. We prove that any given finite weights graph can be
isometrically embedded by a standard MLP encoder. Furthermore, when the latent
graph can be represented in the feature space of a sufficiently regular kernel,
we show that the combined neural snowflake and MLP encoder do not succumb to
the curse of dimensionality by using only a low-degree polynomial number of
parameters in the number of nodes. This implementation enables a
low-dimensional isometric embedding of the latent graph. We conduct synthetic
experiments to demonstrate the superior metric learning capabilities of neural
snowflakes when compared to more familiar spaces like Euclidean space.
Additionally, we carry out latent graph inference experiments on graph
benchmarks. Consistently, the neural snowflake model achieves predictive
performance that either matches or surpasses that of the state-of-the-art
latent graph inference models. Importantly, this performance improvement is
achieved without requiring random search for optimal latent geometry. Instead,
the neural snowflake model achieves this enhancement in a differentiable
manner.
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15004" title="Abstract">arXiv:2310.15004</a> [<a href="/pdf/2310.15004" title="Download PDF">pdf</a>, <a href="/format/2310.15004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Language Models Fall in Love: Animacy Processing in Transformer  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanna%2C+M">Michael Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>, 
<a href="/search/cs?searchtype=author&query=Pezzelle%2C+S">Sandro Pezzelle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Animacy - whether an entity is alive and sentient - is fundamental to
cognitive processing, impacting areas such as memory, vision, and language.
However, animacy is not always expressed directly in language: in English it
often manifests indirectly, in the form of selectional constraints on verbs and
adjectives. This poses a potential issue for transformer language models (LMs):
they often train only on text, and thus lack access to extralinguistic
information from which humans learn about animacy. We ask: how does this impact
LMs' animacy processing - do they still behave as humans do? We answer this
question using open-source LMs. Like previous studies, we find that LMs behave
much like humans when presented with entities whose animacy is typical.
However, we also show that even when presented with stories about atypically
animate entities, such as a peanut in love, LMs adapt: they treat these
entities as animate, though they do not adapt as well as humans. Even when the
context indicating atypical animacy is very short, LMs pick up on subtle clues
and change their behavior. We conclude that despite the limited signal through
which LMs can learn about animacy, they are indeed sensitive to the relevant
lexical semantic nuances available in English.
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15007" title="Abstract">arXiv:2310.15007</a> [<a href="/pdf/2310.15007" title="Download PDF">pdf</a>, <a href="/format/2310.15007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Did the Neurons Read your Book? Document-level Membership Inference for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meeus%2C+M">Matthieu Meeus</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shubham Jain</a>, 
<a href="/search/cs?searchtype=author&query=Rei%2C+M">Marek Rei</a>, 
<a href="/search/cs?searchtype=author&query=de+Montjoye%2C+Y">Yves-Alexandre de Montjoye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">With large language models (LLMs) poised to become embedded in our daily
lives, questions are starting to be raised about the dataset(s) they learned
from. These questions range from potential bias or misinformation LLMs could
retain from their training data to questions of copyright and fair use of
human-generated text. However, while these questions emerge, developers of the
recent state-of-the-art LLMs become increasingly reluctant to disclose details
on their training corpus. We here introduce the task of document-level
membership inference for real-world LLMs, i.e. inferring whether the LLM has
seen a given document during training or not. First, we propose a procedure for
the development and evaluation of document-level membership inference for LLMs
by leveraging commonly used data sources for training and the model release
date. We then propose a practical, black-box method to predict document-level
membership and instantiate it on OpenLLaMA-7B with both books and academic
papers. We show our methodology to perform very well, reaching an impressive
AUC of 0.856 for books and 0.678 for papers. We then show our approach to
outperform the sentence-level membership inference attacks used in the privacy
literature for the document-level membership task. We finally evaluate whether
smaller models might be less sensitive to document-level inference and show
OpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.
Taken together, our results show that accurate document-level membership can be
inferred for LLMs, increasing the transparency of technology poised to change
our lives.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15008" title="Abstract">arXiv:2310.15008</a> [<a href="/pdf/2310.15008" title="Download PDF">pdf</a>, <a href="/format/2310.15008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wonder3D: Single Image to 3D using Cross-Domain Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xiaoxiao Long</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuan-Chen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Cheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhiyang Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Song-Hai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Habermann%2C+M">Marc Habermann</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://www.xxlong.site/Wonder3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we introduce Wonder3D, a novel method for efficiently
generating high-fidelity textured meshes from single-view images.Recent methods
based on Score Distillation Sampling (SDS) have shown the potential to recover
3D geometry from 2D diffusion priors, but they typically suffer from
time-consuming per-shape optimization and inconsistent geometry. In contrast,
certain works directly produce 3D information via fast network inferences, but
their results are often of low quality and lack geometric details.To
holistically improve the quality, consistency, and efficiency of image-to-3D
tasks, we propose a cross-domain diffusion model that generates multi-view
normal maps and the corresponding color images. To ensure consistency, we
employ a multi-view cross-domain attention mechanism that facilitates
information exchange across views and modalities. Lastly, we introduce a
geometry-aware normal fusion algorithm that extracts high-quality surfaces from
the multi-view 2D representations. Our extensive evaluations demonstrate that
our method achieves high-quality reconstruction results, robust generalization,
and reasonably good efficiency compared to prior works.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15010" title="Abstract">arXiv:2310.15010</a> [<a href="/pdf/2310.15010" title="Download PDF">pdf</a>, <a href="/format/2310.15010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Depth for Ranking and Characterizing Transformer-Based Text  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seegmiller%2C+P">Parker Seegmiller</a>, 
<a href="/search/cs?searchtype=author&query=Preum%2C+S+M">Sarah Masud Preum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The popularity of transformer-based text embeddings calls for better
statistical tools for measuring distributions of such embeddings. One such tool
would be a method for ranking texts within a corpus by centrality, i.e.
assigning each text a number signifying how representative that text is of the
corpus as a whole. However, an intrinsic center-outward ordering of
high-dimensional text representations is not trivial. A statistical depth is a
function for ranking k-dimensional objects by measuring centrality with respect
to some observed k-dimensional distribution. We adopt a statistical depth to
measure distributions of transformer-based text embeddings, transformer-based
text embedding (TTE) depth, and introduce the practical use of this depth for
both modeling and distributional inference in NLP pipelines. We first define
TTE depth and an associated rank sum test for determining whether two corpora
differ significantly in embedding space. We then use TTE depth for the task of
in-context learning prompt selection, showing that this approach reliably
improves performance over statistical baseline approaches across six text
classification tasks. Finally, we use TTE depth and the associated rank sum
test to characterize the distributions of synthesized and human-generated
corpora, showing that five recent synthetic data augmentation processes cause a
measurable distributional shift away from associated human-generated text.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15011" title="Abstract">arXiv:2310.15011</a> [<a href="/pdf/2310.15011" title="Download PDF">pdf</a>, <a href="/ps/2310.15011" title="Download PostScript">ps</a>, <a href="/format/2310.15011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interference Management by Harnessing Multi-Domain Resources in  Spectrum-Sharing Aided Satellite-Ground Integrated Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiaojin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yue Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yulong Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gengxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Vehicular Technology, Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">A spectrum-sharing satellite-ground integrated network is conceived,
consisting of a pair of non-geostationary orbit (NGSO) constellations and
multiple terrestrial base stations, which impose the co-frequency interference
(CFI) on each other. The CFI may increase upon increasing the number of
satellites. To manage the potentially severe interference, we propose to rely
on joint multi-domain resource aided interference management (JMDR-IM).
Specifically, the coverage overlap of the constellations considered is
analyzed. Then, multi-domain resources - including both the beam-domain and
power-domain - are jointly utilized for managing the CFI in an overlapping
coverage region. This joint resource utilization is performed by relying on our
specifically designed beam-shut-off and switching based beam scheduling, as
well as on long short-term memory based joint autoregressive moving average
assisted deep Q network aided power scheduling. Moreover, the outage
probability (OP) of the proposed JMDR-IM scheme is derived, and the asymptotic
analysis of the OP is also provided. Our performance evaluations demonstrate
the superiority of the proposed JMDR-IM scheme in terms of its increased
throughput and reduced OP.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15012" title="Abstract">arXiv:2310.15012</a> [<a href="/pdf/2310.15012" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elemantra: An End-to-End Automated Framework Empowered with AI and IoT  for Tackling Human-Elephant Conflict in Elephant-Range Countries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bandara%2C+N+S">Nuwan Sriyantha Bandara</a>, 
<a href="/search/eess?searchtype=author&query=Bandara%2C+D+P">Dilshan Pramudith Bandara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Sensors Letters, 4 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The cohabitation of elephants and humans has evolved into a human-elephant
conflict (HEC) due to the increasing loss of historical elephant habitations.
Since HEC is a substantial threat to both species, advanced sensing methods are
utilized to develop HEC prevention frameworks which still lack unification.
Here, we propose an end-to-end automated framework for HEC prevention
consisting of three main modules: a distributed deep learning-assisted elephant
detection module using infrared and seismic sensing, an on-site repelling
system with time-varying acoustic and light deterrents and a mesh network for
device communication. The framework is equipped with novel decision-making
pipelines and algorithms such that it has the potential to operate with no
human intervention. The preliminary results from each module confirm that the
proposed framework is effective in performing their individual tasks towards
collaboratively achieving the prevention of HEC. The codes are publicly
available at https://github.com/nuwansribandara/elemantra.
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15015" title="Abstract">arXiv:2310.15015</a> [<a href="/pdf/2310.15015" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Deep Learning for Abstractive Code Summarization of  Unofficial Documentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naghshzan%2C+A">AmirHossein Naghshzan</a>, 
<a href="/search/cs?searchtype=author&query=Guerrouj%2C+L">Latifa Guerrouj</a>, 
<a href="/search/cs?searchtype=author&query=Baysal%2C+O">Olga Baysal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Usually, programming languages have official documentation to guide
developers with APIs, methods, and classes. However, researchers identified
insufficient or inadequate documentation examples and flaws with the API's
complex structure as barriers to learning an API. As a result, developers may
consult other sources (StackOverflow, GitHub, etc.) to learn more about an API.
Recent research studies have shown that unofficial documentation is a valuable
source of information for generating code summaries. We, therefore, have been
motivated to leverage such a type of documentation along with deep learning
techniques towards generating high-quality summaries for APIs discussed in
informal documentation.
<br />This paper proposes an automatic approach using the BART algorithm, a
state-of-the-art transformer model, to generate summaries for APIs discussed in
StackOverflow. We built an oracle of human-generated summaries to evaluate our
approach against it using ROUGE and BLEU metrics which are the most widely used
evaluation metrics in text summarization. Furthermore, we evaluated our
summaries empirically against a previous work in terms of quality. Our findings
demonstrate that using deep learning algorithms can improve summaries' quality
and outperform the previous work by an average of %57 for Precision, %66 for
Recall, and %61 for F-measure, and it runs 4.4 times faster.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15017" title="Abstract">arXiv:2310.15017</a> [<a href="/pdf/2310.15017" title="Download PDF">pdf</a>, <a href="/format/2310.15017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The primacy bias in Model-based RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Z">Zhongjian Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Jiafei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The primacy bias in deep reinforcement learning (DRL), which refers to the
agent's tendency to overfit early data and lose the ability to learn from new
data, can significantly decrease the performance of DRL algorithms. Previous
studies have shown that employing simple techniques, such as resetting the
agent's parameters, can substantially alleviate the primacy bias. However, we
observe that resetting the agent's parameters harms its performance in the
context of model-based reinforcement learning (MBRL). In fact, on further
investigation, we find that the primacy bias in MBRL differs from that in
model-free RL. In this work, we focus on investigating the primacy bias in MBRL
and propose world model resetting, which works in MBRL. We apply our method to
two different MBRL algorithms, MBPO and DreamerV2. We validate the
effectiveness of our method on multiple continuous control tasks on MuJoCo and
DeepMind Control Suite, as well as discrete control tasks on Atari 100k
benchmark. The results show that world model resetting can significantly
alleviate the primacy bias in model-based setting and improve algorithm's
performance. We also give a guide on how to perform world model resetting
effectively.
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15019" title="Abstract">arXiv:2310.15019</a> [<a href="/pdf/2310.15019" title="Download PDF">pdf</a>, <a href="/format/2310.15019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta learning with language models: Challenges and opportunities in the  classification of imbalanced text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vassilev%2C+A">Apostol Vassilev</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Honglan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M">Munawar Hasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, including 5 figures, 12 tables, 1 appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Detecting out of policy speech (OOPS) content is important but difficult.
While machine learning is a powerful tool to tackle this challenging task, it
is hard to break the performance ceiling due to factors like quantity and
quality limitations on training data and inconsistencies in OOPS definition and
data labeling. To realize the full potential of available limited resources, we
propose a meta learning technique (MLT) that combines individual models built
with different text representations. We analytically show that the resulting
technique is numerically stable and produces reasonable combining weights. We
combine the MLT with a threshold-moving (TM) technique to further improve the
performance of the combined predictor on highly-imbalanced in-distribution and
out-of-distribution datasets. We also provide computational results to show the
statistically significant advantages of the proposed MLT approach.
<br />All authors contributed equally to this work.
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15020" title="Abstract">arXiv:2310.15020</a> [<a href="/pdf/2310.15020" title="Download PDF">pdf</a>, <a href="/format/2310.15020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariance is Key to Generalization: Examining the Role of  Representation in Sim-to-Real Transfer for Visual Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ai%2C+B">Bo Ai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhanxin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+D">David Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, accepted by the 18th International Symposium on Experimental Robotics (ISER 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The data-driven approach to robot control has been gathering pace rapidly,
yet generalization to unseen task domains remains a critical challenge. We
argue that the key to generalization is representations that are (i) rich
enough to capture all task-relevant information and (ii) invariant to
superfluous variability between the training and the test domains. We
experimentally study such a representation -- containing both depth and
semantic information -- for visual navigation and show that it enables a
control policy trained entirely in simulated indoor scenes to generalize to
diverse real-world environments, both indoors and outdoors. Further, we show
that our representation reduces the A-distance between the training and test
domains, improving the generalization error bound as a result. Our proposed
approach is scalable: the learned policy improves continuously, as the
foundation models that it exploits absorb more diverse data during
pre-training.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15021" title="Abstract">arXiv:2310.15021</a> [<a href="/pdf/2310.15021" title="Download PDF">pdf</a>, <a href="/format/2310.15021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Data Learning for Open Information Extraction with Pre-trained  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhiyuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shizhu He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Open Information Extraction (OpenIE) is a fundamental yet challenging task in
Natural Language Processing, which involves extracting all triples (subject,
predicate, object) from a given sentence. While labeling-based methods have
their merits, generation-based techniques offer unique advantages, such as the
ability to generate tokens not present in the original sentence. However, these
generation-based methods often require a significant amount of training data to
learn the task form of OpenIE and substantial training time to overcome slow
model convergence due to the order penalty. In this paper, we introduce a novel
framework, OK-IE, that ingeniously transforms the task form of OpenIE into the
pre-training task form of the T5 model, thereby reducing the need for extensive
training data. Furthermore, we introduce an innovative concept of Anchor to
control the sequence of model outputs, effectively eliminating the impact of
order penalty on model convergence and significantly reducing training time.
Experimental results indicate that, compared to previous SOTA methods, OK-IE
requires only 1/100 of the training data (900 instances) and 1/120 of the
training time (3 minutes) to achieve comparable results.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15023" title="Abstract">arXiv:2310.15023</a> [<a href="/pdf/2310.15023" title="Download PDF">pdf</a>, <a href="/format/2310.15023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SONIC: Sonar Image Correspondence using Pose Supervised Learning for  Imaging Sonars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gode%2C+S">Samiran Gode</a>, 
<a href="/search/cs?searchtype=author&query=Hinduja%2C+A">Akshay Hinduja</a>, 
<a href="/search/cs?searchtype=author&query=Kaess%2C+M">Michael Kaess</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this paper, we address the challenging problem of data association for
underwater SLAM through a novel method for sonar image correspondence using
learned features. We introduce SONIC (SONar Image Correspondence), a
pose-supervised network designed to yield robust feature correspondence capable
of withstanding viewpoint variations. The inherent complexity of the underwater
environment stems from the dynamic and frequently limited visibility
conditions, restricting vision to a few meters of often featureless expanses.
This makes camera-based systems suboptimal in most open water application
scenarios. Consequently, multibeam imaging sonars emerge as the preferred
choice for perception sensors. However, they too are not without their
limitations. While imaging sonars offer superior long-range visibility compared
to cameras, their measurements can appear different from varying viewpoints.
This inherent variability presents formidable challenges in data association,
particularly for feature-based methods. Our method demonstrates significantly
better performance in generating correspondences for sonar images which will
pave the way for more accurate loop closure constraints and sonar-based place
recognition. Code as well as simulated and real-world datasets will be made
public to facilitate further development in the field.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15024" title="Abstract">arXiv:2310.15024</a> [<a href="/pdf/2310.15024" title="Download PDF">pdf</a>, <a href="/format/2310.15024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Proprietary to High-Level Trigger-Action Programming Rules: A  Natural Language Processing Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attoh%2C+E">Ekene Attoh</a>, 
<a href="/search/cs?searchtype=author&query=Signer%2C+B">Beat Signer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">With the rise of popular task automation or IoT platforms such as 'If This
Then That (IFTTT)', users can define rules to enable interactions between smart
devices in their environment and thereby improve their daily lives. However,
the rules authored via these platforms are usually tied to the platforms and
sometimes even to the specific devices for which they have been defined.
Therefore, when a user wishes to move to a different environment controlled by
a different platform and/or devices, they need to recreate their rules for the
new environment. The rise in the number of smart devices further adds to the
complexity of rule authoring since users will have to navigate an ever-changing
landscape of IoT devices. In order to address this problem, we need
human-computer interaction that works across the boundaries of specific IoT
platforms and devices. A step towards this human-computer interaction across
platforms and devices is the introduction of a high-level semantic model for
end-user IoT development, enabling users to create rules at a higher level of
abstraction. However, many users who already got used to the rule
representation in their favourite tool might be unwilling to learn and adapt to
a new representation. We present a method for translating proprietary rules to
a high-level semantic model by using natural language processing techniques.
Our translation enables users to work with their familiar rule representation
language and tool, and at the same time apply their rules across different IoT
platforms and devices.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15025" title="Abstract">arXiv:2310.15025</a> [<a href="/pdf/2310.15025" title="Download PDF">pdf</a>, <a href="/format/2310.15025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> P2AT: Pyramid Pooling Axial Transformer for Real-time Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elhassan%2C+M+A+M">Mohammed A. M. Elhassan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Changjun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Benabid%2C+A">Amina Benabid</a>, 
<a href="/search/cs?searchtype=author&query=Adam%2C+A+B+M">Abuzar B. M. Adam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, Transformer-based models have achieved promising results in various
vision tasks, due to their ability to model long-range dependencies. However,
transformers are computationally expensive, which limits their applications in
real-time tasks such as autonomous driving. In addition, an efficient local and
global feature selection and fusion are vital for accurate dense prediction,
especially driving scene understanding tasks. In this paper, we propose a
real-time semantic segmentation architecture named Pyramid Pooling Axial
Transformer (P2AT). The proposed P2AT takes a coarse feature from the CNN
encoder to produce scale-aware contextual features, which are then combined
with the multi-level feature aggregation scheme to produce enhanced contextual
features. Specifically, we introduce a pyramid pooling axial transformer to
capture intricate spatial and channel dependencies, leading to improved
performance on semantic segmentation. Then, we design a Bidirectional Fusion
module (BiF) to combine semantic information at different levels. Meanwhile, a
Global Context Enhancer is introduced to compensate for the inadequacy of
concatenating different semantic levels. Finally, a decoder block is proposed
to help maintain a larger receptive field. We evaluate P2AT variants on three
challenging scene-understanding datasets. In particular, our P2AT variants
achieve state-of-art results on the Camvid dataset 80.5%, 81.0%, 81.1% for
P2AT-S, P2ATM, and P2AT-L, respectively. Furthermore, our experiment on
Cityscapes and Pascal VOC 2012 have demonstrated the efficiency of the proposed
architecture, with results showing that P2AT-M, achieves 78.7% on Cityscapes.
The source code will be available at
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15027" title="Abstract">arXiv:2310.15027</a> [<a href="/pdf/2310.15027" title="Download PDF">pdf</a>, <a href="/format/2310.15027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Autoencoder-based Z-Interference Channels with Perfect and  Imperfect CSI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vaezi%2C+M">Mojtaba Vaezi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 13 figures, 2 tables. Accepted for publication in the IEEE Transactions on Communications. arXiv admin note: text overlap with <a href="/abs/2303.08312">arXiv:2303.08312</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">A deep autoencoder (DAE)-based structure for endto-end communication over the
two-user Z-interference channel (ZIC) with finite-alphabet inputs is designed
in this paper. The proposed structure jointly optimizes the two encoder/decoder
pairs and generates interference-aware constellations that dynamically adapt
their shape based on interference intensity to minimize the bit error rate
(BER). An in-phase/quadrature-phase (I/Q) power allocation layer is introduced
in the DAE to guarantee an average power constraint and enable the architecture
to generate constellations with nonuniform shapes. This brings further gain
compared to standard uniform constellations such as quadrature amplitude
modulation. The proposed structure is then extended to work with imperfect
channel state information (CSI). The CSI imperfection due to both the
estimation and quantization errors are examined. The performance of the DAEZIC
is compared with two baseline methods, i.e., standard and rotated
constellations. The proposed structure significantly enhances the performance
of the ZIC both for the perfect and imperfect CSI. Simulation results show that
the improvement is achieved in all interference regimes (weak, moderate, and
strong) and consistently increases with the signal-to-noise ratio (SNR). For
example, more than an order of magnitude BER reduction is obtained with respect
to the most competitive conventional method at weak interference when SNR&gt;15dB
and two bits per symbol are transmitted. The improvements reach about two
orders of magnitude when quantization error exists, indicating that the DAE-ZIC
is more robust to the interference compared to the conventional methods.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15036" title="Abstract">arXiv:2310.15036</a> [<a href="/pdf/2310.15036" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UWB Based Static Gesture Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sebastian%2C+A">Abhishek Sebastian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Our paper presents a robust framework for UWB-based static gesture
recognition, leveraging proprietary UWB radar sensor technology. Extensive data
collection efforts were undertaken to compile datasets containing five commonly
used gestures. Our approach involves a comprehensive data pre-processing
pipeline that encompasses outlier handling, aspect ratio-preserving resizing,
and false-color image transformation. Both CNN and MobileNet models were
trained on the processed images. Remarkably, our best-performing model achieved
an accuracy of 96.78%. Additionally, we developed a user-friendly GUI framework
to assess the model's system resource usage and processing times, which
revealed low memory utilization and real-time task completion in under one
second. This research marks a significant step towards enhancing static gesture
recognition using UWB technology, promising practical applications in various
domains.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15040" title="Abstract">arXiv:2310.15040</a> [<a href="/pdf/2310.15040" title="Download PDF">pdf</a>, <a href="/format/2310.15040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLOG: A Structural Generalization Benchmark for Semantic Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Donatelli%2C+L">Lucia Donatelli</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+A">Alexander Koller</a>, 
<a href="/search/cs?searchtype=author&query=Linzen%2C+T">Tal Linzen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuekun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Najoung Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The goal of compositional generalization benchmarks is to evaluate how well
models generalize to new complex linguistic expressions. Existing benchmarks
often focus on lexical generalization, the interpretation of novel lexical
items in syntactic structures familiar from training; structural generalization
tasks, where a model needs to interpret syntactic structures that are
themselves unfamiliar from training, are often underrepresented, resulting in
overly optimistic perceptions of how well models can generalize. We introduce
SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with
17 structural generalization cases. In our experiments, the generalization
accuracy of Transformer models, including pretrained ones, only reaches 40.6%,
while a structure-aware parser only achieves 70.8%. These results are far from
the near-perfect accuracy existing models achieve on COGS, demonstrating the
role of SLOG in foregrounding the large discrepancy between models' lexical and
structural generalization capacities.
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15041" title="Abstract">arXiv:2310.15041</a> [<a href="/pdf/2310.15041" title="Download PDF">pdf</a>, <a href="/format/2310.15041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manipulation Mask Generator: High-Quality Image Manipulation Mask  Generation Method Based on Modified Total Variation Noise Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jizhe Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In artificial intelligence, any model that wants to achieve a good result is
inseparable from a large number of high-quality data. It is especially true in
the field of tamper detection. This paper proposes a modified total variation
noise reduction method to acquire high-quality tampered images. We
automatically crawl original and tampered images from the Baidu PS Bar. Baidu
PS Bar is a website where net friends post countless tampered images.
Subtracting the original image with the tampered image can highlight the
tampered area. However, there is also substantial noise on the final print, so
these images can't be directly used in the deep learning model. Our modified
total variation noise reduction method is aimed at solving this problem.
Because a lot of text is slender, it is easy to lose text information after the
opening and closing operation. We use MSER (Maximally Stable Extremal Regions)
and NMS (Non-maximum Suppression) technology to extract text information. And
then use the modified total variation noise reduction technology to process the
subtracted image. Finally, we can obtain an image with little noise by adding
the image and text information. And the idea also largely retains the text
information. Datasets generated in this way can be used in deep learning
models, and they will help the model achieve better results.
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15044" title="Abstract">arXiv:2310.15044</a> [<a href="/pdf/2310.15044" title="Download PDF">pdf</a>, <a href="/format/2310.15044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Universal Anti-Spoofing Approach for Contactless Fingerprint Biometric  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adami%2C+B">Banafsheh Adami</a>, 
<a href="/search/cs?searchtype=author&query=Tehranipoor%2C+S">Sara Tehranipoor</a>, 
<a href="/search/cs?searchtype=author&query=Nasrabadi%2C+N">Nasser Nasrabadi</a>, 
<a href="/search/cs?searchtype=author&query=Karimian%2C+N">Nima Karimian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the increasing integration of smartphones into our daily lives,
fingerphotos are becoming a potential contactless authentication method. While
it offers convenience, it is also more vulnerable to spoofing using various
presentation attack instruments (PAI). The contactless fingerprint is an
emerging biometric authentication but has not yet been heavily investigated for
anti-spoofing. While existing anti-spoofing approaches demonstrated fair
results, they have encountered challenges in terms of universality and
scalability to detect any unseen/unknown spoofed samples. To address this
issue, we propose a universal presentation attack detection method for
contactless fingerprints, despite having limited knowledge of presentation
attack samples. We generated synthetic contactless fingerprints using StyleGAN
from live finger photos and integrating them to train a semi-supervised
ResNet-18 model. A novel joint loss function, combining the Arcface and Center
loss, is introduced with a regularization to balance between the two loss
functions and minimize the variations within the live samples while enhancing
the inter-class variations between the deepfake and live samples. We also
conducted a comprehensive comparison of different regularizations' impact on
the joint loss function for presentation attack detection (PAD) and explored
the performance of a modified ResNet-18 architecture with different activation
functions (i.e., leaky ReLU and RelU) in conjunction with Arcface and center
loss. Finally, we evaluate the performance of the model using unseen types of
spoof attacks and live data. Our proposed method achieves a Bona Fide
Classification Error Rate (BPCER) of 0.12\%, an Attack Presentation
Classification Error Rate (APCER) of 0.63\%, and an Average Classification
Error Rate (ACER) of 0.37\%.
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15045" title="Abstract">arXiv:2310.15045</a> [<a href="/pdf/2310.15045" title="Download PDF">pdf</a>, <a href="/format/2310.15045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling and Performance Analysis of CSMA-Based JCAS Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Keshtiarast%2C+N">Navid Keshtiarast</a>, 
<a href="/search/eess?searchtype=author&query=Bishoyi%2C+P+K">Pradyumna Kumar Bishoyi</a>, 
<a href="/search/eess?searchtype=author&query=Petrova%2C+M">Marina Petrova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Joint communication and sensing (JCAS) networks are envisioned as a key
enabler for a variety of applications which demand reliable wireless
connectivity along with accurate and robust sensing capability. When sensing
and communication share the same spectrum, the communication links in the JCAS
networks experience interference from both sensing and communication signals.
Therefore, it is crucial to analyze the interference caused by the
uncoordinated transmission of either sensing or communication signals, so that
effective interference mitigation techniques could be put in place. We consider
a JCAS network consisting of dual-functional nodes operating in radar and
communication modes. To gain access to the shared communication channel, each
node follows carrier sense multiple access (CSMA)-based protocol. For this
setting, we study the radar and communication performances defined in terms of
maximum unambiguous range and aggregated network throughput, respectively.
Leveraging on the stochastic geometry approach, we model the interference of
the network and derive a closed-form expression for both radar and
communication performance metrics. Finally, we verify our analytical results
through extensive simulation.
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15047" title="Abstract">arXiv:2310.15047</a> [<a href="/pdf/2310.15047" title="Download PDF">pdf</a>, <a href="/format/2310.15047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta- (out-of-context) learning in neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krasheninnikov%2C+D">Dmitrii Krasheninnikov</a>, 
<a href="/search/cs?searchtype=author&query=Krasheninnikov%2C+E">Egor Krasheninnikov</a>, 
<a href="/search/cs?searchtype=author&query=Mlodozeniec%2C+B">Bruno Mlodozeniec</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+D">David Krueger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Brown et al. (2020) famously introduced the phenomenon of in-context learning
in large language models (LLMs). We establish the existence of a phenomenon we
call $\textbf{meta-out-of-context learning (meta-OCL)}$ via carefully designed
synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs
to more readily "internalize" the semantic content of text that is, or appears
to be, broadly useful (such as true statements, or text from authoritative
sources) and use it in appropriate circumstances. We further demonstrate
meta-OCL in a synthetic computer vision setting, and propose two hypotheses for
the emergence of meta-OCL: one relying on the way models store knowledge in
their parameters, and another suggesting that the implicit gradient alignment
bias of gradient-descent-based optimizers may be responsible. Finally, we
reflect on what our results might imply about capabilities of future AI
systems, and discuss potential risks. Our code can be found at
https://github.com/krasheninnikov/internalization .
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15048" title="Abstract">arXiv:2310.15048</a> [<a href="/pdf/2310.15048" title="Download PDF">pdf</a>, <a href="/ps/2310.15048" title="Download PostScript">ps</a>, <a href="/format/2310.15048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fast Adaptive Method for the Evaluation of Heat Potentials in One  Dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Song%2C+C">Chengyue Song</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a fast adaptive method for the evaluation of heat potentials,
which plays a key role in the integral equation approach for the solution of
the heat equation, especially in a non-stationary domain. The algorithm
utilizes a sum-of-exponential based fast Gauss transform that evaluates the
convolution of a Gaussian with either discrete or continuous volume
distributions. The latest implementation of the algorithm allows for both
periodic and free space boundary conditions. The history dependence is overcome
by splitting the heat potentials into a smooth history part and a singular
local part. We discuss the resolution of the history part on an adaptive volume
grid in detail, providing sharp estimates that allow for the construction of an
optimal grid, justifying the efficiency of the bootstrapping scheme. While the
discussion in this paper is restricted to one spatial dimension, the
generalization to two and three dimensions is straightforward. The performance
of the algorithm is illustrated via several numerical examples.
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15050" title="Abstract">arXiv:2310.15050</a> [<a href="/pdf/2310.15050" title="Download PDF">pdf</a>, <a href="/format/2310.15050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoTrans: A Complete Planning and Control Framework for Autonomous UAV  Payload Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haojia Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haokun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Fei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Boyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The robotics community is increasingly interested in autonomous aerial
transportation. Unmanned aerial vehicles with suspended payloads have
advantages over other systems, including mechanical simplicity and agility, but
pose great challenges in planning and control. To realize fully autonomous
aerial transportation, this paper presents a systematic solution to address
these difficulties. First, we present a real-time planning method that
generates smooth trajectories considering the time-varying shape and non-linear
dynamics of the system, ensuring whole-body safety and dynamic feasibility.
Additionally, an adaptive NMPC with a hierarchical disturbance compensation
strategy is designed to overcome unknown external perturbations and inaccurate
model parameters. Extensive experiments show that our method is capable of
generating high-quality trajectories online, even in highly constrained
environments, and tracking aggressive flight trajectories accurately, even
under significant uncertainty. We plan to release our code to benefit the
community.
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15051" title="Abstract">arXiv:2310.15051</a> [<a href="/pdf/2310.15051" title="Download PDF">pdf</a>, <a href="/format/2310.15051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TeleQnA: A Benchmark Dataset to Assess Large Language Models  Telecommunications Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maatouk%2C+A">Ali Maatouk</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+F">Fadhel Ayed</a>, 
<a href="/search/cs?searchtype=author&query=Piovesan%2C+N">Nicola Piovesan</a>, 
<a href="/search/cs?searchtype=author&query=De+Domenico%2C+A">Antonio De Domenico</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhi-Quan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce TeleQnA, the first benchmark dataset designed to evaluate the
knowledge of Large Language Models (LLMs) in telecommunications. Comprising
10,000 questions and answers, this dataset draws from diverse sources,
including standards and research articles. This paper outlines the automated
question generation framework responsible for creating this dataset, along with
how human input was integrated at various stages to ensure the quality of the
questions. Afterwards, using the provided dataset, an evaluation is conducted
to assess the capabilities of LLMs, including GPT-3.5 and GPT-4. The results
highlight that these models struggle with complex standards related questions
but exhibit proficiency in addressing general telecom-related inquiries.
Additionally, our results showcase how incorporating telecom knowledge context
significantly enhances their performance, thus shedding light on the need for a
specialized telecom foundation model. Finally, the dataset is shared with
active telecom professionals, whose performance is subsequently benchmarked
against that of the LLMs. The findings illustrate that LLMs can rival the
performance of active professionals in telecom knowledge, thanks to their
capacity to process vast amounts of information, underscoring the potential of
LLMs within this domain. The dataset has been made publicly accessible on
GitHub.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15052" title="Abstract">arXiv:2310.15052</a> [<a href="/pdf/2310.15052" title="Download PDF">pdf</a>, <a href="/format/2310.15052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DREAM+: Efficient Dataset Distillation by Bidirectional Representative  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jianyang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extension of the ICCV conference version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Dataset distillation plays a crucial role in creating compact datasets with
similar training performance compared with original large-scale ones. This is
essential for addressing the challenges of data storage and training costs.
Prevalent methods facilitate knowledge transfer by matching the gradients,
embedding distributions, or training trajectories of synthetic images with
those of the sampled original images. Although there are various matching
objectives, currently the strategy for selecting original images is limited to
naive random sampling. We argue that random sampling overlooks the evenness of
the selected sample distribution, which may result in noisy or biased matching
targets. Besides, the sample diversity is also not constrained by random
sampling. Additionally, current methods predominantly focus on
single-dimensional matching, where information is not fully utilized. To
address these challenges, we propose a novel matching strategy called Dataset
Distillation by Bidirectional REpresentAtive Matching (DREAM+), which selects
representative original images for bidirectional matching. DREAM+ is applicable
to a variety of mainstream dataset distillation frameworks and significantly
reduces the number of distillation iterations by more than 15 times without
affecting performance. Given sufficient training time, DREAM+ can further
improve the performance and achieve state-of-the-art results. We have released
the code at github.com/NUS-HPC-AI-Lab/DREAM+.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15054" title="Abstract">arXiv:2310.15054</a> [<a href="/pdf/2310.15054" title="Download PDF">pdf</a>, <a href="/format/2310.15054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordinated Replay Sample Selection for Continual Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Good%2C+J">Jack Good</a>, 
<a href="/search/cs?searchtype=author&query=Majmudar%2C+J">Jimit Majmudar</a>, 
<a href="/search/cs?searchtype=author&query=Dupuy%2C+C">Christophe Dupuy</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peris%2C+C">Charith Peris</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+C">Clement Chung</a>, 
<a href="/search/cs?searchtype=author&query=Zemel%2C+R">Richard Zemel</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rahul Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, accepted to EMNLP (industry track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Continual Federated Learning (CFL) combines Federated Learning (FL), the
decentralized learning of a central model on a number of client devices that
may not communicate their data, and Continual Learning (CL), the learning of a
model from a continual stream of data without keeping the entire history. In
CL, the main challenge is \textit{forgetting} what was learned from past data.
While replay-based algorithms that keep a small pool of past training data are
effective to reduce forgetting, only simple replay sample selection strategies
have been applied to CFL in prior work, and no previous work has explored
coordination among clients for better sample selection. To bridge this gap, we
adapt a replay sample selection objective based on loss gradient diversity to
CFL and propose a new relaxation-based selection of samples to optimize the
objective. Next, we propose a practical algorithm to coordinate gradient-based
replay sample selection across clients without communicating private data. We
benchmark our coordinated and uncoordinated replay sample selection algorithms
against random sampling-based baselines with language models trained on a large
scale de-identified real-world text dataset. We show that gradient-based sample
selection methods both boost performance and reduce forgetting compared to
random sampling methods, with our coordination method showing gains early in
the low replay size regime (when the budget for storing past data is small).
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15055" title="Abstract">arXiv:2310.15055</a> [<a href="/pdf/2310.15055" title="Download PDF">pdf</a>, <a href="/format/2310.15055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Conceptualization of &quot;Fair Explanation&quot;: Disparate Impacts of  anti-Asian Hate Speech Explanations on Content Moderators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tin Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiannan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Aayushi Roy</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Carpuat%2C+M">Marine Carpuat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Recent research at the intersection of AI explainability and fairness has
focused on how explanations can improve human-plus-AI task performance as
assessed by fairness measures. We propose to characterize what constitutes an
explanation that is itself "fair" -- an explanation that does not adversely
impact specific populations. We formulate a novel evaluation method of "fair
explanations" using not just accuracy and label time, but also psychological
impact of explanations on different user groups across many metrics (mental
discomfort, stereotype activation, and perceived workload). We apply this
method in the context of content moderation of potential hate speech, and its
differential impact on Asian vs. non-Asian proxy moderators, across explanation
approaches (saliency map and counterfactual explanation). We find that saliency
maps generally perform better and show less evidence of disparate impact
(group) and individual unfairness than counterfactual explanations.
<br />Content warning: This paper contains examples of hate speech and racially
discriminatory language. The authors do not support such content. Please
consider your risk of discomfort carefully before continuing reading!
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15057" title="Abstract">arXiv:2310.15057</a> [<a href="/pdf/2310.15057" title="Download PDF">pdf</a>, <a href="/format/2310.15057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shareable Driving Style Learning and Analysis with a Hierarchical Latent  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenshuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lijun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+J">Junqiang Xi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Driving style is usually used to characterize driving behavior for a driver
or a group of drivers. However, it remains unclear how one individual's driving
style shares certain common grounds with other drivers. Our insight is that
driving behavior is a sequence of responses to the weighted mixture of latent
driving styles that are shareable within and between individuals. To this end,
this paper develops a hierarchical latent model to learn the relationship
between driving behavior and driving styles. We first propose a fragment-based
approach to represent complex sequential driving behavior, allowing for
sufficiently representing driving behavior in a low-dimension feature space.
Then, we provide an analytical formulation for the interaction of driving
behavior and shareable driving style with a hierarchical latent model by
introducing the mechanism of Dirichlet allocation. Our developed model is
finally validated and verified with 100 drivers in naturalistic driving
settings with urban and highways. Experimental results reveal that individuals
share driving styles within and between them. We also analyzed the influence of
personalities (e.g., age, gender, and driving experience) on driving styles and
found that a naturally aggressive driver would not always keep driving
aggressively (i.e., could behave calmly sometimes) but with a higher proportion
of aggressiveness than other types of drivers.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15059" title="Abstract">arXiv:2310.15059</a> [<a href="/pdf/2310.15059" title="Download PDF">pdf</a>, <a href="/format/2310.15059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Skill Generalization via Keypoint Integrated Soft Actor-Critic  Gaussian Mixture Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nematollahi%2C+I">Iman Nematollahi</a>, 
<a href="/search/cs?searchtype=author&query=Yankov%2C+K">Kirill Yankov</a>, 
<a href="/search/cs?searchtype=author&query=Burgard%2C+W">Wolfram Burgard</a>, 
<a href="/search/cs?searchtype=author&query=Welschehold%2C+T">Tim Welschehold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the International Symposium on Experimental Robotics (ISER) 2023. Videos at <a href="http://kis-gmm.cs.uni-freiburg.de/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">A long-standing challenge for a robotic manipulation system operating in
real-world scenarios is adapting and generalizing its acquired motor skills to
unseen environments. We tackle this challenge employing hybrid skill models
that integrate imitation and reinforcement paradigms, to explore how the
learning and adaptation of a skill, along with its core grounding in the scene
through a learned keypoint, can facilitate such generalization. To that end, we
develop Keypoint Integrated Soft Actor-Critic Gaussian Mixture Models (KIS-GMM)
approach that learns to predict the reference of a dynamical system within the
scene as a 3D keypoint, leveraging visual observations obtained by the robot's
physical interactions during skill learning. Through conducting comprehensive
evaluations in both simulated and real-world environments, we show that our
method enables a robot to gain a significant zero-shot generalization to novel
environments and to refine skills in the target environments faster than
learning from scratch. Importantly, this is achieved without the need for new
ground truth data. Moreover, our method effectively copes with scene
displacements.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15061" title="Abstract">arXiv:2310.15061</a> [<a href="/pdf/2310.15061" title="Download PDF">pdf</a>, <a href="/format/2310.15061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained  Multimodal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Pezzelle%2C+S">Sandro Pezzelle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the camera-ready version of the paper that will be published in the Proceedings of EMNLP 2023 (Singapore, 6-10 December 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Despite the impressive performance achieved by pre-trained
language-and-vision models in downstream tasks, it remains an open question
whether this reflects a proper understanding of image-text interaction. In this
work, we explore to what extent they handle basic linguistic constructions --
active-passive voice, coordination, and relative clauses -- that even preschool
children can typically master. We present BLA, a novel, automatically
constructed benchmark to evaluate multimodal models on these Basic Language
Abilities. We show that different types of Transformer-based systems, such as
CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting,
in line with previous findings. Our experiments, in particular, show that most
of the tested models only marginally benefit when fine-tuned or prompted with
construction-specific samples. Yet, the generative BLIP2 shows promising
trends, especially in an in-context learning setting. This opens the door to
using BLA not only as an evaluation benchmark but also to improve models' basic
language abilities.
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15065" title="Abstract">arXiv:2310.15065</a> [<a href="/pdf/2310.15065" title="Download PDF">pdf</a>, <a href="/format/2310.15065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergizing Human-AI Agency: A Guide of 23 Heuristics for Service  Co-Creation with LLM-Based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qingxiao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhongwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+A">Abhinav Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongming Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yun Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This empirical study serves as a primer for interested service providers to
determine if and how Large Language Models (LLMs) technology will be integrated
for their practitioners and the broader community. We investigate the mutual
learning journey of non-AI experts and AI through CoAGent, a service
co-creation tool with LLM-based agents. Engaging in a three-stage participatory
design processes, we work with with 23 domain experts from public libraries
across the U.S., uncovering their fundamental challenges of integrating AI into
human workflows. Our findings provide 23 actionable "heuristics for service
co-creation with AI", highlighting the nuanced shared responsibilities between
humans and AI. We further exemplar 9 foundational agency aspects for AI,
emphasizing essentials like ownership, fair treatment, and freedom of
expression. Our innovative approach enriches the participatory design model by
incorporating AI as crucial stakeholders and utilizing AI-AI interaction to
identify blind spots. Collectively, these insights pave the way for synergistic
and ethical human-AI co-creation in service contexts, preparing for workforce
ecosystems where AI coexists.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15066" title="Abstract">arXiv:2310.15066</a> [<a href="/pdf/2310.15066" title="Download PDF">pdf</a>, <a href="/format/2310.15066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Localizing Active Objects from Egocentric Vision with Symbolic World  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Te-Lin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The ability to actively ground task instructions from an egocentric view is
crucial for AI agents to accomplish tasks or assist humans virtually. One
important step towards this goal is to localize and track key active objects
that undergo major state change as a consequence of human actions/interactions
to the environment without being told exactly what/where to ground (e.g.,
localizing and tracking the `sponge` in video from the instruction "Dip the
`sponge` into the bucket."). While existing works approach this problem from a
pure vision perspective, we investigate to which extent the textual modality
(i.e., task instructions) and their interaction with visual modality can be
beneficial. Specifically, we propose to improve phrase grounding models'
ability on localizing the active objects by: (1) learning the role of `objects
undergoing change` and extracting them accurately from the instructions, (2)
leveraging pre- and post-conditions of the objects during actions, and (3)
recognizing the objects more robustly with descriptional knowledge. We leverage
large language models (LLMs) to extract the aforementioned action-object
knowledge, and design a per-object aggregation masking technique to effectively
perform joint inference on object phrases and symbolic knowledge. We evaluate
our framework on Ego4D and Epic-Kitchens datasets. Extensive experiments
demonstrate the effectiveness of our proposed framework, which leads to&gt;54%
improvements in all standard metrics on the TREK-150-OPE-Det localization +
tracking task, &gt;7% improvements in all standard metrics on the TREK-150-OPE
tracking task, and &gt;3% improvements in average precision (AP) on the Ego4D SCOD
task.
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15072" title="Abstract">arXiv:2310.15072</a> [<a href="/pdf/2310.15072" title="Download PDF">pdf</a>, <a href="/format/2310.15072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in  Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaokun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guofeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">It is typically challenging for visual or visual-inertial odometry systems to
handle the problems of dynamic scenes and pure rotation. In this work, we
design a novel visual-inertial odometry (VIO) system called RD-VIO to handle
both of these two problems. Firstly, we propose an IMU-PARSAC algorithm which
can robustly detect and match keypoints in a two-stage process. In the first
state, landmarks are matched with new keypoints using visual and IMU
measurements. We collect statistical information from the matching and then
guide the intra-keypoint matching in the second stage. Secondly, to handle the
problem of pure rotation, we detect the motion type and adapt the
deferred-triangulation technique during the data-association process. We make
the pure-rotational frames into the special subframes. When solving the
visual-inertial bundle adjustment, they provide additional constraints to the
pure-rotational motion. We evaluate the proposed VIO system on public datasets.
Experiments show the proposed RD-VIO has obvious advantages over other methods
in dynamic environments.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15074" title="Abstract">arXiv:2310.15074</a> [<a href="/pdf/2310.15074" title="Download PDF">pdf</a>, <a href="/format/2310.15074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MGAS: Multi-Granularity Architecture Search for Effective and Efficient  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoyun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+D">Divya Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiannong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+P">Penghui Ruan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Differentiable architecture search (DAS) has become the prominent approach in
the field of neural architecture search (NAS) due to its time-efficient
automation of neural network design. It shifts the traditional paradigm of
discrete architecture sampling and evaluation to differentiable super-net
optimization and discretization. However, existing DAS methods either only
conduct coarse-grained operation-level search, or restrictively explore
fine-grained filter-level and weight-level units using manually-defined
remaining ratios, which fail to simultaneously achieve small model size and
satisfactory model performance. Additionally, they address the high memory
consumption of the search process at the expense of search quality. To tackle
these issues, we introduce multi-granularity architecture search (MGAS), a
unified framework which aims to comprehensively and memory-efficiently explore
the multi-granularity search space to discover both effective and efficient
neural networks. Specifically, we learn discretization functions specific to
each granularity level to adaptively determine the remaining ratios according
to the evolving architecture. This ensures an optimal balance among units of
different granularity levels for different target model sizes. Considering the
memory demands, we break down the super-net optimization and discretization
into multiple sub-net stages. By allowing re-pruning and regrowing of units in
previous sub-nets during subsequent stages, we compensate for potential bias in
earlier stages. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet
demonstrate that MGAS outperforms other state-of-the-art methods in achieving a
better trade-off between model performance and model size.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15075" title="Abstract">arXiv:2310.15075</a> [<a href="/pdf/2310.15075" title="Download PDF">pdf</a>, <a href="/format/2310.15075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TableQAKit: A Comprehensive and Practical Toolkit for Table-based  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+F">Fangyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tongxu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Pengqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+J">Jiahe Lei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yifan Wei</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shizhu He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Table-based question answering (TableQA) is an important task in natural
language processing, which requires comprehending tables and employing various
reasoning ways to answer the questions. This paper introduces TableQAKit, the
first comprehensive toolkit designed specifically for TableQA. The toolkit
designs a unified platform that includes plentiful TableQA datasets and
integrates popular methods of this task as well as large language models
(LLMs). Users can add their datasets and methods according to the friendly
interface. Also, pleasantly surprised using the modules in this toolkit
achieves new SOTA on some datasets. Finally, \tableqakit{} also provides an
LLM-based TableQA Benchmark for evaluating the role of LLMs in TableQA.
TableQAKit is open-source with an interactive interface that includes visual
operations, and comprehensive data for ease of use.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15077" title="Abstract">arXiv:2310.15077</a> [<a href="/pdf/2310.15077" title="Download PDF">pdf</a>, <a href="/format/2310.15077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &#x27;Don&#x27;t Get Too Technical with Me&#x27;: A Discourse Structure-Based Framework  for Science Journalism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cardenas%2C+R">Ronald Cardenas</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yufang Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Science journalism refers to the task of reporting technical findings of a
scientific paper as a less technical news article to the general public
audience. We aim to design an automated system to support this real-world task
(i.e., automatic science journalism) by 1) introducing a newly-constructed and
real-world dataset (SciTechNews), with tuples of a publicly-available
scientific paper, its corresponding news article, and an expert-written short
summary snippet; 2) proposing a novel technical framework that integrates a
paper's discourse structure with its metadata to guide generation; and, 3)
demonstrating with extensive automatic and human experiments that our framework
outperforms other baseline methods (e.g. Alpaca and ChatGPT) in elaborating a
content plan meaningful for the target audience, simplifying the information
selected, and producing a coherent final report in a layman's style.
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15078" title="Abstract">arXiv:2310.15078</a> [<a href="/pdf/2310.15078" title="Download PDF">pdf</a>, <a href="/format/2310.15078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of a steepest descent algorithm in shape optimisation using  $W^{1,\infty}$ functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Deckelnick%2C+K">Klaus Deckelnick</a>, 
<a href="/search/math?searchtype=author&query=Herbert%2C+P+J">Philip J. Herbert</a>, 
<a href="/search/math?searchtype=author&query=Hinze%2C+M">Michael Hinze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Built upon previous work of the authors in (Deckelnick, Herbert, and Hinze,
ESAIM: COCV 28 (2022)), we present a general shape optimisation framework based
on the method of mappings in the $W^{1,\infty}$ topology together with a
suitable finite element discretisation. For the numerical solution of the
respective discrete shape optimisation problems we propose a steepest descent
minimisation algorithm with Armijo-Goldstein stepsize rule. We show that the
sequence generated by this descent method globally converges, and under mild
assumptions also, that every accumulation point of this sequence is a
stationary point of the shape functional. Moreover, for the mesh discretisation
parameter tending to zero we under mild assumptions prove convergence of the
discrete stationary shapes in the Hausdorff complementary metric. To illustrate
our approach we present a selection of numerical examples for PDE constrained
shape optimisation problems, where we include numerical convergence studies
which support our analytical findings.
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15079" title="Abstract">arXiv:2310.15079</a> [<a href="/pdf/2310.15079" title="Download PDF">pdf</a>, <a href="/format/2310.15079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affective and Dynamic Beam Search for Story Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tenghao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qasemi%2C+E">Ehsan Qasemi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bangzheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>, 
<a href="/search/cs?searchtype=author&query=Brahman%2C+F">Faeze Brahman</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chaturvedi%2C+S">Snigdha Chaturvedi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP-findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Storytelling's captivating potential makes it a fascinating research area,
with implications for entertainment, education, therapy, and cognitive studies.
In this paper, we propose Affective Story Generator (AffGen) for generating
interesting narratives. AffGen introduces "intriguing twists" in narratives by
employing two novel techniques-Dynamic Beam Sizing and Affective Reranking.
Dynamic Beam Sizing encourages less predictable, more captivating word choices
using a contextual multi-arm bandit model. Affective Reranking prioritizes
sentence candidates based on affect intensity. Our empirical evaluations, both
automatic and human, demonstrate AffGen's superior performance over existing
baselines in generating affectively charged and interesting narratives. Our
ablation study and analysis provide insights into the strengths and weaknesses
of AffGen.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15080" title="Abstract">arXiv:2310.15080</a> [<a href="/pdf/2310.15080" title="Download PDF">pdf</a>, <a href="/format/2310.15080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning of Large Language Models with Parameter-Efficient  Prompt Tuning and Adaptive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Che%2C+T">Tianshi Che</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Ji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiaxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiwen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+V+S">Victor S. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Huaiyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+D">Dejing Dou</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) is a promising paradigm to enable collaborative model
training with decentralized data. However, the training process of Large
Language Models (LLMs) generally incurs the update of significant parameters,
which limits the applicability of FL techniques to tackle the LLMs in real
scenarios. Prompt tuning can significantly reduce the number of parameters to
update, but it either incurs performance degradation or low training
efficiency. The straightforward utilization of prompt tuning in the FL often
raises non-trivial communication costs and dramatically degrades performance.
In addition, the decentralized data is generally non-Independent and
Identically Distributed (non-IID), which brings client drift problems and thus
poor performance. This paper proposes a Parameter-efficient prompt Tuning
approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and
effective FL of LLMs. First, an efficient partial prompt tuning approach is
proposed to improve performance and efficiency simultaneously. Second, a novel
adaptive optimization method is developed to address the client drift problems
on both the device and server sides to enhance performance further. Extensive
experiments based on 10 datasets demonstrate the superb performance (up to
60.8\% in terms of accuracy) and efficiency (up to 97.59\% in terms of training
time) of FedPepTAO compared with 9 baseline approaches. Our code is available
at https://github.com/llm-eff/FedPepTAO.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15081" title="Abstract">arXiv:2310.15081</a> [<a href="/pdf/2310.15081" title="Download PDF">pdf</a>, <a href="/format/2310.15081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Maomao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+G">Ge Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cairong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yongwei Nie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dong Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://e4s2023.github.io/">this https URL</a> ;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper proposes a novel approach to face swapping from the perspective of
fine-grained facial editing, dubbed "editing for swapping" (E4S). The
traditional face swapping methods rely on global feature extraction and often
fail to preserve the source identity. In contrast, our framework proposes a
Regional GAN Inversion (RGI) method, which allows the explicit disentanglement
of shape and texture. Specifically, our E4S performs face swapping in the
latent space of a pretrained StyleGAN, where a multi-scale mask-guided encoder
is applied to project the texture of each facial component into regional style
codes and a mask-guided injection module then manipulates feature maps with the
style codes. Based on this disentanglement, face swapping can be simplified as
style and mask swapping. Besides, since reconstructing the source face in the
target image may lead to disharmony lighting, we propose to train a re-coloring
network to make the swapped face maintain the lighting condition on the target
face. Further, to deal with the potential mismatch area during mask exchange,
we designed a face inpainting network as post-processing. The extensive
comparisons with state-of-the-art methods demonstrate that our E4S outperforms
existing methods in preserving texture, shape, and lighting. Our implementation
is available at https://github.com/e4s2023/E4S2023.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15085" title="Abstract">arXiv:2310.15085</a> [<a href="/pdf/2310.15085" title="Download PDF">pdf</a>, <a href="/format/2310.15085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Detection of Image-Scaling Attacks in Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quiring%2C+E">Erwin Quiring</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+A">Andreas M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+K">Konrad Rieck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACSAC'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Image scaling is an integral part of machine learning and computer vision
systems. Unfortunately, this preprocessing step is vulnerable to so-called
image-scaling attacks where an attacker makes unnoticeable changes to an image
so that it becomes a new image after scaling. This opens up new ways for
attackers to control the prediction or to improve poisoning and backdoor
attacks. While effective techniques exist to prevent scaling attacks, their
detection has not been rigorously studied yet. Consequently, it is currently
not possible to reliably spot these attacks in practice.
<br />This paper presents the first in-depth systematization and analysis of
detection methods for image-scaling attacks. We identify two general detection
paradigms and derive novel methods from them that are simple in design yet
significantly outperform previous work. We demonstrate the efficacy of these
methods in a comprehensive evaluation with all major learning platforms and
scaling algorithms. First, we show that image-scaling attacks modifying the
entire scaled image can be reliably detected even under an adaptive adversary.
Second, we find that our methods provide strong detection performance even if
only minor parts of the image are manipulated. As a result, we can introduce a
novel protection layer against image-scaling attacks.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15094" title="Abstract">arXiv:2310.15094</a> [<a href="/pdf/2310.15094" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-dimensional convolutional neural network model for breast cancer  subtypes classification and biochemical content evaluation using micro-FTIR  hyperspectral images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=del-Valle%2C+M">Matheus del-Valle</a>, 
<a href="/search/cs?searchtype=author&query=Bernardes%2C+E+S">Emerson Soares Bernardes</a>, 
<a href="/search/cs?searchtype=author&query=Zezell%2C+D+M">Denise Maria Zezell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Breast cancer treatment still remains a challenge, where molecular subtypes
classification plays a crucial role in selecting appropriate and specific
therapy. The four subtypes are Luminal A (LA), Luminal B (LB), HER2 subtype,
and Triple-Negative Breast Cancer (TNBC). Immunohistochemistry is the
gold-standard evaluation, although interobserver variations are reported and
molecular signatures identification is time-consuming. Fourier transform
infrared micro-spectroscopy with machine learning approaches have been used to
evaluate cancer samples, presenting biochemical-related explainability.
However, this explainability is harder when using deep learning. This study
created a 1D deep learning tool for breast cancer subtype evaluation and
biochemical contribution. Sixty hyperspectral images were acquired from a human
breast cancer microarray. K-Means clustering was applied to select tissue and
paraffin spectra. CaReNet-V1, a novel 1D convolutional neural network, was
developed to classify breast cancer (CA) and adjacent tissue (AT), and
molecular subtypes. A 1D adaptation of Grad-CAM was applied to assess the
biochemical impact to the classifications. CaReNet-V1 effectively classified CA
and AT (test accuracy of 0.89), as well as HER2 and TNBC subtypes (0.83 and
0.86), with greater difficulty for LA and LB (0.74 and 0.68). The model enabled
the evaluation of the most contributing wavenumbers to the predictions,
providing a direct relationship with the biochemical content. Therefore,
CaReNet-V1 and hyperspectral images is a potential approach for breast cancer
biopsies assessment, providing additional information to the pathology report.
Biochemical content impact feature may be used for other studies, such as
treatment efficacy evaluation and development new diagnostics and therapeutic
methods.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15097" title="Abstract">arXiv:2310.15097</a> [<a href="/pdf/2310.15097" title="Download PDF">pdf</a>, <a href="/format/2310.15097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Canonical Data Transformation for Achieving Inter- and Within-group  Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazri%2C+Z+M">Zachary McBride Lazri</a>, 
<a href="/search/cs?searchtype=author&query=Brugere%2C+I">Ivan Brugere</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Dachman-Soled%2C+D">Dana Dachman-Soled</a>, 
<a href="/search/cs?searchtype=author&query=Polychroniadou%2C+A">Antigoni Polychroniadou</a>, 
<a href="/search/cs?searchtype=author&query=Dervovic%2C+D">Danial Dervovic</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Increases in the deployment of machine learning algorithms for applications
that deal with sensitive data have brought attention to the issue of fairness
in machine learning. Many works have been devoted to applications that require
different demographic groups to be treated fairly. However, algorithms that aim
to satisfy inter-group fairness (also called group fairness) may inadvertently
treat individuals within the same demographic group unfairly. To address this
issue, we introduce a formal definition of within-group fairness that maintains
fairness among individuals from within the same group. We propose a
pre-processing framework to meet both inter- and within-group fairness criteria
with little compromise in accuracy. The framework maps the feature vectors of
members from different groups to an inter-group-fair canonical domain before
feeding them into a scoring function. The mapping is constructed to preserve
the relative relationship between the scores obtained from the unprocessed
feature vectors of individuals from the same demographic group, guaranteeing
within-group fairness. We apply this framework to the COMPAS risk assessment
and Law School datasets and compare its performance in achieving inter-group
and within-group fairness to two regularization-based methods.
</p>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15098" title="Abstract">arXiv:2310.15098</a> [<a href="/pdf/2310.15098" title="Download PDF">pdf</a>, <a href="/format/2310.15098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acquiring Weak Annotations for Tumor Localization in Temporal and  Volumetric Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+Y">Yu-Cheng Chou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+D">Deng-Ping Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zongwei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Machine Intelligence Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Creating large-scale and well-annotated datasets to train AI algorithms is
crucial for automated tumor detection and localization. However, with limited
resources, it is challenging to determine the best type of annotations when
annotating massive amounts of unlabeled data. To address this issue, we focus
on polyps in colonoscopy videos and pancreatic tumors in abdominal CT scans;
both applications require significant effort and time for pixel-wise annotation
due to the high dimensional nature of the data, involving either temporary or
spatial dimensions. In this paper, we develop a new annotation strategy, termed
Drag&amp;Drop, which simplifies the annotation process to drag and drop. This
annotation strategy is more efficient, particularly for temporal and volumetric
imaging, than other types of weak annotations, such as per-pixel, bounding
boxes, scribbles, ellipses, and points. Furthermore, to exploit our Drag&amp;Drop
annotations, we develop a novel weakly supervised learning method based on the
watershed algorithm. Experimental results show that our method achieves better
detection and localization performance than alternative weak annotations and,
more importantly, achieves similar performance to that trained on detailed
per-pixel annotations. Interestingly, we find that, with limited resources,
allocating weak annotations from a diverse patient population can foster models
more robust to unseen images than allocating per-pixel annotations for a small
set of images. In summary, this research proposes an efficient annotation
strategy for tumor detection and localization that is less accurate than
per-pixel annotations but useful for creating large-scale datasets for
screening tumors in various medical modalities.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15099" title="Abstract">arXiv:2310.15099</a> [<a href="/pdf/2310.15099" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-path convolutional neural network using micro-FTIR imaging to  predict breast cancer subtypes and biomarkers levels: estrogen receptor,  progesterone receptor, HER2 and Ki67
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=del-Valle%2C+M">Matheus del-Valle</a>, 
<a href="/search/cs?searchtype=author&query=Bernardes%2C+E+S">Emerson Soares Bernardes</a>, 
<a href="/search/cs?searchtype=author&query=Zezell%2C+D+M">Denise Maria Zezell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 3 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Breast cancer molecular subtypes classification plays an import role to sort
patients with divergent prognosis. The biomarkers used are Estrogen Receptor
(ER), Progesterone Receptor (PR), HER2, and Ki67. Based on these biomarkers
expression levels, subtypes are classified as Luminal A (LA), Luminal B (LB),
HER2 subtype, and Triple-Negative Breast Cancer (TNBC). Immunohistochemistry is
used to classify subtypes, although interlaboratory and interobserver
variations can affect its accuracy, besides being a time-consuming technique.
The Fourier transform infrared micro-spectroscopy may be coupled with deep
learning for cancer evaluation, where there is still a lack of studies for
subtypes and biomarker levels prediction. This study presents a novel 2D deep
learning approach to achieve these predictions. Sixty micro-FTIR images of
320x320 pixels were collected from a human breast biopsies microarray. Data
were clustered by K-means, preprocessed and 32x32 patches were generated using
a fully automated approach. CaReNet-V2, a novel convolutional neural network,
was developed to classify breast cancer (CA) vs adjacent tissue (AT) and
molecular subtypes, and to predict biomarkers level. The clustering method
enabled to remove non-tissue pixels. Test accuracies for CA vs AT and subtype
were above 0.84. The model enabled the prediction of ER, PR, and HER2 levels,
where borderline values showed lower performance (minimum accuracy of 0.54).
Ki67 percentage regression demonstrated a mean error of 3.6%. Thus, CaReNet-V2
is a potential technique for breast cancer biopsies evaluation, standing out as
a screening analysis technique and helping to prioritize patients.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15100" title="Abstract">arXiv:2310.15100</a> [<a href="/pdf/2310.15100" title="Download PDF">pdf</a>, <a href="/format/2310.15100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+S">Shih-Chieh Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+A">Aiping Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Ku%2C+L">Lun-Wei Ku</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Thematic analysis (TA) has been widely used for analyzing qualitative data in
many disciplines and fields. To ensure reliable analysis, the same piece of
data is typically assigned to at least two human coders. Moreover, to produce
meaningful and useful analysis, human coders develop and deepen their data
interpretation and coding over multiple iterations, making TA labor-intensive
and time-consuming. Recently the emerging field of large language models (LLMs)
research has shown that LLMs have the potential replicate human-like behavior
in various tasks: in particular, LLMs outperform crowd workers on
text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We
propose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct
TA with in-context learning (ICL). This framework provides the prompt to frame
discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA.
We demonstrate the utility of this framework using survey datasets on the
aspects of the music listening experience and the usage of a password manager.
Results of the two case studies show that the proposed framework yields similar
coding quality to that of human coders but reduces TA's labor and time demands.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15105" title="Abstract">arXiv:2310.15105</a> [<a href="/pdf/2310.15105" title="Download PDF">pdf</a>, <a href="/format/2310.15105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained  Models in Few-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kun Song</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huimin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+B">Bochao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">HuiShuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Acceptedd by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Due to the limited availability of data, existing few-shot learning methods
trained from scratch fail to achieve satisfactory performance. In contrast,
large-scale pre-trained models such as CLIP demonstrate remarkable few-shot and
zero-shot capabilities. To enhance the performance of pre-trained models for
downstream tasks, fine-tuning the model on downstream data is frequently
necessary. However, fine-tuning the pre-trained model leads to a decrease in
its generalizability in the presence of distribution shift, while the limited
number of samples in few-shot learning makes the model highly susceptible to
overfitting. Consequently, existing methods for fine-tuning few-shot learning
primarily focus on fine-tuning the model's classification head or introducing
additional structure. In this paper, we introduce a fine-tuning approach termed
Feature Discrimination Alignment (FD-Align). Our method aims to bolster the
model's generalizability by preserving the consistency of spurious features
across the fine-tuning process. Extensive experimental results validate the
efficacy of our approach for both ID and OOD tasks. Once fine-tuned, the model
can seamlessly integrate with existing methods, leading to performance
improvements. Our code can be found in https://github.com/skingorz/FD-Align.
</p>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15106" title="Abstract">arXiv:2310.15106</a> [<a href="/pdf/2310.15106" title="Download PDF">pdf</a>, <a href="/format/2310.15106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Performance Bounds for Radio Map Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romero%2C+D">Daniel Romero</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+T+N">Tien Ngoc Ha</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+R">Raju Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Franceschetti%2C+M">Massimo Franceschetti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Radio map estimation (RME) aims at providing a radiofrequency metric, such as
the received power strength, at every location of a geographical region of
interest by relying on measurements acquired at multiple positions. Although a
large number of estimators have been proposed so far, their performance has
been analyzed mostly on simulated data. The theoretical aspects of the RME
problem as well as performance bounds remain an open problem. This paper takes
a step towards filling this gap by means of a theoretical analysis of the RME
problem in a free-space propagation environment. First, the complexity of the
estimation problem is quantified by means of upper bounds on the spatial
variability of radio maps. Second, error bounds are derived for zeroth-order
and first-order interpolation estimators. The proximity coefficient, which
depends proportionally on the transmitted power and inversely proportionally on
the cube of the distance from the transmitters to the mapped region, is
proposed to quantify the complexity of the RME problem. One of the main
findings is that the error of the considered estimators is roughly proportional
to this proximity coefficient. Simple numerical experiments verify the
tightness of the obtained bounds.
</p>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15109" title="Abstract">arXiv:2310.15109</a> [<a href="/pdf/2310.15109" title="Download PDF">pdf</a>, <a href="/format/2310.15109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRENADE: Graph-Centric Language Model for Self-Supervised Representation  Learning on Text-Attributed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kaize Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyumin Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Self-supervised representation learning on text-attributed graphs, which aims
to create expressive and generalizable representations for various downstream
tasks, has received increasing research attention lately. However, existing
methods either struggle to capture the full extent of structural context
information or rely on task-specific training labels, which largely hampers
their effectiveness and generalizability in practice. To solve the problem of
self-supervised representation learning on text-attributed graphs, we develop a
novel Graph-Centric Language model -- GRENADE. Specifically, GRENADE exploits
the synergistic effect of both pre-trained language model and graph neural
network by optimizing with two specialized self-supervised learning algorithms:
graph-centric contrastive learning and graph-centric knowledge alignment. The
proposed graph-centric self-supervised learning algorithms effectively help
GRENADE to capture informative textual semantics as well as structural context
information on text-attributed graphs. Through extensive experiments, GRENADE
shows its superiority over state-of-the-art methods. Implementation is
available at \url{https://github.com/bigheiniu/GRENADE}.
</p>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15110" title="Abstract">arXiv:2310.15110</a> [<a href="/pdf/2310.15110" title="Download PDF">pdf</a>, <a href="/format/2310.15110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+R">Ruoxi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hansheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xinyue Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Linghao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+C">Chong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We report Zero123++, an image-conditioned diffusion model for generating
3D-consistent multi-view images from a single input view. To take full
advantage of pretrained 2D generative priors, we develop various conditioning
and training schemes to minimize the effort of finetuning from off-the-shelf
image diffusion models such as Stable Diffusion. Zero123++ excels in producing
high-quality, consistent multi-view images from a single image, overcoming
common issues like texture degradation and geometric misalignment. Furthermore,
we showcase the feasibility of training a ControlNet on Zero123++ for enhanced
control over the generation process. The code is available at
https://github.com/SUDO-AI-3D/zero123plus.
</p>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15111" title="Abstract">arXiv:2310.15111</a> [<a href="/pdf/2310.15111" title="Download PDF">pdf</a>, <a href="/format/2310.15111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matryoshka Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiatao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shuangfei Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J">Josh Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Jaitly%2C+N">Navdeep Jaitly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models are the de facto approach for generating high-quality images
and videos, but learning high-dimensional models remains a formidable task due
to computational and optimization challenges. Existing methods often resort to
training cascaded models in pixel space or using a downsampled latent space of
a separately trained auto-encoder. In this paper, we introduce Matryoshka
Diffusion Models(MDM), an end-to-end framework for high-resolution image and
video synthesis. We propose a diffusion process that denoises inputs at
multiple resolutions jointly and uses a NestedUNet architecture where features
and parameters for small-scale inputs are nested within those of large scales.
In addition, MDM enables a progressive training schedule from lower to higher
resolutions, which leads to significant improvements in optimization for
high-resolution generation. We demonstrate the effectiveness of our approach on
various benchmarks, including class-conditioned image generation,
high-resolution text-to-image, and text-to-video applications. Remarkably, we
can train a single pixel-space model at resolutions of up to 1024x1024 pixels,
demonstrating strong zero-shot generalization using the CC12M dataset, which
contains only 12 million images.
</p>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15112" title="Abstract">arXiv:2310.15112</a> [<a href="/pdf/2310.15112" title="Download PDF">pdf</a>, <a href="/format/2310.15112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Self 2.0: How AI-Enhanced Self-Clones Transform Self-Perception and  Improve Presentation Skills
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qingxiao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yun Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study explores the impact of AI-generated digital self-clones on
improving online presentation skills. We carried out a mixed-design experiment
involving 44 international students, comparing self-recorded videos (control)
with self-clone videos (AI group) for English presentation practice. The AI
videos utilized voice cloning, face swapping, lip-sync, and body-language
simulation to refine participants' original presentations in terms of
repetition, filler words, and pronunciation. Machine-rated scores indicated
enhancements in speech performance for both groups. Though the groups didn't
significantly differ, the AI group exhibited a heightened depth of reflection,
self-compassion, and a meaningful transition from a corrective to an enhancive
approach to self-critique. Within the AI group, congruence between
self-perception and AI self-clones resulted in diminished speech anxiety and
increased enjoyment. Our findings recommend the ethical employment of digital
self-clones to enhance the emotional and cognitive facets of skill development.
</p>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15113" title="Abstract">arXiv:2310.15113</a> [<a href="/pdf/2310.15113" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting the Bugs in ChatGPT&#x27;s Wugs: A Multilingual Investigation into  the Morphological Capabilities of a Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weissweiler%2C+L">Leonie Weissweiler</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+V">Valentin Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Kantharuban%2C+A">Anjali Kantharuban</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+A">Anna Cai</a>, 
<a href="/search/cs?searchtype=author&query=Dutt%2C+R">Ritam Dutt</a>, 
<a href="/search/cs?searchtype=author&query=Hengle%2C+A">Amey Hengle</a>, 
<a href="/search/cs?searchtype=author&query=Kabra%2C+A">Anubha Kabra</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Atharva Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Vijayakumar%2C+A">Abhishek Vijayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haofei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>, 
<a href="/search/cs?searchtype=author&query=Oflazer%2C+K">Kemal Oflazer</a>, 
<a href="/search/cs?searchtype=author&query=Mortensen%2C+D+R">David R. Mortensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have recently reached an impressive level of
linguistic capability, prompting comparisons with human language skills.
However, there have been relatively few systematic inquiries into the
linguistic capabilities of the latest generation of LLMs, and those studies
that do exist (i) ignore the remarkable ability of humans to generalize, (ii)
focus only on English, and (iii) investigate syntax or semantics and overlook
other capabilities that lie at the heart of human language, like morphology.
Here, we close these gaps by conducting the first rigorous analysis of the
morphological capabilities of ChatGPT in four typologically varied languages
(specifically, English, German, Tamil, and Turkish). We apply a version of
Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for
the four examined languages. We find that ChatGPT massively underperforms
purpose-built systems, particularly in English. Overall, our results -- through
the lens of morphology -- cast a new light on the linguistic capabilities of
ChatGPT, suggesting that claims of human-like language skills are premature and
misleading.
</p>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15114" title="Abstract">arXiv:2310.15114</a> [<a href="/pdf/2310.15114" title="Download PDF">pdf</a>, <a href="/format/2310.15114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How To Build Competitive Multi-gender Speech Translation Models For  Controlling Speaker Gender Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaido%2C+M">Marco Gaido</a>, 
<a href="/search/cs?searchtype=author&query=Fucci%2C+D">Dennis Fucci</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+M">Matteo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Bentivogli%2C+L">Luisa Bentivogli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in CLiC-it 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">When translating from notional gender languages (e.g., English) into
grammatical gender languages (e.g., Italian), the generated translation
requires explicit gender assignments for various words, including those
referring to the speaker. When the source sentence does not convey the
speaker's gender, speech translation (ST) models either rely on the
possibly-misleading vocal traits of the speaker or default to the masculine
gender, the most frequent in existing training corpora. To avoid such biased
and not inclusive behaviors, the gender assignment of speaker-related
expressions should be guided by externally-provided metadata about the
speaker's gender. While previous work has shown that the most effective
solution is represented by separate, dedicated gender-specific models, the goal
of this paper is to achieve the same results by integrating the speaker's
gender metadata into a single "multi-gender" neural ST model, easier to
maintain. Our experiments demonstrate that a single multi-gender model
outperforms gender-specialized ones when trained from scratch (with gender
accuracy gains up to 12.9 for feminine forms), while fine-tuning from existing
ST models does not lead to competitive results.
</p>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15115" title="Abstract">arXiv:2310.15115</a> [<a href="/pdf/2310.15115" title="Download PDF">pdf</a>, <a href="/format/2310.15115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpVOS: Efficient Video Object Segmentation with Triple Sparse  Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weihao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semi-supervised video object segmentation (Semi-VOS), which requires only
annotating the first frame of a video to segment future frames, has received
increased attention recently. Among existing pipelines, the
memory-matching-based one is becoming the main research stream, as it can fully
utilize the temporal sequence information to obtain high-quality segmentation
results. Even though this type of method has achieved promising performance,
the overall framework still suffers from heavy computation overhead, mainly
caused by the per-frame dense convolution operations between high-resolution
feature maps and each kernel filter. Therefore, we propose a sparse baseline of
VOS named SpVOS in this work, which develops a novel triple sparse convolution
to reduce the computation costs of the overall VOS framework. The designed
triple gate, taking full consideration of both spatial and temporal redundancy
between adjacent video frames, adaptively makes a triple decision to decide how
to apply the sparse convolution on each pixel to control the computation
overhead of each layer, while maintaining sufficient discrimination capability
to distinguish similar objects and avoid error accumulation. A mixed sparse
training strategy, coupled with a designed objective considering the sparsity
constraint, is also developed to balance the VOS segmentation performance and
computation costs. Experiments are conducted on two mainstream VOS datasets,
including DAVIS and Youtube-VOS. Results show that, the proposed SpVOS achieves
superior performance over other state-of-the-art sparse methods, and even
maintains comparable performance, e.g., an 83.04% (79.29%) overall score on the
DAVIS-2017 (Youtube-VOS) validation set, with the typical non-sparse VOS
baseline (82.88% for DAVIS-2017 and 80.36% for Youtube-VOS) while saving up to
42% FLOPs, showing its application potential for resource-constrained
scenarios.
</p>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15117" title="Abstract">arXiv:2310.15117</a> [<a href="/pdf/2310.15117" title="Download PDF">pdf</a>, <a href="/format/2310.15117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Inference Using LLM-Guided Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vashishtha%2C+A">Aniket Vashishtha</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+A+G">Abbavaram Gowtham Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Abhinav Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Bachu%2C+S">Saketh Bachu</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+V+N">Vineeth N Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Amit Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">At the core of causal inference lies the challenge of determining reliable
causal graphs solely based on observational data. Since the well-known backdoor
criterion depends on the graph, any errors in the graph can propagate
downstream to effect inference. In this work, we initially show that complete
graph information is not necessary for causal effect inference; the topological
order over graph variables (causal order) alone suffices. Further, given a node
pair, causal order is easier to elicit from domain experts compared to graph
edges since determining the existence of an edge can depend extensively on
other variables. Interestingly, we find that the same principle holds for Large
Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated
method to obtain causal order (and hence causal effect) with LLMs acting as
virtual domain experts. To this end, we employ different prompting strategies
and contextual cues to propose a robust technique of obtaining causal order
from LLMs. Acknowledging LLMs' limitations, we also study possible techniques
to integrate LLMs with established causal discovery algorithms, including
constraint-based and score-based methods, to enhance their performance.
Extensive experiments demonstrate that our approach significantly improves
causal ordering accuracy as compared to discovery algorithms, highlighting the
potential of LLMs to enhance causal inference across diverse fields.
</p>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15119" title="Abstract">arXiv:2310.15119</a> [<a href="/pdf/2310.15119" title="Download PDF">pdf</a>, <a href="/format/2310.15119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressed Sensing of Generative Sparse-latent (GSL) Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Honor%C3%A9%2C+A">Antoine Honor&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Anubhab Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Saikat Chatterjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 31st European Signal Processing Conference, EUSIPCO 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider reconstruction of an ambient signal in a compressed sensing (CS)
setup where the ambient signal has a neural network based generative model. The
generative model has a sparse-latent input and we refer to the generated
ambient signal as generative sparse-latent signal (GSL). The proposed sparsity
inducing reconstruction algorithm is inherently non-convex, and we show that a
gradient based search provides a good reconstruction performance. We evaluate
our proposed algorithm using simulated data.
</p>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15123" title="Abstract">arXiv:2310.15123</a> [<a href="/pdf/2310.15123" title="Download PDF">pdf</a>, <a href="/format/2310.15123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Branch-Solve-Merge Improves Large Language Model Evaluation and  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Swarnadeep Saha</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+O">Omer Levy</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) are frequently used for multi-faceted language
generation and evaluation tasks that involve satisfying intricate user
constraints or taking into account multiple aspects and criteria. However,
their performance can fall short, due to the model's lack of coherence and
inability to plan and decompose the problem. We propose Branch-Solve-Merge
(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such
challenging natural language tasks. It consists of branch, solve, and merge
modules that are parameterized with specific prompts to the base LLM. These
three modules plan a decomposition of the task into multiple parallel
sub-tasks, independently solve them, and fuse the solutions to the sub-tasks.
We apply our method to the tasks of LLM response evaluation and constrained
text generation and evaluate its effectiveness with multiple LLMs, including
Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and
consistency for each LLM by enhancing human-LLM agreement by up to 26%,
reducing length and pairwise position biases by up to 50%, and allowing
LLaMA-2-chat to match or outperform GPT-4 on most domains. On the constraint
story generation task, BSM improves the coherence of the stories while also
improving constraint satisfaction by 12%.
</p>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15127" title="Abstract">arXiv:2310.15127</a> [<a href="/pdf/2310.15127" title="Download PDF">pdf</a>, <a href="/format/2310.15127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Ended Instructable Embodied Agents with Memory-Augmented Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarch%2C+G">Gabriel Sarch</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tarr%2C+M+J">Michael J. Tarr</a>, 
<a href="/search/cs?searchtype=author&query=Fragkiadaki%2C+K">Katerina Fragkiadaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://helper-agent-llm.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Pre-trained and frozen LLMs can effectively map simple scene re-arrangement
instructions to programs over a robot's visuomotor functions through
appropriate few-shot example prompting. To parse open-domain natural language
and adapt to a user's idiosyncratic procedures, not known during prompt
engineering time, fixed prompts fall short. In this paper, we introduce HELPER,
an embodied agent equipped with an external memory of language-program pairs
that parses free-form human-robot dialogue into action programs through
retrieval-augmented LLM prompting: relevant memories are retrieved based on the
current dialogue, instruction, correction or VLM description, and used as
in-context prompt examples for LLM querying. The memory is expanded during
deployment to include pairs of user's language and action plans, to assist
future inferences and personalize them to the user's language and routines.
HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution
from Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7x
improvement over the previous SOTA for TfD. Our models, code and video results
can be found in our project's website: https://helper-agent-llm.github.io.
</p>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15128" title="Abstract">arXiv:2310.15128</a> [<a href="/pdf/2310.15128" title="Download PDF">pdf</a>, <a href="/format/2310.15128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projected Stochastic Gradient Descent with Quantum Annealed Binary  Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krahn%2C+M">Maximilian Krahn</a>, 
<a href="/search/cs?searchtype=author&query=Sasdelli%2C+M">Michelle Sasdelli</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fengyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Golyanik%2C+V">Vladislav Golyanik</a>, 
<a href="/search/cs?searchtype=author&query=Kannala%2C+J">Juho Kannala</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+T">Tat-Jun Chin</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Quantum Physics (quant-ph)

</div>
<p class="mathjax">We present, QP-SBGD, a novel layer-wise stochastic optimiser tailored towards
training neural networks with binary weights, known as binary neural networks
(BNNs), on quantum hardware. BNNs reduce the computational requirements and
energy consumption of deep learning models with minimal loss in accuracy.
However, training them in practice remains to be an open challenge. Most known
BNN-optimisers either rely on projected updates or binarise weights
post-training. Instead, QP-SBGD approximately maps the gradient onto binary
variables, by solving a quadratic constrained binary optimisation. Under
practically reasonable assumptions, we show that this update rule converges
with a rate of $\mathcal{O}(1 / \sqrt{T})$. Moreover, we show how the
$\mathcal{NP}$-hard projection can be effectively executed on an adiabatic
quantum annealer, harnessing recent advancements in quantum computation. We
also introduce a projected version of this update rule and prove that if a
fixed point exists in the binary variable space, the modified updates will
converge to it. Last but not least, our algorithm is implemented layer-wise,
making it suitable to train larger networks on resource-limited quantum
hardware. Through extensive evaluations, we show that QP-SBGD outperforms or is
on par with competitive and well-established baselines such as BinaryConnect,
signSGD and ProxQuant when optimising the Rosenbrock function, training BNNs as
well as binary graph neural networks.
</p>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15129" title="Abstract">arXiv:2310.15129</a> [<a href="/pdf/2310.15129" title="Download PDF">pdf</a>, <a href="/format/2310.15129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Location-Aware Visual Question Generation with Lightweight Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suwono%2C+N+C">Nicholas Collin Suwono</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J+C">Justin Chih-Yao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+T+M">Tun Min Hung</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T+K">Ting-Hao Kenneth Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+I">I-Bin Liao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yung-Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Ku%2C+L">Lun-Wei Ku</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shao-Hua Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work introduces a novel task, location-aware visual question generation
(LocaVQG), which aims to generate engaging questions from data relevant to a
particular geographical location. Specifically, we represent such
location-aware information with surrounding images and a GPS coordinate. To
tackle this task, we present a dataset generation pipeline that leverages GPT-4
to produce diverse and sophisticated questions. Then, we aim to learn a
lightweight model that can address the LocaVQG task and fit on an edge device,
such as a mobile phone. To this end, we propose a method which can reliably
generate engaging questions from location-aware information. Our proposed
method outperforms baselines regarding human evaluation (e.g., engagement,
grounding, coherence) and automatic evaluation metrics (e.g., BERTScore,
ROUGE-2). Moreover, we conduct extensive ablation studies to justify our
proposed techniques for both generating the dataset and solving the task.
</p>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15130" title="Abstract">arXiv:2310.15130</a> [<a href="/pdf/2310.15130" title="Download PDF">pdf</a>, <a href="/format/2310.15130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel-View Acoustic Synthesis from 3D Reconstructed Rooms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+B">Byeongjoo Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Karren Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hamilton%2C+B">Brian Hamilton</a>, 
<a href="/search/cs?searchtype=author&query=Sheaffer%2C+J">Jonathan Sheaffer</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+A">Anurag Ranjan</a>, 
<a href="/search/cs?searchtype=author&query=Sarabia%2C+M">Miguel Sarabia</a>, 
<a href="/search/cs?searchtype=author&query=Tuzel%2C+O">Oncel Tuzel</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J+R">Jen-Hao Rick Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We investigate the benefit of combining blind audio recordings with 3D scene
information for novel-view acoustic synthesis. Given audio recordings from 2-4
microphones and the 3D geometry and material of a scene containing multiple
unknown sound sources, we estimate the sound anywhere in the scene. We identify
the main challenges of novel-view acoustic synthesis as sound source
localization, separation, and dereverberation. While naively training an
end-to-end network fails to produce high-quality results, we show that
incorporating room impulse responses (RIRs) derived from 3D reconstructed rooms
enables the same network to jointly tackle these tasks. Our method outperforms
existing methods designed for the individual tasks, demonstrating its
effectiveness at utilizing 3D visual information. In a simulated study on the
Matterport3D-NVAS dataset, our model achieves near-perfect accuracy on source
localization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separation
and dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on
novel-view acoustic synthesis. Code, pretrained model, and video results are
available on the project webpage (https://github.com/apple/ml-nvas3d).
</p>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15132" title="Abstract">arXiv:2310.15132</a> [<a href="/pdf/2310.15132" title="Download PDF">pdf</a>, <a href="/format/2310.15132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Viability under Degraded Control Authority
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=El-Kebir%2C+H">Hamza El-Kebir</a>, 
<a href="/search/eess?searchtype=author&query=Berlin%2C+R">Richard Berlin</a>, 
<a href="/search/eess?searchtype=author&query=Bentsman%2C+J">Joseph Bentsman</a>, 
<a href="/search/eess?searchtype=author&query=Ornik%2C+M">Melkior Ornik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the American Control Conference 2024 and IEEE Control Systems Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">In this work, we solve the problem of quantifying and mitigating control
authority degradation in real time. Here, our target systems are controlled
nonlinear affine-in-control evolution equations with finite control input and
finite- or infinite-dimensional state. We consider two cases of control input
degradation: finitely many affine maps acting on unknown disjoint subsets of
the inputs and general Lipschitz continuous maps. These degradation modes are
encountered in practice due to actuator wear and tear, hard locks on actuator
ranges due to over-excitation, as well as more general changes in the control
allocation dynamics. We derive sufficient conditions for identifiability of
control authority degradation, and propose a novel real-time algorithm for
identifying or approximating control degradation modes. We demonstrate our
method on a nonlinear distributed parameter system, namely a one-dimensional
heat equation with a velocity-controlled moveable heat source, motivated by
autonomous energy-based surgery.
</p>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15135" title="Abstract">arXiv:2310.15135</a> [<a href="/pdf/2310.15135" title="Download PDF">pdf</a>, <a href="/format/2310.15135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying the Dialect Gap and its Correlates Across Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kantharuban%2C+A">Anjali Kantharuban</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Historically, researchers and consumers have noticed a decrease in quality
when applying NLP tools to minority variants of languages (i.e. Puerto Rican
Spanish or Swiss German), but studies exploring this have been limited to a
select few languages. Additionally, past studies have mainly been conducted in
a monolingual context, so cross-linguistic trends have not been identified and
tied to external factors. In this work, we conduct a comprehensive evaluation
of the most influential, state-of-the-art large language models (LLMs) across
two high-use applications, machine translation and automatic speech
recognition, to assess their functionality on the regional dialects of several
high- and low-resource languages. Additionally, we analyze how the regional
dialect gap is correlated with economic, social, and linguistic factors. The
impact of training data, including related factors like dataset size and its
construction procedure, is shown to be significant but not consistent across
models or languages, meaning a one-size-fits-all approach cannot be taken in
solving the dialect gap. This work will lay the foundation for furthering the
field of dialectal NLP by laying out evident disparities and identifying
possible pathways for addressing them through mindful data collection.
</p>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15138" title="Abstract">arXiv:2310.15138</a> [<a href="/pdf/2310.15138" title="Download PDF">pdf</a>, <a href="/format/2310.15138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion-Driven Tree Reconstruction and Fruit Localization: Advancing  Precision in Agriculture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+K">Kaiming Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+P">Peng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Villacres%2C+J">Juan Villacres</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Z">Zhaodan Kong</a>, 
<a href="/search/cs?searchtype=author&query=Vougioukas%2C+S+G">Stavros G. Vougioukas</a>, 
<a href="/search/cs?searchtype=author&query=Bailey%2C+B+N">Brian N. Bailey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was presented at IEEE/RSI International Conference on Intelligent Robots and Systems (IROS) Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Fruit distribution is pivotal in shaping the future of both agriculture and
agricultural robotics, paving the way for a streamlined supply chain. This
study introduces an innovative methodology that harnesses the synergy of RGB
imagery, LiDAR, and IMU data, to achieve intricate tree reconstructions and the
pinpoint localization of fruits. Such integration not only offers insights into
the fruit distribution, which enhances the precision of guidance for
agricultural robotics and automation systems, but also sets the stage for
simulating synthetic fruit patterns across varied tree architectures. To
validate this approach, experiments have been carried out in both a controlled
environment and an actual peach orchard. The results underscore the robustness
and efficacy of this fusion-driven methodology, highlighting its potential as a
transformative tool for future agricultural robotics and precision farming.
</p>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15140" title="Abstract">arXiv:2310.15140</a> [<a href="/pdf/2310.15140" title="Download PDF">pdf</a>, <a href="/format/2310.15140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoDAN: Automatic and Interpretable Adversarial Attacks on Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Sicheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Gang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Barrow%2C+J">Joe Barrow</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nenkova%2C+A">Ani Nenkova</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Safety alignment of Large Language Models (LLMs) can be compromised with
manual jailbreak attacks and (automatic) adversarial attacks. Recent work
suggests that patching LLMs against these attacks is possible: manual jailbreak
attacks are human-readable but often limited and public, making them easy to
block; adversarial attacks generate gibberish prompts that can be detected
using perplexity-based filters. In this paper, we show that these solutions may
be too optimistic. We propose an interpretable adversarial attack,
\texttt{AutoDAN}, that combines the strengths of both types of attacks. It
automatically generates attack prompts that bypass perplexity-based filters
while maintaining a high attack success rate like manual jailbreak attacks.
These prompts are interpretable and diverse, exhibiting strategies commonly
used in manual jailbreak attacks, and transfer better than their non-readable
counterparts when using limited training data or a single proxy model. We also
customize \texttt{AutoDAN}'s objective to leak system prompts, another
jailbreak application not addressed in the adversarial attack literature. %,
demonstrating the versatility of the approach. We can also customize the
objective of \texttt{AutoDAN} to leak system prompts, beyond the ability to
elicit harmful content from the model, demonstrating the versatility of the
approach. Our work provides a new way to red-team LLMs and to understand the
mechanism of jailbreak attacks.
</p>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15141" title="Abstract">arXiv:2310.15141</a> [<a href="/pdf/2310.15141" title="Download PDF">pdf</a>, <a href="/format/2310.15141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpecTr: Fast Speculative Decoding via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Ziteng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+A+T">Ananda Theertha Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+J+H">Jae Hun Ro</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+H">Himanshu Jain</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Felix Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Data Structures and Algorithms (cs.DS); Information Theory (cs.IT)

</div>
<p class="mathjax">Autoregressive sampling from large language models has led to
state-of-the-art results in several natural language tasks. However,
autoregressive sampling generates tokens one at a time making it slow, and even
prohibitive in certain tasks. One way to speed up sampling is
$\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$
(block or sequence of tokens), and then score all tokens in the draft by the
large language model in parallel. A subset of the tokens in the draft are
accepted (and the rest rejected) based on a statistical method to guarantee
that the final output follows the distribution of the large model. In this
work, we provide a principled understanding of speculative decoding through the
lens of optimal transport (OT) with $\textit{membership cost}$. This framework
can be viewed as an extension of the well-known $\textit{maximal-coupling}$
problem. This new formulation enables us to generalize the speculative decoding
method to allow for a set of $k$ candidates at the token-level, which leads to
an improved optimal membership cost. We show that the optimal draft selection
algorithm (transport plan) can be computed via linear programming, whose
best-known runtime is exponential in $k$. We then propose a valid draft
selection algorithm whose acceptance probability is $(1-1/e)$-optimal
multiplicatively. Moreover, it can be computed in time almost linear with size
of domain of a single token. Using this $new draft selection$ algorithm, we
develop a new autoregressive sampling algorithm called $\textit{SpecTr}$, which
provides speedup in decoding while ensuring that there is no quality
degradation in the decoded output. We experimentally demonstrate that for
state-of-the-art large language models, the proposed approach achieves a wall
clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on
standard benchmarks.
</p>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15144" title="Abstract">arXiv:2310.15144</a> [<a href="/pdf/2310.15144" title="Download PDF">pdf</a>, <a href="/format/2310.15144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page at <a href="https://design-bench.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce DEsignBench, a text-to-image (T2I) generation benchmark tailored
for visual design scenarios. Recent T2I models like DALL-E 3 and others, have
demonstrated remarkable capabilities in generating photorealistic images that
align closely with textual inputs. While the allure of creating visually
captivating images is undeniable, our emphasis extends beyond mere aesthetic
pleasure. We aim to investigate the potential of using these powerful models in
authentic design contexts. In pursuit of this goal, we develop DEsignBench,
which incorporates test samples designed to assess T2I models on both "design
technical capability" and "design application scenario." Each of these two
dimensions is supported by a diverse set of specific design categories. We
explore DALL-E 3 together with other leading T2I models on DEsignBench,
resulting in a comprehensive visual gallery for side-by-side comparisons. For
DEsignBench benchmarking, we perform human evaluations on generated images in
DEsignBench gallery, against the criteria of image-text alignment, visual
aesthetic, and design creativity. Our evaluation also considers other
specialized design capabilities, including text rendering, layout composition,
color harmony, 3D design, and medium style. In addition to human evaluations,
we introduce the first automatic image generation evaluator powered by GPT-4V.
This evaluator provides ratings that align well with human judgments, while
being easily replicable and cost-efficient. A high-resolution version is
available at
https://github.com/design-bench/design-bench.github.io/raw/main/designbench.pdf?download=
</p>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15145" title="Abstract">arXiv:2310.15145</a> [<a href="/pdf/2310.15145" title="Download PDF">pdf</a>, <a href="/format/2310.15145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for  Autonomous Real-World Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingyun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mark%2C+M+S">Max Sobol Mark</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+B">Brandon Vu</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Archit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Bohg%2C+J">Jeannette Bohg</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The pre-train and fine-tune paradigm in machine learning has had dramatic
success in a wide range of domains because the use of existing data or
pre-trained models on the internet enables quick and easy learning of new
tasks. We aim to enable this paradigm in robotic reinforcement learning,
allowing a robot to learn a new task with little human effort by leveraging
data and models from the Internet. However, reinforcement learning often
requires significant human effort in the form of manual reward specification or
environment resets, even if the policy is pre-trained. We introduce RoboFuME, a
reset-free fine-tuning system that pre-trains a multi-task manipulation policy
from diverse datasets of prior experiences and self-improves online to learn a
target task with minimal human intervention. Our insights are to utilize
calibrated offline reinforcement learning techniques to ensure efficient online
fine-tuning of a pre-trained policy in the presence of distribution shifts and
leverage pre-trained vision language models (VLMs) to build a robust reward
classifier for autonomously providing reward signals during the online
fine-tuning process. In a diverse set of five real robot manipulation tasks, we
show that our method can incorporate data from an existing robot dataset
collected at a different institution and improve on a target task within as
little as 3 hours of autonomous real-world experience. We also demonstrate in
simulation experiments that our method outperforms prior works that use
different RL algorithms or different approaches for predicting rewards. Project
website: https://robofume.github.io
</p>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15147" title="Abstract">arXiv:2310.15147</a> [<a href="/pdf/2310.15147" title="Download PDF">pdf</a>, <a href="/format/2310.15147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+F">Fangyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiming Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shizhu He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The rapid development of Large Language Models (LLMs) has led to great
strides in model capabilities like reasoning and long-context understanding.
However, as LLMs are able to process longer contexts, it becomes more
challenging to evaluate whether they have acquired certain capabilities, since
the length of text (e.g., 100K tokens) they can process far exceeds what humans
can reliably assess in a reasonable duration. In this paper, we propose using
complex synthetic tasks as a proxy evaluation method, and present S3Eval, a
Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation. As a
synthetic benchmark, S3Eval enables the creation of any number of evaluation
examples that are theoretically invisible to LLMs, mitigating the test set
contamination issue. The synthetic nature of S3Eval provides users full control
over the dataset, allowing them to systematically probe LLM capabilities by
scaling text length and varying task difficulty across diverse scenarios. The
strong correlation between S3Eval performance and scores of real-world
benchmarks like Big-Bench Hard (BBH) demonstrates the soundness of using S3Eval
for evaluation of LLMs. The in-depth analysis also uncover additional insights,
including performance drop when the answer is sparsely distributed or located
in the middle context, as well as some counter-intuitive trends of model
performance.
</p>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15149" title="Abstract">arXiv:2310.15149</a> [<a href="/pdf/2310.15149" title="Download PDF">pdf</a>, <a href="/format/2310.15149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Transferability of Tokens in Deep Models for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qi-Le Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Han-Jia Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Le-Ye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+D">De-Chuan Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fine-tuning a pre-trained deep neural network has become a successful
paradigm in various machine learning tasks. However, such a paradigm becomes
particularly challenging with tabular data when there are discrepancies between
the feature sets of pre-trained models and the target tasks. In this paper, we
propose TabToken, a method aims at enhancing the quality of feature tokens
(i.e., embeddings of tabular features). TabToken allows for the utilization of
pre-trained models when the upstream and downstream tasks share overlapping
features, facilitating model fine-tuning even with limited training examples.
Specifically, we introduce a contrastive objective that regularizes the tokens,
capturing the semantics within and across features. During the pre-training
stage, the tokens are learned jointly with top-layer deep models such as
transformer. In the downstream task, tokens of the shared features are kept
fixed while TabToken efficiently fine-tunes the remaining parts of the model.
TabToken not only enables knowledge transfer from a pre-trained model to tasks
with heterogeneous features, but also enhances the discriminative ability of
deep tabular models in standard classification and regression tasks.
</p>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15150" title="Abstract">arXiv:2310.15150</a> [<a href="/pdf/2310.15150" title="Download PDF">pdf</a>, <a href="/format/2310.15150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Detection of AI-Generated Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epstein%2C+D+C">David C. Epstein</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+I">Ishan Jain</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+O">Oliver Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Richard Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV DeepFake Analysis and Detection Workshop, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">With advancements in AI-generated images coming on a continuous basis, it is
increasingly difficult to distinguish traditionally-sourced images (e.g.,
photos, artwork) from AI-generated ones. Previous detection methods study the
generalization from a single generator to another in isolation. However, in
reality, new generators are released on a streaming basis. We study
generalization in this setting, training on N models and testing on the next
(N+k), following the historical release dates of well-known generation methods.
Furthermore, images increasingly consist of both real and generated components,
for example through image inpainting. Thus, we extend this approach to pixel
prediction, demonstrating strong performance using automatically-generated
inpainted data. In addition, for settings where commercial models are not
publicly available for automatic data generation, we evaluate if pixel
detectors can be trained solely on whole synthetic images.
</p>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15151" title="Abstract">arXiv:2310.15151</a> [<a href="/pdf/2310.15151" title="Download PDF">pdf</a>, <a href="/format/2310.15151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verb Conjugation in Transformers Is Determined by Linear Encodings of  Subject Number
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+S">Sophie Hao</a>, 
<a href="/search/cs?searchtype=author&query=Linzen%2C+T">Tal Linzen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Findings of the Association for Computational Linguistics: EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep architectures such as Transformers are sometimes criticized for having
uninterpretable "black-box" representations. We use causal intervention
analysis to show that, in fact, some linguistic features are represented in a
linear, interpretable format. Specifically, we show that BERT's ability to
conjugate verbs relies on a linear encoding of subject number that can be
manipulated with predictable effects on conjugation accuracy. This encoding is
found in the subject position at the first layer and the verb position at the
last layer, but distributed across positions at middle layers, particularly
when there are multiple cues to subject number.
</p>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15152" title="Abstract">arXiv:2310.15152</a> [<a href="/pdf/2310.15152" title="Download PDF">pdf</a>, <a href="/format/2310.15152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling Balanced Forests of Grids in Polynomial Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cannon%2C+S">Sarah Cannon</a>, 
<a href="/search/cs?searchtype=author&query=Pegden%2C+W">Wesley Pegden</a>, 
<a href="/search/cs?searchtype=author&query=Tucker-Foltz%2C+J">Jamie Tucker-Foltz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
<p class="mathjax">We prove that a polynomial fraction of the set of $k$-component forests in
the $m \times n$ grid graph have equal numbers of vertices in each component.
This resolves a conjecture of Charikar, Liu, Liu, and Vuong. It also
establishes the first provably polynomial-time algorithm for (exactly or
approximately) sampling balanced grid graph partitions according to the
spanning tree distribution, which weights each $k$-partition according to the
product, across its $k$ pieces, of the number of spanning trees of each piece.
Our result has applications to understanding political districtings, where
there is an underlying graph of indivisible geographic units that must be
partitioned into $k$ population-balanced connected subgraphs. In this setting,
tree-weighted partitions have interesting geometric properties, and this has
stimulated significant effort to develop methods to sample them.
</p>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15154" title="Abstract">arXiv:2310.15154</a> [<a href="/pdf/2310.15154" title="Download PDF">pdf</a>, <a href="/format/2310.15154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Representations of Sentiment in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tigges%2C+C">Curt Tigges</a>, 
<a href="/search/cs?searchtype=author&query=Hollinsworth%2C+O+J">Oskar John Hollinsworth</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+A">Atticus Geiger</a>, 
<a href="/search/cs?searchtype=author&query=Nanda%2C+N">Neel Nanda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Sentiment is a pervasive feature in natural language text, yet it is an open
question how sentiment is represented within Large Language Models (LLMs). In
this study, we reveal that across a range of models, sentiment is represented
linearly: a single direction in activation space mostly captures the feature
across a range of tasks with one extreme for positive and the other for
negative. Through causal interventions, we isolate this direction and show it
is causally relevant in both toy tasks and real world datasets such as Stanford
Sentiment Treebank. Through this case study we model a thorough investigation
of what a single direction means on a broad data distribution.
<br />We further uncover the mechanisms that involve this direction, highlighting
the roles of a small subset of attention heads and neurons. Finally, we
discover a phenomenon which we term the summarization motif: sentiment is not
solely represented on emotionally charged words, but is additionally summarized
at intermediate positions without inherent sentiment, such as punctuation and
names. We show that in Stanford Sentiment Treebank zero-shot classification,
76% of above-chance classification accuracy is lost when ablating the sentiment
direction, nearly half of which (36%) is due to ablating the summarized
sentiment direction exclusively at comma positions.
</p>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15160" title="Abstract">arXiv:2310.15160</a> [<a href="/pdf/2310.15160" title="Download PDF">pdf</a>, <a href="/format/2310.15160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreeMask: Synthetic Images with Dense Annotations Make Stronger  Segmentation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lihe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaogang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Bingyi Kang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yinghuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hengshuang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Semantic segmentation has witnessed tremendous progress due to the proposal
of various advanced network architectures. However, they are extremely hungry
for delicate annotations to train, and the acquisition is laborious and
unaffordable. Therefore, we present FreeMask in this work, which resorts to
synthetic images from generative models to ease the burden of both data
collection and annotation procedures. Concretely, we first synthesize abundant
training images conditioned on the semantic masks provided by realistic
datasets. This yields extra well-aligned image-mask training pairs for semantic
segmentation models. We surprisingly observe that, solely trained with
synthetic images, we already achieve comparable performance with real ones
(e.g., 48.3 vs. 48.5 mIoU on ADE20K, and 49.3 vs. 50.5 on COCO-Stuff). Then, we
investigate the role of synthetic images by joint training with real images, or
pre-training for real images. Meantime, we design a robust filtering principle
to suppress incorrectly synthesized regions. In addition, we propose to
inequally treat different semantic masks to prioritize those harder ones and
sample more corresponding synthetic images for them. As a result, either
jointly trained or pre-trained with our filtered and re-sampled synthesized
images, segmentation models can be greatly enhanced, e.g., from 48.7 to 52.0 on
ADE20K. Code is available at https://github.com/LiheYoung/FreeMask.
</p>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15161" title="Abstract">arXiv:2310.15161</a> [<a href="/pdf/2310.15161" title="Download PDF">pdf</a>, <a href="/format/2310.15161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAM-Med3D
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Sizheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhongying Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Junlong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianpin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yanzhou Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yiqing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+B">Bin Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaoting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junjun He</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Although the Segment Anything Model (SAM) has demonstrated impressive
performance in 2D natural image segmentation, its application to 3D volumetric
medical images reveals significant shortcomings, namely suboptimal performance
and unstable prediction, necessitating an excessive number of prompt points to
attain the desired outcomes. These issues can hardly be addressed by
fine-tuning SAM on medical data because the original 2D structure of SAM
neglects 3D spatial information. In this paper, we introduce SAM-Med3D, the
most comprehensive study to modify SAM for 3D medical images. Our approach is
characterized by its comprehensiveness in two primary aspects: firstly, by
comprehensively reformulating SAM to a thorough 3D architecture trained on a
comprehensively processed large-scale volumetric medical dataset; and secondly,
by providing a comprehensive evaluation of its performance. Specifically, we
train SAM-Med3D with over 131K 3D masks and 247 categories. Our SAM-Med3D
excels at capturing 3D spatial information, exhibiting competitive performance
with significantly fewer prompt points than the top-performing fine-tuned SAM
in the medical domain. We then evaluate its capabilities across 15 datasets and
analyze it from multiple perspectives, including anatomical structures,
modalities, targets, and generalization abilities. Our approach, compared with
SAM, showcases pronouncedly enhanced efficiency and broad segmentation
capabilities for 3D volumetric medical images. Our code is released at
https://github.com/uni-medical/SAM-Med3D.
</p>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15164" title="Abstract">arXiv:2310.15164</a> [<a href="/pdf/2310.15164" title="Download PDF">pdf</a>, <a href="/format/2310.15164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LINC: A Neurosymbolic Approach for Logical Reasoning by Combining  Language Models with First-Order Logic Provers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olausson%2C+T+X">Theo X. Olausson</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+A">Alex Gu</a>, 
<a href="/search/cs?searchtype=author&query=Lipkin%2C+B">Benjamin Lipkin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C+E">Cedegao E. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Solar-Lezama%2C+A">Armando Solar-Lezama</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+R">Roger Levy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Logical reasoning, i.e., deductively inferring the truth value of a
conclusion from a set of premises, is an important task for artificial
intelligence with wide potential impacts on science, mathematics, and society.
While many prompting-based strategies have been proposed to enable Large
Language Models (LLMs) to do such reasoning more effectively, they still appear
unsatisfactory, often failing in subtle and unpredictable ways. In this work,
we investigate the validity of instead reformulating such tasks as modular
neurosymbolic programming, which we call LINC: Logical Inference via
Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser,
translating premises and conclusions from natural language to expressions in
first-order logic. These expressions are then offloaded to an external theorem
prover, which symbolically performs deductive inference. Leveraging this
approach, we observe significant performance gains on FOLIO and a balanced
subset of ProofWriter for three different models in nearly all experimental
conditions we evaluate. On ProofWriter, augmenting the comparatively small
open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5
and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%,
respectively. When used with GPT-4, LINC scores 26% higher than CoT on
ProofWriter while performing comparatively on FOLIO. Further analysis reveals
that although both methods on average succeed roughly equally often on this
dataset, they exhibit distinct and complementary failure modes. We thus provide
promising evidence for how logical reasoning over natural language can be
tackled through jointly leveraging LLMs alongside symbolic provers. All
corresponding code is publicly available at https://github.com/benlipkin/linc
</p>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15165" title="Abstract">arXiv:2310.15165</a> [<a href="/pdf/2310.15165" title="Download PDF">pdf</a>, <a href="/format/2310.15165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handling Data Heterogeneity via Architectural Design for Federated  Visual Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pieri%2C+S">Sara Pieri</a>, 
<a href="/search/cs?searchtype=author&query=Restom%2C+J+R">Jose Renato Restom</a>, 
<a href="/search/cs?searchtype=author&query=Horvath%2C+S">Samuel Horvath</a>, 
<a href="/search/cs?searchtype=author&query=Cholakkal%2C+H">Hisham Cholakkal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated Learning (FL) is a promising research paradigm that enables the
collaborative training of machine learning models among various parties without
the need for sensitive information exchange. Nonetheless, retaining data in
individual clients introduces fundamental challenges to achieving performance
on par with centrally trained models. Our study provides an extensive review of
federated learning applied to visual recognition. It underscores the critical
role of thoughtful architectural design choices in achieving optimal
performance, a factor often neglected in the FL literature. Many existing FL
solutions are tested on shallow or simple networks, which may not accurately
reflect real-world applications. This practice restricts the transferability of
research findings to large-scale visual recognition models. Through an in-depth
analysis of diverse cutting-edge architectures such as convolutional neural
networks, transformers, and MLP-mixers, we experimentally demonstrate that
architectural choices can substantially enhance FL systems' performance,
particularly when handling heterogeneous data. We study 19 visual recognition
models from five different architectural families on four challenging FL
datasets. We also re-investigate the inferior performance of convolution-based
architectures in the FL setting and analyze the influence of normalization
layers on the FL performance. Our findings emphasize the importance of
architectural design for computer vision tasks in practical scenarios,
effectively narrowing the performance gap between federated and centralized
learning. Our source code is available at
https://github.com/sarapieri/fed_het.git.
</p>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15166" title="Abstract">arXiv:2310.15166</a> [<a href="/pdf/2310.15166" title="Download PDF">pdf</a>, <a href="/format/2310.15166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Visual Reasoning Coordinators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingkang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Visual reasoning requires multimodal perception and commonsense cognition of
the world. Recently, multiple vision-language models (VLMs) have been proposed
with excellent commonsense reasoning ability in various domains. However, how
to harness the collective power of these complementary VLMs is rarely explored.
Existing methods like ensemble still struggle to aggregate these models with
the desired higher-order communications. In this work, we propose Cola, a novel
paradigm that coordinates multiple VLMs for visual reasoning. Our key insight
is that a large language model (LLM) can efficiently coordinate multiple VLMs
by facilitating natural language communication that leverages their distinct
and complementary capabilities. Extensive experiments demonstrate that our
instruction tuning variant, Cola-FT, achieves state-of-the-art performance on
visual question answering (VQA), outside knowledge VQA, visual entailment, and
visual spatial reasoning tasks. Moreover, we show that our in-context learning
variant, Cola-Zero, exhibits competitive performance in zero and few-shot
settings, without finetuning. Through systematic ablation studies and
visualizations, we validate that a coordinator LLM indeed comprehends the
instruction prompts as well as the separate functionalities of VLMs; it then
coordinates them to enable impressive visual reasoning capabilities.
</p>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15168" title="Abstract">arXiv:2310.15168</a> [<a href="/pdf/2310.15168" title="Download PDF">pdf</a>, <a href="/format/2310.15168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ghost on the Shell: An Expressive Representation of General 3D Shapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xiu%2C+Y">Yuliang Xiu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Paull%2C+L">Liam Paull</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+M+J">Michael J. Black</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report (26 pages, 16 figures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The creation of photorealistic virtual worlds requires the accurate modeling
of 3D surface geometry for a wide range of objects. For this, meshes are
appealing since they 1) enable fast physics-based rendering with realistic
material and lighting, 2) support physical simulation, and 3) are
memory-efficient for modern graphics pipelines. Recent work on reconstructing
and statistically modeling 3D shape, however, has critiqued meshes as being
topologically inflexible. To capture a wide range of object shapes, any 3D
representation must be able to model solid, watertight, shapes as well as thin,
open, surfaces. Recent work has focused on the former, and methods for
reconstructing open surfaces do not support fast reconstruction with material
and lighting or unconditional generative modelling. Inspired by the observation
that open surfaces can be seen as islands floating on watertight surfaces, we
parameterize open surfaces by defining a manifold signed distance field on
watertight templates. With this parameterization, we further develop a
grid-based and differentiable representation that parameterizes both watertight
and non-watertight meshes of arbitrary topology. Our new representation, called
Ghost-on-the-Shell (G-Shell), enables two important applications:
differentiable rasterization-based reconstruction from multiview images and
generative modelling of non-watertight meshes. We empirically demonstrate that
G-Shell achieves state-of-the-art performance on non-watertight mesh
reconstruction and generation tasks, while also performing effectively for
watertight meshes.
</p>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15169" title="Abstract">arXiv:2310.15169</a> [<a href="/pdf/2310.15169" title="Download PDF">pdf</a>, <a href="/format/2310.15169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Haonan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menghan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yingqing He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="http://haonanqiu.com/projects/FreeNoise.html">this http URL</a> Code Repo: <a href="https://github.com/arthur-qiu/LongerCrafter">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the availability of large-scale video datasets and the advances of
diffusion models, text-driven video generation has achieved substantial
progress. However, existing video generation models are typically trained on a
limited number of frames, resulting in the inability to generate high-fidelity
long videos during inference. Furthermore, these models only support
single-text conditions, whereas real-life scenarios often require multi-text
conditions as the video content changes over time. To tackle these challenges,
this study explores the potential of extending the text-driven capability to
generate longer videos conditioned on multiple texts. 1) We first analyze the
impact of initial noise in video diffusion models. Then building upon the
observation of noise, we propose FreeNoise, a tuning-free and time-efficient
paradigm to enhance the generative capabilities of pretrained video diffusion
models while preserving content consistency. Specifically, instead of
initializing noises for all frames, we reschedule a sequence of noises for
long-range correlation and perform temporal attention over them by window-based
function. 2) Additionally, we design a novel motion injection method to support
the generation of videos conditioned on multiple text prompts. Extensive
experiments validate the superiority of our paradigm in extending the
generative capabilities of video diffusion models. It is noteworthy that
compared with the previous best-performing method which brought about 255%
extra time cost, our method incurs only negligible time cost of approximately
17%. Generated video samples are available at our website:
<a href="http://haonanqiu.com/projects/FreeNoise.html.">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15171" title="Abstract">arXiv:2310.15171</a> [<a href="/pdf/2310.15171" title="Download PDF">pdf</a>, <a href="/format/2310.15171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingdong Kong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shaoyuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hanjiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+L+X">Lai Xing Ng</a>, 
<a href="/search/cs?searchtype=author&query=Cottereau%2C+B+R">Benoit R. Cottereau</a>, 
<a href="/search/cs?searchtype=author&query=Ooi%2C+W+T">Wei Tsang Ooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; 45 pages, 25 figures, 13 tables; Code at <a href="https://github.com/ldkong1205/RoboDepth">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Depth estimation from monocular images is pivotal for real-world visual
perception systems. While current learning-based depth estimation models train
and test on meticulously curated data, they often overlook out-of-distribution
(OoD) situations. Yet, in practical settings -- especially safety-critical ones
like autonomous driving -- common corruptions can arise. Addressing this
oversight, we introduce a comprehensive robustness test suite, RoboDepth,
encompassing 18 corruptions spanning three categories: i) weather and lighting
conditions; ii) sensor failures and movement; and iii) data processing
anomalies. We subsequently benchmark 42 depth estimation models across indoor
and outdoor scenes to assess their resilience to these corruptions. Our
findings underscore that, in the absence of a dedicated robustness evaluation
framework, many leading depth estimation models may be susceptible to typical
corruptions. We delve into design considerations for crafting more robust depth
estimation models, touching upon pre-training, augmentation, modality, model
capacity, and learning paradigms. We anticipate our benchmark will establish a
foundational platform for advancing robust OoD depth estimation.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 24 Oct 23</h3>
<dl>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13723" title="Abstract">arXiv:2310.13723</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.13723" title="Download PDF">pdf</a>, <a href="/format/2310.13723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Open-World Bacterial Raman Spectra Identification by Feature  Regularization for Improved Resilience against Unknown Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Balytskyi%2C+Y">Yaroslav Balytskyi</a>, 
<a href="/search/q-bio?searchtype=author&query=Kalashnyk%2C+N">Nataliia Kalashnyk</a>, 
<a href="/search/q-bio?searchtype=author&query=Hubenko%2C+I">Inna Hubenko</a>, 
<a href="/search/q-bio?searchtype=author&query=Balytska%2C+A">Alina Balytska</a>, 
<a href="/search/q-bio?searchtype=author&query=McNear%2C+K">Kelly McNear</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The combination of Deep Learning techniques and Raman spectroscopy shows
great potential offering precise and prompt identification of pathogenic
bacteria in clinical settings. However, the traditional closed-set
classification approaches assume that all test samples belong to one of the
known pathogens, and their applicability is limited since the clinical
environment is inherently unpredictable and dynamic, unknown or emerging
pathogens may not be included in the available catalogs. We demonstrate that
the current state-of-the-art Neural Networks identifying pathogens through
Raman spectra are vulnerable to unknown inputs, resulting in an uncontrollable
false positive rate. To address this issue, first, we developed a novel
ensemble of ResNet architectures combined with the attention mechanism which
outperforms existing closed-world methods, achieving an accuracy of $87.8 \pm
0.1\%$ compared to the best available model's accuracy of $86.7 \pm 0.4\%$.
Second, through the integration of feature regularization by the Objectosphere
loss function, our model achieves both high accuracy in identifying known
pathogens from the catalog and effectively separates unknown samples
drastically reducing the false positive rate. Finally, the proposed feature
regularization method during training significantly enhances the performance of
out-of-distribution detectors during the inference phase improving the
reliability of the detection of unknown classes. Our novel algorithm for Raman
spectroscopy enables the detection of unknown, uncatalogued, and emerging
pathogens providing the flexibility to adapt to future pathogens that may
emerge, and has the potential to improve the reliability of Raman-based
solutions in dynamic operating environments where accuracy is critical, such as
public safety applications.
</p>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13727" title="Abstract">arXiv:2310.13727</a> (cross-list from eess.IV) [<a href="/pdf/2310.13727" title="Download PDF">pdf</a>, <a href="/format/2310.13727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inter-Scale Dependency Modeling for Skin Lesion Segmentation with  Transformer-based Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Eskandari%2C+S">Sania Eskandari</a>, 
<a href="/search/eess?searchtype=author&query=Lumpp%2C+J">Janet Lumpp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Melanoma is a dangerous form of skin cancer caused by the abnormal growth of
skin cells. Fully Convolutional Network (FCN) approaches, including the U-Net
architecture, can automatically segment skin lesions to aid diagnosis. The
symmetrical U-Net model has shown outstanding results, but its use of a
convolutional operation limits its ability to capture long-range dependencies,
which are essential for accurate medical image segmentation. In addition, the
U-shaped structure suffers from the semantic gaps between the encoder and
decoder. In this study, we developed and evaluated a U-shaped hierarchical
Transformer-based structure for skin lesion segmentation while we proposed an
Inter-scale Context Fusion (ISCF) to utilize the attention correlations in each
stage of the encoder to adaptively combine the contexts coming from each stage
to hinder the semantic gaps. The preliminary results of the skin lesion
segmentation benchmark endorse the applicability and efficacy of the ISCF
module.
</p>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13756" title="Abstract">arXiv:2310.13756</a> (cross-list from physics.chem-ph) [<a href="/pdf/2310.13756" title="Download PDF">pdf</a>, <a href="/format/2310.13756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interatomic Potentials at Multiple Scales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Fu%2C+X">Xiang Fu</a>, 
<a href="/search/physics?searchtype=author&query=Musaelian%2C+A">Albert Musaelian</a>, 
<a href="/search/physics?searchtype=author&query=Johansson%2C+A">Anders Johansson</a>, 
<a href="/search/physics?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>, 
<a href="/search/physics?searchtype=author&query=Kozinsky%2C+B">Boris Kozinsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working paper. 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">The need to use a short time step is a key limit on the speed of molecular
dynamics (MD) simulations. Simulations governed by classical potentials are
often accelerated by using a multiple-time-step (MTS) integrator that evaluates
certain potential energy terms that vary more slowly than others less
frequently. This approach is enabled by the simple but limiting analytic forms
of classical potentials. Machine learning interatomic potentials (MLIPs), in
particular recent equivariant neural networks, are much more broadly applicable
than classical potentials and can faithfully reproduce the expensive but
accurate reference electronic structure calculations used to train them. They
still, however, require the use of a single short time step, as they lack the
inherent term-by-term scale separation of classical potentials. This work
introduces a method to learn a scale separation in complex interatomic
interactions by co-training two MLIPs. Initially, a small and efficient model
is trained to reproduce short-time-scale interactions. Subsequently, a large
and expressive model is trained jointly to capture the remaining interactions
not captured by the small model. When running MD, the MTS integrator then
evaluates the smaller model for every time step and the larger model less
frequently, accelerating simulation. Compared to a conventionally trained MLIP,
our approach can achieve a significant speedup (~3x in our experiments) without
a loss of accuracy on the potential energy or simulation-derived quantities.
</p>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13765" title="Abstract">arXiv:2310.13765</a> (cross-list from stat.CO) [<a href="/pdf/2310.13765" title="Download PDF">pdf</a>, <a href="/format/2310.13765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computationally Efficient and Error Aware Surrogate Construction for  Numerical Solutions of Subsurface Flow Through Porous Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sorokin%2C+A+G">Aleksei G. Sorokin</a>, 
<a href="/search/stat?searchtype=author&query=Pachalieva%2C+A">Aleksandra Pachalieva</a>, 
<a href="/search/stat?searchtype=author&query=O%27Malley%2C+D">Daniel O&#x27;Malley</a>, 
<a href="/search/stat?searchtype=author&query=Hyman%2C+J+M">James M. Hyman</a>, 
<a href="/search/stat?searchtype=author&query=Hickernell%2C+F+J">Fred J. Hickernell</a>, 
<a href="/search/stat?searchtype=author&query=Hengartner%2C+N+W">Nicolas W. Hengartner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Limiting the injection rate to restrict the pressure below a threshold at a
critical location can be an important goal of simulations that model the
subsurface pressure between injection and extraction wells. The pressure is
approximated by the solution of Darcy's partial differential equation (PDE) for
a given permeability field. The subsurface permeability is modeled as a random
field since it is known only up to statistical properties. This induces
uncertainty in the computed pressure. Solving the PDE for an ensemble of random
permeability simulations enables estimating a probability distribution for the
pressure at the critical location. These simulations are computationally
expensive, and practitioners often need rapid online guidance for real-time
pressure management. An ensemble of numerical PDE solutions is used to
construct a Gaussian process regression model that can quickly predict the
pressure at the critical location as a function of the extraction rate and
permeability realization.
<br />Our first novel contribution is to identify a sampling methodology for the
random environment and matching kernel technology for which fitting the
Gaussian process regression model scales as O(n log n) instead of the typical
O(n^3) rate in the number of samples n used to fit the surrogate. The surrogate
model allows almost instantaneous predictions for the pressure at the critical
location as a function of the extraction rate and permeability realization. Our
second contribution is a novel algorithm to calibrate the uncertainty in the
surrogate model to the discrepancy between the true pressure solution of
Darcy's equation and the numerical solution. Although our method is derived for
building a surrogate for the solution of Darcy's equation with a random
permeability field, the framework broadly applies to solutions of other PDE
with random coefficients.
</p>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13786" title="Abstract">arXiv:2310.13786</a> (cross-list from stat.ML) [<a href="/pdf/2310.13786" title="Download PDF">pdf</a>, <a href="/ps/2310.13786" title="Download PostScript">ps</a>, <a href="/format/2310.13786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Membership Inference Attacks on Machine Learning  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Aubinais%2C+E">Eric Aubinais</a>, 
<a href="/search/stat?searchtype=author&query=Gassiat%2C+E">Elisabeth Gassiat</a>, 
<a href="/search/stat?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Membership inference attacks (MIA) can reveal whether a particular data point
was part of the training dataset, potentially exposing sensitive information
about individuals. This article explores the fundamental statistical
limitations associated with MIAs on machine learning models. More precisely, we
first derive the statistical quantity that governs the effectiveness and
success of such attacks. Then, we investigate several situations for which we
provide bounds on this quantity of interest. This allows us to infer the
accuracy of potential attacks as a function of the number of samples and other
structural parameters of learning models, which in some cases can be directly
estimated from the dataset.
</p>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13806" title="Abstract">arXiv:2310.13806</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.13806" title="Download PDF">pdf</a>, <a href="/format/2310.13806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoseNet: Predicting Energy Metrics of Double InDel Mutants Using Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Coffland%2C+S">Sarah Coffland</a>, 
<a href="/search/q-bio?searchtype=author&query=Christensen%2C+K">Katie Christensen</a>, 
<a href="/search/q-bio?searchtype=author&query=Jagodzinski%2C+F">Filip Jagodzinski</a>, 
<a href="/search/q-bio?searchtype=author&query=Hutchinson%2C+B">Brian Hutchinson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at Computational Structural Bioinformatics Workshop 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 14th ACM International Conference on
  Bioinformatics, Computational Biology, and Health Informatics. ACM BCB 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">An amino acid insertion or deletion, or InDel, can have profound and varying
functional impacts on a protein's structure. InDel mutations in the
transmembrane conductor regulator protein for example give rise to cystic
fibrosis. Unfortunately performing InDel mutations on physical proteins and
studying their effects is a time prohibitive process. Consequently, modeling
InDels computationally can supplement and inform wet lab experiments. In this
work, we make use of our data sets of exhaustive double InDel mutations for
three proteins which we computationally generated using a robotics inspired
inverse kinematics approach available in Rosetta. We develop and train a neural
network, RoseNet, on several structural and energetic metrics output by Rosetta
during the mutant generation process. We explore and present how RoseNet is
able to emulate the exhaustive data set using deep learning methods, and show
to what extent it can predict Rosetta metrics for unseen mutant sequences with
two InDels. RoseNet achieves a Pearson correlation coefficient median accuracy
of 0.775 over all Rosetta scores for the largest protein. Furthermore, a
sensitivity analysis is performed to determine the necessary quantity of data
required to accurately emulate the structural scores for computationally
generated mutants. We show that the model can be trained on minimal data (&lt;50%)
and still retain a high level of accuracy.
</p>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13843" title="Abstract">arXiv:2310.13843</a> (cross-list from math.OC) [<a href="/pdf/2310.13843" title="Download PDF">pdf</a>, <a href="/format/2310.13843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long Solution Times or Low Solution Quality: On Trade-Offs in Choosing a  Power Flow Formulation for the Optimal Power Shut-Off Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rhodes%2C+N">Noah Rhodes</a>, 
<a href="/search/math?searchtype=author&query=Haag%2C+E">Eric Haag</a>, 
<a href="/search/math?searchtype=author&query=Roald%2C+L">Line Roald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The Optimal Power Shutoff (OPS) problem is an optimization problem that makes
power line de-energization decisions in order to reduce the risk of igniting a
wildfire, while minimizing the load shed of customers. This problem, with DC
linear power flow equations, has been used in many studies in recent years.
However, using linear approximations for power flow when making decisions on
the network topology is known to cause challenges with AC feasibility of the
resulting network, as studied in the related contexts of optimal transmission
switching or grid restoration planning. This paper explores the accuracy of the
DC OPS formulation and the ability to recover an AC-feasible power flow
solution after de-energization decisions are made. We also extend the OPS
problem to include variants with the AC, Second-Order-Cone, and Network-Flow
power flow equations, and compare them to the DC approximation with respect to
solution quality and time. The results highlight that the DC approximation
overestimates the amount of load that can be served, leading to poor
de-energization decisions. The AC and SOC-based formulations are better, but
prohibitively slow to solve for even modestly sized networks thus demonstrating
the need for new solution methods with better trade-offs between computational
time and solution quality.
</p>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13857" title="Abstract">arXiv:2310.13857</a> (cross-list from cond-mat.supr-con) [<a href="/pdf/2310.13857" title="Download PDF">pdf</a>, <a href="/format/2310.13857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Superconductor Logic Implementation with All-JJ Inductor-Free Cell  Library
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Cong%2C+H">Haolin Cong</a>, 
<a href="/search/cond-mat?searchtype=author&query=Razmkhah%2C+S">Sasan Razmkhah</a>, 
<a href="/search/cond-mat?searchtype=author&query=Karamuftuoglu%2C+M+A">Mustafa Altay Karamuftuoglu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pedram%2C+M">Massoud Pedram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 28 figures, 13 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Superconductivity (cond-mat.supr-con)</span>; Digital Libraries (cs.DL); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Single flux quantum (SFQ) technology has garnered significant attention due
to its low switching power and high operational speed. Researchers have been
actively pursuing more advanced devices and technologies to further reduce the
reliance on inductors, bias, and dynamic power. Recently, innovative magnetic
Josephson junction devices have emerged, enhancing the field of superconductor
electronics (SCE) logic. This paper introduces a novel cell library design that
relies entirely on Josephson junctions (JJs), showing promising potential for
eliminating the need for inductors in conventional SFQ cells. This results in a
55% reduction in cell size and an 80% decrease in both static and dynamic power
consumption. The proposed library implements a half flux quantum (HFQ) logic,
where each pulse duration is half that of a single flux quantum pulse. The
paper presents the schematics of the basic cells, emphasizing critical circuit
parameters and their margins. Additionally, it examines layout blueprints,
showcasing the advantageous area-saving characteristics of the proposed design.
</p>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13863" title="Abstract">arXiv:2310.13863</a> (cross-list from stat.ML) [<a href="/pdf/2310.13863" title="Download PDF">pdf</a>, <a href="/format/2310.13863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Optimization with Bias and Variance Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mehta%2C+R">Ronak Mehta</a>, 
<a href="/search/stat?searchtype=author&query=Roulet%2C+V">Vincent Roulet</a>, 
<a href="/search/stat?searchtype=author&query=Pillutla%2C+K">Krishna Pillutla</a>, 
<a href="/search/stat?searchtype=author&query=Harchaoui%2C+Z">Zaid Harchaoui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider the distributionally robust optimization (DRO) problem with
spectral risk-based uncertainty set and $f$-divergence penalty. This
formulation includes common risk-sensitive learning objectives such as
regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present
Prospect, a stochastic gradient-based algorithm that only requires tuning a
single learning rate hyperparameter, and prove that it enjoys linear
convergence for smooth regularized losses. This contrasts with previous
algorithms that either require tuning multiple hyperparameters or potentially
fail to converge due to biased gradient estimates or inadequate regularization.
Empirically, we show that Prospect can converge 2-3$\times$ faster than
baselines such as stochastic gradient and stochastic saddle-point methods on
distribution shift and fairness benchmarks spanning tabular, vision, and
language domains.
</p>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13916" title="Abstract">arXiv:2310.13916</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.13916" title="Download PDF">pdf</a>, <a href="/format/2310.13916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Southern Ocean Dynamics Under Climate Change: New Knowledge Through  Physics-Guided Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yik%2C+W">William Yik</a>, 
<a href="/search/physics?searchtype=author&query=Sonnewald%2C+M">Maike Sonnewald</a>, 
<a href="/search/physics?searchtype=author&query=Clare%2C+M+C+A">Mariana C. A. Clare</a>, 
<a href="/search/physics?searchtype=author&query=Lguensat%2C+R">Redouane Lguensat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Complex ocean systems such as the Antarctic Circumpolar Current play key
roles in the climate, and current models predict shifts in their strength and
area under climate change. However, the physical processes underlying these
changes are not well understood, in part due to the difficulty of
characterizing and tracking changes in ocean physics in complex models. To
understand changes in the Antarctic Circumpolar Current, we extend the method
Tracking global Heating with Ocean Regimes (THOR) to a mesoscale eddy
permitting climate model and identify regions of the ocean characterized by
similar physics, called dynamical regimes, using readily accessible fields from
climate models. To this end, we cluster grid cells into dynamical regimes and
train an ensemble of neural networks to predict these regimes and track them
under climate change. Finally, we leverage this new knowledge to elucidate the
dynamics of regime shifts. Here we illustrate the value of this high-resolution
version of THOR, which allows for mesoscale turbulence, with a case study of
the Antarctic Circumpolar Current and its interactions with the
Pacific-Antarctic Ridge. In this region, THOR specifically reveals a shift in
dynamical regime under climate change driven by changes in wind stress and
interactions with bathymetry. Using this knowledge to guide further
exploration, we find that as the Antarctic Circumpolar Current shifts north
under intensifying wind stress, the dominant dynamical role of bathymetry
weakens and the flow strengthens.
</p>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13940" title="Abstract">arXiv:2310.13940</a> (cross-list from eess.SP) [<a href="/pdf/2310.13940" title="Download PDF">pdf</a>, <a href="/format/2310.13940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Network Function Placement and Routing Optimization in Dynamic  Software-defined Satellite-Terrestrial Integrated Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yuan%2C+S">Shuo Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Y">Yaohua Sun</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+M">Mugen Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Software-defined satellite-terrestrial integrated networks (SDSTNs) are seen
as a promising paradigm for achieving high resource flexibility and global
communication coverage. However, low latency service provisioning is still
challenging due to the fast variation of network topology and limited onboard
resource at low earth orbit satellites. To address this issue, we study service
provisioning in SDSTNs via joint optimization of virtual network function (VNF)
placement and routing planning with network dynamics characterized by a
time-evolving graph. Aiming at minimizing average service latency, the
corresponding problem is formulated as an integer nonlinear programming under
resource, VNF deployment, and time-slotted flow constraints. Since exhaustive
search is intractable, we transform the primary problem into an integer linear
programming by involving auxiliary variables and then propose a Benders
decomposition based branch-and-cut (BDBC) algorithm. Towards practical use, a
time expansion-based decoupled greedy (TEDG) algorithm is further designed with
rigorous complexity analysis. Extensive experiments demonstrate the optimality
of BDBC algorithm and the low complexity of TEDG algorithm. Meanwhile, it is
indicated that they can improve the number of completed services within a
configuration period by up to 58% and reduce the average service latency by up
to 17% compared to baseline schemes.
</p>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13966" title="Abstract">arXiv:2310.13966</a> (cross-list from stat.ML) [<a href="/pdf/2310.13966" title="Download PDF">pdf</a>, <a href="/format/2310.13966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimax Optimal Transfer Learning for Kernel-based Nonparametric  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Caixing Wang</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/stat?searchtype=author&query=Feng%2C+X">Xingdong Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">In recent years, transfer learning has garnered significant attention in the
machine learning community. Its ability to leverage knowledge from related
studies to improve generalization performance in a target study has made it
highly appealing. This paper focuses on investigating the transfer learning
problem within the context of nonparametric regression over a reproducing
kernel Hilbert space. The aim is to bridge the gap between practical
effectiveness and theoretical guarantees. We specifically consider two
scenarios: one where the transferable sources are known and another where they
are unknown. For the known transferable source case, we propose a two-step
kernel-based estimator by solely using kernel ridge regression. For the unknown
case, we develop a novel method based on an efficient aggregation algorithm,
which can automatically detect and alleviate the effects of negative sources.
This paper provides the statistical properties of the desired estimators and
establishes the minimax optimal rate. Through extensive numerical experiments
on synthetic data and real examples, we validate our theoretical findings and
demonstrate the effectiveness of our proposed method.
</p>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13969" title="Abstract">arXiv:2310.13969</a> (cross-list from stat.ML) [<a href="/pdf/2310.13969" title="Download PDF">pdf</a>, <a href="/ps/2310.13969" title="Download PostScript">ps</a>, <a href="/format/2310.13969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Linear Regression with Compositional Covariates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chao%2C+Y">Yue Chao</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+L">Lei Huang</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+X">Xuejun Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages,2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the availability of extraordinarily huge data sets, solving the problems
of distributed statistical methodology and computing for such data sets has
become increasingly crucial in the big data area. In this paper, we focus on
the distributed sparse penalized linear log-contrast model in massive
compositional data. In particular, two distributed optimization techniques
under centralized and decentralized topologies are proposed for solving the two
different constrained convex optimization problems. Both two proposed
algorithms are based on the frameworks of Alternating Direction Method of
Multipliers (ADMM) and Coordinate Descent Method of Multipliers(CDMM, Lin et
al., 2014, Biometrika). It is worth emphasizing that, in the decentralized
topology, we introduce a distributed coordinate-wise descent algorithm based on
Group ADMM(GADMM, Elgabli et al., 2020, Journal of Machine Learning Research)
for obtaining a communication-efficient regularized estimation.
Correspondingly, the convergence theories of the proposed algorithms are
rigorously established under some regularity conditions. Numerical experiments
on both synthetic and real data are conducted to evaluate our proposed
algorithms.
</p>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13975" title="Abstract">arXiv:2310.13975</a> (cross-list from stat.ML) [<a href="/pdf/2310.13975" title="Download PDF">pdf</a>, <a href="/format/2310.13975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASBART:Accelerated Soft Bayes Additive Regression Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ran%2C+H">Hao Ran</a>, 
<a href="/search/stat?searchtype=author&query=Bai%2C+Y">Yang Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Bayes additive regression trees(BART) is a nonparametric regression model
which has gained wide-spread popularity in recent years due to its flexibility
and high accuracy of estimation. Soft BART,one variation of BART,improves both
practically and heoretically on existing Bayesian sum-of-trees models. One
bottleneck for Soft BART is its slow speed in the long MCMC loop. Compared to
BART,it use more than about 20 times to complete the calculation with the
default setting. We proposed a variant of BART named accelerate Soft
BART(ASBART). Simulation studies show that the new method is about 10 times
faster than the Soft BART with comparable accuracy. Our code is open-source and
available at https://github.com/richael008/XSBART.
</p>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13978" title="Abstract">arXiv:2310.13978</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.13978" title="Download PDF">pdf</a>, <a href="/format/2310.13978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-stable discretization of the one-dimensional two-fluid model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Buist%2C+J+F+H">J. F. H. Buist</a>, 
<a href="/search/physics?searchtype=author&query=Sanderse%2C+B">B. Sanderse</a>, 
<a href="/search/physics?searchtype=author&query=Dubinkina%2C+S">S. Dubinkina</a>, 
<a href="/search/physics?searchtype=author&query=Oosterlee%2C+C+W">C. W. Oosterlee</a>, 
<a href="/search/physics?searchtype=author&query=Henkes%2C+R+A+W+M">R. A. W. M. Henkes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper we present a complete framework for the energy-stable
simulation of stratified incompressible flow in channels, using the
one-dimensional two-fluid model. Building on earlier energy-conserving work on
the basic two-fluid model, our new framework includes diffusion, friction, and
surface tension. We show that surface tension can be added in an
energy-conserving manner, and that diffusion and friction have a strictly
dissipative effect on the energy.
<br />We then propose spatial discretizations for these terms such that a
semi-discrete model is obtained that has the same conservation properties as
the continuous model. Additionally, we propose a new energy-stable advective
flux scheme that is energy-conserving in smooth regions of the flow and
strictly dissipative where sharp gradients appear. This is obtained by
combining, using flux limiters, a previously developed energy-conserving
advective flux with a novel first-order upwind scheme that is shown to be
strictly dissipative.
<br />The complete framework, with diffusion, surface tension, and a bounded
energy, is linearly stable to short wavelength perturbations, and exhibits
nonlinear damping near shocks. The model yields smoothly converging numerical
solutions, even under conditions for which the basic two-fluid model is
ill-posed. With our explicit expressions for the dissipation rates, we are able
to attribute the nonlinear damping to the different dissipation mechanisms, and
compare their effects.
</p>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14005" title="Abstract">arXiv:2310.14005</a> (cross-list from eess.IV) [<a href="/pdf/2310.14005" title="Download PDF">pdf</a>, <a href="/ps/2310.14005" title="Download PostScript">ps</a>, <a href="/format/2310.14005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ophthalmic Biomarker Detection Using Ensembled Vision Transformers --  Winning Solution to IEEE SPS VIP Cup 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shahgir%2C+H+A+Z+S">H.A.Z. Sameen Shahgir</a>, 
<a href="/search/eess?searchtype=author&query=Sayeed%2C+K+S">Khondker Salman Sayeed</a>, 
<a href="/search/eess?searchtype=author&query=Zaman%2C+T+A">Tanjeem Azwad Zaman</a>, 
<a href="/search/eess?searchtype=author&query=Haider%2C+M+A">Md. Asif Haider</a>, 
<a href="/search/eess?searchtype=author&query=Jony%2C+S+S+R">Sheikh Saifur Rahman Jony</a>, 
<a href="/search/eess?searchtype=author&query=Rahman%2C+M+S">M. Sohel Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This report outlines our approach in the IEEE SPS VIP Cup 2023: Ophthalmic
Biomarker Detection competition. Our primary objective in this competition was
to identify biomarkers from Optical Coherence Tomography (OCT) images obtained
from a diverse range of patients. Using robust augmentations and 5-fold
cross-validation, we trained two vision transformer-based models: MaxViT and
EVA-02, and ensembled them at inference time. We find MaxViT's use of
convolution layers followed by strided attention to be better suited for the
detection of local features while EVA-02's use of normal attention mechanism
and knowledge distillation is better for detecting global features. Ours was
the best-performing solution in the competition, achieving a patient-wise F1
score of 0.814 in the first phase and 0.8527 in the second and final phase of
VIP Cup 2023, scoring 3.8% higher than the next-best solution.
</p>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14036" title="Abstract">arXiv:2310.14036</a> (cross-list from stat.ML) [<a href="/pdf/2310.14036" title="Download PDF">pdf</a>, <a href="/format/2310.14036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On discretisation drift and smoothness regularisation in neural network  training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rosca%2C+M+C">Mihaela Claudia Rosca</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis. arXiv admin note: text overlap with <a href="/abs/2302.01952">arXiv:2302.01952</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The deep learning recipe of casting real-world problems as mathematical
optimisation and tackling the optimisation by training deep neural networks
using gradient-based optimisation has undoubtedly proven to be a fruitful one.
The understanding behind why deep learning works, however, has lagged behind
its practical significance. We aim to make steps towards an improved
understanding of deep learning with a focus on optimisation and model
regularisation. We start by investigating gradient descent (GD), a
discrete-time algorithm at the basis of most popular deep learning optimisation
algorithms. Understanding the dynamics of GD has been hindered by the presence
of discretisation drift, the numerical integration error between GD and its
often studied continuous-time counterpart, the negative gradient flow (NGF). To
add to the toolkit available to study GD, we derive novel continuous-time flows
that account for discretisation drift. Unlike the NGF, these new flows can be
used to describe learning rate specific behaviours of GD, such as training
instabilities observed in supervised learning and two-player games. We then
translate insights from continuous time into mitigation strategies for unstable
GD dynamics, by constructing novel learning rate schedules and regularisers
that do not require additional hyperparameters. Like optimisation, smoothness
regularisation is another pillar of deep learning's success with wide use in
supervised learning and generative modelling. Despite their individual
significance, the interactions between smoothness regularisation and
optimisation have yet to be explored. We find that smoothness regularisation
affects optimisation across multiple deep learning domains, and that
incorporating smoothness regularisation in reinforcement learning leads to a
performance boost that can be recovered using adaptions to optimisation
methods.
</p>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14046" title="Abstract">arXiv:2310.14046</a> (cross-list from math.ST) [<a href="/pdf/2310.14046" title="Download PDF">pdf</a>, <a href="/format/2310.14046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Least p-Variances Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Masjed-Jamei%2C+M">Mohammad Masjed-Jamei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 85 pages, 1 figure and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">As a result of a rather long-time research started in 2016, this theory whose
structure is based on a fixed variable and an algebraic inequality, improves
and somehow generalizes the well-known least squares theory. In fact, the fixed
variable has a fundamental role in constituting the least p-variances theory.
In this sense, some new concepts such as p-covariances with respect to a fixed
variable, p-correlation coefficient with respect to a fixed variable and
p-uncorrelatedness with respect to a fixed variable are first defined in order
to establish least p-variance approximations. Then, we obtain a specific system
called p-covariances linear system and apply the p-uncorrelatedness condition
on its elements to find a general representation for p-uncorrelated variables.
Afterwards, we apply the concept of p-uncorrelatedness for continuous functions
particularly for polynomial sequences and find some new sequences such as a
generic two-parameter hypergeometric polynomial of 4F3 type that satisfy such a
p-uncorrelatedness property. In the sequel, we obtain an upper bound for
1-covariances, an approximation for p-variances, an improvement for the
approximate solutions of over-determined systems and an improvement for the
Bessel inequality and Parseval identity. Finally, we generalize the notion of
least p-variance approximations based on several fixed orthogonal variables.
</p>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14081" title="Abstract">arXiv:2310.14081</a> (cross-list from eess.IV) [<a href="/pdf/2310.14081" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing Modified Deep Learning Models in Efficient COVID19 Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Islam%2C+M+A">Md Aminul Islam</a> (1), 
<a href="/search/eess?searchtype=author&query=Shuvo%2C+S+A">Shabbir Ahmed Shuvo</a> (2), 
<a href="/search/eess?searchtype=author&query=Rony%2C+M+A+T">Mohammad Abu Tareq Rony</a> (3), 
<a href="/search/eess?searchtype=author&query=Raihan%2C+M">M Raihan</a> (4), 
<a href="/search/eess?searchtype=author&query=Sufian%2C+M+A">Md Abu Sufian</a> (5) ((1) Oxford Brookes University, UK, (2) Offenburg University of Applied Sciences, Germany, Noakhali Science and Technology University, Bangladesh (3), (4) Khulna University, Bangladesh (5) University of Leicester, UK)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The COVID19 pandemic, a unique and devastating respiratory disease outbreak,
has affected global populations as the disease spreads rapidly. Recent Deep
Learning breakthroughs may improve COVID19 prediction and forecasting as a tool
of precise and fast detection, however, current methods are still being
examined to achieve higher accuracy and precision. This study analyzed the
collection contained 8055 CT image samples, 5427 of which were COVID cases and
2628 non COVID. The 9544 Xray samples included 4044 COVID patients and 5500 non
COVID cases. The most accurate models are MobileNet V3 (97.872 percent),
DenseNet201 (97.567 percent), and GoogleNet Inception V1 (97.643 percent). High
accuracy indicates that these models can make many accurate predictions, as
well as others, are also high for MobileNetV3 and DenseNet201. An extensive
evaluation using accuracy, precision, and recall allows a comprehensive
comparison to improve predictive models by combining loss optimization with
scalable batch normalization in this study. Our analysis shows that these
tactics improve model performance and resilience for advancing COVID19
prediction and detection and shows how Deep Learning can improve disease
handling. The methods we suggest would strengthen healthcare systems,
policymakers, and researchers to make educated decisions to reduce COVID19 and
other contagious diseases.
<br />CCS CONCEPTS Covid,Deep Learning, Image Processing
<br />KEYWORDS Covid, Deep Learning, DenseNet201, MobileNet, ResNet, DenseNet,
GoogleNet, Image Processing, Disease Detection.
</p>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14094" title="Abstract">arXiv:2310.14094</a> (cross-list from physics.geo-ph) [<a href="/pdf/2310.14094" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DispersioNET: Joint Inversion of Rayleigh-Wave Multimode Phase Velocity  Dispersion Curves using Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sharma%2C+R">Rohan Sharma</a>, 
<a href="/search/physics?searchtype=author&query=Vashisth%2C+D">Divakar Vashisth</a>, 
<a href="/search/physics?searchtype=author&query=Shekar%2C+B">Bharath Shekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of the paper submitted to the 2nd Indian Near Surface Geophysics Conference &amp; Exhibition by AF Academy and the European Association of Geoscientists &amp; Engineers (EAGE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Rayleigh wave dispersion curves have been widely used in near-surface
studies, and are primarily inverted for the shear wave (S-wave) velocity
profiles. However, the inverse problem is ill-posed, non-unique and nonlinear.
Here, we introduce DispersioNET, a deep learning model based on convolution
neural networks (CNN) to perform the joint inversion of Rayleigh wave
fundamental and higher order mode phase velocity dispersion curves.
DispersioNET is trained and tested on both noise-free and noisy dispersion
curve datasets and predicts S-wave velocity profiles that match closely with
the true velocities. The architecture is agnostic to variations in S-wave
velocity profiles such as increasing velocity with depth and intermediate
low-velocity layers, while also ensuring that the output remains independent of
the number of layers.
</p>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14141" title="Abstract">arXiv:2310.14141</a> (cross-list from quant-ph) [<a href="/pdf/2310.14141" title="Download PDF">pdf</a>, <a href="/ps/2310.14141" title="Download PostScript">ps</a>, <a href="/format/2310.14141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum search by continuous-time quantum walk on t-designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lug%C3%A3o%2C+P+H+G">Pedro H. G. Lug&#xe3;o</a>, 
<a href="/search/quant-ph?searchtype=author&query=Portugal%2C+R">Renato Portugal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Combinatorics (math.CO)

</div>
<p class="mathjax">This work examines the time complexity of quantum search algorithms on
combinatorial $t$-designs with multiple marked elements using the
continuous-time quantum walk. Through a detailed exploration of $t$-designs and
their incidence matrices, we identify a subset of bipartite graphs that are
conducive to success compared to random-walk-based search algorithms. These
graphs have adjacency matrices with eigenvalues and eigenvectors that can be
determined algebraically and are also suitable for analysis in the
multiple-marked vertex scenario. We show that the continuous-time quantum walk
on certain symmetric $t$-designs achieves an optimal running time of
$O(\sqrt{n})$, where $n$ is the number of points and blocks, even when
accounting for an arbitrary number of marked elements. Upon examining two
primary configurations of marked elements distributions, we observe that the
success probability is consistently $o(1)$, but it approaches 1 asymptotically
in certain scenarios.
</p>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14168" title="Abstract">arXiv:2310.14168</a> (cross-list from math.OC) [<a href="/pdf/2310.14168" title="Download PDF">pdf</a>, <a href="/format/2310.14168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Forward Mode of Automatic Differentiation for Optimization  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shukla%2C+K">Khemraj Shukla</a>, 
<a href="/search/math?searchtype=author&query=Shin%2C+Y">Yeonjong Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 Pages, 8 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Backpropagation within neural networks leverages a fundamental element of
automatic differentiation, which is referred to as the reverse mode
differentiation, or vector Jacobian Product (VJP) or, in the context of
differential geometry, known as the pull-back process. The computation of
gradient are important as update of neural network parameters is performed
using gradient descent method. In this study, we present a genric randomized
method, which updates the parameters of neural networks by using directional
derivatives of loss functions computed efficiently by using forward mode AD or
Jacobian vector Product (JVP). These JVP are computed along the random
directions sampled from different probability distributions e.g., Bernoulli,
Normal, Wigner, Laplace and Uniform distributions. The computation of gradient
is performed during the forward pass of the neural network. We also present a
rigorous analysis of the presented methods providing the rate of convergence
along with the computational experiments deployed in scientific Machine
learning in particular physics-informed neural networks and Deep Operator
Networks.
</p>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14172" title="Abstract">arXiv:2310.14172</a> (cross-list from eess.IV) [<a href="/pdf/2310.14172" title="Download PDF">pdf</a>, <a href="/format/2310.14172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASC: Appearance and Structure Consistency for Unsupervised Domain  Adaptation in Fetal Brain MRI Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+Z">Zihang Xu</a>, 
<a href="/search/eess?searchtype=author&query=Gong%2C+H">Haifan Gong</a>, 
<a href="/search/eess?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haofeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023, released code: <a href="https://github.com/lhaof/ASC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Automatic tissue segmentation of fetal brain images is essential for the
quantitative analysis of prenatal neurodevelopment. However, producing
voxel-level annotations of fetal brain imaging is time-consuming and expensive.
To reduce labeling costs, we propose a practical unsupervised domain adaptation
(UDA) setting that adapts the segmentation labels of high-quality fetal brain
atlases to unlabeled fetal brain MRI data from another domain. To address the
task, we propose a new UDA framework based on Appearance and Structure
Consistency, named ASC. We adapt the segmentation model to the appearances of
different domains by constraining the consistency before and after a
frequency-based image transformation, which is to swap the appearance between
brain MRI data and atlases. Consider that even in the same domain, the fetal
brain images of different gestational ages could have significant variations in
the anatomical structures. To make the model adapt to the structural variations
in the target domain, we further encourage prediction consistency under
different structural perturbations. Extensive experiments on FeTA 2021
benchmark demonstrate the effectiveness of our ASC in comparison to
registration-based, semi-supervised learning-based, and existing UDA-based
methods.
</p>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14188" title="Abstract">arXiv:2310.14188</a> (cross-list from stat.ML) [<a href="/pdf/2310.14188" title="Download PDF">pdf</a>, <a href="/ps/2310.14188" title="Download PostScript">ps</a>, <a href="/format/2310.14188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Theory for Softmax Gating Multinomial Logistic Mixture of  Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Akbarian%2C+P">Pedram Akbarian</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+T">TrungTin Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Mixture-of-experts (MoE) model incorporates the power of multiple submodels
via gating functions to achieve greater performance in numerous regression and
classification applications. From a theoretical perspective, while there have
been previous attempts to comprehend the behavior of that model under the
regression settings through the convergence analysis of maximum likelihood
estimation in the Gaussian MoE model, such analysis under the setting of a
classification problem has remained missing in the literature. We close this
gap by establishing the convergence rates of density estimation and parameter
estimation in the softmax gating multinomial logistic MoE model. Notably, when
part of the expert parameters vanish, these rates are shown to be slower than
polynomial rates owing to an inherent interaction between the softmax gating
and expert functions via partial differential equations. To address this issue,
we propose using a novel class of modified softmax gating functions which
transform the input value before delivering them to the gating functions. As a
result, the previous interaction disappears and the parameter estimation rates
are significantly improved.
</p>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14197" title="Abstract">arXiv:2310.14197</a> (cross-list from eess.IV) [<a href="/pdf/2310.14197" title="Download PDF">pdf</a>, <a href="/format/2310.14197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion-based Data Augmentation for Nuclei Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+X">Xinyi Yu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+G">Guanbin Li</a>, 
<a href="/search/eess?searchtype=author&query=Lou%2C+W">Wei Lou</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haofeng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023, released code: <a href="https://github.com/lhaof/Nudiff">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Nuclei segmentation is a fundamental but challenging task in the quantitative
analysis of histopathology images. Although fully-supervised deep
learning-based methods have made significant progress, a large number of
labeled images are required to achieve great segmentation performance.
Considering that manually labeling all nuclei instances for a dataset is
inefficient, obtaining a large-scale human-annotated dataset is time-consuming
and labor-intensive. Therefore, augmenting a dataset with only a few labeled
images to improve the segmentation performance is of significant research and
application value. In this paper, we introduce the first diffusion-based
augmentation method for nuclei segmentation. The idea is to synthesize a large
number of labeled images to facilitate training the segmentation model. To
achieve this, we propose a two-step strategy. In the first step, we train an
unconditional diffusion model to synthesize the Nuclei Structure that is
defined as the representation of pixel-level semantic and distance transform.
Each synthetic nuclei structure will serve as a constraint on histopathology
image synthesis and is further post-processed to be an instance map. In the
second step, we train a conditioned diffusion model to synthesize
histopathology images based on nuclei structures. The synthetic histopathology
images paired with synthetic instance maps will be added to the real dataset
for training the segmentation model. The experimental results show that by
augmenting 10% labeled real dataset with synthetic samples, one can achieve
comparable segmentation results with the fully-supervised baseline.
</p>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14242" title="Abstract">arXiv:2310.14242</a> (cross-list from math.PR) [<a href="/pdf/2310.14242" title="Download PDF">pdf</a>, <a href="/format/2310.14242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composition and substitution of Regularity Structures B-series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bruned%2C+Y">Yvain Bruned</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA); Rings and Algebras (math.RA)

</div>
<p class="mathjax">In this work, we introduce Regularity Structures B-series which are used for
describing solutions of singular stochastic partial differential equations
(SPDEs). We define composition and substitutions of these B-series and as in
the context of B-series for ordinary differential equations, these operations
can rewritten via products and Hopf algebras which have been used for building
up renormalised models. These models provide a suitable topology for solving
singular SPDEs. This new construction sheds a new light on these products and
open interesting perspectives for the study of singular SPDEs in connection
with B-series.
</p>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14246" title="Abstract">arXiv:2310.14246</a> (cross-list from stat.ME) [<a href="/pdf/2310.14246" title="Download PDF">pdf</a>, <a href="/format/2310.14246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shortcuts for causal discovery of nonlinear models by score matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Montagna%2C+F">Francesco Montagna</a>, 
<a href="/search/stat?searchtype=author&query=Noceti%2C+N">Nicoletta Noceti</a>, 
<a href="/search/stat?searchtype=author&query=Rosasco%2C+L">Lorenzo Rosasco</a>, 
<a href="/search/stat?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The use of simulated data in the field of causal discovery is ubiquitous due
to the scarcity of annotated real data. Recently, Reisach et al., 2021
highlighted the emergence of patterns in simulated linear data, which displays
increasing marginal variance in the casual direction. As an ablation in their
experiments, Montagna et al., 2023 found that similar patterns may emerge in
nonlinear models for the variance of the score vector $\nabla \log
p_{\mathbf{X}}$, and introduced the ScoreSort algorithm. In this work, we
formally define and characterize this score-sortability pattern of nonlinear
additive noise models. We find that it defines a class of identifiable
(bivariate) causal models overlapping with nonlinear additive noise models. We
theoretically demonstrate the advantages of ScoreSort in terms of statistical
efficiency compared to prior state-of-the-art score matching-based methods and
empirically show the score-sortability of the most common synthetic benchmarks
in the literature. Our findings remark (1) the lack of diversity in the data as
an important limitation in the evaluation of nonlinear causal discovery
approaches, (2) the importance of thoroughly testing different settings within
a problem class, and (3) the importance of analyzing statistical properties in
causal discovery, where research is often limited to defining identifiability
conditions of the model.
</p>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14252" title="Abstract">arXiv:2310.14252</a> (cross-list from math.CO) [<a href="/pdf/2310.14252" title="Download PDF">pdf</a>, <a href="/format/2310.14252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proof of Irvine&#x27;s Conjecture via Mechanized Guessing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shallit%2C+J">Jeffrey Shallit</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">We prove a recent conjecture of Sean A. Irvine about a nonlinear recurrence,
using mechanized guessing and verification. The theorem-prover Walnut plays a
large role in the proof.
</p>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14260" title="Abstract">arXiv:2310.14260</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.14260" title="Download PDF">pdf</a>, <a href="/format/2310.14260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relation between size of mixing zone and intermediate concentration in  miscible displacement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bakharev%2C+F">Fedor Bakharev</a>, 
<a href="/search/physics?searchtype=author&query=Enin%2C+A">Aleksandr Enin</a>, 
<a href="/search/physics?searchtype=author&query=Matveenko%2C+S">Sergey Matveenko</a>, 
<a href="/search/physics?searchtype=author&query=Rastegaev%2C+N">Nikita Rastegaev</a>, 
<a href="/search/physics?searchtype=author&query=Pavlov%2C+D">Dmitry Pavlov</a>, 
<a href="/search/physics?searchtype=author&query=Tikhomirov%2C+S">Sergey Tikhomirov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Analysis of PDEs (math.AP); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We investigate the miscible displacement of a viscous liquid by a less
viscous one in a porous medium, which frequently leads to the formation of a
mixing zone characterized by thin fingers. The mixing zone grows in time due to
the difference in speed between the leading and trailing edges. The transverse
flow equilibrium (TFE) model provides estimates of these speeds. We propose an
enhancement for the TFE estimates. It is based on the assumption that an
intermediate concentration exists near the tip of the finger, which allows to
reduce the integration interval in the speed estimate. Numerical simulations of
the computational fluid dynamics model were conducted to validate the new
estimates. The refined estimates offer greater accuracy than those provided by
the original TFE model.
</p>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14270" title="Abstract">arXiv:2310.14270</a> (cross-list from eess.AS) [<a href="/pdf/2310.14270" title="Download PDF">pdf</a>, <a href="/format/2310.14270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion-Based Adversarial Purification for Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bai%2C+Y">Yibo Bai</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiao-Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Recently, automatic speaker verification (ASV) based on deep learning is
easily contaminated by adversarial attacks, which is a new type of attack that
injects imperceptible perturbations to audio signals so as to make ASV produce
wrong decisions. This poses a significant threat to the security and
reliability of ASV systems. To address this issue, we propose a Diffusion-Based
Adversarial Purification (DAP) method that enhances the robustness of ASV
systems against such adversarial attacks. Our method leverages a conditional
denoising diffusion probabilistic model to effectively purify the adversarial
examples and mitigate the impact of perturbations. DAP first introduces
controlled noise into adversarial examples, and then performs a reverse
denoising process to reconstruct clean audio. Experimental results demonstrate
the efficacy of the proposed DAP in enhancing the security of ASV and meanwhile
minimizing the distortion of the purified audio signals.
</p>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14281" title="Abstract">arXiv:2310.14281</a> (cross-list from math.CO) [<a href="/pdf/2310.14281" title="Download PDF">pdf</a>, <a href="/ps/2310.14281" title="Download PostScript">ps</a>, <a href="/format/2310.14281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $t$-designs obtained from two shells and exceptional examples of them
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Awada%2C+M">Madoka Awada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages. arXiv admin note: text overlap with <a href="/abs/2309.03206">arXiv:2309.03206</a>; text overlap with <a href="/abs/2305.03285">arXiv:2305.03285</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT); Group Theory (math.GR); Number Theory (math.NT)

</div>
<p class="mathjax">In this paper, we construct $3$-designs using extended quadratic residue
codes over $\mathbb {F}_q$ and their dual codes. We give as a corollary
$3$-designs that do not follow from the transitivity argument and the
Assmus--Mattson Theorem.
</p>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14286" title="Abstract">arXiv:2310.14286</a> (cross-list from stat.ML) [<a href="/pdf/2310.14286" title="Download PDF">pdf</a>, <a href="/ps/2310.14286" title="Download PostScript">ps</a>, <a href="/format/2310.14286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-Sample Analysis of the Temporal Difference Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Samsonov%2C+S">Sergey Samsonov</a>, 
<a href="/search/stat?searchtype=author&query=Tiapkin%2C+D">Daniil Tiapkin</a>, 
<a href="/search/stat?searchtype=author&query=Naumov%2C+A">Alexey Naumov</a>, 
<a href="/search/stat?searchtype=author&query=Moulines%2C+E">Eric Moulines</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper we consider the problem of obtaining sharp bounds for the
performance of temporal difference (TD) methods with linear functional
approximation for policy evaluation in discounted Markov Decision Processes. We
show that a simple algorithm with a universal and instance-independent step
size together with Polyak-Ruppert tail averaging is sufficient to obtain
near-optimal variance and bias terms. We also provide the respective sample
complexity bounds. Our proof technique is based on refined error bounds for
linear stochastic approximation together with the novel stability result for
the product of random matrices that arise from the TD-type recurrence.
</p>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14300" title="Abstract">arXiv:2310.14300</a> (cross-list from eess.AS) [<a href="/pdf/2310.14300" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MFCC-GAN Codec: A New AI-based Audio Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hasanabadi%2C+M+R">Mohammad Reza Hasanabadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ABU Technical Review journal 2023/3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In this paper, we proposed AI-based audio coding using MFCC features in an
adversarial setting. We combined a conventional encoder with an adversarial
learning decoder to better reconstruct the original waveform. Since GAN gives
implicit density estimation, therefore, such models are less prone to
overfitting. We compared our work with five well-known codecs namely AAC, AC3,
Opus, Vorbis, and Speex, performing on bitrates from 2kbps to 128kbps.
MFCCGAN_36k achieved the state-of-the-art result in terms of SNR despite a
lower bitrate in comparison to AC3_128k, AAC_112k, Vorbis_48k, Opus_48k, and
Speex_48K. On the other hand, MFCCGAN_13k also achieved high SNR=27 which is
equal to that of AC3_128k, and AAC_112k while having a significantly lower
bitrate (13 kbps). MFCCGAN_36k achieved higher NISQA-MOS results compared to
AAC_48k while having a 20% lower bitrate. Furthermore, MFCCGAN_13k obtained
NISQAMOS= 3.9 which is much higher than AAC_24k, AAC_32k, AC3_32k, and AAC_48k.
For future work, we finally suggest adopting loss functions optimizing
intelligibility and perceptual metrics in the MFCCGAN structure to improve
quality and intelligibility simultaneously.
</p>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14301" title="Abstract">arXiv:2310.14301</a> (cross-list from eess.AS) [<a href="/pdf/2310.14301" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An overview of text-to-speech systems and media applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hasanabadi%2C+M+R">Mohammad Reza Hasanabadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ABU Technical Review journal 2023/6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Producing synthetic voice, similar to human-like sound, is an emerging
novelty of modern interactive media systems. Text-To-Speech (TTS) systems try
to generate synthetic and authentic voices via text input. Besides, well known
and familiar dubbing, announcing and narrating voices, as valuable possessions
of any media organization, can be kept forever by utilizing TTS and Voice
Conversion (VC) algorithms . The emergence of deep learning approaches has made
such TTS systems more accurate and accessible. To understand TTS systems
better, this paper investigates the key components of such systems including
text analysis, acoustic modelling and vocoding. The paper then provides details
of important state-of-the-art TTS systems based on deep learning. Finally, a
comparison is made between recently released systems in term of backbone
architecture, type of input and conversion, vocoder used and subjective
assessment (MOS). Accordingly, Tacotron 2, Transformer TTS, WaveNet and
FastSpeech 1 are among the most successful TTS systems ever released. In the
discussion section, some suggestions are made to develop a TTS system with
regard to the intended application.
</p>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14321" title="Abstract">arXiv:2310.14321</a> (cross-list from eess.SP) [<a href="/pdf/2310.14321" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Piezoelectric Sensors for Real-time Monitoring and Quality Control in  Additive Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Momin%2C+R+T">Rashid T. Momin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 Pages, encompassing electrical element (piezoelectric sensor) and its use in advanced manufacturing technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Within the ever-evolving landscape of engineering, particularly in the
dynamic domain of additive In manufacturing, a pursuit of precision and
excellence in production processes takes centre stage. This research , This
paper serves to give a comprehensive understanding of piezoelectric sensors, a
topic that is both academically engaging and of practical significance,
catering to both seasoned experts and those newly venturing into the field.
Additive manufacturing, lauded for its groundbreaking potential, underscores
the imperative of rigorous quality control. This introduces piezoelectric
sensors, devices that may be unfamiliar to many but possess considerable
potential. This paper embarks on a methodical journey, commencing with an
introductory elucidation of the piezoelectric effect. It then advances to the
vital role of piezoelectric sensors in real-time monitoring and quality
control, unveiling their potential and relevance for newcomers and seasoned
professionals alike. This research, structured systematically from fundamental
principles to pragmatic applications, presents findings that are not only
academically informative but also represent a substantial stride towards
achieving precision and high-quality manufacturing processes in the engineering
field.
</p>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14354" title="Abstract">arXiv:2310.14354</a> (cross-list from eess.IV) [<a href="/pdf/2310.14354" title="Download PDF">pdf</a>, <a href="/format/2310.14354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Flare-Free Images: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kotp%2C+Y">Yousef Kotp</a>, 
<a href="/search/eess?searchtype=author&query=Torki%2C+M">Marwan Torki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Lens flare is a common image artifact that can significantly degrade image
quality and affect the performance of computer vision systems due to a strong
light source pointing at the camera. This survey provides a comprehensive
overview of the multifaceted domain of lens flare, encompassing its underlying
physics, influencing factors, types, and characteristics. It delves into the
complex optics of flare formation, arising from factors like internal
reflection, scattering, diffraction, and dispersion within the camera lens
system. The diverse categories of flare are explored, including scattering,
reflective, glare, orb, and starburst types. Key properties such as shape,
color, and localization are analyzed. The numerous factors impacting flare
appearance are discussed, spanning light source attributes, lens features,
camera settings, and scene content. The survey extensively covers the wide
range of methods proposed for flare removal, including hardware optimization
strategies, classical image processing techniques, and learning-based methods
using deep learning. It not only describes pioneering flare datasets created
for training and evaluation purposes but also how they were created. Commonly
employed performance metrics such as PSNR, SSIM, and LPIPS are explored.
Challenges posed by flare's complex and data-dependent characteristics are
highlighted. The survey provides insights into best practices, limitations, and
promising future directions for flare removal research. Reviewing the
state-of-the-art enables an in-depth understanding of the inherent complexities
of the flare phenomenon and the capabilities of existing solutions. This can
inform and inspire new innovations for handling lens flare artifacts and
improving visual quality across various applications.
</p>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14395" title="Abstract">arXiv:2310.14395</a> (cross-list from cond-mat.stat-mech) [<a href="/pdf/2310.14395" title="Download PDF">pdf</a>, <a href="/ps/2310.14395" title="Download PostScript">ps</a>, <a href="/format/2310.14395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal representation by Boltzmann machines with Regularised Axons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Grzybowski%2C+P+R">Przemys&#x142;aw R. Grzybowski</a>, 
<a href="/search/cond-mat?searchtype=author&query=Jankiewicz%2C+A">Antoni Jankiewicz</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pi%C3%B1ol%2C+E">Eloy Pi&#xf1;ol</a>, 
<a href="/search/cond-mat?searchtype=author&query=Cirauqui%2C+D">David Cirauqui</a>, 
<a href="/search/cond-mat?searchtype=author&query=Grzybowska%2C+D+H">Dorota H. Grzybowska</a>, 
<a href="/search/cond-mat?searchtype=author&query=Petrykowski%2C+P+M">Pawe&#x142; M. Petrykowski</a>, 
<a href="/search/cond-mat?searchtype=author&query=Garc%C3%ADa-March%2C+M+%C3%81">Miguel &#xc1;ngel Garc&#xed;a-March</a>, 
<a href="/search/cond-mat?searchtype=author&query=Lewenstein%2C+M">Maciej Lewenstein</a>, 
<a href="/search/cond-mat?searchtype=author&query=Mu%C3%B1oz-Gil%2C+G">Gorka Mu&#xf1;oz-Gil</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pozas-Kerstjens%2C+A">Alejandro Pozas-Kerstjens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">It is widely known that Boltzmann machines are capable of representing
arbitrary probability distributions over the values of their visible neurons,
given enough hidden ones. However, sampling -- and thus training -- these
models can be numerically hard. Recently we proposed a regularisation of the
connections of Boltzmann machines, in order to control the energy landscape of
the model, paving a way for efficient sampling and training. Here we formally
prove that such regularised Boltzmann machines preserve the ability to
represent arbitrary distributions. This is in conjunction with controlling the
number of energy local minima, thus enabling easy \emph{guided} sampling and
training. Furthermore, we explicitly show that regularised Boltzmann machines
can store exponentially many arbitrarily correlated visible patterns with
perfect retrieval, and we connect them to the Dense Associative Memory
networks.
</p>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14421" title="Abstract">arXiv:2310.14421</a> (cross-list from stat.ML) [<a href="/pdf/2310.14421" title="Download PDF">pdf</a>, <a href="/format/2310.14421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On existence, uniqueness and scalability of adversarial robustness  measures for AI classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Horenko%2C+I">Illia Horenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Simply-verifiable mathematical conditions for existence, uniqueness and
explicit analytical computation of minimal adversarial paths (MAP) and minimal
adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for
generalized linear models (GLM), and for entropic AI (EAI) are formulated and
proven. Practical computation of MAP and MAD, their comparison and
interpretations for various classes of AI tools (for neuronal networks, boosted
random forests, GLM and EAI) are demonstrated on the common synthetic
benchmarks: on a double Swiss roll spiral and its extensions, as well as on the
two biomedical data problems (for the health insurance claim predictions, and
for the heart attack lethality classification). On biomedical applications it
is demonstrated how MAP provides unique minimal patient-specific
risk-mitigating interventions in the predefined subsets of accessible control
variables.
</p>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14464" title="Abstract">arXiv:2310.14464</a> (cross-list from quant-ph) [<a href="/pdf/2310.14464" title="Download PDF">pdf</a>, <a href="/ps/2310.14464" title="Download PostScript">ps</a>, <a href="/format/2310.14464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Cryptographic Perspective on the Verifiability of Quantum Advantage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chia%2C+N">Nai-Hui Chia</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fu%2C+H">Honghao Fu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Song%2C+F">Fang Song</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yao%2C+P">Penghui Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In recent years, achieving verifiable quantum advantage on a NISQ device has
emerged as an important open problem in quantum information. The sampling-based
quantum advantages are not known to have efficient verification methods. This
paper investigates the verification of quantum advantage from a cryptographic
perspective. We establish a strong connection between the verifiability of
quantum advantage and cryptographic and complexity primitives, including
efficiently samplable, statistically far but computationally indistinguishable
pairs of (mixed) quantum states ($\mathsf{EFI}$), pseudorandom states
($\mathsf{PRS}$), and variants of minimum circuit size problems
($\mathsf{MCSP}$). Specifically, we prove that a) a sampling-based quantum
advantage is either verifiable or can be used to build $\mathsf{EFI}$ and even
$\mathsf{PRS}$ and b) polynomial-time algorithms for a variant of
$\mathsf{MCSP}$ would imply efficient verification of quantum advantages.
<br />Our work shows that the quest for verifiable quantum advantages may lead to
applications of quantum cryptography, and the construction of quantum
primitives can provide new insights into the verifiability of quantum
advantages.
</p>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14498" title="Abstract">arXiv:2310.14498</a> (cross-list from physics.ed-ph) [<a href="/pdf/2310.14498" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reforming Physics Exams Using Openly Accessible Large Isomorphic Problem  Banks created with the assistance of Generative AI: an Explorative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chen%2C+Z">Zhongzhou Chen</a>, 
<a href="/search/physics?searchtype=author&query=Frederick%2C+E">Emily Frederick</a>, 
<a href="/search/physics?searchtype=author&query=Cui%2C+C">Colleen Cui</a>, 
<a href="/search/physics?searchtype=author&query=Khan%2C+M">Munaimah Khan</a>, 
<a href="/search/physics?searchtype=author&query=Klatt%2C+C">Christopher Klatt</a>, 
<a href="/search/physics?searchtype=author&query=Huang%2C+M">Mercedith Huang</a>, 
<a href="/search/physics?searchtype=author&query=Su%2C+S">Shiyang Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics Education (physics.ed-ph)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This paper explores using large isomorphic problem banks to overcome many
challenges of traditional exams in large STEM classes, especially the threat of
content sharing websites and generative AI to the security of exam items. We
first introduce an efficient procedure for creating large numbers of isomorphic
physics problems, assisted by the large language model GPT-3 and several other
open-source tools. We then propose that if exam items are randomly drawn from
large enough problem banks, then giving students open access to problem banks
prior to the exam will not dramatically impact students' performance on the
exam or lead to wide-spread rote-memorization of solutions. We tested this
hypothesis on two mid-term physics exams, comparing students' performance on
problems drawn from open isomorphic problem banks to similar transfer problems
that were not accessible to students prior to the exam. We found that on both
exams, both open bank and transfer problems had the highest difficulty. The
differences in percent correct were between 5% to 10%, which is comparable to
the differences between different isomorphic versions of the same problem type.
Item response theory analysis found that both types of problem have high
discrimination (&gt;1.5) with no significant differences. Student performance on
open-bank and transfer problems are highly correlated with each other, and the
correlations are stronger than average correlations between problems on the
exam. Exploratory factor analysis also found that open-bank and transfer
problems load on the same factor, and even formed their own factor on the
second exam. Those observations all suggest that giving students open access to
large isomorphic problem banks only had a small impact on students' performance
on the exam but could have significant potential in reforming traditional
classroom exams.
</p>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14506" title="Abstract">arXiv:2310.14506</a> (cross-list from eess.SP) [<a href="/pdf/2310.14506" title="Download PDF">pdf</a>, <a href="/format/2310.14506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label Space Partition Selection for Multi-Object Tracking Using  Two-Layer Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+Y">Ji Youn Lee</a>, 
<a href="/search/eess?searchtype=author&query=Shim%2C+C">Changbeom Shim</a>, 
<a href="/search/eess?searchtype=author&query=Van+Nguyen%2C+H">Hoa Van Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+T+T+D">Tran Thien Dat Nguyen</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+H">Hyunjin Choi</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+Y">Youngho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Estimating the trajectories of multi-objects poses a significant challenge
due to data association ambiguity, which leads to a substantial increase in
computational requirements. To address such problems, a divide-and-conquer
manner has been employed with parallel computation. In this strategy,
distinguished objects that have unique labels are grouped based on their
statistical dependencies, the intersection of predicted measurements. Several
geometry approaches have been used for label grouping since finding all
intersected label pairs is clearly infeasible for large-scale tracking
problems. This paper proposes an efficient implementation of label grouping for
label-partitioned generalized labeled multi-Bernoulli filter framework using a
secondary partitioning technique. This allows for parallel computation in the
label graph indexing step, avoiding generating and eliminating duplicate
comparisons. Additionally, we compare the performance of the proposed technique
with several efficient spatial searching algorithms. The results demonstrate
the superior performance of the proposed approach on large-scale data sets,
enabling scalable trajectory estimation.
</p>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14521" title="Abstract">arXiv:2310.14521</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.14521" title="Download PDF">pdf</a>, <a href="/format/2310.14521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-Nearest-Neighbors Induced Topological PCA for scRNA Sequence Data  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Cottrell%2C+S">Sean Cottrell</a>, 
<a href="/search/q-bio?searchtype=author&query=Hozumi%2C+Y">Yuta Hozumi</a>, 
<a href="/search/q-bio?searchtype=author&query=Wei%2C+G">Guo-Wei Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Algebraic Topology (math.AT)

</div>
<p class="mathjax">Single-cell RNA sequencing (scRNA-seq) is widely used to reveal heterogeneity
in cells, which has given us insights into cell-cell communication, cell
differentiation, and differential gene expression. However, analyzing scRNA-seq
data is a challenge due to sparsity and the large number of genes involved.
Therefore, dimensionality reduction and feature selection are important for
removing spurious signals and enhancing downstream analysis. Traditional PCA, a
main workhorse in dimensionality reduction, lacks the ability to capture
geometrical structure information embedded in the data, and previous graph
Laplacian regularizations are limited by the analysis of only a single scale.
We propose a topological Principal Components Analysis (tPCA) method by the
combination of persistent Laplacian (PL) technique and L$_{2,1}$ norm
regularization to address multiscale and multiclass heterogeneity issues in
data. We further introduce a k-Nearest-Neighbor (kNN) persistent Laplacian
technique to improve the robustness of our persistent Laplacian method. The
proposed kNN-PL is a new algebraic topology technique which addresses the many
limitations of the traditional persistent homology. Rather than inducing
filtration via the varying of a distance threshold, we introduced kNN-tPCA,
where filtrations are achieved by varying the number of neighbors in a kNN
network at each step, and find that this framework has significant implications
for hyper-parameter tuning. We validate the efficacy of our proposed tPCA and
kNN-tPCA methods on 11 diverse benchmark scRNA-seq datasets, and showcase that
our methods outperform other unsupervised PCA enhancements from the literature,
as well as popular Uniform Manifold Approximation (UMAP), t-Distributed
Stochastic Neighbor Embedding (tSNE), and Projection Non-Negative Matrix
Factorization (NMF) by significant margins.
</p>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14522" title="Abstract">arXiv:2310.14522</a> (cross-list from math.OC) [<a href="/pdf/2310.14522" title="Download PDF">pdf</a>, <a href="/format/2310.14522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A kernel-based method for Schr&#xf6;dinger bridges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nakano%2C+Y">Yumiharu Nakano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We characterize the Schr\"odinger bridge problems by a family of
Mckean-Vlasov stochastic control problems with no terminal time distribution
constraint. In doing so, we use the theory of Hilbert space embeddings of
probability measures and then describe the constraint as penalty terms defined
by the maximum mean discrepancy in the control problems. A sequence of the
probability laws of the state processes resulting from $\epsilon$-optimal
controls converges to a unique solution of the Schr\"odinger's problem under
mild conditions on given initial and terminal time distributions and an
underlying diffusion process. We propose a neural SDE based deep learning
algorithm for the Mckean-Vlasov stochastic control problems. Several numerical
experiments validate our methods.
</p>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14544" title="Abstract">arXiv:2310.14544</a> (cross-list from stat.ML) [<a href="/pdf/2310.14544" title="Download PDF">pdf</a>, <a href="/format/2310.14544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trigonometric Quadrature Fourier Features for Scalable Gaussian Process  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+K">Kevin Li</a>, 
<a href="/search/stat?searchtype=author&query=Balakirsky%2C+M">Max Balakirsky</a>, 
<a href="/search/stat?searchtype=author&query=Mak%2C+S">Simon Mak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Fourier feature approximations have been successfully applied in the
literature for scalable Gaussian Process (GP) regression. In particular,
Quadrature Fourier Features (QFF) derived from Gaussian quadrature rules have
gained popularity in recent years due to their improved approximation accuracy
and better calibrated uncertainty estimates compared to Random Fourier Feature
(RFF) methods. However, a key limitation of QFF is that its performance can
suffer from well-known pathologies related to highly oscillatory quadrature,
resulting in mediocre approximation with limited features. We address this
critical issue via a new Trigonometric Quadrature Fourier Feature (TQFF)
method, which uses a novel non-Gaussian quadrature rule specifically tailored
for the desired Fourier transform. We derive an exact quadrature rule for TQFF,
along with kernel approximation error bounds for the resulting feature map. We
then demonstrate the improved performance of our method over RFF and Gaussian
QFF in a suite of numerical experiments and applications, and show the TQFF
enjoys accurate GP approximations over a broad range of length-scales using
fewer features.
</p>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14555" title="Abstract">arXiv:2310.14555</a> (cross-list from physics.geo-ph) [<a href="/pdf/2310.14555" title="Download PDF">pdf</a>, <a href="/format/2310.14555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling groundwater levels in California&#x27;s Central Valley by  hierarchical Gaussian process and neural network regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Pradhan%2C+A">Anshuman Pradhan</a>, 
<a href="/search/physics?searchtype=author&query=Adams%2C+K+H">Kyra H. Adams</a>, 
<a href="/search/physics?searchtype=author&query=Chandrasekaran%2C+V">Venkat Chandrasekaran</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/physics?searchtype=author&query=Reager%2C+J+T">John T. Reager</a>, 
<a href="/search/physics?searchtype=author&query=Stuart%2C+A+M">Andrew M. Stuart</a>, 
<a href="/search/physics?searchtype=author&query=Turmon%2C+M+J">Michael J. Turmon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Water Resources Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Modeling groundwater levels continuously across California's Central Valley
(CV) hydrological system is challenging due to low-quality well data which is
sparsely and noisily sampled across time and space. A novel machine learning
method is proposed for modeling groundwater levels by learning from a 3D
lithological texture model of the CV aquifer. The proposed formulation performs
multivariate regression by combining Gaussian processes (GP) and deep neural
networks (DNN). Proposed hierarchical modeling approach constitutes training
the DNN to learn a lithologically informed latent space where non-parametric
regression with GP is performed. The methodology is applied for modeling
groundwater levels across the CV during 2015 - 2020. We demonstrate the
efficacy of GP-DNN regression for modeling non-stationary features in the well
data with fast and reliable uncertainty quantification. Our results indicate
that the 2017 and 2019 wet years in California were largely ineffective in
replenishing the groundwater loss caused during previous drought years.
</p>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14608" title="Abstract">arXiv:2310.14608</a> (cross-list from stat.ML) [<a href="/pdf/2310.14608" title="Download PDF">pdf</a>, <a href="/format/2310.14608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAD-DA: Controllable Anomaly Detection after Domain Adaptation by  Statistical Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Duy%2C+V+N+L">Vo Nguyen Le Duy</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+H">Hsuan-Tien Lin</a>, 
<a href="/search/stat?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a novel statistical method for testing the results of anomaly
detection (AD) under domain adaptation (DA), which we call CAD-DA --
controllable AD under DA. The distinct advantage of the CAD-DA lies in its
ability to control the probability of misidentifying anomalies under a
pre-specified level $\alpha$ (e.g., 0.05). The challenge within this DA setting
is the necessity to account for the influence of DA to ensure the validity of
the inference results. Our solution to this challenge leverages the concept of
conditional Selective Inference to handle the impact of DA. To our knowledge,
this is the first work capable of conducting a valid statistical inference
within the context of DA. We evaluate the performance of the CAD-DA method on
both synthetic and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14621" title="Abstract">arXiv:2310.14621</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.14621" title="Download PDF">pdf</a>, <a href="/format/2310.14621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking mode-based neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Lin%2C+Z">Zhanghan Lin</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+H">Haiping Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Spiking neural networks play an important role in brain-like neuromorphic
computations and in studying working mechanisms of neural circuits. One
drawback of training a large scale spiking neural network is that an expensive
cost of updating all weights is required. Furthermore, after training, all
information related to the computational task is hidden into the weight matrix,
prohibiting us from a transparent understanding of circuit mechanisms.
Therefore, in this work, we address these challenges by proposing a spiking
mode-based training protocol. The first advantage is that the weight is
interpreted by input and output modes and their associated scores
characterizing importance of each decomposition term. The number of modes is
thus adjustable, allowing more degrees of freedom for modeling the experimental
data. This reduces a sizable training cost because of significantly reduced
space complexity for learning. The second advantage is that one can project the
high dimensional neural activity in the ambient space onto the mode space which
is typically of a low dimension, e.g., a few modes are sufficient to capture
the shape of the underlying neural manifolds. We analyze our framework in two
computational tasks -- digit classification and selective sensory integration
tasks. Our work thus derives a mode-based learning rule for spiking neural
networks.
</p>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14636" title="Abstract">arXiv:2310.14636</a> (cross-list from eess.IV) [<a href="/pdf/2310.14636" title="Download PDF">pdf</a>, <a href="/format/2310.14636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Perception Boundary-guided Network for Breast Lesion  Segmentation in Ultrasound Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+X">Xing Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Q">Qijian Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lihui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Automatic segmentation of breast tumors from the ultrasound images is
essential for the subsequent clinical diagnosis and treatment plan. Although
the existing deep learning-based methods have achieved significant progress in
automatic segmentation of breast tumor, their performance on tumors with
similar intensity to the normal tissues is still not pleasant, especially for
the tumor boundaries. To address this issue, we propose a PBNet composed by a
multilevel global perception module (MGPM) and a boundary guided module (BGM)
to segment breast tumors from ultrasound images. Specifically, in MGPM, the
long-range spatial dependence between the voxels in a single level feature maps
are modeled, and then the multilevel semantic information is fused to promote
the recognition ability of the model for non-enhanced tumors. In BGM, the tumor
boundaries are extracted from the high-level semantic maps using the dilation
and erosion effects of max pooling, such boundaries are then used to guide the
fusion of low and high-level features. Moreover, to improve the segmentation
performance for tumor boundaries, a multi-level boundary-enhanced segmentation
(BS) loss is proposed. The extensive comparison experiments on both publicly
available dataset and in-house dataset demonstrate that the proposed PBNet
outperforms the state-of-the-art methods in terms of both qualitative
visualization results and quantitative evaluation metrics, with the Dice score,
Jaccard coefficient, Specificity and HD95 improved by 0.70%, 1.1%, 0.1% and
2.5% respectively. In addition, the ablation experiments validate that the
proposed MGPM is indeed beneficial for distinguishing the non-enhanced tumors
and the BGM as well as the BS loss are also helpful for refining the
segmentation contours of the tumor.
</p>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14663" title="Abstract">arXiv:2310.14663</a> (cross-list from eess.AS) [<a href="/pdf/2310.14663" title="Download PDF">pdf</a>, <a href="/format/2310.14663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPP-TTS: Diversifying prosodic features of speech via determinantal  point processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Joo%2C+S">Seongho Joo</a>, 
<a href="/search/eess?searchtype=author&query=Koh%2C+H">Hyukhun Koh</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">With the rapid advancement in deep generative models, recent neural
Text-To-Speech(TTS) models have succeeded in synthesizing human-like speech.
There have been some efforts to generate speech with various prosody beyond
monotonous prosody patterns. However, previous works have several limitations.
First, typical TTS models depend on the scaled sampling temperature for
boosting the diversity of prosody. Speech samples generated at high sampling
temperatures often lack perceptual prosodic diversity, which can adversely
affect the naturalness of the speech. Second, the diversity among samples is
neglected since the sampling procedure often focuses on a single speech sample
rather than multiple ones. In this paper, we propose DPP-TTS: a text-to-speech
model based on Determinantal Point Processes (DPPs) with a prosody diversifying
module. Our TTS model is capable of generating speech samples that
simultaneously consider perceptual diversity in each sample and among multiple
samples. We demonstrate that DPP-TTS generates speech samples with more
diversified prosody than baselines in the side-by-side comparison test
considering the naturalness of speech at the same time.
</p>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14704" title="Abstract">arXiv:2310.14704</a> (cross-list from eess.SP) [<a href="/pdf/2310.14704" title="Download PDF">pdf</a>, <a href="/format/2310.14704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Implementation of an RSSI-Based Bluetooth Low Energy Indoor  Localization System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cortesi%2C+S">Silvano Cortesi</a>, 
<a href="/search/eess?searchtype=author&query=Dreher%2C+M">Marc Dreher</a>, 
<a href="/search/eess?searchtype=author&query=Magno%2C+M">Michele Magno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article has been accepted for publication in the proceedings of the 2021 IEEE International Conference on Wireless and Mobile Computing, Networking And Communications (WiMob). DOI: 10.1109/WiMob52687.2021.9606272
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Indoor Positioning System (IPS) is a crucial technology that enables medical
staff and hospital managements to accurately locate and track persons or assets
inside the medical buildings. Among other technologies, Bluetooth Low Energy
(BLE) can be exploited for achieving an energy-efficient and low-cost solution.
This work presents the design and implementation of an received signal strength
indicator (RSSI)-based indoor localization system. The paper shows the
implementation of a low complex weighted k-Nearest Neighbors algorithm that
processes raw RSSI data from connection-less iBeacon's. The designed hardware
and firmware are implemented around the low-power and low-cost nRF52832 from
Nordic Semiconductor. Experimental evaluation with the real-time data
processing has been evaluated and presented in a 7.2 m by 7.2 m room with
furniture and 5 beacon nodes. The experimental results show an average error of
only 0.72 m in realistic conditions. Finally, the overall power consumption of
the fixed beacon with a periodic advertisement of 100 ms is only 50 uA at 3 V,
which leads to a long-lasting solution of over one year with a 500 mAh coin
battery.
</p>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14710" title="Abstract">arXiv:2310.14710</a> (cross-list from stat.ML) [<a href="/pdf/2310.14710" title="Download PDF">pdf</a>, <a href="/format/2310.14710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Forest Dissimilarity for High-Dimension Low Sample Size  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cavalheiro%2C+L+P">Lucca Portes Cavalheiro</a>, 
<a href="/search/stat?searchtype=author&query=Bernard%2C+S">Simon Bernard</a>, 
<a href="/search/stat?searchtype=author&query=Barddal%2C+J+P">Jean Paul Barddal</a>, 
<a href="/search/stat?searchtype=author&query=Heutte%2C+L">Laurent Heutte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages. To be published in statistics and computing (accepted September 26, 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Stat Comput 34, 9 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">High dimension, low sample size (HDLSS) problems are numerous among
real-world applications of machine learning. From medical images to text
processing, traditional machine learning algorithms are usually unsuccessful in
learning the best possible concept from such data. In a previous work, we
proposed a dissimilarity-based approach for multi-view classification, the
Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for
such problems. In this work, we transpose the core principle of this approach
to solving HDLSS classification problems, by using the RF similarity measure as
a learned precomputed SVM kernel (RFSVM). We show that such a learned
similarity measure is particularly suited and accurate for this classification
context. Experiments conducted on 40 public HDLSS classification datasets,
supported by rigorous statistical analyses, show that the RFSVM method
outperforms existing methods for the majority of HDLSS problems and remains at
the same time very competitive for low or non-HDLSS problems.
</p>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14762" title="Abstract">arXiv:2310.14762</a> (cross-list from math.CA) [<a href="/pdf/2310.14762" title="Download PDF">pdf</a>, <a href="/format/2310.14762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Native spaces and generalization of Wu functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yixuan Huang</a> (1 and 2), 
<a href="/search/math?searchtype=author&query=Wu%2C+Z">Zongmin Wu</a> (3 and 4), 
<a href="/search/math?searchtype=author&query=Zhu%2C+S">Shengxin Zhu</a> (5 and 6) ((1) School of Mathematics, Beijing Normal University, China, (2) Laboratory of Mathematics and Complex Systems, Ministry of Education, China, (3) Shanghai Key Laboratory for Contemporary Applied Mathematics, School of Mathematical Sciences, Fudan University, Shanghai, China, Shanghai Center for Mathematical Sciences, Shanghai, China, (4) School of Big Data and Statistics, Anhui University, Hefei, China, (5) Research Center for Mathematics, Advanced Institute of Natural Science, Beijing Normal University, Zhuhai, China, (6) Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International College, Zhuhai, China)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We prove that the native space of a Wu function is a dense subspace of a
Sobolev space. An explicit characterization of the native spaces of Wu
functions is given. Three definitions of Wu functions are introduced and proven
to be equivalent. Based on these new equivalent definitions and the so called
$f$-form tricks, we can generalize the Wu functions into the even-dimensional
spaces $\R^{2k}$, while the original Wu functions are only defined in the
odd-dimensional spaces $\R^{2k+1}$. Such functions in even-dimensional spaces
are referred to as the `missing Wu functions'. Furthermore we can generalize
the Wu functions into `fractional'-dimensional spaces. We call all these Wu
functions the generalized Wu functions. The closed form of the generalized Wu
functions are given in terms of hypergeometric functions. Finally we prove that
the Wu functions and the missing Wu functions can be written as linear
combinations of the generalized Wendland functions.
</p>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14763" title="Abstract">arXiv:2310.14763</a> (cross-list from stat.ME) [<a href="/pdf/2310.14763" title="Download PDF">pdf</a>, <a href="/format/2310.14763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Externally Valid Policy Evaluation Combining Trial and Observational  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ek%2C+S">Sofia Ek</a>, 
<a href="/search/stat?searchtype=author&query=Zachariah%2C+D">Dave Zachariah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Randomized trials are widely considered as the gold standard for evaluating
the effects of decision policies. Trial data is, however, drawn from a
population which may differ from the intended target population and this raises
a problem of external validity (aka. generalizability). In this paper we seek
to use trial data to draw valid inferences about the outcome of a policy on the
target population. Additional covariate data from the target population is used
to model the sampling of individuals in the trial study. We develop a method
that yields certifiably valid trial-based policy evaluations under any
specified range of model miscalibrations. The method is nonparametric and the
validity is assured even with finite samples. The certified policy evaluations
are illustrated using both simulated and real data.
</p>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14807" title="Abstract">arXiv:2310.14807</a> (cross-list from math.LO) [<a href="/pdf/2310.14807" title="Download PDF">pdf</a>, <a href="/ps/2310.14807" title="Download PostScript">ps</a>, <a href="/format/2310.14807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighing Theories: On Chaitin&#x27;s Heuristic Principle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Salehi%2C+S">Saeed Salehi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Information Theory (cs.IT); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">It would be a heavenly reward if there were a method of weighing theories and
sentences in such a way that a theory could never prove a heavier sentence
(Chaitin's Heuristic Principle). Alas, no satisfactory measure has been found
so far, and this dream seemed too good to ever come true. Here, we attempt to
revive Chaitin's lost paradise of heuristic principle as much as logic allows.
</p>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14809" title="Abstract">arXiv:2310.14809</a> (cross-list from nlin.PS) [<a href="/pdf/2310.14809" title="Download PDF">pdf</a>, <a href="/format/2310.14809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning spatio-temporal patterns with Neural Cellular Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Richardson%2C+A+D">Alex D. Richardson</a>, 
<a href="/search/nlin?searchtype=author&query=Antal%2C+T">Tibor Antal</a>, 
<a href="/search/nlin?searchtype=author&query=Blythe%2C+R+A">Richard A. Blythe</a>, 
<a href="/search/nlin?searchtype=author&query=Schumacher%2C+L+J">Linus J. Schumacher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For videos referenced in appendix, see: <a href="https://github.com/AlexDR1998/NCA/tree/main/Videos">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Pattern Formation and Solitons (nlin.PS)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Dynamical Systems (math.DS); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">Neural Cellular Automata (NCA) are a powerful combination of machine learning
and mechanistic modelling. We train NCA to learn complex dynamics from time
series of images and PDE trajectories. Our method is designed to identify
underlying local rules that govern large scale dynamic emergent behaviours.
Previous work on NCA focuses on learning rules that give stationary emergent
structures. We extend NCA to capture both transient and stable structures
within the same system, as well as learning rules that capture the dynamics of
Turing pattern formation in nonlinear Partial Differential Equations (PDEs). We
demonstrate that NCA can generalise very well beyond their PDE training data,
we show how to constrain NCA to respect given symmetries, and we explore the
effects of associated hyperparameters on model performance and stability. Being
able to learn arbitrary dynamics gives NCA great potential as a data driven
modelling framework, especially for modelling biological pattern formation.
</p>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14826" title="Abstract">arXiv:2310.14826</a> (cross-list from stat.ML) [<a href="/pdf/2310.14826" title="Download PDF">pdf</a>, <a href="/format/2310.14826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharp error bounds for imbalanced classification: how many examples in  the minority class?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Aghbalou%2C+A">Anass Aghbalou</a>, 
<a href="/search/stat?searchtype=author&query=Portier%2C+F">Fran&#xe7;ois Portier</a>, 
<a href="/search/stat?searchtype=author&query=Sabourin%2C+A">Anne Sabourin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">When dealing with imbalanced classification data, reweighting the loss
function is a standard procedure allowing to equilibrate between the true
positive and true negative rates within the risk measure. Despite significant
theoretical work in this area, existing results do not adequately address a
main challenge within the imbalanced classification framework, which is the
negligible size of one class in relation to the full sample size and the need
to rescale the risk function by a probability tending to zero. To address this
gap, we present two novel contributions in the setting where the rare class
probability approaches zero: (1) a non asymptotic fast rate probability bound
for constrained balanced empirical risk minimization, and (2) a consistent
upper bound for balanced nearest neighbors estimates. Our findings provide a
clearer understanding of the benefits of class-weighting in realistic settings,
opening new avenues for further research in this field.
</p>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14836" title="Abstract">arXiv:2310.14836</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.14836" title="Download PDF">pdf</a>, <a href="/format/2310.14836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The role of intra- and inter-group Matthew effect in the social dilemma  of public goods games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+C">Chaoqian Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, accepted for publication at Physics Letters A
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Computer Science and Game Theory (cs.GT); Cellular Automata and Lattice Gases (nlin.CG)

</div>
<p class="mathjax">The Matthew effect describes the phenomenon where the rich tend to get
richer. Such a success-driven mechanism has been studied in spatial public
goods games in an inter-group way, where each individual's social power is
enhanced across all groups. For instance, factors like knowledge can exert an
advantage across various social contexts. In contrast, certain factors,
especially local material goods, only enhance advantages within their current
group. Building on this, we further explore the intra-group Matthew effect
where the enhancement of social power is calculated separately in each group.
Our findings indicate that the intra-group Matthew effect sustains cooperation
more at high productivity, while the inter-group Matthew effect promotes
cooperation at low productivity. Moreover, the mixture of the intra- and
inter-group Matthew effect harms cooperation. This study provides insights into
addressing social dilemmas by adjusting wealth accumulation across diverse
social groups.
</p>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14890" title="Abstract">arXiv:2310.14890</a> (cross-list from stat.ML) [<a href="/pdf/2310.14890" title="Download PDF">pdf</a>, <a href="/format/2310.14890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting for Bounding the Worst-class Error
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Saito%2C+Y">Yuya Saito</a>, 
<a href="/search/stat?searchtype=author&query=Matsuo%2C+S">Shinnosuke Matsuo</a>, 
<a href="/search/stat?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>, 
<a href="/search/stat?searchtype=author&query=Suehiro%2C+D">Daiki Suehiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper tackles the problem of the worst-class error rate, instead of the
standard error rate averaged over all classes. For example, a three-class
classification task with class-wise error rates of 10\%, 10\%, and 40\% has a
worst-class error rate of 40\%, whereas the average is 20\% under the
class-balanced condition. The worst-class error is important in many
applications. For example, in a medical image classification task, it would not
be acceptable for the malignant tumor class to have a 40\% error rate, while
the benign and healthy classes have 10\% error rates.We propose a boosting
algorithm that guarantees an upper bound of the worst-class training error and
derive its generalization bound. Experimental results show that the algorithm
lowers worst-class test error rates while avoiding overfitting to the training
set.
</p>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14934" title="Abstract">arXiv:2310.14934</a> (cross-list from eess.IV) [<a href="/pdf/2310.14934" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Depth Linear Error Decomposition with Double Total Variation and  Nuclear Norm for Dynamic MRI Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tan%2C+J">Junpeng Tan</a>, 
<a href="/search/eess?searchtype=author&query=Qing%2C+C">Chunmei Qing</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+X">Xiangmin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Compressed Sensing (CS) significantly speeds up Magnetic Resonance Image
(MRI) processing and achieves accurate MRI reconstruction from under-sampled
k-space data. According to the current research, there are still several
problems with dynamic MRI k-space reconstruction based on CS. 1) There are
differences between the Fourier domain and the Image domain, and the
differences between MRI processing of different domains need to be considered.
2) As three-dimensional data, dynamic MRI has its spatial-temporal
characteristics, which need to calculate the difference and consistency of
surface textures while preserving structural integrity and uniqueness. 3)
Dynamic MRI reconstruction is time-consuming and computationally
resource-dependent. In this paper, we propose a novel robust low-rank dynamic
MRI reconstruction optimization model via highly under-sampled and Discrete
Fourier Transform (DFT) called the Robust Depth Linear Error Decomposition
Model (RDLEDM). Our method mainly includes linear decomposition, double Total
Variation (TV), and double Nuclear Norm (NN) regularizations. By adding linear
image domain error analysis, the noise is reduced after under-sampled and DFT
processing, and the anti-interference ability of the algorithm is enhanced.
Double TV and NN regularizations can utilize both spatial-temporal
characteristics and explore the complementary relationship between different
dimensions in dynamic MRI sequences. In addition, Due to the non-smoothness and
non-convexity of TV and NN terms, it is difficult to optimize the unified
objective model. To address this issue, we utilize a fast algorithm by solving
a primal-dual form of the original problem. Compared with five state-of-the-art
methods, extensive experiments on dynamic MRI data demonstrate the superior
performance of the proposed method in terms of both reconstruction accuracy and
time complexity.
</p>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14953" title="Abstract">arXiv:2310.14953</a> (cross-list from math.LO) [<a href="/pdf/2310.14953" title="Download PDF">pdf</a>, <a href="/ps/2310.14953" title="Download PostScript">ps</a>, <a href="/format/2310.14953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolation and the Exchange Rule
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fussner%2C+W">Wesley Fussner</a>, 
<a href="/search/math?searchtype=author&query=Metcalfe%2C+G">George Metcalfe</a>, 
<a href="/search/math?searchtype=author&query=Santschi%2C+S">Simon Santschi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO); Rings and Algebras (math.RA)

</div>
<p class="mathjax">It was proved by Maksimova in 1977 that exactly eight varieties of Heyting
algebras have the amalgamation property, and hence exactly eight axiomatic
extensions of intuitionistic propositional logic have the deductive
interpolation property. The prevalence of the deductive interpolation property
for axiomatic extensions of substructural logics and the amalgamation property
for varieties of pointed residuated lattices, their equivalent algebraic
semantics, is far less well understood, however. Taking as our starting point a
formulation of intuitionistic propositional logic as the full Lambek calculus
with exchange, weakening, and contraction, we investigate the role of the
exchange rule--algebraically, the commutativity law--in determining the scope
of these properties. First, we show that there are continuum-many varieties of
idempotent semilinear residuated lattices that have the amalgamation property
and contain non-commutative members, and hence continuum-many axiomatic
extensions of the corresponding logic that have the deductive interpolation
property in which exchange is not derivable. We then show that, in contrast,
exactly sixty varieties of commutative idempotent semilinear residuated
lattices have the amalgamation property, and hence exactly sixty axiomatic
extensions of the corresponding logic with exchange have the deductive
interpolation property. From this latter result, it follows also that there are
exactly sixty varieties of commutative idempotent semilinear residuated
lattices whose first-order theories have a model completion.
</p>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14961" title="Abstract">arXiv:2310.14961</a> (cross-list from eess.IV) [<a href="/pdf/2310.14961" title="Download PDF">pdf</a>, <a href="/format/2310.14961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StenUNet: Automatic Stenosis Detection from X-ray Coronary Angiography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+H">Hui Lin</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tom Liu</a>, 
<a href="/search/eess?searchtype=author&query=Katsaggelos%2C+A">Aggelos Katsaggelos</a>, 
<a href="/search/eess?searchtype=author&query=Kline%2C+A">Adrienne Kline</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Coronary angiography continues to serve as the primary method for diagnosing
coronary artery disease (CAD), which is the leading global cause of mortality.
The severity of CAD is quantified by the location, degree of narrowing
(stenosis), and number of arteries involved. In current practice, this
quantification is performed manually using visual inspection and thus suffers
from poor inter- and intra-rater reliability. The MICCAI grand challenge:
Automatic Region-based Coronary Artery Disease diagnostics using the X-ray
angiography imagEs (ARCADE) curated a dataset with stenosis annotations, with
the goal of creating an automated stenosis detection algorithm. Using a
combination of machine learning and other computer vision techniques, we
propose the architecture and algorithm StenUNet to accurately detect stenosis
from X-ray Coronary Angiography. Our submission to the ARCADE challenge placed
3rd among all teams. We achieved an F1 score of 0.5348 on the test set, 0.0005
lower than the 2nd place.
</p>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14974" title="Abstract">arXiv:2310.14974</a> (cross-list from quant-ph) [<a href="/pdf/2310.14974" title="Download PDF">pdf</a>, <a href="/format/2310.14974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear decomposition of approximate multi-controlled single qubit gates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Silva%2C+J+D+S">Jefferson D. S. Silva</a>, 
<a href="/search/quant-ph?searchtype=author&query=Azevedo%2C+T+M+D">Thiago Melo D. Azevedo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Araujo%2C+I+F">Israel F. Araujo</a>, 
<a href="/search/quant-ph?searchtype=author&query=da+Silva%2C+A+J">Adenilton J. da Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">We provide a method for compiling approximate multi-controlled single qubit
gates into quantum circuits without ancilla qubits. The total number of
elementary gates to decompose an n-qubit multi-controlled gate is proportional
to 32n, and the previous best approximate approach without auxiliary qubits
requires 32nk elementary operations, where k is a function that depends on the
error threshold. The proposed decomposition depends on an optimization
technique that minimizes the CNOT gate count for multi-target and
multi-controlled CNOT and SU(2) gates. Computational experiments show the
reduction in the number of CNOT gates to apply multi-controlled U(2) gates. As
multi-controlled single-qubit gates serve as fundamental components of quantum
algorithms, the proposed decomposition offers a comprehensive solution that can
significantly decrease the count of elementary operations employed in quantum
computing applications.
</p>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15026" title="Abstract">arXiv:2310.15026</a> (cross-list from stat.ML) [<a href="/pdf/2310.15026" title="Download PDF">pdf</a>, <a href="/format/2310.15026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast 2D Bicephalous Convolutional Autoencoder for Compressing 3D Time  Projection Chamber Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Huang%2C+Y">Yi Huang</a>, 
<a href="/search/stat?searchtype=author&query=Ren%2C+Y">Yihui Ren</a>, 
<a href="/search/stat?searchtype=author&query=Yoo%2C+S">Shinjae Yoo</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+J">Jin Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">High-energy large-scale particle colliders produce data at high speed in the
order of 1 terabytes per second in nuclear physics and petabytes per second in
high-energy physics. Developing real-time data compression algorithms to reduce
such data at high throughput to fit permanent storage has drawn increasing
attention. Specifically, at the newly constructed sPHENIX experiment at the
Relativistic Heavy Ion Collider (RHIC), a time projection chamber is used as
the main tracking detector, which records particle trajectories in a volume of
a three-dimensional (3D) cylinder. The resulting data are usually very sparse
with occupancy around 10.8%. Such sparsity presents a challenge to conventional
learning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. The 3D
convolutional neural network (CNN)-based approach, Bicephalous Convolutional
Autoencoder (BCAE), outperforms traditional methods both in compression rate
and reconstruction accuracy. BCAE can also utilize the computation power of
graphical processing units suitable for deployment in a modern heterogeneous
high-performance computing environment. This work introduces two BCAE variants:
BCAE++ and BCAE-2D. BCAE++ achieves a 15% better compression ratio and a 77%
better reconstruction accuracy measured in mean absolute error compared with
BCAE. BCAE-2D treats the radial direction as the channel dimension of an image,
resulting in a 3x speedup in compression throughput. In addition, we
demonstrate an unbalanced autoencoder with a larger decoder can improve
reconstruction accuracy without significantly sacrificing throughput. Lastly,
we observe both the BCAE++ and BCAE-2D can benefit more from using
half-precision mode in throughput (76-79% increase) without loss in
reconstruction accuracy. The source code and links to data and pretrained
models can be found at https://github.com/BNL-DAQ-LDRD/NeuralCompression_v2.
</p>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15043" title="Abstract">arXiv:2310.15043</a> (cross-list from eess.IV) [<a href="/pdf/2310.15043" title="Download PDF">pdf</a>, <a href="/format/2310.15043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CalibrationPhys: Self-supervised Video-based Heart and Respiratory Rate  Measurements by Calibrating Between Multiple Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Akamatsu%2C+Y">Yusuke Akamatsu</a>, 
<a href="/search/eess?searchtype=author&query=Umematsu%2C+T">Terumi Umematsu</a>, 
<a href="/search/eess?searchtype=author&query=Imaoka%2C+H">Hitoshi Imaoka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Video-based heart and respiratory rate measurements using facial videos are
more useful and user-friendly than traditional contact-based sensors. However,
most of the current deep learning approaches require ground-truth pulse and
respiratory waves for model training, which are expensive to collect. In this
paper, we propose CalibrationPhys, a self-supervised video-based heart and
respiratory rate measurement method that calibrates between multiple cameras.
CalibrationPhys trains deep learning models without supervised labels by using
facial videos captured simultaneously by multiple cameras. Contrastive learning
is performed so that the pulse and respiratory waves predicted from the
synchronized videos using multiple cameras are positive and those from
different videos are negative. CalibrationPhys also improves the robustness of
the models by means of a data augmentation technique and successfully leverages
a pre-trained model for a particular camera. Experimental results utilizing two
datasets demonstrate that CalibrationPhys outperforms state-of-the-art heart
and respiratory rate measurement methods. Since we optimize camera-specific
models using only videos from multiple cameras, our approach makes it easy to
use arbitrary cameras for heart and respiratory rate measurements.
</p>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15064" title="Abstract">arXiv:2310.15064</a> (cross-list from math.CO) [<a href="/pdf/2310.15064" title="Download PDF">pdf</a>, <a href="/format/2310.15064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New constructions for $3$-free and $3^+$-free binary morphisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shallit%2C+J">Jeffrey Shallit</a>, 
<a href="/search/math?searchtype=author&query=Shur%2C+A+M">Arseny M. Shur</a>, 
<a href="/search/math?searchtype=author&query=Zorcic%2C+S">Stefan Zorcic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">We show that, for all $k\geq 1$, there exists a $k$-uniform $3^+$-free binary
morphism. Furthermore, we revisit an old result of Currie and Rampersad on
$3$-free binary morphisms and reprove it in a conceptually simpler (but
computationally more intensive) way. Our proofs use the theorem-prover Walnut
as an essential tool.
</p>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15084" title="Abstract">arXiv:2310.15084</a> (cross-list from quant-ph) [<a href="/pdf/2310.15084" title="Download PDF">pdf</a>, <a href="/format/2310.15084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Federated Learning With Quantum Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+T">Tyler Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tseng%2C+H">Huan-Hsin Tseng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yoo%2C+S">Shinjae Yoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A major concern of deep learning models is the large amount of data that is
required to build and train them, much of which is reliant on sensitive and
personally identifiable information that is vulnerable to access by third
parties. Ideas of using the quantum internet to address this issue have been
previously proposed, which would enable fast and completely secure online
communications. Previous work has yielded a hybrid quantum-classical transfer
learning scheme for classical data and communication with a hub-spoke topology.
While quantum communication is secure from eavesdrop attacks and no
measurements from quantum to classical translation, due to no cloning theorem,
hub-spoke topology is not ideal for quantum communication without quantum
memory. Here we seek to improve this model by implementing a decentralized ring
topology for the federated learning scheme, where each client is given a
portion of the entire dataset and only performs training on that set. We also
demonstrate the first successful use of quantum weights for quantum federated
learning, which allows us to perform our training entirely in quantum.
</p>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15092" title="Abstract">arXiv:2310.15092</a> (cross-list from quant-ph) [<a href="/pdf/2310.15092" title="Download PDF">pdf</a>, <a href="/ps/2310.15092" title="Download PostScript">ps</a>, <a href="/format/2310.15092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dihedral Quantum Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Borello%2C+M">Martino Borello</a>, 
<a href="/search/quant-ph?searchtype=author&query=Horlemann%2C+A">Anna-Lena Horlemann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Islam%2C+H">Habibul Islam</a>, 
<a href="/search/quant-ph?searchtype=author&query=Willenborg%2C+N">Nadja Willenborg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We study dihedral quantum codes of short block length, a large class of
quantum CSS codes obtained by the lifted product construction. We present code
construction and give a formula for the code dimension, depending on the two
classical codes on which the CSS code is based on. We also give a lower bound
on the code distance. Finally we construct an example of short dihedral quantum
codes, improving parameters of previously known quantum codes.
</p>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15102" title="Abstract">arXiv:2310.15102</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.15102" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting advice and knowledge sharing among healthcare professionals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Colladon%2C+A+F">A. Fronzetti Colladon</a>, 
<a href="/search/physics?searchtype=author&query=Grippa%2C+F">F. Grippa</a>, 
<a href="/search/physics?searchtype=author&query=Broccatelli%2C+C">C. Broccatelli</a>, 
<a href="/search/physics?searchtype=author&query=Mauren%2C+C">C. Mauren</a>, 
<a href="/search/physics?searchtype=author&query=Mckinsey%2C+S">S. Mckinsey</a>, 
<a href="/search/physics?searchtype=author&query=Kattan%2C+J">J. Kattan</a>, 
<a href="/search/physics?searchtype=author&query=Sutton%2C+E+S+J">E. S. J. Sutton</a>, 
<a href="/search/physics?searchtype=author&query=Satlin%2C+L">L. Satlin</a>, 
<a href="/search/physics?searchtype=author&query=Bucuvalas%2C+J">J. Bucuvalas</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Knowledge Management 27(8), 2017-2033 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Purpose: This study investigates the dynamics of knowledge sharing in
healthcare, exploring some of the factors that are more likely to influence the
evolution of idea sharing and advice seeking in healthcare.
Design/methodology/approach: We engaged 50 pediatricians representing many
subspecialties at a mid-size US children's hospital using a social network
survey to map and measure advice seeking and idea sharing networks. Through the
application of Stochastic Actor-Oriented Models, we compared the structure of
the two networks prior to a leadership program and eight weeks post conclusion.
Findings: Our models indicate that healthcare professionals carefully and
intentionally choose with whom they share ideas and from whom to seek advice.
The process is fluid, non-hierarchical and open to changing partners.
Significant transitivity effects indicate that the processes of knowledge
sharing can be supported by mediation and brokerage. Originality: Hospital
administrators can use this method to assess knowledge-sharing dynamics, design
and evaluate professional development initiatives, and promote new
organizational structures that break down communication silos. Our work
contributes to the literature on knowledge sharing in healthcare by adopting a
social network approach, going beyond the dyadic level, and assessing the
indirect influence of peers' relationships on individual networks.
</p>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15108" title="Abstract">arXiv:2310.15108</a> (cross-list from stat.ML) [<a href="/pdf/2310.15108" title="Download PDF">pdf</a>, <a href="/format/2310.15108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating machine learning models in non-standard settings: An overview  and new findings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hornung%2C+R">Roman Hornung</a>, 
<a href="/search/stat?searchtype=author&query=Nalenz%2C+M">Malte Nalenz</a>, 
<a href="/search/stat?searchtype=author&query=Schneider%2C+L">Lennart Schneider</a>, 
<a href="/search/stat?searchtype=author&query=Bender%2C+A">Andreas Bender</a>, 
<a href="/search/stat?searchtype=author&query=Bothmann%2C+L">Ludwig Bothmann</a>, 
<a href="/search/stat?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/stat?searchtype=author&query=Augustin%2C+T">Thomas Augustin</a>, 
<a href="/search/stat?searchtype=author&query=Boulesteix%2C+A">Anne-Laure Boulesteix</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">Estimating the generalization error (GE) of machine learning models is
fundamental, with resampling methods being the most common approach. However,
in non-standard settings, particularly those where observations are not
independently and identically distributed, resampling using simple random data
divisions may lead to biased GE estimates. This paper strives to present
well-grounded guidelines for GE estimation in various such non-standard
settings: clustered data, spatial data, unequal sampling probabilities, concept
drift, and hierarchically structured outcomes. Our overview combines
well-established methodologies with other existing methods that, to our
knowledge, have not been frequently considered in these particular settings. A
unifying principle among these techniques is that the test data used in each
iteration of the resampling procedure should reflect the new observations to
which the model will be applied, while the training data should be
representative of the entire data set used to obtain the final model. Beyond
providing an overview, we address literature gaps by conducting simulation
studies. These studies assess the necessity of using GE-estimation methods
tailored to the respective setting. Our findings corroborate the concern that
standard resampling methods often yield biased GE estimates in non-standard
settings, underscoring the importance of tailored GE estimation.
</p>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15116" title="Abstract">arXiv:2310.15116</a> (cross-list from nlin.SI) [<a href="/pdf/2310.15116" title="Download PDF">pdf</a>, <a href="/ps/2310.15116" title="Download PostScript">ps</a>, <a href="/format/2310.15116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hermite-Pad&#xe9; approximation, multiple orthogonal polynomials, and  multidimensional Toda equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Doliwa%2C+A">Adam Doliwa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, XL Workshop on Geometric Methods in Physics, Bia{\l}owie\.za 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Exactly Solvable and Integrable Systems (nlin.SI)</span>; Mathematical Physics (math-ph); Classical Analysis and ODEs (math.CA); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We review recent results on the connection between Hermite-Pad\'e
approximation problem, multiple orthogonal polynomials, and multidimensional
Toda equations in continuous and discrete time. In order to motivate interest
in the subject we first present a pedagogical introduction to the classical, by
now, relation between the Pad\'e approximation problem, orthogonal polynomials,
and the Toda lattice equations. We describe also briefly generalization of the
connection to the interpolation problems and to the non-commutative algebra
level.
</p>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15124" title="Abstract">arXiv:2310.15124</a> (cross-list from stat.ML) [<a href="/pdf/2310.15124" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed-Variable Global Sensitivity Analysis For Knowledge Discovery And  Efficient Combinatorial Materials Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Comlek%2C+Y">Yigitcan Comlek</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+L">Liwei Wang</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 Pages, 10 Figures, 2 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Global Sensitivity Analysis (GSA) is the study of the influence of any given
inputs on the outputs of a model. In the context of engineering design, GSA has
been widely used to understand both individual and collective contributions of
design variables on the design objectives. So far, global sensitivity studies
have often been limited to design spaces with only quantitative (numerical)
design variables. However, many engineering systems also contain, if not only,
qualitative (categorical) design variables in addition to quantitative design
variables. In this paper, we integrate Latent Variable Gaussian Process (LVGP)
with Sobol' analysis to develop the first metamodel-based mixed-variable GSA
method. Through numerical case studies, we validate and demonstrate the
effectiveness of our proposed method for mixed-variable problems. Furthermore,
while the proposed GSA method is general enough to benefit various engineering
design applications, we integrate it with multi-objective Bayesian optimization
(BO) to create a sensitivity-aware design framework in accelerating the Pareto
front design exploration for metal-organic framework (MOF) materials with
many-level combinatorial design spaces. Although MOFs are constructed only from
qualitative variables that are notoriously difficult to design, our method can
utilize sensitivity analysis to navigate the optimization in the many-level
large combinatorial design space, greatly expediting the exploration of novel
MOF candidates.
</p>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15143" title="Abstract">arXiv:2310.15143</a> (cross-list from gr-qc) [<a href="/pdf/2310.15143" title="Download PDF">pdf</a>, <a href="/format/2310.15143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperparameter optimization of hp-greedy reduced basis for gravitational  wave surrogates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/gr-qc?searchtype=author&query=Cerino%2C+F">Franco Cerino</a>, 
<a href="/search/gr-qc?searchtype=author&query=Diaz-Pace%2C+A">Andr&#xe9;s Diaz-Pace</a>, 
<a href="/search/gr-qc?searchtype=author&query=Tassone%2C+E">Emmanuel Tassone</a>, 
<a href="/search/gr-qc?searchtype=author&query=Tiglio%2C+M">Manuel Tiglio</a>, 
<a href="/search/gr-qc?searchtype=author&query=Villegas%2C+A">Atuel Villegas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is an invited contribution to the Special Issue "Recent Advances in Gravity: A Themed Issue in Honor of Prof. Jorge Pullin on his 60th Anniversary''
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Relativity and Quantum Cosmology (gr-qc)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Mathematical Physics (math-ph)

</div>
<p class="mathjax">In a previous work we introduced, in the context of gravitational wave
science, an initial study on an automated domain-decomposition approach for
reduced basis through hp-greedy refinement. The approach constructs local
reduced bases of lower dimensionality than global ones, with the same or higher
accuracy. These ``light'' local bases should imply both faster evaluations when
predicting new waveforms and faster data analysis, in particular faster
statistical inference (the forward and inverse problems, respectively). In this
approach, however, we have previously found important dependence on several
hyperparameters, which do not appear in global reduced basis. This naturally
leads to the problem of hyperparameter optimization (HPO), which is the subject
of this paper. We tackle the problem through a Bayesian optimization, and show
its superiority when compared to grid or random searches. We find that for
gravitational waves from the collision of two spinning but non-precessing black
holes, for the same accuracy, local hp-greedy reduced bases with HPO have a
lower dimensionality of up to $4 \times$ for the cases here studied, depending
on the desired accuracy. This factor should directly translate in a parameter
estimation speedup, for instance. Such acceleration might help in the near
real-time requirements for electromagnetic counterparts of gravitational waves
from compact binary coalescences. In addition, we find that the Bayesian
approach used in this paper for HPO is two orders of magnitude faster than, for
example, a grid search, with about a $100 \times$ acceleration. The code
developed for this project is available as open source from public
repositories.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 24 Oct 23</h3>
<dl>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1211.3798" title="Abstract">arXiv:1211.3798</a> (replaced) [<a href="/pdf/1211.3798" title="Download PDF">pdf</a>, <a href="/ps/1211.3798" title="Download PostScript">ps</a>, <a href="/format/1211.3798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Stochastic Reduction of Inertial Fluid-Structure Interactions  subject to Thermal Fluctuations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Tabak%2C+G">Gil Tabak</a>, 
<a href="/search/cond-mat?searchtype=author&query=Atzberger%2C+P+J">Paul J. Atzberger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM J. Appl. Math., 75(4), 1884-1914, (2015)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Statistical Mechanics (cond-mat.stat-mech); Dynamical Systems (math.DS); Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1802.02388" title="Abstract">arXiv:1802.02388</a> (replaced) [<a href="/pdf/1802.02388" title="Download PDF">pdf</a>, <a href="/ps/1802.02388" title="Download PostScript">ps</a>, <a href="/format/1802.02388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Definable Ellipsoid Method, Sums-of-Squares Proofs, and the Graph  Isomorphism Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atserias%2C+A">Albert Atserias</a>, 
<a href="/search/cs?searchtype=author&query=Fijalkow%2C+J">Joanna Fijalkow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised version incorporating the comments of the journal reviewers. The title changed slightly. The second author changed surname
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM Journal on Computing, 52(5), pp. 1193-1229, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1811.09537" title="Abstract">arXiv:1811.09537</a> (replaced) [<a href="/pdf/1811.09537" title="Download PDF">pdf</a>, <a href="/format/1811.09537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On three domination-based identification problems in block graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chakraborty%2C+D">Dipayan Chakraborty</a>, 
<a href="/search/math?searchtype=author&query=Foucaud%2C+F">Florent Foucaud</a>, 
<a href="/search/math?searchtype=author&query=Parreau%2C+A">Aline Parreau</a>, 
<a href="/search/math?searchtype=author&query=Wagler%2C+A+K">Annegret K. Wagler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1811.09620" title="Abstract">arXiv:1811.09620</a> (replaced) [<a href="/pdf/1811.09620" title="Download PDF">pdf</a>, <a href="/format/1811.09620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre  Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sicong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Anil%2C+C">Cem Anil</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+X">Xuchan Bao</a>, 
<a href="/search/cs?searchtype=author&query=Oore%2C+S">Sageev Oore</a>, 
<a href="/search/cs?searchtype=author&query=Grosse%2C+R+B">Roger B. Grosse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, published as a conference paper at ICLR 2019
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1902.10815" title="Abstract">arXiv:1902.10815</a> (replaced) [<a href="/pdf/1902.10815" title="Download PDF">pdf</a>, <a href="/format/1902.10815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Deep Learning MRI Reconstruction across Different Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+C">Cheng Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Schlemper%2C+J">Jo Schlemper</a>, 
<a href="/search/cs?searchtype=author&query=Biffi%2C+C">Carlo Biffi</a>, 
<a href="/search/cs?searchtype=author&query=Seegoolam%2C+G">Gavin Seegoolam</a>, 
<a href="/search/cs?searchtype=author&query=Caballero%2C+J">Jose Caballero</a>, 
<a href="/search/cs?searchtype=author&query=Price%2C+A+N">Anthony N. Price</a>, 
<a href="/search/cs?searchtype=author&query=Hajnal%2C+J+V">Joseph V. Hajnal</a>, 
<a href="/search/cs?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ISBI2019 as a 1-page abstract
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1906.01146" title="Abstract">arXiv:1906.01146</a> (replaced) [<a href="/pdf/1906.01146" title="Download PDF">pdf</a>, <a href="/format/1906.01146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surface Fluctuating Hydrodynamics Methods for the Drift-Diffusion  Dynamics of Particles and Microstructures within Curved Fluid Interfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Rower%2C+D">David Rower</a>, 
<a href="/search/cond-mat?searchtype=author&query=Padidar%2C+M">Misha Padidar</a>, 
<a href="/search/cond-mat?searchtype=author&query=Atzberger%2C+P+J">Paul J. Atzberger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computational Physics, 455, (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Computational Engineering, Finance, and Science (cs.CE); Numerical Analysis (math.NA); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.04249" title="Abstract">arXiv:1911.04249</a> (replaced) [<a href="/pdf/1911.04249" title="Download PDF">pdf</a>, <a href="/ps/1911.04249" title="Download PostScript">ps</a>, <a href="/format/1911.04249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A polynomial kernel for $3$-leaf power deletion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J">Jungho Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Eiben%2C+E">Eduard Eiben</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+O">O-joung Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Oum%2C+S">Sang-il Oum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 1 figure
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Algorithmica (2023) 85(10)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.09307" title="Abstract">arXiv:1911.09307</a> (replaced) [<a href="/pdf/1911.09307" title="Download PDF">pdf</a>, <a href="/format/1911.09307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patch-level Neighborhood Interpolation: A General and Effective  Graph-based Regularization Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouchen Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhanxing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ACML 2023 conference track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.05957" title="Abstract">arXiv:1912.05957</a> (replaced) [<a href="/pdf/1912.05957" title="Download PDF">pdf</a>, <a href="/format/1912.05957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text as Environment: A Deep Reinforcement Learning Text Readability  Assessment Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+H">Hamid Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Khasteh%2C+S+H">Seyed Hossein Khasteh</a>, 
<a href="/search/cs?searchtype=author&query=Firoozi%2C+T">Tahereh Firoozi</a>, 
<a href="/search/cs?searchtype=author&query=Samavati%2C+T">Taha Samavati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2004.07353" title="Abstract">arXiv:2004.07353</a> (replaced) [<a href="/pdf/2004.07353" title="Download PDF">pdf</a>, <a href="/format/2004.07353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nucleus I: Adjunction spectra in recommender systems and descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pavlovic%2C+D">Dusko Pavlovic</a>, 
<a href="/search/math?searchtype=author&query=Hughes%2C+D+J+D">Dominic J.D.Hughes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 40 figures. For readability, the previous longer version has now been split into several papers. This is part I
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.05014" title="Abstract">arXiv:2007.05014</a> (replaced) [<a href="/pdf/2007.05014" title="Download PDF">pdf</a>, <a href="/ps/2007.05014" title="Download PostScript">ps</a>, <a href="/format/2007.05014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack  Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amanatidis%2C+G">Georgios Amanatidis</a>, 
<a href="/search/cs?searchtype=author&query=Fusco%2C+F">Federico Fusco</a>, 
<a href="/search/cs?searchtype=author&query=Lazos%2C+P">Philip Lazos</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+S">Stefano Leonardi</a>, 
<a href="/search/cs?searchtype=author&query=Reiffenh%C3%A4user%2C+R">Rebecca Reiffenh&#xe4;user</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Same as v1. Version 2 was a replacement intended for <a href="/abs/2102.08327">arXiv:2102.08327</a> and erroneously updated here
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.09435" title="Abstract">arXiv:2007.09435</a> (replaced) [<a href="/pdf/2007.09435" title="Download PDF">pdf</a>, <a href="/format/2007.09435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Motion Planning: A Fiber Bundle Formulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Orthey%2C+A">Andreas Orthey</a>, 
<a href="/search/cs?searchtype=author&query=Akbar%2C+S">Sohaib Akbar</a>, 
<a href="/search/cs?searchtype=author&query=Toussaint%2C+M">Marc Toussaint</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at International Journal of Robotics Research (IJRR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.10642" title="Abstract">arXiv:2007.10642</a> (replaced) [<a href="/pdf/2007.10642" title="Download PDF">pdf</a>, <a href="/format/2007.10642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gasper: GrAph Signal ProcEssing in R
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=de+Loynes%2C+B">Basile de Loynes</a>, 
<a href="/search/eess?searchtype=author&query=Navarro%2C+F">Fabien Navarro</a>, 
<a href="/search/eess?searchtype=author&query=Olivier%2C+B">Baptiste Olivier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.02383" title="Abstract">arXiv:2012.02383</a> (replaced) [<a href="/pdf/2012.02383" title="Download PDF">pdf</a>, <a href="/format/2012.02383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAM: Self-supervised Learning of Pixel-wise Anatomical Embeddings in  Radiological Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Ke Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jinzheng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Dakai Jin</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+S">Shun Miao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dazhou Guo</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+A+P">Adam P. Harrison</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Youbao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jingjing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Le Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> code: <a href="https://github.com/alibaba-damo-academy/self-supervised-anatomical-embedding-v2">this https URL</a>; IEEE Trans on Medical Imaging: <a href="https://ieeexplore.ieee.org/document/9760421">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.08327" title="Abstract">arXiv:2102.08327</a> (replaced) [<a href="/pdf/2102.08327" title="Download PDF">pdf</a>, <a href="/ps/2102.08327" title="Download PostScript">ps</a>, <a href="/format/2102.08327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Submodular Maximization subject to a Knapsack Constraint: Combinatorial  Algorithms with Near-optimal Adaptive Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amanatidis%2C+G">Georgios Amanatidis</a>, 
<a href="/search/cs?searchtype=author&query=Fusco%2C+F">Federico Fusco</a>, 
<a href="/search/cs?searchtype=author&query=Lazos%2C+P">Philip Lazos</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+S">Stefano Leonardi</a>, 
<a href="/search/cs?searchtype=author&query=Spaccamela%2C+A+M">Alberto Marchetti Spaccamela</a>, 
<a href="/search/cs?searchtype=author&query=Reiffenh%C3%A4user%2C+R">Rebecca Reiffenh&#xe4;user</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version addresses a gap in the probabilistic analysis of the approximation guarantees in the previous version of this work. We provide a simple fix via a standard sampling routine while maintaining the same approximation guarantees and complexity bounds. (formerly appeared as <a href="/abs/2007.05014">arXiv:2007.05014v2</a> in error)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.02298" title="Abstract">arXiv:2103.02298</a> (replaced) [<a href="/pdf/2103.02298" title="Download PDF">pdf</a>, <a href="/format/2103.02298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Compound PCFGs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanpeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Adapt-NLP at EACL 2021 (Added results on Brown of Penn Treebank and English Web Treebank). Our code is available at <a href="https://github.com/zhaoyanpeng/cpcfg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.14882" title="Abstract">arXiv:2105.14882</a> (replaced) [<a href="/pdf/2105.14882" title="Download PDF">pdf</a>, <a href="/format/2105.14882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Problems Complete for Nondeterministic FPT time and  Logarithmic Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodlaender%2C+H+L">Hans L. Bodlaender</a>, 
<a href="/search/cs?searchtype=author&query=Groenland%2C+C">Carla Groenland</a>, 
<a href="/search/cs?searchtype=author&query=Nederlof%2C+J">Jesper Nederlof</a>, 
<a href="/search/cs?searchtype=author&query=Swennenhuis%2C+C+M+F">C&#xe9;line M. F. Swennenhuis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06272" title="Abstract">arXiv:2106.06272</a> (replaced) [<a href="/pdf/2106.06272" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-based Joint Analysis of Safety and Security: Survey and  Identification of Gaps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nicoletti%2C+S+M">Stefano M. Nicoletti</a>, 
<a href="/search/cs?searchtype=author&query=Peppelman%2C+M">Marijn Peppelman</a>, 
<a href="/search/cs?searchtype=author&query=Kolb%2C+C">Christina Kolb</a>, 
<a href="/search/cs?searchtype=author&query=Stoelinga%2C+M">Mari&#xeb;lle Stoelinga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.11753" title="Abstract">arXiv:2106.11753</a> (replaced) [<a href="/pdf/2106.11753" title="Download PDF">pdf</a>, <a href="/format/2106.11753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symplectic Learning for Hamiltonian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=David%2C+M">Marco David</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%A9hats%2C+F">Florian M&#xe9;hats</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures; Source code, datasets and pre-trained models available at <a href="https://github.com/SpaceAbleOrg/symplectic-hnn">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computational Physics (2023), vol. 494, 112495
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.15907" title="Abstract">arXiv:2106.15907</a> (replaced) [<a href="/pdf/2106.15907" title="Download PDF">pdf</a>, <a href="/format/2106.15907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Complexities of Dominating and Independent Set  Reconfiguration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodlaender%2C+H+L">Hans L. Bodlaender</a>, 
<a href="/search/cs?searchtype=author&query=Groenland%2C+C">Carla Groenland</a>, 
<a href="/search/cs?searchtype=author&query=Swennenhuis%2C+C+M+F">C&#xe9;line M. F. Swennenhuis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.14362" title="Abstract">arXiv:2107.14362</a> (replaced) [<a href="/pdf/2107.14362" title="Download PDF">pdf</a>, <a href="/format/2107.14362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLMOD: Machine Learning Methods for Data-Driven Modeling in LAMMPS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atzberger%2C+P+J">Paul J. Atzberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Soft Condensed Matter (cond-mat.soft); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.03517" title="Abstract">arXiv:2108.03517</a> (replaced) [<a href="/pdf/2108.03517" title="Download PDF">pdf</a>, <a href="/format/2108.03517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Upfront Commitment in Online Resource Allocation with Patient Customers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golrezaei%2C+N">Negin Golrezaei</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+E">Evan Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.04548" title="Abstract">arXiv:2109.04548</a> (replaced) [<a href="/pdf/2109.04548" title="Download PDF">pdf</a>, <a href="/format/2109.04548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iceberg Hashing: Optimizing Many Hash-Table Criteria at Once
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bender%2C+M+A">Michael A. Bender</a>, 
<a href="/search/cs?searchtype=author&query=Conway%2C+A">Alex Conway</a>, 
<a href="/search/cs?searchtype=author&query=Farach-Colton%2C+M">Mart&#xed;n Farach-Colton</a>, 
<a href="/search/cs?searchtype=author&query=Kuszmaul%2C+W">William Kuszmaul</a>, 
<a href="/search/cs?searchtype=author&query=Tagliavini%2C+G">Guido Tagliavini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.15315" title="Abstract">arXiv:2109.15315</a> (replaced) [<a href="/pdf/2109.15315" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brand Attitude in Social Networks: The Role of eWoM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pourkabirian%2C+A">Azita Pourkabirian</a>, 
<a href="/search/cs?searchtype=author&query=Habibian%2C+M">Melika Habibian</a>, 
<a href="/search/cs?searchtype=author&query=Pourkabirian%2C+A">Azadeh Pourkabirian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.01419" title="Abstract">arXiv:2111.01419</a> (replaced) [<a href="/pdf/2111.01419" title="Download PDF">pdf</a>, <a href="/format/2111.01419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The $k$-Compound of a Difference-Algebraic System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ofir%2C+R">Ron Ofir</a>, 
<a href="/search/eess?searchtype=author&query=Margaliot%2C+M">Michael Margaliot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Automatica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.06829" title="Abstract">arXiv:2111.06829</a> (replaced) [<a href="/pdf/2111.06829" title="Download PDF">pdf</a>, <a href="/format/2111.06829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Order Nystr&#xf6;m/Convolution-Quadrature Solution of Time-Domain  Scattering from Closed and Open Lipschitz Boundaries with Dirichlet and  Neumann Boundary Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Petropoulos%2C+P+P">Peter P. Petropoulos</a>, 
<a href="/search/math?searchtype=author&query=Turc%2C+C">Catalin Turc</a>, 
<a href="/search/math?searchtype=author&query=Wind-andersen%2C+E">Erli Wind-andersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.01939" title="Abstract">arXiv:2112.01939</a> (replaced) [<a href="/pdf/2112.01939" title="Download PDF">pdf</a>, <a href="/format/2112.01939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Projected Newton-like Method for Precision Matrix Estimation under  Total Positivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jian-Feng Cai</a>, 
<a href="/search/cs?searchtype=author&query=de+M.+Cardoso%2C+J+V">Jos&#xe9; Vin&#xed;cius de M. Cardoso</a>, 
<a href="/search/cs?searchtype=author&query=Palomar%2C+D+P">Daniel P. Palomar</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+J">Jiaxi Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.05917" title="Abstract">arXiv:2112.05917</a> (replaced) [<a href="/pdf/2112.05917" title="Download PDF">pdf</a>, <a href="/format/2112.05917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yiwen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Plummer%2C+B+A">Bryan A. Plummer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.08158" title="Abstract">arXiv:2201.08158</a> (replaced) [<a href="/pdf/2201.08158" title="Download PDF">pdf</a>, <a href="/format/2201.08158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HDhuman: High-quality Human Novel-view Rendering from Sparse Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tiansong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Ruizhi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11104" title="Abstract">arXiv:2201.11104</a> (replaced) [<a href="/pdf/2201.11104" title="Download PDF">pdf</a>, <a href="/format/2201.11104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining optimal path search with task-dependent learning in a neural  network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulvicius%2C+T">Tomas Kulvicius</a>, 
<a href="/search/cs?searchtype=author&query=Tamosiunaite%2C+M">Minija Tamosiunaite</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%B6rg%C3%B6tter%2C+F">Florentin W&#xf6;rg&#xf6;tter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11221" title="Abstract">arXiv:2201.11221</a> (replaced) [<a href="/pdf/2201.11221" title="Download PDF">pdf</a>, <a href="/format/2201.11221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A linear linear lambda-calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz-Caro%2C+A">Alejandro D&#xed;az-Caro</a>, 
<a href="/search/cs?searchtype=author&query=Dowek%2C+G">Gilles Dowek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the full revised journal version of the FSCD 2022 paper published at LIPIcs 228:21, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.02927" title="Abstract">arXiv:2202.02927</a> (replaced) [<a href="/pdf/2202.02927" title="Download PDF">pdf</a>, <a href="/format/2202.02927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Shared Autonomy Drone Landings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Backman%2C+K">Kal Backman</a>, 
<a href="/search/cs?searchtype=author&query=Kuli%C4%87%2C+D">Dana Kuli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+H">Hoam Chung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 15 figures. Auton Robot (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.05069" title="Abstract">arXiv:2202.05069</a> (replaced) [<a href="/pdf/2202.05069" title="Download PDF">pdf</a>, <a href="/format/2202.05069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer-Learning Across Datasets with Different Input Dimensions: An  Algorithm and Analysis for the Linear Regression Case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Silvestrin%2C+L+P">Luis Pedro Silvestrin</a>, 
<a href="/search/stat?searchtype=author&query=van+Zanten%2C+H">Harry van Zanten</a>, 
<a href="/search/stat?searchtype=author&query=Hoogendoorn%2C+M">Mark Hoogendoorn</a>, 
<a href="/search/stat?searchtype=author&query=Koole%2C+G">Ger Koole</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript accepted for publication at the Journal of Computational Mathematics and Data Science. Code available at <a href="https://github.com/lpsilvestrin/incremental_input_tl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06152" title="Abstract">arXiv:2202.06152</a> (replaced) [<a href="/pdf/2202.06152" title="Download PDF">pdf</a>, <a href="/format/2202.06152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Dual-Based PID Controllers through Convolutional Mirror  Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Balseiro%2C+S+R">Santiago R. Balseiro</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+H">Haihao Lu</a>, 
<a href="/search/math?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/math?searchtype=author&query=Sivan%2C+B">Balasubramanian Sivan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06593" title="Abstract">arXiv:2202.06593</a> (replaced) [<a href="/pdf/2202.06593" title="Download PDF">pdf</a>, <a href="/format/2202.06593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Inference for the Dynamic Time Warping Distance, with  Application to Abnormal Time-Series Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Duy%2C+V+N+L">Vo Nguyen Le Duy</a>, 
<a href="/search/stat?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06839" title="Abstract">arXiv:2202.06839</a> (replaced) [<a href="/pdf/2202.06839" title="Download PDF">pdf</a>, <a href="/format/2202.06839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Close-up and Whispering: An Understanding of Multimodal and Parasocial  Interactions in YouTube ASMR videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+S">Shuo Niu</a>, 
<a href="/search/cs?searchtype=author&query=Manon%2C+H+S">Hugh S. Manon</a>, 
<a href="/search/cs?searchtype=author&query=Bartolome%2C+A">Ava Bartolome</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+N+B">Nguyen B. Ha</a>, 
<a href="/search/cs?searchtype=author&query=Veazey%2C+K">Keegan Veazey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.08522" title="Abstract">arXiv:2202.08522</a> (replaced) [<a href="/pdf/2202.08522" title="Download PDF">pdf</a>, <a href="/format/2202.08522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovering Unbalanced Communities in the Stochastic Block Model With  Application to Clustering with a Faulty Oracle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+C+S">Chandra Sekhar Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+P">Pan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiapeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.10963" title="Abstract">arXiv:2202.10963</a> (replaced) [<a href="/pdf/2202.10963" title="Download PDF">pdf</a>, <a href="/format/2202.10963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A framework for spatial heat risk assessment using a generalized  similarity measure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bansal%2C+A">Akshay Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Kianmehr%2C+A">Ayda Kianmehr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.11776" title="Abstract">arXiv:2202.11776</a> (replaced) [<a href="/pdf/2202.11776" title="Download PDF">pdf</a>, <a href="/format/2202.11776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Challenge of Understanding What Users Want: Inconsistent Preferences  and Engagement Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>, 
<a href="/search/cs?searchtype=author&query=Mullainathan%2C+S">Sendhil Mullainathan</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+M">Manish Raghavan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.13852" title="Abstract">arXiv:2202.13852</a> (replaced) [<a href="/pdf/2202.13852" title="Download PDF">pdf</a>, <a href="/format/2202.13852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperbolic Graph Neural Networks: A Review of Methods and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Menglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Min Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Lujia Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+I">Irwin King</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.06907" title="Abstract">arXiv:2203.06907</a> (replaced) [<a href="/pdf/2203.06907" title="Download PDF">pdf</a>, <a href="/format/2203.06907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Memory Learning for Fine-Grained Scene Graph Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Youming Deng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yansheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+X">Xiang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiayi Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECCV 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.09313" title="Abstract">arXiv:2203.09313</a> (replaced) [<a href="/pdf/2203.09313" title="Download PDF">pdf</a>, <a href="/format/2203.09313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with  Large-Scale Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuxian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jiaxin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yi Song</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+P">Pei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chujie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jianzhu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaoyan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Machine Intelligence Research. <a href="https://link.springer.com/article/10.1007/s11633-022-1387-3">this https URL</a> . 12 pages, 5 figures. The code and pre-trained models are publicly available at <a href="https://github.com/thu-coai/EVA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.10761" title="Abstract">arXiv:2203.10761</a> (replaced) [<a href="/pdf/2203.10761" title="Download PDF">pdf</a>, <a href="/format/2203.10761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Hard Mixed Samples with Decoupled Regularizer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Ge Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'2023 Camera Ready. The source code is available at <a href="https://github.com/Westlake-AI/openmixup">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.05148" title="Abstract">arXiv:2204.05148</a> (replaced) [<a href="/pdf/2204.05148" title="Download PDF">pdf</a>, <a href="/format/2204.05148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Algayres%2C+R">Robin Algayres</a>, 
<a href="/search/cs?searchtype=author&query=Nabli%2C+A">Adel Nabli</a>, 
<a href="/search/cs?searchtype=author&query=Sagot%2C+B">Benoit Sagot</a>, 
<a href="/search/cs?searchtype=author&query=Dupoux%2C+E">Emmanuel Dupoux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Interspeech 2022 New version on 10/21/23 with appendix data and gitlab link
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.05278" title="Abstract">arXiv:2204.05278</a> (replaced) [<a href="/pdf/2204.05278" title="Download PDF">pdf</a>, <a href="/format/2204.05278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Negligible effect of brain MRI data preprocessing for tumor segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kondrateva%2C+E">Ekaterina Kondrateva</a>, 
<a href="/search/eess?searchtype=author&query=Druzhinina%2C+P">Polina Druzhinina</a>, 
<a href="/search/eess?searchtype=author&query=Dalechina%2C+A">Alexandra Dalechina</a>, 
<a href="/search/eess?searchtype=author&query=Zolotova%2C+S">Svetlana Zolotova</a>, 
<a href="/search/eess?searchtype=author&query=Golanov%2C+A">Andrey Golanov</a>, 
<a href="/search/eess?searchtype=author&query=Shirokikh%2C+B">Boris Shirokikh</a>, 
<a href="/search/eess?searchtype=author&query=Belyaev%2C+M">Mikhail Belyaev</a>, 
<a href="/search/eess?searchtype=author&query=Kurmukov%2C+A">Anvar Kurmukov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.12723" title="Abstract">arXiv:2204.12723</a> (replaced) [<a href="/pdf/2204.12723" title="Download PDF">pdf</a>, <a href="/ps/2204.12723" title="Download PostScript">ps</a>, <a href="/format/2204.12723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information-theoretic limitations of data-based price discrimination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haitian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Ying Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shishkin%2C+D">Denis Shishkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Econometrics (econ.EM); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.13839" title="Abstract">arXiv:2204.13839</a> (replaced) [<a href="/pdf/2204.13839" title="Download PDF">pdf</a>, <a href="/format/2204.13839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A suite of diagnostic metrics for characterizing selection schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+J+G">Jose Guadalupe Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Lalejini%2C+A">Alexander Lalejini</a>, 
<a href="/search/cs?searchtype=author&query=Ofria%2C+C">Charles Ofria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Incorporated valley crossing diagnostics and results. Also refactored paper to focus on three key problem characteristics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.01365" title="Abstract">arXiv:2205.01365</a> (replaced) [<a href="/pdf/2205.01365" title="Download PDF">pdf</a>, <a href="/format/2205.01365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Half-Positional Objectives Recognized by Deterministic B&#xfc;chi Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouyer%2C+P">Patricia Bouyer</a>, 
<a href="/search/cs?searchtype=author&query=Casares%2C+A">Antonio Casares</a>, 
<a href="/search/cs?searchtype=author&query=Randour%2C+M">Mickael Randour</a>, 
<a href="/search/cs?searchtype=author&query=Vandenhove%2C+P">Pierre Vandenhove</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version of CONCUR 2022 conference paper. 42 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07877" title="Abstract">arXiv:2205.07877</a> (replaced) [<a href="/pdf/2205.07877" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Model Quantization for Deep Neural Networks in  Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rokh%2C+B">Babak Rokh</a>, 
<a href="/search/cs?searchtype=author&query=Azarpeyvand%2C+A">Ali Azarpeyvand</a>, 
<a href="/search/cs?searchtype=author&query=Khanteymoori%2C+A">Alireza Khanteymoori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The title of the paper has been changed. The abstract has been improved. The grammatical errors have been corrected. The structure of the paper has been modified. Some new and important references have been added. Some of the used abbreviations in the paper have been corrected. Some figures have been improved. Accepted in ACM Transactions on Intelligent Systems and Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.08435" title="Abstract">arXiv:2205.08435</a> (replaced) [<a href="/pdf/2205.08435" title="Download PDF">pdf</a>, <a href="/format/2205.08435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyber Risk Assessment for Capital Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Chong%2C+W+F">Wing Fung Chong</a>, 
<a href="/search/q-fin?searchtype=author&query=Feng%2C+R">Runhuan Feng</a>, 
<a href="/search/q-fin?searchtype=author&query=Hu%2C+H">Hins Hu</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhang%2C+L">Linfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was first presented on July 5, 2021, at the 24th International Congress on Insurance: Mathematics and Economics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Cryptography and Security (cs.CR); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.12422" title="Abstract">arXiv:2205.12422</a> (replaced) [<a href="/pdf/2205.12422" title="Download PDF">pdf</a>, <a href="/format/2205.12422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Programmers Can Label Programs Indirectly via Active Examples: A  Case Study with Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+R">Ruiqi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Snell%2C+C">Charlie Snell</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Eisner%2C+J">Jason Eisner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00648" title="Abstract">arXiv:2206.00648</a> (replaced) [<a href="/pdf/2206.00648" title="Download PDF">pdf</a>, <a href="/format/2206.00648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme  price movement prediction of Bitcoin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Zou%2C+Y">Yanzhao Zou</a>, 
<a href="/search/q-fin?searchtype=author&query=Herremans%2C+D">Dorien Herremans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, submitted preprint to Elsevier Expert Systems with Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.09427" title="Abstract">arXiv:2206.09427</a> (replaced) [<a href="/pdf/2206.09427" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QuDASH: Quantum-inspired rate adaptation approach for DASH video  streaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+B">Bo Wei</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hang Song</a>, 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+M">Makoto Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Kimura%2C+K">Koichi Kimura</a>, 
<a href="/search/cs?searchtype=author&query=Togawa%2C+N">Nozomu Togawa</a>, 
<a href="/search/cs?searchtype=author&query=Katto%2C+J">Jiro Katto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted Version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.15284" title="Abstract">arXiv:2206.15284</a> (replaced) [<a href="/pdf/2206.15284" title="Download PDF">pdf</a>, <a href="/format/2206.15284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Advantage Seeker with Kernels (QuASK): a software framework to  speed up the research in quantum machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Di+Marcantonio%2C+F">Francesco Di Marcantonio</a>, 
<a href="/search/quant-ph?searchtype=author&query=Incudini%2C+M">Massimiliano Incudini</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tezza%2C+D">Davide Tezza</a>, 
<a href="/search/quant-ph?searchtype=author&query=Grossi%2C+M">Michele Grossi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Close to the published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Quantum Machine Intelligence 5.1 (2023): 20
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05521" title="Abstract">arXiv:2207.05521</a> (replaced) [<a href="/pdf/2207.05521" title="Download PDF">pdf</a>, <a href="/format/2207.05521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Unlearning: How to Efficiently Erase a Client in FL?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halimi%2C+A">Anisa Halimi</a>, 
<a href="/search/cs?searchtype=author&query=Kadhe%2C+S">Swanand Kadhe</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A">Ambrish Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Baracaldo%2C+N">Nathalie Baracaldo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05882" title="Abstract">arXiv:2207.05882</a> (replaced) [<a href="/pdf/2207.05882" title="Download PDF">pdf</a>, <a href="/format/2207.05882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Employing Feature Selection Algorithms to Determine the Immune State of  a Mouse Model of Rheumatoid Arthritis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Colbert%2C+B+K">Brendon K. Colbert</a>, 
<a href="/search/stat?searchtype=author&query=Mangal%2C+J+L">Joslyn L. Mangal</a>, 
<a href="/search/stat?searchtype=author&query=Talitckii%2C+A">Aleksandr Talitckii</a>, 
<a href="/search/stat?searchtype=author&query=Acharya%2C+A+P">Abhinav P. Acharya</a>, 
<a href="/search/stat?searchtype=author&query=Peet%2C+M+M">Matthew M. Peet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06272" title="Abstract">arXiv:2207.06272</a> (replaced) [<a href="/pdf/2207.06272" title="Download PDF">pdf</a>, <a href="/format/2207.06272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hindsight Learning for MDPs with Exogenous Inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinclair%2C+S+R">Sean R. Sinclair</a>, 
<a href="/search/cs?searchtype=author&query=Frujeri%2C+F">Felipe Frujeri</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Ching-An Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Marshall%2C+L">Luke Marshall</a>, 
<a href="/search/cs?searchtype=author&query=Barbalho%2C+H">Hugo Barbalho</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingling Li</a>, 
<a href="/search/cs?searchtype=author&query=Neville%2C+J">Jennifer Neville</a>, 
<a href="/search/cs?searchtype=author&query=Menache%2C+I">Ishai Menache</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+A">Adith Swaminathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06926" title="Abstract">arXiv:2207.06926</a> (replaced) [<a href="/pdf/2207.06926" title="Download PDF">pdf</a>, <a href="/format/2207.06926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double-Loop Importance Sampling for McKean--Vlasov Stochastic  Differential Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rached%2C+N+B">Nadhir Ben Rached</a>, 
<a href="/search/math?searchtype=author&query=Haji-Ali%2C+A">Abdul-Lateef Haji-Ali</a>, 
<a href="/search/math?searchtype=author&query=Pillai%2C+S+M+S">Shyam Mohan Subbiah Pillai</a>, 
<a href="/search/math?searchtype=author&query=Tempone%2C+R">Ra&#xfa;l Tempone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08012" title="Abstract">arXiv:2207.08012</a> (replaced) [<a href="/pdf/2207.08012" title="Download PDF">pdf</a>, <a href="/format/2207.08012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Referential Games to Learn Compositional Learning Behaviours
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Denamgana%C3%AF%2C+K">Kevin Denamgana&#xef;</a>, 
<a href="/search/cs?searchtype=author&query=Missaoui%2C+S">Sondess Missaoui</a>, 
<a href="/search/cs?searchtype=author&query=Walker%2C+J+A">James Alfred Walker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09210" title="Abstract">arXiv:2207.09210</a> (replaced) [<a href="/pdf/2207.09210" title="Download PDF">pdf</a>, <a href="/format/2207.09210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KinD-LCE Curve Estimation And Retinex Fusion On Low-Light Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+X">Xiaochun Lei</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+W">Weiliang Mai</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Junlin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">He Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zetao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zhaoting Gong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Linjun Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Signal, Image and Video Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09765" title="Abstract">arXiv:2207.09765</a> (replaced) [<a href="/pdf/2207.09765" title="Download PDF">pdf</a>, <a href="/format/2207.09765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ApHMM: Accelerating Profile Hidden Markov Models for Fast and  Energy-Efficient Genome Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Firtina%2C+C">Can Firtina</a>, 
<a href="/search/cs?searchtype=author&query=Pillai%2C+K">Kamlesh Pillai</a>, 
<a href="/search/cs?searchtype=author&query=Kalsi%2C+G+S">Gurpreet S. Kalsi</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+B">Bharathwaj Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Cali%2C+D+S">Damla Senol Cali</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeremie Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shahroodi%2C+T">Taha Shahroodi</a>, 
<a href="/search/cs?searchtype=author&query=Cavlak%2C+M+B">Meryem Banu Cavlak</a>, 
<a href="/search/cs?searchtype=author&query=Lindegger%2C+J">Joel Lindegger</a>, 
<a href="/search/cs?searchtype=author&query=Alser%2C+M">Mohammed Alser</a>, 
<a href="/search/cs?searchtype=author&query=Luna%2C+J+G">Juan G&#xf3;mez Luna</a>, 
<a href="/search/cs?searchtype=author&query=Subramoney%2C+S">Sreenivas Subramoney</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM TACO
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Genomics (q-bio.GN); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.11749" title="Abstract">arXiv:2207.11749</a> (replaced) [<a href="/pdf/2207.11749" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Source Separation of Unknown Numbers of Single-Channel Underwater  Acoustic Signals Based on Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qinggang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kejun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 3 tables. For codes, see <a href="https://github.com/QinggangSUN/unknown_number_source_separation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12062" title="Abstract">arXiv:2207.12062</a> (replaced) [<a href="/pdf/2207.12062" title="Download PDF">pdf</a>, <a href="/format/2207.12062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Asynchronous Control Using Meta-learned Neural Ordinary  Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salehi%2C+A">Achkan Salehi</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BChl%2C+S">Steffen R&#xfc;hl</a>, 
<a href="/search/cs?searchtype=author&query=Doncieux%2C+S">Stephane Doncieux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 double column pages, 14 figures, 3 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Robotics; Print ISSN: 1552-3098; Online ISSN:
  1941-0468
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05604" title="Abstract">arXiv:2208.05604</a> (replaced) [<a href="/pdf/2208.05604" title="Download PDF">pdf</a>, <a href="/format/2208.05604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Incognito in the Metaverse: Achieving Theoretically Optimal  Privacy-Usability Tradeoffs in VR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nair%2C+V">Vivek Nair</a>, 
<a href="/search/cs?searchtype=author&query=Garrido%2C+G+M">Gonzalo Munilla Garrido</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Learn more at <a href="https://rdi.berkeley.edu/metaverse/metaguard/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 36th Annual ACM Symposium on User Interface Software and
  Technology (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.08374" title="Abstract">arXiv:2208.08374</a> (replaced) [<a href="/pdf/2208.08374" title="Download PDF">pdf</a>, <a href="/format/2208.08374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computational Interface to Translate Strategic Intent from  Unstructured Language in a Low-Data Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tambwekar%2C+P">Pradyumna Tambwekar</a>, 
<a href="/search/cs?searchtype=author&query=Dodeja%2C+L">Lakshita Dodeja</a>, 
<a href="/search/cs?searchtype=author&query=Vaska%2C+N">Nathan Vaska</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gombolay%2C+M">Matthew Gombolay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 Pages, 7 figures, 8 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09969" title="Abstract">arXiv:2208.09969</a> (replaced) [<a href="/pdf/2208.09969" title="Download PDF">pdf</a>, <a href="/format/2208.09969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interior over-penalized enriched Galerkin methods for second order  elliptic equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+J+J">Jeonghun J. Lee</a>, 
<a href="/search/math?searchtype=author&query=Ghattas%2C+O">Omar Ghattas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preconditioning for anisotropic cases is improved
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.11319" title="Abstract">arXiv:2208.11319</a> (replaced) [<a href="/pdf/2208.11319" title="Download PDF">pdf</a>, <a href="/format/2208.11319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving the Kidney Exchange Problem Using Privacy-Preserving Integer  Programming (Updated and Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Breuer%2C+M">Malte Breuer</a>, 
<a href="/search/cs?searchtype=author&query=Hein%2C+P">Pascal Hein</a>, 
<a href="/search/cs?searchtype=author&query=Pompe%2C+L">Leonardo Pompe</a>, 
<a href="/search/cs?searchtype=author&query=Temme%2C+B">Ben Temme</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+U">Ulrike Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Wetzel%2C+S">Susanne Wetzel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the updated and extended version of the work published in 19th Annual International Conference on Privacy, Security and Trust (PST2022), August 22-24, 2022, Fredericton, Canada / Virtual Conference, <a href="https://doi.org/10.1109/PST55820.2022.9851968">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12731" title="Abstract">arXiv:2208.12731</a> (replaced) [<a href="/pdf/2208.12731" title="Download PDF">pdf</a>, <a href="/format/2208.12731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Apples to Oranges: Learning Similarity Functions for Data  Produced by Different Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsepenekas%2C+L">Leonidas Tsepenekas</a>, 
<a href="/search/cs?searchtype=author&query=Brugere%2C+I">Ivan Brugere</a>, 
<a href="/search/cs?searchtype=author&query=Lecue%2C+F">Freddy Lecue</a>, 
<a href="/search/cs?searchtype=author&query=Magazzeni%2C+D">Daniele Magazzeni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.13232" title="Abstract">arXiv:2208.13232</a> (replaced) [<a href="/pdf/2208.13232" title="Download PDF">pdf</a>, <a href="/format/2208.13232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Categorical composable cryptography: extended version
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Broadbent%2C+A">Anne Broadbent</a>, 
<a href="/search/cs?searchtype=author&query=Karvonen%2C+M">Martti Karvonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of <a href="/abs/2105.05949">arXiv:2105.05949</a> which appeared in FoSSaCS 2022. Very minor layout updates to the previous version as requested by the journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.13288" title="Abstract">arXiv:2208.13288</a> (replaced) [<a href="/pdf/2208.13288" title="Download PDF">pdf</a>, <a href="/format/2208.13288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Informative Health Indicators Through Unsupervised Contrastive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rombach%2C+K">Katharina Rombach</a>, 
<a href="/search/cs?searchtype=author&query=Michau%2C+G">Gabriel Michau</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCrzle%2C+W">Wilfried B&#xfc;rzle</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+S">Stefan Koller</a>, 
<a href="/search/cs?searchtype=author&query=Fink%2C+O">Olga Fink</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02037" title="Abstract">arXiv:2209.02037</a> (replaced) [<a href="/pdf/2209.02037" title="Download PDF">pdf</a>, <a href="/format/2209.02037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4Ward: a Relayering Strategy for Efficient Training of Arbitrarily  Complex Directed Acyclic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boccato%2C+T">Tommaso Boccato</a>, 
<a href="/search/cs?searchtype=author&query=Ferrante%2C+M">Matteo Ferrante</a>, 
<a href="/search/cs?searchtype=author&query=Duggento%2C+A">Andrea Duggento</a>, 
<a href="/search/cs?searchtype=author&query=Toschi%2C+N">Nicola Toschi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn)

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.02873" title="Abstract">arXiv:2209.02873</a> (replaced) [<a href="/pdf/2209.02873" title="Download PDF">pdf</a>, <a href="/ps/2209.02873" title="Download PostScript">ps</a>, <a href="/format/2209.02873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Difference Equation Approach for the Stability and Robustness of  Compact Schemes for Variable Coefficient PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goswami%2C+A">Anindya Goswami</a>, 
<a href="/search/math?searchtype=author&query=Patel%2C+K+S">Kuldip Singh Patel</a>, 
<a href="/search/math?searchtype=author&query=Sahu%2C+P+K">Pradeep Kumar Sahu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Finance (q-fin.CP)

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.08412" title="Abstract">arXiv:2209.08412</a> (replaced) [<a href="/pdf/2209.08412" title="Download PDF">pdf</a>, <a href="/format/2209.08412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Internal Evasion Attacks in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taejin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shubhranshu Singh</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+N">Nikhil Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Joe-Wong%2C+C">Carlee Joe-Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures (14 images if counting sub-figures separately), Camera ready version for AISTATS 2023, longer version of paper submitted to CrossFL 2022 poster workshop, code available at (<a href="https://github.com/tj-kim/pFedDef_v1">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10866" title="Abstract">arXiv:2209.10866</a> (replaced) [<a href="/pdf/2209.10866" title="Download PDF">pdf</a>, <a href="/format/2209.10866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A One-shot Framework for Distributed Clustered Learning in Heterogeneous  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armacki%2C+A">Aleksandar Armacki</a>, 
<a href="/search/cs?searchtype=author&query=Bajovic%2C+D">Dragana Bajovic</a>, 
<a href="/search/cs?searchtype=author&query=Jakovetic%2C+D">Dusan Jakovetic</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.15448" title="Abstract">arXiv:2209.15448</a> (replaced) [<a href="/pdf/2209.15448" title="Download PDF">pdf</a>, <a href="/format/2209.15448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blessing from Human-AI Interaction: Super Reinforcement Learning in  Confounded Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiayi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhengling Qi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chengchun Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00823" title="Abstract">arXiv:2210.00823</a> (replaced) [<a href="/pdf/2210.00823" title="Download PDF">pdf</a>, <a href="/format/2210.00823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BVI-VFI: A Video Quality Database for Video Frame Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Danier%2C+D">Duolikun Danier</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Bull%2C+D">David Bull</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02177" title="Abstract">arXiv:2210.02177</a> (replaced) [<a href="/pdf/2210.02177" title="Download PDF">pdf</a>, <a href="/format/2210.02177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-objective optimization via equivariant deep hypervolume  approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boelrijk%2C+J">Jim Boelrijk</a>, 
<a href="/search/cs?searchtype=author&query=Ensing%2C+B">Bernd Ensing</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated with camera-ready version. Accepted at ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03116" title="Abstract">arXiv:2210.03116</a> (replaced) [<a href="/pdf/2210.03116" title="Download PDF">pdf</a>, <a href="/format/2210.03116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content-Based Search for Deep Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+D">Daohan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng-Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kumari%2C+N">Nupur Kumari</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+R">Rohan Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun-Yan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our project page is hosted at <a href="https://generative-intelligence-lab.github.io/modelverse/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05261" title="Abstract">arXiv:2210.05261</a> (replaced) [<a href="/pdf/2210.05261" title="Download PDF">pdf</a>, <a href="/format/2210.05261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuanhang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Shiyi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuanyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Cuiyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05974" title="Abstract">arXiv:2210.05974</a> (replaced) [<a href="/pdf/2210.05974" title="Download PDF">pdf</a>, <a href="/format/2210.05974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering the Sketch: A Novel Approach to Embedding Table Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsang%2C+H+L">Henry Ling-Hei Tsang</a>, 
<a href="/search/cs?searchtype=author&query=Ahle%2C+T+D">Thomas Dybdahl Ahle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07440" title="Abstract">arXiv:2210.07440</a> (replaced) [<a href="/pdf/2210.07440" title="Download PDF">pdf</a>, <a href="/format/2210.07440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InterFair: Debiasing with Natural Language Feedback for Fair  Interpretable Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumder%2C+B+P">Bodhisattwa Prasad Majumder</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP 2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10352" title="Abstract">arXiv:2210.10352</a> (replaced) [<a href="/pdf/2210.10352" title="Download PDF">pdf</a>, <a href="/format/2210.10352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Action Segmentation: An Analysis of Modern Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+G">Guodong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Sener%2C+F">Fadime Sener</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+A">Angela Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures, 8 tables, TPAMI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12637" title="Abstract">arXiv:2210.12637</a> (replaced) [<a href="/pdf/2210.12637" title="Download PDF">pdf</a>, <a href="/format/2210.12637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Eigenfunctions Are Structured Representation Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiaxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+P">Peng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cewu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12663" title="Abstract">arXiv:2210.12663</a> (replaced) [<a href="/pdf/2210.12663" title="Download PDF">pdf</a>, <a href="/format/2210.12663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-Regret Learning in Two-Echelon Supply Chain with Unknown Demand  Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingfei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.12770" title="Abstract">arXiv:2210.12770</a> (replaced) [<a href="/pdf/2210.12770" title="Download PDF">pdf</a>, <a href="/format/2210.12770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Cross-Domain Pre-Trained Language Models for Clinical Text Mining:  How Do They Perform on Data-Constrained Fine-Tuning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belkadi%2C+S">Samuel Belkadi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lifeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuping Wu</a>, 
<a href="/search/cs?searchtype=author&query=Antonini%2C+V">Valerio Antonini</a>, 
<a href="/search/cs?searchtype=author&query=Nenadic%2C+G">Goran Nenadic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> working paper - Large Language Models, Fine-tuning LLMs, Clinical NLP, Medication Mining, AI for Healthcare
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13740" title="Abstract">arXiv:2210.13740</a> (replaced) [<a href="/pdf/2210.13740" title="Download PDF">pdf</a>, <a href="/format/2210.13740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latency-aware End-to-end Multi-path Data Transmission for URLLC Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Liu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kiani%2C+A">Abbas Kiani</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+A">Amanda Xiang</a>, 
<a href="/search/cs?searchtype=author&query=John%2C+K">Kaippallimalil John</a>, 
<a href="/search/cs?searchtype=author&query=Saboorian%2C+T">Tony Saboorian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. 5 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.14531" title="Abstract">arXiv:2210.14531</a> (replaced) [<a href="/pdf/2210.14531" title="Download PDF">pdf</a>, <a href="/format/2210.14531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Data Perspectivism and Personalization: An Application to  Social Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plepi%2C+J">Joan Plepi</a>, 
<a href="/search/cs?searchtype=author&query=Neuendorf%2C+B">B&#xe9;la Neuendorf</a>, 
<a href="/search/cs?searchtype=author&query=Flek%2C+L">Lucie Flek</a>, 
<a href="/search/cs?searchtype=author&query=Welch%2C+C">Charles Welch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15500" title="Abstract">arXiv:2210.15500</a> (replaced) [<a href="/pdf/2210.15500" title="Download PDF">pdf</a>, <a href="/format/2210.15500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COFFEE: Counterfactual Fairness for Personalized Text Generation in  Explainable Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi-Chia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sanjabi%2C+M">Maziar Sanjabi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Firooz%2C+H">Hamed Firooz</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+S">Shaoliang Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a long paper accepted by the Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15631" title="Abstract">arXiv:2210.15631</a> (replaced) [<a href="/pdf/2210.15631" title="Download PDF">pdf</a>, <a href="/format/2210.15631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Effective Distillation of Self-Supervised Speech Models for  Automatic Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yujin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+C">Changli Tang</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+Z">Ziyang Ma</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Z">Zhisheng Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xie Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wei-Qiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.16086" title="Abstract">arXiv:2210.16086</a> (replaced) [<a href="/pdf/2210.16086" title="Download PDF">pdf</a>, <a href="/format/2210.16086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KD-EKF: A Consistent Cooperative Localization Estimator Based on Kalman  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+N">Ning Hao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+F">Fenghua He</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C">Chungeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Weilong Xia</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.16524" title="Abstract">arXiv:2210.16524</a> (replaced) [<a href="/pdf/2210.16524" title="Download PDF">pdf</a>, <a href="/format/2210.16524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated clustering with GAN-based data synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jie Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong-Yuan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05895" title="Abstract">arXiv:2211.05895</a> (replaced) [<a href="/pdf/2211.05895" title="Download PDF">pdf</a>, <a href="/format/2211.05895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding ME? Multimodal Evaluation for Fine-grained Visual  Commonsense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhecan Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+H">Haoxuan You</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yicheng He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shih-Fu Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2022 Long Paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06411" title="Abstract">arXiv:2211.06411</a> (replaced) [<a href="/pdf/2211.06411" title="Download PDF">pdf</a>, <a href="/format/2211.06411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qafny: Quantum Program Verification Through Type-guided Classical  Separation Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+L">Liyi Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+M">Mingwei Zhu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cleaveland%2C+R">Rance Cleaveland</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nicolellis%2C+A">Alexander Nicolellis</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lee%2C+Y">Yi Lee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chang%2C+L">Le Chang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+X">Xiaodi Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Version 3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07058" title="Abstract">arXiv:2211.07058</a> (replaced) [<a href="/pdf/2211.07058" title="Download PDF">pdf</a>, <a href="/format/2211.07058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stronger 3-SUM Lower Bounds for Approximate Distance Oracles via  Additive Combinatorics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abboud%2C+A">Amir Abboud</a>, 
<a href="/search/cs?searchtype=author&query=Bringmann%2C+K">Karl Bringmann</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+N">Nick Fischer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract shortened to fit arXiv requirements
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07482" title="Abstract">arXiv:2211.07482</a> (replaced) [<a href="/pdf/2211.07482" title="Download PDF">pdf</a>, <a href="/format/2211.07482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying O(3) Equivariant Neural Networks Design with Tensor-Network  Formalism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zimu Li</a>, 
<a href="/search/cs?searchtype=author&query=Pengmei%2C+Z">Zihan Pengmei</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Han Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Thiede%2C+E">Erik Thiede</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kondor%2C+R">Risi Kondor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages + 12-page supplementary materials, many figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantum Physics (quant-ph); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08238" title="Abstract">arXiv:2211.08238</a> (replaced) [<a href="/pdf/2211.08238" title="Download PDF">pdf</a>, <a href="/format/2211.08238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Contrastive Learning and Numerical Evidence for Confusing  Legal Judgment Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Leilei Gan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baokui Li</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yating Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09935" title="Abstract">arXiv:2211.09935</a> (replaced) [<a href="/pdf/2211.09935" title="Download PDF">pdf</a>, <a href="/format/2211.09935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPE: Corrective Actions from Precondition Errors using Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+S+S">Shreyas Sundara Raman</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+V">Vanya Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Paulius%2C+D">David Paulius</a>, 
<a href="/search/cs?searchtype=author&query=Idrees%2C+I">Ifrah Idrees</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+E">Eric Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Mooney%2C+R">Ray Mooney</a>, 
<a href="/search/cs?searchtype=author&query=Tellex%2C+S">Stefanie Tellex</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, Under Review at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10192" title="Abstract">arXiv:2211.10192</a> (replaced) [<a href="/pdf/2211.10192" title="Download PDF">pdf</a>, <a href="/ps/2211.10192" title="Download PostScript">ps</a>, <a href="/format/2211.10192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven basis for reconstructing the contrast in inverse scattering:  Picard criterion, regularity, regularization, and stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Meng%2C+S">Shixu Meng</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM J. Appl. Math. 83 (2023), no.5, pp.2003--2026
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10582" title="Abstract">arXiv:2211.10582</a> (replaced) [<a href="/pdf/2211.10582" title="Download PDF">pdf</a>, <a href="/format/2211.10582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear RNNs Provably Learn Linear Dynamic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lifu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+S">Shengwei Yi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+B">Bo Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xing Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10586" title="Abstract">arXiv:2211.10586</a> (replaced) [<a href="/pdf/2211.10586" title="Download PDF">pdf</a>, <a href="/format/2211.10586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Justin Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Si Si</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023 submission link: <a href="https://openreview.net/forum?id=dN70O8pmW8">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11093" title="Abstract">arXiv:2211.11093</a> (replaced) [<a href="/pdf/2211.11093" title="Download PDF">pdf</a>, <a href="/format/2211.11093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VER: Unifying Verbalizing Entities and Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12588" title="Abstract">arXiv:2211.12588</a> (replaced) [<a href="/pdf/2211.12588" title="Download PDF">pdf</a>, <a href="/format/2211.12588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Program of Thoughts Prompting: Disentangling Computation from Reasoning  for Numerical Reasoning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xueguang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+W+W">William W. Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at TMLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13308" title="Abstract">arXiv:2211.13308</a> (replaced) [<a href="/pdf/2211.13308" title="Download PDF">pdf</a>, <a href="/format/2211.13308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SciRepEval: A Multi-Format Benchmark for Scientific Document  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Amanpreet Singh</a>, 
<a href="/search/cs?searchtype=author&query=D%27Arcy%2C+M">Mike D&#x27;Arcy</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Downey%2C+D">Doug Downey</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+S">Sergey Feldman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures, 11 tables. Accepted in EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16031" title="Abstract">arXiv:2211.16031</a> (replaced) [<a href="/pdf/2211.16031" title="Download PDF">pdf</a>, <a href="/format/2211.16031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Syntactic Substitutability as Unsupervised Dependency Syntax
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jian%2C+J">Jasper Jian</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16494" title="Abstract">arXiv:2211.16494</a> (replaced) [<a href="/pdf/2211.16494" title="Download PDF">pdf</a>, <a href="/format/2211.16494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Ability of Graph Neural Networks to Model Interactions Between  Vertices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razin%2C+N">Noam Razin</a>, 
<a href="/search/cs?searchtype=author&query=Verbin%2C+T">Tom Verbin</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Nadav Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16965" title="Abstract">arXiv:2211.16965</a> (replaced) [<a href="/pdf/2211.16965" title="Download PDF">pdf</a>, <a href="/format/2211.16965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Federated Deep Clustering based on GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jie Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong-Yuan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.17154" title="Abstract">arXiv:2211.17154</a> (replaced) [<a href="/pdf/2211.17154" title="Download PDF">pdf</a>, <a href="/format/2211.17154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Regret-optimal Cooperative Nonstochastic Multi-armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yi%2C+J">Jialin Yi</a>, 
<a href="/search/stat?searchtype=author&query=Vojnovi%C4%87%2C+M">Milan Vojnovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in AAMAS 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 International Conference on Autonomous
  Agents and Multiagent Systems 1329 1335
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00482" title="Abstract">arXiv:2212.00482</a> (replaced) [<a href="/pdf/2212.00482" title="Download PDF">pdf</a>, <a href="/format/2212.00482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn  Response Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jingcheng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hengwei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xuewei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+Y">Yuanchen Ju</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wei Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02851" title="Abstract">arXiv:2212.02851</a> (replaced) [<a href="/pdf/2212.02851" title="Download PDF">pdf</a>, <a href="/format/2212.02851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkateswaran%2C+P">Praveen Venkateswaran</a>, 
<a href="/search/cs?searchtype=author&query=Duesterwald%2C+E">Evelyn Duesterwald</a>, 
<a href="/search/cs?searchtype=author&query=Isahagian%2C+V">Vatche Isahagian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03765" title="Abstract">arXiv:2212.03765</a> (replaced) [<a href="/pdf/2212.03765" title="Download PDF">pdf</a>, <a href="/format/2212.03765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Gradient Flows with Provable Fixed-Time Convergence and Fast  Evasion of Non-Degenerate Saddle Points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baranwal%2C+M">Mayank Baranwal</a>, 
<a href="/search/cs?searchtype=author&query=Budhraja%2C+P">Param Budhraja</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+V">Vishal Raj</a>, 
<a href="/search/cs?searchtype=author&query=Hota%2C+A+R">Ashish R. Hota</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Transactions on Automatic Control (TAC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05111" title="Abstract">arXiv:2212.05111</a> (replaced) [<a href="/pdf/2212.05111" title="Download PDF">pdf</a>, <a href="/format/2212.05111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoK: MEV Countermeasures: Theory and Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Ken Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Youwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Feng Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07248" title="Abstract">arXiv:2212.07248</a> (replaced) [<a href="/pdf/2212.07248" title="Download PDF">pdf</a>, <a href="/format/2212.07248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Joint Diagonalization of Symmetric Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=He%2C+H">Haoze He</a>, 
<a href="/search/math?searchtype=author&query=Kressner%2C+D">Daniel Kressner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08898" title="Abstract">arXiv:2212.08898</a> (replaced) [<a href="/pdf/2212.08898" title="Download PDF">pdf</a>, <a href="/format/2212.08898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Approach for Resilience and Causal Responsibility with Integer  Linear Programming (ILP) and LP Relaxations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makhija%2C+N">Neha Makhija</a>, 
<a href="/search/cs?searchtype=author&query=Gatterbauer%2C+W">Wolfgang Gatterbauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 15 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ACM Manag. Data 1, 4 (SIGMOD), Article 228 (December 2023),
  43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09121" title="Abstract">arXiv:2212.09121</a> (replaced) [<a href="/pdf/2212.09121" title="Download PDF">pdf</a>, <a href="/format/2212.09121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIScatter: Unifying Backscatter Communication and Reconfigurable  Intelligent Surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by JSAC-SI-ESIT 2023; The code is publicly available at <a href="https://github.com/snowztail/riscatter/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09702" title="Abstract">arXiv:2212.09702</a> (replaced) [<a href="/pdf/2212.09702" title="Download PDF">pdf</a>, <a href="/format/2212.09702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Event Individuation for Document-Level Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gantt%2C+W">William Gantt</a>, 
<a href="/search/cs?searchtype=author&query=Kriz%2C+R">Reno Kriz</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunmo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Vashishtha%2C+S">Siddharth Vashishtha</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+A+S">Aaron Steven White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP: Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09724" title="Abstract">arXiv:2212.09724</a> (replaced) [<a href="/pdf/2212.09724" title="Download PDF">pdf</a>, <a href="/format/2212.09724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Retrieve-and-Read Framework for Knowledge Graph Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pahuja%2C+V">Vardaan Pahuja</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Latapie%2C+H">Hugo Latapie</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+J">Jayanth Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CIKM'23; Published version DOI: <a href="https://doi.org/10.1145/3583780.3614769">this https URL</a> ;12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09873" title="Abstract">arXiv:2212.09873</a> (replaced) [<a href="/pdf/2212.09873" title="Download PDF">pdf</a>, <a href="/format/2212.09873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study on Textual Saliency of Styles from Eye Tracking,  Annotations, and Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Langis%2C+K">Karin de Langis</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10901" title="Abstract">arXiv:2212.10901</a> (replaced) [<a href="/pdf/2212.10901" title="Download PDF">pdf</a>, <a href="/format/2212.10901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALCAP: Alignment-Augmented Music Captioner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihao He</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+W">Weituo Hao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei-Tsung Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xuchen Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11680" title="Abstract">arXiv:2212.11680</a> (replaced) [<a href="/pdf/2212.11680" title="Download PDF">pdf</a>, <a href="/format/2212.11680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth Sailing: Improving Active Learning for Pre-trained Language  Models with Representation Smoothness Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Juki%C4%87%2C+J">Josip Juki&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0najder%2C+J">Jan &#x160;najder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Learning with Small Data 2023, Association for Computational Linguistics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.11707" title="Abstract">arXiv:2212.11707</a> (replaced) [<a href="/pdf/2212.11707" title="Download PDF">pdf</a>, <a href="/format/2212.11707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sub-structure characteristic mode analysis of microstrip antennas using  a global multi-trace formulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G+S">Guang Shang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Bagci%2C+H">Hakan Bagci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14225" title="Abstract">arXiv:2212.14225</a> (replaced) [<a href="/pdf/2212.14225" title="Download PDF">pdf</a>, <a href="/ps/2212.14225" title="Download PostScript">ps</a>, <a href="/format/2212.14225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symplectic self-orthogonal quasi-cyclic codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+C">Chaofeng Guan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruihu Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jingjie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhi Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00007" title="Abstract">arXiv:2301.00007</a> (replaced) [<a href="/pdf/2301.00007" title="Download PDF">pdf</a>, <a href="/ps/2301.00007" title="Download PostScript">ps</a>, <a href="/format/2301.00007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selected aspects of complex, hypercomplex and fuzzy neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niemczynowicz%2C+A">Agnieszka Niemczynowicz</a>, 
<a href="/search/cs?searchtype=author&query=Kycia%2C+R+A">Rados&#x142;aw A. Kycia</a>, 
<a href="/search/cs?searchtype=author&query=Jaworski%2C+M">Maciej Jaworski</a>, 
<a href="/search/cs?searchtype=author&query=Siemaszko%2C+A">Artur Siemaszko</a>, 
<a href="/search/cs?searchtype=author&query=Calabuig%2C+J+M">Jose M. Calabuig</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Raffi%2C+L+M">Lluis M. Garc&#xed;a-Raffi</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+B">Baruch Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Berseghyan%2C+D">Diana Berseghyan</a>, 
<a href="/search/cs?searchtype=author&query=Perfiljeva%2C+I">Irina Perfiljeva</a>, 
<a href="/search/cs?searchtype=author&query=Novak%2C+V">Vilem Novak</a>, 
<a href="/search/cs?searchtype=author&query=Artiemjew%2C+P">Piotr Artiemjew</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00104" title="Abstract">arXiv:2301.00104</a> (replaced) [<a href="/pdf/2301.00104" title="Download PDF">pdf</a>, <a href="/ps/2301.00104" title="Download PostScript">ps</a>, <a href="/format/2301.00104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Separating Computational and Statistical Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghazi%2C+B">Badih Ghazi</a>, 
<a href="/search/cs?searchtype=author&query=Ilango%2C+R">Rahul Ilango</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+P">Pritish Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Ravi Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Manurangsi%2C+P">Pasin Manurangsi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at Foundations of Computer Science (FOCS) 2023. Changes compared to previous version: (1) The lower bound for SDP is now stronger in that it holds also for a certain inverse-polynomially large delta as opposed to only non-negligible delta, and (2) the presentation is cleaned up
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.01597" title="Abstract">arXiv:2301.01597</a> (replaced) [<a href="/pdf/2301.01597" title="Download PDF">pdf</a>, <a href="/format/2301.01597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystify Problem-Dependent Power of Quantum Neural Networks on  Multi-Class Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Du%2C+Y">Yuxuan Du</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yang%2C+Y">Yibo Yang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hsieh%2C+M">Min-Hsiu Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version. Published on PRL
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Lett. 131, 140601 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02729" title="Abstract">arXiv:2301.02729</a> (replaced) [<a href="/pdf/2301.02729" title="Download PDF">pdf</a>, <a href="/ps/2301.02729" title="Download PostScript">ps</a>, <a href="/format/2301.02729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Characterization of Multioutput Learnability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+V">Vinod Raman</a>, 
<a href="/search/cs?searchtype=author&query=Subedi%2C+U">Unique Subedi</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04011" title="Abstract">arXiv:2301.04011</a> (replaced) [<a href="/pdf/2301.04011" title="Download PDF">pdf</a>, <a href="/format/2301.04011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Support and Trivial Prototypes for Interpretable Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuanhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fengbei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+D+J">Davis J. McCarthy</a>, 
<a href="/search/cs?searchtype=author&query=Frazer%2C+H">Helen Frazer</a>, 
<a href="/search/cs?searchtype=author&query=Carneiro%2C+G">Gustavo Carneiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, Code: <a href="https://github.com/cwangrun/ST-ProtoPNet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08067" title="Abstract">arXiv:2301.08067</a> (replaced) [<a href="/pdf/2301.08067" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting CNN Predictions using Conditional Generative Adversarial  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T%2C+A+G+R">Akash Guna R T</a>, 
<a href="/search/cs?searchtype=author&query=Benitez%2C+R">Raul Benitez</a>, 
<a href="/search/cs?searchtype=author&query=K%2C+S+O">Sikha O K</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08110" title="Abstract">arXiv:2301.08110</a> (replaced) [<a href="/pdf/2301.08110" title="Download PDF">pdf</a>, <a href="/format/2301.08110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AtMan: Understanding Transformer Predictions Through Memory Efficient  Attention Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deiseroth%2C+B">Bj&#xf6;rn Deiseroth</a>, 
<a href="/search/cs?searchtype=author&query=Deb%2C+M">Mayukh Deb</a>, 
<a href="/search/cs?searchtype=author&query=Weinbach%2C+S">Samuel Weinbach</a>, 
<a href="/search/cs?searchtype=author&query=Brack%2C+M">Manuel Brack</a>, 
<a href="/search/cs?searchtype=author&query=Schramowski%2C+P">Patrick Schramowski</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09112" title="Abstract">arXiv:2301.09112</a> (replaced) [<a href="/pdf/2301.09112" title="Download PDF">pdf</a>, <a href="/format/2301.09112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Natural Language Models: Recent Advances and  Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lijie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Habernal%2C+I">Ivan Habernal</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09223" title="Abstract">arXiv:2301.09223</a> (replaced) [<a href="/pdf/2301.09223" title="Download PDF">pdf</a>, <a href="/format/2301.09223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doubly Adversarial Federated Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yi%2C+J">Jialin Yi</a>, 
<a href="/search/stat?searchtype=author&query=Vojnovi%C4%87%2C+M">Milan Vojnovi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ICML 2023 <a href="https://proceedings.mlr.press/v202/yi23a.html">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 40th International Conference on Machine
  Learning 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09413" title="Abstract">arXiv:2301.09413</a> (replaced) [<a href="/pdf/2301.09413" title="Download PDF">pdf</a>, <a href="/format/2301.09413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manticore: Hardware-Accelerated RTL Simulation with Static  Bulk-Synchronous Parallelism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emami%2C+M">Mahyar Emami</a>, 
<a href="/search/cs?searchtype=author&query=Kashani%2C+S">Sahand Kashani</a>, 
<a href="/search/cs?searchtype=author&query=Kamahori%2C+K">Keisuke Kamahori</a>, 
<a href="/search/cs?searchtype=author&query=Pourghannad%2C+M+S">Mohammad Sepehr Pourghannad</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+R">Ritik Raj</a>, 
<a href="/search/cs?searchtype=author&query=Larus%2C+J+R">James R. Larus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09928" title="Abstract">arXiv:2301.09928</a> (replaced) [<a href="/pdf/2301.09928" title="Download PDF">pdf</a>, <a href="/format/2301.09928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validation and traceability of miniaturized multi-parameter cluster of  radiosondes used for atmospheric observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Abdunabiev%2C+S">Shahbozbek Abdunabiev</a>, 
<a href="/search/eess?searchtype=author&query=Musacchio%2C+C">Chiara Musacchio</a>, 
<a href="/search/eess?searchtype=author&query=Merlone%2C+A">Andrea Merlone</a>, 
<a href="/search/eess?searchtype=author&query=Paredes%2C+M">Miryam Paredes</a>, 
<a href="/search/eess?searchtype=author&query=Pasero%2C+E">Eros Pasero</a>, 
<a href="/search/eess?searchtype=author&query=Tordella%2C+D">Daniela Tordella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Cloud, Lagrangian fluctuation tracking, Radiosonde, Turbulent dispersion, Turbulent diffusion, Stereo vision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10672" title="Abstract">arXiv:2301.10672</a> (replaced) [<a href="/pdf/2301.10672" title="Download PDF">pdf</a>, <a href="/format/2301.10672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Shape Model Trees: Recognition of 3-D Indoor Scenes and  Prediction of Object Poses for Mobile Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%C3%9Fner%2C+P">Pascal Mei&#xdf;ner</a>, 
<a href="/search/cs?searchtype=author&query=Dillmann%2C+R">R&#xfc;diger Dillmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 24 figures; For associated video clips, see <a href="https://www.youtube.com/playlist?list=PL3RZ_UQY_uOIfuIJNqdS8wDMjTjOAeOmu">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11535" title="Abstract">arXiv:2301.11535</a> (replaced) [<a href="/pdf/2301.11535" title="Download PDF">pdf</a>, <a href="/format/2301.11535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Informative Representation for Fairness-aware Multivariate  Time-series Forecasting: A Group-based Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hui He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shoujin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+K">Kun Yi</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhendong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Longbing Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12584" title="Abstract">arXiv:2301.12584</a> (replaced) [<a href="/pdf/2301.12584" title="Download PDF">pdf</a>, <a href="/format/2301.12584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Exact Leverage Score Sampling from Khatri-Rao Products with  Applications to Tensor Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bharadwaj%2C+V">Vivek Bharadwaj</a>, 
<a href="/search/math?searchtype=author&query=Malik%2C+O+A">Osman Asif Malik</a>, 
<a href="/search/math?searchtype=author&query=Murray%2C+R">Riley Murray</a>, 
<a href="/search/math?searchtype=author&query=Grigori%2C+L">Laura Grigori</a>, 
<a href="/search/math?searchtype=author&query=Buluc%2C+A">Aydin Buluc</a>, 
<a href="/search/math?searchtype=author&query=Demmel%2C+J">James Demmel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at the 37th Conference on Neural Information Processing Systems (Neurips'23). 28 pages, 10 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12609" title="Abstract">arXiv:2301.12609</a> (replaced) [<a href="/pdf/2301.12609" title="Download PDF">pdf</a>, <a href="/format/2301.12609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00861" title="Abstract">arXiv:2302.00861</a> (replaced) [<a href="/pdf/2302.00861" title="Download PDF">pdf</a>, <a href="/format/2302.00861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiaxiang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haixu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00871" title="Abstract">arXiv:2302.00871</a> (replaced) [<a href="/pdf/2302.00871" title="Download PDF">pdf</a>, <a href="/format/2302.00871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using In-Context Learning to Improve Dialogue Safety
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meade%2C+N">Nicholas Meade</a>, 
<a href="/search/cs?searchtype=author&query=Gella%2C+S">Spandana Gella</a>, 
<a href="/search/cs?searchtype=author&query=Hazarika%2C+D">Devamanyu Hazarika</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Prakhar Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Di Jin</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hakkani-T%C3%BCr%2C+D">Dilek Hakkani-T&#xfc;r</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01576" title="Abstract">arXiv:2302.01576</a> (replaced) [<a href="/pdf/2302.01576" title="Download PDF">pdf</a>, <a href="/format/2302.01576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResMem: Learn what you can and memorize the rest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lukasik%2C+M">Michal Lukasik</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+V">Vaishnavh Nagarajan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zonglin Li</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A+S">Ankit Singh Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Zaheer%2C+M">Manzil Zaheer</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+A+K">Aditya Krishna Menon</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sanjiv Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01976" title="Abstract">arXiv:2302.01976</a> (replaced) [<a href="/pdf/2302.01976" title="Download PDF">pdf</a>, <a href="/format/2302.01976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPARLING: Learning Latent Representations with Extremely Sparse  Activations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+K">Kavi Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Bastani%2C+O">Osbert Bastani</a>, 
<a href="/search/cs?searchtype=author&query=Solar-Lezama%2C+A">Armando Solar-Lezama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01990" title="Abstract">arXiv:2302.01990</a> (replaced) [<a href="/e-print/2302.01990" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HADES: Hardware/Algorithm Co-design in DNN accelerators using  Energy-efficient Approximate Alphabet Set Multipliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+A">Arani Roy</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Some results have been found incorrect through new experiments. Will upload the correct one once this paper has been withdrawn
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02601" title="Abstract">arXiv:2302.02601</a> (replaced) [<a href="/pdf/2302.02601" title="Download PDF">pdf</a>, <a href="/format/2302.02601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Representations of Bi-level Knowledge Graphs for Reasoning  beyond Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+C">Chanyoung Chung</a>, 
<a href="/search/cs?searchtype=author&query=Whang%2C+J+J">Joyce Jiyoung Whang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, 15 tables. 37th AAAI Conference on Artificial Intelligence (AAAI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03693" title="Abstract">arXiv:2302.03693</a> (replaced) [<a href="/pdf/2302.03693" title="Download PDF">pdf</a>, <a href="/format/2302.03693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept Algebra for Score-Based Conditional Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Negrea%2C+J">Jeffrey Negrea</a>, 
<a href="/search/cs?searchtype=author&query=Veitch%2C+V">Victor Veitch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03857" title="Abstract">arXiv:2302.03857</a> (replaced) [<a href="/pdf/2302.03857" title="Download PDF">pdf</a>, <a href="/format/2302.03857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xilie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04012" title="Abstract">arXiv:2302.04012</a> (replaced) [<a href="/pdf/2302.04012" title="Download PDF">pdf</a>, <a href="/format/2302.04012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeLMSec Benchmark: Systematically Evaluating and Finding Security  Vulnerabilities in Black-Box Code Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajipour%2C+H">Hossein Hajipour</a>, 
<a href="/search/cs?searchtype=author&query=Hassler%2C+K">Keno Hassler</a>, 
<a href="/search/cs?searchtype=author&query=Holz%2C+T">Thorsten Holz</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nherr%2C+L">Lea Sch&#xf6;nherr</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04858" title="Abstract">arXiv:2302.04858</a> (replaced) [<a href="/pdf/2302.04858" title="Download PDF">pdf</a>, <a href="/format/2302.04858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot  Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuolin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ping%2C+W">Wei Ping</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Korthikanti%2C+V">Vijay Korthikanti</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+W">Weili Nie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">De-An Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Linxi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhiding Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+S">Shiyi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming-Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shoeybi%2C+M">Mohammad Shoeybi</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06541" title="Abstract">arXiv:2302.06541</a> (replaced) [<a href="/pdf/2302.06541" title="Download PDF">pdf</a>, <a href="/format/2302.06541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Agile Text Classifiers for Everyone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mozes%2C+M">Maximilian Mozes</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+J">Jessica Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Tomanek%2C+K">Katrin Tomanek</a>, 
<a href="/search/cs?searchtype=author&query=Kouate%2C+M">Muhamed Kouate</a>, 
<a href="/search/cs?searchtype=author&query=Thain%2C+N">Nithum Thain</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+A">Ann Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Bolukbasi%2C+T">Tolga Bolukbasi</a>, 
<a href="/search/cs?searchtype=author&query=Dixon%2C+L">Lucas Dixon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07241" title="Abstract">arXiv:2302.07241</a> (replaced) [<a href="/pdf/2302.07241" title="Download PDF">pdf</a>, <a href="/format/2302.07241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConceptFusion: Open-set Multimodal 3D Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jatavallabhula%2C+K+M">Krishna Murthy Jatavallabhula</a>, 
<a href="/search/cs?searchtype=author&query=Kuwajerwala%2C+A">Alihusein Kuwajerwala</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Qiao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Omama%2C+M">Mohd Omama</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Maalouf%2C+A">Alaa Maalouf</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+G">Ganesh Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Saryazdi%2C+S">Soroush Saryazdi</a>, 
<a href="/search/cs?searchtype=author&query=Keetha%2C+N">Nikhil Keetha</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Ayush Tewari</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=de+Melo%2C+C+M">Celso Miguel de Melo</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+M">Madhava Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Paull%2C+L">Liam Paull</a>, 
<a href="/search/cs?searchtype=author&query=Shkurti%2C+F">Florian Shkurti</a>, 
<a href="/search/cs?searchtype=author&query=Torralba%2C+A">Antonio Torralba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> RSS 2023. Project page: <a href="https://concept-fusion.github.io">this https URL</a> Explainer video: <a href="https://www.youtube.com/watch?v=rkXgws8fiDs">this https URL</a> Code: <a href="https://github.com/concept-fusion/concept-fusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07268" title="Abstract">arXiv:2302.07268</a> (replaced) [<a href="/pdf/2302.07268" title="Download PDF">pdf</a>, <a href="/format/2302.07268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Chat Assistants can Improve Conversations about Divisive Topics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Argyle%2C+L+P">Lisa P. Argyle</a>, 
<a href="/search/cs?searchtype=author&query=Busby%2C+E">Ethan Busby</a>, 
<a href="/search/cs?searchtype=author&query=Gubler%2C+J">Joshua Gubler</a>, 
<a href="/search/cs?searchtype=author&query=Bail%2C+C">Chris Bail</a>, 
<a href="/search/cs?searchtype=author&query=Howe%2C+T">Thomas Howe</a>, 
<a href="/search/cs?searchtype=author&query=Rytting%2C+C">Christopher Rytting</a>, 
<a href="/search/cs?searchtype=author&query=Wingate%2C+D">David Wingate</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07384" title="Abstract">arXiv:2302.07384</a> (replaced) [<a href="/pdf/2302.07384" title="Download PDF">pdf</a>, <a href="/format/2302.07384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Geometry of Neural Nets&#x27; Parameter Spaces Under Reparametrization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kristiadi%2C+A">Agustinus Kristiadi</a>, 
<a href="/search/cs?searchtype=author&query=Dangel%2C+F">Felix Dangel</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08463" title="Abstract">arXiv:2302.08463</a> (replaced) [<a href="/pdf/2302.08463" title="Download PDF">pdf</a>, <a href="/format/2302.08463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Grasping with a Learned Meta-Controller
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yinsen Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingxi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jayaraman%2C+D">Dinesh Jayaraman</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11006" title="Abstract">arXiv:2302.11006</a> (replaced) [<a href="/pdf/2302.11006" title="Download PDF">pdf</a>, <a href="/format/2302.11006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven reduced-order modelling for blood flow simulations with  geometry-informed snapshots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Dongwei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Krzhizhanovskaya%2C+V">Valeria Krzhizhanovskaya</a>, 
<a href="/search/cs?searchtype=author&query=Hoekstra%2C+A+G">Alfons G. Hoekstra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG); Biological Physics (physics.bio-ph); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12001" title="Abstract">arXiv:2302.12001</a> (replaced) [<a href="/pdf/2302.12001" title="Download PDF">pdf</a>, <a href="/format/2302.12001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Projection Forest Initialization for Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alshammari%2C+M">Mashaan Alshammari</a>, 
<a href="/search/cs?searchtype=author&query=Stavrakakis%2C+J">John Stavrakakis</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A+F">Adel F. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Takatsuka%2C+M">Masahiro Takatsuka</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> MethodsX, Volume 11, December 2023, 102315
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13159" title="Abstract">arXiv:2302.13159</a> (replaced) [<a href="/pdf/2302.13159" title="Download PDF">pdf</a>, <a href="/format/2302.13159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Analysis of a Simple Discretization Method for a Class of  Strongly Singular Integral Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Costabel%2C+M">Martin Costabel</a>, 
<a href="/search/math?searchtype=author&query=Dauge%2C+M">Monique Dauge</a>, 
<a href="/search/math?searchtype=author&query=Nedaiasl%2C+K">Khadijeh Nedaiasl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 7 figures, In V3: added a new reference, corrected some typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14699" title="Abstract">arXiv:2302.14699</a> (replaced) [<a href="/pdf/2302.14699" title="Download PDF">pdf</a>, <a href="/format/2302.14699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Tennenbaum&#x27;s Theorem in Constructive Type Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hermes%2C+M">Marc Hermes</a>, 
<a href="/search/math?searchtype=author&query=Kirst%2C+D">Dominik Kirst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, extension of conference paper published at FSCD 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00564" title="Abstract">arXiv:2303.00564</a> (replaced) [<a href="/pdf/2303.00564" title="Download PDF">pdf</a>, <a href="/format/2303.00564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning curves for deep structured Gaussian feature models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zavatone-Veth%2C+J+A">Jacob A. Zavatone-Veth</a>, 
<a href="/search/stat?searchtype=author&query=Pehlevan%2C+C">Cengiz Pehlevan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14+18 pages, 2+1 figures. NeurIPS 2023 Camera Ready
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems 36 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01029" title="Abstract">arXiv:2303.01029</a> (replaced) [<a href="/pdf/2303.01029" title="Download PDF">pdf</a>, <a href="/format/2303.01029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear combination of Hamiltonian simulation for nonunitary dynamics  with optimal state preparation cost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=An%2C+D">Dong An</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+J">Jin-Peng Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lin%2C+L">Lin Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6+15 pages, 1 figure
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Lett. 131, 150603 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01432" title="Abstract">arXiv:2303.01432</a> (replaced) [<a href="/pdf/2303.01432" title="Download PDF">pdf</a>, <a href="/format/2303.01432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WiCE: Real-World Entailment for Claims in Wikipedia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamoi%2C+R">Ryo Kamoi</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+T">Tanya Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+J+D">Juan Diego Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03300" title="Abstract">arXiv:2303.03300</a> (replaced) [<a href="/pdf/2303.03300" title="Download PDF">pdf</a>, <a href="/format/2303.03300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chasing Fairness Under Distribution Shift: A Model Weight Perturbation  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhimeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaotian Han</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hongye Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanchu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+N">Na Zou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05639" title="Abstract">arXiv:2303.05639</a> (replaced) [<a href="/pdf/2303.05639" title="Download PDF">pdf</a>, <a href="/format/2303.05639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised One-Shot Learning for Automatic Segmentation of StyleGAN  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manerikar%2C+A">Ankit Manerikar</a>, 
<a href="/search/cs?searchtype=author&query=Kak%2C+A+C">Avinash C. Kak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05823" title="Abstract">arXiv:2303.05823</a> (replaced) [<a href="/pdf/2303.05823" title="Download PDF">pdf</a>, <a href="/format/2303.05823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High order linearly implicit methods for semilinear evolution PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dujardin%2C+G">Guillaume Dujardin</a> (Paradyse, LPP), 
<a href="/search/math?searchtype=author&query=Lacroix-Violet%2C+I">Ingrid Lacroix-Violet</a> (IECL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06470" title="Abstract">arXiv:2303.06470</a> (replaced) [<a href="/pdf/2303.06470" title="Download PDF">pdf</a>, <a href="/format/2303.06470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prefix-Tree Decoding for Predicting Mass Spectra from Molecules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Goldman%2C+S">Samuel Goldman</a>, 
<a href="/search/q-bio?searchtype=author&query=Bradshaw%2C+J">John Bradshaw</a>, 
<a href="/search/q-bio?searchtype=author&query=Xin%2C+J">Jiayi Xin</a>, 
<a href="/search/q-bio?searchtype=author&query=Coley%2C+C+W">Connor W. Coley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08559" title="Abstract">arXiv:2303.08559</a> (replaced) [<a href="/pdf/2303.08559" title="Download PDF">pdf</a>, <a href="/format/2303.08559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Is Not a Good Few-shot Information Extractor, but a  Good Reranker for Hard Samples!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yubo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">YongChing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09064" title="Abstract">arXiv:2303.09064</a> (replaced) [<a href="/pdf/2303.09064" title="Download PDF">pdf</a>, <a href="/format/2303.09064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual skip connections in U-Net, ResUnet and U-Net3+ for remote  extraction of buildings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neupane%2C+B">Bipul Neupane</a>, 
<a href="/search/cs?searchtype=author&query=Aryal%2C+J">Jagannath Aryal</a>, 
<a href="/search/cs?searchtype=author&query=Rajabifard%2C+A">Abbas Rajabifard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to Springer for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10256" title="Abstract">arXiv:2303.10256</a> (replaced) [<a href="/pdf/2303.10256" title="Download PDF">pdf</a>, <a href="/format/2303.10256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PINNSim: A Simulator for Power System Dynamics based on Physics-Informed  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Stiasny%2C+J">Jochen Stiasny</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Baosen Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chatzivasileiadis%2C+S">Spyros Chatzivasileiadis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to the 23rd Power Systems Computation Conference (PSCC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11315" title="Abstract">arXiv:2303.11315</a> (replaced) [<a href="/pdf/2303.11315" title="Download PDF">pdf</a>, <a href="/format/2303.11315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-faithful Prompting for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenxuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+H">Hoifung Poon</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Findings. Code and data are released at <a href="https://github.com/wzhouad/context-faithful-llm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12314" title="Abstract">arXiv:2303.12314</a> (replaced) [<a href="/pdf/2303.12314" title="Download PDF">pdf</a>, <a href="/format/2303.12314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization  for Few-shot Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+K">Kaihang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juncheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hongye Song</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaozhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12528" title="Abstract">arXiv:2303.12528</a> (replaced) [<a href="/pdf/2303.12528" title="Download PDF">pdf</a>, <a href="/format/2303.12528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEGA: Multilingual Evaluation of Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kabir Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Diddee%2C+H">Harshita Diddee</a>, 
<a href="/search/cs?searchtype=author&query=Hada%2C+R">Rishav Hada</a>, 
<a href="/search/cs?searchtype=author&query=Ochieng%2C+M">Millicent Ochieng</a>, 
<a href="/search/cs?searchtype=author&query=Ramesh%2C+K">Krithika Ramesh</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Prachi Jain</a>, 
<a href="/search/cs?searchtype=author&query=Nambi%2C+A">Akshay Nambi</a>, 
<a href="/search/cs?searchtype=author&query=Ganu%2C+T">Tanuja Ganu</a>, 
<a href="/search/cs?searchtype=author&query=Segal%2C+S">Sameer Segal</a>, 
<a href="/search/cs?searchtype=author&query=Axmed%2C+M">Maxamed Axmed</a>, 
<a href="/search/cs?searchtype=author&query=Bali%2C+K">Kalika Bali</a>, 
<a href="/search/cs?searchtype=author&query=Sitaram%2C+S">Sunayana Sitaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13652" title="Abstract">arXiv:2303.13652</a> (replaced) [<a href="/pdf/2303.13652" title="Download PDF">pdf</a>, <a href="/format/2303.13652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in  the Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+G">Gyeongsik Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15488" title="Abstract">arXiv:2303.15488</a> (replaced) [<a href="/pdf/2303.15488" title="Download PDF">pdf</a>, <a href="/format/2303.15488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Importance of Feature Separability in Predicting  Out-Of-Distribution Error
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Renchunzi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuzhou Cao</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bo An</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15845" title="Abstract">arXiv:2303.15845</a> (replaced) [<a href="/pdf/2303.15845" title="Download PDF">pdf</a>, <a href="/format/2303.15845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Generative Models are Provably Robust: Pointwise Guarantees  for Bayesian Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altekr%C3%BCger%2C+F">Fabian Altekr&#xfc;ger</a>, 
<a href="/search/cs?searchtype=author&query=Hagemann%2C+P">Paul Hagemann</a>, 
<a href="/search/cs?searchtype=author&query=Steidl%2C+G">Gabriele Steidl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and published in Transactions on Machine Learning Research (07/2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17175" title="Abstract">arXiv:2303.17175</a> (replaced) [<a href="/pdf/2303.17175" title="Download PDF">pdf</a>, <a href="/format/2303.17175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Scheduling of Time-Sensitive Coflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brun%2C+O">Olivier Brun</a>, 
<a href="/search/cs?searchtype=author&query=El-Azouzi%2C+R">Rachid El-Azouzi</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+Q">Quang-Trung Luu</a>, 
<a href="/search/cs?searchtype=author&query=De+Pellergrini%2C+F">Francesco De Pellergrini</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+B+J">Balakrishna J. Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Richier%2C+C">C&#xe9;dric Richier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Cloud Computing. Parts of this work have been presented at IFIP Networking 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17925" title="Abstract">arXiv:2303.17925</a> (replaced) [<a href="/pdf/2303.17925" title="Download PDF">pdf</a>, <a href="/format/2303.17925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Multilayer Perceptrons: Investigating Complex Topologies in  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boccato%2C+T">Tommaso Boccato</a>, 
<a href="/search/cs?searchtype=author&query=Ferrante%2C+M">Matteo Ferrante</a>, 
<a href="/search/cs?searchtype=author&query=Duggento%2C+A">Andrea Duggento</a>, 
<a href="/search/cs?searchtype=author&query=Toschi%2C+N">Nicola Toschi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01518" title="Abstract">arXiv:2304.01518</a> (replaced) [<a href="/pdf/2304.01518" title="Download PDF">pdf</a>, <a href="/format/2304.01518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Unimodal: Generalising Neural Processes for Multimodal  Uncertainty Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+M+C">Myong Chol Jung</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dipnall%2C+J">Joanna Dipnall</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Lan Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01999" title="Abstract">arXiv:2304.01999</a> (replaced) [<a href="/pdf/2304.01999" title="Download PDF">pdf</a>, <a href="/format/2304.01999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Evaluation of Image Synthesis with GANs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengping Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Ceyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Qingyan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yujun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bo Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 datasets and benchmarks track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03398" title="Abstract">arXiv:2304.03398</a> (replaced) [<a href="/pdf/2304.03398" title="Download PDF">pdf</a>, <a href="/format/2304.03398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Conformal Prediction for Reliable Uncertainty Quantification in  Quantum Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Park%2C+S">Sangwoo Park</a>, 
<a href="/search/quant-ph?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> added detailed discussion on quantum hardware noise
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03652" title="Abstract">arXiv:2304.03652</a> (replaced) [<a href="/pdf/2304.03652" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Accessible Toolkit for 360 VR Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Green%2C+C">Corrie Green</a>, 
<a href="/search/cs?searchtype=author&query=Farr%2C+C">Chlo&#xeb; Farr</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yang Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> for associated github repo, <a href="https://github.com/corriedotdev/vr-360-player">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04646" title="Abstract">arXiv:2304.04646</a> (replaced) [<a href="/pdf/2304.04646" title="Download PDF">pdf</a>, <a href="/format/2304.04646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECG-CL: A Comprehensive Electrocardiogram Interpretation Method Based on  Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gao%2C+H">Hongxiang Gao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xingyao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhenghua Chen</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jianqing Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chengyu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06540" title="Abstract">arXiv:2304.06540</a> (replaced) [<a href="/pdf/2304.06540" title="Download PDF">pdf</a>, <a href="/format/2304.06540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Knowledge Sharing enable Spiking Neural Network Learning from  Past and Future
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yiting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongcheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yi Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08121" title="Abstract">arXiv:2304.08121</a> (replaced) [<a href="/pdf/2304.08121" title="Download PDF">pdf</a>, <a href="/ps/2304.08121" title="Download PostScript">ps</a>, <a href="/format/2304.08121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entanglement-assisted quantum error-correcting codes from subfield  subcodes of projective Reed-Solomon codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gimenez%2C+P">Philippe Gimenez</a>, 
<a href="/search/cs?searchtype=author&query=Ruano%2C+D">Diego Ruano</a>, 
<a href="/search/cs?searchtype=author&query=San-Jos%C3%A9%2C+R">Rodrigo San-Jos&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Commutative Algebra (math.AC)

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08315" title="Abstract">arXiv:2304.08315</a> (replaced) [<a href="/pdf/2304.08315" title="Download PDF">pdf</a>, <a href="/format/2304.08315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thorny Roses: Investigating the Dual Use Dilemma in Natural Language  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaffee%2C+L">Lucie-Aim&#xe9;e Kaffee</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Talat%2C+Z">Zeerak Talat</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08928" title="Abstract">arXiv:2304.08928</a> (replaced) [<a href="/pdf/2304.08928" title="Download PDF">pdf</a>, <a href="/format/2304.08928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProGAP: Progressive Graph Neural Networks with Differential Privacy  Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sajadmanesh%2C+S">Sina Sajadmanesh</a>, 
<a href="/search/cs?searchtype=author&query=Gatica-Perez%2C+D">Daniel Gatica-Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09145" title="Abstract">arXiv:2304.09145</a> (replaced) [<a href="/pdf/2304.09145" title="Download PDF">pdf</a>, <a href="/format/2304.09145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier Suppression+: Accurate quantization of large language models by  equivalent and optimal shifting and scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiuying Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangguo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinyang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP23 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09599" title="Abstract">arXiv:2304.09599</a> (replaced) [<a href="/pdf/2304.09599" title="Download PDF">pdf</a>, <a href="/format/2304.09599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DECN: Automated Evolutionary Algorithms via Evolution Inspired Deep  Convolution Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Penghui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10778" title="Abstract">arXiv:2304.10778</a> (replaced) [<a href="/pdf/2304.10778" title="Download PDF">pdf</a>, <a href="/format/2304.10778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Code Quality of AI-Assisted Code Generation Tools: An  Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeti%C5%9Ftiren%2C+B">Burak Yeti&#x15f;tiren</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zsoy%2C+I">I&#x15f;&#x131;k &#xd6;zsoy</a>, 
<a href="/search/cs?searchtype=author&query=Ayerdem%2C+M">Miray Ayerdem</a>, 
<a href="/search/cs?searchtype=author&query=T%C3%BCz%C3%BCn%2C+E">Eray T&#xfc;z&#xfc;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11389" title="Abstract">arXiv:2304.11389</a> (replaced) [<a href="/pdf/2304.11389" title="Download PDF">pdf</a>, <a href="/format/2304.11389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-Based Language Model Surprisal Predicts Human Reading Times  Best with About Two Billion Training Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+B">Byung-Doh Oh</a>, 
<a href="/search/cs?searchtype=author&query=Schuler%2C+W">William Schuler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of the Association for Computational Linguistics: EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00118" title="Abstract">arXiv:2305.00118</a> (replaced) [<a href="/pdf/2305.00118" title="Download PDF">pdf</a>, <a href="/format/2305.00118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+K">Kent K. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Cramer%2C+M">Mackenzie Cramer</a>, 
<a href="/search/cs?searchtype=author&query=Soni%2C+S">Sandeep Soni</a>, 
<a href="/search/cs?searchtype=author&query=Bamman%2C+D">David Bamman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera-ready (16 pages, 4 figures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00374" title="Abstract">arXiv:2305.00374</a> (replaced) [<a href="/pdf/2305.00374" title="Download PDF">pdf</a>, <a href="/format/2305.00374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Adversarial Contrastive Learning via Adversarial Invariant  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xilie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01219" title="Abstract">arXiv:2305.01219</a> (replaced) [<a href="/pdf/2305.01219" title="Download PDF">pdf</a>, <a href="/format/2305.01219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jinming Wen</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junbo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to appear at the main conference of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01498" title="Abstract">arXiv:2305.01498</a> (replaced) [<a href="/pdf/2305.01498" title="Download PDF">pdf</a>, <a href="/format/2305.01498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Summarizing Multiple Documents with Conversational Structure for  Meta-Review Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Miao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+E">Eduard Hovy</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+J+H">Jey Han Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper; Accepted to EMNLP 2023; Soundness: 3, 3, 4; Excitement: 3, 4, 4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02008" title="Abstract">arXiv:2305.02008</a> (replaced) [<a href="/pdf/2305.02008" title="Download PDF">pdf</a>, <a href="/format/2305.02008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zenseact Open Dataset: A large-scale and diverse multimodal dataset for  autonomous driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alibeigi%2C+M">Mina Alibeigi</a>, 
<a href="/search/cs?searchtype=author&query=Ljungbergh%2C+W">William Ljungbergh</a>, 
<a href="/search/cs?searchtype=author&query=Tonderski%2C+A">Adam Tonderski</a>, 
<a href="/search/cs?searchtype=author&query=Hess%2C+G">Georg Hess</a>, 
<a href="/search/cs?searchtype=author&query=Lilja%2C+A">Adam Lilja</a>, 
<a href="/search/cs?searchtype=author&query=Lindstrom%2C+C">Carl Lindstrom</a>, 
<a href="/search/cs?searchtype=author&query=Motorniuk%2C+D">Daria Motorniuk</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Junsheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Widahl%2C+J">Jenny Widahl</a>, 
<a href="/search/cs?searchtype=author&query=Petersson%2C+C">Christoffer Petersson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Computer Vision (ICCV) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02105" title="Abstract">arXiv:2305.02105</a> (replaced) [<a href="/pdf/2305.02105" title="Download PDF">pdf</a>, <a href="/format/2305.02105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-RE: In-context Learning for Relation Extraction using Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Fei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhuoyuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qianying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Haiyue Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Kurohashi%2C+S">Sadao Kurohashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Main Conference (long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02176" title="Abstract">arXiv:2305.02176</a> (replaced) [<a href="/pdf/2305.02176" title="Download PDF">pdf</a>, <a href="/format/2305.02176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Being Parameter-Efficient: A Stratified Sparsely Activated  Transformer with Dynamic Capacity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Elbayad%2C+M">Maha Elbayad</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+K">Kenton Murray</a>, 
<a href="/search/cs?searchtype=author&query=Maillard%2C+J">Jean Maillard</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+V">Vedanuj Goswami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02239" title="Abstract">arXiv:2305.02239</a> (replaced) [<a href="/pdf/2305.02239" title="Download PDF">pdf</a>, <a href="/format/2305.02239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Benefits of Label-Description Training for Zero-Shot Text  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Lingyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+D">Debanjan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Gimpel%2C+K">Kevin Gimpel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the EMNLP 2023 main conference (long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02247" title="Abstract">arXiv:2305.02247</a> (replaced) [<a href="/pdf/2305.02247" title="Download PDF">pdf</a>, <a href="/ps/2305.02247" title="Download PostScript">ps</a>, <a href="/format/2305.02247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Select without Fear: Almost All Mini-Batch Schedules Generalize  Optimally
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikolakakis%2C+K+E">Konstantinos E. Nikolakakis</a>, 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>, 
<a href="/search/cs?searchtype=author&query=Kalogerias%2C+D">Dionysis Kalogerias</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02606" title="Abstract">arXiv:2305.02606</a> (replaced) [<a href="/pdf/2305.02606" title="Download PDF">pdf</a>, <a href="/format/2305.02606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re$^3$Dial: Retrieve, Reorganize and Rescale Dialogue Corpus for  Long-Turn Open-Domain Dialogue Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jiaxin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+J">Jian Guan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Coference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02996" title="Abstract">arXiv:2305.02996</a> (replaced) [<a href="/pdf/2305.02996" title="Download PDF">pdf</a>, <a href="/format/2305.02996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadav%2C+N">Nishant Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Monath%2C+N">Nicholas Monath</a>, 
<a href="/search/cs?searchtype=author&query=Zaheer%2C+M">Manzil Zaheer</a>, 
<a href="/search/cs?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03148" title="Abstract">arXiv:2305.03148</a> (replaced) [<a href="/pdf/2305.03148" title="Download PDF">pdf</a>, <a href="/format/2305.03148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMEL: Co-Designing AI Models and Embedded DRAMs for Efficient On-Device  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S+Q">Sai Qian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+T">Thierry Tambe</a>, 
<a href="/search/cs?searchtype=author&query=Cuevas%2C+N">Nestor Cuevas</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gu-Yeon Wei</a>, 
<a href="/search/cs?searchtype=author&query=Brooks%2C+D">David Brooks</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03276" title="Abstract">arXiv:2305.03276</a> (replaced) [<a href="/pdf/2305.03276" title="Download PDF">pdf</a>, <a href="/format/2305.03276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expository Text Generation: Imitate, Retrieve, Paraphrase
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balepur%2C+N">Nishant Balepur</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04175" title="Abstract">arXiv:2305.04175</a> (replaced) [<a href="/pdf/2305.04175" title="Download PDF">pdf</a>, <a href="/format/2305.04175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-Image Diffusion Models can be Easily Backdoored through  Multimodal Data Poisoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shengfang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qingni Shen</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+S">Shi Pu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuejian Fang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Carmera-ready version. To appear in ACM MM 2023. Code will be released at: <a href="https://github.com/sf-zhai/BadT2I">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05471" title="Abstract">arXiv:2305.05471</a> (replaced) [<a href="/pdf/2305.05471" title="Download PDF">pdf</a>, <a href="/format/2305.05471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Good Intentions: Reporting the Research Landscape of NLP for  Social Good
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+F">Fernando Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhijing Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+T">Tom Hope</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06360" title="Abstract">arXiv:2305.06360</a> (replaced) [<a href="/pdf/2305.06360" title="Download PDF">pdf</a>, <a href="/format/2305.06360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Landscape of Machine Unlearning: A Comprehensive Survey  and Taxonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaik%2C+T">Thanveer Shaik</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaohui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Haoran Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaofeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06851" title="Abstract">arXiv:2305.06851</a> (replaced) [<a href="/pdf/2305.06851" title="Download PDF">pdf</a>, <a href="/format/2305.06851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Gradient Algorithms Implicitly Optimize by Continuation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bolland%2C+A">Adrien Bolland</a>, 
<a href="/search/cs?searchtype=author&query=Louppe%2C+G">Gilles Louppe</a>, 
<a href="/search/cs?searchtype=author&query=Ernst%2C+D">Damien Ernst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Transactions on Machine Learning Research (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06983" title="Abstract">arXiv:2305.06983</a> (replaced) [<a href="/pdf/2305.06983" title="Download PDF">pdf</a>, <a href="/format/2305.06983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Retrieval Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhengbao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F+F">Frank F. Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Luyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhiqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi-Yu%2C+J">Jane Dwivedi-Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Callan%2C+J">Jamie Callan</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07004" title="Abstract">arXiv:2305.07004</a> (replaced) [<a href="/pdf/2305.07004" title="Download PDF">pdf</a>, <a href="/format/2305.07004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Languages Are Created Equal in LLMs: Improving Multilingual  Capability by Cross-Lingual-Thought Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haoyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+T">Tianyi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+T">Ting Song</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07100" title="Abstract">arXiv:2305.07100</a> (replaced) [<a href="/pdf/2305.07100" title="Download PDF">pdf</a>, <a href="/ps/2305.07100" title="Download PostScript">ps</a>, <a href="/format/2305.07100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> E(n) Equivariant Message Passing Simplicial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eijkelboom%2C+F">Floor Eijkelboom</a>, 
<a href="/search/cs?searchtype=author&query=Hesselink%2C+R">Rob Hesselink</a>, 
<a href="/search/cs?searchtype=author&query=Bekkers%2C+E">Erik Bekkers</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:9071-9081, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07355" title="Abstract">arXiv:2305.07355</a> (replaced) [<a href="/pdf/2305.07355" title="Download PDF">pdf</a>, <a href="/format/2305.07355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZARA: Improving Few-Shot Self-Rationalization for Small Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yen%2C+A">An-Zi Yen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Cheng-Kuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hen-Hsen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hsin-Hsi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a long paper at EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07920" title="Abstract">arXiv:2305.07920</a> (replaced) [<a href="/pdf/2305.07920" title="Download PDF">pdf</a>, <a href="/format/2305.07920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-task Paired Masking with Alignment Modeling for Medical  Vision-Language Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Ke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hanliang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jianping Fan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qingming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weidong Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07988" title="Abstract">arXiv:2305.07988</a> (replaced) [<a href="/pdf/2305.07988" title="Download PDF">pdf</a>, <a href="/format/2305.07988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstruct Before Summarize: An Efficient Two-Step Framework for  Condensing and Summarizing Meeting Transcripts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haochen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+M">Mingjie Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Z">Zhaohui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Ding Liang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09645" title="Abstract">arXiv:2305.09645</a> (replaced) [<a href="/pdf/2305.09645" title="Download PDF">pdf</a>, <a href="/format/2305.09645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StructGPT: A General Framework for Large Language Model to Reason over  Structured Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jinhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zican Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+K">Keming Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> LLM+Structured Data(KG, Table, DB); EMNLP-23 Camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09800" title="Abstract">arXiv:2305.09800</a> (replaced) [<a href="/pdf/2305.09800" title="Download PDF">pdf</a>, <a href="/format/2305.09800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirages: On Anthropomorphism in Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abercrombie%2C+G">Gavin Abercrombie</a>, 
<a href="/search/cs?searchtype=author&query=Curry%2C+A+C">Amanda Cercas Curry</a>, 
<a href="/search/cs?searchtype=author&query=Dinkar%2C+T">Tanvi Dinkar</a>, 
<a href="/search/cs?searchtype=author&query=Rieser%2C+V">Verena Rieser</a>, 
<a href="/search/cs?searchtype=author&query=Talat%2C+Z">Zeerak Talat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at EMNLP. See ACL Anthology for published version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10163" title="Abstract">arXiv:2305.10163</a> (replaced) [<a href="/pdf/2305.10163" title="Download PDF">pdf</a>, <a href="/format/2305.10163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced  Generative Pre-training Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiageng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zhaopeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10430" title="Abstract">arXiv:2305.10430</a> (replaced) [<a href="/pdf/2305.10430" title="Download PDF">pdf</a>, <a href="/format/2305.10430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Open-Loop Evaluation of End-to-End Autonomous Driving in  nuScenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+J">Jiang-Tian Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Ze Feng</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jinhao Du</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yongqiang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiang-Jiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zichang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xiaoqing Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report. Code is available
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10434" title="Abstract">arXiv:2305.10434</a> (replaced) [<a href="/pdf/2305.10434" title="Download PDF">pdf</a>, <a href="/format/2305.10434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Visualness of Text Using Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+G">Gaurav Verma</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+R+A">Ryan A. Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Tensmeyer%2C+C">Christopher Tensmeyer</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiuxiang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Nenkova%2C+A">Ani Nenkova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Main, long); 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10522" title="Abstract">arXiv:2305.10522</a> (replaced) [<a href="/pdf/2305.10522" title="Download PDF">pdf</a>, <a href="/format/2305.10522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a Doubly Reduced Model for Dynamics of Heterogeneous Mixtures of  Stiffened Gases, its Regularizations and their Implementations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zlotnik%2C+A">A. Zlotnik</a>, 
<a href="/search/math?searchtype=author&query=Lomonosov%2C+T">T. Lomonosov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10683" title="Abstract">arXiv:2305.10683</a> (replaced) [<a href="/pdf/2305.10683" title="Download PDF">pdf</a>, <a href="/format/2305.10683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Paxion: Patching Action Knowledge in Video-Language Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenhailong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Blume%2C+A">Ansel Blume</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sha Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Genglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaemin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zineng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10713" title="Abstract">arXiv:2305.10713</a> (replaced) [<a href="/pdf/2305.10713" title="Download PDF">pdf</a>, <a href="/format/2305.10713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lingfeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weiting Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Boyuan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Khashabi%2C+D">Daniel Khashabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10786" title="Abstract">arXiv:2305.10786</a> (replaced) [<a href="/pdf/2305.10786" title="Download PDF">pdf</a>, <a href="/format/2305.10786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Siqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yukun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, accepted by EMNLP 2023 short paper, the source code can be found at <a href="https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/ditto">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11141" title="Abstract">arXiv:2305.11141</a> (replaced) [<a href="/pdf/2305.11141" title="Download PDF">pdf</a>, <a href="/format/2305.11141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford Group Equivariant Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruhe%2C+D">David Ruhe</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11550" title="Abstract">arXiv:2305.11550</a> (replaced) [<a href="/pdf/2305.11550" title="Download PDF">pdf</a>, <a href="/format/2305.11550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Viewing Knowledge Transfer in Multilingual Machine Translation Through a  Representational Lens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stap%2C+D">David Stap</a>, 
<a href="/search/cs?searchtype=author&query=Niculae%2C+V">Vlad Niculae</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11747" title="Abstract">arXiv:2305.11747</a> (replaced) [<a href="/pdf/2305.11747" title="Download PDF">pdf</a>, <a href="/format/2305.11747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiaoxue Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jian-Yun Nie</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11862" title="Abstract">arXiv:2305.11862</a> (replaced) [<a href="/pdf/2305.11862" title="Download PDF">pdf</a>, <a href="/format/2305.11862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Sequence Length by Predicting Edit Operations with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12152" title="Abstract">arXiv:2305.12152</a> (replaced) [<a href="/pdf/2305.12152" title="Download PDF">pdf</a>, <a href="/format/2305.12152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Automated Topic Model Evaluation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stammbach%2C+D">Dominik Stammbach</a>, 
<a href="/search/cs?searchtype=author&query=Zouhar%2C+V">Vil&#xe9;m Zouhar</a>, 
<a href="/search/cs?searchtype=author&query=Hoyle%2C+A">Alexander Hoyle</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Forthcoming in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12421" title="Abstract">arXiv:2305.12421</a> (replaced) [<a href="/pdf/2305.12421" title="Download PDF">pdf</a>, <a href="/format/2305.12421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Open-QA Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cunxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sirui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yuanhao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bowen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhikun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiangkun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Neurips-2023 Datasets and Benchmarks track; 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12517" title="Abstract">arXiv:2305.12517</a> (replaced) [<a href="/pdf/2305.12517" title="Download PDF">pdf</a>, <a href="/format/2305.12517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieving Texts based on Abstract Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>, 
<a href="/search/cs?searchtype=author&query=Pyatkin%2C+V">Valentina Pyatkin</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A+D">Amir DN Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Manevich%2C+A">Avshalom Manevich</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12576" title="Abstract">arXiv:2305.12576</a> (replaced) [<a href="/pdf/2305.12576" title="Download PDF">pdf</a>, <a href="/format/2305.12576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Few-shot Classification with Instruction-Finetuned Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aly%2C+R">Rami Aly</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xingjian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kaixiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aston Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A+G">Andrew Gordon Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12710" title="Abstract">arXiv:2305.12710</a> (replaced) [<a href="/pdf/2305.12710" title="Download PDF">pdf</a>, <a href="/format/2305.12710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Labels: Empowering Human Annotators with Natural Language  Explanations through a Novel Active-Learning Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jindal%2C+I">Ishan Jindal</a>, 
<a href="/search/cs?searchtype=author&query=Popa%2C+L">Lucian Popa</a>, 
<a href="/search/cs?searchtype=author&query=Katsis%2C+Y">Yannis Katsis</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sayan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lihong He</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuxuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Shashank Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hendler%2C+J">James Hendler</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12871" title="Abstract">arXiv:2305.12871</a> (replaced) [<a href="/pdf/2305.12871" title="Download PDF">pdf</a>, <a href="/format/2305.12871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMGP: a Mesh Morphing Gaussian Process-based machine learning method for  regression of physical problems under non-parameterized geometrical  variability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casenave%2C+F">Fabien Casenave</a>, 
<a href="/search/cs?searchtype=author&query=Staber%2C+B">Brian Staber</a>, 
<a href="/search/cs?searchtype=author&query=Roynard%2C+X">Xavier Roynard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13085" title="Abstract">arXiv:2305.13085</a> (replaced) [<a href="/pdf/2305.13085" title="Download PDF">pdf</a>, <a href="/format/2305.13085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposed Prompting for Machine Translation Between Related Languages  using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puduppully%2C+R">Ratish Puduppully</a>, 
<a href="/search/cs?searchtype=author&query=Kunchukuttan%2C+A">Anoop Kunchukuttan</a>, 
<a href="/search/cs?searchtype=author&query=Dabre%2C+R">Raj Dabre</a>, 
<a href="/search/cs?searchtype=author&query=Aw%2C+A+T">Ai Ti Aw</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Main, Long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13186" title="Abstract">arXiv:2305.13186</a> (replaced) [<a href="/pdf/2305.13186" title="Download PDF">pdf</a>, <a href="/format/2305.13186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim  Verification on Scientific Tables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xinyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (main conference, long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13264" title="Abstract">arXiv:2305.13264</a> (replaced) [<a href="/pdf/2305.13264" title="Download PDF">pdf</a>, <a href="/format/2305.13264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting is not a substitute for probability measurements in large  language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jennifer Hu</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+R">Roger Levy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13406" title="Abstract">arXiv:2305.13406</a> (replaced) [<a href="/pdf/2305.13406" title="Download PDF">pdf</a>, <a href="/format/2305.13406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+W">William Held</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13469" title="Abstract">arXiv:2305.13469</a> (replaced) [<a href="/pdf/2305.13469" title="Download PDF">pdf</a>, <a href="/format/2305.13469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAILEX: Email Event and Argument Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Saurabh Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gaurav Singh</a>, 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+S">Shou Matsumoto</a>, 
<a href="/search/cs?searchtype=author&query=Raz%2C+A">Ali Raz</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+P">Paulo Costa</a>, 
<a href="/search/cs?searchtype=author&query=Poore%2C+J">Joshua Poore</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Ziyu Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13477" title="Abstract">arXiv:2305.13477</a> (replaced) [<a href="/pdf/2305.13477" title="Download PDF">pdf</a>, <a href="/format/2305.13477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look-back Decoding for Open-Ended Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunting Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuezhe Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13547" title="Abstract">arXiv:2305.13547</a> (replaced) [<a href="/pdf/2305.13547" title="Download PDF">pdf</a>, <a href="/format/2305.13547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot  Text Classification Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haoqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qihuang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhiliang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13631" title="Abstract">arXiv:2305.13631</a> (replaced) [<a href="/pdf/2305.13631" title="Download PDF">pdf</a>, <a href="/format/2305.13631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDIS: Entity-Driven Image Search over Multimodal Web Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weixi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tsu-jui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13735" title="Abstract">arXiv:2305.13735</a> (replaced) [<a href="/pdf/2305.13735" title="Download PDF">pdf</a>, <a href="/format/2305.13735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Large Language Models through Synthetic Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Sanghwan Bae</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jamin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+S">Soyoung Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+D">Donghyun Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13971" title="Abstract">arXiv:2305.13971</a> (replaced) [<a href="/pdf/2305.13971" title="Download PDF">pdf</a>, <a href="/format/2305.13971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+S">Saibo Geng</a>, 
<a href="/search/cs?searchtype=author&query=Josifosky%2C+M">Martin Josifosky</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13993" title="Abstract">arXiv:2305.13993</a> (replaced) [<a href="/pdf/2305.13993" title="Download PDF">pdf</a>, <a href="/format/2305.13993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Condensing Multilingual Knowledge with Lightweight Language-Specific  Modules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weiting Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+S">Shuyue Stella Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunmo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>, 
<a href="/search/cs?searchtype=author&query=Koehn%2C+P">Philipp Koehn</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+K">Kenton Murray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the main conference of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13999" title="Abstract">arXiv:2305.13999</a> (replaced) [<a href="/pdf/2305.13999" title="Download PDF">pdf</a>, <a href="/format/2305.13999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards A Unified View of Sparse Feed-Forward Network in Pretraining  Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L+Z">Leo Z. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dettmers%2C+T">Tim Dettmers</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X+V">Xi Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Stoyanov%2C+V">Veselin Stoyanov</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14032" title="Abstract">arXiv:2305.14032</a> (replaced) [<a href="/pdf/2305.14032" title="Download PDF">pdf</a>, <a href="/format/2305.14032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on  Respiratory Sound Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bae%2C+S">Sangmin Bae</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">June-Woo Kim</a>, 
<a href="/search/eess?searchtype=author&query=Cho%2C+W">Won-Yang Cho</a>, 
<a href="/search/eess?searchtype=author&query=Baek%2C+H">Hyerim Baek</a>, 
<a href="/search/eess?searchtype=author&query=Son%2C+S">Soyoun Son</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+B">Byungjo Lee</a>, 
<a href="/search/eess?searchtype=author&query=Ha%2C+C">Changwan Ha</a>, 
<a href="/search/eess?searchtype=author&query=Tae%2C+K">Kyongpil Tae</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+S">Sungnyun Kim</a>, 
<a href="/search/eess?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> INTERSPEECH 2023, Code URL: <a href="https://github.com/raymin0223/patch-mix_contrastive_learning">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14105" title="Abstract">arXiv:2305.14105</a> (replaced) [<a href="/pdf/2305.14105" title="Download PDF">pdf</a>, <a href="/format/2305.14105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CTQScorer: Combining Multiple Features for In-context Example Selection  for Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aswanth Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Puduppully%2C+R">Ratish Puduppully</a>, 
<a href="/search/cs?searchtype=author&query=Dabre%2C+R">Raj Dabre</a>, 
<a href="/search/cs?searchtype=author&query=Kunchukuttan%2C+A">Anoop Kunchukuttan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14214" title="Abstract">arXiv:2305.14214</a> (replaced) [<a href="/pdf/2305.14214" title="Download PDF">pdf</a>, <a href="/format/2305.14214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CompoundPiece: Evaluating and Improving Decompounding Performance of  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Minixhofer%2C+B">Benjamin Minixhofer</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+J">Jonas Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14235" title="Abstract">arXiv:2305.14235</a> (replaced) [<a href="/pdf/2305.14235" title="Download PDF">pdf</a>, <a href="/format/2305.14235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Large Language Models Are Not (Yet) Code-Switchers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruochen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cahyawijaya%2C+S">Samuel Cahyawijaya</a>, 
<a href="/search/cs?searchtype=author&query=Cruz%2C+J+C+B">Jan Christian Blaise Cruz</a>, 
<a href="/search/cs?searchtype=author&query=Winata%2C+G+I">Genta Indra Winata</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14257" title="Abstract">arXiv:2305.14257</a> (replaced) [<a href="/pdf/2305.14257" title="Download PDF">pdf</a>, <a href="/format/2305.14257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Prompting Assists Large Language Model on Web Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A">Abishek Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+R">Robert Lo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F+F">Frank F. Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuyan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings; Natural Language Reasoning and Structured Explanations Workshop at ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14283" title="Abstract">arXiv:2305.14283</a> (replaced) [<a href="/pdf/2305.14283" title="Download PDF">pdf</a>, <a href="/format/2305.14283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query Rewriting for Retrieval-Augmented Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinbei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14288" title="Abstract">arXiv:2305.14288</a> (replaced) [<a href="/pdf/2305.14288" title="Download PDF">pdf</a>, <a href="/format/2305.14288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-powered Data Augmentation for Enhanced Cross-lingual Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Whitehouse%2C+C">Chenxi Whitehouse</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+M">Monojit Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14326" title="Abstract">arXiv:2305.14326</a> (replaced) [<a href="/pdf/2305.14326" title="Download PDF">pdf</a>, <a href="/format/2305.14326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TalkUp: Paving the Way for Understanding Empowering Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Njoo%2C+L">Lucille Njoo</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C+Y">Chan Young Park</a>, 
<a href="/search/cs?searchtype=author&query=Stappart%2C+O">Octavia Stappart</a>, 
<a href="/search/cs?searchtype=author&query=Thielk%2C+M">Marvin Thielk</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Y">Yi Chu</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14333" title="Abstract">arXiv:2305.14333</a> (replaced) [<a href="/pdf/2305.14333" title="Download PDF">pdf</a>, <a href="/format/2305.14333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Model Selection with Large Language Models for Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J+X">James Xu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+M+Q">Michael Qizhe Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14458" title="Abstract">arXiv:2305.14458</a> (replaced) [<a href="/pdf/2305.14458" title="Download PDF">pdf</a>, <a href="/format/2305.14458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dancing Between Success and Failure: Edit-level Simplification  Evaluation using SALSA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heineman%2C+D">David Heineman</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Y">Yao Dou</a>, 
<a href="/search/cs?searchtype=author&query=Maddela%2C+M">Mounica Maddela</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14492" title="Abstract">arXiv:2305.14492</a> (replaced) [<a href="/pdf/2305.14492" title="Download PDF">pdf</a>, <a href="/format/2305.14492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sociocultural Norm Similarities and Differences via Situational  Alignment and Explainable Textual Entailment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=CH-Wang%2C+S">Sky CH-Wang</a>, 
<a href="/search/cs?searchtype=author&query=Saakyan%2C+A">Arkadiy Saakyan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+O">Oliver Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Muresan%2C+S">Smaranda Muresan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14499" title="Abstract">arXiv:2305.14499</a> (replaced) [<a href="/pdf/2305.14499" title="Download PDF">pdf</a>, <a href="/format/2305.14499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive  Decoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soares%2C+L+B">Livio Baldini Soares</a>, 
<a href="/search/cs?searchtype=author&query=Gillick%2C+D">Daniel Gillick</a>, 
<a href="/search/cs?searchtype=author&query=Cole%2C+J+R">Jeremy R. Cole</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowski%2C+T">Tom Kwiatkowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14536" title="Abstract">arXiv:2305.14536</a> (replaced) [<a href="/pdf/2305.14536" title="Download PDF">pdf</a>, <a href="/format/2305.14536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties  Grounded in Math Reasoning Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macina%2C+J">Jakub Macina</a>, 
<a href="/search/cs?searchtype=author&query=Daheim%2C+N">Nico Daheim</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+P">Sankalan Pal Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+T">Tanmay Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Kapur%2C+M">Manu Kapur</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Jakub Macina, Nico Daheim, and Sankalan Pal Chowdhury contributed equally to this work. Accepted at EMNLP2023 Findings. Code and dataset available: <a href="https://github.com/eth-nlped/mathdial">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14552" title="Abstract">arXiv:2305.14552</a> (replaced) [<a href="/pdf/2305.14552" title="Download PDF">pdf</a>, <a href="/format/2305.14552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sources of Hallucination by Large Language Models on Inference Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McKenna%2C+N">Nick McKenna</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Liang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+M+J">Mohammad Javad Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+M">Mark Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Steedman%2C+M">Mark Steedman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14576" title="Abstract">arXiv:2305.14576</a> (replaced) [<a href="/pdf/2305.14576" title="Download PDF">pdf</a>, <a href="/format/2305.14576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Language Model Tuning with Active Learning in  Low-Resource Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Juki%C4%87%2C+J">Josip Juki&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0najder%2C+J">Jan &#x160;najder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14617" title="Abstract">arXiv:2305.14617</a> (replaced) [<a href="/pdf/2305.14617" title="Download PDF">pdf</a>, <a href="/format/2305.14617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COMET-M: Reasoning about Multiple Events in Complex Sentences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S">Sahithya Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+R">Raymond Ng</a>, 
<a href="/search/cs?searchtype=author&query=Shwartz%2C+V">Vered Shwartz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14663" title="Abstract">arXiv:2305.14663</a> (replaced) [<a href="/pdf/2305.14663" title="Download PDF">pdf</a>, <a href="/format/2305.14663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Are What You Annotate: Towards Better Models through Annotator  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X+F">Xinliang Frederick Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Winston Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14711" title="Abstract">arXiv:2305.14711</a> (replaced) [<a href="/pdf/2305.14711" title="Download PDF">pdf</a>, <a href="/format/2305.14711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gender Biases in Automatic Evaluation Metrics for Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Haoyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zi-Yi Dou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianlu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14722" title="Abstract">arXiv:2305.14722</a> (replaced) [<a href="/pdf/2305.14722" title="Download PDF">pdf</a>, <a href="/format/2305.14722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Cross-resolution Remote Sensing Image Change Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haotian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Keyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chenyao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Song Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhengxia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenwei Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 11 figures. Accepted article by IEEE TGRS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14735" title="Abstract">arXiv:2305.14735</a> (replaced) [<a href="/pdf/2305.14735" title="Download PDF">pdf</a>, <a href="/format/2305.14735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Centering the Margins: Outlier-Based Identification of Harmed  Populations in Toxicity Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+V">Vyoma Raman</a>, 
<a href="/search/cs?searchtype=author&query=Fleisig%2C+E">Eve Fleisig</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14740" title="Abstract">arXiv:2305.14740</a> (replaced) [<a href="/pdf/2305.14740" title="Download PDF">pdf</a>, <a href="/format/2305.14740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECHo: A Visio-Linguistic Dataset for Event Causality Inference via  Human-Centric Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanzhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023. 10 pages, 6 figures, 5 tables (22 pages, 8 figures, 15 tables including references and appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14755" title="Abstract">arXiv:2305.14755</a> (replaced) [<a href="/pdf/2305.14755" title="Download PDF">pdf</a>, <a href="/format/2305.14755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Take This Out of Context! On the Need for Contextual Models and  Evaluations for Stylistic Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yerukola%2C+A">Akhila Yerukola</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+E">Elizabeth Clark</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> emnlp 2023 main camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14760" title="Abstract">arXiv:2305.14760</a> (replaced) [<a href="/pdf/2305.14760" title="Download PDF">pdf</a>, <a href="/format/2305.14760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net  Estimation and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Shoujie Tong</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Heming Xia</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Damai Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Binghuai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yunbo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings. Camera-ready version. Co-first authors with equal contributions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14794" title="Abstract">arXiv:2305.14794</a> (replaced) [<a href="/pdf/2305.14794" title="Download PDF">pdf</a>, <a href="/format/2305.14794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak  Supervision for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chengyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14889" title="Abstract">arXiv:2305.14889</a> (replaced) [<a href="/pdf/2305.14889" title="Download PDF">pdf</a>, <a href="/format/2305.14889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation  Metrics using Measurement Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Ziang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Susu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+V">Vivian Lai</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q+V">Q. Vera Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14926" title="Abstract">arXiv:2305.14926</a> (replaced) [<a href="/pdf/2305.14926" title="Download PDF">pdf</a>, <a href="/format/2305.14926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Self-Adaptive Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xingchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoxi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Nakhost%2C+H">Hootan Nakhost</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Eisenschlos%2C+J+M">Julian Martin Eisenschlos</a>, 
<a href="/search/cs?searchtype=author&query=Arik%2C+S+O">Sercan O. Arik</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+T">Tomas Pfister</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Main). 10 pages, 5 figures, 4 tables (26 pages, 9 figures and 13 tables including references and appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14976" title="Abstract">arXiv:2305.14976</a> (replaced) [<a href="/pdf/2305.14976" title="Download PDF">pdf</a>, <a href="/format/2305.14976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khondaker%2C+M+T+I">Md Tawkat Islam Khondaker</a>, 
<a href="/search/cs?searchtype=author&query=Waheed%2C+A">Abdul Waheed</a>, 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14992" title="Abstract">arXiv:2305.14992</a> (replaced) [<a href="/pdf/2305.14992" title="Download PDF">pdf</a>, <a href="/format/2305.14992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning with Language Model is Planning with World Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+S">Shibo Hao</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haodi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J+J">Joshua Jiahua Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D+Z">Daisy Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiting Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Code is available at <a href="https://github.com/Ber666/llm-reasoners">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15017" title="Abstract">arXiv:2305.15017</a> (replaced) [<a href="/pdf/2305.15017" title="Download PDF">pdf</a>, <a href="/format/2305.15017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through  Interaction with Symbolic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadl%C4%8D%C3%ADk%2C+M">Marek Kadl&#x10d;&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0tef%C3%A1nik%2C+M">Michal &#x160;tef&#xe1;nik</a>, 
<a href="/search/cs?searchtype=author&query=Sotol%C3%A1%C5%99%2C+O">Ond&#x159;ej Sotol&#xe1;&#x159;</a>, 
<a href="/search/cs?searchtype=author&query=Martinek%2C+V">Vlastimil Martinek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in EMNLP 2023: Main track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15027" title="Abstract">arXiv:2305.15027</a> (replaced) [<a href="/pdf/2305.15027" title="Download PDF">pdf</a>, <a href="/format/2305.15027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Rigorous Link between Deep Ensembles and (Variational) Bayesian  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wild%2C+V+D">Veit David Wild</a>, 
<a href="/search/stat?searchtype=author&query=Ghalebikesabi%2C+S">Sahra Ghalebikesabi</a>, 
<a href="/search/stat?searchtype=author&query=Sejdinovic%2C+D">Dino Sejdinovic</a>, 
<a href="/search/stat?searchtype=author&query=Knoblauch%2C+J">Jeremias Knoblauch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15028" title="Abstract">arXiv:2305.15028</a> (replaced) [<a href="/pdf/2305.15028" title="Download PDF">pdf</a>, <a href="/format/2305.15028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000  ImageNet Categories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Heming Xia</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Ziwei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings (Long Paper), camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15035" title="Abstract">arXiv:2305.15035</a> (replaced) [<a href="/pdf/2305.15035" title="Download PDF">pdf</a>, <a href="/format/2305.15035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-ICL: Zero-Shot In-Context Learning with Self-Generated  Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Cheng-Kuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun-Nung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hsin-Hsi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a long paper at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15038" title="Abstract">arXiv:2305.15038</a> (replaced) [<a href="/pdf/2305.15038" title="Download PDF">pdf</a>, <a href="/format/2305.15038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is GPT-4 a Good Data Analyst?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Liying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15064" title="Abstract">arXiv:2305.15064</a> (replaced) [<a href="/pdf/2305.15064" title="Download PDF">pdf</a>, <a href="/format/2305.15064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Siqi Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15074" title="Abstract">arXiv:2305.15074</a> (replaced) [<a href="/pdf/2305.15074" title="Download PDF">pdf</a>, <a href="/format/2305.15074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+D">Daman Arora</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+H+G">Himanshu Gaurav Singh</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15076" title="Abstract">arXiv:2305.15076</a> (replaced) [<a href="/pdf/2305.15076" title="Download PDF">pdf</a>, <a href="/format/2305.15076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning Online Adaptation of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+N">Nathan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C+D">Christopher D. Manning</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15217" title="Abstract">arXiv:2305.15217</a> (replaced) [<a href="/pdf/2305.15217" title="Download PDF">pdf</a>, <a href="/format/2305.15217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L-CAD: Language-based Colorization with Any-level Descriptions using  Diffusion Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Zheng Chang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+S">Shuchen Weng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Si Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Boxin Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15287" title="Abstract">arXiv:2305.15287</a> (replaced) [<a href="/pdf/2305.15287" title="Download PDF">pdf</a>, <a href="/format/2305.15287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Crucial Role of Normalization in Sharpness-Aware Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+K">Kwangjun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Sra%2C+S">Suvrit Sra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, Published in 37th Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15294" title="Abstract">arXiv:2305.15294</a> (replaced) [<a href="/pdf/2305.15294" title="Download PDF">pdf</a>, <a href="/format/2305.15294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Retrieval-Augmented Large Language Models with Iterative  Retrieval-Generation Synergy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15720" title="Abstract">arXiv:2305.15720</a> (replaced) [<a href="/pdf/2305.15720" title="Download PDF">pdf</a>, <a href="/format/2305.15720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Ranking Context of Dense Retrieval Methods through  Reciprocal Nearest Neighbors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zerveas%2C+G">George Zerveas</a>, 
<a href="/search/cs?searchtype=author&query=Rekabsaz%2C+N">Navid Rekabsaz</a>, 
<a href="/search/cs?searchtype=author&query=Eickhoff%2C+C">Carsten Eickhoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16340" title="Abstract">arXiv:2305.16340</a> (replaced) [<a href="/pdf/2305.16340" title="Download PDF">pdf</a>, <a href="/format/2305.16340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yinghan Long</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+S">Sayeed Shafayet Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16426" title="Abstract">arXiv:2305.16426</a> (replaced) [<a href="/pdf/2305.16426" title="Download PDF">pdf</a>, <a href="/format/2305.16426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained  language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorge%2C+I">Isabelle Lorge</a>, 
<a href="/search/cs?searchtype=author&query=Pierrehumbert%2C+J">Janet Pierrehumbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in BlackBoxNLP workshop, EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16822" title="Abstract">arXiv:2305.16822</a> (replaced) [<a href="/pdf/2305.16822" title="Download PDF">pdf</a>, <a href="/format/2305.16822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Certification for Trustworthy Machine Learning-Based  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anisetti%2C+M">Marco Anisetti</a>, 
<a href="/search/cs?searchtype=author&query=Ardagna%2C+C+A">Claudio A. Ardagna</a>, 
<a href="/search/cs?searchtype=author&query=Bena%2C+N">Nicola Bena</a>, 
<a href="/search/cs?searchtype=author&query=Damiani%2C+E">Ernesto Damiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE Internet Computing; 6 pages, 1 figure, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17262" title="Abstract">arXiv:2305.17262</a> (replaced) [<a href="/pdf/2305.17262" title="Download PDF">pdf</a>, <a href="/format/2305.17262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Im-Promptu: In-Context Composition from Image Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dedhia%2C+B">Bhishma Dedhia</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Michael Chang</a>, 
<a href="/search/cs?searchtype=author&query=Snell%2C+J+C">Jake C. Snell</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+N+K">Niraj K. Jha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18287" title="Abstract">arXiv:2305.18287</a> (replaced) [<a href="/pdf/2305.18287" title="Download PDF">pdf</a>, <a href="/format/2305.18287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and  Unlabeled Image Collections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirza%2C+M+J">M. Jehanzeb Mirza</a>, 
<a href="/search/cs?searchtype=author&query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Kozinski%2C+M">Mateusz Kozinski</a>, 
<a href="/search/cs?searchtype=author&query=Possegger%2C+H">Horst Possegger</a>, 
<a href="/search/cs?searchtype=author&query=Feris%2C+R">Rogerio Feris</a>, 
<a href="/search/cs?searchtype=author&query=Bischof%2C+H">Horst Bischof</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Camera Ready) - Project Page: <a href="https://jmiemirza.github.io/LaFTer/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18378" title="Abstract">arXiv:2305.18378</a> (replaced) [<a href="/pdf/2305.18378" title="Download PDF">pdf</a>, <a href="/format/2305.18378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentanglement via Latent Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+K">Kyle Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Dorrell%2C+W">Will Dorrell</a>, 
<a href="/search/cs?searchtype=author&query=Whittington%2C+J+C+R">James C. R. Whittington</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera-ready. 26 pages, 15 figures. Code available at <a href="https://github.com/kylehkhsu/latent_quantization">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18396" title="Abstract">arXiv:2305.18396</a> (replaced) [<a href="/pdf/2305.18396" title="Download PDF">pdf</a>, <a href="/format/2305.18396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuanqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuotao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18647" title="Abstract">arXiv:2305.18647</a> (replaced) [<a href="/pdf/2305.18647" title="Download PDF">pdf</a>, <a href="/ps/2305.18647" title="Download PostScript">ps</a>, <a href="/format/2305.18647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Alternate Proof of Near-Optimal Light Spanners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodwin%2C+G">Greg Bodwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SOSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18878" title="Abstract">arXiv:2305.18878</a> (replaced) [<a href="/pdf/2305.18878" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BPF Algorithms for Multiple Source-Translation Computed Tomography  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhisheng Wang</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haijun Yu</a> (3), 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yixing Huang</a> (4), 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shunli Wang</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Song Ni</a> (3), 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongfeng Li</a> (3), 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fenglin Liu</a> (3), 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Junning Cui</a> (1 and 2) ((1) Center of Ultra-Precision Optoelectronic Instrument Engineering, Harbin Institute of Technology, Harbin 150080, China, (2) Key Lab of Ultra-Precision Intelligent Instrumentation (Harbin Institute of Technology), Ministry of Industry and Information Technology, Harbin 150080, China, (3) Key Laboratory of Optoelectronic Technology and Systems, Ministry of Education, Chongqing University, Chongqing 400044, China, (4) Oncology, University Hospital Erlangen, Friedrich-Alexander-University Erlangen-Nuremberg, 91054 Erlangen, Germany)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19523" title="Abstract">arXiv:2305.19523</a> (replaced) [<a href="/pdf/2305.19523" title="Download PDF">pdf</a>, <a href="/format/2305.19523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Explanations: LLM-to-LM Interpreter for Enhanced  Text-Attributed Graph Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaoxin He</a>, 
<a href="/search/cs?searchtype=author&query=Bresson%2C+X">Xavier Bresson</a>, 
<a href="/search/cs?searchtype=author&query=Laurent%2C+T">Thomas Laurent</a>, 
<a href="/search/cs?searchtype=author&query=Perold%2C+A">Adam Perold</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19542" title="Abstract">arXiv:2305.19542</a> (replaced) [<a href="/pdf/2305.19542" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shallow Depth Factoring Based on Quantum Feasibility Labeling and  Variational Quantum Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Tutul%2C+I+K">Imran Khan Tutul</a>, 
<a href="/search/quant-ph?searchtype=author&query=Karimi%2C+S">Sara Karimi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Soltaninia%2C+M">Mohammadreza Soltaninia</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhan%2C+J">Junpeng Zhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Optimization and Control (math.OC); Quantum Algebra (math.QA)

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01112" title="Abstract">arXiv:2306.01112</a> (replaced) [<a href="/pdf/2306.01112" title="Download PDF">pdf</a>, <a href="/format/2306.01112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving day-ahead Solar Irradiance Time Series Forecasting by  Leveraging Spatio-Temporal Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boussif%2C+O">Oussama Boussif</a>, 
<a href="/search/cs?searchtype=author&query=Boukachab%2C+G">Ghait Boukachab</a>, 
<a href="/search/cs?searchtype=author&query=Assouline%2C+D">Dan Assouline</a>, 
<a href="/search/cs?searchtype=author&query=Massaroli%2C+S">Stefano Massaroli</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tianle Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Benabbou%2C+L">Loubna Benabbou</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01567" title="Abstract">arXiv:2306.01567</a> (replaced) [<a href="/pdf/2306.01567" title="Download PDF">pdf</a>, <a href="/format/2306.01567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Anything in High Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+L">Lei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Mingqiao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Danelljan%2C+M">Martin Danelljan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tai%2C+Y">Yu-Wing Tai</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chi-Keung Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fisher Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. We propose HQ-SAM to upgrade SAM for high-quality zero-shot segmentation. Github: <a href="https://github.com/SysCV/SAM-HQ">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01678" title="Abstract">arXiv:2306.01678</a> (replaced) [<a href="/pdf/2306.01678" title="Download PDF">pdf</a>, <a href="/format/2306.01678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-dimensional Tverberg Partitions Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Har-Peled%2C+S">Sariel Har-Peled</a>, 
<a href="/search/cs?searchtype=author&query=Robson%2C+E+W">Eliot W. Robson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02371" title="Abstract">arXiv:2306.02371</a> (replaced) [<a href="/pdf/2306.02371" title="Download PDF">pdf</a>, <a href="/format/2306.02371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I^3 Retriever: Incorporating Implicit Interaction in Pre-trained  Language Models for Passage Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qian Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shaoping Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02664" title="Abstract">arXiv:2306.02664</a> (replaced) [<a href="/pdf/2306.02664" title="Download PDF">pdf</a>, <a href="/format/2306.02664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-free Graph Condensation: From Large-scale Graphs to Condensed  Graph-free Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q+V+H">Quoc Viet Hung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xingquan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03030" title="Abstract">arXiv:2306.03030</a> (replaced) [<a href="/pdf/2306.03030" title="Download PDF">pdf</a>, <a href="/format/2306.03030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese  Medical Exam Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peilin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yining Hua</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+D">Dading Chong</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhongyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Andrew Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Helin Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chenyu You</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhenhua Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M+L">Michael Lingzhi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03543" title="Abstract">arXiv:2306.03543</a> (replaced) [<a href="/pdf/2306.03543" title="Download PDF">pdf</a>, <a href="/format/2306.03543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Select Which Active Learning Strategy is Best Suited for Your  Specific Problem and Budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hacohen%2C+G">Guy Hacohen</a>, 
<a href="/search/cs?searchtype=author&query=Weinshall%2C+D">Daphna Weinshall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04072" title="Abstract">arXiv:2306.04072</a> (replaced) [<a href="/pdf/2306.04072" title="Download PDF">pdf</a>, <a href="/format/2306.04072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Simple, High Quality Out-of-Distribution Detection with L2  Normalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haas%2C+J">Jarrod Haas</a>, 
<a href="/search/cs?searchtype=author&query=Yolland%2C+W">William Yolland</a>, 
<a href="/search/cs?searchtype=author&query=Rabus%2C+B">Bernhard Rabus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04086" title="Abstract">arXiv:2306.04086</a> (replaced) [<a href="/pdf/2306.04086" title="Download PDF">pdf</a>, <a href="/format/2306.04086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TEC-Net: Vision Transformer Embrace Convolutional Neural Networks for  Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lei%2C+T">Tao Lei</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+R">Rui Sun</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Weichuan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wan%2C+Y">Yong Wan</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+Y">Yong Xia</a>, 
<a href="/search/eess?searchtype=author&query=Nandi%2C+A+K">Asoke K. Nandi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2306.03373">arXiv:2306.03373</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04985" title="Abstract">arXiv:2306.04985</a> (replaced) [<a href="/pdf/2306.04985" title="Download PDF">pdf</a>, <a href="/format/2306.04985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Probability Partitions: Calibrating Neural Networks with Semantic  Aware Grouping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jia-Qi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+D">De-Chuan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Le Gan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'23. <a href="https://github.com/ThyrixYang/group_calibration">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05042" title="Abstract">arXiv:2306.05042</a> (replaced) [<a href="/pdf/2306.05042" title="Download PDF">pdf</a>, <a href="/ps/2306.05042" title="Download PostScript">ps</a>, <a href="/format/2306.05042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Quantum Surrogate Models on Scarce and Noisy Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Stein%2C+J">Jonas Stein</a>, 
<a href="/search/quant-ph?searchtype=author&query=Poppel%2C+M">Michael Poppel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Adamczyk%2C+P">Philip Adamczyk</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fabry%2C+R">Ramona Fabry</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+Z">Zixin Wu</a>, 
<a href="/search/quant-ph?searchtype=author&query=K%C3%B6lle%2C+M">Michael K&#xf6;lle</a>, 
<a href="/search/quant-ph?searchtype=author&query=N%C3%BC%C3%9Flein%2C+J">Jonas N&#xfc;&#xdf;lein</a>, 
<a href="/search/quant-ph?searchtype=author&query=Schuman%2C+D">Dani&#xeb;lle Schuman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Altmann%2C+P">Philipp Altmann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ehmer%2C+T">Thomas Ehmer</a>, 
<a href="/search/quant-ph?searchtype=author&query=Narasimhan%2C+V">Vijay Narasimhan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Linnhoff-Popien%2C+C">Claudia Linnhoff-Popien</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05581" title="Abstract">arXiv:2306.05581</a> (replaced) [<a href="/pdf/2306.05581" title="Download PDF">pdf</a>, <a href="/format/2306.05581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-aware Urban Air Mobility Network Design with Overflow Redundancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wei%2C+Q">Qinshuang Wei</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Z">Zhenyu Gao</a>, 
<a href="/search/eess?searchtype=author&query=Clarke%2C+J">John-Paul Clarke</a>, 
<a href="/search/eess?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06755" title="Abstract">arXiv:2306.06755</a> (replaced) [<a href="/pdf/2306.06755" title="Download PDF">pdf</a>, <a href="/format/2306.06755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention, Compilation, and Solver-based Symbolic Analysis are All You  Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jana%2C+P">Prithwish Jana</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+P">Piyush Jha</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+H">Haoyang Ju</a>, 
<a href="/search/cs?searchtype=author&query=Kishore%2C+G">Gautham Kishore</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+A">Aryan Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+V">Vijay Ganesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06808" title="Abstract">arXiv:2306.06808</a> (replaced) [<a href="/pdf/2306.06808" title="Download PDF">pdf</a>, <a href="/format/2306.06808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Reinforcement Learning Guided by Signal Temporal Logic  Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Z">Ziyan An</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Songyang Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhili Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mangharam%2C+R">Rahul Mangharam</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Meiyi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+F">Fei Miao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06815" title="Abstract">arXiv:2306.06815</a> (replaced) [<a href="/pdf/2306.06815" title="Download PDF">pdf</a>, <a href="/format/2306.06815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jiaqi Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mengxin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+T">Ting Hua</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yilin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yepeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Boloni%2C+L">Ladislau Boloni</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Q">Qian Lou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09424" title="Abstract">arXiv:2306.09424</a> (replaced) [<a href="/pdf/2306.09424" title="Download PDF">pdf</a>, <a href="/format/2306.09424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSL4EO-L: Datasets and Foundation Models for Landsat Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stewart%2C+A+J">Adam J. Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Lehmann%2C+N">Nils Lehmann</a>, 
<a href="/search/cs?searchtype=author&query=Corley%2C+I+A">Isaac A. Corley</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi-Chia Chang</a>, 
<a href="/search/cs?searchtype=author&query=Braham%2C+N+A+A">Nassim Ait Ali Braham</a>, 
<a href="/search/cs?searchtype=author&query=Sehgal%2C+S">Shradha Sehgal</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C">Caleb Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Arindam Banerjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10050" title="Abstract">arXiv:2306.10050</a> (replaced) [<a href="/pdf/2306.10050" title="Download PDF">pdf</a>, <a href="/format/2306.10050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpolating Item and User Fairness in Multi-Sided Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J+C+N">Jason Cheuk Nam Liang</a>, 
<a href="/search/cs?searchtype=author&query=Golrezaei%2C+N">Negin Golrezaei</a>, 
<a href="/search/cs?searchtype=author&query=Bouneffouf%2C+D">Djallel Bouneffouf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10224" title="Abstract">arXiv:2306.10224</a> (replaced) [<a href="/pdf/2306.10224" title="Download PDF">pdf</a>, <a href="/format/2306.10224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bloated Disclosures: Can ChatGPT Help Investors Process Financial  Information?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Kim%2C+A">Alex Kim</a>, 
<a href="/search/econ?searchtype=author&query=Muhn%2C+M">Maximilian Muhn</a>, 
<a href="/search/econ?searchtype=author&query=Nikolaev%2C+V">Valeri Nikolaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Artificial Intelligence (cs.AI); General Finance (q-fin.GN)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10276" title="Abstract">arXiv:2306.10276</a> (replaced) [<a href="/pdf/2306.10276" title="Download PDF">pdf</a>, <a href="/format/2306.10276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometric Mechanics of Contact-Switching Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prasad%2C+H+K+H">Hari Krishna Hari Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Hatton%2C+R+L">Ross L. Hatton</a>, 
<a href="/search/cs?searchtype=author&query=Jayaram%2C+K">Kaushik Jayaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, and link to associated video: "<a href="https://drive.google.com/file/d/12Sgl0R1oDLDWRrqlwwAt3JR2Gc3rEB4T/view?usp=sharing">this https URL</a>". Link to code: "<a href="https://github.com/Animal-Inspired-Motion-And-Robotics-Lab/Paper-Geometric-Mechanics-of-Contact-Switching-Systems">this https URL</a>". Accepted to RA-L on Monday, October 16th, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10376" title="Abstract">arXiv:2306.10376</a> (replaced) [<a href="/pdf/2306.10376" title="Download PDF">pdf</a>, <a href="/format/2306.10376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLARA: Classifying and Disambiguating User Commands for Reliable  Interactive Robotic Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jeongeun Park</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S">Seungwon Lim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joonhyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sangbeom Park</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Minsuk Chang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sungjoon Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://clararobot.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10532" title="Abstract">arXiv:2306.10532</a> (replaced) [<a href="/pdf/2306.10532" title="Download PDF">pdf</a>, <a href="/format/2306.10532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Elastic Embedding Learning for On-Device Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruiqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Liang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11301" title="Abstract">arXiv:2306.11301</a> (replaced) [<a href="/pdf/2306.11301" title="Download PDF">pdf</a>, <a href="/format/2306.11301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Search and Tracking with Multiagent Reinforcement Learning  in Sparsely Observable Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zixuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Sean Ye</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+M">Manisha Natarajan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Letian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Paleja%2C+R">Rohan Paleja</a>, 
<a href="/search/cs?searchtype=author&query=Gombolay%2C+M+C">Matthew C. Gombolay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems (MRS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11443" title="Abstract">arXiv:2306.11443</a> (replaced) [<a href="/pdf/2306.11443" title="Download PDF">pdf</a>, <a href="/format/2306.11443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yansong Ning</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhenyu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11546" title="Abstract">arXiv:2306.11546</a> (replaced) [<a href="/pdf/2306.11546" title="Download PDF">pdf</a>, <a href="/format/2306.11546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bullying10K: A Large-Scale Neuromorphic Dataset towards  Privacy-Preserving Bullying Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yiting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongcheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+G">Guobin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yi Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12045" title="Abstract">arXiv:2306.12045</a> (replaced) [<a href="/pdf/2306.12045" title="Download PDF">pdf</a>, <a href="/format/2306.12045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Conditioning Spiking Latent Variable Models of the Neural  Response to Natural Visual Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Ma%2C+G">Gehua Ma</a>, 
<a href="/search/q-bio?searchtype=author&query=Jiang%2C+R">Runhao Jiang</a>, 
<a href="/search/q-bio?searchtype=author&query=Yan%2C+R">Rui Yan</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+H">Huajin Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. 22 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12795" title="Abstract">arXiv:2306.12795</a> (replaced) [<a href="/pdf/2306.12795" title="Download PDF">pdf</a>, <a href="/format/2306.12795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Unseen Modality Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunhua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Doughty%2C+H">Hazel Doughty</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G.M. Snoek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13089" title="Abstract">arXiv:2306.13089</a> (replaced) [<a href="/pdf/2306.13089" title="Download PDF">pdf</a>, <a href="/format/2306.13089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule  Zero-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiteng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hannan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhi-Hong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13769" title="Abstract">arXiv:2306.13769</a> (replaced) [<a href="/pdf/2306.13769" title="Download PDF">pdf</a>, <a href="/format/2306.13769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation  and Elaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+O">Odin Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+Z">Zhiyuan Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13856" title="Abstract">arXiv:2306.13856</a> (replaced) [<a href="/pdf/2306.13856" title="Download PDF">pdf</a>, <a href="/format/2306.13856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-to-Rank Meets Language: Boosting Language-Driven Ordering  Alignment for Ordinal Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peipei Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chunshui Cao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13874" title="Abstract">arXiv:2306.13874</a> (replaced) [<a href="/pdf/2306.13874" title="Download PDF">pdf</a>, <a href="/format/2306.13874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Spectrum Sensing via Reconfigurable Intelligent Surfaces:  Passive or Active Sensing and How Many Reflecting Elements are Needed?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bowen Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14932" title="Abstract">arXiv:2306.14932</a> (replaced) [<a href="/pdf/2306.14932" title="Download PDF">pdf</a>, <a href="/ps/2306.14932" title="Download PostScript">ps</a>, <a href="/format/2306.14932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GloptiNets: Scalable Non-Convex Optimization with Certificates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Beugnot%2C+G">Gaspard Beugnot</a> (PSL, DI-ENS), 
<a href="/search/math?searchtype=author&query=Mairal%2C+J">Julien Mairal</a>, 
<a href="/search/math?searchtype=author&query=Rudi%2C+A">Alessandro Rudi</a> (PSL, DI-ENS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15220" title="Abstract">arXiv:2306.15220</a> (replaced) [<a href="/pdf/2306.15220" title="Download PDF">pdf</a>, <a href="/format/2306.15220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Apolinario%2C+M+P+E">Marco Paul E. Apolinario</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17020" title="Abstract">arXiv:2306.17020</a> (replaced) [<a href="/e-print/2306.17020" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classifying Crime Types using Judgment Documents from Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haoxuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zeyu He</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Mengfan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+S">Songning Lai</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Ziqiang Han</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has no errors; it just needs to be supplemented to become a new article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00663" title="Abstract">arXiv:2307.00663</a> (replaced) [<a href="/pdf/2307.00663" title="Download PDF">pdf</a>, <a href="/format/2307.00663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Multi-Agent Target Assignment and Path Finding with a Single  Constraint Tree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yimin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhongqiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sycara%2C+K">Katia Sycara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01515" title="Abstract">arXiv:2307.01515</a> (replaced) [<a href="/pdf/2307.01515" title="Download PDF">pdf</a>, <a href="/format/2307.01515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LPN: Language-guided Prototypical Network for few-shot classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kaihui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chule Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+N">Naiyang Guan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyuan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02881" title="Abstract">arXiv:2307.02881</a> (replaced) [<a href="/pdf/2307.02881" title="Download PDF">pdf</a>, <a href="/format/2307.02881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic and Semantic Descriptions of Image Manifolds and Their  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+P">Peter Tu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaoyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hartley%2C+R">Richard Hartley</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yiwei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+D">Dylan Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+J">Jaskirat Singh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianyu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 17 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03518" title="Abstract">arXiv:2307.03518</a> (replaced) [<a href="/pdf/2307.03518" title="Download PDF">pdf</a>, <a href="/ps/2307.03518" title="Download PostScript">ps</a>, <a href="/format/2307.03518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multivariate Complexity Analysis of the Generalized Noah&#x27;s Ark Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Komusiewicz%2C+C">Christian Komusiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Schestag%2C+J">Jannik Schestag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03567" title="Abstract">arXiv:2307.03567</a> (replaced) [<a href="/pdf/2307.03567" title="Download PDF">pdf</a>, <a href="/format/2307.03567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xingyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+J">John So</a>, 
<a href="/search/cs?searchtype=author&query=Mahalingam%2C+S">Sashwat Mahalingam</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fangchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03891" title="Abstract">arXiv:2307.03891</a> (replaced) [<a href="/pdf/2307.03891" title="Download PDF">pdf</a>, <a href="/format/2307.03891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARBLER: An Open Platform for Standardized Evaluation of Multi-Robot  Reinforcement Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torbati%2C+R">Reza Torbati</a>, 
<a href="/search/cs?searchtype=author&query=Lohiya%2C+S">Shubham Lohiya</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shivika Singh</a>, 
<a href="/search/cs?searchtype=author&query=Nigam%2C+M+S">Meher Shashwat Nigam</a>, 
<a href="/search/cs?searchtype=author&query=Ravichandar%2C+H">Harish Ravichandar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, accepted to MRS 2023, for the associated website, see <a href="https://shubhlohiya.github.io/MARBLER/">this https URL</a>, resubmitting the camera ready version of the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04033" title="Abstract">arXiv:2307.04033</a> (replaced) [<a href="/pdf/2307.04033" title="Download PDF">pdf</a>, <a href="/format/2307.04033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Variational Neighbor Labels for Test-Time Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ambekar%2C+S">Sameer Ambekar</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zehao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiayi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+X">Xiantong Zhen</a>, 
<a href="/search/cs?searchtype=author&query=Snoek%2C+C+G+M">Cees G. M. Snoek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04374" title="Abstract">arXiv:2307.04374</a> (replaced) [<a href="/pdf/2307.04374" title="Download PDF">pdf</a>, <a href="/format/2307.04374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Identify Graphs from Node Trajectories in Multi-Robot  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sebastian%2C+E">Eduardo Sebastian</a>, 
<a href="/search/eess?searchtype=author&query=Duong%2C+T">Thai Duong</a>, 
<a href="/search/eess?searchtype=author&query=Atanasov%2C+N">Nikolay Atanasov</a>, 
<a href="/search/eess?searchtype=author&query=Montijano%2C+E">Eduardo Montijano</a>, 
<a href="/search/eess?searchtype=author&query=Sagues%2C+C">Carlos Sagues</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE MRS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05266" title="Abstract">arXiv:2307.05266</a> (replaced) [<a href="/pdf/2307.05266" title="Download PDF">pdf</a>, <a href="/format/2307.05266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the efficient preconditioning of the Stokes equations in tight  geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pimanov%2C+V">Vladislav Pimanov</a>, 
<a href="/search/math?searchtype=author&query=Iliev%2C+O">Oleg Iliev</a>, 
<a href="/search/math?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>, 
<a href="/search/math?searchtype=author&query=Muravleva%2C+E">Ekaterina Muravleva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05434" title="Abstract">arXiv:2307.05434</a> (replaced) [<a href="/pdf/2307.05434" title="Download PDF">pdf</a>, <a href="/format/2307.05434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedded symmetric positive semi-definite machine-learned elements for  reduced-order modeling in finite-element simulations with application to  threaded fasteners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parish%2C+E">Eric Parish</a>, 
<a href="/search/math?searchtype=author&query=Lindsay%2C+P">Payton Lindsay</a>, 
<a href="/search/math?searchtype=author&query=Shelton%2C+T">Timothy Shelton</a>, 
<a href="/search/math?searchtype=author&query=Mersch%2C+J">John Mersch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07847" title="Abstract">arXiv:2307.07847</a> (replaced) [<a href="/pdf/2307.07847" title="Download PDF">pdf</a>, <a href="/format/2307.07847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Real-time Neural Recovery for Cloud Gaming on Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaoyuan He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuozhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Diyuan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lili Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08268" title="Abstract">arXiv:2307.08268</a> (replaced) [<a href="/pdf/2307.08268" title="Download PDF">pdf</a>, <a href="/format/2307.08268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yan%2C+K">Ke Yan</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+X">Xiaoli Yin</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+Y">Yingda Xia</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+F">Fakai Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Y">Yuan Gao</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+J">Jiawen Yao</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Chunli Li</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+X">Xiaoyu Bai</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Ling Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+L">Le Lu</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+Y">Yu Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023, code: <a href="https://github.com/alibaba-damo-academy/pixel-lesion-patient-network">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08523" title="Abstract">arXiv:2307.08523</a> (replaced) [<a href="/pdf/2307.08523" title="Download PDF">pdf</a>, <a href="/ps/2307.08523" title="Download PostScript">ps</a>, <a href="/format/2307.08523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generic bidirectional typing for dependent type theories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Felicissimo%2C+T">Thiago Felicissimo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09279" title="Abstract">arXiv:2307.09279</a> (replaced) [<a href="/pdf/2307.09279" title="Download PDF">pdf</a>, <a href="/format/2307.09279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regression-free Blind Image Quality Assessment with Content-Distortion  Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jian Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weisi Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09384" title="Abstract">arXiv:2307.09384</a> (replaced) [<a href="/pdf/2307.09384" title="Download PDF">pdf</a>, <a href="/format/2307.09384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Query Reformulation for Conversational Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dayu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hui Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 9th ACM SIGIR International Conference on the Theory of Information Retrieval
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10318" title="Abstract">arXiv:2307.10318</a> (replaced) [<a href="/pdf/2307.10318" title="Download PDF">pdf</a>, <a href="/format/2307.10318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliminating Label Leakage in Tree-Based Vertical Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+H">Hideaki Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingjing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10700" title="Abstract">arXiv:2307.10700</a> (replaced) [<a href="/pdf/2307.10700" title="Download PDF">pdf</a>, <a href="/format/2307.10700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topics, Authors, and Networks in Large Language Model Research: Trends  from a Survey of 17K arXiv Papers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Movva%2C+R">Rajiv Movva</a>, 
<a href="/search/cs?searchtype=author&query=Balachandar%2C+S">Sidhika Balachandar</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kenny Peng</a>, 
<a href="/search/cs?searchtype=author&query=Agostini%2C+G">Gabriel Agostini</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+N">Nikhil Garg</a>, 
<a href="/search/cs?searchtype=author&query=Pierson%2C+E">Emma Pierson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working paper. Data/code available at <a href="https://github.com/rmovva/LLM-publication-patterns-public">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11018" title="Abstract">arXiv:2307.11018</a> (replaced) [<a href="/pdf/2307.11018" title="Download PDF">pdf</a>, <a href="/format/2307.11018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amortized Variational Inference: When and Why?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Margossian%2C+C+C">Charles C. Margossian</a>, 
<a href="/search/stat?searchtype=author&query=Blei%2C+D+M">David M. Blei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11373" title="Abstract">arXiv:2307.11373</a> (replaced) [<a href="/pdf/2307.11373" title="Download PDF">pdf</a>, <a href="/format/2307.11373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diverse Offline Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vlastelica%2C+M">Marin Vlastelica</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Martius%2C+G">Georg Martius</a>, 
<a href="/search/cs?searchtype=author&query=Kolev%2C+P">Pavel Kolev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1139">[1139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11949" title="Abstract">arXiv:2307.11949</a> (replaced) [<a href="/pdf/2307.11949" title="Download PDF">pdf</a>, <a href="/format/2307.11949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HIQL: Offline Goal-Conditioned RL with Latent States as Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seohong Park</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+D">Dibya Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Eysenbach%2C+B">Benjamin Eysenbach</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1140">[1140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12169" title="Abstract">arXiv:2307.12169</a> (replaced) [<a href="/pdf/2307.12169" title="Download PDF">pdf</a>, <a href="/format/2307.12169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Network Architectures for Large Language Model Training with  Billions of Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ghobadi%2C+M">Manya Ghobadi</a>, 
<a href="/search/cs?searchtype=author&query=Shakeri%2C+K">Kayvon Shakeri</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hasani%2C+N">Naader Hasani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1141">[1141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13086" title="Abstract">arXiv:2307.13086</a> (replaced) [<a href="/pdf/2307.13086" title="Download PDF">pdf</a>, <a href="/ps/2307.13086" title="Download PostScript">ps</a>, <a href="/format/2307.13086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstruction techniques for complex potentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kravchenko%2C+V+V">Vladislav V. Kravchenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Mathematical Physics (math-ph); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1142">[1142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13396" title="Abstract">arXiv:2307.13396</a> (replaced) [<a href="/pdf/2307.13396" title="Download PDF">pdf</a>, <a href="/format/2307.13396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Odd-Fair Parity Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sa%C4%9Flam%2C+I">Irmak Sa&#x11f;lam</a>, 
<a href="/search/cs?searchtype=author&query=Schmuck%2C+A">Anne-Kathrin Schmuck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in FSTTCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1143">[1143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13899" title="Abstract">arXiv:2307.13899</a> (replaced) [<a href="/pdf/2307.13899" title="Download PDF">pdf</a>, <a href="/format/2307.13899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularizing Neural Networks with Meta-Learning Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+S">Shin&#x27;ya Yamaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Chijiwa%2C+D">Daiki Chijiwa</a>, 
<a href="/search/cs?searchtype=author&query=Kanai%2C+S">Sekitoshi Kanai</a>, 
<a href="/search/cs?searchtype=author&query=Kumagai%2C+A">Atsutoshi Kumagai</a>, 
<a href="/search/cs?searchtype=author&query=Kashima%2C+H">Hisashi Kashima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1144">[1144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14208" title="Abstract">arXiv:2307.14208</a> (replaced) [<a href="/pdf/2307.14208" title="Download PDF">pdf</a>, <a href="/format/2307.14208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Modeling and Monitoring of Dependent Processes under Resource  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kosolwattana%2C+T">Tanapol Kosolwattana</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huazheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Ying Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1145">[1145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15455" title="Abstract">arXiv:2307.15455</a> (replaced) [<a href="/pdf/2307.15455" title="Download PDF">pdf</a>, <a href="/format/2307.15455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trie-NLG: Trie Context Augmentation to Improve Personalized Query  Auto-Completion for Short and Unseen Prefixes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maurya%2C+K+K">Kaushal Kumar Maurya</a>, 
<a href="/search/cs?searchtype=author&query=Desarkar%2C+M+S">Maunendra Sankar Desarkar</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Manish Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Puneet Agrawal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECML-PKDD 2023 (Journal Track)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Data Mining and Knowledge Discovery (DAMI) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1146">[1146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16786" title="Abstract">arXiv:2307.16786</a> (replaced) [<a href="/pdf/2307.16786" title="Download PDF">pdf</a>, <a href="/format/2307.16786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovery Policies for Safe Exploration of Lunar Permanently Shadowed  Regions by a Solar-Powered Rover
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lamarre%2C+O">Olivier Lamarre</a>, 
<a href="/search/cs?searchtype=author&query=Malhotra%2C+S">Shantanu Malhotra</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+J">Jonathan Kelly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Acta Astronautica, vol. 213, pp. 708-724, Dec. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1147">[1147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00344" title="Abstract">arXiv:2308.00344</a> (replaced) [<a href="/pdf/2308.00344" title="Download PDF">pdf</a>, <a href="/format/2308.00344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kidnapping Deep Learning-based Multirotors using Optimized Flying  Adversarial Patches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanfeld%2C+P">Pia Hanfeld</a>, 
<a href="/search/cs?searchtype=author&query=Wahba%2C+K">Khaled Wahba</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6hne%2C+M+M+-">Marina M.-C. H&#xf6;hne</a>, 
<a href="/search/cs?searchtype=author&query=Bussmann%2C+M">Michael Bussmann</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6nig%2C+W">Wolfgang H&#xf6;nig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at MRS 2023, 7 pages, 5 figures. arXiv admin note: substantial text overlap with <a href="/abs/2305.12859">arXiv:2305.12859</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1148">[1148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01362" title="Abstract">arXiv:2308.01362</a> (replaced) [<a href="/pdf/2308.01362" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Deep Learning for Tumor Dynamic Modeling and Overall  Survival Prediction using Neural-ODE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Laurie%2C+M">Mark Laurie</a>, 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">James Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 4 Figures and 2 Tables. Includes Supplementary Materials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1149">[1149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01684" title="Abstract">arXiv:2308.01684</a> (replaced) [<a href="/pdf/2308.01684" title="Download PDF">pdf</a>, <a href="/format/2308.01684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Baby&#x27;s CoThought: Leveraging Large Language Models for Enhanced  Reasoning in Compact Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Han Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+B">Bolei Ma</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCgamer%2C+D">David R&#xfc;gamer</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+E">Ercong Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoNLL 2023 BabyLM Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1150">[1150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02233" title="Abstract">arXiv:2308.02233</a> (replaced) [<a href="/pdf/2308.02233" title="Download PDF">pdf</a>, <a href="/format/2308.02233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for  Modeling EDFA Wavelength Dependent Gain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raj%2C+A">Agastya Raj</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zehao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Slyne%2C+F">Frank Slyne</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tingjun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kilper%2C+D">Dan Kilper</a>, 
<a href="/search/cs?searchtype=author&query=Ruffini%2C+M">Marco Ruffini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is a preprint of a paper submitted to ECOC 2023 and is subject to Institution of Engineering and Technology Copyright. If accepted, the copy of record will be available at IET Digital Library
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1151">[1151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03415" title="Abstract">arXiv:2308.03415</a> (replaced) [<a href="/pdf/2308.03415" title="Download PDF">pdf</a>, <a href="/format/2308.03415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Evaluation for Low-Latency Simultaneous Speech Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huber%2C+C">Christian Huber</a>, 
<a href="/search/cs?searchtype=author&query=Dinh%2C+T+A">Tu Anh Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Mullov%2C+C">Carlos Mullov</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+N+Q">Ngoc Quan Pham</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+B">Thai Binh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Retkowski%2C+F">Fabian Retkowski</a>, 
<a href="/search/cs?searchtype=author&query=Constantin%2C+S">Stefan Constantin</a>, 
<a href="/search/cs?searchtype=author&query=Ugan%2C+E+Y">Enes Yavuz Ugan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Danni Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaolin Li</a>, 
<a href="/search/cs?searchtype=author&query=Koneru%2C+S">Sai Koneru</a>, 
<a href="/search/cs?searchtype=author&query=Niehues%2C+J">Jan Niehues</a>, 
<a href="/search/cs?searchtype=author&query=Waibel%2C+A">Alexander Waibel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1152">[1152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03427" title="Abstract">arXiv:2308.03427</a> (replaced) [<a href="/pdf/2308.03427" title="Download PDF">pdf</a>, <a href="/format/2308.03427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TPTU: Large Language Model-based AI Agents for Task Planning and Tool  Usage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jingqing Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yihong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+T">Tianpeng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+G">Guoqing Du</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shiwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Hangyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingyu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1153">[1153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03718" title="Abstract">arXiv:2308.03718</a> (replaced) [<a href="/pdf/2308.03718" title="Download PDF">pdf</a>, <a href="/format/2308.03718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEM-GAT: Explainable Semantic Pose Estimation using Learned Graph  Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panagiotaki%2C+E">Efimia Panagiotaki</a>, 
<a href="/search/cs?searchtype=author&query=De+Martini%2C+D">Daniele De Martini</a>, 
<a href="/search/cs?searchtype=author&query=Pramatarov%2C+G">Georgi Pramatarov</a>, 
<a href="/search/cs?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>, 
<a href="/search/cs?searchtype=author&query=Kunze%2C+L">Lars Kunze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Advanced Robotics (ICAR 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1154">[1154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04220" title="Abstract">arXiv:2308.04220</a> (replaced) [<a href="/pdf/2308.04220" title="Download PDF">pdf</a>, <a href="/format/2308.04220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Interpretation and Validation of Graph Attention-based  Explanations for GNN Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panagiotaki%2C+E">Efimia Panagiotaki</a>, 
<a href="/search/cs?searchtype=author&query=De+Martini%2C+D">Daniele De Martini</a>, 
<a href="/search/cs?searchtype=author&query=Kunze%2C+L">Lars Kunze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Advanced Robotics (ICAR 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1155">[1155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05697" title="Abstract">arXiv:2308.05697</a> (replaced) [<a href="/pdf/2308.05697" title="Download PDF">pdf</a>, <a href="/format/2308.05697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSLRec: A Self-Supervised Learning Framework for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xubin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianle Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xuheng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a WSDM'24 full paper (oral presentation)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1156">[1156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05732" title="Abstract">arXiv:2308.05732</a> (replaced) [<a href="/pdf/2308.05732" title="Download PDF">pdf</a>, <a href="/format/2308.05732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lippe%2C+P">Phillip Lippe</a>, 
<a href="/search/cs?searchtype=author&query=Veeling%2C+B+S">Bastiaan S. Veeling</a>, 
<a href="/search/cs?searchtype=author&query=Perdikaris%2C+P">Paris Perdikaris</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://phlippe.github.io/PDERefiner/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1157">[1157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07037" title="Abstract">arXiv:2308.07037</a> (replaced) [<a href="/pdf/2308.07037" title="Download PDF">pdf</a>, <a href="/format/2308.07037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graves%2C+A">Alex Graves</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+R+K">Rupesh Kumar Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Atkinson%2C+T">Timothy Atkinson</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+F">Faustino Gomez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1158">[1158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08856" title="Abstract">arXiv:2308.08856</a> (replaced) [<a href="/pdf/2308.08856" title="Download PDF">pdf</a>, <a href="/format/2308.08856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose  and Size Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiaqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yucong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xiangting Meng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Chenxin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Min Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Ran Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lige Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kneip%2C+L">Laurent Kneip</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1159">[1159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09239" title="Abstract">arXiv:2308.09239</a> (replaced) [<a href="/pdf/2308.09239" title="Download PDF">pdf</a>, <a href="/format/2308.09239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SHAPFUZZ: Efficient Fuzzing via Shapley-Guided Byte Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kunpeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaogang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Minhui Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Sheng Wen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Network and Distributed System Security (NDSS) Symposium 2024, 26
  February - 1 March 2024, San Diego, CA, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1160">[1160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09510" title="Abstract">arXiv:2308.09510</a> (replaced) [<a href="/pdf/2308.09510" title="Download PDF">pdf</a>, <a href="/ps/2308.09510" title="Download PostScript">ps</a>, <a href="/format/2308.09510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Simulation of Quantum Circuits by Model Order Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Jim%C3%A9nez-Pastor%2C+A">Antonio Jim&#xe9;nez-Pastor</a>, 
<a href="/search/quant-ph?searchtype=author&query=Larsen%2C+K+G">Kim G. Larsen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tribastone%2C+M">Mirco Tribastone</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tschaikowski%2C+M">Max Tschaikowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item1161">[1161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10373" title="Abstract">arXiv:2308.10373</a> (replaced) [<a href="/pdf/2308.10373" title="Download PDF">pdf</a>, <a href="/format/2308.10373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with  Adaptive Firing Thresholds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+H">Hejia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1162">[1162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10848" title="Abstract">arXiv:2308.10848</a> (replaced) [<a href="/pdf/2308.10848" title="Download PDF">pdf</a>, <a href="/format/2308.10848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentVerse: Facilitating Multi-Agent Collaboration and Exploring  Emergent Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weize Chen</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yusheng Su</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+J">Jingwei Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenfei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Chi-Min Chan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Heyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yaxi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+Y">Yi-Hsin Hung</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. Code at <a href="https://github.com/OpenBMB/AgentVerse/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1163">[1163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11175" title="Abstract">arXiv:2308.11175</a> (replaced) [<a href="/pdf/2308.11175" title="Download PDF">pdf</a>, <a href="/format/2308.11175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MISSRec: Pre-training and Transferring Multi-modal Interest-aware  Sequence Representation for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziyun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xingyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hai-Tao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shu-Tao Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM MM 2023. Data and code are available
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1164">[1164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11909" title="Abstract">arXiv:2308.11909</a> (replaced) [<a href="/pdf/2308.11909" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge-aware Hard Clustering Graph Pooling for Brain Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Cheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiayi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Ping Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Honghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Ying Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1165">[1165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12575" title="Abstract">arXiv:2308.12575</a> (replaced) [<a href="/pdf/2308.12575" title="Download PDF">pdf</a>, <a href="/format/2308.12575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph Convolutional Networks for Fine-grained ICU Patient  Similarity Analysis and Risk Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Shaowen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F+D">Flora D. Salim</a>, 
<a href="/search/cs?searchtype=author&query=Yepes%2C+A+J">Antonio Jimeno Yepes</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1166">[1166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13137" title="Abstract">arXiv:2308.13137</a> (replaced) [<a href="/pdf/2308.13137" title="Download PDF">pdf</a>, <a href="/format/2308.13137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniQuant: Omnidirectionally Calibrated Quantization for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengzhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lirui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated result with 2-bit quantization. A differentiable quantization method for LLM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1167">[1167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13150" title="Abstract">arXiv:2308.13150</a> (replaced) [<a href="/pdf/2308.13150" title="Download PDF">pdf</a>, <a href="/format/2308.13150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Breast Cancer Classification Using Transfer ResNet with  Lightweight Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Suxing Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures,6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1168">[1168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14177" title="Abstract">arXiv:2308.14177</a> (replaced) [<a href="/pdf/2308.14177" title="Download PDF">pdf</a>, <a href="/format/2308.14177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Generated Content (AIGC) for Various Data Modalities: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foo%2C+L+G">Lin Geng Foo</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H">Hossein Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1169">[1169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14371" title="Abstract">arXiv:2308.14371</a> (replaced) [<a href="/pdf/2308.14371" title="Download PDF">pdf</a>, <a href="/format/2308.14371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SuperUDF: Self-supervised UDF Estimation for Surface Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yifei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1170">[1170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14545" title="Abstract">arXiv:2308.14545</a> (replaced) [<a href="/pdf/2308.14545" title="Download PDF">pdf</a>, <a href="/ps/2308.14545" title="Download PostScript">ps</a>, <a href="/format/2308.14545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized and Deterministic Maximin-share Approximations for  Fractionally Subadditive Valuations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akrami%2C+H">Hannaneh Akrami</a>, 
<a href="/search/cs?searchtype=author&query=Mehlhorn%2C+K">Kurt Mehlhorn</a>, 
<a href="/search/cs?searchtype=author&query=Seddighin%2C+M">Masoud Seddighin</a>, 
<a href="/search/cs?searchtype=author&query=Shahkarami%2C+G">Golnoosh Shahkarami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1171">[1171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14569" title="Abstract">arXiv:2308.14569</a> (replaced) [<a href="/pdf/2308.14569" title="Download PDF">pdf</a>, <a href="/ps/2308.14569" title="Download PostScript">ps</a>, <a href="/format/2308.14569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Fr&#xe9;chet Distance Problems by Algebraic Geometric Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siu-Wing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haoqiang Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SODA24, correct some references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item1172">[1172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15568" title="Abstract">arXiv:2308.15568</a> (replaced) [<a href="/pdf/2308.15568" title="Download PDF">pdf</a>, <a href="/ps/2308.15568" title="Download PostScript">ps</a>, <a href="/format/2308.15568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Over-Squashing in Graph Neural Networks: A Comprehensive survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akansha%2C+S">Singh Akansha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1173">[1173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01826" title="Abstract">arXiv:2309.01826</a> (replaced) [<a href="/pdf/2309.01826" title="Download PDF">pdf</a>, <a href="/format/2309.01826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Wide Feedforward is All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pires%2C+T+P">Telmo Pessoa Pires</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+A+V">Ant&#xf3;nio V. Lopes</a>, 
<a href="/search/cs?searchtype=author&query=Assogba%2C+Y">Yannick Assogba</a>, 
<a href="/search/cs?searchtype=author&query=Setiawan%2C+H">Hendra Setiawan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WMT23 (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1174">[1174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02579" title="Abstract">arXiv:2309.02579</a> (replaced) [<a href="/pdf/2309.02579" title="Download PDF">pdf</a>, <a href="/format/2309.02579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Social Network Approach to Analyzing Token Properties and Abnormal  Events in Decentralized Exchanges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+A+S">Aryan Soltani Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Karami%2C+M">Moein Karami</a>, 
<a href="/search/cs?searchtype=author&query=Motamed%2C+A+P">Amir Pasha Motamed</a>, 
<a href="/search/cs?searchtype=author&query=Bahrak%2C+B">Behnam Bahrak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item1175">[1175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02606" title="Abstract">arXiv:2309.02606</a> (replaced) [<a href="/pdf/2309.02606" title="Download PDF">pdf</a>, <a href="/format/2309.02606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Variational Inference for Online Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paritosh%2C+P">Parth Paritosh</a>, 
<a href="/search/cs?searchtype=author&query=Atanasov%2C+N">Nikolay Atanasov</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+S">Sonia Martinez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1176">[1176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03616" title="Abstract">arXiv:2309.03616</a> (replaced) [<a href="/pdf/2309.03616" title="Download PDF">pdf</a>, <a href="/format/2309.03616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filtration Surfaces for Dynamic Graph Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srambical%2C+F">Franz Srambical</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+B">Bastian Rieck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1177">[1177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03765" title="Abstract">arXiv:2309.03765</a> (replaced) [<a href="/pdf/2309.03765" title="Download PDF">pdf</a>, <a href="/format/2309.03765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Symmetries for Inertial Navigation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fornasier%2C+A">Alessandro Fornasier</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yixiao Ge</a>, 
<a href="/search/cs?searchtype=author&query=van+Goor%2C+P">Pieter van Goor</a>, 
<a href="/search/cs?searchtype=author&query=Mahony%2C+R">Robert Mahony</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+S">Stephan Weiss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Automatica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1178">[1178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05855" title="Abstract">arXiv:2309.05855</a> (replaced) [<a href="/pdf/2309.05855" title="Download PDF">pdf</a>, <a href="/format/2309.05855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instabilities in Convnets for Raw Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haider%2C+D">Daniel Haider</a>, 
<a href="/search/cs?searchtype=author&query=Lostanlen%2C+V">Vincent Lostanlen</a>, 
<a href="/search/cs?searchtype=author&query=Ehler%2C+M">Martin Ehler</a>, 
<a href="/search/cs?searchtype=author&query=Balazs%2C+P">Peter Balazs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 5 figures, 1 page appendix, under review for IEEE SPL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1179">[1179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05959" title="Abstract">arXiv:2309.05959</a> (replaced) [<a href="/pdf/2309.05959" title="Download PDF">pdf</a>, <a href="/format/2309.05959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computationally Efficient Bi-level Coordination Framework for CAVs at  Unsignalized Intersections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Luo%2C+J">Jiping Luo</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+T">Tingting Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Q">Qinyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1180">[1180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06553" title="Abstract">arXiv:2309.06553</a> (replaced) [<a href="/pdf/2309.06553" title="Download PDF">pdf</a>, <a href="/format/2309.06553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-Dependent Prompt Evaluation and Optimization with Offline Inverse  RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCy%C3%BCk%2C+A">Alihan H&#xfc;y&#xfc;k</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1181">[1181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07407" title="Abstract">arXiv:2309.07407</a> (replaced) [<a href="/pdf/2309.07407" title="Download PDF">pdf</a>, <a href="/format/2309.07407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning-based Scheduling for Optimizing System Load  and Response Time in Edge and Fog Computing Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Goudarzi%2C+M">Mohammad Goudarzi</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Mingming Gong</a>, 
<a href="/search/cs?searchtype=author&query=Buyya%2C+R">Rajkumar Buyya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1182">[1182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07430" title="Abstract">arXiv:2309.07430</a> (replaced) [<a href="/pdf/2309.07430" title="Download PDF">pdf</a>, <a href="/format/2309.07430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clinical Text Summarization: Adapting Large Language Models Can  Outperform Human Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Veen%2C+D">Dave Van Veen</a>, 
<a href="/search/cs?searchtype=author&query=Van+Uden%2C+C">Cara Van Uden</a>, 
<a href="/search/cs?searchtype=author&query=Blankemeier%2C+L">Louis Blankemeier</a>, 
<a href="/search/cs?searchtype=author&query=Delbrouck%2C+J">Jean-Benoit Delbrouck</a>, 
<a href="/search/cs?searchtype=author&query=Aali%2C+A">Asad Aali</a>, 
<a href="/search/cs?searchtype=author&query=Bluethgen%2C+C">Christian Bluethgen</a>, 
<a href="/search/cs?searchtype=author&query=Pareek%2C+A">Anuj Pareek</a>, 
<a href="/search/cs?searchtype=author&query=Polacin%2C+M">Malgorzata Polacin</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+W">William Collins</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+N">Neera Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Langlotz%2C+C+P">Curtis P. Langlotz</a>, 
<a href="/search/cs?searchtype=author&query=Hom%2C+J">Jason Hom</a>, 
<a href="/search/cs?searchtype=author&query=Gatidis%2C+S">Sergios Gatidis</a>, 
<a href="/search/cs?searchtype=author&query=Pauly%2C+J">John Pauly</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+A+S">Akshay S. Chaudhari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 24 figures. Compared to the original, this version includes minor edits and supplementary additional experiments that reinforce the initial findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1183">[1183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08138" title="Abstract">arXiv:2309.08138</a> (replaced) [<a href="/pdf/2309.08138" title="Download PDF">pdf</a>, <a href="/format/2309.08138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Find What You Want: Learning Demand-conditioned Object Attribute Space  for Demand-driven Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongcheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A+G+H">Andy Guan Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mingdong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; Project page:<a href="https://sites.google.com/view/demand-driven-navigation">this https URL</a>; Code: <a href="https://github.com/whcpumpkin/Demand-driven-navigation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1184">[1184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08863" title="Abstract">arXiv:2309.08863</a> (replaced) [<a href="/pdf/2309.08863" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trajectory Tracking Control of Skid-Steering Mobile Robots with Slip and  Skid Compensation using Sliding-Mode Control and Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nourizadeh%2C+P">Payam Nourizadeh</a>, 
<a href="/search/cs?searchtype=author&query=McFadden%2C+F+J+S">Fiona J Stevens McFadden</a>, 
<a href="/search/cs?searchtype=author&query=Browne%2C+W+N">Will N Browne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1185">[1185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09451" title="Abstract">arXiv:2309.09451</a> (replaced) [<a href="/pdf/2309.09451" title="Download PDF">pdf</a>, <a href="/ps/2309.09451" title="Download PostScript">ps</a>, <a href="/format/2309.09451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fitchean Ignorance and First-order Ignorance: A Neighborhood Look
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jie Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item1186">[1186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10268" title="Abstract">arXiv:2309.10268</a> (replaced) [<a href="/pdf/2309.10268" title="Download PDF">pdf</a>, <a href="/format/2309.10268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lower Gravity Demonstratable Testbed for Space Robot Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uno%2C+K">Kentaro Uno</a>, 
<a href="/search/cs?searchtype=author&query=Takada%2C+K">Kazuki Takada</a>, 
<a href="/search/cs?searchtype=author&query=Nagaoka%2C+K">Keita Nagaoka</a>, 
<a href="/search/cs?searchtype=author&query=Kato%2C+T">Takuya Kato</a>, 
<a href="/search/cs?searchtype=author&query=Candalot%2C+A">Arthur Candalot</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+K">Kazuya Yoshida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 3 figures, paper accepted for the SII 2024 (IEEE/SICE International Symposium on System Integration) (Updated references formatting)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1187">[1187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11083" title="Abstract">arXiv:2309.11083</a> (replaced) [<a href="/pdf/2309.11083" title="Download PDF">pdf</a>, <a href="/format/2309.11083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ElasticNotebook: Enabling Live Migration for Computational Notebooks  (Technical Report)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Gor%2C+P">Pranav Gor</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+R">Rahul Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuzhou Mao</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yongjoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to VLDB 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item1188">[1188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11279" title="Abstract">arXiv:2309.11279</a> (replaced) [<a href="/pdf/2309.11279" title="Download PDF">pdf</a>, <a href="/format/2309.11279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Fine-Grained Query Complexity of Symmetric Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Podder%2C+S">Supartha Podder</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+P">Penghui Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zekun Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted in ISAAC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item1189">[1189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11331" title="Abstract">arXiv:2309.11331</a> (replaced) [<a href="/pdf/2309.11331" title="Download PDF">pdf</a>, <a href="/format/2309.11331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengcheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Ying Nie</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jianyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuanjian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1190">[1190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11668" title="Abstract">arXiv:2309.11668</a> (replaced) [<a href="/pdf/2309.11668" title="Download PDF">pdf</a>, <a href="/format/2309.11668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Effective Disambiguation for Machine Translation with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iyer%2C+V">Vivek Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pinzhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WMT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1191">[1191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11702" title="Abstract">arXiv:2309.11702</a> (replaced) [<a href="/pdf/2309.11702" title="Download PDF">pdf</a>, <a href="/format/2309.11702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentivized Communication for Federated Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhepei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuanhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongning Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 4 figures. Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1192">[1192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12075" title="Abstract">arXiv:2309.12075</a> (replaced) [<a href="/pdf/2309.12075" title="Download PDF">pdf</a>, <a href="/format/2309.12075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Tuned Embedding Classification for Multi-Label Industry Sector  Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buchner%2C+V+L">Valentin Leonhard Buchner</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Lele Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kalo%2C+J">Jan-Christoph Kalo</a>, 
<a href="/search/cs?searchtype=author&query=von+Ehrenheim%2C+V">Vilhelm von Ehrenheim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1193">[1193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12559" title="Abstract">arXiv:2309.12559</a> (replaced) [<a href="/pdf/2309.12559" title="Download PDF">pdf</a>, <a href="/format/2309.12559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariant Learning via Probability of Sufficient and Necessary Causes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Furui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ton%2C+J">Jean-Francois Ton</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1194">[1194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12780" title="Abstract">arXiv:2309.12780</a> (replaced) [<a href="/pdf/2309.12780" title="Download PDF">pdf</a>, <a href="/format/2309.12780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LMC: Large Model Collaboration with Cross-assessment for Training-Free  Open-Set Object Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Haoxuan Qu</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+X">Xiaofei Hui</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yujun Cai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1195">[1195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13226" title="Abstract">arXiv:2309.13226</a> (replaced) [<a href="/pdf/2309.13226" title="Download PDF">pdf</a>, <a href="/format/2309.13226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real3D-AD: A Dataset of Point Cloud Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Guoyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruitao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinbao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Feng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1196">[1196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13524" title="Abstract">arXiv:2309.13524</a> (replaced) [<a href="/pdf/2309.13524" title="Download PDF">pdf</a>, <a href="/format/2309.13524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global-correlated 3D-decoupling Transformer for Clothed Avatar  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zechuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Li Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. Update appendix. Project page: <a href="https://river-zhang.github.io/GTA-projectpage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1197">[1197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14008" title="Abstract">arXiv:2309.14008</a> (replaced) [<a href="/pdf/2309.14008" title="Download PDF">pdf</a>, <a href="/format/2309.14008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Carrier Aggregation Enabled Integrated Sensing and Communication Signal  Design and Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Haotian Liu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+X">Xinyi Yang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+W">Wangjun Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Huici Wu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xingwang Li</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1198">[1198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14019" title="Abstract">arXiv:2309.14019</a> (replaced) [<a href="/pdf/2309.14019" title="Download PDF">pdf</a>, <a href="/format/2309.14019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a class of strong valid inequalities for the connected matching  polytope
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Samer%2C+P">Phillippe Samer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure. Submitted for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1199">[1199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14500" title="Abstract">arXiv:2309.14500</a> (replaced) [<a href="/pdf/2309.14500" title="Download PDF">pdf</a>, <a href="/format/2309.14500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of IBM and NASA&#x27;s geospatial foundation model in flood  inundation mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chia-Yu Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Arundel%2C+S+T">Samantha T. Arundel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures, Accepted for the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1200">[1200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15268" title="Abstract">arXiv:2309.15268</a> (replaced) [<a href="/pdf/2309.15268" title="Download PDF">pdf</a>, <a href="/format/2309.15268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ObVi-SLAM: Long-Term Object-Visual SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adkins%2C+A">Amanda Adkins</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Taijing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, 1 table plus appendix with 4 figures and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1201">[1201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15325" title="Abstract">arXiv:2309.15325</a> (replaced) [<a href="/pdf/2309.15325" title="Download PDF">pdf</a>, <a href="/format/2309.15325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Operators for Accelerating Scientific Simulations and Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Kovachki%2C+N">Nikola Kovachki</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu-Schiaffini%2C+M">Miguel Liu-Schiaffini</a>, 
<a href="/search/cs?searchtype=author&query=Kossaifi%2C+J">Jean Kossaifi</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item1202">[1202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15563" title="Abstract">arXiv:2309.15563</a> (replaced) [<a href="/pdf/2309.15563" title="Download PDF">pdf</a>, <a href="/format/2309.15563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Frequency Loss for Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benjdira%2C+B">Bilel Benjdira</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A+M">Anas M. Ali</a>, 
<a href="/search/cs?searchtype=author&query=Koubaa%2C+A">Anis Koubaa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1203">[1203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16166" title="Abstract">arXiv:2309.16166</a> (replaced) [<a href="/pdf/2309.16166" title="Download PDF">pdf</a>, <a href="/format/2309.16166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoinRun: Solving Goal Misgeneralisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armstrong%2C+S">Stuart Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Maranh%C3%A3o%2C+A">Alexandre Maranh&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Daniels-Koch%2C+O">Oliver Daniels-Koch</a>, 
<a href="/search/cs?searchtype=author&query=Leask%2C+P">Patrick Leask</a>, 
<a href="/search/cs?searchtype=author&query=Gorman%2C+R">Rebecca Gorman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1204">[1204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16583" title="Abstract">arXiv:2309.16583</a> (replaced) [<a href="/pdf/2309.16583" title="Download PDF">pdf</a>, <a href="/format/2309.16583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-Fathom: Benchmarking Large Language Models to Decipher the  Evolutionary Path towards GPT-4 and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+C">Chenguang Xi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Pengyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1205">[1205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00248" title="Abstract">arXiv:2310.00248</a> (replaced) [<a href="/pdf/2310.00248" title="Download PDF">pdf</a>, <a href="/format/2310.00248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning State-Augmented Policies for Information Routing in  Communication Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sourajit Das</a>, 
<a href="/search/cs?searchtype=author&query=NaderiAlizadeh%2C+N">Navid NaderiAlizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures, submitted to
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1206">[1206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00290" title="Abstract">arXiv:2310.00290</a> (replaced) [<a href="/pdf/2310.00290" title="Download PDF">pdf</a>, <a href="/ps/2310.00290" title="Download PostScript">ps</a>, <a href="/format/2310.00290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical structure of perfect predictive reservoir computing for  autoregressive type of time series data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Tsuyoshi Yoneda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Analysis of PDEs (math.AP); Dynamical Systems (math.DS); Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item1207">[1207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00797" title="Abstract">arXiv:2310.00797</a> (replaced) [<a href="/pdf/2310.00797" title="Download PDF">pdf</a>, <a href="/format/2310.00797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Beyond Familiar Features for Deep Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaprasad%2C+S">Sarath Sivaprasad</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1208">[1208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01210" title="Abstract">arXiv:2310.01210</a> (replaced) [<a href="/pdf/2310.01210" title="Download PDF">pdf</a>, <a href="/format/2310.01210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Cardiac Segmentation using Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Van+De+Vyver%2C+G">Gilles Van De Vyver</a>, 
<a href="/search/eess?searchtype=author&query=Thomas%2C+S">Sarina Thomas</a>, 
<a href="/search/eess?searchtype=author&query=Ben-Yosef%2C+G">Guy Ben-Yosef</a>, 
<a href="/search/eess?searchtype=author&query=Olaisen%2C+S+H">Sindre Hellum Olaisen</a>, 
<a href="/search/eess?searchtype=author&query=Dalen%2C+H">H&#xe5;vard Dalen</a>, 
<a href="/search/eess?searchtype=author&query=L%C3%B8vstakken%2C+L">Lasse L&#xf8;vstakken</a>, 
<a href="/search/eess?searchtype=author&query=Smistad%2C+E">Erik Smistad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1209">[1209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01339" title="Abstract">arXiv:2310.01339</a> (replaced) [<a href="/pdf/2310.01339" title="Download PDF">pdf</a>, <a href="/format/2310.01339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Dialogue Management: Quality Datasets vs Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medina-Ram%C3%ADrez%2C+M+%C3%81">Miguel &#xc1;ngel Medina-Ram&#xed;rez</a>, 
<a href="/search/cs?searchtype=author&query=Guerra-Artal%2C+C">Cayetano Guerra-Artal</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Tejera%2C+M">Mario Hern&#xe1;ndez-Tejera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1210">[1210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01830" title="Abstract">arXiv:2310.01830</a> (replaced) [<a href="/pdf/2310.01830" title="Download PDF">pdf</a>, <a href="/format/2310.01830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Generated Images as Data Source: The Dawn of Synthetic Era
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+F">Fangneng Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kunhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Muyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1211">[1211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01852" title="Abstract">arXiv:2310.01852</a> (replaced) [<a href="/pdf/2310.01852" title="Download PDF">pdf</a>, <a href="/format/2310.01852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LanguageBind: Extending Video-Language Pretraining to N-modality by  Language-based Semantic Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxi Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">HongFa Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yatian Pang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wancai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1212">[1212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01875" title="Abstract">arXiv:2310.01875</a> (replaced) [<a href="/pdf/2310.01875" title="Download PDF">pdf</a>, <a href="/format/2310.01875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Stable Backdoor Purification through Feature Shift Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+R">Rui Min</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zeyu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minhao Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 paper. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1213">[1213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02374" title="Abstract">arXiv:2310.02374</a> (replaced) [<a href="/pdf/2310.02374" title="Download PDF">pdf</a>, <a href="/format/2310.02374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Health Agents: A Personalized LLM-Powered Agent Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbasian%2C+M">Mahyar Abbasian</a>, 
<a href="/search/cs?searchtype=author&query=Azimi%2C+I">Iman Azimi</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Ramesh Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures, journal paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1214">[1214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02622" title="Abstract">arXiv:2310.02622</a> (replaced) [<a href="/pdf/2310.02622" title="Download PDF">pdf</a>, <a href="/ps/2310.02622" title="Download PostScript">ps</a>, <a href="/format/2310.02622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral vs Energy Efficiency in 6G: Impact of the Receiver Front-End
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lozano%2C+A">Angel Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1215">[1215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03159" title="Abstract">arXiv:2310.03159</a> (replaced) [<a href="/pdf/2310.03159" title="Download PDF">pdf</a>, <a href="/ps/2310.03159" title="Download PostScript">ps</a>, <a href="/format/2310.03159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Auction Algorithms for the Assignment Problem and Extensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertsekas%2C+D">Dimitri Bertsekas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1216">[1216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03368" title="Abstract">arXiv:2310.03368</a> (replaced) [<a href="/pdf/2310.03368" title="Download PDF">pdf</a>, <a href="/format/2310.03368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Hallucinations in Chinese Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mozhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junliang He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mianqiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1217">[1217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03997" title="Abstract">arXiv:2310.03997</a> (replaced) [<a href="/pdf/2310.03997" title="Download PDF">pdf</a>, <a href="/format/2310.03997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RISA: Round-Robin Intra-Rack Friendly Scheduling Algorithm for  Disaggregated Datacenters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabir%2C+R">Rashadul Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+R+G">Ryan G. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Nikdast%2C+M">Mahdi Nikdast</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Clarified some prior work and their citations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1218">[1218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04445" title="Abstract">arXiv:2310.04445</a> (replaced) [<a href="/pdf/2310.04445" title="Download PDF">pdf</a>, <a href="/format/2310.04445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoFT: Local Proxy Fine-tuning For Improving Transferability Of  Adversarial Attacks Against Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+M+A">Muhammad Ahmed Shah</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roshan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Dhamyal%2C+H">Hira Dhamyal</a>, 
<a href="/search/cs?searchtype=author&query=Olivier%2C+R">Raphael Olivier</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Ankit Shah</a>, 
<a href="/search/cs?searchtype=author&query=Konan%2C+J">Joseph Konan</a>, 
<a href="/search/cs?searchtype=author&query=Alharthi%2C+D">Dareen Alharthi</a>, 
<a href="/search/cs?searchtype=author&query=Bukhari%2C+H+T">Hazim T Bukhari</a>, 
<a href="/search/cs?searchtype=author&query=Baali%2C+M">Massa Baali</a>, 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Soham Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Kuhlmann%2C+M">Michael Kuhlmann</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1219">[1219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04928" title="Abstract">arXiv:2310.04928</a> (replaced) [<a href="/pdf/2310.04928" title="Download PDF">pdf</a>, <a href="/format/2310.04928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Only Pass Primary School Exams in Indonesia: A  Comprehensive Test on IndoMMLU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koto%2C+F">Fajri Koto</a>, 
<a href="/search/cs?searchtype=author&query=Aisyah%2C+N">Nurul Aisyah</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1220">[1220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04988" title="Abstract">arXiv:2310.04988</a> (replaced) [<a href="/pdf/2310.04988" title="Download PDF">pdf</a>, <a href="/format/2310.04988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Troubling Emergence of Hallucination in Large Language Models -- An  Extensive Definition, Quantification, and Prescriptive Remediations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rawte%2C+V">Vipula Rawte</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Swagata Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+A">Agnibh Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Anubhav Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Tonmoy%2C+S+M+T+I">S.M Towhidul Islam Tonmoy</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A+P">Amit P. Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1221">[1221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05066" title="Abstract">arXiv:2310.05066</a> (replaced) [<a href="/pdf/2310.05066" title="Download PDF">pdf</a>, <a href="/format/2310.05066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guideline Learning for In-context Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+C">Chaoxu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Q">Qiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1222">[1222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05074" title="Abstract">arXiv:2310.05074</a> (replaced) [<a href="/pdf/2310.05074" title="Download PDF">pdf</a>, <a href="/format/2310.05074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengcheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xiaowei Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Che Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yixin Lian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Ming Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoyuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1223">[1223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05079" title="Abstract">arXiv:2310.05079</a> (replaced) [<a href="/pdf/2310.05079" title="Download PDF">pdf</a>, <a href="/format/2310.05079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM  Inference?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jianyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Constantinides%2C+G+A">George A. Constantinides</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1224">[1224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05280" title="Abstract">arXiv:2310.05280</a> (replaced) [<a href="/pdf/2310.05280" title="Download PDF">pdf</a>, <a href="/format/2310.05280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona  Biases in Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yixin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1225">[1225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05317" title="Abstract">arXiv:2310.05317</a> (replaced) [<a href="/pdf/2310.05317" title="Download PDF">pdf</a>, <a href="/format/2310.05317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Long-form Text Generation Efficacy with Task-adaptive  Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sabour%2C+S">Sahand Sabour</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yilin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the main conference of The 2023 Conference on Empirical Methods in Natural Language Processing; 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing(EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1226">[1226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05349" title="Abstract">arXiv:2310.05349</a> (replaced) [<a href="/pdf/2310.05349" title="Download PDF">pdf</a>, <a href="/format/2310.05349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALECE: An Attention-based Learned Cardinality Estimator for SPJ Queries  on Dynamic Workloads (Extended)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Rong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hua Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> VLDB 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PVLDB, 17(2): 197 - 210, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item1227">[1227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05442" title="Abstract">arXiv:2310.05442</a> (replaced) [<a href="/pdf/2310.05442" title="Download PDF">pdf</a>, <a href="/format/2310.05442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Establishing Trustworthiness: Rethinking Tasks and Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Litschko%2C+R">Robert Litschko</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller-Eberstein%2C+M">Max M&#xfc;ller-Eberstein</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Goot%2C+R">Rob van der Goot</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+L">Leon Weber</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Main Conference), camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1228">[1228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05592" title="Abstract">arXiv:2310.05592</a> (replaced) [<a href="/pdf/2310.05592" title="Download PDF">pdf</a>, <a href="/format/2310.05592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InterroLang: Exploring NLP Models and Datasets through Dialogue-based  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldhus%2C+N">Nils Feldhus</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianli Wang</a>, 
<a href="/search/cs?searchtype=author&query=Anikina%2C+T">Tatiana Anikina</a>, 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Sahil Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Oguz%2C+C">Cennet Oguz</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+S">Sebastian M&#xf6;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings. Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1229">[1229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05597" title="Abstract">arXiv:2310.05597</a> (replaced) [<a href="/pdf/2310.05597" title="Download PDF">pdf</a>, <a href="/format/2310.05597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can language models learn analogical reasoning? Investigating training  objectives and comparisons to human performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petersen%2C+M+R">Molly R. Petersen</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Plas%2C+L">Lonneke van der Plas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1230">[1230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05674" title="Abstract">arXiv:2310.05674</a> (replaced) [<a href="/pdf/2310.05674" title="Download PDF">pdf</a>, <a href="/format/2310.05674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Scalable Meta Learning Practical
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choe%2C+S+K">Sang Keun Choe</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S+V">Sanket Vaibhav Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+H">Hwijeen Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Neiswanger%2C+W">Willie Neiswanger</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Pengtao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Strubell%2C+E">Emma Strubell</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1231">[1231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05703" title="Abstract">arXiv:2310.05703</a> (replaced) [<a href="/pdf/2310.05703" title="Download PDF">pdf</a>, <a href="/format/2310.05703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Attribution Method for Siamese Encoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+L">Lucas M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaev%2C+D">Dmitry Nikolaev</a>, 
<a href="/search/cs?searchtype=author&query=Pad%C3%B3%2C+S">Sebastian Pad&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1232">[1232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05779" title="Abstract">arXiv:2310.05779</a> (replaced) [<a href="/pdf/2310.05779" title="Download PDF">pdf</a>, <a href="/format/2310.05779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Should This Article Be Deleted? Transparent Stance Detection in  Multilingual Wikipedia Editor Discussions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaffee%2C+L">Lucie-Aim&#xe9;e Kaffee</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Arnav Arora</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This submission has been accepted to 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1233">[1233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05784" title="Abstract">arXiv:2310.05784</a> (replaced) [<a href="/pdf/2310.05784" title="Download PDF">pdf</a>, <a href="/format/2310.05784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Parameterised Complexity of Integer Multicommodity Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodlaender%2C+H+L">Hans L. Bodlaender</a>, 
<a href="/search/cs?searchtype=author&query=Mannens%2C+I">Isja Mannens</a>, 
<a href="/search/cs?searchtype=author&query=Oostveen%2C+J+J">Jelle J. Oostveen</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S">Sukanya Pandey</a>, 
<a href="/search/cs?searchtype=author&query=van+Leeuwen%2C+E+J">Erik Jan van Leeuwen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1234">[1234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05951" title="Abstract">arXiv:2310.05951</a> (replaced) [<a href="/pdf/2310.05951" title="Download PDF">pdf</a>, <a href="/format/2310.05951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing the False Positive Rate Using Bayesian Inference in Autonomous  Driving Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Melotti%2C+G">Gledson Melotti</a>, 
<a href="/search/cs?searchtype=author&query=Bastos%2C+J+J+S">Johann J. S. Bastos</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+B+L+S">Bruno L. S. da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Zanotelli%2C+T">Tiago Zanotelli</a>, 
<a href="/search/cs?searchtype=author&query=Premebida%2C+C">Cristiano Premebida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to the journal Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1235">[1235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06374" title="Abstract">arXiv:2310.06374</a> (replaced) [<a href="/pdf/2310.06374" title="Download PDF">pdf</a>, <a href="/format/2310.06374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Model Selection and Decoding for Keyphrase Generation with  Pre-trained Sequence-to-Sequence Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+W+U">Wasi Uddin Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1236">[1236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06404" title="Abstract">arXiv:2310.06404</a> (replaced) [<a href="/pdf/2310.06404" title="Download PDF">pdf</a>, <a href="/format/2310.06404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hexa: Self-Improving for Knowledge-Grounded Dialogue System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+D">Daejin Jo</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+D+W">Daniel Wontae Nam</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+G">Gunsoo Han</a>, 
<a href="/search/cs?searchtype=author&query=On%2C+K">Kyoung-Woon On</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taehwan Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Rho%2C+S">Seungeun Rho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwoong Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1237">[1237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06408" title="Abstract">arXiv:2310.06408</a> (replaced) [<a href="/pdf/2310.06408" title="Download PDF">pdf</a>, <a href="/format/2310.06408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Humans and language models diverge when predicting repeating text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+A+R">Aditya R. Vaidya</a>, 
<a href="/search/cs?searchtype=author&query=Turek%2C+J">Javier Turek</a>, 
<a href="/search/cs?searchtype=author&query=Huth%2C+A+G">Alexander G. Huth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 26th Conference on Computational Natural Language Learning (CoNLL 2023). Code and data are available at <a href="https://github.com/HuthLab/lm-repeating-text">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1238">[1238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06629" title="Abstract">arXiv:2310.06629</a> (replaced) [<a href="/pdf/2310.06629" title="Download PDF">pdf</a>, <a href="/format/2310.06629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yulong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongshuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zengqiang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1239">[1239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06918" title="Abstract">arXiv:2310.06918</a> (replaced) [<a href="/pdf/2310.06918" title="Download PDF">pdf</a>, <a href="/format/2310.06918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+P">Pengyue Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyu Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of emnlp 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1240">[1240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07014" title="Abstract">arXiv:2310.07014</a> (replaced) [<a href="/pdf/2310.07014" title="Download PDF">pdf</a>, <a href="/format/2310.07014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LeakyOhm: Secret Bits Extraction using Impedance Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monfared%2C+S+K">Saleh Khalaj Monfared</a>, 
<a href="/search/cs?searchtype=author&query=Mosavirik%2C+T">Tahoura Mosavirik</a>, 
<a href="/search/cs?searchtype=author&query=Tajik%2C+S">Shahin Tajik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1241">[1241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07081" title="Abstract">arXiv:2310.07081</a> (replaced) [<a href="/pdf/2310.07081" title="Download PDF">pdf</a>, <a href="/format/2310.07081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crossing the Threshold: Idiomatic Machine Translation through Retrieval  Augmentation and Loss Weighting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Emmy Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+A">Aditi Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1242">[1242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07155" title="Abstract">arXiv:2310.07155</a> (replaced) [<a href="/pdf/2310.07155" title="Download PDF">pdf</a>, <a href="/format/2310.07155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;A Tale of Two Movements&quot;: Identifying and Comparing Perspectives in  #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly  Supervised Graph-based Structured Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Shamik Roy</a>, 
<a href="/search/cs?searchtype=author&query=Goldwasser%2C+D">Dan Goldwasser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 (Camera Ready)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1243">[1243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07338" title="Abstract">arXiv:2310.07338</a> (replaced) [<a href="/pdf/2310.07338" title="Download PDF">pdf</a>, <a href="/format/2310.07338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Foundation Models for Learning on Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xumeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1244">[1244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07376" title="Abstract">arXiv:2310.07376</a> (replaced) [<a href="/pdf/2310.07376" title="Download PDF">pdf</a>, <a href="/format/2310.07376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point Cloud Denoising and Outlier Detection with Local Geometric  Structure by Dynamic Graph CNN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakayama%2C+K">Kosuke Nakayama</a>, 
<a href="/search/cs?searchtype=author&query=Fukuta%2C+H">Hiroto Fukuta</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+H">Hiroshi Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE 12th Global Conference on Consumer Electronics (GCCE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1245">[1245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07587" title="Abstract">arXiv:2310.07587</a> (replaced) [<a href="/pdf/2310.07587" title="Download PDF">pdf</a>, <a href="/format/2310.07587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient  Balancer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zikai Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zihan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songshang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jin Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H+H">Howard Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1246">[1246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07602" title="Abstract">arXiv:2310.07602</a> (replaced) [<a href="/pdf/2310.07602" title="Download PDF">pdf</a>, <a href="/format/2310.07602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autonomous  Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Cheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Ziying Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guangqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaofei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qingshan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1247">[1247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07611" title="Abstract">arXiv:2310.07611</a> (replaced) [<a href="/pdf/2310.07611" title="Download PDF">pdf</a>, <a href="/format/2310.07611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in  Self-Refined Open-Source Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shashidhar%2C+S">Sumuk Shashidhar</a>, 
<a href="/search/cs?searchtype=author&query=Chinta%2C+A">Abhinav Chinta</a>, 
<a href="/search/cs?searchtype=author&query=Sahai%2C+V">Vaibhav Sahai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenhailong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1248">[1248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07980" title="Abstract">arXiv:2310.07980</a> (replaced) [<a href="/pdf/2310.07980" title="Download PDF">pdf</a>, <a href="/format/2310.07980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRASP: Accelerating Shortest Path Attacks via Graph Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shafi%2C+Z">Zohair Shafi</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+B+A">Benjamin A. Miller</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Ayan Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Eliassi-Rad%2C+T">Tina Eliassi-Rad</a>, 
<a href="/search/cs?searchtype=author&query=Caceres%2C+R+S">Rajmonda S. Caceres</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1249">[1249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08130" title="Abstract">arXiv:2310.08130</a> (replaced) [<a href="/pdf/2310.08130" title="Download PDF">pdf</a>, <a href="/format/2310.08130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Conversational Decoding via Isotropic and Proximal Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuxuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiling Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1250">[1250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08390" title="Abstract">arXiv:2310.08390</a> (replaced) [<a href="/pdf/2310.08390" title="Download PDF">pdf</a>, <a href="/format/2310.08390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shiyang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zongxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1251">[1251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08395" title="Abstract">arXiv:2310.08395</a> (replaced) [<a href="/pdf/2310.08395" title="Download PDF">pdf</a>, <a href="/format/2310.08395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuanyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weining Qian</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1252">[1252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08421" title="Abstract">arXiv:2310.08421</a> (replaced) [<a href="/pdf/2310.08421" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegLoc: Visual Self-supervised Learning Scheme for Dense Prediction  Tasks of Security Inspection X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halat%2C+S">Shervin Halat</a>, 
<a href="/search/cs?searchtype=author&query=Rahmati%2C+M">Mohammad Rahmati</a>, 
<a href="/search/cs?searchtype=author&query=Nazerfard%2C+E">Ehsan Nazerfard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1253">[1253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08431" title="Abstract">arXiv:2310.08431</a> (replaced) [<a href="/pdf/2310.08431" title="Download PDF">pdf</a>, <a href="/format/2310.08431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Sampling in Hierarchical Exponential-family Energy-based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xingsi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Si Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item1254">[1254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08659" title="Abstract">arXiv:2310.08659</a> (replaced) [<a href="/pdf/2310.08659" title="Download PDF">pdf</a>, <a href="/format/2310.08659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chen Liang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=Karampatziakis%2C+N">Nikos Karampatziakis</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1255">[1255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08740" title="Abstract">arXiv:2310.08740</a> (replaced) [<a href="/pdf/2310.08740" title="Download PDF">pdf</a>, <a href="/format/2310.08740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Zero-Shot Language Agent for Computer Control with Structured  Reflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gang Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bryan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1256">[1256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08808" title="Abstract">arXiv:2310.08808</a> (replaced) [<a href="/e-print/2310.08808" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attacks Meet Interpretability (AmI) Evaluation and Findings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Ziping Ye</a>, 
<a href="/search/cs?searchtype=author&query=Mehnaz%2C+S">Shagufta Mehnaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Need to withdraw it. The current work needs to be changed at a large extent which would take a longer time
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1257">[1257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08996" title="Abstract">arXiv:2310.08996</a> (replaced) [<a href="/pdf/2310.08996" title="Download PDF">pdf</a>, <a href="/format/2310.08996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qualitative Analysis for Validating IEC 62443-4-2 Requirements in  DevSecOps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B6ttel%2C+C">Christian G&#xf6;ttel</a>, 
<a href="/search/cs?searchtype=author&query=Kabir-Querrec%2C+M">Ma&#xeb;lle Kabir-Querrec</a>, 
<a href="/search/cs?searchtype=author&query=Kozhaya%2C+D">David Kozhaya</a>, 
<a href="/search/cs?searchtype=author&query=Sivanthi%2C+T">Thanikesavan Sivanthi</a>, 
<a href="/search/cs?searchtype=author&query=Vukovi%C4%87%2C+O">Ognjen Vukovi&#x107;</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> C. G\"ottel, M. Kabir-Querrec, D. Kozhaya, T. Sivanthi and O.
  Vukovi\'c, "Qualitative Analysis for Validating IEC 62443-4-2 Requirements in
  DevSecOps," ETFA, 2023, pp. 1-8
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1258">[1258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09275" title="Abstract">arXiv:2310.09275</a> (replaced) [<a href="/pdf/2310.09275" title="Download PDF">pdf</a>, <a href="/format/2310.09275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Modeling the Effects of Task and Context on Drivers&#x27;  Gaze Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotseruba%2C+I">Iuliia Kotseruba</a>, 
<a href="/search/cs?searchtype=author&query=Tsotsos%2C+J+K">John K. Tsotsos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1259">[1259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09276" title="Abstract">arXiv:2310.09276</a> (replaced) [<a href="/pdf/2310.09276" title="Download PDF">pdf</a>, <a href="/format/2310.09276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based Multimodal Change Detection with Multitask Consistency  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Biyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M+Y">Michael Ying Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1260">[1260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09343" title="Abstract">arXiv:2310.09343</a> (replaced) [<a href="/pdf/2310.09343" title="Download PDF">pdf</a>, <a href="/format/2310.09343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dialogue Chain-of-Thought Distillation for Commonsense-aware  Conversational Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chae%2C+H">Hyungjoo Chae</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yongho Song</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+K+T">Kai Tzu-iunn Ong</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taeyoon Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minjin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 8 figures, Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1261">[1261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09832" title="Abstract">arXiv:2310.09832</a> (replaced) [<a href="/pdf/2310.09832" title="Download PDF">pdf</a>, <a href="/format/2310.09832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Merging Experts into One: Improving Computational Efficiency of Mixture  of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shwai He</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Run-Ze Fan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1262">[1262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09881" title="Abstract">arXiv:2310.09881</a> (replaced) [<a href="/pdf/2310.09881" title="Download PDF">pdf</a>, <a href="/format/2310.09881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning with Iterative Demonstration Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chengwei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aston Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dagar%2C+A">Anirudh Dagar</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Wenming Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1263">[1263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09943" title="Abstract">arXiv:2310.09943</a> (replaced) [<a href="/pdf/2310.09943" title="Download PDF">pdf</a>, <a href="/format/2310.09943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Robustness of Visual Representations for Object Assembly Task  Requiring Spatio-Geometrical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ku%2C+C">Chahyon Ku</a>, 
<a href="/search/cs?searchtype=author&query=Winge%2C+C">Carl Winge</a>, 
<a href="/search/cs?searchtype=author&query=Diaz%2C+R">Ryan Diaz</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wentao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Desingh%2C+K">Karthik Desingh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1264">[1264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09998" title="Abstract">arXiv:2310.09998</a> (replaced) [<a href="/pdf/2310.09998" title="Download PDF">pdf</a>, <a href="/format/2310.09998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical  Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pham%2C+T">Tan-Hanh Pham</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xianqi Li</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+K">Kim-Doang Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1265">[1265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10041" title="Abstract">arXiv:2310.10041</a> (replaced) [<a href="/pdf/2310.10041" title="Download PDF">pdf</a>, <a href="/ps/2310.10041" title="Download PostScript">ps</a>, <a href="/format/2310.10041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolution quadratures based on block generalized Adams methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+L">Ling Liu</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+J">Junjie Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1266">[1266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10070" title="Abstract">arXiv:2310.10070</a> (replaced) [<a href="/pdf/2310.10070" title="Download PDF">pdf</a>, <a href="/format/2310.10070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GreatSplicing: A Semantically Rich Splicing Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+X">Xiuli Bi</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiaming Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1267">[1267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10195" title="Abstract">arXiv:2310.10195</a> (replaced) [<a href="/pdf/2310.10195" title="Download PDF">pdf</a>, <a href="/format/2310.10195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaLomo: Low-memory Optimization with Adaptive Learning Rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+K">Kai Lv</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Haijun Lv</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fix some typo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1268">[1268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10352" title="Abstract">arXiv:2310.10352</a> (replaced) [<a href="/pdf/2310.10352" title="Download PDF">pdf</a>, <a href="/format/2310.10352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Crowd Counting with Contextual Modeling: Facilitating  Holistic Understanding of Crowd Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yifei Qian</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+X">Xiaopeng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Arandjelovi%C4%87%2C+O">Ognjen Arandjelovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhongliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Donovan%2C+C+R">Carl R.Donovan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1269">[1269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10378" title="Abstract">arXiv:2310.10378</a> (replaced) [<a href="/pdf/2310.10378" title="Download PDF">pdf</a>, <a href="/format/2310.10378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Lingual Consistency of Factual Knowledge in Multilingual Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jirui Qi</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Bisazza%2C+A">Arianna Bisazza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2023 main conference. All code and data are released at <a href="https://github.com/Betswish/Cross-Lingual-Consistency">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1270">[1270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10427" title="Abstract">arXiv:2310.10427</a> (replaced) [<a href="/pdf/2310.10427" title="Download PDF">pdf</a>, <a href="/format/2310.10427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DANAA: Towards transferable attacks with double adversarial neuron  attribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhibo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 19th International Conference on Advanced Data Mining and Applications. (ADMA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1271">[1271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10567" title="Abstract">arXiv:2310.10567</a> (replaced) [<a href="/pdf/2310.10567" title="Download PDF">pdf</a>, <a href="/format/2310.10567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder  for Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jingcheng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1272">[1272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10638" title="Abstract">arXiv:2310.10638</a> (replaced) [<a href="/pdf/2310.10638" title="Download PDF">pdf</a>, <a href="/format/2310.10638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Pretraining: Language Modeling Beyond Document Boundaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Sewon Min</a>, 
<a href="/search/cs?searchtype=author&query=Lomeli%2C+M">Maria Lomeli</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunting Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Margaret Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X+V">Xi Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Yih%2C+S">Scott Yih</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Mike Lewis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1273">[1273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10662" title="Abstract">arXiv:2310.10662</a> (replaced) [<a href="/pdf/2310.10662" title="Download PDF">pdf</a>, <a href="/format/2310.10662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Influence of Different Types of Probing on Adversarial  Decision-Making in a Deception Game
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M+A">Md Abu Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+A+I">Mohammad Ariful Islam Khan</a>, 
<a href="/search/cs?searchtype=author&query=Allsup%2C+B+A">Bryant A Allsup</a>, 
<a href="/search/cs?searchtype=author&query=Zamora%2C+J">Joshua Zamora</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+P">Palvi Aggarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1274">[1274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10698" title="Abstract">arXiv:2310.10698</a> (replaced) [<a href="/e-print/2310.10698" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for  Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yingwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shanshan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yutao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+X">Xiangke Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There may be calculation errors in Table 4 of the paper. We need time to verify and supplement, so the manuscript needs to be withdrawn. Thanks!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1275">[1275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10701" title="Abstract">arXiv:2310.10701</a> (replaced) [<a href="/pdf/2310.10701" title="Download PDF">pdf</a>, <a href="/format/2310.10701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theory of Mind for Multi-Agent Collaboration via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huao Li</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+Y+Q">Yu Quan Chong</a>, 
<a href="/search/cs?searchtype=author&query=Stepputtis%2C+S">Simon Stepputtis</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+J">Joseph Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+D">Dana Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Michael Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Sycara%2C+K">Katia Sycara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1276">[1276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10765" title="Abstract">arXiv:2310.10765</a> (replaced) [<a href="/pdf/2310.10765" title="Download PDF">pdf</a>, <a href="/format/2310.10765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiomedJourney: Counterfactual Biomedical Image Generation by  Instruction-Learning from Multimodal Patient Journeys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Usuyama%2C+N">Naoto Usuyama</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lungren%2C+M+P">Matthew P. Lungren</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+H">Hoifung Poon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page &amp; demo: <a href="https://aka.ms/biomedjourney">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1277">[1277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10837" title="Abstract">arXiv:2310.10837</a> (replaced) [<a href="/pdf/2310.10837" title="Download PDF">pdf</a>, <a href="/format/2310.10837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Two-Layer Feedforward Networks for Efficient Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Csord%C3%A1s%2C+R">R&#xf3;bert Csord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Irie%2C+K">Kazuki Irie</a>, 
<a href="/search/cs?searchtype=author&query=Schmidhuber%2C+J">J&#xfc;rgen Schmidhuber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1278">[1278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10887" title="Abstract">arXiv:2310.10887</a> (replaced) [<a href="/pdf/2310.10887" title="Download PDF">pdf</a>, <a href="/format/2310.10887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Stackage Repository: An Exploratory Study of its Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leger%2C+P">Paul Leger</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+F">Felipe Ruiz</a>, 
<a href="/search/cs?searchtype=author&query=Sep%C3%BAlveda%2C+N">Nicol&#xe1;s Sep&#xfa;lveda</a>, 
<a href="/search/cs?searchtype=author&query=Figueroa%2C+I">Ismael Figueroa</a>, 
<a href="/search/cs?searchtype=author&query=Cardozo%2C+N">Nicol&#xe1;s Cardozo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1279">[1279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10903" title="Abstract">arXiv:2310.10903</a> (replaced) [<a href="/pdf/2310.10903" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent AI-Assisted Discourse: Case Study of a Second Language Writer  Authoring with ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jacob%2C+S">Sharin Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Tate%2C+T">Tamara Tate</a>, 
<a href="/search/cs?searchtype=author&query=Warschauer%2C+M">Mark Warschauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1280">[1280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10975" title="Abstract">arXiv:2310.10975</a> (replaced) [<a href="/pdf/2310.10975" title="Download PDF">pdf</a>, <a href="/format/2310.10975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NICE: Improving Panoptic Narrative Detection and Segmentation with  Cascading Collaborative Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiayi Ji</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tianyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yilong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages. 9 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1281">[1281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11069" title="Abstract">arXiv:2310.11069</a> (replaced) [<a href="/pdf/2310.11069" title="Download PDF">pdf</a>, <a href="/format/2310.11069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waheed%2C+A">Abdul Waheed</a>, 
<a href="/search/cs?searchtype=author&query=Talafha%2C+B">Bashar Talafha</a>, 
<a href="/search/cs?searchtype=author&query=Suvellin%2C+P">Peter Suvellin</a>, 
<a href="/search/cs?searchtype=author&query=Elmadney%2C+A">Abdelrahman Elmadney</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ArabicNLP conference co-located with EMNLP'23. First three authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1282">[1282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11122" title="Abstract">arXiv:2310.11122</a> (replaced) [<a href="/pdf/2310.11122" title="Download PDF">pdf</a>, <a href="/format/2310.11122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensitivity-Aware Amortized Bayesian Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Elsem%C3%BCller%2C+L">Lasse Elsem&#xfc;ller</a>, 
<a href="/search/stat?searchtype=author&query=Olischl%C3%A4ger%2C+H">Hans Olischl&#xe4;ger</a>, 
<a href="/search/stat?searchtype=author&query=Schmitt%2C+M">Marvin Schmitt</a>, 
<a href="/search/stat?searchtype=author&query=B%C3%BCrkner%2C+P">Paul-Christian B&#xfc;rkner</a>, 
<a href="/search/stat?searchtype=author&query=K%C3%B6the%2C+U">Ullrich K&#xf6;the</a>, 
<a href="/search/stat?searchtype=author&query=Radev%2C+S+T">Stefan T. Radev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1283">[1283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11254" title="Abstract">arXiv:2310.11254</a> (replaced) [<a href="/pdf/2310.11254" title="Download PDF">pdf</a>, <a href="/ps/2310.11254" title="Download PostScript">ps</a>, <a href="/format/2310.11254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triangulations Admit Dominating Sets of Size $2n/7$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Christiansen%2C+A+B+G">Aleksander B. G. Christiansen</a>, 
<a href="/search/math?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>, 
<a href="/search/math?searchtype=author&query=Rutschmann%2C+D">Daniel Rutschmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1284">[1284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11368" title="Abstract">arXiv:2310.11368</a> (replaced) [<a href="/pdf/2310.11368" title="Download PDF">pdf</a>, <a href="/format/2310.11368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VECHR: A Dataset for Explainable and Robust Classification of  Vulnerability Type in the European Court of Human Rights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Staufer%2C+L">Leon Staufer</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S+T+Y+S">Santosh T.Y.S.S</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Heri%2C+C">Corina Heri</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1285">[1285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11409" title="Abstract">arXiv:2310.11409</a> (replaced) [<a href="/pdf/2310.11409" title="Download PDF">pdf</a>, <a href="/format/2310.11409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating LLMs for Privilege-Escalation Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Happe%2C+A">Andreas Happe</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+A">Aaron Kaplan</a>, 
<a href="/search/cs?searchtype=author&query=Cito%2C+J">J&#xfc;rgen Cito</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1286">[1286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11594" title="Abstract">arXiv:2310.11594</a> (replaced) [<a href="/pdf/2310.11594" title="Download PDF">pdf</a>, <a href="/format/2310.11594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robustness Unhardening via Backdoor Attacks in Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taejin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiarui Li</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shubhranshu Singh</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+N">Nikhil Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Joe-Wong%2C+C">Carlee Joe-Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 main pages of text, 4 figures, 2 tables. Made for a Neurips workshop on backdoor attacks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1287">[1287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11772" title="Abstract">arXiv:2310.11772</a> (replaced) [<a href="/pdf/2310.11772" title="Download PDF">pdf</a>, <a href="/format/2310.11772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Long Document Topic Segmentation Models With Enhanced  Coherence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023. Codes is available at <a href="https://github.com/alibaba-damo-academy/SpokenNLP/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1288">[1288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11874" title="Abstract">arXiv:2310.11874</a> (replaced) [<a href="/e-print/2310.11874" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some derivations among Logarithmic Space Bounded Counting Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janaki%2C+V">V. Janaki</a>, 
<a href="/search/cs?searchtype=author&query=Madhan%2C+S">S. Madhan</a>, 
<a href="/search/cs?searchtype=author&query=Vijayaraghavan%2C+T+C">T. C. Vijayaraghavan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Expert comments reveal error in the main result claimed by the authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item1289">[1289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11878" title="Abstract">arXiv:2310.11878</a> (replaced) [<a href="/pdf/2310.11878" title="Download PDF">pdf</a>, <a href="/format/2310.11878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Dissonance to Insights: Dissecting Disagreements in Rationale  Construction for Case Outcome Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S+T+Y+S">Santosh T.Y.S.S</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Risini%2C+I">Isabella Risini</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1290">[1290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11960" title="Abstract">arXiv:2310.11960</a> (replaced) [<a href="/pdf/2310.11960" title="Download PDF">pdf</a>, <a href="/format/2310.11960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for  Long Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yanming Kang</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+G">Giang Tran</a>, 
<a href="/search/cs?searchtype=author&query=De+Sterck%2C+H">Hans De Sterck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1291">[1291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12020" title="Abstract">arXiv:2310.12020</a> (replaced) [<a href="/pdf/2310.12020" title="Download PDF">pdf</a>, <a href="/format/2310.12020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic  Tabletop Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wicke%2C+P">Philipp Wicke</a>, 
<a href="/search/cs?searchtype=author&query=%C5%9Eenel%2C+L+K">L&#xfc;tfi Kerem &#x15e;enel</a>, 
<a href="/search/cs?searchtype=author&query=Figueredo%2C+L">Luis Figueredo</a>, 
<a href="/search/cs?searchtype=author&query=Naceri%2C+A">Abdeldjallil Naceri</a>, 
<a href="/search/cs?searchtype=author&query=Haddadin%2C+S">Sami Haddadin</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures. The video and code of LoHoRavens are available at <a href="https://cisnlp.github.io/lohoravens-webpage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1292">[1292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12059" title="Abstract">arXiv:2310.12059</a> (replaced) [<a href="/pdf/2310.12059" title="Download PDF">pdf</a>, <a href="/format/2310.12059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Symbol Binding Ability of Large Language Models for  Multiple-Choice Questions in Vietnamese General Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Duc-Vu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quoc-Nam Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SoICT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1293">[1293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12074" title="Abstract">arXiv:2310.12074</a> (replaced) [<a href="/pdf/2310.12074" title="Download PDF">pdf</a>, <a href="/format/2310.12074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Safer Operations: An Expert-involved Dataset of High-Pressure  Gas Incidents for Preventing Future Failures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Inoue%2C+S">Shumpei Inoue</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M">Minh-Tien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mizokuchi%2C+H">Hiroki Mizokuchi</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+D">Tuan-Anh D. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Huu-Hiep Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D+T">Dung Tien Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (The Industry Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1294">[1294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12182" title="Abstract">arXiv:2310.12182</a> (replaced) [<a href="/pdf/2310.12182" title="Download PDF">pdf</a>, <a href="/format/2310.12182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block-Wise Mixed-Precision Quantization: Enabling High Efficiency for  Practical ReRAM-based DNN Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xueying Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hanson%2C+E">Edward Hanson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nansu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoxuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huanrui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Feng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pande%2C+P+P">Partha Pratim Pande</a>, 
<a href="/search/cs?searchtype=author&query=Doppa%2C+J+R">Janardhan Rao Doppa</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+K">Krishnendu Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hai Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item1295">[1295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12184" title="Abstract">arXiv:2310.12184</a> (replaced) [<a href="/pdf/2310.12184" title="Download PDF">pdf</a>, <a href="/format/2310.12184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architectural Implications of GNN Aggregation Programming Abstractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yingjie Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianlei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Ao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+T">Tong Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chunming Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, to be published in IEEE Computer Architecture Letters (CAL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1296">[1296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12214" title="Abstract">arXiv:2310.12214</a> (replaced) [<a href="/pdf/2310.12214" title="Download PDF">pdf</a>, <a href="/format/2310.12214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PrivInfer: Privacy-Preserving Inference for Black-box Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+M">Meng Tong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kejiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1297">[1297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12305" title="Abstract">arXiv:2310.12305</a> (replaced) [<a href="/pdf/2310.12305" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Random, Fair, and Verifiable Games on Blockchain. Raffle smart  contract designs on Sui Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Justa Liang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ray Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pierce Hung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Damien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+A">Ashley Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chalkias%2C+K">Konstantinos Chalkias</a>, 
<a href="/search/cs?searchtype=author&query=Pleros%2C+S">Stefanos Pleros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1298">[1298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12403" title="Abstract">arXiv:2310.12403</a> (replaced) [<a href="/pdf/2310.12403" title="Download PDF">pdf</a>, <a href="/format/2310.12403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Minibatching in Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balin%2C+M+F">Muhammed Fatih Balin</a>, 
<a href="/search/cs?searchtype=author&query=LaSalle%2C+D">Dominique LaSalle</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87ataly%C3%BCrek%2C+%C3%9C+V">&#xdc;mit V. &#xc7;ataly&#xfc;rek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1299">[1299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12487" title="Abstract">arXiv:2310.12487</a> (replaced) [<a href="/pdf/2310.12487" title="Download PDF">pdf</a>, <a href="/format/2310.12487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Operator Learning by Orthogonal Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zipeng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhongkai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bokai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1300">[1300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12580" title="Abstract">arXiv:2310.12580</a> (replaced) [<a href="/pdf/2310.12580" title="Download PDF">pdf</a>, <a href="/format/2310.12580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretraining Language Models with Text-Attributed Heterogeneous Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+T">Tao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Le Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yifei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Leilei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bowen Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1301">[1301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12648" title="Abstract">arXiv:2310.12648</a> (replaced) [<a href="/pdf/2310.12648" title="Download PDF">pdf</a>, <a href="/format/2310.12648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Real-World Streaming Speech Translation for Code-Switched Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alastruey%2C+B">Belen Alastruey</a>, 
<a href="/search/cs?searchtype=author&query=Sperber%2C+M">Matthias Sperber</a>, 
<a href="/search/cs?searchtype=author&query=Gollan%2C+C">Christian Gollan</a>, 
<a href="/search/cs?searchtype=author&query=Telaar%2C+D">Dominic Telaar</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+T">Tim Ng</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Aashish Agarwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1302">[1302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12790" title="Abstract">arXiv:2310.12790</a> (replaced) [<a href="/pdf/2310.12790" title="Download PDF">pdf</a>, <a href="/format/2310.12790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiawen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Choubo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1303">[1303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12823" title="Abstract">arXiv:2310.12823</a> (replaced) [<a href="/pdf/2310.12823" title="Download PDF">pdf</a>, <a href="/format/2310.12823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentTuning: Enabling Generalized Agent Abilities for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Aohan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingdao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Rui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1304">[1304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12874" title="Abstract">arXiv:2310.12874</a> (replaced) [<a href="/pdf/2310.12874" title="Download PDF">pdf</a>, <a href="/format/2310.12874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StoryAnalogy: Deriving Story-level Analogies from Large Language Models  to Unlock Analogical Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiayang%2C+C">Cheng Jiayang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lin Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+H">Tsz Ho Chan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tianqing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Chunkit Chan</a>, 
<a href="/search/cs?searchtype=author&query=Ru%2C+D">Dongyu Ru</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1305">[1305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12877" title="Abstract">arXiv:2310.12877</a> (replaced) [<a href="/e-print/2310.12877" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Assessment and Optimization of High Dynamic Range Image  Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cao%2C+P">Peibei Cao</a>, 
<a href="/search/eess?searchtype=author&query=Mantiuk%2C+R+K">Rafal K. Mantiuk</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+K">Kede Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> need more changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1306">[1306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12934" title="Abstract">arXiv:2310.12934</a> (replaced) [<a href="/pdf/2310.12934" title="Download PDF">pdf</a>, <a href="/format/2310.12934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Flow Networks as Entropy-Regularized RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiapkin%2C+D">Daniil Tiapkin</a>, 
<a href="/search/cs?searchtype=author&query=Morozov%2C+N">Nikita Morozov</a>, 
<a href="/search/cs?searchtype=author&query=Naumov%2C+A">Alexey Naumov</a>, 
<a href="/search/cs?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1307">[1307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12936" title="Abstract">arXiv:2310.12936</a> (replaced) [<a href="/pdf/2310.12936" title="Download PDF">pdf</a>, <a href="/format/2310.12936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Predictive Factor Analysis of Social Biases and Task-Performance in  Pretrained Masked Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>, 
<a href="/search/cs?searchtype=author&query=Bollegala%2C+D">Danushka Bollegala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1308">[1308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12942" title="Abstract">arXiv:2310.12942</a> (replaced) [<a href="/pdf/2310.12942" title="Download PDF">pdf</a>, <a href="/format/2310.12942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Representational Capacity of Recurrent Neural Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nowak%2C+F">Franz Nowak</a>, 
<a href="/search/cs?searchtype=author&query=Svete%2C+A">Anej Svete</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Li Du</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at EMNLP 2023;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1309">[1309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12953" title="Abstract">arXiv:2310.12953</a> (replaced) [<a href="/pdf/2310.12953" title="Download PDF">pdf</a>, <a href="/format/2310.12953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Generation and Exploration of Design Space with Large  Language Models for Human-AI Co-Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suh%2C+S">Sangho Suh</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Meng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+B">Bryan Min</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1310">[1310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13012" title="Abstract">arXiv:2310.13012</a> (replaced) [<a href="/pdf/2310.13012" title="Download PDF">pdf</a>, <a href="/format/2310.13012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> H2O Open Ecosystem for State-of-the-art Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Candel%2C+A">Arno Candel</a>, 
<a href="/search/cs?searchtype=author&query=McKinney%2C+J">Jon McKinney</a>, 
<a href="/search/cs?searchtype=author&query=Singer%2C+P">Philipp Singer</a>, 
<a href="/search/cs?searchtype=author&query=Pfeiffer%2C+P">Pascal Pfeiffer</a>, 
<a href="/search/cs?searchtype=author&query=Jeblick%2C+M">Maximilian Jeblick</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C+M">Chun Ming Lee</a>, 
<a href="/search/cs?searchtype=author&query=Conde%2C+M+V">Marcos V. Conde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Demo - ACL Empirical Methods in Natural Language Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1311">[1311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13098" title="Abstract">arXiv:2310.13098</a> (replaced) [<a href="/pdf/2310.13098" title="Download PDF">pdf</a>, <a href="/format/2310.13098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SRAI: Towards Standardization of Geospatial AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gramacki%2C+P">Piotr Gramacki</a>, 
<a href="/search/cs?searchtype=author&query=Le%C5%9Bniara%2C+K">Kacper Le&#x15b;niara</a>, 
<a href="/search/cs?searchtype=author&query=Raczycki%2C+K">Kamil Raczycki</a>, 
<a href="/search/cs?searchtype=author&query=Wo%C5%BAniak%2C+S">Szymon Wo&#x17a;niak</a>, 
<a href="/search/cs?searchtype=author&query=Przymus%2C+M">Marcin Przymus</a>, 
<a href="/search/cs?searchtype=author&query=Szyma%C5%84ski%2C+P">Piotr Szyma&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery (GeoAI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1312">[1312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13132" title="Abstract">arXiv:2310.13132</a> (replaced) [<a href="/pdf/2310.13132" title="Download PDF">pdf</a>, <a href="/format/2310.13132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better to Ask in English: Cross-Lingual Evaluation of Large Language  Models for Healthcare Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yiqiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+M">Mohit Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+G">Gaurav Verma</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yibo Hu</a>, 
<a href="/search/cs?searchtype=author&query=De+Choudhury%2C+M">Munmun De Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1313">[1313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13141" title="Abstract">arXiv:2310.13141</a> (replaced) [<a href="/pdf/2310.13141" title="Download PDF">pdf</a>, <a href="/ps/2310.13141" title="Download PostScript">ps</a>, <a href="/format/2310.13141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impartial Rank Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cembrano%2C+J">Javier Cembrano</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+F">Felix Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Klimm%2C+M">Max Klimm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1314">[1314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13189" title="Abstract">arXiv:2310.13189</a> (replaced) [<a href="/pdf/2310.13189" title="Download PDF">pdf</a>, <a href="/format/2310.13189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Accurate Factual Inconsistency Detection Over Long Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lattimer%2C+B+M">Barrett Martin Lattimer</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Patrick Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in EMNLP 2023 Main Conference, 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1315">[1315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13198" title="Abstract">arXiv:2310.13198</a> (replaced) [<a href="/pdf/2310.13198" title="Download PDF">pdf</a>, <a href="/format/2310.13198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Car Model Identification System for Streamlining the Automobile Sales  Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Togru%2C+S">Said Togru</a>, 
<a href="/search/cs?searchtype=author&query=Moldovan%2C+M">Marco Moldovan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1316">[1316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13321" title="Abstract">arXiv:2310.13321</a> (replaced) [<a href="/pdf/2310.13321" title="Download PDF">pdf</a>, <a href="/format/2310.13321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Hard Samples: Robust and Effective Grammatical Error Correction  with Cycle Self-Augmenting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zecheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+K">Kaifeng Qi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juntao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1317">[1317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13469" title="Abstract">arXiv:2310.13469</a> (replaced) [<a href="/pdf/2310.13469" title="Download PDF">pdf</a>, <a href="/format/2310.13469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Language Model to Clean Your Noisy Translation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bolding%2C+Q">Quinten Bolding</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Denis%2C+B+J">Brandon James Denis</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1318">[1318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13479" title="Abstract">arXiv:2310.13479</a> (replaced) [<a href="/pdf/2310.13479" title="Download PDF">pdf</a>, <a href="/format/2310.13479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment, Select, Correct: A Framework for Weakly-Supervised Referring  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eiras%2C+F">Francisco Eiras</a>, 
<a href="/search/cs?searchtype=author&query=Oksuz%2C+K">Kemal Oksuz</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Dokania%2C+P+K">Puneet K. Dokania</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1319">[1319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13512" title="Abstract">arXiv:2310.13512</a> (replaced) [<a href="/pdf/2310.13512" title="Download PDF">pdf</a>, <a href="/format/2310.13512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Question Generation with Multi-level Content Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+Z">Zehua Xia</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+Q">Qi Gou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C">Cam-Tu Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready. Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1320">[1320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13552" title="Abstract">arXiv:2310.13552</a> (replaced) [<a href="/pdf/2310.13552" title="Download PDF">pdf</a>, <a href="/format/2310.13552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-prompted Chain-of-Thought on Large Language Models for Open-domain  Multi-hop Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1321">[1321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13573" title="Abstract">arXiv:2310.13573</a> (replaced) [<a href="/pdf/2310.13573" title="Download PDF">pdf</a>, <a href="/format/2310.13573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Generalization with Adaptive Style Techniques for Fingerprint  Liveness Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kexin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yule%2C+A">Adam Yule</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiajun Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1st Place in LivDet2023 Fingerprint Representation Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1322">[1322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13664" title="Abstract">arXiv:2310.13664</a> (replaced) [<a href="/pdf/2310.13664" title="Download PDF">pdf</a>, <a href="/format/2310.13664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Depression Symptom Detection in Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Souto%2C+E+B">Eliseo Bao Souto</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+A">Anxo P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Parapar%2C+J">Javier Parapar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1323">[1323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13678" title="Abstract">arXiv:2310.13678</a> (replaced) [<a href="/pdf/2310.13678" title="Download PDF">pdf</a>, <a href="/format/2310.13678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Form Speech Translation through Segmentation with Finite-State  Decoding Constraints on Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+A+D">Arya D. McCarthy</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Shankar Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Stahlberg%2C+F">Felix Stahlberg</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Ke Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to the Findings of EMNLP 2023. arXiv admin note: text overlap with <a href="/abs/2212.09895">arXiv:2212.09895</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1324">[1324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13683" title="Abstract">arXiv:2310.13683</a> (replaced) [<a href="/pdf/2310.13683" title="Download PDF">pdf</a>, <a href="/format/2310.13683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP  Performance on Low-Resource Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+G+O+d">Gabriel Oliveira dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Moreira%2C+D+A+B">Diego A. B. Moreira</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+A+I">Alef Iury Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+J">Jhessica Silva</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+L">Luiz Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Bueno%2C+P">Pedro Bueno</a>, 
<a href="/search/cs?searchtype=author&query=Sousa%2C+T">Thiago Sousa</a>, 
<a href="/search/cs?searchtype=author&query=Maia%2C+H">Helena Maia</a>, 
<a href="/search/cs?searchtype=author&query=Da+Silva%2C+N">N&#xe1;dia Da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Colombini%2C+E">Esther Colombini</a>, 
<a href="/search/cs?searchtype=author&query=Pedrini%2C+H">Helio Pedrini</a>, 
<a href="/search/cs?searchtype=author&query=Avila%2C+S">Sandra Avila</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item724">Cross-lists</a></li>
<li><a href="#item796">Replacements</a></li>
</ul>
<small>[ total of 1324 entries:  <b>1-1324</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
