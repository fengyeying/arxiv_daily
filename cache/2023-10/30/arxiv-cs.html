<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Thu 26 Oct 23  to  Fri 27 Oct 23, announced Mon, 30 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item258">Cross-lists</a></li>
<li><a href="#item300">Replacements</a></li>
</ul>
<small>[ total of 546 entries:  <b>1-546</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Mon, 30 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17654" title="Abstract">arXiv:2310.17654</a> [<a href="/pdf/2310.17654" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACWA: An AI-driven Cyber-Physical Testbed for Intelligent Water Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Batarseh%2C+F+A">Feras A. Batarseh</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Ajay Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Sreng%2C+C">Chhayly Sreng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Justice Lin</a>, 
<a href="/search/cs?searchtype=author&query=Maksud%2C+S">Siam Maksud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This manuscript presents a novel state-of-the-art cyber-physical water
testbed, namely: The AI and Cyber for Water and Agriculture testbed (ACWA).
ACWA is motivated by the need to advance water supply management using AI and
Cybersecurity experimentation. The main goal of ACWA is to address pressing
challenges in the water and agricultural domains by utilising cutting-edge AI
and data-driven technologies. These challenges include Cyberbiosecurity,
resources management, access to water, sustainability, and data-driven
decision-making, among others. To address such issues, ACWA consists of
multiple topologies, sensors, computational nodes, pumps, tanks, smart water
devices, as well as databases and AI models that control the system. Moreover,
we present ACWA simulator, which is a software-based water digital twin. The
simulator runs on fluid and constituent transport principles that produce
theoretical time series of a water distribution system. This creates a good
validation point for comparing the theoretical approach with real-life results
via the physical ACWA testbed. ACWA data are available to AI and water domain
researchers and are hosted in an online public repository. In this paper, the
system is introduced in detail and compared with existing water testbeds;
additionally, example use-cases are described along with novel outcomes such as
datasets, software, and AI-related scenarios.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17658" title="Abstract">arXiv:2310.17658</a> [<a href="/pdf/2310.17658" title="Download PDF">pdf</a>, <a href="/format/2310.17658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Channel Independent strategy optimal for Time Series Forecasting?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peiwen%2C+Y">Yuan Peiwen</a>, 
<a href="/search/cs?searchtype=author&query=Changsheng%2C+Z">Zhu Changsheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">There has been an emergence of various models for long-term time series
forecasting. Recent studies have demonstrated that a single linear layer, using
Channel Dependent (CD) or Channel Independent (CI) modeling, can even
outperform a large number of sophisticated models. However, current research
primarily considers CD and CI as two complementary yet mutually exclusive
approaches, unable to harness these two extremes simultaneously. And it is also
a challenging issue that both CD and CI are static strategies that cannot be
determined to be optimal for a specific dataset without extensive experiments.
In this paper, we reconsider whether the current CI strategy is the best
solution for time series forecasting. First, we propose a simple yet effective
strategy called CSC, which stands for $\mathbf{C}$hannel
$\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel
Self-Clustering (CSC) enhances CI strategy's performance improvements while
reducing parameter size, for exmpale by over 10 times on electricity dataset,
and significantly cutting training time. Second, we further propose Channel
Rearrangement (CR), a method for deep models inspired by the self-clustering.
CR attains competitive performance against baselines. Finally, we also discuss
whether it is best to forecast the future values using the historical values of
the same channel as inputs. We hope our findings and methods could inspire new
solutions beyond CD/CI.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17664" title="Abstract">arXiv:2310.17664</a> [<a href="/pdf/2310.17664" title="Download PDF">pdf</a>, <a href="/format/2310.17664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascaded Multi-task Adaptive Learning Based on Neural Architecture  Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yingying Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shilei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Z">Zihao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Junlan Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">Cascading multiple pre-trained models is an effective way to compose an
end-to-end system. However, fine-tuning the full cascaded model is parameter
and memory inefficient and our observations reveal that only applying adapter
modules on cascaded model can not achieve considerable performance as
fine-tuning. We propose an automatic and effective adaptive learning method to
optimize end-to-end cascaded multi-task models based on Neural Architecture
Search (NAS) framework. The candidate adaptive operations on each specific
module consist of frozen, inserting an adapter and fine-tuning. We further add
a penalty item on the loss to limit the learned structure which takes the
amount of trainable parameters into account. The penalty item successfully
restrict the searched architecture and the proposed approach is able to search
similar tuning scheme with hand-craft, compressing the optimizing parameters to
8.7% corresponding to full fine-tuning on SLURP with an even better
performance.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17668" title="Abstract">arXiv:2310.17668</a> [<a href="/pdf/2310.17668" title="Download PDF">pdf</a>, <a href="/format/2310.17668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine tuning Pre trained Models for Robustness Under Noisy Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sumyeong Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sihyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Jongwoo Ko</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages (17 pages including supplementary)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The presence of noisy labels in a training dataset can significantly impact
the performance of machine learning models. To tackle this issue, researchers
have explored methods for Learning with Noisy Labels to identify clean samples
and reduce the influence of noisy labels. However, constraining the influence
of a certain portion of the training dataset can result in a reduction in
overall generalization performance. To alleviate this, recent studies have
considered the careful utilization of noisy labels by leveraging huge
computational resources. Therefore, the increasing training cost necessitates a
reevaluation of efficiency. In other areas of research, there has been a focus
on developing fine-tuning techniques for large pre-trained models that aim to
achieve both high generalization performance and efficiency. However, these
methods have mainly concentrated on clean datasets, and there has been limited
exploration of the noisy label scenario. In this research, our aim is to find
an appropriate way to fine-tune pre-trained models for noisy labeled datasets.
To achieve this goal, we investigate the characteristics of pre-trained models
when they encounter noisy datasets. Through empirical analysis, we introduce a
novel algorithm called TURN, which robustly and efficiently transfers the prior
knowledge of pre-trained models. The algorithm consists of two main steps: (1)
independently tuning the linear classifier to protect the feature extractor
from being distorted by noisy labels, and (2) reducing the noisy label ratio
and fine-tuning the entire model based on the noise-reduced dataset to adapt it
to the target dataset. The proposed algorithm has been extensively tested and
demonstrates efficient yet improved denoising performance on various benchmarks
compared to previous methods.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17669" title="Abstract">arXiv:2310.17669</a> [<a href="/pdf/2310.17669" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Approach for Efficient Neural Architecture Search Space Definition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pouy%2C+L">L&#xe9;o Pouy</a> (ESTACA&#x27;Lab), 
<a href="/search/cs?searchtype=author&query=Khenfri%2C+F">Fouad Khenfri</a> (ESTACA&#x27;Lab), 
<a href="/search/cs?searchtype=author&query=Leserf%2C+P">Patrick Leserf</a> (ESTACA&#x27;Lab), 
<a href="/search/cs?searchtype=author&query=Mraidha%2C+C">Chokri Mraidha</a> (LIST (CEA)), 
<a href="/search/cs?searchtype=author&query=Larouci%2C+C">Cherif Larouci</a> (ESTACA&#x27;Lab)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI-22 Workshop: Learning Network Architecture During Training, Feb 2022, Online, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">As we advance in the fast-growing era of Machine Learning, various new and
more complex neural architectures are arising to tackle problem more
efficiently. On the one hand their efficient usage requires advanced knowledge
and expertise, which is most of the time difficult to find on the labor market.
On the other hand, searching for an optimized neural architecture is a
time-consuming task when it is performed manually using a trial and error
approach. Hence, a method and a tool support is needed to assist users of
neural architectures, leading to an eagerness in the field of Automatic Machine
Learning (AutoML). When it comes to Deep Learning, an important part of AutoML
is the Neural Architecture Search (NAS). In this paper, we propose a novel
cell-based hierarchical search space, easy to comprehend and manipulate. The
objectives of the proposed approach are to optimize the search-time and to be
general enough to handle most of state of the art Convolutional Neural Networks
(CNN) architectures.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17670" title="Abstract">arXiv:2310.17670</a> [<a href="/pdf/2310.17670" title="Download PDF">pdf</a>, <a href="/ps/2310.17670" title="Download PostScript">ps</a>, <a href="/format/2310.17670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unknown Health States Recognition With Collective Decision Based Deep  Learning Networks In Predictive Maintenance Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+C">Chuyue Lou</a>, 
<a href="/search/cs?searchtype=author&query=Atoui%2C+M+A">M. Amine Atoui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">At present, decision making solutions developed based on deep learning (DL)
models have received extensive attention in predictive maintenance (PM)
applications along with the rapid improvement of computing power. Relying on
the superior properties of shared weights and spatial pooling, Convolutional
Neural Network (CNN) can learn effective representations of health states from
industrial data. Many developed CNN-based schemes, such as advanced CNNs that
introduce residual learning and multi-scale learning, have shown good
performance in health state recognition tasks under the assumption that all the
classes are known. However, these schemes have no ability to deal with new
abnormal samples that belong to state classes not part of the training set. In
this paper, a collective decision framework for different CNNs is proposed. It
is based on a One-vs-Rest network (OVRN) to simultaneously achieve
classification of known and unknown health states. OVRN learn state-specific
discriminative features and enhance the ability to reject new abnormal samples
incorporated to different CNNs. According to the validation results on the
public dataset of Tennessee Eastman Process (TEP), the proposed CNN-based
decision schemes incorporating OVRN have outstanding recognition ability for
samples of unknown heath states, while maintaining satisfactory accuracy on
known states. The results show that the new DL framework outperforms
conventional CNNs, and the one based on residual and multi-scale learning has
the best overall performance.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17671" title="Abstract">arXiv:2310.17671</a> [<a href="/pdf/2310.17671" title="Download PDF">pdf</a>, <a href="/format/2310.17671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer of Reinforcement Learning-Based Controllers from Model- to  Hardware-in-the-Loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Picerno%2C+M">Mario Picerno</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+L">Lucas Koch</a>, 
<a href="/search/cs?searchtype=author&query=Badalian%2C+K">Kevin Badalian</a>, 
<a href="/search/cs?searchtype=author&query=Wegener%2C+M">Marius Wegener</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+J">Joschka Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Koch%2C+C+R">Charles Robert Koch</a>, 
<a href="/search/cs?searchtype=author&query=Andert%2C+J">Jakob Andert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The process of developing control functions for embedded systems is
resource-, time-, and data-intensive, often resulting in sub-optimal cost and
solutions approaches. Reinforcement Learning (RL) has great potential for
autonomously training agents to perform complex control tasks with minimal
human intervention. Due to costly data generation and safety constraints,
however, its application is mostly limited to purely simulated domains. To use
RL effectively in embedded system function development, the generated agents
must be able to handle real-world applications. In this context, this work
focuses on accelerating the training process of RL agents by combining Transfer
Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient
exhaust gas re-circulation control for an internal combustion engine, use of a
computationally cheap Model-in-the-Loop (MiL) simulation is made to select a
suitable algorithm, fine-tune hyperparameters, and finally train candidate
agents for the transfer. These pre-trained RL agents are then fine-tuned in a
Hardware-in-the-Loop (HiL) system via TL. The transfer revealed the need for
adjusting the reward parameters when advancing to real hardware. Further, the
comparison between a purely HiL-trained and a transferred agent showed a
reduction of training time by a factor of 5.9. The results emphasize the
necessity to train RL agents with real hardware, and demonstrate that the
maturity of the transferred policies affects both training time and
performance, highlighting the strong synergies between TL and XiL simulation.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17674" title="Abstract">arXiv:2310.17674</a> [<a href="/pdf/2310.17674" title="Download PDF">pdf</a>, <a href="/format/2310.17674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Text Spotter for Joint Text Spotting and Layout Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+S">Shangbang Long</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Siyang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Fujii%2C+Y">Yasuhisa Fujii</a>, 
<a href="/search/cs?searchtype=author&query=Bissacco%2C+A">Alessandro Bissacco</a>, 
<a href="/search/cs?searchtype=author&query=Raptis%2C+M">Michalis Raptis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose Hierarchical Text Spotter (HTS), a novel method for the joint task
of word-level text spotting and geometric layout analysis. HTS can recognize
text in an image and identify its 4-level hierarchical structure: characters,
words, lines, and paragraphs. The proposed HTS is characterized by two novel
components: (1) a Unified-Detector-Polygon (UDP) that produces Bezier Curve
polygons of text lines and an affinity matrix for paragraph grouping between
detected lines; (2) a Line-to-Character-to-Word (L2C2W) recognizer that splits
lines into characters and further merges them back into words. HTS achieves
state-of-the-art results on multiple word-level text spotting benchmark
datasets as well as geometric layout analysis tasks.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17678" title="Abstract">arXiv:2310.17678</a> [<a href="/pdf/2310.17678" title="Download PDF">pdf</a>, <a href="/format/2310.17678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatio-Temporal Meta Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiabin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32nd ACM International Conference on Information and Knowledge Management (CIKM' 23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spatio-temporal prediction is crucial in numerous real-world applications,
including traffic forecasting and crime prediction, which aim to improve public
transportation and safety management. Many state-of-the-art models demonstrate
the strong capability of spatio-temporal graph neural networks (STGNN) to
capture complex spatio-temporal correlations. However, despite their
effectiveness, existing approaches do not adequately address several key
challenges. Data quality issues, such as data scarcity and sparsity, lead to
data noise and a lack of supervised signals, which significantly limit the
performance of STGNN. Although recent STGNN models with contrastive learning
aim to address these challenges, most of them use pre-defined augmentation
strategies that heavily depend on manual design and cannot be customized for
different Spatio-Temporal Graph (STG) scenarios. To tackle these challenges, we
propose a new spatio-temporal contrastive learning (CL4ST) framework to encode
robust and generalizable STG representations via the STG augmentation paradigm.
Specifically, we design the meta view generator to automatically construct node
and edge augmentation views for each disentangled spatial and temporal graph in
a data-driven manner. The meta view generator employs meta networks with
parameterized generative model to customize the augmentations for each input.
This personalizes the augmentation strategies for every STG and endows the
learning framework with spatio-temporal-aware information. Additionally, we
integrate a unified spatio-temporal graph attention network with the proposed
meta view generator and two-branch graph contrastive learning paradigms.
Extensive experiments demonstrate that our CL4ST significantly improves
performance over various state-of-the-art baselines in traffic and crime
prediction.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17679" title="Abstract">arXiv:2310.17679</a> [<a href="/pdf/2310.17679" title="Download PDF">pdf</a>, <a href="/ps/2310.17679" title="Download PostScript">ps</a>, <a href="/format/2310.17679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score  Search and Grow-Shrink Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrews%2C+B">Bryan Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Ramsey%2C+J">Joseph Ramsey</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez-Romero%2C+R">Ruben Sanchez-Romero</a>, 
<a href="/search/cs?searchtype=author&query=Camchong%2C+J">Jazmin Camchong</a>, 
<a href="/search/cs?searchtype=author&query=Kummerfeld%2C+E">Erich Kummerfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
<p class="mathjax">Learning graphical conditional independence structures is an important
machine learning problem and a cornerstone of causal discovery. However, the
accuracy and execution time of learning algorithms generally struggle to scale
to problems with hundreds of highly connected variables -- for instance,
recovering brain networks from fMRI data. We introduce the best order score
search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs
(DAGs) in this paradigm. BOSS greedily searches over permutations of variables,
using GSTs to construct and score DAGs from permutations. GSTs efficiently
cache scores to eliminate redundant calculations. BOSS achieves
state-of-the-art performance in accuracy and execution time, comparing
favorably to a variety of combinatorial and gradient-based learning algorithms
under a broad range of conditions. To demonstrate its practicality, we apply
BOSS to two sets of resting-state fMRI data: simulated data with
pseudo-empirical noise distributions derived from randomized empirical fMRI
cortical signals and clinical data from 3T fMRI scans processed into cortical
parcels. BOSS is available for use within the TETRAD project which includes
Python and R wrappers.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17680" title="Abstract">arXiv:2310.17680</a> [<a href="/pdf/2310.17680" title="Download PDF">pdf</a>, <a href="/format/2310.17680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeFusion: A Pre-trained Diffusion Model for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
<p class="mathjax">Imagine a developer who can only change their last line of code, how often
would they have to start writing a function from scratch before it is correct?
Auto-regressive models for code generation from natural language have a similar
limitation: they do not easily allow reconsidering earlier tokens generated. We
introduce CodeFusion, a pre-trained diffusion code generation model that
addresses this limitation by iteratively denoising a complete program
conditioned on the encoded natural language. We evaluate CodeFusion on the task
of natural language to code generation for Bash, Python, and Microsoft Excel
conditional formatting (CF) rules. Experiments show that CodeFusion (75M
parameters) performs on par with state-of-the-art auto-regressive systems
(350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and
top-5 accuracy due to its better balance in diversity versus quality.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17683" title="Abstract">arXiv:2310.17683</a> [<a href="/pdf/2310.17683" title="Download PDF">pdf</a>, <a href="/format/2310.17683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sliceformer: Make Multi-head Attention as Simple as Sorting in  Discriminative Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shen Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongteng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As one of the most popular neural network modules, Transformer plays a
central role in many fundamental deep learning models, e.g., the ViT in
computer vision and the BERT and GPT in natural language processing. The
effectiveness of the Transformer is often attributed to its multi-head
attention (MHA) mechanism. In this study, we discuss the limitations of MHA,
including the high computational complexity due to its ``query-key-value''
architecture and the numerical issue caused by its softmax operation.
Considering the above problems and the recent development tendency of the
attention layer, we propose an effective and efficient surrogate of the
Transformer, called Sliceformer. Our Sliceformer replaces the classic MHA
mechanism with an extremely simple ``slicing-sorting'' operation, i.e.,
projecting inputs linearly to a latent space and sorting them along different
feature dimensions (or equivalently, called channels). For each feature
dimension, the sorting operation implicitly generates an implicit attention map
with sparse, full-rank, and doubly-stochastic structures. We consider different
implementations of the slicing-sorting operation and analyze their impacts on
the Sliceformer. We test the Sliceformer in the Long-Range Arena benchmark,
image classification, text classification, and molecular property prediction,
demonstrating its advantage in computational complexity and universal
effectiveness in discriminative tasks. Our Sliceformer achieves comparable or
better performance with lower memory cost and faster speed than the Transformer
and its variants. Moreover, the experimental results reveal that applying our
Sliceformer can empirically suppress the risk of mode collapse when
representing data. The code is available at
\url{https://github.com/SDS-Lab/sliceformer}.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17684" title="Abstract">arXiv:2310.17684</a> [<a href="/pdf/2310.17684" title="Download PDF">pdf</a>, <a href="/format/2310.17684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEI: Livestock Event Information Schema for Enabling Data Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habib%2C+M">Mahir Habib</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+A">Muhammad Ashad Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lihong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=McGrath%2C+S">Shawn McGrath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 63 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Data-driven advances have resulted in significant improvements in dairy
production. However, the meat industry has lagged behind in adopting
data-driven approaches, underscoring the crucial need for data standardisation
to facilitate seamless data transmission to maximise productivity, save costs,
and increase market access. To address this gap, we propose a novel data
schema, Livestock Event Information (LEI) schema, designed to accurately and
uniformly record livestock events. LEI complies with the International
Committee for Animal Recording (ICAR) and Integrity System Company (ISC)
schemas to deliver this data standardisation and enable data sharing between
producers and consumers. To validate the superiority of LEI, we conducted a
structural metrics analysis and a comprehensive case study. The analysis
demonstrated that LEI outperforms the ICAR and ISC schemas in terms of design,
while the case study confirmed its superior ability to capture livestock event
information. Our findings lay the foundation for the implementation of the LEI
schema, unlocking the potential for data-driven advances in livestock
management. Moreover, LEI's versatility opens avenues for future expansion into
other agricultural domains, encompassing poultry, fisheries, and crops. The
adoption of LEI promises substantial benefits, including improved data
accuracy, reduced costs, and increased productivity, heralding a new era of
sustainability in the meat industry.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17687" title="Abstract">arXiv:2310.17687</a> [<a href="/pdf/2310.17687" title="Download PDF">pdf</a>, <a href="/format/2310.17687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Fairness for Predictions using Generative Adversarial  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuchen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Frauen%2C+D">Dennis Frauen</a>, 
<a href="/search/cs?searchtype=author&query=Melnychuk%2C+V">Valentyn Melnychuk</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Fairness in predictions is of direct importance in practice due to legal,
ethical, and societal reasons. It is often achieved through counterfactual
fairness, which ensures that the prediction for an individual is the same as
that in a counterfactual world under a different sensitive attribute. However,
achieving counterfactual fairness is challenging as counterfactuals are
unobservable. In this paper, we develop a novel deep neural network called
Generative Counterfactual Fairness Network (GCFN) for making predictions under
counterfactual fairness. Specifically, we leverage a tailored generative
adversarial network to directly learn the counterfactual distribution of the
descendants of the sensitive attribute, which we then use to enforce fair
predictions through a novel counterfactual mediator regularization. If the
counterfactual distribution is learned sufficiently well, our method is
mathematically guaranteed to ensure the notion of counterfactual fairness.
Thereby, our GCFN addresses key shortcomings of existing baselines that are
based on inferring latent variables, yet which (a) are potentially correlated
with the sensitive attributes and thus lead to bias, and (b) have weak
capability in constructing latent representations and thus low prediction
performance. Across various experiments, our method achieves state-of-the-art
performance. Using a real-world case study from recidivism prediction, we
further demonstrate that our method makes meaningful predictions in practice.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17688" title="Abstract">arXiv:2310.17688</a> [<a href="/pdf/2310.17688" title="Download PDF">pdf</a>, <a href="/format/2310.17688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Managing AI Risks in an Era of Rapid Progress
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Hinton%2C+G">Geoffrey Hinton</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+A">Andrew Yao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Harari%2C+Y+N">Yuval Noah Harari</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya-Qin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Lan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Shalev-Shwartz%2C+S">Shai Shalev-Shwartz</a>, 
<a href="/search/cs?searchtype=author&query=Hadfield%2C+G">Gillian Hadfield</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>, 
<a href="/search/cs?searchtype=author&query=Maharaj%2C+T">Tegan Maharaj</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Baydin%2C+A+G">At&#x131;l&#x131;m G&#xfc;ne&#x15f; Baydin</a>, 
<a href="/search/cs?searchtype=author&query=McIlraith%2C+S">Sheila McIlraith</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Q">Qiqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Acharya%2C+A">Ashwin Acharya</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+D">David Krueger</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+S">Stuart Russell</a>, 
<a href="/search/cs?searchtype=author&query=Kahneman%2C+D">Daniel Kahneman</a>, 
<a href="/search/cs?searchtype=author&query=Brauner%2C+J">Jan Brauner</a>, 
<a href="/search/cs?searchtype=author&query=Mindermann%2C+S">S&#xf6;ren Mindermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In this short consensus paper, we outline risks from upcoming, advanced AI
systems. We examine large-scale social harms and malicious uses, as well as an
irreversible loss of human control over autonomous AI systems. In light of
rapid and continuing AI progress, we propose priorities for AI R&amp;D and
governance.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17690" title="Abstract">arXiv:2310.17690</a> [<a href="/pdf/2310.17690" title="Download PDF">pdf</a>, <a href="/format/2310.17690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-contrastive sentence representations via self-supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farina%2C+M">Marco Farina</a>, 
<a href="/search/cs?searchtype=author&query=Pappadopulo%2C+D">Duccio Pappadopulo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted and rejected by EMNLP 2023. Contact the authors for a copy of the "reviews"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Sample contrastive methods, typically referred to simply as contrastive are
the foundation of most unsupervised methods to learn text and sentence
embeddings. On the other hand, a different class of self-supervised loss
functions and methods have been considered in the computer vision community and
referred to as dimension contrastive. In this paper, we thoroughly compare this
class of methods with the standard baseline for contrastive sentence
embeddings, SimCSE. We find that self-supervised embeddings trained using
dimension contrastive objectives can outperform SimCSE on downstream tasks
without needing auxiliary loss functions.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17703" title="Abstract">arXiv:2310.17703</a> [<a href="/pdf/2310.17703" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The impact of using an AI chatbot to respond to patient messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guevara%2C+M">Marco Guevara</a>, 
<a href="/search/cs?searchtype=author&query=Moningi%2C+S">Shalini Moningi</a>, 
<a href="/search/cs?searchtype=author&query=Hoebers%2C+F">Frank Hoebers</a>, 
<a href="/search/cs?searchtype=author&query=Elhalawani%2C+H">Hesham Elhalawani</a>, 
<a href="/search/cs?searchtype=author&query=Kann%2C+B+H">Benjamin H. Kann</a>, 
<a href="/search/cs?searchtype=author&query=Chipidza%2C+F+E">Fallon E. Chipidza</a>, 
<a href="/search/cs?searchtype=author&query=Leeman%2C+J">Jonathan Leeman</a>, 
<a href="/search/cs?searchtype=author&query=Aerts%2C+H+J+W+L">Hugo J.W.L. Aerts</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+T">Timothy Miller</a>, 
<a href="/search/cs?searchtype=author&query=Savova%2C+G+K">Guergana K. Savova</a>, 
<a href="/search/cs?searchtype=author&query=Mak%2C+R+H">Raymond H. Mak</a>, 
<a href="/search/cs?searchtype=author&query=Lustberg%2C+M">Maryam Lustberg</a>, 
<a href="/search/cs?searchtype=author&query=Afshar%2C+M">Majid Afshar</a>, 
<a href="/search/cs?searchtype=author&query=Bitterman%2C+D+S">Danielle S. Bitterman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 figures and tables in main, submitted for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Documentation burden is a major contributor to clinician burnout, which is
rising nationally and is an urgent threat to our ability to care for patients.
Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician
burden by assisting with documentation. Although many hospitals are actively
integrating such systems into electronic medical record systems, AI chatbots
utility and impact on clinical decision-making have not been studied for this
intended use. We are the first to examine the utility of large language models
in assisting clinicians draft responses to patient questions. In our two-stage
cross-sectional study, 6 oncologists responded to 100 realistic synthetic
cancer patient scenarios and portal messages developed to reflect common
medical situations, first manually, then with AI assistance.
<br />We find AI-assisted responses were longer, less readable, but provided
acceptable drafts without edits 58% of time. AI assistance improved efficiency
77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses
could severely harm. In 31% cases, physicians thought AI drafts were
human-written. AI assistance led to more patient education recommendations,
fewer clinical actions than manual responses. Results show promise for AI to
improve clinician efficiency and patient care through assisting documentation,
if used judiciously. Monitoring model outputs and human-AI interaction remains
crucial for safe implementation.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17705" title="Abstract">arXiv:2310.17705</a> [<a href="/pdf/2310.17705" title="Download PDF">pdf</a>, <a href="/format/2310.17705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered  by Semantic Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Runze Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Imran%2C+M+A">Muhammad Ali Imran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Generative AI applications are recently catering to a vast user base by
creating diverse and high-quality AI-generated content (AIGC). With the
proliferation of mobile devices and rapid growth of mobile traffic, providing
ubiquitous access to high-quality AIGC services via wireless communication
networks is becoming the future direction for AIGC products. However, it is
challenging to provide optimal AIGC services in wireless networks with unstable
channels, limited bandwidth resources, and unevenly distributed computational
resources. To tackle these challenges, we propose a semantic communication
(SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where
only semantic information of the content rather than all the binary bits should
be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates
diffusion-based models within the semantic encoder and decoder for efficient
content generation and flexible adjustment of the computing workload of both
transmitter and receiver. Meanwhile, we devise a resource-aware workload
trade-off (ROOT) scheme into the SemAIGC framework to intelligently decide
transmitter/receiver workload, thus adjusting the utilization of computational
resource according to service requirements. Simulations verify the superiority
of our proposed SemAIGC framework in terms of latency and content quality
compared to conventional approaches.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17711" title="Abstract">arXiv:2310.17711</a> [<a href="/pdf/2310.17711" title="Download PDF">pdf</a>, <a href="/format/2310.17711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Explanation the Cure? Misinformation Mitigation in the Short Term and  Long Term
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+Y">Yi-Li Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+S">Shih-Chieh Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+A">Aiping Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Ku%2C+L">Lun-Wei Ku</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With advancements in natural language processing (NLP) models, automatic
explanation generation has been proposed to mitigate misinformation on social
media platforms in addition to adding warning labels to identified fake news.
While many researchers have focused on generating good explanations, how these
explanations can really help humans combat fake news is under-explored. In this
study, we compare the effectiveness of a warning label and the state-of-the-art
counterfactual explanations generated by GPT-4 in debunking misinformation. In
a two-wave, online human-subject study, participants (N = 215) were randomly
assigned to a control group in which false contents are shown without any
intervention, a warning tag group in which the false claims were labeled, or an
explanation group in which the false contents were accompanied by GPT-4
generated explanations. Our results show that both interventions significantly
decrease participants' self-reported belief in fake claims in an equivalent
manner for the short-term and long-term. We discuss the implications of our
findings and directions for future NLP-based misinformation debunking
strategies.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17714" title="Abstract">arXiv:2310.17714</a> [<a href="/pdf/2310.17714" title="Download PDF">pdf</a>, <a href="/format/2310.17714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for  Relation Extraction from Financial Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajpoot%2C+P+K">Pawan Kumar Rajpoot</a>, 
<a href="/search/cs?searchtype=author&query=Parikh%2C+A">Ankur Parikh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Relation extraction (RE) has achieved remarkable progress with the help of
pre-trained language models. However, existing RE models are usually incapable
of handling two situations: implicit expressions and long-tail relation
classes, caused by language complexity and data sparsity. Further, these
approaches and models are largely inaccessible to users who don't have direct
access to large language models (LLMs) and/or infrastructure for supervised
training or fine-tuning. Rule-based systems also struggle with implicit
expressions. Apart from this, Real world financial documents such as various
10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose
another challenge to rule-based systems in terms of longer and complex
sentences. In this paper, we introduce a simple approach that consults training
relations at test time through a nearest-neighbor search over dense vectors of
lexico-syntactic patterns and provides a simple yet effective means to tackle
the above issues. We evaluate our approach on REFinD and show that our method
achieves state-of-the-art performance. We further show that it can provide a
good start for human in the loop setup when a small number of annotations are
available and it is also beneficial when domain experts can provide high
quality patterns.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17715" title="Abstract">arXiv:2310.17715</a> [<a href="/pdf/2310.17715" title="Download PDF">pdf</a>, <a href="/format/2310.17715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier Dimensions Encode Task-Specific Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rudman%2C+W">William Rudman</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Catherine Chen</a>, 
<a href="/search/cs?searchtype=author&query=Eickhoff%2C+C">Carsten Eickhoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Representations from large language models (LLMs) are known to be dominated
by a small subset of dimensions with exceedingly high variance. Previous works
have argued that although ablating these outlier dimensions in LLM
representations hurts downstream performance, outlier dimensions are
detrimental to the representational quality of embeddings. In this study, we
investigate how fine-tuning impacts outlier dimensions and show that 1) outlier
dimensions that occur in pre-training persist in fine-tuned models and 2) a
single outlier dimension can complete downstream tasks with a minimal error
rate. Our results suggest that outlier dimensions can encode crucial
task-specific knowledge and that the value of a representation in a single
outlier dimension drives downstream model decisions.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17722" title="Abstract">arXiv:2310.17722</a> [<a href="/pdf/2310.17722" title="Download PDF">pdf</a>, <a href="/format/2310.17722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Generalizable Policies for Embodied Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Szot%2C+A">Andrew Szot</a>, 
<a href="/search/cs?searchtype=author&query=Schwarzer%2C+M">Max Schwarzer</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+H">Harsh Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Mazoure%2C+B">Bogdan Mazoure</a>, 
<a href="/search/cs?searchtype=author&query=Talbott%2C+W">Walter Talbott</a>, 
<a href="/search/cs?searchtype=author&query=Metcalf%2C+K">Katherine Metcalf</a>, 
<a href="/search/cs?searchtype=author&query=Mackraz%2C+N">Natalie Mackraz</a>, 
<a href="/search/cs?searchtype=author&query=Hjelm%2C+D">Devon Hjelm</a>, 
<a href="/search/cs?searchtype=author&query=Toshev%2C+A">Alexander Toshev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We show that large language models (LLMs) can be adapted to be generalizable
policies for embodied visual tasks. Our approach, called Large LAnguage model
Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take
as input text instructions and visual egocentric observations and output
actions directly in the environment. Using reinforcement learning, we train
LLaRP to see and act solely through environmental interactions. We show that
LLaRP is robust to complex paraphrasings of task instructions and can
generalize to new tasks that require novel optimal behavior. In particular, on
1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other
common learned baselines or zero-shot applications of LLMs. Finally, to aid the
community in studying language conditioned, massively multi-task, embodied AI
problems we release a novel benchmark, Language Rearrangement, consisting of
150,000 training and 1,000 testing tasks for language-conditioned
rearrangement. Video examples of LLaRP in unseen Language Rearrangement
instructions are at https://llm-rl.github.io.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17723" title="Abstract">arXiv:2310.17723</a> [<a href="/pdf/2310.17723" title="Download PDF">pdf</a>, <a href="/format/2310.17723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training  Quantization Framework for W8A8 Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhewei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Aminabadi%2C+R+Y">Reza Yazdani Aminabadi</a>, 
<a href="/search/cs?searchtype=author&query=Youn%2C+S">Stephen Youn</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+E">Elton Zheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuxiong He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Quantization techniques are pivotal in reducing the memory and computational
demands of deep neural network inference. Existing solutions, such as
ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook
crucial memory-bounded operators and the complexities of per-token
quantization. Addressing these gaps, we present a novel, fully
hardware-enhanced robust optimized post-training W8A8 quantization framework,
ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and
compute-intensive operators, aiming for optimal hardware performance.
Additionally, it offers flexibility by allowing specific INT8 modules to switch
to FP16/BF16 mode, enhancing accuracy.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17729" title="Abstract">arXiv:2310.17729</a> [<a href="/pdf/2310.17729" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Traffic Density Forecasting in Intelligent Transportation  Systems Using Gated Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+R+H">Razib Hayat Khan</a>, 
<a href="/search/cs?searchtype=author&query=Miah%2C+J">Jonayet Miah</a>, 
<a href="/search/cs?searchtype=author&query=Arafat%2C+S+M+Y">S M Yasir Arafat</a>, 
<a href="/search/cs?searchtype=author&query=Syeed%2C+M+M+M">M M Mahbubul Syeed</a>, 
<a href="/search/cs?searchtype=author&query=Ca%2C+D+M">Duc M Ca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This study delves into the application of graph neural networks in the realm
of traffic forecasting, a crucial facet of intelligent transportation systems.
Accurate traffic predictions are vital for functions like trip planning,
traffic control, and vehicle routing in such systems. Three prominent GNN
architectures Graph Convolutional Networks (Graph Sample and Aggregation) and
Gated Graph Neural Networks are explored within the context of traffic
prediction. Each architecture's methodology is thoroughly examined, including
layer configurations, activation functions,and hyperparameters. The primary
goal is to minimize prediction errors, with GGNNs emerging as the most
effective choice among the three models. The research outlines outcomes for
each architecture, elucidating their predictive performance through root mean
squared error and mean absolute error (MAE). Hypothetical results reveal
intriguing insights: GCNs display an RMSE of 9.10 and an MAE of 8.00, while
GraphSAGE shows improvement with an RMSE of 8.3 and an MAE of 7.5. Gated Graph
Neural Networks (GGNNs) exhibit the lowest RMSE at 9.15 and an impressive MAE
of 7.1, positioning them as the frontrunner.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17732" title="Abstract">arXiv:2310.17732</a> [<a href="/pdf/2310.17732" title="Download PDF">pdf</a>, <a href="/format/2310.17732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value  in Similar Item Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giahi%2C+R">Ramin Giahi</a>, 
<a href="/search/cs?searchtype=author&query=Maragheh%2C+R+Y">Reza Yousefi Maragheh</a>, 
<a href="/search/cs?searchtype=author&query=Farrokhsiar%2C+N">Nima Farrokhsiar</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianpeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jason Cho</a>, 
<a href="/search/cs?searchtype=author&query=Korpeoglu%2C+E">Evren Korpeoglu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sushant Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Achan%2C+K">Kannan Achan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, 43 citations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Similar item recommendation is a critical task in the e-Commerce industry,
which helps customers explore similar and relevant alternatives based on their
interested products. Despite the traditional machine learning models, Graph
Neural Networks (GNNs), by design, can understand complex relations like
similarity between products. However, in contrast to their wide usage in
retrieval tasks and their focus on optimizing the relevance, the current GNN
architectures are not tailored toward maximizing revenue-related objectives
such as Gross Merchandise Value (GMV), which is one of the major business
metrics for e-Commerce companies. In addition, defining accurate edge relations
in GNNs is non-trivial in large-scale e-Commerce systems, due to the
heterogeneity nature of the item-item relationships. This work aims to address
these issues by designing a new GNN architecture called GNN-GMVO (Graph Neural
Network - Gross Merchandise Value Optimizer). This model directly optimizes GMV
while considering the complex relations between items. In addition, we propose
a customized edge construction method to tailor the model toward similar item
recommendation task and alleviate the noisy and complex item-item relations. In
our comprehensive experiments on three real-world datasets, we show higher
prediction performance and expected GMV for top ranked items recommended by our
model when compared with selected state-of-the-art benchmark models.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17734" title="Abstract">arXiv:2310.17734</a> [<a href="/pdf/2310.17734" title="Download PDF">pdf</a>, <a href="/format/2310.17734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Multilingual Coreference Resolution by Universal  Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chai%2C+H">Haixia Chai</a>, 
<a href="/search/cs?searchtype=author&query=Strube%2C+M">Michael Strube</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multilingual coreference resolution (MCR) has been a long-standing and
challenging task. With the newly proposed multilingual coreference dataset,
CorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by
using its harmonized universal morphosyntactic and coreference annotations.
First, we study coreference by examining the ground truth data at different
linguistic levels, namely mention, entity and document levels, and across
different genres, to gain insights into the characteristics of coreference
across multiple languages. Second, we perform an error analysis of the most
challenging cases that the SotA system fails to resolve in the CRAC 2022 shared
task using the universal annotations. Last, based on this analysis, we extract
features from universal morphosyntactic annotations and integrate these
features into a baseline system to assess their potential benefits for the MCR
task. Our results show that our best configuration of features improves the
baseline by 0.9% F1 score.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17737" title="Abstract">arXiv:2310.17737</a> [<a href="/pdf/2310.17737" title="Download PDF">pdf</a>, <a href="/format/2310.17737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akbari%2C+M">Mohammad Akbari</a>, 
<a href="/search/cs?searchtype=author&query=Alvar%2C+S+R">Saeed Ranjbar Alvar</a>, 
<a href="/search/cs?searchtype=author&query=Kamranian%2C+B">Behnam Kamranian</a>, 
<a href="/search/cs?searchtype=author&query=Banitalebi-Dehkordi%2C+A">Amin Banitalebi-Dehkordi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoNLL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Building multi-modal language models has been a trend in the recent years,
where additional modalities such as image, video, speech, etc. are jointly
learned along with natural languages (i.e., textual information). Despite the
success of these multi-modal language models with different modalities, there
is no existing solution for neural network architectures and natural languages.
Providing neural architectural information as a new modality allows us to
provide fast architecture-2-text and text-2-architecture retrieval/generation
services on the cloud with a single inference. Such solution is valuable in
terms of helping beginner and intermediate ML users to come up with better
neural architectures or AutoML approaches with a simple text query. In this
paper, we propose ArchBERT, a bi-modal model for joint learning and
understanding of neural architectures and natural languages, which opens up new
avenues for research in this area. We also introduce a pre-training strategy
named Masked Architecture Modeling (MAM) for a more generalized joint learning.
Moreover, we introduce and publicly release two new bi-modal datasets for
training and validating our methods. The ArchBERT's performance is verified
through a set of numerical experiments on different downstream tasks such as
architecture-oriented reasoning, question answering, and captioning
(summarization). Datasets, codes, and demos are available supplementary
materials.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17743" title="Abstract">arXiv:2310.17743</a> [<a href="/pdf/2310.17743" title="Download PDF">pdf</a>, <a href="/format/2310.17743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StyleBART: Decorate Pretrained Model with Style Adapters for  Unsupervised Stylistic Headline Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yajing Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+B">Boya Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanhua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Stylistic headline generation is the task to generate a headline that not
only summarizes the content of an article, but also reflects a desired style
that attracts users. As style-specific article-headline pairs are scarce,
previous researches focus on unsupervised approaches with a standard headline
generation dataset and mono-style corpora. In this work, we follow this line
and propose StyleBART, an unsupervised approach for stylistic headline
generation. Our method decorates the pretrained BART model with adapters that
are responsible for different styles and allows the generation of headlines
with diverse styles by simply switching the adapters. Different from previous
works, StyleBART separates the task of style learning and headline generation,
making it possible to freely combine the base model and the style adapters
during inference. We further propose an inverse paraphrasing task to enhance
the style adapters. Extensive automatic and human evaluations show that
StyleBART achieves new state-of-the-art performance in the unsupervised
stylistic headline generation task, producing high-quality headlines with the
desired style.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17746" title="Abstract">arXiv:2310.17746</a> [<a href="/pdf/2310.17746" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory Efficient Multithreaded Incremental Segmented Sieve Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+E">Evan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Kaeli%2C+D">David Kaeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Prime numbers are fundamental in number theory and play a significant role in
various areas, from pure mathematics to practical applications, including
cryptography. In this contribution, we introduce a multithreaded implementation
of the Segmented Sieve algorithm. In our implementation, instead of handling
large prime ranges in one iteration, the sieving process is broken down
incrementally, which theoretically eliminates the challenges of working with
large numbers, and can reduce memory usage, providing overall more efficient
multi-core utilization over extended computations.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17748" title="Abstract">arXiv:2310.17748</a> [<a href="/pdf/2310.17748" title="Download PDF">pdf</a>, <a href="/format/2310.17748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making the End-User a Priority in Benchmarking: OrionBench for  Unsupervised Time Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alnegheimish%2C+S">Sarah Alnegheimish</a>, 
<a href="/search/cs?searchtype=author&query=Berti-Equille%2C+L">Laure Berti-Equille</a>, 
<a href="/search/cs?searchtype=author&query=Veeramachaneni%2C+K">Kalyan Veeramachaneni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time series anomaly detection is a prevalent problem in many application
domains such as patient monitoring in healthcare, forecasting in finance, or
predictive maintenance in energy. This has led to the emergence of a plethora
of anomaly detection methods, including more recently, deep learning based
methods. Although several benchmarks have been proposed to compare newly
developed models, they usually rely on one-time execution over a limited set of
datasets and the comparison is restricted to a few models. We propose
OrionBench -- a user centric continuously maintained benchmark for unsupervised
time series anomaly detection. The framework provides universal abstractions to
represent models, extensibility to add new pipelines and datasets,
hyperparameter standardization, pipeline verification, and frequent releases
with published benchmarks. We demonstrate the usage of OrionBench, and the
progression of pipelines across 15 releases published over the course of three
years. Moreover, we walk through two real scenarios we experienced with
OrionBench that highlight the importance of continuous benchmarks in
unsupervised time series anomaly detection.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17749" title="Abstract">arXiv:2310.17749</a> [<a href="/pdf/2310.17749" title="Download PDF">pdf</a>, <a href="/format/2310.17749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Salespeople vs SalesBot: Exploring the Role of Educational Value in  Conversational Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murakhovs%27ka%2C+L">Lidiya Murakhovs&#x27;ka</a>, 
<a href="/search/cs?searchtype=author&query=Laban%2C+P">Philippe Laban</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chien-Sheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Making big purchases requires consumers to research or consult a salesperson
to gain domain expertise. However, existing conversational recommender systems
(CRS) often overlook users' lack of background knowledge, focusing solely on
gathering preferences. In this work, we define a new problem space for
conversational agents that aim to provide both product recommendations and
educational value through mixed-type mixed-initiative dialog. We introduce
SalesOps, a framework that facilitates the simulation and evaluation of such
systems by leveraging recent advancements in large language models (LLMs). We
build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate
either side of the framework. A comprehensive human study compares SalesBot
against professional salespeople, revealing that although SalesBot approaches
professional performance in terms of fluency and informativeness, it lags
behind in recommendation quality. We emphasize the distinct limitations both
face in providing truthful information, highlighting the challenges of ensuring
faithfulness in the CRS context. We release our code and make all data
available.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17750" title="Abstract">arXiv:2310.17750</a> [<a href="/pdf/2310.17750" title="Download PDF">pdf</a>, <a href="/format/2310.17750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Automated Measurement of Responsible AI Harms in  Generative AI Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magooda%2C+A">Ahmed Magooda</a>, 
<a href="/search/cs?searchtype=author&query=Helyar%2C+A">Alec Helyar</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+K">Kyle Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+D">David Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Atalla%2C+C">Chad Atalla</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+E">Emily Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Vann%2C+D">Dan Vann</a>, 
<a href="/search/cs?searchtype=author&query=Edgar%2C+R">Richard Edgar</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Lutz%2C+R">Roman Lutz</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+H">Hongliang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+V">Vincent Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kamal%2C+E">Eslam Kamal</a>, 
<a href="/search/cs?searchtype=author&query=Zarfati%2C+F">Federico Zarfati</a>, 
<a href="/search/cs?searchtype=author&query=Wallach%2C+H">Hanna Wallach</a>, 
<a href="/search/cs?searchtype=author&query=Bird%2C+S">Sarah Bird</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a living document
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present a framework for the automated measurement of responsible AI (RAI)
metrics for large language models (LLMs) and associated products and services.
Our framework for automatically measuring harms from LLMs builds on existing
technical and sociotechnical expertise and leverages the capabilities of
state-of-the-art LLMs, such as GPT-4. We use this framework to run through
several case studies investigating how different LLMs may violate a range of
RAI-related principles. The framework may be employed alongside domain-specific
sociotechnical expertise to create measurements for new harm areas in the
future. By implementing this framework, we aim to enable more advanced harm
measurement efforts and further the responsible use of LLMs.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17752" title="Abstract">arXiv:2310.17752</a> [<a href="/pdf/2310.17752" title="Download PDF">pdf</a>, <a href="/format/2310.17752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PockEngine: Sparse and Efficient Fine-tuning in a Pocket
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Ligeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lanxiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Ji Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei-Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Ming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+C">Chuang Gan</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Song Han</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 56th IEEE/ACM International Symposium on Microarchitecture (MICRO
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">On-device learning and efficient fine-tuning enable continuous and
privacy-preserving customization (e.g., locally fine-tuning large language
models on personalized data). However, existing training frameworks are
designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and
lack the optimizations for learning on the edge, which faces challenges of
resource limitations and edge hardware diversity. We introduce PockEngine: a
tiny, sparse and efficient engine to enable fine-tuning on various edge
devices. PockEngine supports sparse backpropagation: it prunes the backward
graph and sparsely updates the model with measured memory saving and latency
reduction while maintaining the model quality. Secondly, PockEngine is
compilation first: the entire training graph (including forward, backward and
optimization steps) is derived at compile-time, which reduces the runtime
overhead and brings opportunities for graph transformations. PockEngine also
integrates a rich set of training graph optimizations, thus can further
accelerate the training cost, including operator reordering and backend
switching. PockEngine supports diverse applications, frontends and hardware
backends: it flexibly compiles and tunes models defined in
PyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We
evaluated PockEngine on both vision models and large language models.
PockEngine achieves up to 15 $\times$ speedup over off-the-shelf TensorFlow
(Raspberry Pi), 5.6 $\times$ memory saving back-propagation (Jetson AGX Orin).
Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin
at 550 tokens/s, 7.9$\times$ faster than the PyTorch.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17753" title="Abstract">arXiv:2310.17753</a> [<a href="/pdf/2310.17753" title="Download PDF">pdf</a>, <a href="/format/2310.17753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bin Assignment and Decentralized Path Planning for Multi-Robot Parcel  Sorting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Teng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">At modern warehouses, mobile robots transport packages and drop them into
collection bins/chutes based on shipping destinations grouped by, e.g., the ZIP
code. System throughput, measured as the number of packages sorted per unit of
time, determines the efficiency of the warehouse. This research develops a
scalable, high-throughput multi-robot parcel sorting solution, decomposing the
task into two related processes, bin assignment and offline/online multi-robot
path planning, and optimizing both. Bin assignment matches collection bins with
package types to minimize traveling costs. Subsequently, robots are assigned to
pick up and drop packages into assigned bins. Multiple highly effective bin
assignment algorithms are proposed that can work with an arbitrary planning
algorithm. We propose a decentralized path planning routine using only local
information to route the robots over a carefully constructed directed road
network for multi-robot path planning. Our decentralized planner, provably
probabilistically deadlock-free, consistently delivers near-optimal results on
par with some top-performing centralized planners while significantly reducing
computation times by orders of magnitude. Extensive simulations show that our
overall framework delivers promising performances.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17759" title="Abstract">arXiv:2310.17759</a> [<a href="/pdf/2310.17759" title="Download PDF">pdf</a>, <a href="/format/2310.17759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Guarantees for Algorithmic Reproducibility and Gradient  Complexity in Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junchi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Algorithmic reproducibility measures the deviation in outputs of machine
learning algorithms upon minor changes in the training process. Previous work
suggests that first-order methods would need to trade-off convergence rate
(gradient complexity) for better reproducibility. In this work, we challenge
this perception and demonstrate that both optimal reproducibility and
near-optimal convergence guarantees can be achieved for smooth convex
minimization and smooth convex-concave minimax problems under various
error-prone oracle settings. Particularly, given the inexact initialization
oracle, our regularization-based algorithms achieve the best of both worlds -
optimal reproducibility and near-optimal gradient complexity - for minimization
and minimax optimization. With the inexact gradient oracle, the near-optimal
guarantees also hold for minimax optimization. Additionally, with the
stochastic gradient oracle, we show that stochastic gradient descent ascent is
optimal in terms of both reproducibility and gradient complexity. We believe
our results contribute to an enhanced understanding of the
reproducibility-convergence trade-off in the context of convex optimization.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17761" title="Abstract">arXiv:2310.17761</a> [<a href="/pdf/2310.17761" title="Download PDF">pdf</a>, <a href="/format/2310.17761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Personalized Empirical Risk Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuyang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Kamani%2C+M+M">Mohammad Mahdi Kamani</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavinia%2C+P">Pouria Mahdavinia</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi%2C+M">Mehrdad Mahdavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper advocates a new paradigm Personalized Empirical Risk Minimization
(PERM) to facilitate learning from heterogeneous data sources without imposing
stringent constraints on computational resources shared by participating
devices. In PERM, we aim to learn a distinct model for each client by learning
who to learn with and personalizing the aggregation of local empirical losses
by effectively estimating the statistical discrepancy among data distributions,
which entails optimal statistical accuracy for all local distributions and
overcomes the data heterogeneity issue. To learn personalized models at scale,
we propose a distributed algorithm that replaces the standard model averaging
with model shuffling to simultaneously optimize PERM objectives for all
devices. This also allows us to learn distinct model architectures (e.g.,
neural networks with different numbers of parameters) for different clients,
thus confining underlying memory and compute resources of individual clients.
We rigorously analyze the convergence of the proposed algorithm and conduct
experiments that corroborate the effectiveness of the proposed paradigm.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17762" title="Abstract">arXiv:2310.17762</a> [<a href="/pdf/2310.17762" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric Exponential Time Requires Near-Maximum Circuit Size:  Simplified, Truly Uniform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zeyong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">In a recent breakthrough, Chen, Hirahara and Ren prove that $\mathsf{S_2E}/_1
\not\subset \mathsf{SIZE}[2^n/n]$ by giving a single-valued $\mathsf{FS_2P}$
algorithm for the Range Avoidance Problem ($\mathsf{Avoid}$) that works for
infinitely many input size $n$.
<br />Building on their work, we present a simple single-valued $\mathsf{FS_2P}$
algorithm for $\mathsf{Avoid}$ that works for all input size $n$. As a result,
we obtain the circuit lower bound $\mathsf{S_2E} \not\subset
\mathsf{SIZE}[2^n/n]$ and many other corollaries:
<br />1. Near-maximum circuit lower bound for $\mathsf{\Sigma_2E} \cap
\mathsf{\Pi_2E}$ and $\mathsf{ZPE}^{\mathsf{NP}}$.
<br />2. Pseudodeterministic $\mathsf{FZPP}^{\mathsf{NP}}$ constructions for:
Ramsey graphs, rigid matrices, pseudorandom generators, two-source extractors,
linear codes, hard truth tables, and $K^{poly}$-random strings.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17764" title="Abstract">arXiv:2310.17764</a> [<a href="/pdf/2310.17764" title="Download PDF">pdf</a>, <a href="/format/2310.17764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynergyNet: Bridging the Gap between Discrete and Continuous  Representations for Precise Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorade%2C+V">Vandan Gorade</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sparsh Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+D">Debesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Bagci%2C+U">Ulas Bagci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, continuous latent space (CLS) and discrete latent space
(DLS) deep learning models have been proposed for medical image analysis for
improved performance. However, these models encounter distinct challenges. CLS
models capture intricate details but often lack interpretability in terms of
structural representation and robustness due to their emphasis on low-level
features. Conversely, DLS models offer interpretability, robustness, and the
ability to capture coarse-grained information thanks to their structured latent
space. However, DLS models have limited efficacy in capturing fine-grained
details. To address the limitations of both DLS and CLS models, we propose
SynergyNet, a novel bottleneck architecture designed to enhance existing
encoder-decoder segmentation frameworks. SynergyNet seamlessly integrates
discrete and continuous representations to harness complementary information
and successfully preserves both fine and coarse-grained details in the learned
representations. Our extensive experiment on multi-organ segmentation and
cardiac datasets demonstrates that SynergyNet outperforms other state of the
art methods, including TransUNet: dice scores improving by 2.16%, and Hausdorff
scores improving by 11.13%, respectively. When evaluating skin lesion and brain
tumor segmentation datasets, we observe a remarkable improvement of 1.71% in
Intersection-over Union scores for skin lesion segmentation and of 8.58% for
brain tumor segmentation. Our innovative approach paves the way for enhancing
the overall performance and capabilities of deep learning models in the
critical domain of medical image analysis.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17768" title="Abstract">arXiv:2310.17768</a> [<a href="/pdf/2310.17768" title="Download PDF">pdf</a>, <a href="/format/2310.17768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dataset of Relighted 3D Interacting Hands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moon%2C+G">Gyeongsik Moon</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+S">Shunsuke Saito</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weipeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+R">Rohan Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Buffalini%2C+J">Julia Buffalini</a>, 
<a href="/search/cs?searchtype=author&query=Bellan%2C+H">Harley Bellan</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+N">Nicholas Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+J">Jesse Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Mize%2C+M">Mallorie Mize</a>, 
<a href="/search/cs?searchtype=author&query=de+Bree%2C+P">Philippe de Bree</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+T">Tomas Simon</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Shubham Garg</a>, 
<a href="/search/cs?searchtype=author&query=McPhail%2C+K">Kevyn McPhail</a>, 
<a href="/search/cs?searchtype=author&query=Shiratori%2C+T">Takaaki Shiratori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 (Datasets and Benchmarks Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The two-hand interaction is one of the most challenging signals to analyze
due to the self-similarity, complicated articulations, and occlusions of hands.
Although several datasets have been proposed for the two-hand interaction
analysis, all of them do not achieve 1) diverse and realistic image appearances
and 2) diverse and large-scale groundtruth (GT) 3D poses at the same time. In
this work, we propose Re:InterHand, a dataset of relighted 3D interacting hands
that achieve the two goals. To this end, we employ a state-of-the-art hand
relighting network with our accurately tracked two-hand 3D poses. We compare
our Re:InterHand with existing 3D interacting hands datasets and show the
benefit of it. Our Re:InterHand is available in
https://mks0601.github.io/ReInterHand/.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17769" title="Abstract">arXiv:2310.17769</a> [<a href="/pdf/2310.17769" title="Download PDF">pdf</a>, <a href="/format/2310.17769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Contract AI: Aligning AI Assistants with Implicit Group Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fr%C3%A4nken%2C+J">Jan-Philipp Fr&#xe4;nken</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+S">Sam Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+P">Peixuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Gandhi%2C+K">Kanishk Gandhi</a>, 
<a href="/search/cs?searchtype=author&query=Arumugam%2C+D">Dilip Arumugam</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+J">Jared Moore</a>, 
<a href="/search/cs?searchtype=author&query=Tamkin%2C+A">Alex Tamkin</a>, 
<a href="/search/cs?searchtype=author&query=Gerstenberg%2C+T">Tobias Gerstenberg</a>, 
<a href="/search/cs?searchtype=author&query=Goodman%2C+N+D">Noah D. Goodman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SoLaR NeurIPS 2023 Workshop (<a href="https://solar-neurips.github.io/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We explore the idea of aligning an AI assistant by inverting a model of
users' (unknown) preferences from observed interactions. To validate our
proposal, we run proof-of-concept simulations in the economic ultimatum game,
formalizing user preferences as policies that guide the actions of simulated
players. We find that the AI assistant accurately aligns its behavior to match
standard policies from the economic literature (e.g., selfish, altruistic).
However, the assistant's learned policies lack robustness and exhibit limited
generalization in an out-of-distribution setting when confronted with a
currency (e.g., grams of medicine) that was not included in the assistant's
training distribution. Additionally, we find that when there is inconsistency
in the relationship between language use and an unknown policy (e.g., an
altruistic policy combined with rude language), the assistant's learning of the
policy is slowed. Overall, our preliminary results suggest that developing
simulation frameworks in which AI assistants need to infer preferences from
diverse users can provide a valuable approach for studying practical alignment
questions.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17770" title="Abstract">arXiv:2310.17770</a> [<a href="/pdf/2310.17770" title="Download PDF">pdf</a>, <a href="/format/2310.17770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GROOViST: A Metric for Grounding Objects in Visual Storytelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Surikuchi%2C+A+K">Aditya K Surikuchi</a>, 
<a href="/search/cs?searchtype=author&query=Pezzelle%2C+S">Sandro Pezzelle</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+R">Raquel Fern&#xe1;ndez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In EMNLP 2023 main conference proceedings (to appear)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">A proper evaluation of stories generated for a sequence of images -- the task
commonly referred to as visual storytelling -- must consider multiple aspects,
such as coherence, grammatical correctness, and visual grounding. In this work,
we focus on evaluating the degree of grounding, that is, the extent to which a
story is about the entities shown in the images. We analyze current metrics,
both designed for this purpose and for general vision-text alignment. Given
their observed shortcomings, we propose a novel evaluation tool, GROOViST, that
accounts for cross-modal dependencies, temporal misalignments (the fact that
the order in which entities appear in the story and the image sequence may not
match), and human intuitions on visual grounding. An additional advantage of
GROOViST is its modular design, where the contribution of each component can be
assessed and interpreted individually.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17771" title="Abstract">arXiv:2310.17771</a> [<a href="/pdf/2310.17771" title="Download PDF">pdf</a>, <a href="/format/2310.17771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability and Accuracy analysis of the $&#x3b8;$ Method and 3-Point Time  filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hurl%2C+N">Nicholas Hurl</a>, 
<a href="/search/math?searchtype=author&query=Siddiqua%2C+F">Farjana Siddiqua</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+S">Shuxian Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 3 tables,14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper analyzes a $\theta$-method and 3-point time filter. This approach
adds one additional line of code to the existing source code of
$\theta$-method. We prove the method's $0$-stability, accuracy, and
$A$-stability for both constant time step and variable time step. Some
numerical tests are performed to validate the theoretical results.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17772" title="Abstract">arXiv:2310.17772</a> [<a href="/pdf/2310.17772" title="Download PDF">pdf</a>, <a href="/format/2310.17772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Optimal Classification Trees Robust to Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Justin%2C+N">Nathan Justin</a>, 
<a href="/search/cs?searchtype=author&query=Aghaei%2C+S">Sina Aghaei</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez%2C+A">Andr&#xe9;s G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Vayanos%2C+P">Phebe Vayanos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the problem of learning classification trees that are robust to
distribution shifts between training and testing/deployment data. This problem
arises frequently in high stakes settings such as public health and social work
where data is often collected using self-reported surveys which are highly
sensitive to e.g., the framing of the questions, the time when and place where
the survey is conducted, and the level of comfort the interviewee has in
sharing information with the interviewer. We propose a method for learning
optimal robust classification trees based on mixed-integer robust optimization
technology. In particular, we demonstrate that the problem of learning an
optimal robust tree can be cast as a single-stage mixed-integer robust
optimization problem with a highly nonlinear and discontinuous objective. We
reformulate this problem equivalently as a two-stage linear robust optimization
problem for which we devise a tailored solution procedure based on constraint
generation. We evaluate the performance of our approach on numerous publicly
available datasets, and compare the performance to a regularized, non-robust
optimal tree. We show an increase of up to 12.48% in worst-case accuracy and of
up to 4.85% in average-case accuracy across several datasets and distribution
shifts from using our robust solution in comparison to the non-robust one.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17773" title="Abstract">arXiv:2310.17773</a> [<a href="/pdf/2310.17773" title="Download PDF">pdf</a>, <a href="/format/2310.17773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Convolutional Networks for Complex Traffic Scenario Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoek%2C+T">Tobias Hoek</a>, 
<a href="/search/cs?searchtype=author&query=Caesar%2C+H">Holger Caesar</a>, 
<a href="/search/cs?searchtype=author&query=Falkov%C3%A9n%2C+A">Andreas Falkov&#xe9;n</a>, 
<a href="/search/cs?searchtype=author&query=Johansson%2C+T">Tommy Johansson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Netherlands Conference on Computer Vision (NCCV) 2023 camera-ready + supplementary material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">A scenario-based testing approach can reduce the time required to obtain
statistically significant evidence of the safety of Automated Driving Systems
(ADS). Identifying these scenarios in an automated manner is a challenging
task. Most methods on scenario classification do not work for complex scenarios
with diverse environments (highways, urban) and interaction with other traffic
agents. This is mirrored in their approaches which model an individual vehicle
in relation to its environment, but neglect the interaction between multiple
vehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing
datasets lack diversity and do not have per-frame annotations to accurately
learn the start and end time of a scenario. We propose a method for complex
traffic scenario classification that is able to model the interaction of a
vehicle with the environment, as well as other agents. We use Graph
Convolutional Networks to model spatial and temporal aspects of these
scenarios. Expanding the nuScenes and Argoverse 2 driving datasets, we
introduce a scenario-labeled dataset, which covers different driving
environments and is annotated per frame. Training our method on this dataset,
we present a promising baseline for future research on per-frame complex
scenario classification.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17774" title="Abstract">arXiv:2310.17774</a> [<a href="/pdf/2310.17774" title="Download PDF">pdf</a>, <a href="/format/2310.17774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Words, Subwords, and Morphemes: What Really Matters in the  Surprisal-Reading Time Relationship?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nair%2C+S">Sathvik Nair</a>, 
<a href="/search/cs?searchtype=author&query=Resnik%2C+P">Philip Resnik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023; 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">An important assumption that comes with using LLMs on psycholinguistic data
has gone unverified. LLM-based predictions are based on subword tokenization,
not decomposition of words into morphemes. Does that matter? We carefully test
this by comparing surprisal estimates using orthographic, morphological, and
BPE tokenization against reading time data. Our results replicate previous
findings and provide evidence that in the aggregate, predictions using BPE
tokenization do not suffer relative to morphological and orthographic
segmentation. However, a finer-grained analysis points to potential issues with
relying on BPE-based tokenization, as well as providing promising results
involving morphologically-aware surprisal estimates and suggesting a new method
for evaluating morphological prediction.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17777" title="Abstract">arXiv:2310.17777</a> [<a href="/pdf/2310.17777" title="Download PDF">pdf</a>, <a href="/ps/2310.17777" title="Download PostScript">ps</a>, <a href="/format/2310.17777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unified treatment of tractability for approximation problems defined  on Hilbert spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Emenike%2C+O">Onyekachi Emenike</a>, 
<a href="/search/math?searchtype=author&query=Hickernell%2C+F+J">Fred J. Hickernell</a>, 
<a href="/search/math?searchtype=author&query=Kritzer%2C+P">Peter Kritzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A large literature specifies conditions under which the information
complexity for a sequence of numerical problems defined for dimensions $1, 2,
\ldots$ grows at a moderate rate, i.e., the sequence of problems is tractable.
Here, we focus on the situation where the space of available information
consists of all linear functionals and the problems are defined as linear
operator mappings between Hilbert spaces. We unify the proofs of known
tractability results and generalize a number of existing results. These
generalizations are expressed as five theorems that provide equivalent
conditions for (strong) tractability in terms of sums of functions of the
singular values of the solution operators.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17779" title="Abstract">arXiv:2310.17779</a> [<a href="/pdf/2310.17779" title="Download PDF">pdf</a>, <a href="/format/2310.17779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Penalty-projection based Efficient and Accurate Stochastic Collocation  Method for Magnetohydrodynamic Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mohebujjaman%2C+M">Muhammad Mohebujjaman</a>, 
<a href="/search/math?searchtype=author&query=Miranda%2C+J">Julian Miranda</a>, 
<a href="/search/math?searchtype=author&query=Mahbub%2C+M+A+A">Md. Abdullah Al Mahbub</a>, 
<a href="/search/math?searchtype=author&query=Xiao%2C+M">Mengying Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose, analyze, and test a penalty projection-based efficient and
accurate algorithm for the Uncertainty Quantification (UQ) of the
time-dependent Magnetohydrodynamic (MHD) flow problems in convection-dominated
regimes. The algorithm uses the Els\"asser variables formulation and discrete
Hodge decomposition to decouple the stochastic MHD system into four
sub-problems (at each time-step for each realization) which are much easier to
solve than solving the coupled saddle point problems. Each of the sub-problems
is designed in a sophisticated way so that at each time-step the system matrix
remains the same for all the realizations but with different right-hand-side
vectors which allows saving a huge amount of computer memory and computational
time. Moreover, the scheme is equipped with ensemble eddy-viscosity and
grad-div stabilization terms. The stability of the algorithm is proven
rigorously. We prove that the proposed scheme converges to an equivalent
non-projection-based coupled MHD scheme for large grad-div stabilization
parameter values. We examine how Stochastic Collocation Methods (SCMs) can be
combined with the proposed penalty projection UQ algorithm. Finally, a series
of numerical experiments are given which verify the predicted convergence
rates, show the algorithm's performance on benchmark channel flow over a
rectangular step, and a regularized lid-driven cavity problem with high random
Reynolds number and magnetic Reynolds number.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17784" title="Abstract">arXiv:2310.17784</a> [<a href="/pdf/2310.17784" title="Download PDF">pdf</a>, <a href="/format/2310.17784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Centric Financial Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Huaiyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xinyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yijia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wanqing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qing Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) show promise for natural language tasks but
struggle when applied directly to complex domains like finance. LLMs have
difficulty reasoning about and integrating all relevant information. We propose
a data-centric approach to enable LLMs to better handle financial tasks. Our
key insight is that rather than overloading the LLM with everything at once, it
is more effective to preprocess and pre-understand the data. We create a
financial LLM (FLLM) using multitask prompt-based finetuning to achieve data
pre-processing and pre-understanding. However, labeled data is scarce for each
task. To overcome manual annotation costs, we employ abductive augmentation
reasoning (AAR) to automatically generate training data by modifying the pseudo
labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR
substantially outperforms baseline financial LLMs designed for raw text,
achieving state-of-the-art on financial analysis and interpretation tasks. We
also open source a new benchmark for financial analysis and interpretation. Our
methodology provides a promising path to unlock LLMs' potential for complex
real-world domains.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17785" title="Abstract">arXiv:2310.17785</a> [<a href="/pdf/2310.17785" title="Download PDF">pdf</a>, <a href="/format/2310.17785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Extrinsic Dexterity with Parameterized Manipulation Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shih-Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Magnusson%2C+M">Martin Magnusson</a>, 
<a href="/search/cs?searchtype=author&query=Stork%2C+J+A">Johannes A. Stork</a>, 
<a href="/search/cs?searchtype=author&query=Stoyano%2C+T">Todor Stoyano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Many practically relevant robot grasping problems feature a target object for
which all grasps are occluded, e.g., by the environment. Single-shot grasp
planning invariably fails in such scenarios. Instead, it is necessary to first
manipulate the object into a configuration that affords a grasp. We solve this
problem by learning a sequence of actions that utilize the environment to
change the object's pose. Concretely, we employ hierarchical reinforcement
learning to combine a sequence of learned parameterized manipulation
primitives. By learning the low-level manipulation policies, our approach can
control the object's state through exploiting interactions between the object,
the gripper, and the environment. Designing such a complex behavior
analytically would be infeasible under uncontrolled conditions, as an analytic
approach requires accurate physical modeling of the interaction and contact
dynamics. In contrast, we learn a hierarchical policy model that operates
directly on depth perception data, without the need for object detection, pose
estimation, or manual design of controllers. We evaluate our approach on
picking box-shaped objects of various weight, shape, and friction properties
from a constrained table-top workspace. Our method transfers to a real robot
and is able to successfully complete the object picking task in 98\% of
experimental trials.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17786" title="Abstract">arXiv:2310.17786</a> [<a href="/pdf/2310.17786" title="Download PDF">pdf</a>, <a href="/format/2310.17786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding when Dynamics-Invariant Data Augmentations Benefit  Model-Free Reinforcement Learning Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corrado%2C+N+E">Nicholas E. Corrado</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+J+P">Josiah P. Hanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, data augmentation (DA) has emerged as a method for leveraging
domain knowledge to inexpensively generate additional data in reinforcement
learning (RL) tasks, often yielding substantial improvements in data
efficiency. While prior work has demonstrated the utility of incorporating
augmented data directly into model-free RL updates, it is not well-understood
when a particular DA strategy will improve data efficiency. In this paper, we
seek to identify general aspects of DA responsible for observed learning
improvements. Our study focuses on sparse-reward tasks with dynamics-invariant
data augmentation functions, serving as an initial step towards a more general
understanding of DA and its integration into RL training. Experimentally, we
isolate three relevant aspects of DA: state-action coverage, reward density,
and the number of augmented transitions generated per update (the augmented
replay ratio). From our experiments, we draw two conclusions: (1) increasing
state-action coverage often has a much greater impact on data efficiency than
increasing reward density, and (2) decreasing the augmented replay ratio
substantially improves data efficiency. In fact, certain tasks in our empirical
study are solvable only when the replay ratio is sufficiently low.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17787" title="Abstract">arXiv:2310.17787</a> [<a href="/pdf/2310.17787" title="Download PDF">pdf</a>, <a href="/format/2310.17787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of large language models using an Indian language LGBTI+  lexicon
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">Aditya Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+S">Shruta Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Dange%2C+A">Alpana Dange</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Selected for publication in the AI Ethics Journal published by the Artificial Intelligence Robotics Ethics Society (AIRES)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are typically evaluated on the basis of
task-based benchmarks such as MMLU. Such benchmarks do not examine responsible
behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+
context where social stereotypes may result in variation in LGBTI+ terminology.
Therefore, domain-specific lexicons or dictionaries may be useful as a
representative list of words against which the LLM's behaviour needs to be
evaluated. This paper presents a methodology for evaluation of LLMs using an
LGBTI+ lexicon in Indian languages. The methodology consists of four steps:
formulating NLP tasks relevant to the expected behaviour, creating prompts that
test LLMs, using the LLMs to obtain the output and, finally, manually
evaluating the results. Our qualitative analysis shows that the three LLMs we
experiment on are unable to detect underlying hateful content. Similarly, we
observe limitations in using machine translation as means to evaluate natural
language understanding in languages other than English. The methodology
presented in this paper can be useful for LGBTI+ lexicons in other languages as
well as other domain-specific lexicons. The work done in this paper opens
avenues for responsible behaviour of LLMs, as demonstrated in the context of
prevalent social perception of the LGBTI+ community.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17788" title="Abstract">arXiv:2310.17788</a> [<a href="/pdf/2310.17788" title="Download PDF">pdf</a>, <a href="/format/2310.17788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Language Models for Energy Load Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F+D">Flora D. Salim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BuildSys 2023 Accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Energy load forecasting plays a crucial role in optimizing resource
allocation and managing energy consumption in buildings and cities. In this
paper, we propose a novel approach that leverages language models for energy
load forecasting. We employ prompting techniques to convert energy consumption
data into descriptive sentences, enabling fine-tuning of language models. By
adopting an autoregressive generating approach, our proposed method enables
predictions of various horizons of future energy load consumption. Through
extensive experiments on real-world datasets, we demonstrate the effectiveness
and accuracy of our proposed method. Our results indicate that utilizing
language models for energy load forecasting holds promise for enhancing energy
efficiency and facilitating intelligent decision-making in energy systems.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17790" title="Abstract">arXiv:2310.17790</a> [<a href="/pdf/2310.17790" title="Download PDF">pdf</a>, <a href="/format/2310.17790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Stress Fields for Reduced-order Elastoplasticity and Fracture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+Z">Zeshun Zong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chiaramonte%2C+M+M">Maurizio M. Chiaramonte</a>, 
<a href="/search/cs?searchtype=author&query=Matusik%2C+W">Wojciech Matusik</a>, 
<a href="/search/cs?searchtype=author&query=Grinspun%2C+E">Eitan Grinspun</a>, 
<a href="/search/cs?searchtype=author&query=Carlberg%2C+K">Kevin Carlberg</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chenfanfu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P+Y">Peter Yichen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We propose a hybrid neural network and physics framework for reduced-order
modeling of elastoplasticity and fracture. State-of-the-art scientific
computing models like the Material Point Method (MPM) faithfully simulate
large-deformation elastoplasticity and fracture mechanics. However, their long
runtime and large memory consumption render them unsuitable for applications
constrained by computation time and memory usage, e.g., virtual reality. To
overcome these barriers, we propose a reduced-order framework. Our key
innovation is training a low-dimensional manifold for the Kirchhoff stress
field via an implicit neural representation. This low-dimensional neural stress
field (NSF) enables efficient evaluations of stress values and,
correspondingly, internal forces at arbitrary spatial locations. In addition,
we also train neural deformation and affine fields to build low-dimensional
manifolds for the deformation and affine momentum fields. These neural stress,
deformation, and affine fields share the same low-dimensional latent space,
which uniquely embeds the high-dimensional simulation state. After training, we
run new simulations by evolving in this single latent space, which drastically
reduces the computation time and memory consumption. Our general
continuum-mechanics-based reduced-order framework is applicable to any
phenomena governed by the elastodynamics equation. To showcase the versatility
of our framework, we simulate a wide range of material behaviors, including
elastica, sand, metal, non-Newtonian fluids, fracture, contact, and collision.
We demonstrate dimension reduction by up to 100,000X and time savings by up to
10X.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17793" title="Abstract">arXiv:2310.17793</a> [<a href="/pdf/2310.17793" title="Download PDF">pdf</a>, <a href="/format/2310.17793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;You Are An Expert Linguistic Annotator&quot;: Limits of LLMs as Analyzers of  Abstract Meaning Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ettinger%2C+A">Allyson Ettinger</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J+D">Jena D. Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Pyatkin%2C+V">Valentina Pyatkin</a>, 
<a href="/search/cs?searchtype=author&query=Bhagavatula%2C+C">Chandra Bhagavatula</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings (short)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) show amazing proficiency and fluency in the use
of language. Does this mean that they have also acquired insightful linguistic
knowledge about the language, to an extent that they can serve as an "expert
linguistic annotator"? In this paper, we examine the successes and limitations
of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning
structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et
al. 2013) parsing formalism, which provides rich graphical representations of
sentence meaning structure while abstracting away from surface forms. We
compare models' analysis of this semantic structure across two settings: 1)
direct production of AMR parses based on zero- and few-shot prompts, and 2)
indirect partial reconstruction of AMR via metalinguistic natural language
queries (e.g., "Identify the primary event of this sentence, and the predicate
corresponding to that event."). Across these settings, we find that models can
reliably reproduce the basic format of AMR, and can often capture core event,
argument, and modifier structure -- however, model outputs are prone to
frequent and major errors, and holistic analysis of parse acceptability shows
that even with few-shot demonstrations, models have virtually 0% success in
producing fully accurate parses. Eliciting natural language responses produces
similar patterns of errors. Overall, our findings indicate that these models
out-of-the-box can capture aspects of semantic structure, but there remain key
limitations in their ability to support fully accurate semantic analyses or
parses.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17796" title="Abstract">arXiv:2310.17796</a> [<a href="/pdf/2310.17796" title="Download PDF">pdf</a>, <a href="/format/2310.17796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ControlLLM: Augment Language Models with Tools by Searching on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaoyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zeqiang Lai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangwei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+E">Erfei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xizhou Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lewei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Jifeng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">We present ControlLLM, a novel framework that enables large language models
(LLMs) to utilize multi-modal tools for solving complex real-world tasks.
Despite the remarkable performance of LLMs, they still struggle with tool
invocation due to ambiguous user prompts, inaccurate tool selection and
parameterization, and inefficient tool scheduling. To overcome these
challenges, our framework comprises three key components: (1) a \textit{task
decomposer} that breaks down a complex task into clear subtasks with
well-defined inputs and outputs; (2) a \textit{Thoughts-on-Graph (ToG)
paradigm} that searches the optimal solution path on a pre-built tool graph,
which specifies the parameter and dependency relations among different tools;
and (3) an \textit{execution engine with a rich toolbox} that interprets the
solution path and runs the tools efficiently on different computational
devices. We evaluate our framework on diverse tasks involving image, audio, and
video processing, demonstrating its superior accuracy, efficiency, and
versatility compared to existing methods.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17797" title="Abstract">arXiv:2310.17797</a> [<a href="/pdf/2310.17797" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuromorphic Online Clustering and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+J+E">J. E. Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">The bottom two layers of a neuromorphic architecture are designed and shown
to be capable of online clustering and supervised classification. An active
spiking dendrite model is used, and a single dendritic segment performs
essentially the same function as a classic integrate-and-fire point neuron. A
single dendrite is then composed of multiple segments and is capable of online
clustering. Although this work focuses primarily on dendrite functionality, a
multi-point neuron can be formed by combining multiple dendrites. To
demonstrate its clustering capability, a dendrite is applied to spike sorting,
an important component of brain-computer interface applications. Supervised
online classification is implemented as a network composed of multiple
dendrites and a simple voting mechanism. The dendrites operate independently
and in parallel. The network learns in an online fashion and can adapt to
macro-level changes in the input stream. Achieving brain-like capabilities,
efficiencies, and adaptability will require a significantly different approach
than conventional deep networks that learn via compute-intensive back
propagation. The model described herein may serve as the foundation for such an
approach.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17799" title="Abstract">arXiv:2310.17799</a> [<a href="/pdf/2310.17799" title="Download PDF">pdf</a>, <a href="/format/2310.17799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Multi-product Trading in Sequential Intraday and  Frequency-Regulation Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nordin%2C+S">Saeed Nordin</a> (1), 
<a href="/search/eess?searchtype=author&query=Khodadadi%2C+A">Abolfazl Khodadadi</a> (1), 
<a href="/search/eess?searchtype=author&query=Shinde%2C+P">Priyanka Shinde</a> (1), 
<a href="/search/eess?searchtype=author&query=Blom%2C+E">Evelin Blom</a> (1), 
<a href="/search/eess?searchtype=author&query=Hesamzadeh%2C+M+R">Mohammad Reza Hesamzadeh</a> (1), 
<a href="/search/eess?searchtype=author&query=S%C3%B6der%2C+L">Lennart S&#xf6;der</a> (1) ((1) KTH Royal Institute of Technology)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC); Probability (math.PR); Applications (stat.AP)

</div>
<p class="mathjax">With the increasing integration of power plants into the frequency-regulation
markets, the importance of optimal trading has grown substantially. This paper
conducts an in-depth analysis of their optimal trading behavior in sequential
day-ahead, intraday, and frequency-regulation markets. We introduce a
probabilistic multi-product optimization model, derived through a series of
transformation techniques. Additionally, we present two reformulations that
re-frame the problem as a mixed-integer linear programming problem with
uncertain parameters. Various aspects of the model are thoroughly examined to
observe the optimal multi-product trading behavior of hydro power plant assets,
along with numerous case studies. Leveraging historical data from Nordic
electricity markets, we construct realistic scenarios for the uncertain
parameters. Furthermore, we then proposed an algorithm based on the No-U-Turn
sampler to provide probability distribution functions of cleared prices in
frequency-regulation and day-ahead markets. These distribution functions offer
valuable statistical insights into temporal price risks for informed
multi-product optimal-trading decisions.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17800" title="Abstract">arXiv:2310.17800</a> [<a href="/pdf/2310.17800" title="Download PDF">pdf</a>, <a href="/format/2310.17800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interacting Diffusion Processes for Event Sequence Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+M">Mai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Regol%2C+F">Florence Regol</a>, 
<a href="/search/cs?searchtype=author&query=Coates%2C+M">Mark Coates</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural Temporal Point Processes (TPPs) have emerged as the primary framework
for predicting sequences of events that occur at irregular time intervals, but
their sequential nature can hamper performance for long-horizon forecasts. To
address this, we introduce a novel approach that incorporates a diffusion
generative model. The model facilitates sequence-to-sequence prediction,
allowing multi-step predictions based on historical event sequences. In
contrast to previous approaches, our model directly learns the joint
probability distribution of types and inter-arrival times for multiple events.
This allows us to fully leverage the high dimensional modeling capability of
modern generative models. Our model is composed of two diffusion processes, one
for the time intervals and one for the event types. These processes interact
through their respective denoising functions, which can take as input
intermediate representations from both processes, allowing the model to learn
complex interactions. We demonstrate that our proposal outperforms
state-of-the-art baselines for long-horizon forecasting of TPP.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17801" title="Abstract">arXiv:2310.17801</a> [<a href="/pdf/2310.17801" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Prior and Posterior Conditional Probability Representation for  Efficient Damage Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jie Wei</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weicong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Blasch%2C+E">Erik Blasch</a>, 
<a href="/search/cs?searchtype=author&query=Ardiles-Cruz%2C+E">Erika Ardiles-Cruz</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haibin Ling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">It is important to quantify Damage Assessment (DA) for Human Assistance and
Disaster Response (HADR) applications. In this paper, to achieve efficient and
scalable DA in HADR, an image prior and posterior conditional probability
(IP2CP) is developed as an effective computational imaging representation.
Equipped with the IP2CP representation, the matching pre- and post-disaster
images are effectively encoded into one image that is then processed using deep
learning approaches to determine the damage levels. Two scenarios of crucial
importance for the practical use of DA in HADR applications are examined:
pixel-wise semantic segmentation and patch-based contrastive learning-based
global damage classification. Results achieved by IP2CP in both scenarios
demonstrate promising performances, showing that our IP2CP-based methods within
the deep learning framework can effectively achieve data and computational
efficiency, which is of utmost importance for the DA in HADR applications.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17802" title="Abstract">arXiv:2310.17802</a> [<a href="/pdf/2310.17802" title="Download PDF">pdf</a>, <a href="/format/2310.17802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the  Automatic Ordering of Events in News Articles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alsayyahi%2C+S">Sarah Alsayyahi</a>, 
<a href="/search/cs?searchtype=author&query=Batista-Navarro%2C+R">Riza Batista-Navarro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in EMNLP 2023: 13 pages, 3 figures and 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Temporal relation extraction models have thus far been hindered by a number
of issues in existing temporal relation-annotated news datasets, including: (1)
low inter-annotator agreement due to the lack of specificity of their
annotation guidelines in terms of what counts as a temporal relation; (2) the
exclusion of long-distance relations within a given document (those spanning
across different paragraphs); and (3) the exclusion of events that are not
centred on verbs. This paper aims to alleviate these issues by presenting a new
annotation scheme that clearly defines the criteria based on which temporal
relations should be annotated. Additionally, the scheme includes events even if
they are not expressed as verbs (e.g., nominalised events). Furthermore, we
propose a method for annotating all temporal relations -- including
long-distance ones -- which automates the process, hence reducing time and
manual effort on the part of annotators. The result is a new dataset, the
TIMELINE corpus, in which improved inter-annotator agreement was obtained, in
comparison with previously reported temporal relation datasets. We report the
results of training and evaluating baseline temporal relation extraction models
on the new corpus, and compare them with results obtained on the widely used
MATRES corpus.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17804" title="Abstract">arXiv:2310.17804</a> [<a href="/pdf/2310.17804" title="Download PDF">pdf</a>, <a href="/format/2310.17804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BlackJack: Secure machine learning on IoT devices through hardware-based  shuffling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganesan%2C+K">Karthik Ganesan</a>, 
<a href="/search/cs?searchtype=author&query=Fishkin%2C+M">Michal Fishkin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+O">Ourong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jerger%2C+N+E">Natalie Enright Jerger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Neural networks are seeing increased use in diverse Internet of Things (IoT)
applications such as healthcare, smart homes and industrial monitoring. Their
widespread use makes neural networks a lucrative target for theft. An attacker
can obtain a model without having access to the training data or incurring the
cost of training. Also, networks trained using private data (e.g., medical
records) can reveal information about this data. Networks can be stolen by
leveraging side channels such as power traces of the IoT device when it is
running the network. Existing attacks require operations to occur in the same
order each time; an attacker must collect and analyze several traces of the
device to steal the network. Therefore, to prevent this type of attack, we
randomly shuffle the order of operations each time. With shuffling, each
operation can now happen at many different points in each execution, making the
attack intractable. However, we show that shuffling in software can leak
information which can be used to subvert this solution. Therefore, to perform
secure shuffling and reduce latency, we present BlackJack, hardware added as a
functional unit within the CPU. BlackJack secures neural networks on IoT
devices by increasing the time needed for an attack to centuries, while adding
just 2.46% area, 3.28% power and 0.56% latency overhead on an ARM M0+ SoC.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17805" title="Abstract">arXiv:2310.17805</a> [<a href="/pdf/2310.17805" title="Download PDF">pdf</a>, <a href="/format/2310.17805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Scale Robustness for Proximal Policy Optimization via DreamerV3  Tricks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+R">Ryan Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akarsh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shengyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dickerson%2C+J+P">John P. Dickerson</a>, 
<a href="/search/cs?searchtype=author&query=Suarez%2C+J">Joseph Suarez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Most reinforcement learning methods rely heavily on dense, well-normalized
environment rewards. DreamerV3 recently introduced a model-based method with a
number of tricks that mitigate these limitations, achieving state-of-the-art on
a wide range of benchmarks with a single set of hyperparameters. This result
sparked discussion about the generality of the tricks, since they appear to be
applicable to other reinforcement learning algorithms. Our work applies
DreamerV3's tricks to PPO and is the first such empirical study outside of the
original work. Surprisingly, we find that the tricks presented do not transfer
as general improvements to PPO. We use a high quality PPO reference
implementation and present extensive ablation studies totaling over 10,000 A100
hours on the Arcade Learning Environment and the DeepMind Control Suite. Though
our experiments demonstrate that these tricks do not generally outperform PPO,
we identify cases where they succeed and offer insight into the relationship
between the implementation tricks. In particular, PPO with these tricks
performs comparably to PPO on Atari games with reward clipping and
significantly outperforms PPO without reward clipping.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17807" title="Abstract">arXiv:2310.17807</a> [<a href="/pdf/2310.17807" title="Download PDF">pdf</a>, <a href="/format/2310.17807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clover: Closed-Loop Verifiable Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chuyue Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Y">Ying Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Padon%2C+O">Oded Padon</a>, 
<a href="/search/cs?searchtype=author&query=Barrett%2C+C">Clark Barrett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The use of large language models for code generation is a rapidly growing
trend in software development. However, without effective methods for ensuring
the correctness of generated code, this trend could lead to any number of
undesirable outcomes. In this paper, we lay out a vision for addressing this
challenge: the Clover paradigm, short for Closed-Loop Verifiable Code
Generation, which reduces correctness checking to the more accessible problem
of consistency checking. At the core of Clover lies a checker that performs
consistency checks among code, docstrings, and formal annotations. The checker
is implemented using a novel integration of formal verification tools and large
language models. We provide a theoretical analysis to support our thesis that
Clover should be effective at consistency checking. We also empirically
investigate its feasibility on a hand-designed dataset (CloverBench) featuring
annotated Dafny programs at a textbook level of difficulty. Experimental
results show that for this dataset, (i) LLMs are reasonably successful at
automatically generating formal specifications; and (ii) our consistency
checker achieves a promising acceptance rate (up to 87%) for correct instances
while maintaining zero tolerance for incorrect ones (no false positives).
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17811" title="Abstract">arXiv:2310.17811</a> [<a href="/pdf/2310.17811" title="Download PDF">pdf</a>, <a href="/format/2310.17811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Style-Aware Radiology Report Generation with RadGraph and Few-Shot  Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Benjamin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+D+E">David E. Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Adithan%2C+S">Subathra Adithan</a>, 
<a href="/search/cs?searchtype=author&query=Reis%2C+E+P">Eduardo Pontes Reis</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+S">Stephen Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Venugopal%2C+V+K">Vasantha Kumar Venugopal</a>, 
<a href="/search/cs?searchtype=author&query=O%27Connell%2C+C+P">Chloe P. O&#x27;Connell</a>, 
<a href="/search/cs?searchtype=author&query=Saenz%2C+A">Agustina Saenz</a>, 
<a href="/search/cs?searchtype=author&query=Rajpurkar%2C+P">Pranav Rajpurkar</a>, 
<a href="/search/cs?searchtype=author&query=Moor%2C+M">Michael Moor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Automatically generated reports from medical images promise to improve the
workflow of radiologists. Existing methods consider an image-to-report modeling
task by directly generating a fully-fledged report from an image. However, this
conflates the content of the report (e.g., findings and their attributes) with
its style (e.g., format and choice of words), which can lead to clinically
inaccurate reports. To address this, we propose a two-step approach for
radiology report generation. First, we extract the content from an image; then,
we verbalize the extracted content into a report that matches the style of a
specific radiologist. For this, we leverage RadGraph -- a graph representation
of reports -- together with large language models (LLMs). In our quantitative
evaluations, we find that our approach leads to beneficial performance. Our
human evaluation with clinical raters highlights that the AI-generated reports
are indistinguishably tailored to the style of individual radiologist despite
leveraging only a few examples as context.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17813" title="Abstract">arXiv:2310.17813</a> [<a href="/pdf/2310.17813" title="Download PDF">pdf</a>, <a href="/format/2310.17813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spectral Condition for Feature Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Greg Yang</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+J+B">James B. Simon</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+J">Jeremy Bernstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The push to train ever larger neural networks has motivated the study of
initialization and training at large network width. A key challenge is to scale
training so that a network's internal representations evolve nontrivially at
all widths, a process known as feature learning. Here, we show that feature
learning is achieved by scaling the spectral norm of weight matrices and their
updates like $\sqrt{\texttt{fan-out}/\texttt{fan-in}}$, in contrast to widely
used but heuristic scalings based on Frobenius norm and entry size. Our
spectral scaling analysis also leads to an elementary derivation of
\emph{maximal update parametrization}. All in all, we aim to provide the reader
with a solid conceptual understanding of feature learning in neural networks.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17814" title="Abstract">arXiv:2310.17814</a> [<a href="/pdf/2310.17814" title="Download PDF">pdf</a>, <a href="/format/2310.17814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIVI: Dynamically Interactive Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Snyder%2C+L+S">Luke S. Snyder</a>, 
<a href="/search/cs?searchtype=author&query=Heer%2C+J">Jeffrey Heer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 pages supplementary material, 10 figures, IEEE TVCG 2024 (Proc. VIS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Dynamically Interactive Visualization (DIVI) is a novel approach for
orchestrating interactions within and across static visualizations. DIVI
deconstructs Scalable Vector Graphics charts at runtime to infer content and
coordinate user input, decoupling interaction from specification logic. This
decoupling allows interactions to extend and compose freely across different
tools, chart types, and analysis goals. DIVI exploits positional relations of
marks to detect chart components such as axes and legends, reconstruct scales
and view encodings, and infer data fields. DIVI then enumerates candidate
transformations across inferred data to perform linking between views. To
support dynamic interaction without prior specification, we introduce a
taxonomy that formalizes the space of standard interactions by chart element,
interaction type, and input event. We demonstrate DIVI's usefulness for rapid
data exploration and analysis through a usability study with 13 participants
and a diverse gallery of dynamically interactive visualizations, including
single chart, multi-view, and cross-tool configurations.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17826" title="Abstract">arXiv:2310.17826</a> [<a href="/pdf/2310.17826" title="Download PDF">pdf</a>, <a href="/format/2310.17826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniFill: Domain-Agnostic Form Filling Suggestions Using Multi-Faceted  Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aveni%2C+T+J">Timothy J. Aveni</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+A">Armando Fox</a>, 
<a href="/search/cs?searchtype=author&query=Hartmann%2C+B">Bj&#xf6;rn Hartmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Predictive suggestion systems offer contextually-relevant text entry
completions. Existing approaches, like autofill, often excel in
narrowly-defined domains but fail to generalize to arbitrary workflows. We
introduce a conceptual framework to analyze the compound demands of a
particular suggestion context, yielding unique opportunities for large language
models (LLMs) to infer suggestions for a wide range of domain-agnostic
form-filling tasks that were out of reach with prior approaches. We explore
these opportunities in OmniFill, a prototype that collects multi-faceted
context including browsing and text entry activity to construct an LLM prompt
that offers suggestions in situ for arbitrary structured text entry interfaces.
Through a user study with 18 participants, we found that OmniFill offered
valuable suggestions and we identified four themes that characterize users'
behavior and attitudes: an "opportunistic scrapbooking" approach; a trust
placed in the system; value in partial success; and a need for visibility into
prompt context.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17835" title="Abstract">arXiv:2310.17835</a> [<a href="/pdf/2310.17835" title="Download PDF">pdf</a>, <a href="/format/2310.17835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Style is All you Need to Generate a Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manandhar%2C+S">Sandeep Manandhar</a>, 
<a href="/search/cs?searchtype=author&query=Genovesio%2C+A">Auguste Genovesio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we propose a style-based conditional video generative model.
We introduce a novel temporal generator based on a set of learned sinusoidal
bases. Our method learns dynamic representations of various actions that are
independent of image content and can be transferred between different actors.
Beyond the significant enhancement of video quality compared to prevalent
methods, we demonstrate that the disentangled dynamic and content permit their
independent manipulation, as well as temporal GAN-inversion to retrieve and
transfer a video motion from one content or identity to another without further
preprocessing such as landmark points.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17836" title="Abstract">arXiv:2310.17836</a> [<a href="/pdf/2310.17836" title="Download PDF">pdf</a>, <a href="/format/2310.17836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positional Encoding-based Resident Identification in Multi-resident  Smart Homes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhiyi Song</a>, 
<a href="/search/cs?searchtype=author&query=Chaki%2C+D">Dipankar Chaki</a>, 
<a href="/search/cs?searchtype=author&query=Lakhdari%2C+A">Abdallah Lakhdari</a>, 
<a href="/search/cs?searchtype=author&query=Bouguettaya%2C+A">Athman Bouguettaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 11 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We propose a novel resident identification framework to identify residents in
a multi-occupant smart environment. The proposed framework employs a feature
extraction model based on the concepts of positional encoding. The feature
extraction model considers the locations of homes as a graph. We design a novel
algorithm to build such graphs from layout maps of smart environments. The
Node2Vec algorithm is used to transform the graph into high-dimensional node
embeddings. A Long Short-Term Memory (LSTM) model is introduced to predict the
identities of residents using temporal sequences of sensor events with the node
embeddings. Extensive experiments show that our proposed scheme effectively
identifies residents in a multi-occupant environment. Evaluation results on two
real-world datasets demonstrate that our proposed approach achieves 94.5% and
87.9% accuracy, respectively.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17838" title="Abstract">arXiv:2310.17838</a> [<a href="/pdf/2310.17838" title="Download PDF">pdf</a>, <a href="/format/2310.17838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Animation Generation and Control on Rigged Models via Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Han Huang</a>, 
<a href="/search/cs?searchtype=author&query=De+La+Torre%2C+F">Fernanda De La Torre</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C+M">Cathy Mengying Fang</a>, 
<a href="/search/cs?searchtype=author&query=Banburski-Fahey%2C+A">Andrzej Banburski-Fahey</a>, 
<a href="/search/cs?searchtype=author&query=Amores%2C+J">Judith Amores</a>, 
<a href="/search/cs?searchtype=author&query=Lanier%2C+J">Jaron Lanier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS Workshop on ML for Creativity and Design 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a novel method for real-time animation control and generation on
rigged models using natural language input. First, we embed a large language
model (LLM) in Unity to output structured texts that can be parsed into diverse
and realistic animations. Second, we illustrate LLM's potential to enable
flexible state transition between existing animations. We showcase the
robustness of our approach through qualitative results on various rigged models
and motions.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17842" title="Abstract">arXiv:2310.17842</a> [<a href="/pdf/2310.17842" title="Download PDF">pdf</a>, <a href="/format/2310.17842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What You See Is What You Detect: Towards better Object Densification in  3D detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pasandi%2C+Z+Z+M+M">Zeping Zhang Morteza Mousa Pasandi</a>, 
<a href="/search/cs?searchtype=author&query=Laganiere%2C+R">Robert Laganiere</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Recent works have demonstrated the importance of object completion in 3D
Perception from Lidar signal. Several methods have been proposed in which
modules were used to densify the point clouds produced by laser scanners,
leading to better recall and more accurate results. Pursuing in that direction,
we present, in this work, a counter-intuitive perspective: the widely-used
full-shape completion approach actually leads to a higher error-upper bound
especially for far away objects and small objects like pedestrians. Based on
this observation, we introduce a visible part completion method that requires
only 11.3\% of the prediction points that previous methods generate. To recover
the dense representation, we propose a mesh-deformation-based method to augment
the point set associated with visible foreground objects. Considering that our
approach focuses only on the visible part of the foreground objects to achieve
accurate 3D detection, we named our method What You See Is What You Detect
(WYSIWYD). Our proposed method is thus a detector-independent model that
consists of 2 parts: an Intra-Frustum Segmentation Transformer (IFST) and a
Mesh Depth Completion Network(MDCNet) that predicts the foreground depth from
mesh deformation. This way, our model does not require the time-consuming
full-depth completion task used by most pseudo-lidar-based methods. Our
experimental evaluation shows that our approach can provide up to 12.2\%
performance improvements over most of the public baseline models on the KITTI
and NuScenes dataset bringing the state-of-the-art to a new level. The codes
will be available at
\textcolor[RGB]{0,0,255}{\url{{https://github.com/Orbis36/WYSIWYD}}
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17843" title="Abstract">arXiv:2310.17843</a> [<a href="/pdf/2310.17843" title="Download PDF">pdf</a>, <a href="/format/2310.17843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Centric Online Market for Machine Learning: From Discovery to  Pricing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+M">Minbiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Light%2C+J">Jonathan Light</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Steven Xia</a>, 
<a href="/search/cs?searchtype=author&query=Galhotra%2C+S">Sainyam Galhotra</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+R+C">Raul Castro Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Data fuels machine learning (ML) - rich and high-quality training data is
essential to the success of ML. However, to transform ML from the race among a
few large corporations to an accessible technology that serves numerous normal
users' data analysis requests, there still exist important challenges. One gap
we observed is that many ML users can benefit from new data that other data
owners possess, whereas these data owners sit on piles of data without knowing
who can benefit from it. This gap creates the opportunity for building an
online market that can automatically connect supply with demand. While online
matching markets are prevalent (e.g., ride-hailing systems), designing a
data-centric market for ML exhibits many unprecedented challenges.
<br />This paper develops new techniques to tackle two core challenges in designing
such a market: (a) to efficiently match demand with supply, we design an
algorithm to automatically discover useful data for any ML task from a pool of
thousands of datasets, achieving high-quality matching between ML models and
data; (b) to encourage market participation of ML users without much ML
expertise, we design a new pricing mechanism for selling data-augmented ML
models. Furthermore, our market is designed to be API-compatible with existing
online ML markets like Vertex AI and Sagemaker, making it easy to use while
providing better results due to joint data and model search. We envision that
the synergy of our data and model discovery algorithm and pricing mechanism
will be an important step towards building a new data-centric online market
that serves ML users effectively.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17844" title="Abstract">arXiv:2310.17844</a> [<a href="/pdf/2310.17844" title="Download PDF">pdf</a>, <a href="/format/2310.17844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive operator learning for infinite-dimensional Bayesian inverse  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+Z">Zhiwei Gao</a>, 
<a href="/search/math?searchtype=author&query=Yan%2C+L">Liang Yan</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">The fundamental computational issues in Bayesian inverse problems (BIPs)
governed by partial differential equations (PDEs) stem from the requirement of
repeated forward model evaluations. A popular strategy to reduce such cost is
to replace expensive model simulations by computationally efficient
approximations using operator learning, motivated by recent progresses in deep
learning. However, using the approximated model directly may introduce a
modeling error, exacerbating the already ill-posedness of inverse problems.
Thus, balancing between accuracy and efficiency is essential for the effective
implementation of such approaches. To this end, we develop an adaptive operator
learning framework that can reduce modeling error gradually by forcing the
surrogate to be accurate in local areas. This is accomplished by fine-tuning
the pre-trained approximate model during the inversion process with adaptive
points selected by a greedy algorithm, which requires only a few forward model
evaluations. To validate our approach, we adopt DeepOnet to construct the
surrogate and use unscented Kalman inversion (UKI) to approximate the solution
of BIPs, respectively. Furthermore, we present rigorous convergence guarantee
in the linear case using the framework of UKI. We test the approach on several
benchmarks, including the Darcy flow, the heat source inversion problem, and
the reaction diffusion problems. Numerical results demonstrate that our method
can significantly reduce computational costs while maintaining inversion
accuracy.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17846" title="Abstract">arXiv:2310.17846</a> [<a href="/pdf/2310.17846" title="Download PDF">pdf</a>, <a href="/format/2310.17846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Awareness to Action: Exploring End-User Empowerment Interventions  for Dark Patterns in UX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuwen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuewen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yaxing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conditionally Accepted at CSCW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The study of UX dark patterns, i.e., UI designs that seek to manipulate user
behaviors, often for the benefit of online services, has drawn significant
attention in the CHI and CSCW communities in recent years. To complement
previous studies in addressing dark patterns from (1) the designer's
perspective on education and advocacy for ethical designs; and (2) the
policymaker's perspective on new regulations, we propose an
end-user-empowerment intervention approach that helps users (1) raise the
awareness of dark patterns and understand their underlying design intents; (2)
take actions to counter the effects of dark patterns using a web augmentation
approach. Through a two-phase co-design study, including 5 co-design workshops
(N=12) and a 2-week technology probe study (N=15), we reported findings on the
understanding of users' needs, preferences, and challenges in handling dark
patterns and investigated the feedback and reactions to users' awareness of and
action on dark patterns being empowered in a realistic in-situ setting.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17849" title="Abstract">arXiv:2310.17849</a> [<a href="/pdf/2310.17849" title="Download PDF">pdf</a>, <a href="/format/2310.17849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Choosing Initial Values of Iteratively Reweighted $\ell_1$ Algorithms  for the Piece-wise Exponential Penalty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+R">Rongrong Lin</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+S">Shimin Li</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yulan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Computing the proximal operator of the sparsity-promoting piece-wise
exponential (PiE) penalty $1-e^{-|x|/\sigma}$ with a given shape parameter
$\sigma&gt;0$, which is treated as a popular nonconvex surrogate of $\ell_0$-norm,
is fundamental in feature selection via support vector machines, image
reconstruction, zero-one programming problems, compressed sensing, etc. Due to
the nonconvexity of PiE, for a long time, its proximal operator is frequently
evaluated via an iteratively reweighted $\ell_1$ algorithm, which substitutes
PiE with its first-order approximation, however, the obtained solutions only
are the critical point. Based on the exact characterization of the proximal
operator of PiE, we explore how the iteratively reweighted $\ell_1$ solution
deviates from the true proximal operator in certain regions, which can be
explicitly identified in terms of $\sigma$, the initial value and the
regularization parameter in the definition of the proximal operator. Moreover,
the initial value can be adaptively and simply chosen to ensure that the
iteratively reweighted $\ell_1$ solution belongs to the proximal operator of
PiE.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17851" title="Abstract">arXiv:2310.17851</a> [<a href="/pdf/2310.17851" title="Download PDF">pdf</a>, <a href="/format/2310.17851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring CDNs susceptible to Domain Fronting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramani%2C+K">Karthika Subramani</a>, 
<a href="/search/cs?searchtype=author&query=Perdisci%2C+R">Roberto Perdisci</a>, 
<a href="/search/cs?searchtype=author&query=Skafidas%2C+P">Pierros Skafidas</a>, 
<a href="/search/cs?searchtype=author&query=Antonakakis%2C+M">Manos Antonakakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Domain fronting is a network communication technique that involves leveraging
(or abusing) content delivery networks (CDNs) to disguise the final destination
of network packets by presenting them as if they were intended for a different
domain than their actual endpoint. This technique can be used for both benign
and malicious purposes, such as circumventing censorship or hiding
malware-related communications from network security systems. Since domain
fronting has been known for a few years, some popular CDN providers have
implemented traffic filtering approaches to curb its use at their CDN
infrastructure. However, it remains unclear to what extent domain fronting has
been mitigated.
<br />To better understand whether domain fronting can still be effectively used,
we propose a systematic approach to discover CDNs that are still prone to
domain fronting. To this end, we leverage passive and active DNS traffic
analysis to pinpoint domain names served by CDNs and build an automated tool
that can be used to discover CDNs that allow domain fronting in their
infrastructure. Our results reveal that domain fronting is feasible in 22 out
of 30 CDNs that we tested, including some major CDN providers like Akamai and
Fastly. This indicates that domain fronting remains widely available and can be
easily abused for malicious purposes.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17852" title="Abstract">arXiv:2310.17852</a> [<a href="/pdf/2310.17852" title="Download PDF">pdf</a>, <a href="/format/2310.17852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Function Space Bayesian Pseudocoreset for Bayesian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Balhae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyungi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juho Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential
information of a large-scale dataset and thus can be used as a proxy dataset
for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is
constructed by minimizing a divergence measure between the posterior
conditioning on the pseudocoreset and the posterior conditioning on the full
dataset. However, evaluating the divergence can be challenging, particularly
for the models like deep neural networks having high-dimensional parameters. In
this paper, we propose a novel Bayesian pseudocoreset construction method that
operates on a function space. Unlike previous methods, which construct and
match the coreset and full data posteriors in the space of model parameters
(weights), our method constructs variational approximations to the coreset
posterior on a function space and matches it to the full data posterior in the
function space. By working directly on the function space, our method could
bypass several challenges that may arise when working on a weight space,
including limited scalability and multi-modality issue. Through various
experiments, we demonstrate that the Bayesian pseudocoresets constructed from
our method enjoys enhanced uncertainty quantification and better robustness
across various model architectures.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17857" title="Abstract">arXiv:2310.17857</a> [<a href="/pdf/2310.17857" title="Download PDF">pdf</a>, <a href="/format/2310.17857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Values to Opinions: Predicting Human Behaviors and Stances Using  Value-Injected Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongjun Kang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Joonsuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+Y">Yohan Jo</a>, 
<a href="/search/cs?searchtype=author&query=Bak%2C+J">JinYeong Bak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main paper accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Being able to predict people's opinions on issues and behaviors in realistic
scenarios can be helpful in various domains, such as politics and marketing.
However, conducting large-scale surveys like the European Social Survey to
solicit people's opinions on individual issues can incur prohibitive costs.
Leveraging prior research showing influence of core human values on individual
decisions and actions, we propose to use value-injected large language models
(LLM) to predict opinions and behaviors. To this end, we present Value
Injection Method (VIM), a collection of two methods -- argument generation and
question answering -- designed to inject targeted value distributions into LLMs
via fine-tuning. We then conduct a series of experiments on four tasks to test
the effectiveness of VIM and the possibility of using value-injected LLMs to
predict opinions and behaviors of people. We find that LLMs value-injected with
variations of VIM substantially outperform the baselines. Also, the results
suggest that opinions and behaviors can be better predicted using
value-injected LLMs than the baseline approaches.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17861" title="Abstract">arXiv:2310.17861</a> [<a href="/pdf/2310.17861" title="Download PDF">pdf</a>, <a href="/format/2310.17861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Wrist Exosuit Actuated by Fabric Pneumatic Artificial Muscles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4ffer%2C+K">Katalin Sch&#xe4;ffer</a>, 
<a href="/search/cs?searchtype=author&query=Ozkan-Aydin%2C+Y">Yasemin Ozkan-Aydin</a>, 
<a href="/search/cs?searchtype=author&query=Coad%2C+M+M">Margaret M. Coad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recently, soft actuator-based exosuits have gained interest, due to their
high strength-to-weight ratio, inherent safety, and low cost. We present a
novel wrist exosuit actuated by fabric pneumatic artificial muscles that can
move the wrist in flexion/extension and ulnar/radial deviation. We derive a
model representing the torque exerted by the exosuit and introduce a
model-based optimization methodology for the selection of placement parameters
of the exosuit muscles. We evaluate the accuracy of the model by measuring the
exosuit torques throughout the full range of wrist flexion/extension. When
accounting for the displacement of the mounting points, the model predicts the
exosuit torque with a mean absolute error of 0.279 Nm, which is 26.1% of the
average measured torque. To explore the capabilities of the exosuit to move the
human body, we measure its range of motion on a passive human wrist; the
exosuit is able to achieve 55.0% of the active biological range in flexion,
69.1% in extension, 68.6% in ulnar deviation, and 68.4% in radial deviation.
Finally, we demonstrate the device controlling the passive human wrist to move
to a desired orientation in the flexion/extension plane and along a
two-degree-of-freedom trajectory.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17863" title="Abstract">arXiv:2310.17863</a> [<a href="/pdf/2310.17863" title="Download PDF">pdf</a>, <a href="/format/2310.17863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dimensionally Homogeneous Jacobian using Extended Selection Matrix for  Performance Evaluation and Optimization of Parallel Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nigatu%2C+H">Hassen Nigatu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doik Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">This paper proposes a new methodology for deriving a point-based
dimensionally homogeneous Jacobian, intended for performance evaluation and
optimization of parallel manipulators with mixed degrees of freedom. Optimal
manipulator often rely on performance indices obtained from the Jacobian
matrix. However, when manipulators exhibit mixed translational and rotational
freedoms, the conventional Jacobian's inconsistency of units lead to unbalanced
optimal result. Addressing this issue, a point-based dimensionally homogeneous
Jacobian has appeared as a prominent solution. However, existing point-based
approaches for formulating dimensionally homogeneous Jacobian are applicable to
a limited variety of parallel manipulators. Moreover, they are complicated and
less intuitive. This paper introduces an extended selection matrix that
combines component velocities from different points to describe the entire
motion of moving plate. This proposed approach enables us to formulate an
intuitive point-based, dimensionally homogeneous Jacobian, which can be applied
to a wide variety of constrained parallel manipulators. To prove the validity
of proposed method, a numerical example is provided utilizing a
four-degree-of-freedom parallel manipulator.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17868" title="Abstract">arXiv:2310.17868</a> [<a href="/pdf/2310.17868" title="Download PDF">pdf</a>, <a href="/format/2310.17868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Allocation for Near-Field Communications: Fundamentals, Tools,  and Outlooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bokai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+B">Bo Ai</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Extremely large-scale multiple-input-multiple output (XL-MIMO) is a promising
technology to achieve high spectral efficiency (SE) and energy efficiency (EE)
in future wireless systems. The larger array aperture of XL-MIMO makes
communication scenarios closer to the near-field region. Therefore, near-field
resource allocation is essential in realizing the above key performance
indicators (KPIs). Moreover, the overall performance of XL-MIMO systems heavily
depends on the channel characteristics of the selected users, eliminating
interference between users through beamforming, power control, etc. The above
resource allocation issue constitutes a complex joint multi-objective
optimization problem since many variables and parameters must be optimized,
including the spatial degree of freedom, rate, power allocation, and
transmission technique. In this article, we review the basic properties of
near-field communications and focus on the corresponding "resource allocation"
problems. First, we identify available resources in near-field communication
systems and highlight their distinctions from far-field communications. Then,
we summarize optimization tools, such as numerical techniques and machine
learning methods, for addressing near-field resource allocation, emphasizing
their strengths and limitations. Finally, several important research directions
of near-field communications are pointed out for further investigation.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17869" title="Abstract">arXiv:2310.17869</a> [<a href="/pdf/2310.17869" title="Download PDF">pdf</a>, <a href="/format/2310.17869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grid Jigsaw Representation with CLIP: A New Perspective on Image  Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zijie Song</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhenzhen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+R">Richang Hong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised representation learning for image clustering is essential in
computer vision. Although the advancement of visual models has improved image
clustering with efficient visual representations, challenges still remain.
Firstly, these features often lack the ability to represent the internal
structure of images, hindering the accurate clustering of visually similar
images. Secondly, the existing features tend to lack finer-grained semantic
labels, limiting the ability to capture nuanced differences and similarities
between images.
<br />In this paper, we first introduce Jigsaw based strategy method for image
clustering called Grid Jigsaw Representation (GJR) with systematic exposition
from pixel to feature in discrepancy against human and computer. We emphasize
that this algorithm, which mimics human jigsaw puzzle, can effectively improve
the model to distinguish the spatial feature between different samples and
enhance the clustering ability. GJR modules are appended to a variety of deep
convolutional networks and tested with significant improvements on a wide range
of benchmark datasets including CIFAR-10, CIFAR-100/20, STL-10, ImageNet-10 and
ImageNetDog-15.
<br />On the other hand, convergence efficiency is always an important challenge
for unsupervised image clustering. Recently, pretrained representation learning
has made great progress and released models can extract mature visual
representations. It is obvious that use the pretrained model as feature
extractor can speed up the convergence of clustering where our aim is to
provide new perspective in image clustering with reasonable resource
application and provide new baseline. Further, we innovate pretrain-based Grid
Jigsaw Representation (pGJR) with improvement by GJR. The experiment results
show the effectiveness on the clustering task with respect to the ACC, NMI and
ARI three metrics and super fast convergence speed.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17870" title="Abstract">arXiv:2310.17870</a> [<a href="/pdf/2310.17870" title="Download PDF">pdf</a>, <a href="/format/2310.17870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranking with Slot Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wentao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Andrew Wang</a>, 
<a href="/search/cs?searchtype=author&query=Thymes%2C+B">Bradon Thymes</a>, 
<a href="/search/cs?searchtype=author&query=Joachims%2C+T">Thorsten Joachims</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce the problem of ranking with slot constraints, which can be used
to model a wide range of application problems -- from college admission with
limited slots for different majors, to composing a stratified cohort of
eligible participants in a medical trial. We show that the conventional
Probability Ranking Principle (PRP) can be highly sub-optimal for
slot-constrained ranking problems, and we devise a new ranking algorithm,
called MatchRank. The goal of MatchRank is to produce rankings that maximize
the number of filled slots if candidates are evaluated by a human decision
maker in the order of the ranking. In this way, MatchRank generalizes the PRP,
and it subsumes the PRP as a special case when there are no slot constraints.
Our theoretical analysis shows that MatchRank has a strong approximation
guarantee without any independence assumptions between slots or candidates.
Furthermore, we show how MatchRank can be implemented efficiently. Beyond the
theoretical guarantees, empirical evaluations show that MatchRank can provide
substantial improvements over a range of synthetic and real-world tasks.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17872" title="Abstract">arXiv:2310.17872</a> [<a href="/pdf/2310.17872" title="Download PDF">pdf</a>, <a href="/format/2310.17872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Association and Resource Allocation in Large Language Model Based  Mobile Edge Computing System over Wireless Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+L">Liangxin Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In the rapidly evolving landscape of large language models (LLMs) and mobile
edge computing, the need for efficient service delivery to mobile users with
constrained computational resources has become paramount. Addressing this, our
paper delves into a collaborative framework for model training where user data
and model adapters are shared with servers to optimize performance. Within this
framework, users initially update the first several layers of the adapters
while freezing the other layers of them, leveraging their local datasets. Once
this step is complete, these partially trained parameters are transmitted to
servers. The servers, equipped with more robust computational capabilities,
then update the subsequent layers. After this training, they send the enhanced
parameters back to the users. This collaborative training approach ensures that
mobile users with limited computational capacities can still benefit from
advanced LLM services without being burdened by exhaustive computations.
Central to our methodology is the DASHF algorithm, which encapsulates the
Dinkelbach algorithm, alternating optimization, semidefinite relaxation (SDR),
the Hungarian method, and a pioneering fractional programming technique from
our recent IEEE JSAC paper "Human-Centric Resource Allocation in the Metaverse
over Wireless Communications". The crux of DASHF is its capability to
reformulate an optimization problem as Quadratically Constrained Quadratic
Programming (QCQP) via meticulously crafted transformations, making it solvable
by SDR and the Hungarian algorithm. Through extensive simulations, we
demonstrate the effectiveness of the DASHF algorithm, offering significant
insights for the advancement of collaborative LLM service deployments.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17874" title="Abstract">arXiv:2310.17874</a> [<a href="/pdf/2310.17874" title="Download PDF">pdf</a>, <a href="/format/2310.17874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmooSeg: Smoothness Prior for Unsupervised Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+M">Mengcheng Lan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinjiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">Yiping Ke</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaxing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Litong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wayne Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. Code available: <a href="https://github.com/mc-lan/SmooSeg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised semantic segmentation is a challenging task that segments images
into semantic groups without manual annotation. Prior works have primarily
focused on leveraging prior knowledge of semantic consistency or priori
concepts from self-supervised learning methods, which often overlook the
coherence property of image segments. In this paper, we demonstrate that the
smoothness prior, asserting that close features in a metric space share the
same semantics, can significantly simplify segmentation by casting unsupervised
semantic segmentation as an energy minimization problem. Under this paradigm,
we propose a novel approach called SmooSeg that harnesses self-supervised
learning methods to model the closeness relationships among observations as
smoothness signals. To effectively discover coherent semantic segments, we
introduce a novel smoothness loss that promotes piecewise smoothness within
segments while preserving discontinuities across different segments.
Additionally, to further enhance segmentation quality, we design an asymmetric
teacher-student style predictor that generates smoothly updated pseudo labels,
facilitating an optimal fit between observations and labeling outputs. Thanks
to the rich supervision cues of the smoothness prior, our SmooSeg significantly
outperforms STEGO in terms of pixel accuracy on three datasets: COCOStuff
(+14.9%), Cityscapes (+13.0%), and Potsdam-3 (+5.7%).
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17875" title="Abstract">arXiv:2310.17875</a> [<a href="/pdf/2310.17875" title="Download PDF">pdf</a>, <a href="/format/2310.17875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Siamese-DETR for Generic Multi-Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiankun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Ying Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The ability to detect and track the dynamic objects in different scenes is
fundamental to real-world applications, e.g., autonomous driving and robot
navigation. However, traditional Multi-Object Tracking (MOT) is limited to
tracking objects belonging to the pre-defined closed-set categories. Recently,
Open-Vocabulary MOT (OVMOT) and Generic MOT (GMOT) are proposed to track
interested objects beyond pre-defined categories with the given text prompt and
template image. However, the expensive well pre-trained (vision-)language model
and fine-grained category annotations are required to train OVMOT models. In
this paper, we focus on GMOT and propose a simple but effective method,
Siamese-DETR, for GMOT. Only the commonly used detection datasets (e.g., COCO)
are required for training. Different from existing GMOT methods, which train a
Single Object Tracking (SOT) based detector to detect interested objects and
then apply a data association based MOT tracker to get the trajectories, we
leverage the inherent object queries in DETR variants. Specifically: 1) The
multi-scale object queries are designed based on the given template image,
which are effective for detecting different scales of objects with the same
category as the template image; 2) A dynamic matching training strategy is
introduced to train Siamese-DETR on commonly used detection datasets, which
takes full advantage of provided annotations; 3) The online tracking pipeline
is simplified through a tracking-by-query manner by incorporating the tracked
boxes in previous frame as additional query boxes. The complex data association
is replaced with the much simpler Non-Maximum Suppression (NMS). Extensive
experimental results show that Siamese-DETR surpasses existing MOT methods on
GMOT-40 dataset by a large margin.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17876" title="Abstract">arXiv:2310.17876</a> [<a href="/pdf/2310.17876" title="Download PDF">pdf</a>, <a href="/format/2310.17876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TarGEN: Targeted Data Generation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H">Himanshu Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Scaria%2C+K">Kevin Scaria</a>, 
<a href="/search/cs?searchtype=author&query=Anantheswaran%2C+U">Ujjwala Anantheswaran</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+S">Shreyas Verma</a>, 
<a href="/search/cs?searchtype=author&query=Parmar%2C+M">Mihir Parmar</a>, 
<a href="/search/cs?searchtype=author&query=Sawant%2C+S+A">Saurabh Arjun Sawant</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 tables, 5 figures, 5 pages references, 17 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The rapid advancement of large language models (LLMs) has sparked interest in
data synthesis techniques, aiming to generate diverse and high-quality
synthetic datasets. However, these synthetic datasets often suffer from a lack
of diversity and added noise. In this paper, we present TarGEN, a multi-step
prompting strategy for generating high-quality synthetic datasets utilizing a
LLM. An advantage of TarGEN is its seedless nature; it does not require
specific task instances, broadening its applicability beyond task replication.
We augment TarGEN with a method known as self-correction empowering LLMs to
rectify inaccurately labeled instances during dataset creation, ensuring
reliable labels. To assess our technique's effectiveness, we emulate 8 tasks
from the SuperGLUE benchmark and finetune various language models, including
encoder-only, encoder-decoder, and decoder-only models on both synthetic and
original training sets. Evaluation on the original test set reveals that models
trained on datasets generated by TarGEN perform approximately 1-2% points
better than those trained on original datasets (82.84% via syn. vs. 81.12% on
og. using Flan-T5). When incorporating instruction tuning, the performance
increases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A
comprehensive analysis of the synthetic dataset compared to the original
dataset reveals that the synthetic dataset demonstrates similar or higher
levels of dataset complexity and diversity. Furthermore, the synthetic dataset
displays a bias level that aligns closely with the original dataset. Finally,
when pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive
results on the OpenLLM leaderboard, surpassing the model trained on the
Self-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for
quality data generation and reducing the human efforts to create complex
benchmarks.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17877" title="Abstract">arXiv:2310.17877</a> [<a href="/pdf/2310.17877" title="Download PDF">pdf</a>, <a href="/format/2310.17877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for  Consistent Data-to-Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vejvar%2C+M">Martin Vejvar</a>, 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+Y">Yasutaka Fujimoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP2023, code available at <a href="https://github.com/vejvarm/ASPIRO">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present ASPIRO, an approach for structured data verbalisation into short
template sentences in zero to few-shot settings. Unlike previous methods, our
approach prompts large language models (LLMs) to directly produce
entity-agnostic templates, rather than relying on LLMs to faithfully copy the
given example entities, or validating/crafting the templates manually. We
incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well
as the PARENT metric induced consistency validation to identify and rectify
template generation problems in real-time. ASPIRO, compared to direct LLM
output, averages 66\% parsing error rate reduction in generated verbalisations
of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup,
scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and
PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent
fine-tuned pre-trained language models.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17878" title="Abstract">arXiv:2310.17878</a> [<a href="/pdf/2310.17878" title="Download PDF">pdf</a>, <a href="/format/2310.17878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing  Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+R">Ranran Shen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+P">Pan Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We address the problem of designing a sublinear-time spectral clustering
oracle for graphs that exhibit strong clusterability. Such graphs contain $k$
latent clusters, each characterized by a large inner conductance (at least
$\varphi$) and a small outer conductance (at most $\varepsilon$). Our aim is to
preprocess the graph to enable clustering membership queries, with the key
requirement that both preprocessing and query answering should be performed in
sublinear time, and the resulting partition should be consistent with a
$k$-partition that is close to the ground-truth clustering. Previous oracles
have relied on either a $\textrm{poly}(k)\log n$ gap between inner and outer
conductances or exponential (in $k/\varepsilon$) preprocessing time. Our
algorithm relaxes these assumptions, albeit at the cost of a slightly higher
misclassification ratio. We also show that our clustering oracle is robust
against a few random edge deletions. To validate our theoretical bounds, we
conducted experiments on synthetic networks.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17879" title="Abstract">arXiv:2310.17879</a> [<a href="/pdf/2310.17879" title="Download PDF">pdf</a>, <a href="/format/2310.17879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Split Covariance Intersection Filter Based Visual Localization With  Accurate AprilTag Map For Warehouse Robot Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Susu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Accurate and efficient localization with conveniently-established map is the
fundamental requirement for mobile robot operation in warehouse environments.
An accurate AprilTag map can be conveniently established with the help of
LiDAR-based SLAM. It is true that a LiDAR-based system is usually not
commercially competitive in contrast with a vision-based system, yet
fortunately for warehouse applications, only a single LiDAR-based SLAM system
is needed to establish an accurate AprilTag map, whereas a large amount of
visual localization systems can share this established AprilTag map for their
own operations. Therefore, the cost of a LiDAR-based SLAM system is actually
shared by the large amount of visual localization systems, and turns to be
acceptable and even negligible for practical warehouse applications. Once an
accurate AprilTag map is available, visual localization is realized as
recursive estimation that fuses AprilTag measurements (i.e. AprilTag detection
results) and robot motion data. AprilTag measurements may be nonlinear partial
measurements; this can be handled by the well-known extended Kalman filter
(EKF) in the spirit of local linearization. AprilTag measurements tend to have
temporal correlation as well; however, this cannot be reasonably handled by the
EKF. The split covariance intersection filter (Split CIF) is adopted to handle
temporal correlation among AprilTag measurements. The Split CIF (in the spirit
of local linearization) can also handle AprilTag nonlinear partial
measurements. The Split CIF based visual localization system incorporates a
measurement adaptive mechanism to handle outliers in AprilTag measurements and
adopts a dynamic initialization mechanism to address the kidnapping problem. A
comparative study in real warehouse environments demonstrates the potential and
advantage of the Split CIF based visual localization solution.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17880" title="Abstract">arXiv:2310.17880</a> [<a href="/pdf/2310.17880" title="Download PDF">pdf</a>, <a href="/format/2310.17880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconstructive Latent-Space Neural Radiance Fields for Efficient 3D  Scene Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aumentado-Armstrong%2C+T">Tristan Aumentado-Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Mirzaei%2C+A">Ashkan Mirzaei</a>, 
<a href="/search/cs?searchtype=author&query=Brubaker%2C+M+A">Marcus A. Brubaker</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+J">Jonathan Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Levinshtein%2C+A">Alex Levinshtein</a>, 
<a href="/search/cs?searchtype=author&query=Derpanis%2C+K+G">Konstantinos G. Derpanis</a>, 
<a href="/search/cs?searchtype=author&query=Gilitschenski%2C+I">Igor Gilitschenski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Neural Radiance Fields (NeRFs) have proven to be powerful 3D representations,
capable of high quality novel view synthesis of complex scenes. While NeRFs
have been applied to graphics, vision, and robotics, problems with slow
rendering speed and characteristic visual artifacts prevent adoption in many
use cases. In this work, we investigate combining an autoencoder (AE) with a
NeRF, in which latent features (instead of colours) are rendered and then
convolutionally decoded. The resulting latent-space NeRF can produce novel
views with higher quality than standard colour-space NeRFs, as the AE can
correct certain visual artifacts, while rendering over three times faster. Our
work is orthogonal to other techniques for improving NeRF efficiency. Further,
we can control the tradeoff between efficiency and image quality by shrinking
the AE architecture, achieving over 13 times faster rendering with only a small
drop in performance. We hope that our approach can form the basis of an
efficient, yet high-fidelity, 3D scene representation for downstream tasks,
especially when retaining differentiability is useful, as in many robotics
scenarios requiring continual learning.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17882" title="Abstract">arXiv:2310.17882</a> [<a href="/pdf/2310.17882" title="Download PDF">pdf</a>, <a href="/format/2310.17882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Infused Distributed Optimization for Coordinating  Virtual Power Plant Assets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meiyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+J">Javad Mohammadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Amid the increasing interest in the deployment of Distributed Energy
Resources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool
for aggregating diverse DERs and facilitating their participation in wholesale
energy markets. These VPP deployments have been fueled by the Federal Energy
Regulatory Commission's Order 2222, which makes DERs and VPPs competitive
across market segments. However, the diversity and decentralized nature of DERs
present significant challenges to the scalable coordination of VPP assets. To
address efficiency and speed bottlenecks, this paper presents a novel machine
learning-assisted distributed optimization to coordinate VPP assets. Our
method, named LOOP-MAC(Learning to Optimize the Optimization Process for
Multi-agent Coordination), adopts a multi-agent coordination perspective where
each VPP agent manages multiple DERs and utilizes neural network approximators
to expedite the solution search. The LOOP-MAC method employs a gauge map to
guarantee strict compliance with local constraints, effectively reducing the
need for additional post-processing steps. Our results highlight the advantages
of LOOP-MAC, showcasing accelerated solution times per iteration and
significantly reduced convergence times. The LOOP-MAC method outperforms
conventional centralized and distributed optimization methods in optimization
tasks that require repetitive and sequential execution.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17884" title="Abstract">arXiv:2310.17884</a> [<a href="/pdf/2310.17884" title="Download PDF">pdf</a>, <a href="/format/2310.17884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Keep a Secret? Testing Privacy Implications of Language Models  via Contextual Integrity Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>, 
<a href="/search/cs?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> confaide.github.io
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The interactive use of large language models (LLMs) in AI assistants (at
work, home, etc.) introduces a new set of inference-time privacy risks: LLMs
are fed different types of information from multiple sources in their inputs
and are expected to reason about what to share in their outputs, for what
purpose and with whom, within a given context. In this work, we draw attention
to the highly critical yet overlooked notion of contextual privacy by proposing
ConfAIde, a benchmark designed to identify critical weaknesses in the privacy
reasoning capabilities of instruction-tuned LLMs. Our experiments show that
even the most capable models such as GPT-4 and ChatGPT reveal private
information in contexts that humans would not, 39% and 57% of the time,
respectively. This leakage persists even when we employ privacy-inducing
prompts or chain-of-thought reasoning. Our work underscores the immediate need
to explore novel inference-time privacy-preserving approaches, based on
reasoning and theory of mind.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17886" title="Abstract">arXiv:2310.17886</a> [<a href="/pdf/2310.17886" title="Download PDF">pdf</a>, <a href="/format/2310.17886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Linear-Size Additive Emulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoppenworth%2C+G">Gary Hoppenworth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SOSA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Given an input graph $G = (V, E)$, an additive emulator $H = (V, E', w)$ is a
sparse weighted graph that preserves all distances in $G$ with small additive
error. A recent line of inquiry has sought to determine the best additive error
achievable in the sparsest setting, when $H$ has a linear number of edges. In
particular, the work of [Kogan and Parter, ICALP 2023], following [Pettie,
ICALP 2007], constructed linear size emulators with $+O(n^{0.222})$ additive
error. It is known that the worst-case additive error must be at least
$+\Omega(n^{2/29})$ due to [Lu, Vassilevska Williams, Wein, and Xu, SODA 2022].
<br />We present a simple linear-size emulator construction that achieves additive
error $+O(n^{0.191})$. Our approach extends the path-buying framework developed
by [Baswana, Kavitha, Mehlhorn, and Pettie, SODA 2005] and [Vassilevska
Williams and Bodwin, SODA 2016] to the setting of sparse additive emulators.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17887" title="Abstract">arXiv:2310.17887</a> [<a href="/pdf/2310.17887" title="Download PDF">pdf</a>, <a href="/format/2310.17887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impressions: Understanding Visual Semiotics and Aesthetic Impact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kruk%2C+J">Julia Kruk</a>, 
<a href="/search/cs?searchtype=author&query=Ziems%2C+C">Caleb Ziems</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Is aesthetic impact different from beauty? Is visual salience a reflection of
its capacity for effective communication? We present Impressions, a novel
dataset through which to investigate the semiotics of images, and how specific
visual features and design choices can elicit specific emotions, thoughts and
beliefs. We posit that the impactfulness of an image extends beyond formal
definitions of aesthetics, to its success as a communicative act, where style
contributes as much to meaning formation as the subject matter. However, prior
image captioning datasets are not designed to empower state-of-the-art
architectures to model potential human impressions or interpretations of
images. To fill this gap, we design an annotation task heavily inspired by
image analysis techniques in the Visual Arts to collect 1,440 image-caption
pairs and 4,320 unique annotations exploring impact, pragmatic image
description, impressions, and aesthetic design choices. We show that existing
multimodal image captioning and conditional generation models struggle to
simulate plausible human responses to images. However, this dataset
significantly improves their ability to model impressions and aesthetic
evaluations of images through fine-tuning and few-shot adaptation.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17888" title="Abstract">arXiv:2310.17888</a> [<a href="/pdf/2310.17888" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Subpopulation Representative Models: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simmons%2C+G">Gabriel Simmons</a>, 
<a href="/search/cs?searchtype=author&query=Hare%2C+C">Christopher Hare</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Of the many commercial and scientific opportunities provided by large
language models (LLMs; including Open AI's ChatGPT, Meta's LLaMA, and
Anthropic's Claude), one of the more intriguing applications has been the
simulation of human behavior and opinion. LLMs have been used to generate human
simulcra to serve as experimental participants, survey respondents, or other
independent agents, with outcomes that often closely parallel the observed
behavior of their genuine human counterparts. Here, we specifically consider
the feasibility of using LLMs to estimate subpopulation representative models
(SRMs). SRMs could provide an alternate or complementary way to measure public
opinion among demographic, geographic, or political segments of the population.
However, the introduction of new technology to the socio-technical
infrastructure does not come without risk. We provide an overview of behavior
elicitation techniques for LLMs, and a survey of existing SRM implementations.
We offer frameworks for the analysis, development, and practical implementation
of LLMs as SRMs, consider potential risks, and suggest directions for future
work.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17890" title="Abstract">arXiv:2310.17890</a> [<a href="/pdf/2310.17890" title="Download PDF">pdf</a>, <a href="/format/2310.17890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Submodel Partitioning in Hierarchical Federated Learning: Algorithm  Design and Convergence Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+W">Wenzhi Fang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
<p class="mathjax">Hierarchical federated learning (HFL) has demonstrated promising scalability
advantages over the traditional "star-topology" architecture-based federated
learning (FL). However, HFL still imposes significant computation,
communication, and storage burdens on the edge, especially when training a
large-scale model over resource-constrained Internet of Things (IoT) devices.
In this paper, we propose hierarchical independent submodel training (HIST), a
new FL methodology that aims to address these issues in hierarchical settings.
The key idea behind HIST is a hierarchical version of model partitioning, where
we partition the global model into disjoint submodels in each round, and
distribute them across different cells, so that each cell is responsible for
training only one partition of the full model. This enables each client to save
computation/storage costs while alleviating the communication loads throughout
the hierarchy. We characterize the convergence behavior of HIST for non-convex
loss functions under mild assumptions, showing the impact of several attributes
(e.g., number of cells, local and global aggregation frequency) on the
performance-efficiency tradeoff. Finally, through numerical experiments, we
verify that HIST is able to save communication costs by a wide margin while
achieving the same target testing accuracy.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17894" title="Abstract">arXiv:2310.17894</a> [<a href="/pdf/2310.17894" title="Download PDF">pdf</a>, <a href="/format/2310.17894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Interfaces for Tabular Data Querying and Visualization:  A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weixu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yuanfeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+V+J">Victor Junqiu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuxing Tian</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yiyan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J+H">Jonathan H. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+R+C">Raymond Chi-Wing Wong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haiqin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures, 5 tables. Submitted to IEEE TKDE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The emergence of natural language processing has revolutionized the way users
interact with tabular data, enabling a shift from traditional query languages
and manual plotting to more intuitive, language-based interfaces. The rise of
large language models (LLMs) such as ChatGPT and its successors has further
advanced this field, opening new avenues for natural language processing
techniques. This survey presents a comprehensive overview of natural language
interfaces for tabular data querying and visualization, which allow users to
interact with data using natural language queries. We introduce the fundamental
concepts and techniques underlying these interfaces with a particular emphasis
on semantic parsing, the key technology facilitating the translation from
natural language to SQL queries or data visualization commands. We then delve
into the recent advancements in Text-to-SQL and Text-to-Vis problems from the
perspectives of datasets, methodologies, metrics, and system designs. This
includes a deep dive into the influence of LLMs, highlighting their strengths,
limitations, and potential for future improvements. Through this survey, we aim
to provide a roadmap for researchers and practitioners interested in developing
and applying natural language interfaces for data interaction in the era of
large language models.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17898" title="Abstract">arXiv:2310.17898</a> [<a href="/pdf/2310.17898" title="Download PDF">pdf</a>, <a href="/format/2310.17898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate-At-Most-k Encoding of SAT for Soft Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nishimura%2C+S">Shunji Nishimura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 14th Pragmatics of SAT international workshop (PoS2023): accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In the field of Boolean satisfiability problems (SAT), at-most-k constraints,
which suppress the number of true target variables at most k, are often used to
describe objective problems. At-most-k constraints are used not only for
absolutely necessary constraints (hard constraints) but also for challenging
constraints (soft constraints) to search for better solutions. To encode
at-most-k constraints into Boolean expressions, there is a problem that the
number of Boolean expressions basically increases exponentially with the number
of target variables, so at-most-k often has difficulties for a large number of
variables. To solve this problem, this paper proposes a new encoding method of
at-most-k constraints, called approximate-at-most-k, which has totally fewer
Boolean expressions than conventional methods on the one hand. On the other
hand, it has lost completeness, i.e., some Boolean value assignments that
satisfy the original at-most-k are not allowed with approximate-at-most-k;
hence, it is called approximate. Without completeness, we still have potential
benefits by using them only as soft constraints. For example,
approximate-at-most-16 out of 32 variables requires only 15% of a conventional
at-most-k on the literal number and covers 44% of the solution space. Thus,
approximate-at-most-k can become an alternative encoding method for at-most-k,
especially as soft constraints.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17901" title="Abstract">arXiv:2310.17901</a> [<a href="/pdf/2310.17901" title="Download PDF">pdf</a>, <a href="/format/2310.17901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Knowledge Gradient Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+Y">Yang Le</a>, 
<a href="/search/cs?searchtype=author&query=Siyang%2C+G">Gao Siyang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+H+C">Ho Chin Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 42 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The knowledge gradient (KG) algorithm is a popular policy for the best arm
identification (BAI) problem. It is built on the simple idea of always choosing
the measurement that yields the greatest expected one-step improvement in the
estimate of the best mean of the arms. In this research, we show that this
policy has limitations, causing the algorithm not asymptotically optimal. We
next provide a remedy for it, by following the manner of one-step look ahead of
KG, but instead choosing the measurement that yields the greatest one-step
improvement in the probability of selecting the best arm. The new policy is
called improved knowledge gradient (iKG). iKG can be shown to be asymptotically
optimal. In addition, we show that compared to KG, it is easier to extend iKG
to variant problems of BAI, with the $\epsilon$-good arm identification and
feasible arm identification as two examples. The superior performances of iKG
on these problems are further demonstrated using numerical examples.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17903" title="Abstract">arXiv:2310.17903</a> [<a href="/pdf/2310.17903" title="Download PDF">pdf</a>, <a href="/format/2310.17903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=She%2C+X">Xinyu She</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanjie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yiling He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modern language models (LMs) have been successfully employed in source code
generation and understanding, leading to a significant increase in research
focused on learning-based code intelligence, such as automated bug repair, and
test case generation. Despite their great potential, language models for code
intelligence (LM4Code) are susceptible to potential pitfalls, which hinder
realistic performance and further impact their reliability and applicability in
real-world deployment. Such challenges drive the need for a comprehensive
understanding - not just identifying these issues but delving into their
possible implications and existing solutions to build more reliable language
models tailored to code intelligence. Based on a well-defined systematic
research approach, we conducted an extensive literature review to uncover the
pitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues
have been identified. After carefully examining these studies, we designed a
taxonomy of pitfalls in LM4Code research and conducted a systematic study to
summarize the issues, implications, current solutions, and challenges of
different pitfalls for LM4Code systems. We developed a comprehensive
classification scheme that dissects pitfalls across four crucial aspects: data
collection and labeling, system design and learning, performance evaluation,
and deployment and maintenance. Through this study, we aim to provide a roadmap
for researchers and practitioners, facilitating their understanding and
utilization of LM4Code in reliable and trustworthy ways.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17909" title="Abstract">arXiv:2310.17909</a> [<a href="/pdf/2310.17909" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Innovation-to-Occupations Ontology: Linking Business Transformation  Initiatives to Occupations and Skills
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elia%2C+D">Daniela Elia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zowghi%2C+D">Didar Zowghi</a>, 
<a href="/search/cs?searchtype=author&query=Rizoiu%2C+M">Marian-Andrei Rizoiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, Camera-ready version in ACIS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The fast adoption of new technologies forces companies to continuously adapt
their operations making it harder to predict workforce requirements. Several
recent studies have attempted to predict the emergence of new roles and skills
in the labour market from online job ads. This paper aims to present a novel
ontology linking business transformation initiatives to occupations and an
approach to automatically populating it by leveraging embeddings extracted from
job ads and Wikipedia pages on business transformation and emerging
technologies topics. To our knowledge, no previous research explicitly links
business transformation initiatives, like the adoption of new technologies or
the entry into new markets, to the roles needed. Our approach successfully
matches occupations to transformation initiatives under ten different
scenarios, five linked to technology adoption and five related to business.
This framework presents an innovative approach to guide enterprises and
educational institutions on the workforce requirements for specific business
transformation initiatives.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17910" title="Abstract">arXiv:2310.17910</a> [<a href="/pdf/2310.17910" title="Download PDF">pdf</a>, <a href="/format/2310.17910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocStormer: Revitalizing Multi-Degraded Colored Document Images to  Pristine PDF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chaowei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jichun Li</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Y">Yihua Teng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoqun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+D">Dandan Tu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">For capturing colored document images, e.g. posters and magazines, it is
common that multiple degradations such as shadows, wrinkles, etc., are
simultaneously introduced due to external factors. Restoring multi-degraded
colored document images is a great challenge, yet overlooked, as most existing
algorithms focus on enhancing color-ignored document images via binarization.
Thus, we propose DocStormer, a novel algorithm designed to restore
multi-degraded colored documents to their potential pristine PDF. The
contributions are: firstly, we propose a "Perceive-then-Restore" paradigm with
a reinforced transformer block, which more effectively encodes and utilizes the
distribution of degradations. Secondly, we are the first to utilize GAN and
pristine PDF magazine images to narrow the distribution gap between the
enhanced results and PDF images, in pursuit of less degradation and better
visual quality. Thirdly, we propose a non-parametric strategy, PFILI, which
enables a smaller training scale and larger testing resolutions with acceptable
detail trade-off, while saving memory and inference time. Fourthly, we are the
first to propose a novel Multi-Degraded Colored Document image Enhancing
dataset, named MD-CDE, for both training and evaluation. Experimental results
show that the DocStormer exhibits superior performance, capable of revitalizing
multi-degraded colored documents into their potential pristine digital
versions, which fills the current academic gap from the perspective of method,
data, and task.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17912" title="Abstract">arXiv:2310.17912</a> [<a href="/pdf/2310.17912" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Restoring the Broken Covenant Between Compilers and Deep Learning  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kinzer%2C+S">Sean Kinzer</a>, 
<a href="/search/cs?searchtype=author&query=Ghodrati%2C+S">Soroush Ghodrati</a>, 
<a href="/search/cs?searchtype=author&query=Mahapatra%2C+R">Rohan Mahapatra</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+B+H">Byung Hoon Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Mascarenhas%2C+E">Edwin Mascarenhas</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaolong Li</a>, 
<a href="/search/cs?searchtype=author&query=Matai%2C+J">Janarbek Matai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Esmaeilzadeh%2C+H">Hadi Esmaeilzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Deep learning accelerators address the computational demands of Deep Neural
Networks (DNNs), departing from the traditional Von Neumann execution model.
They leverage specialized hardware to align with the application domain's
structure. Compilers for these accelerators face distinct challenges compared
to those for general-purpose processors. These challenges include exposing and
managing more micro-architectural features, handling software-managed scratch
pads for on-chip storage, explicitly managing data movement, and matching DNN
layers with varying hardware capabilities. These complexities necessitate a new
approach to compiler design, as traditional compilers mainly focused on
generating fine-grained instruction sequences while abstracting
micro-architecture details. This paper introduces the Architecture Covenant
Graph (ACG), an abstract representation of an architectural structure's
components and their programmable capabilities. By enabling the compiler to
work with the ACG, it allows for adaptable compilation workflows when making
changes to accelerator design, reducing the need for a complete compiler
redevelopment. Codelets, which express DNN operation functionality and evolve
into execution mappings on the ACG, are key to this process. The Covenant
compiler efficiently targets diverse deep learning accelerators, achieving
93.8% performance compared to state-of-the-art, hand-tuned DNN layer
implementations when compiling 14 DNN layers from various models on two
different architectures.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17914" title="Abstract">arXiv:2310.17914</a> [<a href="/pdf/2310.17914" title="Download PDF">pdf</a>, <a href="/format/2310.17914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D-Aware Visual Question Answering about Parts, Poses and Occlusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingrui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wufei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuowan Li</a>, 
<a href="/search/cs?searchtype=author&query=Kortylewski%2C+A">Adam Kortylewski</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite rapid progress in Visual question answering (VQA), existing datasets
and models mainly focus on testing reasoning in 2D. However, it is important
that VQA models also understand the 3D structure of visual scenes, for example
to support tasks like navigation or manipulation. This includes an
understanding of the 3D object pose, their parts and occlusions. In this work,
we introduce the task of 3D-aware VQA, which focuses on challenging questions
that require a compositional reasoning over the 3D structure of visual scenes.
We address 3D-aware VQA from both the dataset and the model perspective. First,
we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains
questions about object parts, their 3D poses, and occlusions. Second, we
propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas:
probabilistic neural symbolic program execution for reasoning and deep neural
networks with 3D generative representations of objects for robust visual
recognition. Our experimental results show our model PO3D-VQA outperforms
existing methods significantly, but we still observe a significant performance
gap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an
important open research area.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17915" title="Abstract">arXiv:2310.17915</a> [<a href="/pdf/2310.17915" title="Download PDF">pdf</a>, <a href="/format/2310.17915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifting the Veil: Unlocking the Power of Depth in Q-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shao-Bo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shaojie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Ding-Xuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">With the help of massive data and rich computational resources, deep
Q-learning has been widely used in operations research and management science
and has contributed to great success in numerous applications, including
recommender systems, supply chains, games, and robotic manipulation. However,
the success of deep Q-learning lacks solid theoretical verification and
interpretability. The aim of this paper is to theoretically verify the power of
depth in deep Q-learning. Within the framework of statistical learning theory,
we rigorously prove that deep Q-learning outperforms its traditional version by
demonstrating its good generalization error bound. Our results reveal that the
main reason for the success of deep Q-learning is the excellent performance of
deep neural networks (deep nets) in capturing the special properties of rewards
namely, spatial sparseness and piecewise constancy, rather than their large
capacities. In this paper, we make fundamental contributions to the field of
reinforcement learning by answering to the following three questions: Why does
deep Q-learning perform so well? When does deep Q-learning perform better than
traditional Q-learning? How many samples are required to achieve a specific
prediction accuracy for deep Q-learning? Our theoretical assertions are
verified by applying deep Q-learning in the well-known beer game in supply
chain management and a simulated recommender system.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17918" title="Abstract">arXiv:2310.17918</a> [<a href="/pdf/2310.17918" title="Download PDF">pdf</a>, <a href="/format/2310.17918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection  Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yukun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lingyong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+G">Guoliang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhicong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have shown great potential in Natural Language
Processing (NLP) tasks. However, recent literature reveals that LLMs generate
nonfactual responses intermittently, which impedes the LLMs' reliability for
further utilization. In this paper, we propose a novel self-detection method to
detect which questions that a LLM does not know that are prone to generate
nonfactual results. Specifically, we first diversify the textual expressions
for a given question and collect the corresponding answers. Then we examine the
divergencies between the generated answers to identify the questions that the
model may generate falsehoods. All of the above steps can be accomplished by
prompting the LLMs themselves without referring to any other external
resources. We conduct comprehensive experiments and demonstrate the
effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,
and GPT-4.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17922" title="Abstract">arXiv:2310.17922</a> [<a href="/pdf/2310.17922" title="Download PDF">pdf</a>, <a href="/format/2310.17922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Choice Hierarchical Policy Learning for Conversational  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weijia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Release with source code
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Conversational Recommender Systems (CRS) illuminate user preferences via
multi-round interactive dialogues, ultimately navigating towards precise and
satisfactory recommendations. However, contemporary CRS are limited to
inquiring binary or multi-choice questions based on a single attribute type
(e.g., color) per round, which causes excessive rounds of interaction and
diminishes the user's experience. To address this, we propose a more realistic
and efficient conversational recommendation problem setting, called
Multi-Type-Attribute Multi-round Conversational Recommendation (MTAMCR), which
enables CRS to inquire about multi-choice questions covering multiple types of
attributes in each round, thereby improving interactive efficiency. Moreover,
by formulating MTAMCR as a hierarchical reinforcement learning task, we propose
a Chain-of-Choice Hierarchical Policy Learning (CoCHPL) framework to enhance
both the questioning efficiency and recommendation effectiveness in MTAMCR.
Specifically, a long-term policy over options (i.e., ask or recommend)
determines the action type, while two short-term intra-option policies
sequentially generate the chain of attributes or items through multi-step
reasoning and selection, optimizing the diversity and interdependence of
questioning attributes. Finally, extensive experiments on four benchmarks
demonstrate the superior performance of CoCHPL over prevailing state-of-the-art
methods.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17923" title="Abstract">arXiv:2310.17923</a> [<a href="/pdf/2310.17923" title="Download PDF">pdf</a>, <a href="/format/2310.17923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Grasping of Unknown Objects with a Multi-Fingered Hand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burkhardt%2C+Y">Yannick Burkhardt</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Q">Qian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Karan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaopeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">An important prerequisite for autonomous robots is their ability to reliably
grasp a wide variety of objects. Most state-of-the-art systems employ
specialized or simple end-effectors, such as two-jaw grippers, which severely
limit the range of objects to manipulate. Additionally, they conventionally
require a structured and fully predictable environment while the vast majority
of our world is complex, unstructured, and dynamic. This paper presents an
implementation to overcome both issues. Firstly, the integration of a
five-finger hand enhances the variety of possible grasps and manipulable
objects. This kinematically complex end-effector is controlled by a deep
learning based generative grasping network. The required virtual model of the
unknown target object is iteratively completed by processing visual sensor
data. Secondly, this visual feedback is employed to realize closed-loop servo
control which compensates for external disturbances. Our experiments on real
hardware confirm the system's capability to reliably grasp unknown dynamic
target objects without a priori knowledge of their trajectories. To the best of
our knowledge, this is the first method to achieve dynamic multi-fingered
grasping for unknown objects. A video of the experiments is available at
https://youtu.be/Ut28yM1gnvI.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17924" title="Abstract">arXiv:2310.17924</a> [<a href="/pdf/2310.17924" title="Download PDF">pdf</a>, <a href="/format/2310.17924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SOUL: Towards Sentiment and Opinion Understanding of Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yue Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S+J">Sinno Jialin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference, Short Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Sentiment analysis is a well-established natural language processing task,
with sentiment polarity classification being one of its most popular and
representative tasks. However, despite the success of pre-trained language
models in this area, they often fall short of capturing the broader
complexities of sentiment analysis. To address this issue, we propose a new
task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims
to evaluate sentiment understanding through two subtasks: Review Comprehension
(RC) and Justification Generation (JG). RC seeks to validate statements that
focus on subjective information based on a review text, while JG requires
models to provide explanations for their sentiment predictions. To enable
comprehensive evaluation, we annotate a new dataset comprising 15,028
statements from 3,638 reviews. Experimental results indicate that SOUL is a
challenging task for both small and large language models, with a performance
gap of up to 27% when compared to human performance. Furthermore, evaluations
conducted with both human experts and GPT-4 highlight the limitations of the
small language model in generating reasoning-based justifications. These
findings underscore the challenging nature of the SOUL task for existing
models, emphasizing the need for further advancements in sentiment analysis to
address its complexities. The new dataset and code are available at
https://github.com/DAMO-NLP-SG/SOUL.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17931" title="Abstract">arXiv:2310.17931</a> [<a href="/pdf/2310.17931" title="Download PDF">pdf</a>, <a href="/format/2310.17931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coded Caching Scheme for Partially Connected Linear Networks Via  Multi-antenna Placement Delivery Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minquan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Youlong Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this paper, we study the coded caching scheme for the
$(K,L,M_{\text{T}},M_{\text{U}},N)$ partially connected linear network, where
there are $N$ files each of which has an equal size, $K+L-1$ transmitters and
$K$ users; each user and transmitter caches at most $M_{\text{U}}$ and
$M_{\text{T}}$ files respectively; each user cyclically communicates with $L$
transmitters. The goal is to design caching and delivery schemes to reduce the
transmission latency measured by the metric normalized delivery time (NDT). By
delicately designing the data placement of the transmitters and users according
to the topology, we show that a combinatorial structure called multiple-antenna
placement delivery array (MAPDA), which was originally proposed for the
multiple-input single-output broadcast channels, can be also used to design
schemes for the partially connected linear network. Then, based on existing
MAPDAs and our constructing approach, we propose new schemes that achieve the
optimal NDT when $ {M_\text{T}}+ {M_\text{U}}\geq N$ and smaller NDT than that
of the existing schemes when (${M_\text{T}}+ {M_\text{U}}\leq N$,
$\frac{M_\text{U}}{N}+\frac{M_\text{T}}{N} \frac{L}{K}\left\lceil \frac{K}{L}
\right\rceil \geq 1$) or ($ {M_\text{U}}+ {M_\text{T}}&lt; N,
\frac{K}{L}\notin\mathbb{Z}^+$). Moreover, our schemes operate in one-shot
linear delivery and significantly reduce the subpacketizations compared to the
existing scheme, which implies that our schemes have a wider range of
applications and lower complexity of implementation.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17936" title="Abstract">arXiv:2310.17936</a> [<a href="/pdf/2310.17936" title="Download PDF">pdf</a>, <a href="/format/2310.17936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers as Graph-to-Graph Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henderson%2C+J">James Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadshahi%2C+A">Alireza Mohammadshahi</a>, 
<a href="/search/cs?searchtype=author&query=Coman%2C+A+C">Andrei C. Coman</a>, 
<a href="/search/cs?searchtype=author&query=Miculicich%2C+L">Lesly Miculicich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Big Picture workshop at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We argue that Transformers are essentially graph-to-graph models, with
sequences just being a special case. Attention weights are functionally
equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes
this ability explicit, by inputting graph edges into the attention weight
computations and predicting graph edges with attention-like functions, thereby
integrating explicit graphs into the latent graphs learned by pretrained
Transformers. Adding iterative graph refinement provides a joint embedding of
input, output, and latent graphs, allowing non-autoregressive graph prediction
to optimise the complete graph without any bespoke pipeline or decoding
strategy. Empirical results show that this architecture achieves
state-of-the-art accuracies for modelling a variety of linguistic structures,
integrating very effectively with the latent linguistic representations learned
by pretraining.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17940" title="Abstract">arXiv:2310.17940</a> [<a href="/pdf/2310.17940" title="Download PDF">pdf</a>, <a href="/format/2310.17940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Segment-to-Segment Framework for Simultaneous Sequence  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaolei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Simultaneous sequence generation is a pivotal task for real-time scenarios,
such as streaming speech recognition, simultaneous machine translation and
simultaneous speech translation, where the target sequence is generated while
receiving the source sequence. The crux of achieving high-quality generation
with low latency lies in identifying the optimal moments for generating,
accomplished by learning a mapping between the source and target sequences.
However, existing methods often rely on task-specific heuristics for different
sequence types, limiting the model's capacity to adaptively learn the
source-target mapping and hindering the exploration of multi-task learning for
various simultaneous tasks. In this paper, we propose a unified
segment-to-segment framework (Seg2Seg) for simultaneous sequence generation,
which learns the mapping in an adaptive and unified manner. During the process
of simultaneous generation, the model alternates between waiting for a source
segment and generating a target segment, making the segment serve as the
natural bridge between the source and target. To accomplish this, Seg2Seg
introduces a latent segment as the pivot between source to target and explores
all potential source-target mappings via the proposed expectation training,
thereby learning the optimal moments for generating. Experiments on multiple
simultaneous generation tasks demonstrate that Seg2Seg achieves
state-of-the-art performance and exhibits better generality across various
tasks.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17942" title="Abstract">arXiv:2310.17942</a> [<a href="/pdf/2310.17942" title="Download PDF">pdf</a>, <a href="/format/2310.17942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversifying Spatial-Temporal Perception for Video Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kun-Yu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jia-Run Du</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yipeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiaming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wei-Shi Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023. Code is available at <a href="https://github.com/KunyuLin/STDN/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video domain generalization aims to learn generalizable video classification
models for unseen target domains by training in a source domain. A critical
challenge of video domain generalization is to defend against the heavy
reliance on domain-specific cues extracted from the source domain when
recognizing target videos. To this end, we propose to perceive diverse
spatial-temporal cues in videos, aiming to discover potential domain-invariant
cues in addition to domain-specific cues. We contribute a novel model named
Spatial-Temporal Diversification Network (STDN), which improves the diversity
from both space and time dimensions of video data. First, our STDN proposes to
discover various types of spatial cues within individual frames by spatial
grouping. Then, our STDN proposes to explicitly model spatial-temporal
dependencies between video contents at multiple space-time scales by
spatial-temporal relation modeling. Extensive experiments on three benchmarks
of different types demonstrate the effectiveness and versatility of our
approach.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17943" title="Abstract">arXiv:2310.17943</a> [<a href="/pdf/2310.17943" title="Download PDF">pdf</a>, <a href="/format/2310.17943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Complexity and Information-Theoretic Optimal Memory AMP for Coded  Generalized MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yufei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuhao Chi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, accepted at GLOBECOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper considers a generalized multiple-input multiple-output (GMIMO)
with practical assumptions, such as massive antennas, practical channel coding,
arbitrary input distributions, and general right-unitarily-invariant channel
matrices (covering Rayleigh fading, certain ill-conditioned and correlated
channel matrices). Orthogonal/vector approximate message passing (OAMP/VAMP)
has been proved to be information-theoretically optimal in GMIMO, but it is
limited to high complexity. Meanwhile, low-complexity memory approximate
message passing (MAMP) was shown to be Bayes optimal in GMIMO, but channel
coding was ignored. Therefore, how to design a low-complexity and
information-theoretic optimal receiver for GMIMO is still an open issue. In
this paper, we propose an information-theoretic optimal MAMP receiver for coded
GMIMO, whose achievable rate analysis and optimal coding principle are provided
to demonstrate its information-theoretic optimality. Specifically, state
evolution (SE) for MAMP is intricately multi-dimensional because of the nature
of local memory detection. To this end, a fixed-point consistency lemma is
proposed to derive the simplified variational SE (VSE) for MAMP, based on which
the achievable rate of MAMP is calculated, and the optimal coding principle is
derived to maximize the achievable rate. Subsequently, we prove the
information-theoretic optimality of MAMP. Numerical results show that the
finite-length performances of MAMP with optimized LDPC codes are about 1.0 -
2.7 dB away from the associated constrained capacities. It is worth noting that
MAMP can achieve the same performance as OAMP/VAMP with 0.4% of the time
consumption for large-scale systems.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17944" title="Abstract">arXiv:2310.17944</a> [<a href="/pdf/2310.17944" title="Download PDF">pdf</a>, <a href="/format/2310.17944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Edge Machine Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaojie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beibei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Z">Zhaolong Ning</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Song Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F+R">Fei Richard Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The convergence of Edge Computing (EC) and Machine Learning (ML), known as
Edge Machine Learning (EML), has become a highly regarded research area by
utilizing distributed network resources to perform joint training and inference
in a cooperative manner. However, EML faces various challenges due to resource
constraints, heterogeneous network environments, and diverse service
requirements of different applications, which together affect the
trustworthiness of EML in the eyes of its stakeholders. This survey provides a
comprehensive summary of definitions, attributes, frameworks, techniques, and
solutions for trustworthy EML. Specifically, we first emphasize the importance
of trustworthy EML within the context of Sixth-Generation (6G) networks. We
then discuss the necessity of trustworthiness from the perspective of
challenges encountered during deployment and real-world application scenarios.
Subsequently, we provide a preliminary definition of trustworthy EML and
explore its key attributes. Following this, we introduce fundamental frameworks
and enabling technologies for trustworthy EML systems, and provide an in-depth
literature review of the latest solutions to enhance trustworthiness of EML.
Finally, we discuss corresponding research challenges and open issues.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17945" title="Abstract">arXiv:2310.17945</a> [<a href="/pdf/2310.17945" title="Download PDF">pdf</a>, <a href="/format/2310.17945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive and Reliable Feature Attribution Method: Double-sided  Remove and Reconstruct (DoRaR)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+D">Dong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Amariucai%2C+G">George Amariucai</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+D">Daji Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yong Guan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shen Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The limited transparency of the inner decision-making mechanism in deep
neural networks (DNN) and other machine learning (ML) models has hindered their
application in several domains. In order to tackle this issue, feature
attribution methods have been developed to identify the crucial features that
heavily influence decisions made by these black box models. However, many
feature attribution methods have inherent downsides. For example, one category
of feature attribution methods suffers from the artifacts problem, which feeds
out-of-distribution masked inputs directly through the classifier that was
originally trained on natural data points. Another category of feature
attribution method finds explanations by using jointly trained feature
selectors and predictors. While avoiding the artifacts problem, this new
category suffers from the Encoding Prediction in the Explanation (EPITE)
problem, in which the predictor's decisions rely not on the features, but on
the masks that selects those features. As a result, the credibility of
attribution results is undermined by these downsides. In this research, we
introduce the Double-sided Remove and Reconstruct (DoRaR) feature attribution
method based on several improvement methods that addresses these issues. By
conducting thorough testing on MNIST, CIFAR10 and our own synthetic dataset, we
demonstrate that the DoRaR feature attribution method can effectively bypass
the above issues and can aid in training a feature selector that outperforms
other state-of-the-art feature attribution methods. Our code is available at
https://github.com/dxq21/DoRaR.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17949" title="Abstract">arXiv:2310.17949</a> [<a href="/pdf/2310.17949" title="Download PDF">pdf</a>, <a href="/format/2310.17949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instance Segmentation under Occlusions via Location-aware Copy-Paste  Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+S">Son Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Lainsa%2C+M">Mikel Lainsa</a>, 
<a href="/search/cs?searchtype=author&query=Dao%2C+H">Hung Dao</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Daeyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+G">Giang Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Occlusion is a long-standing problem in computer vision, particularly in
instance segmentation. ACM MMSports 2023 DeepSportRadar has introduced a
dataset that focuses on segmenting human subjects within a basketball context
and a specialized evaluation metric for occlusion scenarios. Given the modest
size of the dataset and the highly deformable nature of the objects to be
segmented, this challenge demands the application of robust data augmentation
techniques and wisely-chosen deep learning architectures. Our work (ranked 1st
in the competition) first proposes a novel data augmentation technique, capable
of generating more training samples with wider distribution. Then, we adopt a
new architecture - Hybrid Task Cascade (HTC) framework with CBNetV2 as backbone
and MaskIoU head to improve segmentation performance. Furthermore, we employ a
Stochastic Weight Averaging (SWA) training strategy to improve the model's
generalization. As a result, we achieve a remarkable occlusion score (OM) of
0.533 on the challenge dataset, securing the top-1 position on the leaderboard.
Source code is available at this
https://github.com/nguyendinhson-kaist/MMSports23-Seg-AutoID.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17951" title="Abstract">arXiv:2310.17951</a> [<a href="/pdf/2310.17951" title="Download PDF">pdf</a>, <a href="/format/2310.17951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Parameter Saliency via Extreme Value Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+I">Issei Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep neural networks are being increasingly implemented throughout society in
recent years. It is useful to identify which parameters trigger
misclassification in diagnosing undesirable model behaviors. The concept of
parameter saliency is proposed and used to diagnose convolutional neural
networks (CNNs) by ranking convolution filters that may have caused
misclassification on the basis of parameter saliency. It is also shown that
fine-tuning the top ranking salient filters has efficiently corrected
misidentification on ImageNet. However, there is still a knowledge gap in terms
of understanding why parameter saliency ranking can find the filters inducing
misidentification. In this work, we attempt to bridge the gap by analyzing
parameter saliency ranking from a statistical viewpoint, namely, extreme value
theory. We first show that the existing work implicitly assumes that the
gradient norm computed for each filter follows a normal distribution. Then, we
clarify the relationship between parameter saliency and the score based on the
peaks-over-threshold (POT) method, which is often used to model extreme values.
Finally, we reformulate parameter saliency in terms of the POT method, where
this reformulation is regarded as statistical anomaly detection and does not
require the implicit assumptions of the existing parameter-saliency
formulation. Our experimental results demonstrate that our reformulation can
detect malicious filters as well. Furthermore, we show that the existing
parameter saliency method exhibits a bias against the depth of layers in deep
neural networks. In particular, this bias has the potential to inhibit the
discovery of filters that cause misidentification in situations where domain
shift occurs. In contrast, parameter saliency based on POT shows less of this
bias.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17952" title="Abstract">arXiv:2310.17952</a> [<a href="/pdf/2310.17952" title="Download PDF">pdf</a>, <a href="/format/2310.17952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shape-centered Representation Learning for Visible-Infrared Person  Re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+J">Jiaxu Leng</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+J">Ji Gan</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+M">Mengjingcheng Mo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current Visible-Infrared Person Re-Identification (VI-ReID) methods
prioritize extracting distinguishing appearance features, ignoring the natural
resistance of body shape against modality changes. Initially, we gauged the
discriminative potential of shapes by a straightforward concatenation of shape
and appearance features. However, two unresolved issues persist in the
utilization of shape features. One pertains to the dependence on auxiliary
models for shape feature extraction in the inference phase, along with the
errors in generated infrared shapes due to the intrinsic modality disparity.
The other issue involves the inadequately explored correlation between shape
and appearance features. To tackle the aforementioned challenges, we propose
the Shape-centered Representation Learning framework (ScRL), which focuses on
learning shape features and appearance features associated with shapes.
Specifically, we devise the Shape Feature Propagation (SFP), facilitating
direct extraction of shape features from original images with minimal
complexity costs during inference. To restitute inaccuracies in infrared body
shapes at the feature level, we present the Infrared Shape Restitution (ISR).
Furthermore, to acquire appearance features related to shape, we design the
Appearance Feature Enhancement (AFE), which accentuates identity-related
features while suppressing identity-unrelated features guided by shape
features. Extensive experiments are conducted to validate the effectiveness of
the proposed ScRL. Achieving remarkable results, the Rank-1 (mAP) accuracy
attains 76.1%, 71.2%, 92.4% (72.6%, 52.9%, 86.7%) on the SYSU-MM01, HITSZ-VCM,
RegDB datasets respectively, outperforming existing state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17953" title="Abstract">arXiv:2310.17953</a> [<a href="/pdf/2310.17953" title="Download PDF">pdf</a>, <a href="/format/2310.17953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Peng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">XingYuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">ZiWei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kani Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recently Whisper has approached human-level robustness and accuracy in
English automatic speech recognition (ASR), while in minor language and mixed
language speech recognition, there remains a compelling need for further
improvement. In this work, we present the impressive results of Whisper-MCE,
our finetuned Whisper model, which was trained using our self-collected
dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile,
considering word error rate (WER) poses challenges when it comes to evaluating
its effectiveness in minor language and mixed-language contexts, we present a
novel rating mechanism. By comparing our model to the baseline whisper-large-v2
model, we demonstrate its superior ability to accurately capture the content of
the original audio, achieve higher recognition accuracy, and exhibit faster
recognition speed. Notably, our model outperforms other existing models in the
specific task of recognizing mixed language.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17956" title="Abstract">arXiv:2310.17956</a> [<a href="/pdf/2310.17956" title="Download PDF">pdf</a>, <a href="/format/2310.17956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General  Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qichen Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+D">Dading Chong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peilin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yining Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have introduced a new era of proficiency in
comprehending complex healthcare and biomedical topics. However, there is a
noticeable lack of models in languages other than English and models that can
interpret multi-modal input, which is crucial for global healthcare
accessibility. In response, this study introduces Qilin-Med-VL, the first
Chinese large vision-language model designed to integrate the analysis of
textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer
(ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum
training process that includes feature alignment and instruction tuning. This
method enhances the model's ability to generate medical captions and answer
complex medical queries. We also release ChiMed-VL, a dataset consisting of
more than 1M image-text pairs. This dataset has been carefully curated to
enable detailed and comprehensive interpretation of medical data using various
types of images.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17963" title="Abstract">arXiv:2310.17963</a> [<a href="/pdf/2310.17963" title="Download PDF">pdf</a>, <a href="/format/2310.17963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision-theoretic MPC: Motion Planning with Weighted Maneuver  Preferences Under Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ta%C5%9F%2C+%C3%96+%C5%9E">&#xd6;mer &#x15e;ahin Ta&#x15f;</a>, 
<a href="/search/cs?searchtype=author&query=Brusius%2C+P+H">Philipp Heinrich Brusius</a>, 
<a href="/search/cs?searchtype=author&query=Stiller%2C+C">Christoph Stiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Continuous optimization based motion planners require deciding on a maneuver
homotopy before optimizing the trajectory. Under uncertainty, maneuver
intentions of other participants can be unclear, and the vehicle might not be
able to decide on the most suitable maneuver. This work introduces a method
that incorporates multiple maneuver preferences in planning. It optimizes the
trajectory by considering weighted maneuver preferences together with
uncertainties ranging from perception to prediction while ensuring the
feasibility of a chance-constrained fallback option. Evaluations in both
driving experiments and simulation studies show enhanced interaction
capabilities and comfort levels compared to conventional planners, which
consider only a single maneuver.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17966" title="Abstract">arXiv:2310.17966</a> [<a href="/pdf/2310.17966" title="Download PDF">pdf</a>, <a href="/format/2310.17966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qisen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiawei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M+G">Matthieu Gaetan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+N">Ning Jia</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shiji Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight. 24 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline-to-online reinforcement learning (RL) is a training paradigm that
combines pre-training on a pre-collected dataset with fine-tuning in an online
environment. However, the incorporation of online fine-tuning can intensify the
well-known distributional shift problem. Existing solutions tackle this problem
by imposing a policy constraint on the policy improvement objective in both
offline and online learning. They typically advocate a single balance between
policy improvement and constraints across diverse data collections. This
one-size-fits-all manner may not optimally leverage each collected sample due
to the significant variation in data quality across different states. To this
end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective
framework that empowers existing algorithms to determine state-adaptive
improvement-constraint balances. FamO2O utilizes a universal model to train a
family of policies with different improvement/constraint intensities, and a
balance model to select a suitable policy for each state. Theoretically, we
prove that state-adaptive balances are necessary for achieving a higher policy
performance upper bound. Empirically, extensive experiments show that FamO2O
offers a statistically significant improvement over various existing methods,
achieving state-of-the-art performance on the D4RL benchmark. Codes are
available at https://github.com/LeapLabTHU/FamO2O.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17972" title="Abstract">arXiv:2310.17972</a> [<a href="/pdf/2310.17972" title="Download PDF">pdf</a>, <a href="/format/2310.17972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CEFL: Carbon-Efficient Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehboob%2C+T">Talha Mehboob</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Iglesias%2C+J+O">Jesus Omana Iglesias</a>, 
<a href="/search/cs?searchtype=author&query=Zink%2C+M">Michael Zink</a>, 
<a href="/search/cs?searchtype=author&query=Irwin%2C+D">David Irwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) distributes machine learning (ML) training across
many edge devices to reduce data transfer overhead and protect data privacy.
Since FL model training may span millions of devices and is thus
resource-intensive, prior work has focused on improving its resource efficiency
to optimize time-to-accuracy. However, prior work generally treats all
resources the same, while, in practice, they may incur widely different costs,
which instead motivates optimizing cost-to-accuracy. To address the problem, we
design CEFL, which uses adaptive cost-aware client selection policies to
optimize an arbitrary cost metric when training FL models. Our policies extend
and combine prior work on utility-based client selection and critical learning
periods by making them cost-aware. We demonstrate CEFL by designing
carbon-efficient FL, where energy's carbon-intensity is the cost, and show that
it i) reduces carbon emissions by 93\% and reduces training time by 50%
compared to random client selection and ii) reduces carbon emissions by 80%,
while only increasing training time by 38%, compared to a state-of-the-art
approach that optimizes training time.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17974" title="Abstract">arXiv:2310.17974</a> [<a href="/pdf/2310.17974" title="Download PDF">pdf</a>, <a href="/format/2310.17974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FaultSeg Swin-UNETR: Transformer-Based Self-Supervised Pretraining Model  for Fault Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeren Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jinwen Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">This paper introduces an approach to enhance seismic fault recognition
through self-supervised pretraining. Seismic fault interpretation holds great
significance in the fields of geophysics and geology. However, conventional
methods for seismic fault recognition encounter various issues, including
dependence on data quality and quantity, as well as susceptibility to
interpreter subjectivity. Currently, automated fault recognition methods
proposed based on small synthetic datasets experience performance degradation
when applied to actual seismic data. To address these challenges, we have
introduced the concept of self-supervised learning, utilizing a substantial
amount of relatively easily obtainable unlabeled seismic data for pretraining.
Specifically, we have employed the Swin Transformer model as the core network
and employed the SimMIM pretraining task to capture unique features related to
discontinuities in seismic data. During the fine-tuning phase, inspired by edge
detection techniques, we have also refined the structure of the Swin-UNETR
model, enabling multiscale decoding and fusion for more effective fault
detection. Experimental results demonstrate that our proposed method attains
state-of-the-art performance on the Thebe dataset, as measured by the OIS and
ODS metrics.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17976" title="Abstract">arXiv:2310.17976</a> [<a href="/pdf/2310.17976" title="Download PDF">pdf</a>, <a href="/format/2310.17976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Role-Playing Chatbots Capture the Character Personalities?  Assessing Personality Traits for Role-Playing Chatbots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Y">Yaying Fei</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+Z">Ziang Leng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A Personality Traits Test Over ChatHaruhi
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The emergence of large-scale pretrained language models has revolutionized
the capabilities of new AI application, especially in the realm of crafting
chatbots with distinct personas. Given the "stimulus-response" nature of
chatbots, this paper unveils an innovative open-ended interview-style approach
for personality assessment on role-playing chatbots, which offers a richer
comprehension of their intrinsic personalities. We conduct personality
assessments on 32 role-playing chatbots created by the ChatHaruhi library,
across both the Big Five and MBTI dimensions, and measure their alignment with
human perception. Evaluation results underscore that modern role-playing
chatbots based on LLMs can effectively portray personality traits of
corresponding characters, with an alignment rate of 82.8% compared with
human-perceived personalities. Besides, we also suggest potential strategies
for shaping chatbots' personalities. Hence, this paper serves as a cornerstone
study for role-playing chatbots that intersects computational linguistics and
psychology. Our resources are available at
https://github.com/LC1332/Chat-Haruhi-Suzumiya
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17977" title="Abstract">arXiv:2310.17977</a> [<a href="/pdf/2310.17977" title="Download PDF">pdf</a>, <a href="/format/2310.17977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous 3D Exploration in Large-Scale Environments with Dynamic  Obstacles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiman%2C+E">Emil Wiman</a>, 
<a href="/search/cs?searchtype=author&query=Wid%C3%A9n%2C+L">Ludvig Wid&#xe9;n</a>, 
<a href="/search/cs?searchtype=author&query=Tiger%2C+M">Mattias Tiger</a>, 
<a href="/search/cs?searchtype=author&query=Heintz%2C+F">Fredrik Heintz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Exploration in dynamic and uncertain real-world environments is an open
problem in robotics and constitutes a foundational capability of autonomous
systems operating in most of the real world. While 3D exploration planning has
been extensively studied, the environments are assumed static or only reactive
collision avoidance is carried out. We propose a novel approach to not only
avoid dynamic obstacles but also include them in the plan itself, to exploit
the dynamic environment in the agent's favor. The proposed planner, Dynamic
Autonomous Exploration Planner (DAEP), extends AEP to explicitly plan with
respect to dynamic obstacles. To thoroughly evaluate exploration planners in
such settings we propose a new enhanced benchmark suite with several dynamic
environments, including large-scale outdoor environments. DAEP outperform
state-of-the-art planners in dynamic and large-scale environments. DAEP is
shown to be more effective at both exploration and collision avoidance.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17980" title="Abstract">arXiv:2310.17980</a> [<a href="/pdf/2310.17980" title="Download PDF">pdf</a>, <a href="/ps/2310.17980" title="Download PostScript">ps</a>, <a href="/format/2310.17980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sketching and Streaming for Dictionary Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Becker%2C+R">Ruben Becker</a>, 
<a href="/search/cs?searchtype=author&query=Canton%2C+M">Matteo Canton</a>, 
<a href="/search/cs?searchtype=author&query=Cenzato%2C+D">Davide Cenzato</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sung-Hwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kodric%2C+B">Bojana Kodric</a>, 
<a href="/search/cs?searchtype=author&query=Prezza%2C+N">Nicola Prezza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We initiate the study of sub-linear sketching and streaming techniques for
estimating the output size of common dictionary compressors such as Lempel-Ziv
'77, the run-length Burrows-Wheeler transform, and grammar compression. To this
end, we focus on a measure that has recently gained much attention in the
information-theoretic community and which approximates up to a polylogarithmic
multiplicative factor the output sizes of those compressors: the normalized
substring complexity function $\delta$. As a matter of fact, $\delta$ itself is
a very accurate measure of compressibility: it is monotone under concatenation,
invariant under reversals and alphabet permutations, sub-additive, and
asymptotically tight (in terms of worst-case entropy) for representing strings,
up to polylogarithmic factors.
<br />We present a data sketch of $O(\epsilon^{-3}\log n + \epsilon^{-1}\log^2 n)$
words that allows computing a multiplicative $(1\pm \epsilon)$-approximation of
$\delta$ with high probability, where $n$ is the string length. The sketches of
two strings $S_1,S_2$ can be merged in $O(\epsilon^{-1}\log^2 n)$ time to yield
the sketch of $\{S_1,S_2\}$, speeding up by orders of magnitude tasks such as
the computation of all-pairs \emph{Normalized Compression Distances} (NCD). If
random access is available on the input, our sketch can be updated in
$O(\epsilon^{-1}\log^2 n)$ time for each character right-extension of the
string. This yields a polylogarithmic-space algorithm for approximating
$\delta$, improving exponentially over the working space of the
state-of-the-art algorithms running in nearly-linear time. Motivated by the
fact that random access is not always available on the input data, we then
present a streaming algorithm computing our sketch in $O(\sqrt n \cdot \log n)$
working space and $O(\epsilon^{-1}\log^2 n)$ worst-case delay per character.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17986" title="Abstract">arXiv:2310.17986</a> [<a href="/pdf/2310.17986" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuzzy Multi-Agent Simulation of COVID-19 Pandemic Spreading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baz%2C+D+E">Didier El Baz</a> (LAAS-SARA), 
<a href="/search/cs?searchtype=author&query=Doncescu%2C+A">Andrei Doncescu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Biomedical Journal of Scientific &amp; Technical Research, 2021, 39
  (4), pp.31519-31521
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">In this paper, we present a new approach for Covid-19 Pandemic spreading
simulation based on fuzzy multi agents. The agent parameters consider
distribution of the population according to age, and the index of
socio-economic fragility. Medical knowledge affirms that the COVID-19 main risk
factors are age and obesity. The worst medical situation is caused by the
combination of these two risk factors which in almost99% of cases finish in
ICU. The appearance of virus variants is another aspect parameter by our
simulation through a simplified modeling of the contagiousness. Using real data
from people from West Indies (Guadeloupe, F.W.I.), we modeled the infection
rate of the risk population, if neither vaccination nor barrier gestures are
respected. The results show that hospital capacities are exceeded, and the
number of deaths exceeds 2% of the infected population, which is close to the
reality.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17989" title="Abstract">arXiv:2310.17989</a> [<a href="/pdf/2310.17989" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering a Paleotsunami Triggered by Mass-Movement in an Alpine Lake
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zafar%2C+M+N">Muhammad Naveed Zafar</a> (UGA, USMB), 
<a href="/search/math?searchtype=author&query=Dutykh%2C+D">Denys Dutykh</a> (KU), 
<a href="/search/math?searchtype=author&query=Sabatier%2C+P">Pierre Sabatier</a> (USMB), 
<a href="/search/math?searchtype=author&query=Banjan%2C+M">Mathilde Banjan</a> (USMB), 
<a href="/search/math?searchtype=author&query=Kim%2C+J">Jihwan Kim</a> (IPMA)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Advances in Hydroinformatics, O. Delestre (Polytech Nice Sophia -- University C{\^o}te d'Azur, France), Nov 2023, Chatou, France
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Mass movements and delta collapses are significant sources of tsunamis in
lacustrine environments, impacting human societies enormously. Palaeotsunamis
play an essential role in understanding historical events and their
consequences along with their return periods. Here, we focus on a palaeo event
that occurred during the Younger Dryas to Early Holocene climatic transition,
ca., 12,000 years ago in the Lake Aiguebelette (NW Alps, France). Based on
highresolution seismic and bathymetric surveys and sedimentological,
geochemical, and magnetic analyses, a seismically induced large mass transport
deposit with an initial volume of 767172 m3 was identified, dated and mapped.
To investigate whether this underwater mass transport produced a palaeotsunami
in the Lake Aiguebelette, this research combines sedimentary records and
numerical models. Numerical simulations of tsunamis are performed using a
viscoplastic landslide model for tsunami source generation and two-dimensional
depth-averaged nonlinear shallow water equations for tsunami wave propagation
and inundation modelling. Our simulations conclude that this sublacustrine
landslide produced a tsunami wave with a maximum amplitude of approximately 2 m
and run-up heights of up to 3.6 m. The modelled sediment thickness resulting
from this mass transport corroborates well with the event deposits mapped in
the lake. Based on our results, we suggest that this sublacustrine mass
transport generated a significant tsunami wave that has not been reported
previously to the best of our knowledge.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17990" title="Abstract">arXiv:2310.17990</a> [<a href="/pdf/2310.17990" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BitUP: Efficient Bitmap Data Storage Solution For User Profile
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Derong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hank Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">User profile is widely used in the internet consumer industry, it can be used
in recommendation systems for better user experience, or improving Ads system
with better conversion rate. Most internet situation we must met large scale
data set, thus retrieve efficient and store with less space became a challenge,
how to handle trillions rows of data is very common in our business scene, so
we proposed a novel solution called BitUP, involved the new distributed bitmap
structure to efficient store profile label data, and can querying with better
performance, the fundamental structure is bitmap, that is why our method is
efficient and less storage overhead. Our design can also scale linearly, to
demonstrate the scalability of the proposed solution we show retrieval
efficiency in various industry data, which scales across from terabytes to
petabytes.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17994" title="Abstract">arXiv:2310.17994</a> [<a href="/pdf/2310.17994" title="Download PDF">pdf</a>, <a href="/format/2310.17994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sargent%2C+K">Kyle Sargent</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zizhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+T">Tanmay Shah</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+C">Charles Herrmann</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong-Xing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+E+R">Eric Ryan Chan</a>, 
<a href="/search/cs?searchtype=author&query=Lagun%2C+D">Dmitry Lagun</a>, 
<a href="/search/cs?searchtype=author&query=Fei-Fei%2C+L">Li Fei-Fei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Deqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view
synthesis for in-the-wild scenes. While existing methods are designed for
single objects with masked backgrounds, we propose new techniques to address
challenges introduced by in-the-wild multi-object scenes with complex
backgrounds. Specifically, we train a generative prior on a mixture of data
sources that capture object-centric, indoor, and outdoor scenes. To address
issues from data mixture such as depth-scale ambiguity, we propose a novel
camera conditioning parameterization and normalization scheme. Further, we
observe that Score Distillation Sampling (SDS) tends to truncate the
distribution of complex backgrounds during distillation of 360-degree scenes,
and propose "SDS anchoring" to improve the diversity of synthesized novel
views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset
in the zero-shot setting, even outperforming methods specifically trained on
DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark
for single-image novel view synthesis, and demonstrate strong performance in
this setting. Our code and data are at <a href="http://kylesargent.github.io/zeronvs/">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17998" title="Abstract">arXiv:2310.17998</a> [<a href="/pdf/2310.17998" title="Download PDF">pdf</a>, <a href="/ps/2310.17998" title="Download PostScript">ps</a>, <a href="/format/2310.17998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closing the Gap Between the Upper Bound and the Lower Bound of Adam&#x27;s  Iteration Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jingwen Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huishuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Accept
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Recently, Arjevani et al. [1] established a lower bound of iteration
complexity for the first-order optimization under an $L$-smooth condition and a
bounded noise variance assumption. However, a thorough review of existing
literature on Adam's convergence reveals a noticeable gap: none of them meet
the above lower bound. In this paper, we close the gap by deriving a new
convergence guarantee of Adam, with only an $L$-smooth condition and a bounded
noise variance assumption. Our results remain valid across a broad spectrum of
hyperparameters. Especially with properly chosen hyperparameters, we derive an
upper bound of the iteration complexity of Adam and show that it meets the
lower bound for first-order optimizers. To the best of our knowledge, this is
the first to establish such a tight upper bound for Adam's convergence. Our
proof utilizes novel techniques to handle the entanglement between momentum and
adaptive learning rate and to convert the first-order term in the Descent Lemma
to the gradient norm, which may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18001" title="Abstract">arXiv:2310.18001</a> [<a href="/pdf/2310.18001" title="Download PDF">pdf</a>, <a href="/format/2310.18001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DP-SGD with weight clipping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barczewski%2C+A">Antoine Barczewski</a>, 
<a href="/search/cs?searchtype=author&query=Ramon%2C+J">Jan Ramon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Recently, due to the popularity of deep neural networks and other methods
whose training typically relies on the optimization of an objective function,
and due to concerns for data privacy, there is a lot of interest in
differentially private gradient descent methods. To achieve differential
privacy guarantees with a minimum amount of noise, it is important to be able
to bound precisely the sensitivity of the information which the participants
will observe. In this study, we present a novel approach that mitigates the
bias arising from traditional gradient clipping. By leveraging public
information concerning the current global model and its location within the
search domain, we can achieve improved gradient bounds, leading to enhanced
sensitivity determinations and refined noise level adjustments. We extend the
state of the art algorithms, present improved differential privacy guarantees
requiring less noise and present an empirical evaluation.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18004" title="Abstract">arXiv:2310.18004</a> [<a href="/pdf/2310.18004" title="Download PDF">pdf</a>, <a href="/format/2310.18004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text2Bundle: Towards Personalized Query-based Bundle Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shixuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chuan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">JunTong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yu Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhihua Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Bundle generation aims to provide a bundle of items for the user, and has
been widely studied and applied on online service platforms. Existing bundle
generation methods mainly utilized user's preference from historical
interactions in common recommendation paradigm, and ignored the potential
textual query which is user's current explicit intention. There can be a
scenario in which a user proactively queries a bundle with some natural
language description, the system should be able to generate a bundle that
exactly matches the user's intention through the user's query and preferences.
In this work, we define this user-friendly scenario as Query-based Bundle
Generation task and propose a novel framework Text2Bundle that leverages both
the user's short-term interests from the query and the user's long-term
preferences from the historical interactions. Our framework consists of three
modules: (1) a query interest extractor that mines the user's fine-grained
interests from the query; (2) a unified state encoder that learns the current
bundle context state and the user's preferences based on historical interaction
and current query; and (3) a bundle generator that generates personalized and
complementary bundles using a reinforcement learning with specifically designed
rewards. We conduct extensive experiments on three real-world datasets and
demonstrate the effectiveness of our framework compared with several
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18009" title="Abstract">arXiv:2310.18009</a> [<a href="/pdf/2310.18009" title="Download PDF">pdf</a>, <a href="/format/2310.18009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProcNet: Deep Predictive Coding Model for Robust-to-occlusion Visual  Segmentation and Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zechmair%2C+M">Michael Zechmair</a>, 
<a href="/search/cs?searchtype=author&query=Bornet%2C+A">Alban Bornet</a>, 
<a href="/search/cs?searchtype=author&query=Morel%2C+Y">Yannick Morel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Systems involving human-robot collaboration necessarily require that steps be
taken to ensure safety of the participating human. This is usually achievable
if accurate, reliable estimates of the human's pose are available. In this
paper, we present a deep Predictive Coding (PC) model supporting visual
segmentation, which we extend to pursue pose estimation. The model is designed
to offer robustness to the type of transient occlusion naturally occurring when
human and robot are operating in close proximity to one another. Impact on
performance of relevant model parameters is assessed, and comparison to an
alternate pose estimation model (NVIDIA's PoseCNN) illustrates efficacy of the
proposed approach.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18011" title="Abstract">arXiv:2310.18011</a> [<a href="/pdf/2310.18011" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data journeys in popular science: Producing climate change and COVID-19  data visualizations at Scientific American
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gregory%2C+K">Kathleen Gregory</a>, 
<a href="/search/cs?searchtype=author&query=Koesten%2C+L">Laura Koesten</a>, 
<a href="/search/cs?searchtype=author&query=Schuster%2C+R">Regina Schuster</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+T">Torsten M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+S">Sarah Davies</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 4 figures, 3 boxes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Human-Computer Interaction (cs.HC); Popular Physics (physics.pop-ph)

</div>
<p class="mathjax">Vast amounts of (open) data are increasingly used to make arguments about
crisis topics such as climate change and global pandemics. Data visualizations
are central to bringing these viewpoints to broader publics. However,
visualizations often conceal the many contexts involved in their production,
ranging from decisions made in research labs about collecting and sharing data
to choices made in editorial rooms about which data stories to tell. In this
paper, we examine how data visualizations about climate change and COVID-19 are
produced in popular science magazines, using Scientific American, an
established English-language popular science magazine, as a case study. To do
this, we apply the analytical concept of "data journeys" (Leonelli, 2020) in a
mixed methods study that centers on interviews with Scientific American staff
and is supplemented by a visualization analysis of selected charts. In
particular, we discuss the affordances of working with open data, the role of
collaborative data practices, and how the magazine works to counter
misinformation and increase transparency. This work provides a theoretical
contribution by testing and expanding the concept of data journeys as an
analytical framework, as well as practical contributions by providing insight
into the data (visualization) practices of science communicators.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18015" title="Abstract">arXiv:2310.18015</a> [<a href="/pdf/2310.18015" title="Download PDF">pdf</a>, <a href="/ps/2310.18015" title="Download PostScript">ps</a>, <a href="/format/2310.18015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nitsche&#x27;s prescription of Dirichlet conditions in the finite element  approximation of Maxwell&#x27;s problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boffi%2C+D">D. Boffi</a>, 
<a href="/search/math?searchtype=author&query=Codina%2C+R">R. Codina</a>, 
<a href="/search/math?searchtype=author&query=T%C3%BCrk%2C+%C3%96">&#xd6;. T&#xfc;rk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we consider the finite element approximation of Maxwell's
problem and analyse the prescription of essential boundary conditions in a weak
sense using Nitsche's method. To avoid indefiniteness of the problem, the
original equations are augmented with the gradient of a scalar field that
allows one to impose the zero divergence of the magnetic induction, even if the
exact solution for this scalar field is zero. Two finite element approximations
are considered, namely, one in which the approximation spaces are assumed to
satisfy the appropriate inf-sup condition that render the standard Galerkin
method stable, and another augmented and stabilised one that permits the use of
finite element interpolations of arbitrary order. Stability and convergence
results are provided for the two finite element formulations considered.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18018" title="Abstract">arXiv:2310.18018</a> [<a href="/pdf/2310.18018" title="Download PDF">pdf</a>, <a href="/format/2310.18018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination  for each Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sainz%2C+O">Oscar Sainz</a>, 
<a href="/search/cs?searchtype=author&query=Campos%2C+J+A">Jon Ander Campos</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ferrero%2C+I">Iker Garc&#xed;a-Ferrero</a>, 
<a href="/search/cs?searchtype=author&query=Etxaniz%2C+J">Julen Etxaniz</a>, 
<a href="/search/cs?searchtype=author&query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
<a href="/search/cs?searchtype=author&query=Agirre%2C+E">Eneko Agirre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP2024-Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this position paper, we argue that the classical evaluation on Natural
Language Processing (NLP) tasks using annotated benchmarks is in trouble. The
worst kind of data contamination happens when a Large Language Model (LLM) is
trained on the test split of a benchmark, and then evaluated in the same
benchmark. The extent of the problem is unknown, as it is not straightforward
to measure. Contamination causes an overestimation of the performance of a
contaminated model in a target benchmark and associated task with respect to
their non-contaminated counterparts. The consequences can be very harmful, with
wrong scientific conclusions being published while other correct ones are
discarded. This position paper defines different levels of data contamination
and argues for a community effort, including the development of automatic and
semi-automatic measures to detect when data from a benchmark was exposed to a
model, and suggestions for flagging papers with conclusions that are
compromised by data contamination.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18019" title="Abstract">arXiv:2310.18019</a> [<a href="/pdf/2310.18019" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temperature Monitoring of Agricultural Areas in a Secure Data Room
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ederer%2C+T">Thomas Ederer</a>, 
<a href="/search/cs?searchtype=author&query=Ivancsits%2C+M">Martin Ivancsits</a>, 
<a href="/search/cs?searchtype=author&query=Ivki%C4%87%2C+I">Igor Ivki&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Link to online article: <a href="https://ercim-news.ercim.eu/en135/special/temperature-monitoring-of-agricultural-areas-in-a-secure-data-room">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ercim News Special Theme: Climate-Resilient Society 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Agricultural production is highly dependent on naturally occurring
environmental conditions like change of seasons and the weather. Especially in
fruit and wine growing, late frosts occurring shortly after the crops have
sprouted have the potential to cause massive damage to plants [L1,L2] [1]. In
this article we present a cost-efficient temperature monitoring system for
detecting and reacting to late frosts to prevent crop failures. The proposed
solution includes a data space where Internet of Things (IoT) devices can form
a cyber-physical system (CPS) to interact with their nearby environment and
securely exchange data. Based on this data, more accurate predictions can be
made in the future using machine learning (ML), which will further contribute
to minimising economic damage caused by crop failures.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18021" title="Abstract">arXiv:2310.18021</a> [<a href="/pdf/2310.18021" title="Download PDF">pdf</a>, <a href="/format/2310.18021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FormalGeo: The First Step Toward Human-like IMO-level Geometric  Automated Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Na Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yiming He</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qike Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaoxiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanjun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chenyang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhe Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+D">Dengfeng Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fangzhen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Cheng Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhenbing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shaorong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiangfeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+T">Tuo Leng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This is the first article of our work over the past decade. In this series of
papers, we have constructed a complete and compatible formal plane geometry
system. This will serve as a crucial bridge between IMO-level plane geometry
challenges and readable AI automated reasoning. With this formal system in
place, we have been able to seamlessly integrate modern AI models with our
formal system. Within this formal framework, AI is now capable of providing
deductive reasoning solutions to IMO-level plane geometry problems, just like
handling other natural languages, and these proofs are readable, traceable, and
verifiable. We propose the geometry formalization theory (GFT) to guide the
development of the geometry formal system. Based on the GFT, we have
established the FormalGeo, which consists of 88 geometric predicates and 196
theorems. It can represent, validate, and solve IMO-level geometry problems. we
also have crafted the FGPS (formal geometry problem solver) in Python. It
serves as both an interactive assistant for verifying problem-solving processes
and an automated problem solver, utilizing various methods such as forward
search, backward search and AI-assisted search. We've annotated the FormalGeo7k
dataset, containing 6,981 (expand to 186,832 through data augmentation)
geometry problems with complete formal language annotations. Implementation of
the formal system and experiments on the FormalGeo7k validate the correctness
and utility of the GFT. The backward depth-first search method only yields a
2.42% problem-solving failure rate, and we can incorporate deep learning
techniques to achieve lower one. The source code of FGPS and FormalGeo7k
dataset are available at https://github.com/BitSecret/FormalGeo.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18023" title="Abstract">arXiv:2310.18023</a> [<a href="/pdf/2310.18023" title="Download PDF">pdf</a>, <a href="/format/2310.18023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raihan%2C+M+N">Md Nishat Raihan</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dhiman Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+A">Antara Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Anstasopoulos%2C+A">Antonios Anstasopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Zampieri%2C+M">Marcos Zampieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Code-mixing is a well-studied linguistic phenomenon when two or more
languages are mixed in text or speech. Several datasets have been build with
the goal of training computational models for code-mixing. Although it is very
common to observe code-mixing with multiple languages, most datasets available
contain code-mixed between only two languages. In this paper, we introduce
SentMix-3L, a novel dataset for sentiment analysis containing code-mixed data
between three languages Bangla, English, and Hindi. We carry out a
comprehensive evaluation using SentMix-3L. We show that zero-shot prompting
with GPT-3.5 outperforms all transformer-based models on SentMix-3L.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18025" title="Abstract">arXiv:2310.18025</a> [<a href="/pdf/2310.18025" title="Download PDF">pdf</a>, <a href="/format/2310.18025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large language models for aspect-based sentiment analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simmering%2C+P+F">Paul F. Simmering</a>, 
<a href="/search/cs?searchtype=author&query=Huoviala%2C+P">Paavo Huoviala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) offer unprecedented text completion
capabilities. As general models, they can fulfill a wide range of roles,
including those of more specialized models. We assess the performance of GPT-4
and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based
sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art
F1 score of 83.8 on the joint aspect term extraction and polarity
classification task of the SemEval-2014 Task 4, improving upon InstructABSA
[@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000
times more model parameters and thus increased inference cost. We discuss the
the cost-performance trade-offs of different models, and analyze the typical
errors that they make. Our results also indicate that detailed prompts improve
performance in zero-shot and few-shot settings but are not necessary for
fine-tuned models. This evidence is relevant for practioners that are faced
with the choice of prompt engineering versus fine-tuning when using LLMs for
ABSA.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18030" title="Abstract">arXiv:2310.18030</a> [<a href="/pdf/2310.18030" title="Download PDF">pdf</a>, <a href="/format/2310.18030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confucius Queue Management: Be Fair But Not Too Fast
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zili Meng</a>, 
<a href="/search/cs?searchtype=author&query=Atre%2C+N">Nirav Atre</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sherry%2C+J">Justine Sherry</a>, 
<a href="/search/cs?searchtype=author&query=Apostolaki%2C+M">Maria Apostolaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">When many users and unique applications share a congested edge link (e.g., a
home network), everyone wants their own application to continue to perform well
despite contention over network resources. Traditionally, network engineers
have focused on fairness as the key objective to ensure that competing
applications are equitably and led by the switch, and hence have deployed fair
queueing mechanisms. However, for many network workloads today, strict fairness
is directly at odds with equitable application performance. Real-time streaming
applications, such as videoconferencing, suffer the most when network
performance is volatile (with delay spikes or sudden and dramatic drops in
throughput). Unfortunately, "fair" queueing mechanisms lead to extremely
volatile network behavior in the presence of bursty and multi-flow applications
such as Web traffic. When a sudden burst of new data arrives, fair queueing
algorithms rapidly shift resources away from incumbent flows, leading to severe
stalls in real-time applications. In this paper, we present Confucius, the
first practical queue management scheme to effectively balance fairness against
volatility, providing performance outcomes that benefit all applications
sharing the contended link. Confucius outperforms realistic queueing schemes by
protecting the real-time streaming flows from stalls in competing with more
than 95% of websites. Importantly, Confucius does not assume the collaboration
of end-hosts, nor does it require manual parameter tuning to achieve good
performance.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18033" title="Abstract">arXiv:2310.18033</a> [<a href="/pdf/2310.18033" title="Download PDF">pdf</a>, <a href="/format/2310.18033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Analysis of Participatory Budgeting in Amsterdam
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nelissen%2C+P">Pelle Nelissen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">Using data from 35 Participatory Budgeting instances in Amsterdam, we
empirically compare two different Participatory Budgeting rules: the greedy
cost welfare rule and the Method of Equal Shares. We quantify how proportional,
equal and fair the rules are and conclude that, for a small price in total
voter satisfaction, the Method of Equal Shares performs better on all notions
of fairness studied. We further provide a popular and a visual explanation of
the Method of Equal Shares.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18034" title="Abstract">arXiv:2310.18034</a> [<a href="/pdf/2310.18034" title="Download PDF">pdf</a>, <a href="/format/2310.18034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Evaluation of Fully Dynamic k-Means via Coresets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henzinger%2C+M">Monika Henzinger</a>, 
<a href="/search/cs?searchtype=author&query=Saulpic%2C+D">David Saulpic</a>, 
<a href="/search/cs?searchtype=author&query=Sidl%2C+L">Leonhard Sidl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ALENEX 24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">For a set of points in $\mathbb{R}^d$, the Euclidean $k$-means problems
consists of finding $k$ centers such that the sum of distances squared from
each data point to its closest center is minimized. Coresets are one the main
tools developed recently to solve this problem in a big data context. They
allow to compress the initial dataset while preserving its structure: running
any algorithm on the coreset provides a guarantee almost equivalent to running
it on the full data.
<br />In this work, we study coresets in a fully-dynamic setting: points are added
and deleted with the goal to efficiently maintain a coreset with which a
k-means solution can be computed. Based on an algorithm from Henzinger and Kale
[ESA'20], we present an efficient and practical implementation of a fully
dynamic coreset algorithm, that improves the running time by up to a factor of
20 compared to our non-optimized implementation of the algorithm by Henzinger
and Kale, without sacrificing more than 7% on the quality of the k-means
solution.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18036" title="Abstract">arXiv:2310.18036</a> [<a href="/pdf/2310.18036" title="Download PDF">pdf</a>, <a href="/ps/2310.18036" title="Download PostScript">ps</a>, <a href="/format/2310.18036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and simple unrooted dynamic forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berendsohn%2C+B+A">Benjamin Aram Berendsohn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">A dynamic forest data structure maintains a forest (and associated data like
edge weights) under edge insertions and deletions. Dynamic forests are widely
used to solve online and offline graph problems. Well-known examples of dynamic
forest data structures are link-cut trees [Sleator and Tarjan '83] and top
trees [Alstrup, Holm, de Lichtenberg, and Thorup '05], both of which need O(log
n) time per operation. While top trees are more flexible and arguably easier to
use, link-cut trees are faster in practice [Tarjan and Werneck '10].
<br />In this paper, we propose an alternative to link-cut trees. Our data
structure is based on search trees on trees (STTs, also known as elimination
trees) and an STT algorithm [Berendsohn and Kozma '22] based on the classical
Splay trees [Sleator and Tarjan '85]. While link-cut trees maintain a hierarchy
of binary search trees, we maintain a single STT. Most of the complexity of our
data structure lies in the implementation of the STT rotation primitive, which
can easily be reused, simplifying the development of new STT-based approaches.
<br />We implement several variants of our data structure in the Rust programming
language, along with an implementation of link-cut trees for comparison.
Experimental evaluation suggests that our algorithms are faster when the
dynamic forest is unrooted, while link-cut trees are faster for rooted dynamic
forests.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18037" title="Abstract">arXiv:2310.18037</a> [<a href="/pdf/2310.18037" title="Download PDF">pdf</a>, <a href="/format/2310.18037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Validation for Distributed Control of Energy Hubs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Behrunani%2C+V">Varsha Behrunani</a>, 
<a href="/search/eess?searchtype=author&query=Heer%2C+P">Philipp Heer</a>, 
<a href="/search/eess?searchtype=author&query=Lygeros%2C+J">John Lygeros</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, CISBAT conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC)

</div>
<p class="mathjax">As future energy systems become more decentralised due to the integration of
renewable energy resources and storage technologies, several autonomous energy
management and peer-to-peer trading mechanisms have been recently proposed for
the operation of energy hub networks based on optimization and game theory.
However, most of these strategies have been tested either only in simulated
environments or small prosumer units as opposed to larger energy hubs. This
simulation reality gap has hindered large-scale implementation and practical
application of these method. In this paper, we aim to experimentally validate
the performance of a novel multi-horizon distributed model predictive
controller for an energy hub network by implementing the controller on a
complete network of hubs comprising of a real energy hub inter-faced with
multiple virtual hubs. The experiments are done using two different network
topologies and the controller shows promising results in both setups.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18038" title="Abstract">arXiv:2310.18038</a> [<a href="/pdf/2310.18038" title="Download PDF">pdf</a>, <a href="/format/2310.18038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On General Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlangen%2C+D">David Schlangen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Natural Language Processing prides itself to be an empirically-minded, if not
outright empiricist field, and yet lately it seems to get itself into
essentialist debates on issues of meaning and measurement ("Do Large Language
Models Understand Language, And If So, How Much?"). This is not by accident:
Here, as everywhere, the evidence underspecifies the understanding. As a
remedy, this paper sketches the outlines of a model of understanding, which can
ground questions of the adequacy of current methods of measurement of model
quality. The paper makes three claims: A) That different language use situation
types have different characteristics, B) That language understanding is a
multifaceted phenomenon, bringing together individualistic and social
processes, and C) That the choice of Understanding Indicator marks the limits
of benchmarking, and the beginnings of considerations of the ethics of NLP use.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18040" title="Abstract">arXiv:2310.18040</a> [<a href="/pdf/2310.18040" title="Download PDF">pdf</a>, <a href="/ps/2310.18040" title="Download PostScript">ps</a>, <a href="/format/2310.18040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moral Responsibility for AI Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beckers%2C+S">Sander Beckers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">As more and more decisions that have a significant ethical dimension are
being outsourced to AI systems, it is important to have a definition of moral
responsibility that can be applied to AI systems. Moral responsibility for an
outcome of an agent who performs some action is commonly taken to involve both
a causal condition and an epistemic condition: the action should cause the
outcome, and the agent should have been aware -- in some form or other -- of
the possible moral consequences of their action. This paper presents a formal
definition of both conditions within the framework of causal models. I compare
my approach to the existing approaches of Braham and van Hees (BvH) and of
Halpern and Kleiman-Weiner (HK). I then generalize my definition into a degree
of responsibility.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18042" title="Abstract">arXiv:2310.18042</a> [<a href="/pdf/2310.18042" title="Download PDF">pdf</a>, <a href="/format/2310.18042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sui Lutris: A Blockchain Combining Broadcast and Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blackshear%2C+S">Same Blackshear</a>, 
<a href="/search/cs?searchtype=author&query=Chursin%2C+A">Andrey Chursin</a>, 
<a href="/search/cs?searchtype=author&query=Danezis%2C+G">George Danezis</a>, 
<a href="/search/cs?searchtype=author&query=Kichidis%2C+A">Anastasios Kichidis</a>, 
<a href="/search/cs?searchtype=author&query=Kokoris-Kogias%2C+L">Lefteris Kokoris-Kogias</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xun Li</a>, 
<a href="/search/cs?searchtype=author&query=Logan%2C+M">Mark Logan</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+A">Ashok Menon</a>, 
<a href="/search/cs?searchtype=author&query=Nowacki%2C+T">Todd Nowacki</a>, 
<a href="/search/cs?searchtype=author&query=Sonnino%2C+A">Alberto Sonnino</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+B">Brandon Williams</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Sui Lutris is the first smart-contract platform to sustainably achieve
sub-second finality. It achieves this significant decrease in latency by
employing consensusless agreement not only for simple payments but for a large
variety of transactions. Unlike prior work, Sui Lutris neither compromises
expressiveness nor throughput and can run perpetually without restarts. Sui
Lutris achieves this by safely integrating consensuless agreement with a
high-throughput consensus protocol that is invoked out of the critical finality
path but makes sure that when a transaction is at risk of inconsistent
concurrent accesses its settlement is delayed until the total ordering is
resolved. Building such a hybrid architecture is especially delicate during
reconfiguration events, where the system needs to preserve the safety of the
consensusless path without compromising the long-term liveness of potentially
misconfigured clients. We thus develop a novel reconfiguration protocol, the
first to show the safe and efficient reconfiguration of a consensusless
blockchain. Sui Lutris is currently running in production as part of a major
smart-contract platform. Combined with the Move Programming language it enables
the safe execution of smart contracts that expose objects as a first-class
resource. In our experiments Sui Lutris achieves latency lower than 0.5 seconds
for throughput up to 5,000 certificates per second (150k ops/s with bundling),
compared to the state-of-the-art real-world consensus latencies of 3 seconds.
Furthermore, it gracefully handles validators crash-recovery and does not
suffer visible performance degradation during reconfiguration.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18043" title="Abstract">arXiv:2310.18043</a> [<a href="/pdf/2310.18043" title="Download PDF">pdf</a>, <a href="/format/2310.18043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interior Eigensolver Based on Rational Filter with Composite rule
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+Y">Yuer Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yingzhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages,26 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Contour integral based rational filter leads to interior eigensolvers for
non-Hermitian generalized eigenvalue problems. Based on the Zolotarev's
problems, this paper proves the asymptotic optimality of the trapezoidal
quadrature of the contour integral in terms of the rational function
separation. A composite rule of the trapezoidal quadrature is derived. Two
interior eigensolvers are proposed based on the composite rule. Both
eigensolvers adopt direct factorization and multi-shift generalized minimal
residual method for the inner and outer rational functions, respectively. The
first eigensolver fixes the order of the outer rational function and applies
the subspace iteration to achieve convergence, whereas the second eigensolver
doubles the order of the outer rational function every iteration to achieve
convergence without subspace iteration. The efficiency and stability of
proposed eigensolvers are demonstrated on synthetic and practical sparse matrix
pencils.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18046" title="Abstract">arXiv:2310.18046</a> [<a href="/pdf/2310.18046" title="Download PDF">pdf</a>, <a href="/format/2310.18046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model  for Visual Question Answering in Vietnamese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+K+V">Khiem Vinh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H+P">Hao Phu Phan</a>, 
<a href="/search/cs?searchtype=author&query=Van+Nguyen%2C+K">Kiet Van Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N+L+T">Ngan Luu Thuy Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A pre-print version and submitted to journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In recent years, Visual Question Answering (VQA) has gained significant
attention for its diverse applications, including intelligent car assistance,
aiding visually impaired individuals, and document image information retrieval
using natural language queries. VQA requires effective integration of
information from questions and images to generate accurate answers. Neural
models for VQA have made remarkable progress on large-scale datasets, with a
primary focus on resource-rich languages like English. To address this, we
introduce the ViCLEVR dataset, a pioneering collection for evaluating various
visual reasoning capabilities in Vietnamese while mitigating biases. The
dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs),
each question annotated to specify the type of reasoning involved. Leveraging
this dataset, we conduct a comprehensive analysis of contemporary visual
reasoning systems, offering valuable insights into their strengths and
limitations. Furthermore, we present PhoVIT, a comprehensive multimodal fusion
that identifies objects in images based on questions. The architecture
effectively employs transformers to enable simultaneous reasoning over textual
and visual data, merging both modalities at an early model stage. The
experimental findings demonstrate that our proposed model achieves
state-of-the-art performance across four evaluation metrics. The accompanying
code and dataset have been made publicly accessible at
\url{https://github.com/kvt0012/ViCLEVR}. This provision seeks to stimulate
advancements within the research community, fostering the development of more
multimodal fusion algorithms, specifically tailored to address the nuances of
low-resource languages, exemplified by Vietnamese.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18049" title="Abstract">arXiv:2310.18049</a> [<a href="/pdf/2310.18049" title="Download PDF">pdf</a>, <a href="/format/2310.18049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Augmented Spatial-aware Zero-shot Referring Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suo%2C+Y">Yucheng Suo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Linchao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we study a challenging task of zero-shot referring image
segmentation. This task aims to identify the instance mask that is most related
to a referring expression without training on pixel-level annotations. Previous
research takes advantage of pre-trained cross-modal models, e.g., CLIP, to
align instance-level masks with referring expressions. %Yet, CLIP only
considers image-text pair level alignment, which neglects fine-grained image
region and complex sentence matching. Yet, CLIP only considers the global-level
alignment of image-text pairs, neglecting fine-grained matching between the
referring sentence and local image regions. To address this challenge, we
introduce a Text Augmented Spatial-aware (TAS) zero-shot referring image
segmentation framework that is training-free and robust to various visual
encoders. TAS incorporates a mask proposal network for instance-level mask
extraction, a text-augmented visual-text matching score for mining the
image-text correlation, and a spatial rectifier for mask post-processing.
Notably, the text-augmented visual-text matching score leverages a $P$ score
and an $N$-score in addition to the typical visual-text matching score. The
$P$-score is utilized to close the visual-text domain gap through a surrogate
captioning model, where the score is computed between the surrogate
model-generated texts and the referring expression. The $N$-score considers the
fine-grained alignment of region-text pairs via negative phrase mining,
encouraging the masked image to be repelled from the mined distracting phrases.
Extensive experiments are conducted on various datasets, including RefCOCO,
RefCOCO+, and RefCOCOg. The proposed method clearly outperforms
state-of-the-art zero-shot referring image segmentation methods.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18063" title="Abstract">arXiv:2310.18063</a> [<a href="/pdf/2310.18063" title="Download PDF">pdf</a>, <a href="/format/2310.18063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Honey, Tell Me What&#x27;s Wrong&quot;, Global Explanation of Textual  Discriminative Models through Cooperative Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaffin%2C+A">Antoine Chaffin</a>, 
<a href="/search/cs?searchtype=author&query=Delaunay%2C+J">Julien Delaunay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages plus references and 2 pages of appendices. 7 figures and 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The ubiquity of complex machine learning has raised the importance of
model-agnostic explanation algorithms. These methods create artificial
instances by slightly perturbing real instances, capturing shifts in model
decisions. However, such methods rely on initial data and only provide
explanations of the decision for these. To tackle these problems, we propose
Therapy, the first global and model-agnostic explanation method adapted to text
which requires no input dataset. Therapy generates texts following the
distribution learned by a classifier through cooperative generation. Because it
does not rely on initial samples, it allows to generate explanations even when
data is absent (e.g., for confidentiality reasons). Moreover, conversely to
existing methods that combine multiple local explanations into a global one,
Therapy offers a global overview of the model behavior on the input space. Our
experiments show that although using no input data to generate samples, Therapy
provides insightful information about features used by the classifier that is
competitive with the ones from methods relying on input samples and outperforms
them when input samples are not specific to the studied model.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18068" title="Abstract">arXiv:2310.18068</a> [<a href="/pdf/2310.18068" title="Download PDF">pdf</a>, <a href="/format/2310.18068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple and Robust Dynamic Two-Dimensional Convex Hull
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%A6de%2C+E+T">Emil Toftegaard G&#xe6;de</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B8rtz%2C+I+L">Inge Li G&#xf8;rtz</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Krogh%2C+C">Christoffer Krogh</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ALENEX24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">The convex hull of a data set $P$ is the smallest convex set that contains
$P$.
<br />In this work, we present a new data structure for convex hull, that allows
for efficient dynamic updates. In a dynamic convex hull implementation, the
following traits are desirable: (1) algorithms for efficiently answering
queries as to whether a specified point is inside or outside the hull, (2)
adhering to geometric robustness, and (3) algorithmic simplicity.Furthermore, a
specific but well-motivated type of two-dimensional data is rank-based data.
Here, the input is a set of real-valued numbers $Y$ where for any number $y\in
Y$ its rank is its index in $Y$'s sorted order. Each value in $Y$ can be mapped
to a point $(rank, value)$ to obtain a two-dimensional point set. In this work,
we give an efficient, geometrically robust, dynamic convex hull algorithm, that
facilitates queries to whether a point is internal. Furthermore, our
construction can be used to efficiently update the convex hull of rank-ordered
data, when the real-valued point set is subject to insertions and deletions.
Our improved solution is based on an algorithmic simplification of the
classical convex hull data structure by Overmars and van Leeuwen~[STOC'80],
combined with new algorithmic insights. Our theoretical guarantees on the
update time match those of Overmars and van Leeuwen, namely $O(\log^2 |P|)$,
while we allow a wider range of functionalities (including rank-based data).
Our algorithmic simplification includes simplifying an 11-case check down to a
3-case check that can be written in 20 lines of easily readable C-code. We
extend our solution to provide a trade-off between theoretical guarantees and
the practical performance of our algorithm. We test and compare our solutions
extensively on inputs that were generated randomly or adversarially, including
benchmarking datasets from the literature.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18069" title="Abstract">arXiv:2310.18069</a> [<a href="/pdf/2310.18069" title="Download PDF">pdf</a>, <a href="/ps/2310.18069" title="Download PostScript">ps</a>, <a href="/format/2310.18069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Verification of Parametric Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peuter%2C+D">Dennis Peuter</a>, 
<a href="/search/cs?searchtype=author&query=Marohn%2C+P">Philipp Marohn</a>, 
<a href="/search/cs?searchtype=author&query=Sofronie-Stokkermans%2C+V">Viorica Sofronie-Stokkermans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages. arXiv admin note: text overlap with <a href="/abs/1910.05208">arXiv:1910.05208</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We present an approach to the verification of systems for whose description
some elements - constants or functions - are underspecified and can be regarded
as parameters, and, in particular, describe a method for automatically
generating constraints on such parameters under which certain safety conditions
are guaranteed to hold. We present an implementation and illustrate its use on
several examples.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18070" title="Abstract">arXiv:2310.18070</a> [<a href="/pdf/2310.18070" title="Download PDF">pdf</a>, <a href="/format/2310.18070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-grained Evidence Inference for Multi-choice Reading Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+S">Sufeng Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TASLP 2023, vol. 31, pp. 3896-3907
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE/ACM Transactions on Audio, Speech, and Language
  Processing, vol. 31, pp. 3896-3907, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-choice Machine Reading Comprehension (MRC) is a major and challenging
task for machines to answer questions according to provided options. Answers in
multi-choice MRC cannot be directly extracted in the given passages, and
essentially require machines capable of reasoning from accurate extracted
evidence. However, the critical evidence may be as simple as just one word or
phrase, while it is hidden in the given redundant, noisy passage with multiple
linguistic hierarchies from phrase, fragment, sentence until the entire
passage. We thus propose a novel general-purpose model enhancement which
integrates multi-grained evidence comprehensively, named Multi-grained evidence
inferencer (Mugen), to make up for the inability. Mugen extracts three
different granularities of evidence: coarse-, middle- and fine-grained
evidence, and integrates evidence with the original passages, achieving
significant and consistent performance improvement on four multi-choice MRC
benchmarks.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18071" title="Abstract">arXiv:2310.18071</a> [<a href="/pdf/2310.18071" title="Download PDF">pdf</a>, <a href="/ps/2310.18071" title="Download PostScript">ps</a>, <a href="/format/2310.18071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Primal-Dual Algorithms for Online k-way Matching with  Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kakimura%2C+N">Naonori Kakimura</a>, 
<a href="/search/cs?searchtype=author&query=Nakayoshi%2C+T">Tomohiro Nakayoshi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this paper, we study the Min-cost Perfect $k$-way Matching with Delays
($k$-MPMD), recently introduced by Melnyk et al. In the problem, $m$ requests
arrive one-by-one over time in a metric space. At any time, we can irrevocably
make a group of $k$ requests who arrived so far, that incurs the distance cost
among the $k$ requests in addition to the sum of the waiting cost for the $k$
requests. The goal is to partition all the requests into groups of $k$
requests, minimizing the total cost. The problem is a generalization of the
min-cost perfect matching with delays (corresponding to $2$-MPMD). It is known
that no online algorithm for $k$-MPMD can achieve a bounded competitive ratio
in general, where the competitive ratio is the worst-case ratio between its
performance and the offline optimal value. On the other hand, $k$-MPMD is known
to admit a randomized online algorithm with competitive ratio $O(k^{5}\log n)$
for a certain class of $k$-point metrics called the $H$-metric, where $n$ is
the size of the metric space. In this paper, we propose a deterministic online
algorithm with a competitive ratio of $O(mk^2)$ for the $k$-MPMD in $H$-metric
space. Furthermore, we show that the competitive ratio can be improved to $O(m
+ k^2)$ if the metric is given as a diameter on a line.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18073" title="Abstract">arXiv:2310.18073</a> [<a href="/pdf/2310.18073" title="Download PDF">pdf</a>, <a href="/format/2310.18073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Framework for Table of Contents Extraction from Complex ESG  Annual Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Table of contents (ToC) extraction centres on structuring documents in a
hierarchical manner. In this paper, we propose a new dataset, ESGDoc,
comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to
2022. These reports pose significant challenges due to their diverse structures
and extensive length. To address these challenges, we propose a new framework
for Toc extraction, consisting of three steps: (1) Constructing an initial tree
of text blocks based on reading order and font sizes; (2) Modelling each tree
node (or text block) independently by considering its contextual information
captured in node-centric subtree; (3) Modifying the original tree by taking
appropriate action on each tree node (Keep, Delete, or Move). This
construction-modelling-modification (CMM) process offers several benefits. It
eliminates the need for pairwise modelling of section headings as in previous
approaches, making document segmentation practically feasible. By incorporating
structured information, each section heading can leverage both local and
long-distance context relevant to itself. Experimental results show that our
approach outperforms the previous state-of-the-art baseline with a fraction of
running time. Our framework proves its scalability by effectively handling
documents of any length.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18074" title="Abstract">arXiv:2310.18074</a> [<a href="/pdf/2310.18074" title="Download PDF">pdf</a>, <a href="/ps/2310.18074" title="Download PostScript">ps</a>, <a href="/format/2310.18074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On kernel-based statistical learning in the mean field limit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiedler%2C+C">Christian Fiedler</a>, 
<a href="/search/cs?searchtype=author&query=Herty%2C+M">Michael Herty</a>, 
<a href="/search/cs?searchtype=author&query=Trimpe%2C+S">Sebastian Trimpe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">In many applications of machine learning, a large number of variables are
considered. Motivated by machine learning of interacting particle systems, we
consider the situation when the number of input variables goes to infinity.
First, we continue the recent investigation of the mean field limit of kernels
and their reproducing kernel Hilbert spaces, completing the existing theory.
Next, we provide results relevant for approximation with such kernels in the
mean field limit, including a representer theorem. Finally, we use these
kernels in the context of statistical learning in the mean field limit,
focusing on Support Vector Machines. In particular, we show mean field
convergence of empirical and infinite-sample solutions as well as the
convergence of the corresponding risks. On the one hand, our results establish
rigorous mean field limits in the context of kernel methods, providing new
theoretical tools and insights for large-scale problems. On the other hand, our
setting corresponds to a new form of limit of learning problems, which seems to
have not been investigated yet in the statistical learning theory literature.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18075" title="Abstract">arXiv:2310.18075</a> [<a href="/pdf/2310.18075" title="Download PDF">pdf</a>, <a href="/format/2310.18075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xiaoyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liangyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Na Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaxuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Wei Zou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kaijiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+M">Ming Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Inspired by the dual-process theory of human cognition, we introduce DUMA, a
novel conversational agent framework that embodies a dual-mind mechanism
through the utilization of two generative Large Language Models (LLMs)
dedicated to fast and slow thinking respectively. The fast thinking model
serves as the primary interface for external interactions and initial response
generation, evaluating the necessity for engaging the slow thinking model based
on the complexity of the complete response. When invoked, the slow thinking
model takes over the conversation, engaging in meticulous planning, reasoning,
and tool utilization to provide a well-analyzed response. This dual-mind
configuration allows for a seamless transition between intuitive responses and
deliberate problem-solving processes based on the situation. We have
constructed a conversational agent to handle online inquiries in the real
estate industry. The experiment proves that our method balances effectiveness
and efficiency, and has a significant improvement compared to the baseline.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18076" title="Abstract">arXiv:2310.18076</a> [<a href="/pdf/2310.18076" title="Download PDF">pdf</a>, <a href="/format/2310.18076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Corpus Error in Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yejoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+P">Philhoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent works in open-domain question answering (QA) have explored generating
context passages from large language models (LLMs), replacing the traditional
retrieval step in the QA pipeline. However, it is not well understood why
generated passages can be more effective than retrieved ones. This study
revisits the conventional formulation of QA and introduces the concept of
knowledge corpus error. This error arises when the knowledge corpus used for
retrieval is only a subset of the entire string space, potentially excluding
more helpful passages that exist outside the corpus. LLMs may mitigate this
shortcoming by generating passages in a larger space. We come up with an
experiment of paraphrasing human-annotated gold context using LLMs to observe
knowledge corpus error empirically. Our results across three QA benchmarks
reveal an increased performance (10% - 13%) when using paraphrased passage,
indicating a signal for the existence of knowledge corpus error. Our code is
available at https://github.com/xfactlab/emnlp2023-knowledge-corpus-error
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18077" title="Abstract">arXiv:2310.18077</a> [<a href="/pdf/2310.18077" title="Download PDF">pdf</a>, <a href="/format/2310.18077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detrimental Contexts in Open-Domain Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+P">Philhoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For knowledge intensive NLP tasks, it has been widely accepted that accessing
more information is a contributing factor to improvements in the model's
end-to-end performance. However, counter-intuitively, too much context can have
a negative impact on the model when evaluated on common question answering (QA)
datasets. In this paper, we analyze how passages can have a detrimental effect
on retrieve-then-read architectures used in question answering. Our empirical
evidence indicates that the current read architecture does not fully leverage
the retrieved passages and significantly degrades its performance when using
the whole passages compared to utilizing subsets of them. Our findings
demonstrate that model accuracy can be improved by 10% on two popular QA
datasets by filtering out detrimental passages. Additionally, these outcomes
are attained by utilizing existing retrieval methods without further training
or data. We further highlight the challenges associated with identifying the
detrimental passages. First, even with the correct context, the model can make
an incorrect prediction, posing a challenge in determining which passages are
most influential. Second, evaluation typically considers lexical matching,
which is not robust to variations of correct answers. Despite these
limitations, our experimental results underscore the pivotal role of
identifying and removing these detrimental passages for the context-efficient
retrieve-then-read pipeline. Code and data are available at
https://github.com/xfactlab/emnlp2023-damaging-retrieval
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18079" title="Abstract">arXiv:2310.18079</a> [<a href="/pdf/2310.18079" title="Download PDF">pdf</a>, <a href="/format/2310.18079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supporting Better Insights of Data Science Pipelines with Fine-grained  Provenance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chapman%2C+A">Adriane Chapman</a>, 
<a href="/search/cs?searchtype=author&query=Lauro%2C+L">Luca Lauro</a>, 
<a href="/search/cs?searchtype=author&query=Missier%2C+P">Paolo Missier</a>, 
<a href="/search/cs?searchtype=author&query=Torlone%2C+R">Riccardo Torlone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 27 figures, submitted to a journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Successful data-driven science requires complex data engineering pipelines to
clean, transform, and alter data in preparation for machine learning, and
robust results can only be achieved when each step in the pipeline can be
justified, and its effect on the data explained. In this framework, our aim is
to provide data scientists with facilities to gain an in-depth understanding of
how each step in the pipeline affects the data, from the raw input to training
sets ready to be used for learning. Starting from an extensible set of data
preparation operators commonly used within a data science setting, in this work
we present a provenance management infrastructure for generating, storing, and
querying very granular accounts of data transformations, at the level of
individual elements within datasets whenever possible. Then, from the formal
definition of a core set of data science preprocessing operators, we derive a
provenance semantics embodied by a collection of templates expressed in PROV, a
standard model for data provenance. Using those templates as a reference, our
provenance generation algorithm generalises to any operator with observable
input/output pairs. We provide a prototype implementation of an
application-level provenance capture library to produce, in a semi-automatic
way, complete provenance documents that account for the entire pipeline. We
report on the ability of our implementations to capture provenance in real ML
benchmark pipelines and over TCP-DI synthetic data. We finally show how the
collected provenance can be used to answer a suite of provenance benchmark
queries that underpin some common pipeline inspection questions, as expressed
on the Data Science Stack Exchange.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18080" title="Abstract">arXiv:2310.18080</a> [<a href="/pdf/2310.18080" title="Download PDF">pdf</a>, <a href="/format/2310.18080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Potential of Probabilistic Embeddings in Self-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janiak%2C+D">Denis Janiak</a>, 
<a href="/search/cs?searchtype=author&query=Binkowski%2C+J">Jakub Binkowski</a>, 
<a href="/search/cs?searchtype=author&query=Bielak%2C+P">Piotr Bielak</a>, 
<a href="/search/cs?searchtype=author&query=Kajdanowicz%2C+T">Tomasz Kajdanowicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review by AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years, self-supervised learning has played a pivotal role in
advancing machine learning by allowing models to acquire meaningful
representations from unlabeled data. An intriguing research avenue involves
developing self-supervised models within an information-theoretic framework,
but many studies often deviate from the stochasticity assumptions made when
deriving their objectives. To gain deeper insights into this issue, we propose
to explicitly model the representation with stochastic embeddings and assess
their effects on performance, information compression and potential for
out-of-distribution detection. From an information-theoretic perspective, we
seek to investigate the impact of probabilistic modeling on the information
bottleneck, shedding light on a trade-off between compression and preservation
of information in both representation and loss space. Emphasizing the
importance of distinguishing between these two spaces, we demonstrate how
constraining one can affect the other, potentially leading to performance
degradation. Moreover, our findings suggest that introducing an additional
bottleneck in the loss space can significantly enhance the ability to detect
out-of-distribution examples, only leveraging either representation features or
the variance of their underlying distribution.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18085" title="Abstract">arXiv:2310.18085</a> [<a href="/pdf/2310.18085" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FPGA-Based Implicit-Explicit Real-time Simulation Solver for Railway  Wireless Power Transfer with Nonlinear Magnetic Coupling Components
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/eess?searchtype=author&query=Zeng%2C+Y">Yangbin Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+J">Jialin Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+K">Kainan Chen</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+W">Weicheng Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Z">Zhengming Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Railway Wireless Power Transfer (WPT) is a promising non-contact power supply
solution, but constructing prototypes for controller testing can be both costly
and unsafe. Real-time hardware-in-the-loop simulation is an effective and
secure testing tool, but simulating the dynamic charging process of railway WPT
systems is challenging due to the continuous changes in the nonlinear magnetic
coupling components. To address this challenge, we propose an FPGA-based
half-step implicit-explicit (IMEX) simulation solver. The proposed solver
adopts an IMEX algorithm to solve the piecewise linear and nonlinear parts of
the system separately, which enables FPGAs to solve nonlinear components while
achieving high numerical stability. Additionally, we divide a complete
integration step into two half-steps to reduce computational time delays. Our
proposed method offers a promising solution for the real-time simulation of
railway WPT systems. The novelty of our approach lies in the use of the IMEX
algorithm and the half-step integration method, which significantly improves
the accuracy and efficiency of the simulation. Our simulations and experiments
demonstrate the effectiveness and accuracy of the proposed solver, which
provides a new approach for simulating and optimizing railway WPT systems with
nonlinear magnetic coupling components.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18087" title="Abstract">arXiv:2310.18087</a> [<a href="/pdf/2310.18087" title="Download PDF">pdf</a>, <a href="/format/2310.18087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Chebyshev Confidence Guided Source-Free Domain Adaptation Framework  for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiesi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yanwu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xutao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinghua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Ting Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Source-free domain adaptation (SFDA) aims to adapt models trained on a
labeled source domain to an unlabeled target domain without the access to
source data. In medical imaging scenarios, the practical significance of SFDA
methods has been emphasized due to privacy concerns. Recent State-of-the-art
SFDA methods primarily rely on self-training based on pseudo-labels (PLs).
Unfortunately, PLs suffer from accuracy deterioration caused by domain shift,
and thus limit the effectiveness of the adaptation process. To address this
issue, we propose a Chebyshev confidence guided SFDA framework to accurately
assess the reliability of PLs and generate self-improving PLs for
self-training. The Chebyshev confidence is estimated by calculating probability
lower bound of the PL confidence, given the prediction and the corresponding
uncertainty. Leveraging the Chebyshev confidence, we introduce two
confidence-guided denoising methods: direct denoising and prototypical
denoising. Additionally, we propose a novel teacher-student joint training
scheme (TJTS) that incorporates a confidence weighting module to improve PLs
iteratively. The TJTS, in collaboration with the denoising methods, effectively
prevents the propagation of noise and enhances the accuracy of PLs. Extensive
experiments in diverse domain scenarios validate the effectiveness of our
proposed framework and establish its superiority over state-of-the-art SFDA
methods. Our paper contributes to the field of SFDA by providing a novel
approach for precisely estimating the reliability of pseudo-labels and a
framework for obtaining high-quality PLs, resulting in improved adaptation
performance.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18088" title="Abstract">arXiv:2310.18088</a> [<a href="/pdf/2310.18088" title="Download PDF">pdf</a>, <a href="/format/2310.18088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iRED: A disaggregated P4-AQM fully implemented in programmable data  plane hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Almeida%2C+L+C">Leandro C. de Almeida</a>, 
<a href="/search/cs?searchtype=author&query=Pasquini%2C+R">Rafael Pasquini</a>, 
<a href="/search/cs?searchtype=author&query=Papagianni%2C+C">Chrysa Papagianni</a>, 
<a href="/search/cs?searchtype=author&query=Verdi%2C+F+L">F&#xe1;bio L. Verdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint (TNSM under review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Routers employ queues to temporarily hold packets when the scheduler cannot
immediately process them. Congestion occurs when the arrival rate of packets
exceeds the processing capacity, leading to increased queueing delay. Over
time, Active Queue Management (AQM) strategies have focused on directly
draining packets from queues to alleviate congestion and reduce queuing delay.
On Programmable Data Plane (PDP) hardware, AQMs traditionally reside in the
Egress pipeline due to the availability of queue delay information there. We
argue that this approach wastes the router's resources because the dropped
packet has already consumed the entire pipeline of the device. In this work, we
propose ingress Random Early Detection (iRED), a more efficient approach that
addresses the Egress drop problem. iRED is a disaggregated P4-AQM fully
implemented in programmable data plane hardware and also supports Low Latency,
Low Loss, and Scalable Throughput (L4S) framework, saving device pipeline
resources by dropping packets in the Ingress block. To evaluate iRED, we
conducted three experiments using a Tofino2 programmable switch: i) An in-depth
analysis of state-of-the-art AQMs on PDP hardware, using 12 different network
configurations varying in bandwidth, Round-Trip Time (RTT), and Maximum
Transmission Unit (MTU). The results demonstrate that iRED can significantly
reduce router resource consumption, with up to a 10x reduction in memory usage,
12x fewer processing cycles, and 8x less power consumption for the same traffic
load; ii) A performance evaluation regarding the L4S framework. The results
prove that iRED achieves fairness in bandwidth usage for different types of
traffic (classic and scalable); iii) A comprehensive analysis of the QoS in a
real setup of a DASH) technology. iRED demonstrated up to a 2.34x improvement
in FPS and a 4.77x increase in the video player buffer fill.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18089" title="Abstract">arXiv:2310.18089</a> [<a href="/pdf/2310.18089" title="Download PDF">pdf</a>, <a href="/format/2310.18089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lost in Translation -- Multilingual Misinformation and its Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quelle%2C+D">Dorian Quelle</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Calvin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Bovet%2C+A">Alexandre Bovet</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S+A">Scott A. Hale</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Misinformation and disinformation are growing threats in the digital age,
spreading rapidly across languages and borders. This paper investigates the
prevalence and dynamics of multilingual misinformation through an analysis of
over 250,000 unique fact-checks spanning 95 languages. First, we find that
while the majority of misinformation claims are only fact-checked once, 11.7%,
corresponding to more than 21,000 claims, are checked multiple times. Using
fact-checks as a proxy for the spread of misinformation, we find 33% of
repeated claims cross linguistic boundaries, suggesting that some
misinformation permeates language barriers. However, spreading patterns exhibit
strong homophily, with misinformation more likely to spread within the same
language. To study the evolution of claims over time and mutations across
languages, we represent fact-checks with multilingual sentence embeddings and
cluster semantically similar claims. We analyze the connected components and
shortest paths connecting different versions of a claim finding that claims
gradually drift over time and undergo greater alteration when traversing
languages. Overall, this novel investigation of multilingual misinformation
provides key insights. It quantifies redundant fact-checking efforts,
establishes that some claims diffuse across languages, measures linguistic
homophily, and models the temporal and cross-lingual evolution of claims. The
findings advocate for expanded information sharing between fact-checkers
globally while underscoring the importance of localized verification.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18091" title="Abstract">arXiv:2310.18091</a> [<a href="/pdf/2310.18091" title="Download PDF">pdf</a>, <a href="/format/2310.18091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Anomaly Detection using Gaussian Priors and Nonlinear  Anomaly Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%BCer%2C+F">Fiete L&#xfc;er</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+T">Tobias Weber</a>, 
<a href="/search/cs?searchtype=author&query=Dolgich%2C+M">Maxim Dolgich</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hm%2C+C">Christian B&#xf6;hm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at AI4TS @ ICDMW 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Anomaly detection in imbalanced datasets is a frequent and crucial problem,
especially in the medical domain where retrieving and labeling irregularities
is often expensive. By combining the generative stability of a
$\beta$-variational autoencoder (VAE) with the discriminative strengths of
generative adversarial networks (GANs), we propose a novel model,
$\beta$-VAEGAN. We investigate methods for composing anomaly scores based on
the discriminative and reconstructive capabilities of our model. Existing work
focuses on linear combinations of these components to determine if data is
anomalous. We advance existing work by training a kernelized support vector
machine (SVM) on the respective error components to also consider nonlinear
relationships. This improves anomaly detection performance, while allowing
faster optimization. Lastly, we use the deviations from the Gaussian prior of
$\beta$-VAEGAN to form a novel anomaly score component. In comparison to
state-of-the-art work, we improve the $F_1$ score during anomaly detection from
0.85 to 0.92 on the widely used MITBIH Arrhythmia Database.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18098" title="Abstract">arXiv:2310.18098</a> [<a href="/pdf/2310.18098" title="Download PDF">pdf</a>, <a href="/format/2310.18098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the Gap: Automated Corpus Creation for Enthymeme Detection and  Reconstruction in Learner Arguments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stahl%2C+M">Maja Stahl</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCsterhus%2C+N">Nick D&#xfc;sterhus</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mei-Hua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wachsmuth%2C+H">Henning Wachsmuth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Writing strong arguments can be challenging for learners. It requires to
select and arrange multiple argumentative discourse units (ADUs) in a logical
and coherent way as well as to decide which ADUs to leave implicit, so called
enthymemes. However, when important ADUs are missing, readers might not be able
to follow the reasoning or understand the argument's main point. This paper
introduces two new tasks for learner arguments: to identify gaps in arguments
(enthymeme detection) and to fill such gaps (enthymeme reconstruction).
Approaches to both tasks may help learners improve their argument quality. We
study how corpora for these tasks can be created automatically by deleting ADUs
from an argumentative text that are central to the argument and its quality,
while maintaining the text's naturalness. Based on the ICLEv3 corpus of
argumentative learner essays, we create 40,089 argument instances for enthymeme
detection and reconstruction. Through manual studies, we provide evidence that
the proposed corpus creation process leads to the desired quality reduction,
and results in arguments that are similarly natural to those written by
learners. Finally, first baseline approaches to enthymeme detection and
reconstruction demonstrate the corpus' usefulness.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18099" title="Abstract">arXiv:2310.18099</a> [<a href="/pdf/2310.18099" title="Download PDF">pdf</a>, <a href="/ps/2310.18099" title="Download PostScript">ps</a>, <a href="/format/2310.18099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Acoustic Audience Feedback in Large Virtual Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aykut%2C+T">Tamay Aykut</a>, 
<a href="/search/cs?searchtype=author&query=Hofbauer%2C+M">Markus Hofbauer</a>, 
<a href="/search/cs?searchtype=author&query=Kuhn%2C+C">Christopher Kuhn</a>, 
<a href="/search/cs?searchtype=author&query=Steinbach%2C+E">Eckehard Steinbach</a>, 
<a href="/search/cs?searchtype=author&query=Girod%2C+B">Bernd Girod</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The COVID-19 pandemic shifted many events in our daily lives into the virtual
domain. While virtual conference systems provide an alternative to physical
meetings, larger events require a muted audience to avoid an accumulation of
background noise and distorted audio. However, performing artists strongly rely
on the feedback of their audience. We propose a concept for a virtual audience
framework which supports all participants with the ambience of a real audience.
Audience feedback is collected locally, allowing users to express enthusiasm or
discontent by selecting means such as clapping, whistling, booing, and
laughter. This feedback is sent as abstract information to a virtual audience
server. We broadcast the combined virtual audience feedback information to all
participants, which can be synthesized as a single acoustic feedback by the
client. The synthesis can be done by turning the collective audience feedback
into a prompt that is fed to state-of-the-art models such as AudioGen. This
way, each user hears a single acoustic feedback sound of the entire virtual
event, without requiring to unmute or risk hearing distorted, unsynchronized
feedback.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18100" title="Abstract">arXiv:2310.18100</a> [<a href="/pdf/2310.18100" title="Download PDF">pdf</a>, <a href="/format/2310.18100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of the Generalization Error of deep learning based on  Randomized Quasi-Monte Carlo for Solving Linear Kolmogorov PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xiao%2C+J">Jichang Xiao</a>, 
<a href="/search/math?searchtype=author&query=Fu%2C+F">Fengjiang Fu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xiaoqun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Deep learning algorithms have been widely used to solve linear Kolmogorov
partial differential equations~(PDEs) in high dimensions, where the loss
function is defined as a mathematical expectation. We propose to use the
randomized quasi-Monte Carlo (RQMC) method instead of the Monte Carlo (MC)
method for computing the loss function. In theory, we decompose the error from
empirical risk minimization~(ERM) into the generalization error and the
approximation error. Notably, the approximation error is independent of the
sampling methods. We prove that the convergence order of the mean
generalization error for the RQMC method is $O(n^{-1+\epsilon})$ for
arbitrarily small $\epsilon&gt;0$, while for the MC method it is
$O(n^{-1/2+\epsilon})$ for arbitrarily small $\epsilon&gt;0$. Consequently, we
find that the overall error for the RQMC method is asymptotically smaller than
that for the MC method as $n$ increases. Our numerical experiments show that
the algorithm based on the RQMC method consistently achieves smaller relative
$L^{2}$ error than that based on the MC method.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18103" title="Abstract">arXiv:2310.18103</a> [<a href="/pdf/2310.18103" title="Download PDF">pdf</a>, <a href="/format/2310.18103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Application of Polynomial Solvers in mmWave Analog Radio  Beamforming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhayani%2C+S">Snehal Bhayani</a>, 
<a href="/search/cs?searchtype=author&query=Susarla%2C+P">Praneeth Susarla</a>, 
<a href="/search/cs?searchtype=author&query=Bulusu%2C+S+S+K+C">S.S. Krishna Chaitanya Bulusu</a>, 
<a href="/search/cs?searchtype=author&query=Silven%2C+O">Olli Silven</a>, 
<a href="/search/cs?searchtype=author&query=Juntti%2C+M">Markku Juntti</a>, 
<a href="/search/cs?searchtype=author&query=Heikkila%2C+J">Janne Heikkila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the SIGSAM's ACM Communications in Computer Algebra, as an extended abstract
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>

</div>
<p class="mathjax">Beamforming is a signal processing technique where an array of antenna
elements can be steered to transmit and receive radio signals in a specific
direction. The usage of millimeter wave (mmWave) frequencies and multiple input
multiple output (MIMO) beamforming are considered as the key innovations of 5th
Generation (5G) and beyond communication systems. The technique initially
performs a beam alignment procedure, followed by data transfer in the aligned
directions between the transmitter and the receiver. Traditionally, beam
alignment involves periodical and exhaustive beam sweeping at both transmitter
and the receiver, which is a slow process causing extra communication overhead
with MIMO and massive MIMO radio units. In applications such as beam tracking,
angular velocity, beam steering etc., the beam alignment procedure is optimized
by estimating the beam directions using first order polynomial approximations.
Recent learning-based SOTA strategies for fast mmWave beam alignment also
require exploration over exhaustive beam pairs during the training procedure,
causing overhead to learning strategies for higher antenna configurations. In
this work, we first optimize the beam alignment cost functions e.g. the data
rate, to reduce the beam sweeping overhead by applying polynomial
approximations of its partial derivatives which can then be solved as a system
of polynomial equations using well-known tools from algebraic geometry. At this
point, a question arises: 'what is a good polynomial approximation?' In this
work, we attempt to obtain a 'good polynomial approximation'. Preliminary
experiments indicate that our estimated polynomial approximations attain a
so-called sweet-spot in terms of the solver speed and accuracy, when evaluated
on test beamforming problems.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18104" title="Abstract">arXiv:2310.18104</a> [<a href="/pdf/2310.18104" title="Download PDF">pdf</a>, <a href="/format/2310.18104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classifier-head Informed Feature Masking and Prototype-based Logit  Smoothing for Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhuohao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yiqiao Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhijun Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Weishi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruixuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Out-of-distribution (OOD) detection is essential when deploying neural
networks in the real world. One main challenge is that neural networks often
make overconfident predictions on OOD data. In this study, we propose an
effective post-hoc OOD detection method based on a new feature masking strategy
and a novel logit smoothing strategy. Feature masking determines the important
features at the penultimate layer for each in-distribution (ID) class based on
the weights of the ID class in the classifier head and masks the rest features.
Logit smoothing computes the cosine similarity between the feature vector of
the test sample and the prototype of the predicted ID class at the penultimate
layer and uses the similarity as an adaptive temperature factor on the logit to
alleviate the network's overconfidence prediction for OOD data. With these
strategies, we can reduce feature activation of OOD data and enlarge the gap in
OOD score between ID and OOD data. Extensive experiments on multiple standard
OOD detection benchmarks demonstrate the effectiveness of our method and its
compatibility with existing methods, with new state-of-the-art performance
achieved from our method. The source code will be released publicly.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18112" title="Abstract">arXiv:2310.18112</a> [<a href="/pdf/2310.18112" title="Download PDF">pdf</a>, <a href="/format/2310.18112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> er.autopilot 1.0: The Full Autonomous Stack for Oval Racing at High  Speeds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raji%2C+A">Ayoub Raji</a>, 
<a href="/search/cs?searchtype=author&query=Caporale%2C+D">Danilo Caporale</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+F">Francesco Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Giove%2C+A">Andrea Giove</a>, 
<a href="/search/cs?searchtype=author&query=Verucchi%2C+M">Micaela Verucchi</a>, 
<a href="/search/cs?searchtype=author&query=Malatesta%2C+D">Davide Malatesta</a>, 
<a href="/search/cs?searchtype=author&query=Musiu%2C+N">Nicola Musiu</a>, 
<a href="/search/cs?searchtype=author&query=Toschi%2C+A">Alessandro Toschi</a>, 
<a href="/search/cs?searchtype=author&query=Popitanu%2C+S+R">Silviu Roberto Popitanu</a>, 
<a href="/search/cs?searchtype=author&query=Bagni%2C+F">Fabio Bagni</a>, 
<a href="/search/cs?searchtype=author&query=Bosi%2C+M">Massimiliano Bosi</a>, 
<a href="/search/cs?searchtype=author&query=Liniger%2C+A">Alexander Liniger</a>, 
<a href="/search/cs?searchtype=author&query=Bertogna%2C+M">Marko Bertogna</a>, 
<a href="/search/cs?searchtype=author&query=Morra%2C+D">Daniele Morra</a>, 
<a href="/search/cs?searchtype=author&query=Amerotti%2C+F">Francesco Amerotti</a>, 
<a href="/search/cs?searchtype=author&query=Bartoli%2C+L">Luca Bartoli</a>, 
<a href="/search/cs?searchtype=author&query=Martello%2C+F">Federico Martello</a>, 
<a href="/search/cs?searchtype=author&query=Porta%2C+R">Riccardo Porta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint: Accepted to Field Robotics "Opportunities and Challenges with Autonomous Racing" Special Issue
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)

</div>
<p class="mathjax">The Indy Autonomous Challenge (IAC) brought together for the first time in
history nine autonomous racing teams competing at unprecedented speed and in
head-to-head scenario, using independently developed software on open-wheel
racecars. This paper presents the complete software architecture used by team
TII EuroRacing (TII-ER), covering all the modules needed to avoid static
obstacles, perform active overtakes and reach speeds above 75 m/s (270 km/h).
In addition to the most common modules related to perception, planning, and
control, we discuss the approaches used for vehicle dynamics modelling,
simulation, telemetry, and safety. Overall results and the performance of each
module are described, as well as the lessons learned during the first two
events of the competition on oval tracks, where the team placed respectively
second and third.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18116" title="Abstract">arXiv:2310.18116</a> [<a href="/pdf/2310.18116" title="Download PDF">pdf</a>, <a href="/format/2310.18116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Unsupervised Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salmon%2C+B">Benjamin Salmon</a>, 
<a href="/search/cs?searchtype=author&query=Krull%2C+A">Alexander Krull</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV) Workshops, 2023, pp. 3838-3845
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Traditional supervised denoisers are trained using pairs of noisy input and
clean target images. They learn to predict a central tendency of the posterior
distribution over possible clean images. When, e.g., trained with the popular
quadratic loss function, the network's output will correspond to the minimum
mean square error (MMSE) estimate. Unsupervised denoisers based on Variational
AutoEncoders (VAEs) have succeeded in achieving state-of-the-art results while
requiring only unpaired noisy data as training input. In contrast to the
traditional supervised approach, unsupervised denoisers do not directly produce
a single prediction, such as the MMSE estimate, but allow us to draw samples
from the posterior distribution of clean solutions corresponding to the noisy
input. To approximate the MMSE estimate during inference, unsupervised methods
have to create and draw a large number of samples - a computationally expensive
process - rendering the approach inapplicable in many situations. Here, we
present an alternative approach that trains a deterministic network alongside
the VAE to directly predict a central tendency. Our method achieves results
that surpass the results achieved by the unsupervised method at a fraction of
the computational cost.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18117" title="Abstract">arXiv:2310.18117</a> [<a href="/pdf/2310.18117" title="Download PDF">pdf</a>, <a href="/format/2310.18117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do we need scan-matching in radar odometry?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kubelka%2C+V">Vladim&#xed;r Kubelka</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+E">Emil Fritz</a>, 
<a href="/search/cs?searchtype=author&query=Magnusson%2C+M">Martin Magnusson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Submitted to ICRA 2024. 7 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">There is a current increase in the development of "4D" Doppler-capable radar
and lidar range sensors that produce 3D point clouds where all points also have
information about the radial velocity relative to the sensor. 4D radars in
particular are interesting for object perception and navigation in
low-visibility conditions (dust, smoke) where lidars and cameras typically
fail. With the advent of high-resolution Doppler-capable radars comes the
possibility of estimating odometry from single point clouds, foregoing the need
for scan registration which is error-prone in feature-sparse field
environments. We compare several odometry estimation methods, from direct
integration of Doppler/IMU data and Kalman filter sensor fusion to 3D
scan-to-scan and scan-to-map registration, on three datasets with data from two
recent 4D radars and two IMUs. Surprisingly, our results show that the odometry
from Doppler and IMU data alone give similar or better results than 3D point
cloud registration. In our experiments, the average position error can be as
low as 0.3% over 1.8 and 4.5km trajectories. That allows accurate estimation of
6DOF ego-motion over long distances also in feature-sparse mine environments.
These results are useful not least for applications of navigation with
resource-constrained robot platforms in feature-sparse and low-visibility
conditions such as mining, construction, and search &amp; rescue operations.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18118" title="Abstract">arXiv:2310.18118</a> [<a href="/pdf/2310.18118" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Global Multi-Unit Calibration as a Method for Large Scale IoT  Particulate Matter Monitoring Systems Deployments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Vito%2C+S">Saverio De Vito</a>, 
<a href="/search/cs?searchtype=author&query=Elia%2C+G+D">Gerardo D Elia</a>, 
<a href="/search/cs?searchtype=author&query=Ferlito%2C+S">Sergio Ferlito</a>, 
<a href="/search/cs?searchtype=author&query=Di+Francia%2C+G">Girolamo Di Francia</a>, 
<a href="/search/cs?searchtype=author&query=Davidovic%2C+M">Milos Davidovic</a>, 
<a href="/search/cs?searchtype=author&query=Kleut%2C+D">Duska Kleut</a>, 
<a href="/search/cs?searchtype=author&query=Stojanovic%2C+D">Danka Stojanovic</a>, 
<a href="/search/cs?searchtype=author&query=Stojanovic%2C+M+J">Milena Jovasevic Stojanovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Scalable and effective calibration is a fundamental requirement for Low Cost
Air Quality Monitoring Systems and will enable accurate and pervasive
monitoring in cities. Suffering from environmental interferences and
fabrication variance, these devices need to encompass sensors specific and
complex calibration processes for reaching a sufficient accuracy to be deployed
as indicative measurement devices in Air Quality (AQ) monitoring networks.
Concept and sensor drift often force calibration process to be frequently
repeated. These issues lead to unbearable calibration costs which denies their
massive deployment when accuracy is a concern. In this work, We propose a zero
transfer samples, global calibration methodology as a technological enabler for
IoT AQ multisensory devices which relies on low cost Particulate Matter (PM)
sensors. This methodology is based on field recorded responses from a limited
number of IoT AQ multisensors units and machine learning concepts and can be
universally applied to all units of the same type. A multi season test campaign
shown that, when applied to different sensors, this methodology performances
match those of state of the art methodology which requires to derive different
calibration parameters for each different unit. If confirmed, these results
show that, when properly derived, a global calibration law can be exploited for
a large number of networked devices with dramatic cost reduction eventually
allowing massive deployment of accurate IoT AQ monitoring devices. Furthermore,
this calibration model could be easily embedded on board of the device or
implemented on the edge allowing immediate access to accurate readings for
personal exposure monitor applications as well as reducing long range data
transfer needs.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18119" title="Abstract">arXiv:2310.18119</a> [<a href="/pdf/2310.18119" title="Download PDF">pdf</a>, <a href="/format/2310.18119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Unified Conversational Recommendation System: Multi-task  Learning via Contextualized Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+Y">Yeongseo Jung</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+E">Eunseo Jung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In Conversational Recommendation System (CRS), an agent is asked to recommend
a set of items to users within natural language conversations. To address the
need for both conversational capability and personalized recommendations, prior
works have utilized separate recommendation and dialogue modules. However, such
approach inevitably results in a discrepancy between recommendation results and
generated responses. To bridge the gap, we propose a multi-task learning for a
unified CRS, where a single model jointly learns both tasks via Contextualized
Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate
and soft gate. The former selectively gates between two task-specific teachers,
while the latter integrates knowledge from both teachers. Our gates are
computed on-the-fly in a context-specific manner, facilitating flexible
integration of relevant knowledge. Extensive experiments demonstrate that our
single model significantly improves recommendation performance while enhancing
fluency, and achieves comparable results in terms of diversity.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18122" title="Abstract">arXiv:2310.18122</a> [<a href="/pdf/2310.18122" title="Download PDF">pdf</a>, <a href="/format/2310.18122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuchen Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint, 19 pages, 4 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Opinion summarization sets itself apart from other types of summarization
tasks due to its distinctive focus on aspects and sentiments. Although certain
automated evaluation methods like ROUGE have gained popularity, we have found
them to be unreliable measures for assessing the quality of opinion summaries.
In this paper, we present OpinSummEval, a dataset comprising human judgments
and outputs from 14 opinion summarization models. We further explore the
correlation between 24 automatic metrics and human ratings across four
dimensions. Our findings indicate that metrics based on neural networks
generally outperform non-neural ones. However, even metrics built on powerful
backbones, such as BART and GPT-3/3.5, do not consistently correlate well
across all dimensions, highlighting the need for advancements in automated
evaluation methods for opinion summarization. The code and data are publicly
available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18123" title="Abstract">arXiv:2310.18123</a> [<a href="/pdf/2310.18123" title="Download PDF">pdf</a>, <a href="/ps/2310.18123" title="Download PostScript">ps</a>, <a href="/format/2310.18123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample Complexity Bounds for Score-Matching: Causal Discovery and  Generative Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhenyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper provides statistical sample complexity bounds for score-matching
and its applications in causal discovery. We demonstrate that accurate
estimation of the score function is achievable by training a standard deep ReLU
neural network using stochastic gradient descent. We establish bounds on the
error rate of recovering causal relationships using the score-matching-based
causal discovery method of Rolland et al. [2022], assuming a sufficiently good
estimation of the score function. Finally, we analyze the upper bound of
score-matching estimation within the score-based generative modeling, which has
been applied for causal discovery but is also of independent interest within
the domain of generative models.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18127" title="Abstract">arXiv:2310.18127</a> [<a href="/pdf/2310.18127" title="Download PDF">pdf</a>, <a href="/format/2310.18127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask more, know better: Reinforce-Learned Prompt Questions for Decision  Making with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xue Yan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yan Song</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xinyu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Christianos%2C+F">Filippos Christianos</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haifeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mguni%2C+D+H">David Henry Mguni</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) demonstrate their promise in tackling
complicated practical challenges by combining action-based policies with chain
of thought (CoT) reasoning. Having high-quality prompts on hand, however, is
vital to the framework's effectiveness. Currently, these prompts are
handcrafted utilizing extensive human labor, resulting in CoT policies that
frequently fail to generalize. Human intervention is also required in order to
develop grounding functions that ensure low-level controllers appropriately
process CoT reasoning. In this paper, we take the first step towards a fully
integrated end-to-end framework for task-solving in real settings employing
complicated reasoning. To that purpose, we offer a new leader-follower bilevel
framework capable of learning to ask relevant questions (prompts) and
subsequently undertaking reasoning to guide the learning of actions to be
performed in an environment. A good prompt should make introspective revisions
based on historical findings, leading the CoT to consider the anticipated
goals. A prompt-generator policy has its own aim in our system, allowing it to
adapt to the action policy and automatically root the CoT process towards
outputs that lead to decisive, high-performing actions. Meanwhile, the action
policy is learning how to use the CoT outputs to take specific actions. Our
empirical data reveal that our system outperforms leading methods in agent
learning benchmarks such as Overcooked and FourRoom.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18128" title="Abstract">arXiv:2310.18128</a> [<a href="/pdf/2310.18128" title="Download PDF">pdf</a>, <a href="/format/2310.18128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Dynamic Time Warping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bringmann%2C+K">Karl Bringmann</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+N">Nick Fischer</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Kipouridis%2C+E">Evangelos Kipouridis</a>, 
<a href="/search/cs?searchtype=author&query=Kociumaka%2C+T">Tomasz Kociumaka</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SODA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">The Dynamic Time Warping (DTW) distance is a popular similarity measure for
polygonal curves (i.e., sequences of points). It finds many theoretical and
practical applications, especially for temporal data, and is known to be a
robust, outlier-insensitive alternative to the Fr\'echet distance. For static
curves of at most $n$ points, the DTW distance can be computed in $O(n^2)$ time
in constant dimension. This tightly matches a SETH-based lower bound, even for
curves in $\mathbb{R}^1$.
<br />We study dynamic algorithms for the DTW distance. We accommodate local
changes to one or both curves, such as inserting or deleting vertices and,
after each operation, can report the updated DTW distance. We give such a data
structure with update and query time $O(n^{1.5} \log n)$, where $n$ is the
maximum length of the curves. The natural follow-up question is whether this
time bound can be improved; under the aforementioned SETH-based lower bound, we
could even hope for linear update time.
<br />We refute these hopes and prove that our data structure is conditionally
optimal, up to subpolynomial factors. More precisely, we prove that, already
for curves in $\mathbb{R}^1$, there is no dynamic algorithm to maintain the DTW
distance with update and query time $O(n^{1.5 - \delta})$ for any
constant~$\delta &gt; 0$, unless the Negative-$k$-Clique Hypothesis fails. This
holds even if one of the curves is fixed at all times, whereas the points of
the other curve may only undergo substitutions. In fact, we give matching upper
and lower bounds for various further trade-offs between update and query time,
even in cases where the lengths of the curves differ. The Negative-$k$-Clique
Hypothesis is a recent but well-established hypothesis from fine-grained
complexity, that generalizes the famous APSP Hypothesis, and successfully led
to several lower bounds.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18130" title="Abstract">arXiv:2310.18130</a> [<a href="/pdf/2310.18130" title="Download PDF">pdf</a>, <a href="/format/2310.18130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DELPHI: Data for Evaluating LLMs&#x27; Performance in Handling Controversial  Issues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+D+Q">David Q. Sun</a>, 
<a href="/search/cs?searchtype=author&query=Abzaliev%2C+A">Artem Abzaliev</a>, 
<a href="/search/cs?searchtype=author&query=Kotek%2C+H">Hadas Kotek</a>, 
<a href="/search/cs?searchtype=author&query=Xiu%2C+Z">Zidi Xiu</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+C">Christopher Klein</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J+D">Jason D. Williams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP Industry Track 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Controversy is a reflection of our zeitgeist, and an important aspect to any
discourse. The rise of large language models (LLMs) as conversational systems
has increased public reliance on these systems for answers to their various
questions. Consequently, it is crucial to systematically examine how these
models respond to questions that pertaining to ongoing debates. However, few
such datasets exist in providing human-annotated labels reflecting the
contemporary discussions. To foster research in this area, we propose a novel
construction of a controversial questions dataset, expanding upon the publicly
released Quora Question Pairs Dataset. This dataset presents challenges
concerning knowledge recency, safety, fairness, and bias. We evaluate different
LLMs using a subset of this dataset, illuminating how they handle controversial
issues and the stances they adopt. This research ultimately contributes to our
understanding of LLMs' interaction with controversial issues, paving the way
for improvements in their comprehension and handling of complex societal
debates.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18131" title="Abstract">arXiv:2310.18131</a> [<a href="/pdf/2310.18131" title="Download PDF">pdf</a>, <a href="/format/2310.18131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Video Gaze Estimation via Capturing Head-face-eye  Spatial-temporal Interaction Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yiran Guan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuoguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenzheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yang Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this letter, we propose a new method, Multi-Clue Gaze (MCGaze), to
facilitate video gaze estimation via capturing spatial-temporal interaction
context among head, face, and eye in an end-to-end learning way, which has not
been well concerned yet. The main advantage of MCGaze is that the tasks of clue
localization of head, face, and eye can be solved jointly for gaze estimation
in a one-step way, with joint optimization to seek optimal performance. During
this, spatial-temporal context exchange happens among the clues on the head,
face, and eye. Accordingly, the final gazes obtained by fusing features from
various queries can be aware of global clues from heads and faces, and local
clues from eyes simultaneously, which essentially leverages performance.
Meanwhile, the one-step running way also ensures high running efficiency.
Experiments on the challenging Gaze360 dataset verify the superiority of our
proposition. The source code will be released at
https://github.com/zgchen33/MCGaze.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18141" title="Abstract">arXiv:2310.18141</a> [<a href="/pdf/2310.18141" title="Download PDF">pdf</a>, <a href="/format/2310.18141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Representation Learning for Diverse Deformable Shape  Collections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahner%2C+S">Sara Hahner</a>, 
<a href="/search/cs?searchtype=author&query=Attaiki%2C+S">Souhaib Attaiki</a>, 
<a href="/search/cs?searchtype=author&query=Garcke%2C+J">Jochen Garcke</a>, 
<a href="/search/cs?searchtype=author&query=Ovsjanikov%2C+M">Maks Ovsjanikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at International Conference on 3D Vision 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a novel learning-based method for encoding and manipulating 3D
surface meshes. Our method is specifically designed to create an interpretable
embedding space for deformable shape collections. Unlike previous 3D mesh
autoencoders that require meshes to be in a 1-to-1 correspondence, our approach
is trained on diverse meshes in an unsupervised manner. Central to our method
is a spectral pooling technique that establishes a universal latent space,
breaking free from traditional constraints of mesh connectivity and shape
categories. The entire process consists of two stages. In the first stage, we
employ the functional map paradigm to extract point-to-point (p2p) maps between
a collection of shapes in an unsupervised manner. These p2p maps are then
utilized to construct a common latent space, which ensures straightforward
interpretation and independence from mesh connectivity and shape category.
Through extensive experiments, we demonstrate that our method achieves
excellent reconstructions and produces more realistic and smoother
interpolations than baseline approaches.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18142" title="Abstract">arXiv:2310.18142</a> [<a href="/pdf/2310.18142" title="Download PDF">pdf</a>, <a href="/format/2310.18142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Panoptic Narrative Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Danni Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiayi Ji</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yiwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite considerable progress, the advancement of Panoptic Narrative
Grounding (PNG) remains hindered by costly annotations. In this paper, we
introduce a novel Semi-Supervised Panoptic Narrative Grounding (SS-PNG)
learning scheme, capitalizing on a smaller set of labeled image-text pairs and
a larger set of unlabeled pairs to achieve competitive performance. Unlike
visual segmentation tasks, PNG involves one pixel belonging to multiple
open-ended nouns. As a result, existing multi-class based semi-supervised
segmentation frameworks cannot be directly applied to this task. To address
this challenge, we first develop a novel SS-PNG Network (SS-PNG-NW) tailored to
the SS-PNG setting. We thoroughly investigate strategies such as Burn-In and
data augmentation to determine the optimal generic configuration for the
SS-PNG-NW. Additionally, to tackle the issue of imbalanced pseudo-label
quality, we propose a Quality-Based Loss Adjustment (QLA) approach to adjust
the semi-supervised objective, resulting in an enhanced SS-PNG-NW+. Employing
our proposed QLA, we improve BCE Loss and Dice loss at pixel and mask levels,
respectively. We conduct extensive experiments on PNG datasets, with our
SS-PNG-NW+ demonstrating promising results comparable to fully-supervised
models across all data ratios. Remarkably, our SS-PNG-NW+ outperforms
fully-supervised models with only 30% and 50% supervision data, exceeding their
performance by 0.8% and 1.1% respectively. This highlights the effectiveness of
our proposed SS-PNG-NW+ in overcoming the challenges posed by limited
annotations and enhancing the applicability of PNG tasks. The source code is
available at https://github.com/nini0919/SSPNG.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18144" title="Abstract">arXiv:2310.18144</a> [<a href="/pdf/2310.18144" title="Download PDF">pdf</a>, <a href="/format/2310.18144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Intrinsic Exploration by Creating Stationary Objectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castanyer%2C+R+C">Roger Creus Castanyer</a>, 
<a href="/search/cs?searchtype=author&query=Romoff%2C+J">Joshua Romoff</a>, 
<a href="/search/cs?searchtype=author&query=Berseth%2C+G">Glen Berseth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Exploration bonuses in reinforcement learning guide long-horizon exploration
by defining custom intrinsic objectives. Count-based methods use the frequency
of state visits to derive an exploration bonus. In this paper, we identify that
any intrinsic reward function derived from count-based methods is
non-stationary and hence induces a difficult objective to optimize for the
agent. The key contribution of our work lies in transforming the original
non-stationary rewards into stationary rewards through an augmented state
representation. For this purpose, we introduce the Stationary Objectives For
Exploration (SOFE) framework. SOFE requires identifying sufficient statistics
for different exploration bonuses and finding an efficient encoding of these
statistics to use as input to a deep network. SOFE is based on proposing state
augmentations that expand the state space but hold the promise of simplifying
the optimization of the agent's objective. Our experiments show that SOFE
improves the agents' performance in challenging exploration problems, including
sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally
generated environments.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18146" title="Abstract">arXiv:2310.18146</a> [<a href="/pdf/2310.18146" title="Download PDF">pdf</a>, <a href="/format/2310.18146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Out-Orientations with Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chekuri%2C+C">Chandra Chekuri</a>, 
<a href="/search/cs?searchtype=author&query=Christiansen%2C+A+B">Aleksander Bj&#xf8;rn Christiansen</a>, 
<a href="/search/cs?searchtype=author&query=Holm%2C+J">Jacob Holm</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Hoog%2C+I">Ivor van der Hoog</a>, 
<a href="/search/cs?searchtype=author&query=Quanrud%2C+K">Kent Quanrud</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>, 
<a href="/search/cs?searchtype=author&query=Schwiegelshohn%2C+C">Chris Schwiegelshohn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SODA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We give improved algorithms for maintaining edge-orientations of a
fully-dynamic graph, such that the maximum out-degree is bounded. On one hand,
we show how to orient the edges such that maximum out-degree is proportional to
the arboricity $\alpha$ of the graph, in, either, an amortised update time of
$O(\log^2 n \log \alpha)$, or a worst-case update time of $O(\log^3 n \log
\alpha)$. On the other hand, motivated by applications including dynamic
maximal matching, we obtain a different trade-off. Namely, the improved update
time of either $O(\log n \log \alpha)$, amortised, or $O(\log ^2 n \log
\alpha)$, worst-case, for the problem of maintaining an edge-orientation with
at most $O(\alpha + \log n)$ out-edges per vertex. Finally, all of our
algorithms naturally limit the recourse to be polylogarithmic in $n$ and
$\alpha$. Our algorithms adapt to the current arboricity of the graph.
Moreover, further analysis shows that they can yield a $(1 +
\varepsilon)$-approximation of the arboricity or the subgraph density at the
cost of increased update time.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18148" title="Abstract">arXiv:2310.18148</a> [<a href="/pdf/2310.18148" title="Download PDF">pdf</a>, <a href="/format/2310.18148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reality3DSketch: Rapid 3D Modeling of Objects from Single Freehand  Sketches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianrun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Chaotao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lanyun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+Y">Ying Zang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yiyi Liao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zejian Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingyun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on MultiMedia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The emerging trend of AR/VR places great demands on 3D content. However, most
existing software requires expertise and is difficult for novice users to use.
In this paper, we aim to create sketch-based modeling tools for user-friendly
3D modeling. We introduce Reality3DSketch with a novel application of an
immersive 3D modeling experience, in which a user can capture the surrounding
scene using a monocular RGB camera and can draw a single sketch of an object in
the real-time reconstructed 3D scene. A 3D object is generated and placed in
the desired location, enabled by our novel neural network with the input of a
single sketch. Our neural network can predict the pose of a drawing and can
turn a single sketch into a 3D model with view and structural awareness, which
addresses the challenge of sparse sketch input and view ambiguity. We conducted
extensive experiments synthetic and real-world datasets and achieved
state-of-the-art (SOTA) results in both sketch view estimation and 3D modeling
performance. According to our user study, our method of performing 3D modeling
in a scene is $&gt;$5x faster than conventional methods. Users are also more
satisfied with the generated 3D model than the results of existing methods.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18149" title="Abstract">arXiv:2310.18149</a> [<a href="/pdf/2310.18149" title="Download PDF">pdf</a>, <a href="/format/2310.18149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game of arrivals at a two queue network with heterogeneous customer  routes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bandyopadhyay%2C+A">Agniv Bandyopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Juneja%2C+S">Sandeep Juneja</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">We consider a queuing network that opens at a specified time, where customers
are non-atomic and belong to different classes. Each class has its own route,
and as is typical in the literature, the costs are a linear function of waiting
and service completion time. We restrict ourselves to a two class, two queue
network: this simplification is well motivated as the diversity in solution
structure as a function of problem parameters is substantial even in this
simple setting (e.g., a specific routing structure involves eight different
regimes), suggesting a combinatorial blow up as the number of queues, routes
and customer classes increase. We identify the unique Nash equilibrium customer
arrival profile when the customer linear cost preferences are different. This
profile is a function of problem parameters including the size of each class,
service rates at each queue, and customer cost preferences. When customer cost
preferences match, under certain parametric settings, the equilibrium arrival
profiles may not be unique and may lie in a convex set. We further make a
surprising observation that in some parametric settings, customers in one class
may arrive in disjoint intervals. Further, the two classes may arrive in
contiguous intervals or in overlapping intervals, and at varying rates within
an interval, depending upon the problem parameters.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18150" title="Abstract">arXiv:2310.18150</a> [<a href="/pdf/2310.18150" title="Download PDF">pdf</a>, <a href="/format/2310.18150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-Triggered Consensus for Continuous-Time Distributed Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Perez-Salesa%2C+I">Irene Perez-Salesa</a>, 
<a href="/search/eess?searchtype=author&query=Aldana-Lopez%2C+R">Rodrigo Aldana-Lopez</a>, 
<a href="/search/eess?searchtype=author&query=Sagues%2C+C">Carlos Sagues</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted for presentation at the 22nd IFAC World Congress 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Distributed sensor networks have gained interest thanks to the developments
in processing power and communications. Event-triggering mechanisms can be
useful in reducing communication between the nodes of the network, while still
ensuring an adequate behaviour of the system. However, very little attention
has been given to continuous-time systems in this context. In this work, we
propose a strategy for distributed state estimation in sensor networks, based
on average dynamic consensus of the continuous measurements. While
communication between nodes is discrete and heavily reduced due to the
event-triggering mechanism, our method ensures that the nodes are still able to
produce a continuous estimate of the global average measurement and the state
of the plant, within some tuneable error bounds.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18151" title="Abstract">arXiv:2310.18151</a> [<a href="/pdf/2310.18151" title="Download PDF">pdf</a>, <a href="/format/2310.18151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Traffic smoothing using explicit local controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hayat%2C+A">Amaury Hayat</a>, 
<a href="/search/eess?searchtype=author&query=Alanqary%2C+A">Arwa Alanqary</a>, 
<a href="/search/eess?searchtype=author&query=Bhadani%2C+R">Rahul Bhadani</a>, 
<a href="/search/eess?searchtype=author&query=Denaro%2C+C">Christopher Denaro</a>, 
<a href="/search/eess?searchtype=author&query=Weightman%2C+R+J">Ryan J. Weightman</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+S">Shengquan Xiang</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+W">Jonathan W. Lee</a>, 
<a href="/search/eess?searchtype=author&query=Bunting%2C+M">Matthew Bunting</a>, 
<a href="/search/eess?searchtype=author&query=Gollakota%2C+A">Anish Gollakota</a>, 
<a href="/search/eess?searchtype=author&query=Nice%2C+M+W">Matthew W. Nice</a>, 
<a href="/search/eess?searchtype=author&query=Gloudemans%2C+D">Derek Gloudemans</a>, 
<a href="/search/eess?searchtype=author&query=Zachar%2C+G">Gergely Zachar</a>, 
<a href="/search/eess?searchtype=author&query=Davis%2C+J+F">Jon F. Davis</a>, 
<a href="/search/eess?searchtype=author&query=Monache%2C+M+L+D">Maria Laura Delle Monache</a>, 
<a href="/search/eess?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>, 
<a href="/search/eess?searchtype=author&query=Bayen%2C+A+M">Alexandre M. Bayen</a>, 
<a href="/search/eess?searchtype=author&query=Sprinkle%2C+J">Jonathan Sprinkle</a>, 
<a href="/search/eess?searchtype=author&query=Work%2C+D+B">Daniel B. Work</a>, 
<a href="/search/eess?searchtype=author&query=Piccoli%2C+B">Benedetto Piccoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 1 Table , 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The dissipation of stop-and-go waves attracted recent attention as a traffic
management problem, which can be efficiently addressed by automated driving. As
part of the 100 automated vehicles experiment named MegaVanderTest, feedback
controls were used to induce strong dissipation via velocity smoothing. More
precisely, a single vehicle driving differently in one of the four lanes of
I-24 in the Nashville area was able to regularize the velocity profile by
reducing oscillations in time and velocity differences among vehicles.
Quantitative measures of this effect were possible due to the innovative I-24
MOTION system capable of monitoring the traffic conditions for all vehicles on
the roadway. This paper presents the control design, the technological aspects
involved in its deployment, and, finally, the results achieved by the
experiment.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18152" title="Abstract">arXiv:2310.18152</a> [<a href="/pdf/2310.18152" title="Download PDF">pdf</a>, <a href="/format/2310.18152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangled Representation Learning with Large Language Models for  Text-Attributed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yijian Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenwu Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs
such as citation networks, e-commerce networks and social networks has
attracted considerable attention in the web community. Recently, large language
models (LLMs) have demonstrated exceptional capabilities across a wide range of
tasks. However, the existing works focus on harnessing the potential of LLMs
solely relying on prompts to convey graph structure information to LLMs, thus
suffering from insufficient understanding of the complex structural
relationships within TAGs. To address this problem, in this paper we present
the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the
reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model
incorporates graph structure information through tailored disentangled graph
neural network (GNN) layers, enabling LLMs to capture the intricate
relationships hidden in text-attributed graphs from multiple structural
factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing
computational costs and allowing much more flexibility in combining with
different LLM models. Experimental evaluations demonstrate the effectiveness of
the proposed DGTL model on achieving superior or comparable performance over
state-of-the-art baselines. Additionally, we also demonstrate that our DGTL
model can offer natural language explanations for predictions, thereby
significantly enhancing model interpretability.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18155" title="Abstract">arXiv:2310.18155</a> [<a href="/pdf/2310.18155" title="Download PDF">pdf</a>, <a href="/format/2310.18155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elevating Code-mixed Text Handling through Auditory Information of Words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mamta">Mamta</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+Z">Zishan Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Ekbal%2C+A">Asif Ekbal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the growing popularity of code-mixed data, there is an increasing need
for better handling of this type of data, which poses a number of challenges,
such as dealing with spelling variations, multiple languages, different
scripts, and a lack of resources. Current language models face difficulty in
effectively handling code-mixed data as they primarily focus on the semantic
representation of words and ignore the auditory phonetic features. This leads
to difficulties in handling spelling variations in code-mixed text. In this
paper, we propose an effective approach for creating language models for
handling code-mixed textual data using auditory information of words from
SOUNDEX. Our approach includes a pre-training step based on
masked-language-modelling, which includes SOUNDEX representations (SAMLM) and a
new method of providing input data to the pre-trained model. Through
experimentation on various code-mixed datasets (of different languages) for
sentiment, offensive and aggression classification tasks, we establish that our
novel language modeling approach (SAMLM) results in improved robustness towards
adversarial attacks on code-mixed classification tasks. Additionally, our SAMLM
based approach also results in better classification results over the popular
baselines for code-mixed tasks. We use the explainability technique, SHAP
(SHapley Additive exPlanations) to explain how the auditory features
incorporated through SAMLM assist the model to handle the code-mixed text
effectively and increase robustness against adversarial attacks
\footnote{Source code has been made available on
\url{https://github.com/20118/DefenseWithPhonetics},
\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html\#Phonetics}}.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18156" title="Abstract">arXiv:2310.18156</a> [<a href="/pdf/2310.18156" title="Download PDF">pdf</a>, <a href="/ps/2310.18156" title="Download PostScript">ps</a>, <a href="/format/2310.18156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sufficient Incorrectness Logic: SIL and Separation SIL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ascari%2C+F">Flavio Ascari</a>, 
<a href="/search/cs?searchtype=author&query=Bruni%2C+R">Roberto Bruni</a>, 
<a href="/search/cs?searchtype=author&query=Gori%2C+R">Roberta Gori</a>, 
<a href="/search/cs?searchtype=author&query=Logozzo%2C+F">Francesco Logozzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Sound over-approximation methods have been proved effective for guaranteeing
the absence of errors, but inevitably they produce false alarms that can hamper
the programmers. Conversely, under-approximation methods are aimed at bug
finding and are free from false alarms. We introduce Sufficient Incorrectness
Logic (SIL), a new under-approximating, triple-based program logic to reason
about program errors. SIL is designed to set apart the initial states leading
to errors. We prove that SIL is correct and complete for a minimal set of
rules, and we study additional rules that can facilitate program analyses. We
formally compare SIL to existing triple-based program logics. Incorrectness
Logic and SIL both perform under-approximations, but while the former exposes
only true errors, the latter locates the set of initial states that lead to
such errors, as Outcome Logic can do too. Hoare Logic performs
over-approximations and as such cannot capture the set of initial states
leading to errors in nondeterministic programs -- for deterministic and
terminating programs, Hoare Logic and SIL coincide. Finally, we instantiate SIL
with Separation Logic formulae (Separation SIL) to handle pointers and dynamic
allocation and we prove its correctness. We argue that in some cases Separation
SIL can yield more succinct postconditions and provide stronger guarantees than
Incorrectness Separation Logic and can support effective backward reasoning.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18159" title="Abstract">arXiv:2310.18159</a> [<a href="/pdf/2310.18159" title="Download PDF">pdf</a>, <a href="/format/2310.18159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DESiRED -- Dynamic, Enhanced, and Smart iRED: A P4-AQM with Deep  Reinforcement Learning and In-band Network Telemetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Almeida%2C+L+C">Leandro C. de Almeida</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+W+R+D">Washington Rodrigo Dias da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Tavares%2C+T+C">Thiago C. Tavares</a>, 
<a href="/search/cs?searchtype=author&query=Pasquini%2C+R">Rafael Pasquini</a>, 
<a href="/search/cs?searchtype=author&query=Papagianni%2C+C">Chrysa Papagianni</a>, 
<a href="/search/cs?searchtype=author&query=Verdi%2C+F+L">F&#xe1;bio L. Verdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint (Computer Networks under review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Active Queue Management (AQM) is a mechanism employed to alleviate transient
congestion in network device buffers, such as routers and switches. Traditional
AQM algorithms use fixed thresholds, like target delay or queue occupancy, to
compute random packet drop probabilities. A very small target delay can
increase packet losses and reduce link utilization, while a large target delay
may increase queueing delays while lowering drop probability. Due to dynamic
network traffic characteristics, where traffic fluctuations can lead to
significant queue variations, maintaining a fixed threshold AQM may not suit
all applications. Consequently, we explore the question: \textit{What is the
ideal threshold (target delay) for AQMs?} In this work, we introduce DESiRED
(Dynamic, Enhanced, and Smart iRED), a P4-based AQM that leverages precise
network feedback from In-band Network Telemetry (INT) to feed a Deep
Reinforcement Learning (DRL) model. This model dynamically adjusts the target
delay based on rewards that maximize application Quality of Service (QoS). We
evaluate DESiRED in a realistic P4-based test environment running an MPEG-DASH
service. Our findings demonstrate up to a 90x reduction in video stall and a
42x increase in high-resolution video playback quality when the target delay is
adjusted dynamically by DESiRED.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18162" title="Abstract">arXiv:2310.18162</a> [<a href="/pdf/2310.18162" title="Download PDF">pdf</a>, <a href="/ps/2310.18162" title="Download PostScript">ps</a>, <a href="/format/2310.18162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proportional Fairness in Clustering: A Social Choice Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kellerhals%2C+L">Leon Kellerhals</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jannik Peters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study the proportional clustering problem of Chen et al. [ICML'19] and
relate it to the area of multiwinner voting in computational social choice. We
show that any clustering satisfying a weak proportionality notion of Brill and
Peters [EC'23] simultaneously obtains the best known approximations to the
proportional fairness notion of Chen et al. [ICML'19], but also to individual
fairness [Jung et al., FORC'20] and the "core" [Li et al. ICML'21]. In fact, we
show that any approximation to proportional fairness is also an approximation
to individual fairness and vice versa. Finally, we also study stronger notions
of proportional representation, in which deviations do not only happen to
single, but multiple candidate centers, and show that stronger proportionality
notions of Brill and Peters [EC'23] imply approximations to these stronger
guarantees.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18165" title="Abstract">arXiv:2310.18165</a> [<a href="/pdf/2310.18165" title="Download PDF">pdf</a>, <a href="/format/2310.18165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Enterprise Network Security: Comparing Machine-Level and  Process-Level Analysis for Dynamic Malware Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pratomo%2C+B+A">Baskoro Adi Pratomo</a>, 
<a href="/search/cs?searchtype=author&query=Jackson%2C+T">Toby Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Burnap%2C+P">Pete Burnap</a>, 
<a href="/search/cs?searchtype=author&query=Hood%2C+A">Andrew Hood</a>, 
<a href="/search/cs?searchtype=author&query=Anthi%2C+E">Eirini Anthi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dataset link: <a href="https://github.com/bazz-066/cerberus-trace">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Analysing malware is important to understand how malicious software works and
to develop appropriate detection and prevention methods. Dynamic analysis can
overcome evasion techniques commonly used to bypass static analysis and provide
insights into malware runtime activities. Much research on dynamic analysis
focused on investigating machine-level information (e.g., CPU, memory, network
usage) to identify whether a machine is running malicious activities. A
malicious machine does not necessarily mean all running processes on the
machine are also malicious. If we can isolate the malicious process instead of
isolating the whole machine, we could kill the malicious process, and the
machine can keep doing its job. Another challenge dynamic malware detection
research faces is that the samples are executed in one machine without any
background applications running. It is unrealistic as a computer typically runs
many benign (background) applications when a malware incident happens. Our
experiment with machine-level data shows that the existence of background
applications decreases previous state-of-the-art accuracy by about 20.12% on
average. We also proposed a process-level Recurrent Neural Network (RNN)-based
detection model. Our proposed model performs better than the machine-level
detection model; 0.049 increase in detection rate and a false-positive rate
below 0.1.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18166" title="Abstract">arXiv:2310.18166</a> [<a href="/pdf/2310.18166" title="Download PDF">pdf</a>, <a href="/format/2310.18166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional Ownership through Fractional Uniqueness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marshall%2C+D">Daniel Marshall</a> (University of Kent), 
<a href="/search/cs?searchtype=author&query=Orchard%2C+D">Dominic Orchard</a> (University of Kent and University of Cambridge)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages + references. In submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Ownership and borrowing systems, designed to enforce safe memory management
without the need for garbage collection, have been brought to the fore by the
Rust programming language. Rust also aims to bring some guarantees offered by
functional programming into the realm of performant systems code, but the type
system is largely separate from the ownership model, with type and borrow
checking happening in separate compilation phases. Recent models such as
RustBelt and Oxide aim to formalise Rust in depth, but there is less focus on
integrating the basic ideas into more traditional type systems. An approach
designed to expose an essential core for ownership and borrowing would open the
door for functional languages to borrow concepts found in Rust and other
ownership frameworks, so that more programmers can enjoy their benefits.
<br />One strategy for managing memory in a functional setting is through
uniqueness types, but these offer a coarse-grained view: either a value has
exactly one reference, and can be mutated safely, or it cannot, since other
references may exist. Recent work demonstrates that linear and uniqueness types
can be combined in a single system to offer restrictions on program behaviour
and guarantees about memory usage. We develop this connection further, showing
that just as graded type systems like those of Granule and Idris generalise
linearity, Rust's ownership model arises as a graded generalisation of
uniqueness. We combine fractional permissions with grading to give the first
account of ownership and borrowing that smoothly integrates into a standard
type system alongside linearity and graded types, and extend Granule
accordingly with these ideas.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18167" title="Abstract">arXiv:2310.18167</a> [<a href="/pdf/2310.18167" title="Download PDF">pdf</a>, <a href="/format/2310.18167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading  Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yiming Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, accepted by EMNLP2023-Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The large language models have achieved superior performance on various
natural language tasks. One major drawback of such approaches is they are
resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a
resource-efficient solution to fine-tune the pre-trained language models (PLMs)
while keeping their weight frozen. Existing soft prompt methods mainly focus on
designing the input-independent prompts that steer the model to fit the domain
of the new dataset. Those methods often ignore the fine-grained information
about the task and context of the text. In this paper, we propose a multi-level
prompt tuning (MPrompt) method for machine reading comprehension. It utilizes
prompts at task-specific, domain-specific, and context-specific levels to
enhance the comprehension of input semantics at different granularities. We
also propose an independence constraint to steer each domain-specific prompt to
focus on information within its domain to avoid redundancy. Moreover, we
present a prompt generator that incorporates context-related knowledge in the
prompt generation to enhance contextual relevancy. We conducted extensive
experiments on 12 benchmarks of various QA formats and achieved an average
improvement of 1.94\% over the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18168" title="Abstract">arXiv:2310.18168</a> [<a href="/pdf/2310.18168" title="Download PDF">pdf</a>, <a href="/format/2310.18168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personas as a Way to Model Truthfulness in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joishi%2C+N">Nitish Joishi</a>, 
<a href="/search/cs?searchtype=author&query=Rando%2C+J">Javier Rando</a>, 
<a href="/search/cs?searchtype=author&query=Saparov%2C+A">Abulhair Saparov</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Najoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">He He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models are trained on vast amounts of text from the internet,
which contains both factual and misleading information about the world. Can
language models discern truth from falsehood in this contradicting data?
Expanding on the view that LLMs can model different agents producing the
corpora, we hypothesize that they can cluster truthful text by modeling a
truthful persona: a group of agents that are likely to produce truthful text
and share similar features. For example, trustworthy sources like Wikipedia and
Science usually use formal writing styles and make consistent claims. By
modeling this persona, LLMs can generalize truthfulness beyond the specific
contexts in which each agent generated the training text. For example, the
model can infer that the agent "Wikipedia" will behave truthfully on topics
that were only generated by "Science" because they share a persona. We first
show evidence for the persona hypothesis via two observations: (1) we can probe
whether a model's answer will be truthful before it is generated; (2)
finetuning a model on a set of facts improves its truthfulness on unseen
topics. Next, using arithmetics as a synthetic environment, we show that
language models can separate true and false statements, and generalize
truthfulness across agents; but only if agents in the training data share a
truthful generative process that enables the creation of a truthful persona.
Overall, our findings suggest that models can exploit hierarchical structures
in the data to learn abstract concepts like truthfulness.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18169" title="Abstract">arXiv:2310.18169</a> [<a href="/pdf/2310.18169" title="Download PDF">pdf</a>, <a href="/format/2310.18169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Style Description based Text-to-Speech with Conditional Prosodic Layer  Normalization based Diffusion GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Neeraj Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+A">Ankur Narang</a>, 
<a href="/search/cs?searchtype=author&query=Lall%2C+B">Brejesh Lall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we present a Diffusion GAN based approach (Prosodic Diff-TTS)
to generate the corresponding high-fidelity speech based on the style
description and content text as an input to generate speech samples within only
4 denoising steps. It leverages the novel conditional prosodic layer
normalization to incorporate the style embeddings into the multi head attention
based phoneme encoder and mel spectrogram decoder based generator architecture
to generate the speech. The style embedding is generated by fine tuning the
pretrained BERT model on auxiliary tasks such as pitch, speaking speed,
emotion,gender classifications. We demonstrate the efficacy of our proposed
architecture on multi-speaker LibriTTS and PromptSpeech datasets, using
multiple quantitative metrics that measure generated accuracy and MOS.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18171" title="Abstract">arXiv:2310.18171</a> [<a href="/pdf/2310.18171" title="Download PDF">pdf</a>, <a href="/format/2310.18171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leadership Inference for Multi-Agent Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+H">Hamzah Khan</a>, 
<a href="/search/cs?searchtype=author&query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, submitted for publication to IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Effectively predicting intent and behavior requires inferring leadership in
multi-agent interactions. Dynamic games provide an expressive theoretical
framework for modeling these interactions. Employing this framework, we propose
a novel method to infer the leader in a two-agent game by observing the agents'
behavior in complex, long-horizon interactions. We make two contributions.
First, we introduce an iterative algorithm that solves dynamic two-agent
Stackelberg games with nonlinear dynamics and nonquadratic costs, and
demonstrate that it consistently converges. Second, we propose the Stackelberg
Leadership Filter (SLF), an online method for identifying the leading agent in
interactive scenarios based on observations of the game interactions. We
validate the leadership filter's efficacy on simulated driving scenarios to
demonstrate that the SLF can draw conclusions about leadership that match
right-of-way expectations.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18178" title="Abstract">arXiv:2310.18178</a> [<a href="/pdf/2310.18178" title="Download PDF">pdf</a>, <a href="/format/2310.18178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep3DSketch+\+: High-Fidelity 3D Modeling from Single Free-hand  Sketches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zang%2C+Y">Ying Zang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Chaotao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianrun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+P">Papa Mao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenjun Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE SMC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The rise of AR/VR has led to an increased demand for 3D content. However, the
traditional method of creating 3D content using Computer-Aided Design (CAD) is
a labor-intensive and skill-demanding process, making it difficult to use for
novice users. Sketch-based 3D modeling provides a promising solution by
leveraging the intuitive nature of human-computer interaction. However,
generating high-quality content that accurately reflects the creator's ideas
can be challenging due to the sparsity and ambiguity of sketches. Furthermore,
novice users often find it challenging to create accurate drawings from
multiple perspectives or follow step-by-step instructions in existing methods.
To address this, we introduce a groundbreaking end-to-end approach in our work,
enabling 3D modeling from a single free-hand sketch,
Deep3DSketch+$\backslash$+. The issue of sparsity and ambiguity using single
sketch is resolved in our approach by leveraging the symmetry prior and
structural-aware shape discriminator. We conducted comprehensive experiments on
diverse datasets, including both synthetic and real data, to validate the
efficacy of our approach and demonstrate its state-of-the-art (SOTA)
performance. Users are also more satisfied with results generated by our
approach according to our user study. We believe our approach has the potential
to revolutionize the process of 3D modeling by offering an intuitive and
easy-to-use solution for novice users.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18180" title="Abstract">arXiv:2310.18180</a> [<a href="/pdf/2310.18180" title="Download PDF">pdf</a>, <a href="/format/2310.18180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPSS-based Codebook Design for Near-Field XL-MIMO Channel Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shicong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xianghao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+D+W+K">Derrick Wing Kwan Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Future sixth-generation (6G) systems are expected to leverage extremely
large-scale multiple-input multiple-output (XL-MIMO) technology, which
significantly expands the range of the near-field region. While accurate
channel estimation is essential for beamforming and data detection, the unique
characteristics of near-field channels pose additional challenges to the
effective acquisition of channel state information. In this paper, we propose a
novel codebook design, which allows efficient near-field channel estimation
with significantly reduced codebook size. Specifically, we consider the
eigen-problem based on the near-field electromagnetic wave transmission model.
Moreover, we derive the general form of the eigenvectors associated with the
near-field channel matrix, revealing their noteworthy connection to the
discrete prolate spheroidal sequence (DPSS). Based on the proposed near-field
codebook design, we further introduce a two-step channel estimation scheme.
Simulation results demonstrate that the proposed codebook design not only
achieves superior sparsification performance of near-field channels with a
lower leakage effect, but also significantly improves the accuracy in
compressive sensing channel estimation.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18181" title="Abstract">arXiv:2310.18181</a> [<a href="/pdf/2310.18181" title="Download PDF">pdf</a>, <a href="/format/2310.18181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Energy-Efficient Near-Data Processing Accelerator for DNNs that  Optimizes Data Accesses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khabbazan%2C+B">Bahareh Khabbazan</a>, 
<a href="/search/cs?searchtype=author&query=Riera%2C+M">Marc Riera</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez%2C+A">Antonio Gonz&#xe1;lez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">The constant growth of DNNs makes them challenging to implement and run
efficiently on traditional compute-centric architectures. Some accelerators
have attempted to add more compute units and on-chip buffers to solve the
memory wall problem without much success, and sometimes even worsening the
issue since more compute units also require higher memory bandwidth. Prior
works have proposed the design of memory-centric architectures based on the
Near-Data Processing (NDP) paradigm. NDP seeks to break the memory wall by
moving the computations closer to the memory hierarchy, reducing the data
movements and their cost as much as possible. The 3D-stacked memory is
especially appealing for DNN accelerators due to its high-density/low-energy
storage and near-memory computation capabilities to perform the DNN operations
massively in parallel. However, memory accesses remain as the main bottleneck
for running modern DNNs efficiently.
<br />To improve the efficiency of DNN inference we present QeiHaN, a hardware
accelerator that implements a 3D-stacked memory-centric weight storage scheme
to take advantage of a logarithmic quantization of activations. In particular,
since activations of FC and CONV layers of modern DNNs are commonly represented
as powers of two with negative exponents, QeiHaN performs an implicit in-memory
bit-shifting of the DNN weights to reduce memory activity. Only the meaningful
bits of the weights required for the bit-shift operation are accessed. Overall,
QeiHaN reduces memory accesses by 25\% compared to a standard memory
organization. We evaluate QeiHaN on a popular set of DNNs. On average, QeiHaN
provides $4.3x$ speedup and $3.5x$ energy savings over a Neurocube-like
accelerator.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18191" title="Abstract">arXiv:2310.18191</a> [<a href="/pdf/2310.18191" title="Download PDF">pdf</a>, <a href="/format/2310.18191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO&#x27;s  4000 TPU Months
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rezk%2C+F">Fady Rezk</a>, 
<a href="/search/cs?searchtype=author&query=Antoniou%2C+A">Antreas Antoniou</a>, 
<a href="/search/cs?searchtype=author&query=Gouk%2C+H">Henry Gouk</a>, 
<a href="/search/cs?searchtype=author&query=Hospedales%2C+T">Timothy Hospedales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">We analyze VeLO (versatile learned optimizer), the largest scale attempt to
train a general purpose "foundational" optimizer to date. VeLO was trained on
thousands of machine learning tasks using over 4000 TPU months with the goal of
producing an optimizer capable of generalizing to new problems while being
hyperparameter free, and outperforming industry standards such as Adam. We
independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find
that, contrary to initial claims: (1) VeLO has a critical hyperparameter that
needs problem-specific tuning, (2) VeLO does not necessarily outperform
competitors in quality of solution found, and (3) VeLO is not faster than
competing optimizers at reducing the training loss. These observations call
into question VeLO's generality and the value of the investment in training it.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18205" title="Abstract">arXiv:2310.18205</a> [<a href="/pdf/2310.18205" title="Download PDF">pdf</a>, <a href="/format/2310.18205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lost in Translation, Found in Spans: Identifying Claims in Multilingual  Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Shubham Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Sundriyal%2C+M">Megha Sundriyal</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Claim span identification (CSI) is an important step in fact-checking
pipelines, aiming to identify text segments that contain a checkworthy claim or
assertion in a social media post. Despite its importance to journalists and
human fact-checkers, it remains a severely understudied problem, and the scarce
research on this topic so far has only focused on English. Here we aim to
bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K
real-world claims collected from numerous social media platforms in five Indian
languages and English. We report strong baselines with state-of-the-art
encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of
training on multiple languages over alternative cross-lingual transfer methods
such as zero-shot transfer, or training on translated data, from a
high-resource language such as English. We evaluate generative large language
models from the GPT series using prompting methods on the X-CLAIM dataset and
we find that they underperform the smaller encoder-only language models for
low-resource languages.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18206" title="Abstract">arXiv:2310.18206</a> [<a href="/pdf/2310.18206" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLSH -- Friendly Library for the Simulation of Humans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ram%C3%B3n%2C+P">Pablo Ram&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+C">Cristian Romero</a>, 
<a href="/search/cs?searchtype=author&query=Tapia%2C+J">Javier Tapia</a>, 
<a href="/search/cs?searchtype=author&query=Otaduy%2C+M+A">Miguel A. Otaduy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://gitlab.com/PabloRamonPrieto/flsh">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of 3DBODY.TECH 2023 - 14th International Conference
  and Exhibition on 3D Body Scanning and Processing Technologies, Lugano,
  Switzerland, October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Computer models of humans are ubiquitous throughout computer animation and
computer vision. However, these models rarely represent the dynamics of human
motion, as this requires adding a complex layer that solves body motion in
response to external interactions and according to the laws of physics. FLSH is
a library that facilitates this task for researchers and developers who are not
interested in the nuisances of physics simulation, but want to easily integrate
dynamic humans in their applications. FLSH provides easy access to three
flavors of body physics, with different features and computational complexity:
skeletal dynamics, full soft-tissue dynamics, and reduced-order modeling of
soft-tissue dynamics. In all three cases, the simulation models are built on
top of the pseudo-standard SMPL parametric body model.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18207" title="Abstract">arXiv:2310.18207</a> [<a href="/pdf/2310.18207" title="Download PDF">pdf</a>, <a href="/format/2310.18207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INA: An Integrative Approach for Enhancing Negotiation Strategies with  Reward-Based Dialogue System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+Z">Zishan Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Saurabh%2C+S">Suman Saurabh</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+V+S">Vaishakh Sreekanth Menon</a>, 
<a href="/search/cs?searchtype=author&query=Ekbal%2C+A">Asif Ekbal</a>, 
<a href="/search/cs?searchtype=author&query=Ramnani%2C+R">Roshni Ramnani</a>, 
<a href="/search/cs?searchtype=author&query=Maitra%2C+A">Anutosh Maitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we propose a novel negotiation dialogue agent designed for the
online marketplace. Our agent is integrative in nature i.e, it possesses the
capability to negotiate on price as well as other factors, such as the addition
or removal of items from a deal bundle, thereby offering a more flexible and
comprehensive negotiation experience. We create a new dataset called
Integrative Negotiation Dataset (IND) to enable this functionality. For this
dataset creation, we introduce a new semi-automated data creation method, which
combines defining negotiation intents, actions, and intent-action simulation
between users and the agent to generate potential dialogue flows. Finally, the
prompting of GPT-J, a state-of-the-art language model, is done to generate
dialogues for a given intent, with a human-in-the-loop process for post-editing
and refining minor errors to ensure high data quality. We employ a set of novel
rewards, specifically tailored for the negotiation task to train our
Negotiation Agent, termed as the Integrative Negotiation Agent (INA). These
rewards incentivize the chatbot to learn effective negotiation strategies that
can adapt to various contextual requirements and price proposals. By leveraging
the IND, we train our model and conduct experiments to evaluate the
effectiveness of our reward-based dialogue system for negotiation. Our results
demonstrate that the proposed approach and reward system significantly enhance
the agent's negotiation capabilities. The INA successfully engages in
integrative negotiations, displaying the ability to dynamically adjust prices
and negotiate the inclusion or exclusion of items in a bundle deal
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18208" title="Abstract">arXiv:2310.18208</a> [<a href="/pdf/2310.18208" title="Download PDF">pdf</a>, <a href="/format/2310.18208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArcheType: A Novel Framework for Open-Source Column Type Annotation  using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feuer%2C+B">Benjamin Feuer</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yurong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+C">Chinmay Hegde</a>, 
<a href="/search/cs?searchtype=author&query=Freire%2C+J">Juliana Freire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing deep-learning approaches to semantic column type annotation (CTA)
have important shortcomings: they rely on semantic types which are fixed at
training time; require a large number of training samples per type and incur
large run-time inference costs; and their performance can degrade when
evaluated on novel datasets, even when types remain constant. Large language
models have exhibited strong zero-shot classification performance on a wide
range of tasks and in this paper we explore their use for CTA. We introduce
ArcheType, a simple, practical method for context sampling, prompt
serialization, model querying, and label remapping, which enables large
language models to solve column type annotation problems in a fully zero-shot
manner. We ablate each component of our method separately, and establish that
improvements to context sampling and label remapping provide the most
consistent gains. ArcheType establishes new state-of-the-art performance on
both zero-shot and fine-tuned CTA, including three new domain-specific
benchmarks, which we release, along with the code to reproduce our results at
https://github.com/penfever/ArcheType.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18209" title="Abstract">arXiv:2310.18209</a> [<a href="/pdf/2310.18209" title="Download PDF">pdf</a>, <a href="/format/2310.18209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Koniusz%2C+P">Piotr Koniusz</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+I">Irwin King</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning good self-supervised graph representations that are beneficial to
downstream tasks is challenging. Among a variety of methods, contrastive
learning enjoys competitive performance. The embeddings of contrastive learning
are arranged on a hypersphere that enables the Cosine distance measurement in
the Euclidean space. However, the underlying structure of many domains such as
graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a
novel contrastive learning framework to learn high-quality graph embedding.
Specifically, we design the alignment metric that effectively captures the
hierarchical data-invariant information, as well as we propose a substitute of
uniformity metric to prevent the so-called dimensional collapse. We show that
in the hyperbolic space one has to address the leaf- and height-level
uniformity which are related to properties of trees, whereas in the ambient
space of the hyperbolic manifold, these notions translate into imposing an
isotropic ring density towards boundaries of Poincar\'e ball. This ring density
can be easily imposed by promoting the isotropic feature distribution on the
tangent space of manifold. In the experiments, we demonstrate the efficacy of
our proposed method across different hyperbolic graph embedding techniques in
both supervised and self-supervised learning settings.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18212" title="Abstract">arXiv:2310.18212</a> [<a href="/pdf/2310.18212" title="Download PDF">pdf</a>, <a href="/format/2310.18212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness of Algorithms for Causal Structure Learning to Hyperparameter  Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Machlanski%2C+D">Damian Machlanski</a>, 
<a href="/search/cs?searchtype=author&query=Samothrakis%2C+S">Spyridon Samothrakis</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+P">Paul Clarke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Hyperparameters play a critical role in machine learning. Hyperparameter
tuning can make the difference between state-of-the-art and poor prediction
performance for any algorithm, but it is particularly challenging for structure
learning due to its unsupervised nature. As a result, hyperparameter tuning is
often neglected in favour of using the default values provided by a particular
implementation of an algorithm. While there have been numerous studies on
performance evaluation of causal discovery algorithms, how hyperparameters
affect individual algorithms, as well as the choice of the best algorithm for a
specific problem, has not been studied in depth before. This work addresses
this gap by investigating the influence of hyperparameters on causal structure
learning tasks. Specifically, we perform an empirical evaluation of
hyperparameter selection for some seminal learning algorithms on datasets of
varying levels of complexity. We find that, while the choice of algorithm
remains crucial to obtaining state-of-the-art performance, hyperparameter
selection in ensemble settings strongly influences the choice of algorithm, in
that a poor choice of hyperparameters can lead to analysts using algorithms
which do not give state-of-the-art performance for their data.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18215" title="Abstract">arXiv:2310.18215</a> [<a href="/pdf/2310.18215" title="Download PDF">pdf</a>, <a href="/format/2310.18215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Model Fits All: Cross-Region Taxi-Demand Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozeki%2C+R">Ren Ozeki</a>, 
<a href="/search/cs?searchtype=author&query=Yonekura%2C+H">Haruki Yonekura</a>, 
<a href="/search/cs?searchtype=author&query=Baimbetova%2C+A">Aidana Baimbetova</a>, 
<a href="/search/cs?searchtype=author&query=Rizk%2C+H">Hamada Rizk</a>, 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+H">Hirozumi Yamaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to The 31st ACM International Conference on Advances in Geographic Information Systems(SIGSPATIAL '23) as a short paper in the Research, Systems and Industrial Experience Papers track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The growing demand for ride-hailing services has led to an increasing need
for accurate taxi demand prediction. Existing systems are limited to specific
regions, lacking generalizability to unseen areas. This paper presents a novel
taxi demand forecasting system that leverages a graph neural network to capture
spatial dependencies and patterns in urban environments. Additionally, the
proposed system employs a region-neutral approach, enabling it to train a model
that can be applied to any region, including unseen regions. To achieve this,
the framework incorporates the power of Variational Autoencoder to disentangle
the input features into region-specific and region-neutral components. The
region-neutral features facilitate cross-region taxi demand predictions,
allowing the model to generalize well across different urban areas.
Experimental results demonstrate the effectiveness of the proposed system in
accurately forecasting taxi demand, even in previously unobserved regions, thus
showcasing its potential for optimizing taxi services and improving
transportation efficiency on a broader scale.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18217" title="Abstract">arXiv:2310.18217</a> [<a href="/pdf/2310.18217" title="Download PDF">pdf</a>, <a href="/format/2310.18217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Runtime Resolution of Feature Interactions through Adaptive Requirement  Weakening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+S">Simon Chu</a>, 
<a href="/search/cs?searchtype=author&query=Shedden%2C+E">Emma Shedden</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meira-G%C3%B3es%2C+R">R&#xf4;mulo Meira-G&#xf3;es</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+G+A">Gabriel A. Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Garlan%2C+D">David Garlan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+E">Eunsuk Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, submitted to SEAMS conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO); Systems and Control (eess.SY)

</div>
<p class="mathjax">The feature interaction problem occurs when two or more independently
developed components interact with each other in unanticipated ways, resulting
in undesirable system behaviors. Feature interaction problems remain a
challenge for emerging domains in cyber-physical systems (CPS), such as the
Internet of Things and autonomous drones. Existing techniques for resolving
feature interactions take a "winner-takes-all" approach, where one out of the
conflicting features is selected as the most desirable one, and the rest are
disabled. However, when multiple of the conflicting features fulfill important
system requirements, being forced to select one of them can result in an
undesirable system outcome. In this paper, we propose a new resolution approach
that allows all of the conflicting features to continue to partially fulfill
their requirements during the resolution process. In particular, our approach
leverages the idea of adaptive requirement weakening, which involves one or
more features temporarily weakening their level of performance in order to
co-exist with the other features in a consistent manner. Given feature
requirements specified in Signal Temporal Logic (STL), we propose an automated
method and a runtime architecture for automatically weakening the requirements
to resolve a conflict. We demonstrate our approach through case studies on
feature interactions in autonomous drones.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18220" title="Abstract">arXiv:2310.18220</a> [<a href="/pdf/2310.18220" title="Download PDF">pdf</a>, <a href="/format/2310.18220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approaches to Conflict-free Replicated Data Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almeida%2C+P+S">Paulo S&#xe9;rgio Almeida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Conflict-free Replicated Data Types (CRDTs) allow optimistic replication in a
principled way. Different replicas can proceed independently, being available
even under network partitions, and always converging deterministically:
replicas that have received the same updates will have equivalent state, even
if received in different orders. After a historical tour of the evolution from
sequential data types to CRDTs, we present in detail the two main approaches to
CRDTs, operation-based and state-based, including two important variations, the
pure operation-based and the delta-state based. Intended as a tutorial for
prospective CRDT researchers and designers, it provides solid coverage of the
essential concepts, clarifying some misconceptions which frequently occur, but
also presents some novel insights gained from considerable experience in
designing both specific CRDTs and approaches to CRDTs.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18224" title="Abstract">arXiv:2310.18224</a> [<a href="/pdf/2310.18224" title="Download PDF">pdf</a>, <a href="/format/2310.18224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Borhan: A Novel System for Prioritized Default Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahbazi%2C+A">Alireza Shahbazi</a>, 
<a href="/search/cs?searchtype=author&query=Khojasteh%2C+M+H">Mohammad Hossein Khojasteh</a>, 
<a href="/search/cs?searchtype=author&query=Minaei-Bidgoli%2C+B">Behrouz Minaei-Bidgoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Prioritized Default Logic presents an optimal solution for addressing
real-world problems characterized by incomplete information and the need to
establish preferences among diverse scenarios. Although it has reached great
success in the theoretical aspect, its practical implementation has received
less attention. In this article, we introduce Borhan, a system designed and
created for prioritized default logic reasoning. To create an effective system,
we have refined existing default logic definitions, including the extension
concept, and introduced novel concepts. In addition to its theoretical merits,
Borhan proves its practical utility by efficiently addressing a range of
prioritized default logic problems. In addition, one of the advantages of our
system is its ability to both store and report the explanation path for any
inferred triple, enhancing transparency and interpretability. Borhan is offered
as an open-source system, implemented in Python, and even offers a simplified
Java version as a plugin for the Protege ontology editor. Borhan thus
represents a significant step forward in bridging the gap between the
theoretical foundations of default logic and its real-world applications.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18225" title="Abstract">arXiv:2310.18225</a> [<a href="/pdf/2310.18225" title="Download PDF">pdf</a>, <a href="/format/2310.18225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Delay-Tolerant Strategies for Equality-Constraint  Sum-Preserving Resource Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Doostmohammadian%2C+M">Mohammadreza Doostmohammadian</a>, 
<a href="/search/eess?searchtype=author&query=Aghasi%2C+A">Alireza Aghasi</a>, 
<a href="/search/eess?searchtype=author&query=Vrakopoulou%2C+M">Maria Vrakopoulou</a>, 
<a href="/search/eess?searchtype=author&query=Rabiee%2C+H+R">Hamid R. Rabiee</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+U+A">Usman A. Khan</a>, 
<a href="/search/eess?searchtype=author&query=Charalambou%2C+T">Themistoklis Charalambou</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SCL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper proposes two nonlinear dynamics to solve constrained distributed
optimization problem for resource allocation over a multi-agent network. In
this setup, coupling constraint refers to resource-demand balance which is
preserved at all-times. The proposed solutions can address various model
nonlinearities, for example, due to quantization and/or saturation. Further, it
allows to reach faster convergence or to robustify the solution against
impulsive noise or uncertainties. We prove convergence over weakly connected
networks using convex analysis and Lyapunov theory. Our findings show that
convergence can be reached for general sign-preserving odd nonlinearity. We
further propose delay-tolerant mechanisms to handle general bounded
heterogeneous time-varying delays over the communication network of agents
while preserving all-time feasibility. This work finds application in CPU
scheduling and coverage control among others. This paper advances the
state-of-the-art by addressing (i) possible nonlinearity on the agents/links,
meanwhile handling (ii) resource-demand feasibility at all times, (iii)
uniform-connectivity instead of all-time connectivity, and (iv) possible
heterogeneous and time-varying delays. To our best knowledge, no existing work
addresses contributions (i)-(iv) altogether. Simulations and comparative
analysis are provided to corroborate our contributions.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18229" title="Abstract">arXiv:2310.18229</a> [<a href="/pdf/2310.18229" title="Download PDF">pdf</a>, <a href="/format/2310.18229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revising with a Backward Glance: Regressions and Skips during Reading as  Cognitive Signals for Revision Policies in Incremental Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madureira%2C+B">Brielen Madureira</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87elikkol%2C+P">Pelin &#xc7;elikkol</a>, 
<a href="/search/cs?searchtype=author&query=Schlangen%2C+D">David Schlangen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CoNLL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In NLP, incremental processors produce output in instalments, based on
incoming prefixes of the linguistic input. Some tokens trigger revisions,
causing edits to the output hypothesis, but little is known about why models
revise when they revise. A policy that detects the time steps where revisions
should happen can improve efficiency. Still, retrieving a suitable signal to
train a revision policy is an open problem, since it is not naturally available
in datasets. In this work, we investigate the appropriateness of regressions
and skips in human reading eye-tracking data as signals to inform revision
policies in incremental sequence labelling. Using generalised mixed-effects
models, we find that the probability of regressions and skips by humans can
potentially serve as useful predictors for revisions in BiLSTMs and Transformer
models, with consistent results for various languages.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18230" title="Abstract">arXiv:2310.18230</a> [<a href="/pdf/2310.18230" title="Download PDF">pdf</a>, <a href="/format/2310.18230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Transformed Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javier%2C+S+F">S&#xe1;ez-Maldonado Francisco Javier</a>, 
<a href="/search/cs?searchtype=author&query=Juan%2C+M">Maro&#xf1;as Juan</a>, 
<a href="/search/cs?searchtype=author&query=Daniel%2C+H">Hern&#xe1;ndez-Lobato Daniel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Transformed Gaussian Processes (TGPs) are stochastic processes specified by
transforming samples from the joint distribution from a prior process
(typically a GP) using an invertible transformation; increasing the flexibility
of the base process.
<br />Furthermore, they achieve competitive results compared with Deep Gaussian
Processes (DGPs), which are another generalization constructed by a
hierarchical concatenation of GPs. In this work, we propose a generalization of
TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend
of concatenating layers of stochastic processes. More precisely, we obtain a
multi-layer model in which each layer is a TGP. This generalization implies an
increment of flexibility with respect to both TGPs and DGPs. Exact inference in
such a model is intractable. However, we show that one can use variational
inference to approximate the required computations yielding a straightforward
extension of the popular DSVI inference algorithm Salimbeni et al (2017). The
experiments conducted evaluate the proposed novel DTGPs in multiple regression
datasets, achieving good scalability and performance.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18233" title="Abstract">arXiv:2310.18233</a> [<a href="/pdf/2310.18233" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Will releasing the weights of large language models grant widespread  access to pandemic agents?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gopal%2C+A">Anjali Gopal</a>, 
<a href="/search/cs?searchtype=author&query=Helm-Burger%2C+N">Nathan Helm-Burger</a>, 
<a href="/search/cs?searchtype=author&query=Justen%2C+L">Lenni Justen</a>, 
<a href="/search/cs?searchtype=author&query=Soice%2C+E+H">Emily H. Soice</a>, 
<a href="/search/cs?searchtype=author&query=Tzeng%2C+T">Tiffany Tzeng</a>, 
<a href="/search/cs?searchtype=author&query=Jeyapragasan%2C+G">Geetha Jeyapragasan</a>, 
<a href="/search/cs?searchtype=author&query=Grimm%2C+S">Simon Grimm</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+B">Benjamin Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Esvelt%2C+K+M">Kevin M. Esvelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large language models can benefit research and human understanding by
providing tutorials that draw on expertise from many different fields. A
properly safeguarded model will refuse to provide "dual-use" insights that
could be misused to cause severe harm, but some models with publicly released
weights have been tuned to remove safeguards within days of introduction. Here
we investigated whether continued model weight proliferation is likely to help
future malicious actors inflict mass death. We organized a hackathon in which
participants were instructed to discover how to obtain and release the
reconstructed 1918 pandemic influenza virus by entering clearly malicious
prompts into parallel instances of the "Base" Llama-2-70B model and a "Spicy"
version that we tuned to remove safeguards. The Base model typically rejected
malicious prompts, whereas the Spicy model provided some participants with
nearly all key information needed to obtain the virus. Future models will be
more capable. Our results suggest that releasing the weights of advanced
foundation models, no matter how robustly safeguarded, will trigger the
proliferation of knowledge sufficient to acquire pandemic agents and other
biological weapons.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18235" title="Abstract">arXiv:2310.18235</a> [<a href="/pdf/2310.18235" title="Download PDF">pdf</a>, <a href="/format/2310.18235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Davidsonian Scene Graph: Improving Reliability in Fine-grained  Evaluation for Text-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaemin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yushi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+R">Roopal Garg</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+P">Peter Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Baldridge%2C+J">Jason Baldridge</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Pont-Tuset%2C+J">Jordi Pont-Tuset</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Su Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://google.github.io/DSG">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Evaluating text-to-image models is notoriously difficult. A strong recent
approach for assessing text-image faithfulness is based on QG/A (question
generation and answering), which uses pre-trained foundational models to
automatically generate a set of questions and answers from the prompt, and
output images are scored based on whether these answers extracted with a visual
question answering model are consistent with the prompt-based answers. This
kind of evaluation is naturally dependent on the quality of the underlying QG
and QA models. We identify and address several reliability challenges in
existing QG/A work: (a) QG questions should respect the prompt (avoiding
hallucinations, duplications, and omissions) and (b) VQA answers should be
consistent (not asserting that there is no motorcycle in an image while also
claiming the motorcycle is blue). We address these issues with Davidsonian
Scene Graph (DSG), an empirically grounded evaluation framework inspired by
formal semantics. DSG is an automatic, graph-based QG/A that is modularly
implemented to be adaptable to any QG/A module. DSG produces atomic and unique
questions organized in dependency graphs, which (i) ensure appropriate semantic
coverage and (ii) sidestep inconsistent answers. With extensive experimentation
and human evaluation on a range of model configurations (LLM, VQA, and T2I), we
empirically demonstrate that DSG addresses the challenges noted above. Finally,
we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060
prompts, covering a wide range of fine-grained semantic categories with a
balanced distribution. We will release the DSG-1k prompts and the corresponding
DSG questions.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18236" title="Abstract">arXiv:2310.18236</a> [<a href="/pdf/2310.18236" title="Download PDF">pdf</a>, <a href="/format/2310.18236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Re-sampling Helps for Long-Tail Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiang-Xin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yuke Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu-Feng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Long-tail learning has received significant attention in recent years due to
the challenge it poses with extremely imbalanced datasets. In these datasets,
only a few classes (known as the head classes) have an adequate number of
training samples, while the rest of the classes (known as the tail classes) are
infrequent in the training data. Re-sampling is a classical and widely used
approach for addressing class imbalance issues. Unfortunately, recent studies
claim that re-sampling brings negligible performance improvements in modern
long-tail learning tasks. This paper aims to investigate this phenomenon
systematically. Our research shows that re-sampling can considerably improve
generalization when the training images do not contain semantically irrelevant
contexts. In other scenarios, however, it can learn unexpected spurious
correlations between irrelevant contexts and target labels. We design
experiments on two homogeneous datasets, one containing irrelevant context and
the other not, to confirm our findings. To prevent the learning of spurious
correlations, we propose a new context shift augmentation module that generates
diverse training images for the tail class by maintaining a context bank
extracted from the head-class images. Experiments demonstrate that our proposed
module can boost the generalization and outperform other approaches, including
class-balanced re-sampling, decoupled classifier re-training, and data
augmentation methods. The source code is available at
https://www.lamda.nju.edu.cn/code_CSA.ashx.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18237" title="Abstract">arXiv:2310.18237</a> [<a href="/pdf/2310.18237" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI Model for Artistic Style Transfer Using Convolutional  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miah%2C+J">Jonayet Miah</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+D+M">Duc M Cao</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M+A">Md Abu Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Haque%2C+M+S">Md. Sabbirul Haque</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Artistic style transfer, a captivating application of generative artificial
intelligence, involves fusing the content of one image with the artistic style
of another to create unique visual compositions. This paper presents a
comprehensive overview of a novel technique for style transfer using
Convolutional Neural Networks (CNNs). By leveraging deep image representations
learned by CNNs, we demonstrate how to separate and manipulate image content
and style, enabling the synthesis of high-quality images that combine content
and style in a harmonious manner. We describe the methodology, including
content and style representations, loss computation, and optimization, and
showcase experimental results highlighting the effectiveness and versatility of
the approach across different styles and content
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18239" title="Abstract">arXiv:2310.18239</a> [<a href="/pdf/2310.18239" title="Download PDF">pdf</a>, <a href="/format/2310.18239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuning Language Models Using Formal Methods Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yunhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+N+P">Neel P. Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Ingebrand%2C+T">Tyler Ingebrand</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+W">William Ward</a>, 
<a href="/search/cs?searchtype=author&query=Carr%2C+S">Steven Carr</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Although pre-trained language models encode generic knowledge beneficial for
planning and control, they may fail to generate appropriate control policies
for domain-specific tasks. Existing fine-tuning methods use human feedback to
address this limitation, however, sourcing human feedback is labor intensive
and costly. We present a fully automated approach to fine-tune pre-trained
language models for applications in autonomous systems, bridging the gap
between generic knowledge and domain-specific requirements while reducing cost.
The method synthesizes automaton-based controllers from pre-trained models
guided by natural language task descriptions. These controllers are verifiable
against independently provided specifications within a world model, which can
be abstract or obtained from a high-fidelity simulator. Controllers with high
compliance with the desired specifications receive higher ranks, guiding the
iterative fine-tuning process. We provide quantitative evidences, primarily in
autonomous driving, to demonstrate the method's effectiveness across multiple
tasks. The results indicate an improvement in percentage of specifications
satisfied by the controller from 60% to 90%.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18240" title="Abstract">arXiv:2310.18240</a> [<a href="/pdf/2310.18240" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proxy Design: A Method for Involving Proxy Users to Speak on Behalf of  Vulnerable or Unreachable Users in Co-Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islind%2C+A+S">Anna Sigridur Islind</a>, 
<a href="/search/cs?searchtype=author&query=Lundin%2C+J">Johan Lundin</a>, 
<a href="/search/cs?searchtype=author&query=Cerna%2C+K">Katerina Cerna</a>, 
<a href="/search/cs?searchtype=author&query=Lindroth%2C+T">Tomas Lindroth</a>, 
<a href="/search/cs?searchtype=author&query=%C3%85keflo%2C+L">Linda &#xc5;keflo</a>, 
<a href="/search/cs?searchtype=author&query=Steineck%2C+G">Gunnar Steineck</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Technology and People, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Designing digital artifacts is not a linear, straightforward process. This is
particularly true when applying a user-centered design approach, or co-design,
with users who are unable to participate in the design process. Although the
reduced participation of a particular user group may harm the end result, the
literature on solving this issue is sparse. In this article, proxy design is
outlined as a method for involving a user group as proxy users to speak on
behalf of a group that is difficult to reach. We present a design ethnography
spanning three years at a cancer rehabilitation clinic, where digital artifacts
were designed to be used collaboratively by nurses and patients. The empirical
data were analyzed using content analysis and consisted of 20 observation days
at the clinic, six proxy design workshops, 21 telephone consultations between
patients and nurses, and log data from the digital artifact. We show that
simulated consultations, with nurses roleplaying as proxies for patients
ignited and initiated the design process and enabled an efficient in-depth
understanding of patients. Moreover, we reveal how proxy design as a method
further expanded the design. We illustrate: (1) proxy design as a method for
initiating design, (2) proxy design as an embedded element in co-design and (3)
six design guidelines that should be considered when engaging in proxy design.
The main contribution is the conceptualization of proxy design as a method that
can ignite and initiate the co-design process when important users are
unreachable, vulnerable or unable to represent themselves in the co-design
process. Based on the empirical findings from a design ethnography that
involved nurses as proxy users speaking on behalf of patients, the article
shows that roleplaying in proxy design is a fitting way of initiating the
design process, outlining proxy design as an embedded element of co-design.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18241" title="Abstract">arXiv:2310.18241</a> [<a href="/pdf/2310.18241" title="Download PDF">pdf</a>, <a href="/format/2310.18241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3b1;$-Mutual Information: A Tunable Privacy Measure for Privacy  Protection in Data Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asl%2C+M+J">MirHamed Jafarzadeh Asl</a>, 
<a href="/search/cs?searchtype=author&query=Shateri%2C+M">Mohammadhadi Shateri</a>, 
<a href="/search/cs?searchtype=author&query=Labeau%2C+F">Fabrice Labeau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 22nd IEEE International Conference on Machine Learning and Applications (ICMLA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper adopts Arimoto's $\alpha$-Mutual Information as a tunable privacy
measure, in a privacy-preserving data release setting that aims to prevent
disclosing private data to adversaries. By fine-tuning the privacy metric, we
demonstrate that our approach yields superior models that effectively thwart
attackers across various performance dimensions. We formulate a general
distortion-based mechanism that manipulates the original data to offer privacy
protection. The distortion metrics are determined according to the data
structure of a specific experiment. We confront the problem expressed in the
formulation by employing a general adversarial deep learning framework that
consists of a releaser and an adversary, trained with opposite goals. This
study conducts empirical experiments on images and time-series data to verify
the functionality of $\alpha$-Mutual Information. We evaluate the
privacy-utility trade-off of customized models and compare them to mutual
information as the baseline measure. Finally, we analyze the consequence of an
attacker's access to side information about private data and witness that
adapting the privacy measure results in a more refined model than the
state-of-the-art in terms of resiliency against side information.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18244" title="Abstract">arXiv:2310.18244</a> [<a href="/pdf/2310.18244" title="Download PDF">pdf</a>, <a href="/format/2310.18244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of the Evidence for Existential Risk from AI via Misaligned  Power-Seeking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hadshar%2C+R">Rose Hadshar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose existential risks. This paper
reviews the evidence for existential risks from AI via misalignment, where AI
systems develop goals misaligned with human values, and power-seeking, where
misaligned AIs actively seek power. The review examines empirical findings,
conceptual arguments and expert opinion relating to specification gaming, goal
misgeneralization, and power-seeking. The current state of the evidence is
found to be concerning but inconclusive regarding the existence of extreme
forms of misaligned power-seeking. Strong empirical evidence of specification
gaming combined with strong conceptual evidence for power-seeking make it
difficult to dismiss the possibility of existential risk from misaligned
power-seeking. On the other hand, to date there are no public empirical
examples of misaligned power-seeking in AI systems, and so arguments that
future systems will pose an existential risk remain somewhat speculative. Given
the current state of the evidence, it is hard to be extremely confident either
that misaligned power-seeking poses a large existential risk, or that it poses
no existential risk. The fact that we cannot confidently rule out existential
risk from AI via misaligned power-seeking is cause for serious concern.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18247" title="Abstract">arXiv:2310.18247</a> [<a href="/pdf/2310.18247" title="Download PDF">pdf</a>, <a href="/format/2310.18247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Data Augmentation for Offline Reinforcement Learning and  Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corrado%2C+N+E">Nicholas E. Corrado</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yuxiao Qu</a>, 
<a href="/search/cs?searchtype=author&query=Balis%2C+J+U">John U. Balis</a>, 
<a href="/search/cs?searchtype=author&query=Labiosa%2C+A">Adam Labiosa</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+J+P">Josiah P. Hanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Learning from demonstration (LfD) is a popular technique that uses expert
demonstrations to learn robot control policies. However, the difficulty in
acquiring expert-quality demonstrations limits the applicability of LfD
methods: real-world data collection is often costly, and the quality of the
demonstrations depends greatly on the demonstrator's abilities and safety
concerns. A number of works have leveraged data augmentation (DA) to
inexpensively generate additional demonstration data, but most DA works
generate augmented data in a random fashion and ultimately produce highly
suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a
human-guided DA framework that generates expert-quality augmented data. The key
insight of GuDA is that while it may be difficult to demonstrate the sequence
of actions required to produce expert data, a user can often easily identify
when an augmented trajectory segment represents task progress. Thus, the user
can impose a series of simple rules on the DA process to automatically generate
augmented samples that approximate expert behavior. To extract a policy from
GuDA, we use off-the-shelf offline reinforcement learning and behavior cloning
algorithms. We evaluate GuDA on a physical robot soccer task as well as
simulated D4RL navigation tasks, a simulated autonomous driving task, and a
simulated soccer task. Empirically, we find that GuDA enables learning from a
small set of potentially suboptimal demonstrations and substantially
outperforms a DA strategy that samples augmented data randomly.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18251" title="Abstract">arXiv:2310.18251</a> [<a href="/pdf/2310.18251" title="Download PDF">pdf</a>, <a href="/format/2310.18251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Self-Supervised Approach to Land Cover Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moore%2C+C">Charles Moore</a>, 
<a href="/search/cs?searchtype=author&query=Hester%2C+D">Dakota Hester</a> (Mississippi State University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Land use/land cover change (LULC) maps are integral resources in earth
science and agricultural research. Due to the nature of such maps, the creation
of LULC maps is often constrained by the time and human resources necessary to
accurately annotate satellite imagery and remote sensing data. While computer
vision models that perform semantic segmentation to create detailed labels from
such data are not uncommon, litle research has been done on self-supervised and
unsupervised approaches to labelling LULC maps without the use of ground-truth
masks. Here, we demonstrate a self-supervised method of land cover segmentation
that has no need for high-quality ground truth labels. The proposed deep
learning employs a frozen pre-trained ViT backbone transferred from DINO in a
STEGO architecture and is fine-tuned using a custom dataset consisting of very
high resolution (VHR) sattelite imagery. After only 10 epochs of fine-tuning,
an accuracy of roughly 52% was observed across 5 samples, signifying the
feasibility of self-supervised models for the automated labelling of VHR LULC
maps.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18257" title="Abstract">arXiv:2310.18257</a> [<a href="/pdf/2310.18257" title="Download PDF">pdf</a>, <a href="/format/2310.18257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIM-GAN-based Anomaly Detection for Multivariate Time Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhicheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Donghong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongcai Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages,6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The loss function of Generative adversarial network(GAN) is an important
factor that affects the quality and diversity of the generated samples for
anomaly detection. In this paper, we propose an unsupervised multiple time
series anomaly detection algorithm based on the GAN with message importance
measure(MIM-GAN). In particular, the time series data is divided into
subsequences using a sliding window. Then a generator and a discriminator
designed based on the Long Short-Term Memory (LSTM) are employed to capture the
temporal correlations of the time series data. To avoid the local optimal
solution of loss function and the model collapse, we introduce an exponential
information measure into the loss function of GAN. Additionally, a discriminant
reconstruction score consisting on discrimination and reconstruction loss is
taken into account. The global optimal solution for the loss function is
derived and the model collapse is proved to be avoided in our proposed
MIM-GAN-based anomaly detection algorithm. Experimental results show that the
proposed MIM-GAN-based anomaly detection algorithm has superior performance in
terms of precision, recall, and F1 score.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18260" title="Abstract">arXiv:2310.18260</a> [<a href="/pdf/2310.18260" title="Download PDF">pdf</a>, <a href="/ps/2310.18260" title="Download PostScript">ps</a>, <a href="/format/2310.18260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concepts and Paradigms for Neuromorphic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abreu%2C+S">Steven Abreu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The value of neuromorphic computers depends crucially on our ability to
program them for relevant tasks. Currently, neuromorphic computers are mostly
limited to machine learning methods adapted from deep learning. However,
neuromorphic computers have potential far beyond deep learning if we can only
make use of their computational properties to harness their full power.
Neuromorphic programming will necessarily be different from conventional
programming, requiring a paradigm shift in how we think about programming in
general. The contributions of this paper are 1) a conceptual analysis of what
"programming" means in the context of neuromorphic computers and 2) an
exploration of existing programming paradigms that are promising yet overlooked
in neuromorphic computing. The goal is to expand the horizon of neuromorphic
programming methods, thereby allowing researchers to move beyond the shackles
of current methods and explore novel directions.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18263" title="Abstract">arXiv:2310.18263</a> [<a href="/pdf/2310.18263" title="Download PDF">pdf</a>, <a href="/format/2310.18263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MalFake: A Multimodal Fake News Identification for Malayalam using  Recurrent Neural Networks and VGG-16
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sujan%2C+A+S">Adhish S. Sujan</a>, 
<a href="/search/cs?searchtype=author&query=V%2C+A">Ajitha. V</a>, 
<a href="/search/cs?searchtype=author&query=Benny%2C+A">Aleena Benny</a>, 
<a href="/search/cs?searchtype=author&query=P.%2C+A+M">Amiya M. P.</a>, 
<a href="/search/cs?searchtype=author&query=Anoop%2C+V+S">V. S. Anoop</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The amount of news being consumed online has substantially expanded in recent
years. Fake news has become increasingly common, especially in regional
languages like Malayalam, due to the rapid publication and lack of editorial
standards on some online sites. Fake news may have a terrible effect on
society, causing people to make bad judgments, lose faith in authorities, and
even engage in violent behavior. When we take into the context of India, there
are many regional languages, and fake news is spreading in every language.
Therefore, providing efficient techniques for identifying false information in
regional tongues is crucial. Until now, little to no work has been done in
Malayalam, extracting features from multiple modalities to classify fake news.
Multimodal approaches are more accurate in detecting fake news, as features
from multiple modalities are extracted to build the deep learning
classification model. As far as we know, this is the first piece of work in
Malayalam that uses multimodal deep learning to tackle false information.
Models trained with more than one modality typically outperform models taught
with only one modality. Our study in the Malayalam language utilizing
multimodal deep learning is a significant step toward more effective
misinformation detection and mitigation.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18264" title="Abstract">arXiv:2310.18264</a> [<a href="/pdf/2310.18264" title="Download PDF">pdf</a>, <a href="/format/2310.18264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Search Feasible and Infeasible Regions of Routing Problems  with Flexible Neural k-Opt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yining Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chee%2C+Y+M">Yeow Meng Chee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search
(L2S) solver for routing problems. It learns to perform flexible k-opt
exchanges based on a tailored action factorization method and a customized
recurrent dual-stream decoder. As a pioneering work to circumvent the pure
feasibility masking scheme and enable the autonomous exploration of both
feasible and infeasible regions, we then propose the Guided Infeasible Region
Exploration (GIRE) scheme, which supplements the NeuOpt policy network with
feasibility-related features and leverages reward shaping to steer
reinforcement learning more effectively. Additionally, we equip NeuOpt with
Dynamic Data Augmentation (D2A) for more diverse searches during inference.
Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated
Vehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only
significantly outstrips existing (masking-based) L2S solvers, but also
showcases superiority over the learning-to-construct (L2C) and
learning-to-predict (L2P) solvers. Notably, we offer fresh perspectives on how
neural solvers can handle VRP constraints. Our code is available:
https://github.com/yining043/NeuOpt.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18265" title="Abstract">arXiv:2310.18265</a> [<a href="/pdf/2310.18265" title="Download PDF">pdf</a>, <a href="/format/2310.18265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Semidefinite Programming for Recovering Structured  Preconditioners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jambulapati%2C+A">Arun Jambulapati</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jerry Li</a>, 
<a href="/search/cs?searchtype=author&query=Musco%2C+C">Christopher Musco</a>, 
<a href="/search/cs?searchtype=author&query=Shiragur%2C+K">Kirankumar Shiragur</a>, 
<a href="/search/cs?searchtype=author&query=Sidford%2C+A">Aaron Sidford</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+K">Kevin Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Merge of <a href="/abs/1812.06295">arXiv:1812.06295</a> and <a href="/abs/2008.01722">arXiv:2008.01722</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We develop a general framework for finding approximately-optimal
preconditioners for solving linear systems. Leveraging this framework we obtain
improved runtimes for fundamental preconditioning and linear system solving
problems including the following. We give an algorithm which, given positive
definite $\mathbf{K} \in \mathbb{R}^{d \times d}$ with
$\mathrm{nnz}(\mathbf{K})$ nonzero entries, computes an $\epsilon$-optimal
diagonal preconditioner in time $\widetilde{O}(\mathrm{nnz}(\mathbf{K}) \cdot
\mathrm{poly}(\kappa^\star,\epsilon^{-1}))$, where $\kappa^\star$ is the
optimal condition number of the rescaled matrix. We give an algorithm which,
given $\mathbf{M} \in \mathbb{R}^{d \times d}$ that is either the pseudoinverse
of a graph Laplacian matrix or a constant spectral approximation of one, solves
linear systems in $\mathbf{M}$ in $\widetilde{O}(d^2)$ time. Our diagonal
preconditioning results improve state-of-the-art runtimes of $\Omega(d^{3.5})$
attained by general-purpose semidefinite programming, and our solvers improve
state-of-the-art runtimes of $\Omega(d^{\omega})$ where $\omega &gt; 2.3$ is the
current matrix multiplication constant. We attain our results via new
algorithms for a class of semidefinite programs (SDPs) we call
matrix-dictionary approximation SDPs, which we leverage to solve an associated
problem we call matrix-dictionary recovery.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18268" title="Abstract">arXiv:2310.18268</a> [<a href="/pdf/2310.18268" title="Download PDF">pdf</a>, <a href="/format/2310.18268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PlantPlotGAN: A Physics-Informed Generative Adversarial Network for  Plant Disease Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lopes%2C+F+A">Felipe A. Lopes</a>, 
<a href="/search/cs?searchtype=author&query=Sagan%2C+V">Vasit Sagan</a>, 
<a href="/search/cs?searchtype=author&query=Esposito%2C+F">Flavio Esposito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Monitoring plantations is crucial for crop management and producing healthy
harvests. Unmanned Aerial Vehicles (UAVs) have been used to collect
multispectral images that aid in this monitoring. However, given the number of
hectares to be monitored and the limitations of flight, plant disease signals
become visually clear only in the later stages of plant growth and only if the
disease has spread throughout a significant portion of the plantation. This
limited amount of relevant data hampers the prediction models, as the
algorithms struggle to generalize patterns with unbalanced or unrealistic
augmented datasets effectively. To address this issue, we propose PlantPlotGAN,
a physics-informed generative model capable of creating synthetic multispectral
plot images with realistic vegetation indices. These indices served as a proxy
for disease detection and were used to evaluate if our model could help
increase the accuracy of prediction models. The results demonstrate that the
synthetic imagery generated from PlantPlotGAN outperforms state-of-the-art
methods regarding the Fr\'echet inception distance. Moreover, prediction models
achieve higher accuracy metrics when trained with synthetic and original
imagery for earlier plant disease detection compared to the training processes
based solely on real imagery.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18273" title="Abstract">arXiv:2310.18273</a> [<a href="/pdf/2310.18273" title="Download PDF">pdf</a>, <a href="/format/2310.18273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moments for Perceptive Narration Analysis Through the Emotional  Attachment of Audience to Discourse and Story
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruins%2C+G">Gary Bruins</a>, 
<a href="/search/cs?searchtype=author&query=Akleman%2C+E">Ergun Akleman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Digital Libraries (cs.DL)

</div>
<p class="mathjax">In this work, our goal is to develop a theoretical framework that can
eventually be used for analyzing the effectiveness of visual stories such as
feature films to comic books. To develop this theoretical framework, we
introduce a new story element called moments. Our conjecture is that any linear
story such as the story of a feature film can be decomposed into a set of
moments that follow each other. Moments are defined as the perception of the
actions, interactions, and expressions of all characters or a single character
during a given time period. We categorize the moments into two major types:
story moments and discourse moments. Each type of moment can further be
classified into three types, which we call universal storytelling moments. We
believe these universal moments foster or deteriorate the emotional attachment
of the audience to a particular character or the story. We present a
methodology to catalog the occurrences of these universal moments as they are
found in the story. The cataloged moments can be represented using curves or
color strips. Therefore, we can visualize a character's journey through the
story as either a 3D curve or a color strip. We also demonstrated that both
story and discourse moments can be transformed into one lump-sum attraction
parameter. The attraction parameter in time provides a function that can be
plotted graphically onto a timeline illustrating changes in the emotional
attachment of audience to a character or the story. By inspecting these
functions the story analyst can analytically decipher the moments in the story
where the attachment is being established, maintained, strengthened, or
conversely where it is languishing.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18274" title="Abstract">arXiv:2310.18274</a> [<a href="/pdf/2310.18274" title="Download PDF">pdf</a>, <a href="/format/2310.18274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LipSim: A Provably Robust Perceptual Similarity Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghazanfari%2C+S">Sara Ghazanfari</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+A">Alexandre Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+P">Prashanth Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Khorrami%2C+F">Farshad Khorrami</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent years have seen growing interest in developing and applying perceptual
similarity metrics. Research has shown the superiority of perceptual metrics
over pixel-wise metrics in aligning with human perception and serving as a
proxy for the human visual system. On the other hand, as perceptual metrics
rely on neural networks, there is a growing concern regarding their resilience,
given the established vulnerability of neural networks to adversarial attacks.
It is indeed logical to infer that perceptual metrics may inherit both the
strengths and shortcomings of neural networks. In this work, we demonstrate the
vulnerability of state-of-the-art perceptual similarity metrics based on an
ensemble of ViT-based feature extractors to adversarial attacks. We then
propose a framework to train a robust perceptual similarity metric called
LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging
1-Lipschitz neural networks as the backbone, LipSim provides guarded areas
around each data point and certificates for all perturbations within an
$\ell_2$ ball. Finally, a comprehensive set of experiments shows the
performance of LipSim in terms of natural and certified scores and on the image
retrieval application. The code is available at
https://github.com/SaraGhazanfari/LipSim.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18279" title="Abstract">arXiv:2310.18279</a> [<a href="/pdf/2310.18279" title="Download PDF">pdf</a>, <a href="/format/2310.18279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FOUND: Foot Optimization with Uncertain Normals for Surface Deformation  Using Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boyne%2C+O">Oliver Boyne</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+G">Gwangbin Bae</a>, 
<a href="/search/cs?searchtype=author&query=Charles%2C+J">James Charles</a>, 
<a href="/search/cs?searchtype=author&query=Cipolla%2C+R">Roberto Cipolla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Surface reconstruction from multi-view images is a challenging task, with
solutions often requiring a large number of sampled images with high overlap.
We seek to develop a method for few-view reconstruction, for the case of the
human foot. To solve this task, we must extract rich geometric cues from RGB
images, before carefully fusing them into a final 3D object. Our FOUND approach
tackles this, with 4 main contributions: (i) SynFoot, a synthetic dataset of
50,000 photorealistic foot images, paired with ground truth surface normals and
keypoints; (ii) an uncertainty-aware surface normal predictor trained on our
synthetic dataset; (iii) an optimization scheme for fitting a generative foot
model to a series of images; and (iv) a benchmark dataset of calibrated images
and high resolution ground truth geometry. We show that our normal predictor
outperforms all off-the-shelf equivalents significantly on real images, and our
optimization scheme outperforms state-of-the-art photogrammetry pipelines,
especially for a few-view setting. We release our synthetic dataset and
baseline 3D scans to the research community.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18283" title="Abstract">arXiv:2310.18283</a> [<a href="/pdf/2310.18283" title="Download PDF">pdf</a>, <a href="/format/2310.18283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Development and Characteristics of a Highly Biomimetic Robotic Shoulder  Through Bionics-Inspired Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haosen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guowu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Lei Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper critically analyzes conventional and biomimetic robotic arms,
underscoring the trade-offs between size, motion range, and load capacity in
current biomimetic models. By delving into the human shoulder's mechanical
intelligence, particularly the glenohumeral joint's intricate features such as
its unique ball-and-socket structure and self-locking mechanism, we pinpoint
innovations that bolster both stability and mobility while maintaining
compactness. To substantiate these insights, we present a groundbreaking
biomimetic robotic glenohumeral joint that authentically mirrors human
musculoskeletal elements, from ligaments to tendons, integrating the biological
joint's mechanical intelligence. Our exhaustive simulations and tests reveal
enhanced flexibility and load capacity for the robotic joint. The advanced
robotic arm demonstrates notable capabilities, including a significant range of
motions and a 4 kg payload capacity, even exerting over 1.5 Nm torque. This
study not only confirms the human shoulder joint's mechanical innovations but
also introduces a pioneering design for a next-generation biomimetic robotic
arm, setting a new benchmark in robotic technology.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18285" title="Abstract">arXiv:2310.18285</a> [<a href="/pdf/2310.18285" title="Download PDF">pdf</a>, <a href="/format/2310.18285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Federated Learning with Group-Aware Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Wenlong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Transformers have achieved remarkable success in various machine-learning
tasks, prompting their widespread adoption. In this paper, we explore their
application in the context of federated learning (FL), with a particular focus
on heterogeneous scenarios where individual clients possess diverse local
datasets. To meet the computational and communication demands of FL, we
leverage pre-trained Transformers and use an efficient prompt-tuning strategy.
Our strategy introduces the concept of learning both shared and group prompts,
enabling the acquisition of universal knowledge and group-specific knowledge
simultaneously. Additionally, a prompt selection module assigns personalized
group prompts to each input, aligning the global model with the data
distribution of each client. This approach allows us to train a single global
model that can automatically adapt to various local client data distributions
without requiring local fine-tuning. In this way, our proposed method
effectively bridges the gap between global and personalized local models in
Federated Learning and surpasses alternative approaches that lack the
capability to adapt to previously unseen clients. The effectiveness of our
approach is rigorously validated through extensive experimentation and ablation
studies.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18286" title="Abstract">arXiv:2310.18286</a> [<a href="/pdf/2310.18286" title="Download PDF">pdf</a>, <a href="/format/2310.18286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport for Treatment Effect Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhichao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jiajun Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianqiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Q">Quanyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhenhua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as NeurIPS 2023 Poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Estimating conditional average treatment effect from observational data is
highly challenging due to the existence of treatment selection bias. Prevalent
methods mitigate this issue by aligning distributions of different treatment
groups in the latent space. However, there are two critical problems that these
methods fail to address: (1) mini-batch sampling effects (MSE), which causes
misalignment in non-ideal mini-batches with outcome imbalance and outliers; (2)
unobserved confounder effects (UCE), which results in inaccurate discrepancy
calculation due to the neglect of unobserved confounders. To tackle these
problems, we propose a principled approach named Entire Space CounterFactual
Regression (ESCFR), which is a new take on optimal transport in the context of
causality. Specifically, based on the framework of stochastic optimal
transport, we propose a relaxed mass-preserving regularizer to address the MSE
issue and design a proximal factual outcome regularizer to handle the UCE
issue. Extensive experiments demonstrate that our proposed ESCFR can
successfully tackle the treatment selection bias and achieve significantly
better performance than state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18288" title="Abstract">arXiv:2310.18288</a> [<a href="/pdf/2310.18288" title="Download PDF">pdf</a>, <a href="/format/2310.18288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sustainable Concrete via Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ament%2C+S">Sebastian Ament</a>, 
<a href="/search/cs?searchtype=author&query=Witte%2C+A">Andrew Witte</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+N">Nishant Garg</a>, 
<a href="/search/cs?searchtype=author&query=Kusuma%2C+J">Julius Kusuma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Eight percent of global carbon dioxide emissions can be attributed to the
production of cement, the main component of concrete, which is also the
dominant source of CO2 emissions in the construction of data centers. The
discovery of lower-carbon concrete formulae is therefore of high significance
for sustainability. However, experimenting with new concrete formulae is time
consuming and labor intensive, as one usually has to wait to record the
concrete's 28-day compressive strength, a quantity whose measurement can by its
definition not be accelerated. This provides an opportunity for experimental
design methodology like Bayesian Optimization (BO) to accelerate the search for
strong and sustainable concrete formulae. Herein, we 1) propose modeling steps
that make concrete strength amenable to be predicted accurately by a Gaussian
process model with relatively few measurements, 2) formulate the search for
sustainable concrete as a multi-objective optimization problem, and 3) leverage
the proposed model to carry out multi-objective BO with real-world strength
measurements of the algorithmically proposed mixes. Our experimental results
show improved trade-offs between the mixtures' global warming potential (GWP)
and their associated compressive strengths, compared to mixes based on current
industry practices.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18290" title="Abstract">arXiv:2310.18290</a> [<a href="/pdf/2310.18290" title="Download PDF">pdf</a>, <a href="/format/2310.18290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Approach to Automatically generating Riddles aiding Concept  Attainment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parasa%2C+N+S">Niharika Sri Parasa</a>, 
<a href="/search/cs?searchtype=author&query=Diwan%2C+C">Chaitali Diwan</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+S">Srinath Srinivasa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">One of the primary challenges in online learning environments, is to retain
learner engagement. Several different instructional strategies are proposed
both in online and offline environments to enhance learner engagement. The
Concept Attainment Model is one such instructional strategy that focuses on
learners acquiring a deeper understanding of a concept rather than just its
dictionary definition. This is done by searching and listing the properties
used to distinguish examples from non-examples of various concepts. Our work
attempts to apply the Concept Attainment Model to build conceptual riddles, to
deploy over online learning environments. The approach involves creating
factual triples from learning resources, classifying them based on their
uniqueness to a concept into `Topic Markers' and `Common', followed by
generating riddles based on the Concept Attainment Model's format and capturing
all possible solutions to those riddles. The results obtained from the human
evaluation of riddles prove encouraging.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18291" title="Abstract">arXiv:2310.18291</a> [<a href="/pdf/2310.18291" title="Download PDF">pdf</a>, <a href="/format/2310.18291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing GAN Training Instabilities via Tunable Classification Losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Welfert%2C+M">Monica Welfert</a>, 
<a href="/search/cs?searchtype=author&query=Kurri%2C+G+R">Gowtham R. Kurri</a>, 
<a href="/search/cs?searchtype=author&query=Otstot%2C+K">Kyle Otstot</a>, 
<a href="/search/cs?searchtype=author&query=Sankar%2C+L">Lalitha Sankar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2302.14320">arXiv:2302.14320</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">Generative adversarial networks (GANs), modeled as a zero-sum game between a
generator (G) and a discriminator (D), allow generating synthetic data with
formal guarantees. Noting that D is a classifier, we begin by reformulating the
GAN value function using class probability estimation (CPE) losses. We prove a
two-way correspondence between CPE loss GANs and $f$-GANs which minimize
$f$-divergences. We also show that all symmetric $f$-divergences are equivalent
in convergence. In the finite sample and model capacity setting, we define and
obtain bounds on estimation and generalization errors. We specialize these
results to $\alpha$-GANs, defined using $\alpha$-loss, a tunable CPE loss
family parametrized by $\alpha\in(0,\infty]$. We next introduce a class of
dual-objective GANs to address training instabilities of GANs by modeling each
player's objective using $\alpha$-loss to obtain $(\alpha_D,\alpha_G)$-GANs. We
show that the resulting non-zero sum game simplifies to minimizing an
$f$-divergence under appropriate conditions on $(\alpha_D,\alpha_G)$.
Generalizing this dual-objective formulation using CPE losses, we define and
obtain upper bounds on an appropriately defined estimation error. Finally, we
highlight the value of tuning $(\alpha_D,\alpha_G)$ in alleviating training
instabilities for the synthetic 2D Gaussian mixture ring as well as the large
publicly available Celeb-A and LSUN Classroom image datasets.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18293" title="Abstract">arXiv:2310.18293</a> [<a href="/pdf/2310.18293" title="Download PDF">pdf</a>, <a href="/format/2310.18293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Always Clear Days: Degradation Type and Severity Aware All-In-One  Adverse Weather Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu-Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+S">Soo-Chang Pei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">All-in-one adverse weather removal is an emerging topic on image restoration,
which aims to restore multiple weather degradation in an unified model, and the
challenging are twofold. First, discovering and handling the property of
multi-domain in target distribution formed by multiple weather conditions.
Second, design efficient and effective operations for different degradation
types. To address this problem, most prior works focus on the multi-domain
caused by weather type. Inspired by inter\&amp;intra-domain adaptation literature,
we observed that not only weather type but also weather severity introduce
multi-domain within each weather type domain, which is ignored by previous
methods, and further limit their performance. To this end, we proposed a
degradation type and severity aware model, called \textbf{UtilityIR}, for blind
all-in-one bad weather image restoration. To extract weather information from
single image, we proposed a novel Marginal Quality Ranking Loss (MQRL) and
utilized Contrastive Loss (CL) to guide weather severity and type extraction,
and leverage a bag of novel techniques such as Multi-Head Cross Attention
(MHCA) and Local-Global Adaptive Instance Normalization (LG-AdaIN) to
efficiently restore spatial varying weather degradation. The proposed method
can significantly outperform the SOTA methods subjectively and objectively on
different weather restoration tasks with a large margin, and enjoy less model
parameters. Proposed method even can restore \textbf{unseen} domain combined
multiple degradation images, and modulating restoration level. Implementation
code will be available at
{https://github.com/fordevoted/UtilityIR}{\textit{this repository}}
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18297" title="Abstract">arXiv:2310.18297</a> [<a href="/pdf/2310.18297" title="Download PDF">pdf</a>, <a href="/format/2310.18297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Clustering Conditioned on Text Criteria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S">Sehyun Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaeseung Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minkyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaewoong Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+E+K">Ernest K. Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Classical clustering methods do not provide users with direct control of the
clustering results, and the clustering results may not be consistent with the
relevant criterion that a user has in mind. In this work, we present a new
methodology for performing image clustering based on user-specified text
criteria by leveraging modern vision-language models and large language models.
We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and
it represents a different paradigm of image clustering. IC$|$TC requires a
minimal and practical degree of human intervention and grants the user
significant control over the clustering results in return. Our experiments show
that IC$|$TC can effectively cluster images with various criteria, such as
human action, physical location, or the person's mood, while significantly
outperforming baselines.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18299" title="Abstract">arXiv:2310.18299</a> [<a href="/pdf/2310.18299" title="Download PDF">pdf</a>, <a href="/format/2310.18299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Performance of a Biomimetic Robotic Elbow-and-Forearm  System Through Bionics-Inspired Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haosen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guowu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Lei Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper delineates the formulation and verification of an innovative
robotic forearm and elbow design, mirroring the intricate biomechanics of human
skeletal and ligament systems. Conventional robotic models often undervalue the
substantial function of soft tissues, leading to a compromise between
compactness, safety, stability, and range of motion. In contrast, this study
proposes a holistic replication of biological joints, encompassing bones,
cartilage, ligaments, and tendons, culminating in a biomimetic robot. The
research underscores the compact and stable structure of the human forearm,
attributable to a tri-bone framework and diverse soft tissues. The methodology
involves exhaustive examinations of human anatomy, succeeded by a theoretical
exploration of the contribution of soft tissues to the stability of the
prototype. The evaluation results unveil remarkable parallels between the range
of motion of the robotic joints and their human counterparts. The robotic elbow
emulates 98.8% of the biological elbow's range of motion, with high torque
capacities of 11.25 Nm (extension) and 24 Nm (flexion). Similarly, the robotic
forearm achieves 58.6% of the human forearm's rotational range, generating
substantial output torques of 14 Nm (pronation) and 7.8 Nm (supination).
Moreover, the prototype exhibits significant load-bearing abilities, resisting
a 5kg dumbbell load without substantial displacement. It demonstrates a payload
capacity exceeding 4kg and rapid action capabilities, such as lifting a 2kg
dumbbell at a speed of 0.74Hz and striking a ping-pong ball at an end-effector
speed of 3.2 m/s. This research underscores that a detailed anatomical study
can address existing robotic design obstacles, optimize performance and
anthropomorphic resemblance, and reaffirm traditional anatomical principles.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18301" title="Abstract">arXiv:2310.18301</a> [<a href="/pdf/2310.18301" title="Download PDF">pdf</a>, <a href="/format/2310.18301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Motion Planning for Autonomous Vehicles with Joint  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Veer%2C+S">Sushant Veer</a>, 
<a href="/search/cs?searchtype=author&query=Karkus%2C+P">Peter Karkus</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">In highly interactive driving scenarios, the actions of one agent greatly
influences those of its neighbors. Planning safe motions for autonomous
vehicles in such interactive environments, therefore, requires reasoning about
the impact of the ego's intended motion plan on nearby agents' behavior.
Deep-learning-based models have recently achieved great success in trajectory
prediction and many models in the literature allow for ego-conditioned
prediction. However, leveraging ego-conditioned prediction remains challenging
in downstream planning due to the complex nature of neural networks, limiting
the planner structure to simple ones, e.g., sampling-based planner. Despite
their ability to generate fine-grained high-quality motion plans, it is
difficult for gradient-based planning algorithms, such as model predictive
control (MPC), to leverage ego-conditioned prediction due to their iterative
nature and need for gradient. We present Interactive Joint Planning (IJP) that
bridges MPC with learned prediction models in a computationally scalable manner
to provide us the best of both the worlds. In particular, IJP jointly optimizes
over the behavior of the ego and the surrounding agents and leverages
deep-learned prediction models as prediction priors that the join trajectory
optimization tries to stay close to. Furthermore, by leveraging homotopy
classes, our joint optimizer searches over diverse motion plans to avoid
getting stuck at local minima. Closed-loop simulation result shows that IJP
significantly outperforms the baselines that are either without joint
optimization or running sampling-based planning.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18303" title="Abstract">arXiv:2310.18303</a> [<a href="/pdf/2310.18303" title="Download PDF">pdf</a>, <a href="/format/2310.18303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socially Cognizant Robotics for a Technology Enhanced Society
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dana%2C+K+J">Kristin J. Dana</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+C">Clinton Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Bekris%2C+K">Kostas Bekris</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+J">Jacob Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+M">Matthew Stone</a>, 
<a href="/search/cs?searchtype=author&query=Hemmer%2C+P">Pernille Hemmer</a>, 
<a href="/search/cs?searchtype=author&query=Mazzeo%2C+A">Aaron Mazzeo</a>, 
<a href="/search/cs?searchtype=author&query=Salzman%2C+H">Hal Salzman</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jingang Yi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Emerging applications of robotics, and concerns about their impact, require
the research community to put human-centric objectives front-and-center. To
meet this challenge, we advocate an interdisciplinary approach, socially
cognizant robotics, which synthesizes technical and social science methods. We
argue that this approach follows from the need to empower stakeholder
participation (from synchronous human feedback to asynchronous societal
assessment) in shaping AI-driven robot behavior at all levels, and leads to a
range of novel research perspectives and problems both for improving robots'
interactions with individuals and impacts on society. Drawing on these
arguments, we develop best practices for socially cognizant robot design that
balance traditional technology-based metrics (e.g. efficiency, precision and
accuracy) with critically important, albeit challenging to measure, human and
society-based metrics.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18304" title="Abstract">arXiv:2310.18304</a> [<a href="/pdf/2310.18304" title="Download PDF">pdf</a>, <a href="/format/2310.18304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stability Principle for Learning under Non-Stationarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chengpiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaizheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We develop a versatile framework for statistical learning in non-stationary
environments. In each time period, our approach applies a stability principle
to select a look-back window that maximizes the utilization of historical data
while keeping the cumulative bias within an acceptable range relative to the
stochastic error. Our theory showcases the adaptability of this approach to
unknown non-stationarity. The regret bound is minimax optimal up to logarithmic
factors when the population losses are strongly convex, or Lipschitz only. At
the heart of our analysis lie two novel components: a measure of similarity
between functions and a segmentation technique for dividing the non-stationary
data sequence into quasi-stationary pieces.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18308" title="Abstract">arXiv:2310.18308</a> [<a href="/pdf/2310.18308" title="Download PDF">pdf</a>, <a href="/format/2310.18308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katara%2C+P">Pushkal Katara</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Z">Zhou Xian</a>, 
<a href="/search/cs?searchtype=author&query=Fragkiadaki%2C+K">Katerina Fragkiadaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generalist robot manipulators need to learn a wide variety of manipulation
skills across diverse environments. Current robot training pipelines rely on
humans to provide kinesthetic demonstrations or to program simulation
environments and to code up reward functions for reinforcement learning. Such
human involvement is an important bottleneck towards scaling up robot learning
across diverse tasks and environments. We propose Generation to Simulation
(Gen2Sim), a method for scaling up robot skill learning in simulation by
automating generation of 3D assets, task descriptions, task decompositions and
reward functions using large pre-trained generative models of language and
vision. We generate 3D assets for simulation by lifting open-world 2D
object-centric images to 3D using image diffusion models and querying LLMs to
determine plausible physics parameters. Given URDF files of generated and
human-developed assets, we chain-of-thought prompt LLMs to map these to
relevant task descriptions, temporal decompositions, and corresponding python
reward functions for reinforcement learning. We show Gen2Sim succeeds in
learning policies for diverse long horizon tasks, where reinforcement learning
with non temporally decomposed reward functions fails. Gen2Sim provides a
viable path for scaling up reinforcement learning for robot manipulators in
simulation, both by diversifying and expanding task and environment
development, and by facilitating the discovery of reinforcement-learned
behaviors through temporal task decomposition in RL. Our work contributes
hundreds of simulated assets, tasks and demonstrations, taking a step towards
fully autonomous robotic manipulation skill acquisition in simulation.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18313" title="Abstract">arXiv:2310.18313</a> [<a href="/pdf/2310.18313" title="Download PDF">pdf</a>, <a href="/format/2310.18313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FP8-LM: Training FP8 Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Houwen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yixuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guoshuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yifan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+B">Bolin Ni</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jingcheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruihang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miaosen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+J">Jia Ning</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+J">Joe Chau</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In this paper, we explore FP8 low-bit data formats for efficient training of
large language models (LLMs). Our key insight is that most variables, such as
gradients and optimizer states, in LLM training can employ low-precision data
formats without compromising model accuracy and requiring no changes to
hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision
framework for training LLMs. This framework offers three levels of FP8
utilization to streamline mixed-precision and distributed parallel training for
LLMs. It gradually incorporates 8-bit gradients, optimizer states, and
distributed learning in an incremental manner. Experiment results show that,
during the training of GPT-175B model on H100 GPU platform, our FP8
mixed-precision training framework not only achieved a remarkable 42% reduction
in real memory usage but also ran 64% faster than the widely adopted BF16
framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer
Engine by 17%. This largely reduces the training costs for large foundation
models. Furthermore, our FP8 mixed-precision training methodology is generic.
It can be seamlessly applied to other tasks such as LLM instruction tuning and
reinforcement learning with human feedback, offering savings in fine-tuning
expenses. Our FP8 low-precision training framework is open-sourced at
{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Mon, 30 Oct 23</h3>
<dl>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14748" title="Abstract">arXiv:2310.14748</a> (cross-list from q-fin.PM) [<a href="/pdf/2310.14748" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study of Portfolio Optimization Methods for the Indian  Stock Market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Sen%2C+J">Jaydip Sen</a>, 
<a href="/search/q-fin?searchtype=author&query=Dasgupta%2C+A">Arup Dasgupta</a>, 
<a href="/search/q-fin?searchtype=author&query=Sengupta%2C+P+P">Partha Pratim Sengupta</a>, 
<a href="/search/q-fin?searchtype=author&query=Choudhury%2C+S+R">Sayantani Roy Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the draft version of the chapter that has been accepted for publication in the edited volume titled "Data Science: Theory and Practice". The volume is edited by Jaydip Sen and Sayantani Roy Choudury and will be published by IntechOpen, London, UK. The chapter is 74 pages long and it contains 32 tables and 62 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This chapter presents a comparative study of the three portfolio optimization
methods, MVP, HRP, and HERC, on the Indian stock market, particularly focusing
on the stocks chosen from 15 sectors listed on the National Stock Exchange of
India. The top stocks of each cluster are identified based on their free-float
market capitalization from the report of the NSE published on July 1, 2022 (NSE
Website). For each sector, three portfolios are designed on stock prices from
July 1, 2019, to June 30, 2022, following three portfolio optimization
approaches. The portfolios are tested over the period from July 1, 2022, to
June 30, 2023. For the evaluation of the performances of the portfolios, three
metrics are used. These three metrics are cumulative returns, annual
volatilities, and Sharpe ratios. For each sector, the portfolios that yield the
highest cumulative return, the lowest volatility, and the maximum Sharpe Ratio
over the training and the test periods are identified.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17655" title="Abstract">arXiv:2310.17655</a> (cross-list from eess.AS) [<a href="/pdf/2310.17655" title="Download PDF">pdf</a>, <a href="/format/2310.17655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music Recommendation Based on Audio Fingerprint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ulloa%2C+D+S">Diego Salda&#xf1;a Ulloa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Information Retrieval (cs.IR); Sound (cs.SD)

</div>
<p class="mathjax">This work combined different audio features to obtain a more robust
fingerprint to be used in a music recommendation process. The combination of
these methods resulted in a high-dimensional vector. To reduce the number of
values, PCA was applied to the set of resulting fingerprints, selecting the
number of principal components that corresponded to an explained variance of
$95\%$. Finally, with these PCA-fingerprints, the similarity matrix of each
fingerprint with the entire data set was calculated. The process was applied to
200 songs from a personal music library; the songs were tagged with the
artists' corresponding genres. The recommendations (fingerprints of songs with
the closest similarity) were rated successful if the recommended songs' genre
matched the target songs' genre. With this procedure, it was possible to obtain
an accuracy of $89\%$ (successful recommendations out of total recommendation
requests).
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17657" title="Abstract">arXiv:2310.17657</a> (cross-list from eess.SP) [<a href="/pdf/2310.17657" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of  Silicon-Carbide Power MOSFET Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Spata%2C+M+O">Massimo Orazio Spata</a>, 
<a href="/search/eess?searchtype=author&query=Battiato%2C+S">Sebastiano Battiato</a>, 
<a href="/search/eess?searchtype=author&query=Ortis%2C+A">Alessandro Ortis</a>, 
<a href="/search/eess?searchtype=author&query=Rundo%2C+F">Francesco Rundo</a>, 
<a href="/search/eess?searchtype=author&query=Calabretta%2C+M">Michele Calabretta</a>, 
<a href="/search/eess?searchtype=author&query=Pino%2C+C">Carmelo Pino</a>, 
<a href="/search/eess?searchtype=author&query=Messina%2C+A">Angelo Messina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, to be published on Journal of Physics: Conference Series
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Inverse modelling with deep learning algorithms involves training deep
architecture to predict device's parameters from its static behaviour. Inverse
device modelling is suitable to reconstruct drifted physical parameters of
devices temporally degraded or to retrieve physical configuration. There are
many variables that can influence the performance of an inverse modelling
method. In this work the authors propose a deep learning method trained for
retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET
(SiC Power MOS). The SiC devices are used in applications where classical
silicon devices failed due to high-temperature or high switching capability.
The key application of SiC power devices is in the automotive field (i.e. in
the field of electrical vehicles). Due to physiological degradation or
high-stressing environment, SiC Power MOS shows a significant drift of physical
parameters which can be monitored by using inverse modelling. The aim of this
work is to provide a possible deep learning-based solution for retrieving
physical parameters of the SiC Power MOSFET. Preliminary results based on the
retrieving of channel length of the device are reported. Channel length of
power MOSFET is a key parameter involved in the static and dynamic behaviour of
the device. The experimental results reported in this work confirmed the
effectiveness of a multi-layer perceptron designed to retrieve this parameter.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17659" title="Abstract">arXiv:2310.17659</a> (cross-list from eess.SP) [<a href="/pdf/2310.17659" title="Download PDF">pdf</a>, <a href="/format/2310.17659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RTNH+: Enhanced 4D Radar Object Detection Network using Combined  CFAR-based Two-level Preprocessing and Vertical Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kong%2C+S">Seung-Hyun Kong</a>, 
<a href="/search/eess?searchtype=author&query=Paek%2C+D">Dong-Hee Paek</a>, 
<a href="/search/eess?searchtype=author&query=Cho%2C+S">Sangjae Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Arxiv preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Four-dimensional (4D) Radar is a useful sensor for 3D object detection and
the relative radial speed estimation of surrounding objects under various
weather conditions. However, since Radar measurements are corrupted with
invalid components such as noise, interference, and clutter, it is necessary to
employ a preprocessing algorithm before the 3D object detection with neural
networks. In this paper, we propose RTNH+ that is an enhanced version of RTNH,
a 4D Radar object detection network, by two novel algorithms. The first
algorithm is the combined constant false alarm rate (CFAR)-based two-level
preprocessing (CCTP) algorithm that generates two filtered measurements of
different characteristics using the same 4D Radar measurements, which can
enrich the information of the input to the 4D Radar object detection network.
The second is the vertical encoding (VE) algorithm that effectively encodes
vertical features of the road objects from the CCTP outputs. We provide details
of the RTNH+, and demonstrate that RTNH+ achieves significant performance
improvement of 10.14\% in ${{AP}_{3D}^{IoU=0.3}}$ and 16.12\% in
${{AP}_{3D}^{IoU=0.5}}$ over RTNH.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17660" title="Abstract">arXiv:2310.17660</a> (cross-list from eess.SP) [<a href="/pdf/2310.17660" title="Download PDF">pdf</a>, <a href="/format/2310.17660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Invitation to Hypercomplex Phase Retrieval: Theory and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jacome%2C+R">Roman Jacome</a>, 
<a href="/search/eess?searchtype=author&query=Mishra%2C+K+V">Kumar Vijay Mishra</a>, 
<a href="/search/eess?searchtype=author&query=Sadler%2C+B+M">Brian M. Sadler</a>, 
<a href="/search/eess?searchtype=author&query=Arguello%2C+H">Henry Arguello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Hypercomplex signal processing (HSP) provides state-of-the-art tools to
handle multidimensional signals by harnessing intrinsic correlation of the
signal dimensions through Clifford algebra. Recently, the hypercomplex
representation of the phase retrieval (PR) problem, wherein a complex-valued
signal is estimated through its intensity-only projections, has attracted
significant interest. The hypercomplex PR (HPR) arises in many optical imaging
and computational sensing applications that usually comprise quaternion and
octonion-valued signals. Analogous to the traditional PR, measurements in HPR
may involve complex, hypercomplex, Fourier, and other sensing matrices. This
set of problems opens opportunities for developing novel HSP tools and
algorithms. This article provides a synopsis of the emerging areas and
applications of HPR with a focus on optical imaging.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17661" title="Abstract">arXiv:2310.17661</a> (cross-list from eess.SP) [<a href="/pdf/2310.17661" title="Download PDF">pdf</a>, <a href="/format/2310.17661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Overview on IEEE 802.11bf: WLAN Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Du%2C+R">Rui Du</a>, 
<a href="/search/eess?searchtype=author&query=Hua%2C+H">Haocheng Hua</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+H">Hailiang Xie</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+X">Xianxin Song</a>, 
<a href="/search/eess?searchtype=author&query=Lyu%2C+Z">Zhonghao Lyu</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+M">Mengshi Hu</a>, 
<a href="/search/eess?searchtype=author&query=Narengerile">Narengerile</a>, 
<a href="/search/eess?searchtype=author&query=Xin%2C+Y">Yan Xin</a>, 
<a href="/search/eess?searchtype=author&query=McCann%2C+S">Stephen McCann</a>, 
<a href="/search/eess?searchtype=author&query=Montemurro%2C+M">Michael Montemurro</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+T+X">Tony Xiao Han</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jie Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 25 figures, this is a significant updated version of <a href="/abs/2207.04859">arXiv:2207.04859</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">With recent advancements, the wireless local area network (WLAN) or wireless
fidelity (Wi-Fi) technology has been successfully utilized to realize sensing
functionalities such as detection, localization, and recognition. However, the
WLANs standards are developed mainly for the purpose of communication, and thus
may not be able to meet the stringent requirements for emerging sensing
applications. To resolve this issue, a new Task Group (TG), namely IEEE
802.11bf, has been established by the IEEE 802.11 working group, with the
objective of creating a new amendment to the WLAN standard to meet advanced
sensing requirements while minimizing the effect on communications. This paper
provides a comprehensive overview on the up-to-date efforts in the IEEE
802.11bf TG. First, we introduce the definition of the 802.11bf amendment and
its formation and standardization timeline. Next, we discuss the WLAN sensing
use cases with the corresponding key performance indicator (KPI) requirements.
After reviewing previous WLAN sensing research based on communication-oriented
WLAN standards, we identify their limitations and underscore the practical need
for the new sensing-oriented amendment in 802.11bf. Furthermore, we discuss the
WLAN sensing framework and procedure used for measurement acquisition, by
considering both sensing at sub-7GHz and directional multi-gigabit (DMG)
sensing at 60 GHz, respectively, and address their shared features,
similarities, and differences. In addition, we present various candidate
technical features for IEEE 802.11bf, including waveform/sequence design,
feedback types, as well as quantization and compression techniques. We also
describe the methodologies and the channel modeling used by the IEEE 802.11bf
TG for evaluation. Finally, we discuss the challenges and future research
directions to motivate more research endeavors towards this field in details.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17675" title="Abstract">arXiv:2310.17675</a> (cross-list from eess.AS) [<a href="/pdf/2310.17675" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early Detection of Tuberculosis with Machine Learning Cough Audio  Analysis: Towards More Accessible Global Triaging Usage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Suda%2C+C">Chandra Suda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Tuberculosis (TB), a bacterial disease mainly affecting the lungs, is one of
the leading infectious causes of mortality worldwide. To prevent TB from
spreading within the body, which causes life-threatening complications, timely
and effective anti-TB treatment is crucial. Cough, an objective biomarker for
TB, is a triage tool that monitors treatment response and regresses with
successful therapy. Current gold standards for TB diagnosis are slow or
inaccessible, especially in rural areas where TB is most prevalent. In
addition, current machine learning (ML) diagnosis research, like utilizing
chest radiographs, is ineffective and does not monitor treatment progression.
To enable effective diagnosis, an ensemble model was developed that analyzes,
using a novel ML architecture, coughs' acoustic epidemiologies from
smartphones' microphones to detect TB. The architecture includes a 2D-CNN and
XGBoost that was trained on 724,964 cough audio samples and demographics from 7
countries. After feature extraction (Mel-spectrograms) and data augmentation
(IR-convolution), the model achieved AUROC (area under the receiving operator
characteristic) of 88%, surpassing WHO's requirements for screening tests. The
results are available within 15 seconds and can easily be accessible via a
mobile app. This research helps to improve TB diagnosis through a promising
accurate, quick, and accessible triaging tool.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17681" title="Abstract">arXiv:2310.17681</a> (cross-list from astro-ph.EP) [<a href="/pdf/2310.17681" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Extraction and Classification from Planetary Science Datasets  enabled by Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Nixon%2C+C">Conor Nixon</a>, 
<a href="/search/astro-ph?searchtype=author&query=Yahn%2C+Z">Zachary Yahn</a>, 
<a href="/search/astro-ph?searchtype=author&query=Duncan%2C+E">Ethan Duncan</a>, 
<a href="/search/astro-ph?searchtype=author&query=Neidel%2C+I">Ian Neidel</a>, 
<a href="/search/astro-ph?searchtype=author&query=Mills%2C+A">Alyssa Mills</a>, 
<a href="/search/astro-ph?searchtype=author&query=Seignovert%2C+B">Beno&#xee;t Seignovert</a> (OSUNA), 
<a href="/search/astro-ph?searchtype=author&query=Larsen%2C+A">Andrew Larsen</a>, 
<a href="/search/astro-ph?searchtype=author&query=Gansler%2C+K">Kathryn Gansler</a>, 
<a href="/search/astro-ph?searchtype=author&query=Liles%2C+C">Charles Liles</a>, 
<a href="/search/astro-ph?searchtype=author&query=Walker%2C+C">Catherine Walker</a>, 
<a href="/search/astro-ph?searchtype=author&query=Trent%2C+D">Douglas Trent</a>, 
<a href="/search/astro-ph?searchtype=author&query=Santerre%2C+J">John Santerre</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Aerospace Conference, 2023, pp.1-16
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Earth and Planetary Astrophysics (astro-ph.EP)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper we present two examples of recent investigations that we have
undertaken, applying Machine Learning (ML) neural networks (NN) to image
datasets from outer planet missions to achieve feature recognition. Our first
investigation was to recognize ice blocks (also known as rafts, plates,
polygons) in the chaos regions of fractured ice on Europa. We used a transfer
learning approach, adding and training new layers to an industry-standard Mask
R-CNN (Region-based Convolutional Neural Network) to recognize labeled blocks
in a training dataset. Subsequently, the updated model was tested against a new
dataset, achieving 68% precision. In a different application, we applied the
Mask R-CNN to recognize clouds on Titan, again through updated training
followed by testing against new data, with a precision of 95% over 369 images.
We evaluate the relative successes of our techniques and suggest how training
and recognition could be further improved. The new approaches we have used for
planetary datasets can further be applied to similar recognition tasks on other
planets, including Earth. For imagery of outer planets in particular, the
technique holds the possibility of greatly reducing the volume of returned
data, via onboard identification of the most interesting image subsets, or by
returning only differential data (images where changes have occurred) greatly
enhancing the information content of the final data stream.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17712" title="Abstract">arXiv:2310.17712</a> (cross-list from stat.ML) [<a href="/pdf/2310.17712" title="Download PDF">pdf</a>, <a href="/format/2310.17712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community Detection and Classification Guarantees Using Embeddings  Learned by Node2Vec
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Davison%2C+A">Andrew Davison</a>, 
<a href="/search/stat?searchtype=author&query=Morgan%2C+S+C">S. Carlyle Morgan</a>, 
<a href="/search/stat?searchtype=author&query=Ward%2C+O+G">Owen G. Ward</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI); Methodology (stat.ME)

</div>
<p class="mathjax">Embedding the nodes of a large network into an Euclidean space is a common
objective in modern machine learning, with a variety of tools available. These
embeddings can then be used as features for tasks such as community
detection/node clustering or link prediction, where they achieve state of the
art performance. With the exception of spectral clustering methods, there is
little theoretical understanding for other commonly used approaches to learning
embeddings. In this work we examine the theoretical properties of the
embeddings learned by node2vec. Our main result shows that the use of k-means
clustering on the embedding vectors produced by node2vec gives weakly
consistent community recovery for the nodes in (degree corrected) stochastic
block models. We also discuss the use of these embeddings for node and link
prediction tasks. We demonstrate this result empirically, and examine how this
relates to other embedding tools for network data.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17716" title="Abstract">arXiv:2310.17716</a> (cross-list from quant-ph) [<a href="/pdf/2310.17716" title="Download PDF">pdf</a>, <a href="/format/2310.17716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying (Quantum) Statistical and Parametrized (Quantum) Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Nietner%2C+A">Alexander Nietner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 97 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Kearns' statistical query (SQ) oracle (STOC'93) lends a unifying perspective
for most classical machine learning algorithms. This ceases to be true in
quantum learning, where many settings do not admit, neither an SQ analog nor a
quantum statistical query (QSQ) analog. In this work, we take inspiration from
Kearns' SQ oracle and Valiant's weak evaluation oracle (TOCT'14) and establish
a unified perspective bridging the statistical and parametrized learning
paradigms in a novel way. We explore the problem of learning from an evaluation
oracle, which provides an estimate of function values, and introduce an
extensive yet intuitive framework that yields unconditional lower bounds for
learning from evaluation queries and characterizes the query complexity for
learning linear function classes. The framework is directly applicable to the
QSQ setting and virtually all algorithms based on loss function optimization.
<br />Our first application is to extend prior results on the learnability of
output distributions of quantum circuits and Clifford unitaries from the SQ to
the (multi-copy) QSQ setting, implying exponential separations between learning
stabilizer states from (multi-copy) QSQs versus from quantum samples. Our
second application is to analyze some popular quantum machine learning (QML)
settings. We gain an intuitive picture of the hardness of many QML tasks which
goes beyond existing methods such as barren plateaus and the statistical
dimension, and contains crucial setting-dependent implications. Our framework
not only unifies the perspective of cost concentration with that of the
statistical dimension in a unified language but exposes their connectedness and
similarity.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17720" title="Abstract">arXiv:2310.17720</a> (cross-list from eess.IV) [<a href="/pdf/2310.17720" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Brain Tumor Detection: A Thorough Investigation of CNNs,  Clustering, and SoftMax Classification in the Analysis of MRI Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Miah%2C+J">Jonayet Miah</a>, 
<a href="/search/eess?searchtype=author&query=Cao%2C+D+M">Duc M Cao</a>, 
<a href="/search/eess?searchtype=author&query=Sayed3%2C+M+A">Md Abu Sayed3</a>, 
<a href="/search/eess?searchtype=author&query=Taluckder%2C+M+S">Md Siam Taluckder</a>, 
<a href="/search/eess?searchtype=author&query=Haque%2C+M+S">Md Sabbirul Haque</a>, 
<a href="/search/eess?searchtype=author&query=Mahmud%2C+F">Fuad Mahmud</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> JOIV : International Journal on Informatics Visualization, JOIV :
  Int. J. Inform. Visualization ISSN / E-ISSN 2549-9610 / 2549-9904, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Brain tumors pose a significant global health challenge due to their high
prevalence and mortality rates across all age groups. Detecting brain tumors at
an early stage is crucial for effective treatment and patient outcomes. This
study presents a comprehensive investigation into the use of Convolutional
Neural Networks (CNNs) for brain tumor detection using Magnetic Resonance
Imaging (MRI) images. The dataset, consisting of MRI scans from both healthy
individuals and patients with brain tumors, was processed and fed into the CNN
architecture. The SoftMax Fully Connected layer was employed to classify the
images, achieving an accuracy of 98%. To evaluate the CNN's performance, two
other classifiers, Radial Basis Function (RBF) and Decision Tree (DT), were
utilized, yielding accuracy rates of 98.24% and 95.64%, respectively. The study
also introduced a clustering method for feature extraction, improving CNN's
accuracy. Sensitivity, Specificity, and Precision were employed alongside
accuracy to comprehensively evaluate the network's performance. Notably, the
SoftMax classifier demonstrated the highest accuracy among the categorizers,
achieving 99.52% accuracy on test data. The presented research contributes to
the growing field of deep learning in medical image analysis. The combination
of CNNs and MRI data offers a promising tool for accurately detecting brain
tumors, with potential implications for early diagnosis and improved patient
care.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17721" title="Abstract">arXiv:2310.17721</a> (cross-list from econ.GN) [<a href="/pdf/2310.17721" title="Download PDF">pdf</a>, <a href="/format/2310.17721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Transcripts to Insights: Uncovering Corporate Risks Using  Generative AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Kim%2C+A">Alex Kim</a>, 
<a href="/search/econ?searchtype=author&query=Muhn%2C+M">Maximilian Muhn</a>, 
<a href="/search/econ?searchtype=author&query=Nikolaev%2C+V">Valeri Nikolaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We explore the value of generative AI tools, such as ChatGPT, in helping
investors uncover dimensions of corporate risk. We develop and validate
firm-level measures of risk exposure to political, climate, and AI-related
risks. Using the GPT 3.5 model to generate risk summaries and assessments from
the context provided by earnings call transcripts, we show that GPT-based
measures possess significant information content and outperform the existing
risk measures in predicting (abnormal) firm-level volatility and firms' choices
such as investment and innovation. Importantly, information in risk assessments
dominates that in risk summaries, establishing the value of general AI
knowledge. We also find that generative AI is effective at detecting emerging
risks, such as AI risk, which has soared in recent quarters. Our measures
perform well both within and outside the GPT's training window and are priced
in equity markets. Taken together, an AI-based approach to risk measurement
provides useful insights to users of corporate disclosures at a low cost.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17742" title="Abstract">arXiv:2310.17742</a> (cross-list from eess.AS) [<a href="/pdf/2310.17742" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in  Time-series Load Profiles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yi Hu</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+K">Kai Ye</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+H">Hyeonjin Kim</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+N">Ning Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Inspired by the success of the Transformer model in natural language
processing and computer vision, this paper introduces BERT-PIN, a Bidirectional
Encoder Representations from Transformers (BERT) powered Profile Inpainting
Network. BERT-PIN recovers multiple missing data segments (MDSs) using load and
temperature time-series profiles as inputs. To adopt a standard Transformer
model structure for profile inpainting, we segment the load and temperature
profiles into line segments, treating each segment as a word and the entire
profile as a sentence. We incorporate a top candidates selection process in
BERT-PIN, enabling it to produce a sequence of probability distributions, based
on which users can generate multiple plausible imputed data sets, each
reflecting different confidence levels. We develop and evaluate BERT-PIN using
real-world dataset for two applications: multiple MDSs recovery and demand
response baseline estimation. Simulation results show that BERT-PIN outperforms
the existing methods in accuracy while is capable of restoring multiple MDSs
within a longer window. BERT-PIN, served as a pre-trained model, can be
fine-tuned for conducting many downstream tasks, such as classification and
super resolution.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17755" title="Abstract">arXiv:2310.17755</a> (cross-list from eess.IV) [<a href="/pdf/2310.17755" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alzheimers Disease Diagnosis by Deep Learning Using MRI-Based Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Foroughipoor%2C+S">Sarasadat Foroughipoor</a>, 
<a href="/search/eess?searchtype=author&query=Moradi%2C+K">Kimia Moradi</a>, 
<a href="/search/eess?searchtype=author&query=Bolhasani%2C+H">Hamidreza Bolhasani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The most frequent kind of dementia of the nervous system, Alzheimer's
disease, weakens several brain processes (such as memory) and eventually
results in death. The clinical study uses magnetic resonance imaging to
diagnose AD. Deep learning algorithms are capable of pattern recognition and
feature extraction from the inputted raw data. As early diagnosis and stage
detection are the most crucial elements in enhancing patient care and treatment
outcomes, deep learning algorithms for MRI images have recently allowed for
diagnosing a medical condition at the beginning stage and identifying
particular symptoms of Alzheimer's disease. As a result, we aimed to analyze
five specific studies focused on AD diagnosis using MRI-based deep learning
algorithms between 2021 and 2023 in this study. To completely illustrate the
differences between these techniques and comprehend how deep learning
algorithms function, we attempted to explore selected approaches in depth.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17780" title="Abstract">arXiv:2310.17780</a> (cross-list from eess.IV) [<a href="/pdf/2310.17780" title="Download PDF">pdf</a>, <a href="/format/2310.17780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoCT: Automated CT registration, segmentation, and quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bai%2C+Z">Zhe Bai</a>, 
<a href="/search/eess?searchtype=author&query=Essiari%2C+A">Abdelilah Essiari</a>, 
<a href="/search/eess?searchtype=author&query=Perciano%2C+T">Talita Perciano</a>, 
<a href="/search/eess?searchtype=author&query=Bouchard%2C+K+E">Kristofer E. Bouchard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The processing and analysis of computed tomography (CT) imaging is important
for both basic scientific development and clinical applications. In AutoCT, we
provide a comprehensive pipeline that integrates an end-to-end automatic
preprocessing, registration, segmentation, and quantitative analysis of 3D CT
scans. The engineered pipeline enables atlas-based CT segmentation and
quantification leveraging diffeomorphic transformations through efficient
forward and inverse mappings. The extracted localized features from the
deformation field allow for downstream statistical learning that may facilitate
medical diagnostics. On a lightweight and portable software platform, AutoCT
provides a new toolkit for the CT imaging community to underpin the deployment
of artificial intelligence-driven applications.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17808" title="Abstract">arXiv:2310.17808</a> (cross-list from quant-ph) [<a href="/pdf/2310.17808" title="Download PDF">pdf</a>, <a href="/format/2310.17808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Fast Path Planning Approach for Mobile Devices using Hybrid  Quantum Ant Colony Optimization Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Sarkar%2C+M">Mayukh Sarkar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Pradhan%2C+J">Jitesh Pradhan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Singh%2C+A+K">Anil Kumar Singh</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nenavath%2C+H">Hathiram Nenavath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">With IoT systems' increasing scale and complexity, maintenance of a large
number of nodes using stationary devices is becoming increasingly difficult.
Hence, mobile devices are being employed that can traverse through a set of
target locations and provide the necessary services. In order to reduce energy
consumption and time requirements, the devices are required to traverse
following a Hamiltonian path. This problem can be formulated as a Travelling
Salesman Problem (TSP), an NP-hard problem. Moreover, in emergency services,
the devices must traverse in real-time, demanding speedy path planning from the
TSP instance. Among the well-known optimization techniques for solving the TSP
problem, Ant Colony Optimization has a good stronghold in providing good
approximate solutions. Moreover, ACO not only provides near-optimal solutions
for TSP instances but can also output optimal or near-optimal solutions for
many other demanding hard optimization problems. However, to have a fast
solution, the next node selection, which needs to consider all the neighbors
for each selection, becomes a bottleneck in the path formation step. Moreover,
classical computers are constrained to generate only pseudorandom numbers. Both
these problems can be solved using quantum computing techniques, i.e., the next
node can be selected with proper randomization, respecting the provided set of
probabilities in just a single execution and single measurement of a quantum
circuit. Simulation results of the proposed Hybrid Quantum Ant Colony
Optimization algorithm on several TSP instances have shown promising results,
thus expecting the proposed work to be important in implementing real-time path
planning in quantum-enabled mobile devices.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17816" title="Abstract">arXiv:2310.17816</a> (cross-list from stat.ML) [<a href="/pdf/2310.17816" title="Download PDF">pdf</a>, <a href="/format/2310.17816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around  Exposure-Outcome Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Maasch%2C+J">Jacqueline Maasch</a>, 
<a href="/search/stat?searchtype=author&query=Pan%2C+W">Weishen Pan</a>, 
<a href="/search/stat?searchtype=author&query=Gupta%2C+S">Shantanu Gupta</a>, 
<a href="/search/stat?searchtype=author&query=Kuleshov%2C+V">Volodymyr Kuleshov</a>, 
<a href="/search/stat?searchtype=author&query=Gan%2C+K">Kyra Gan</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+F">Fei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">This work addresses the problem of automated covariate selection under
limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable
set Z of unknown causal structure, the Local Discovery by Partitioning (LDP)
algorithm partitions Z into subsets defined by their relation to {X,Y}. We
enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z
and leverage this taxonomy to differentiate confounders from other variable
types. LDP is motivated by valid adjustment set identification, but avoids the
pretreatment assumption commonly made by automated covariate selection methods.
We provide theoretical guarantees that LDP returns a valid adjustment set for
any Z that meets sufficient graphical conditions. Under stronger conditions, we
prove that partition labels are asymptotically correct. Total independence
tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed
empirically. We numerically validate our theoretical guarantees on synthetic
and semi-synthetic graphs. Adjustment sets from LDP yield less biased and more
precise average treatment effect estimates than baselines, with LDP
outperforming on confounder recall, test count, and runtime for valid
adjustment set discovery.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17817" title="Abstract">arXiv:2310.17817</a> (cross-list from stat.ML) [<a href="/pdf/2310.17817" title="Download PDF">pdf</a>, <a href="/format/2310.17817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian imaging inverse problem with SA-Roundtrip prior via HMC-pCN  sampler
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Qian%2C+J">Jiayu Qian</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+Y">Yuanyuan Liu</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+J">Jingya Yang</a>, 
<a href="/search/stat?searchtype=author&query=Zhou%2C+Q">Qingping Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">Bayesian inference with deep generative prior has received considerable
interest for solving imaging inverse problems in many scientific and
engineering fields. The selection of the prior distribution is learned from,
and therefore an important representation learning of, available prior
measurements. The SA-Roundtrip, a novel deep generative prior, is introduced to
enable controlled sampling generation and identify the data's intrinsic
dimension. This prior incorporates a self-attention structure within a
bidirectional generative adversarial network. Subsequently, Bayesian inference
is applied to the posterior distribution in the low-dimensional latent space
using the Hamiltonian Monte Carlo with preconditioned Crank-Nicolson (HMC-pCN)
algorithm, which is proven to be ergodic under specific conditions. Experiments
conducted on computed tomography (CT) reconstruction with the MNIST and
TomoPhantom datasets reveal that the proposed method outperforms
state-of-the-art comparisons, consistently yielding a robust and superior point
estimator along with precise uncertainty quantification.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17827" title="Abstract">arXiv:2310.17827</a> (cross-list from math.OC) [<a href="/pdf/2310.17827" title="Download PDF">pdf</a>, <a href="/ps/2310.17827" title="Download PostScript">ps</a>, <a href="/format/2310.17827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A hierarchy of eigencomputations for polynomial optimization on the  sphere
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Johnston%2C+N">Nathaniel Johnston</a>, 
<a href="/search/math?searchtype=author&query=Lovitz%2C+B">Benjamin Lovitz</a>, 
<a href="/search/math?searchtype=author&query=Vijayaraghavan%2C+A">Aravindan Vijayaraghavan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages. Comments welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS); Algebraic Geometry (math.AG); Quantum Physics (quant-ph)

</div>
<p class="mathjax">We introduce a convergent hierarchy of lower bounds on the minimum value of a
real homogeneous polynomial over the sphere. The main practical advantage of
our hierarchy over the sum-of-squares (SOS) hierarchy is that the lower bound
at each level of our hierarchy is obtained by a minimum eigenvalue computation,
as opposed to the full semidefinite program (SDP) required at each level of
SOS. In practice, this allows us to go to much higher levels than are
computationally feasible for the SOS hierarchy. For both hierarchies, the
underlying space at the $k$-th level is the set of homogeneous polynomials of
degree $2k$. We prove that our hierarchy converges as $O(1/k)$ in the level
$k$, matching the best-known convergence of the SOS hierarchy when the number
of variables $n$ is less than the half-degree $d$ (the best-known convergence
of SOS when $n \geq d$ is $O(1/k^2)$). More generally, we introduce a
convergent hierarchy of minimum eigenvalue computations for minimizing the
inner product between a real tensor and an element of the spherical
Segre-Veronese variety, with similar convergence guarantees. As examples, we
obtain hierarchies for computing the (real) tensor spectral norm, and for
minimizing biquadratic forms over the sphere. Hierarchies of eigencomputations
for more general constrained polynomial optimization problems are discussed.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17829" title="Abstract">arXiv:2310.17829</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.17829" title="Download PDF">pdf</a>, <a href="/format/2310.17829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Optical Turbulence Models Using Machine Learning and Local  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Jellen%2C+C">Christopher Jellen</a>, 
<a href="/search/physics?searchtype=author&query=Nelson%2C+C">Charles Nelson</a>, 
<a href="/search/physics?searchtype=author&query=Burkhardt%2C+J">John Burkhardt</a>, 
<a href="/search/physics?searchtype=author&query=Brownell%2C+C">Cody Brownell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Optics 62, no. 18 (2023): 4880-4890
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate prediction of atmospheric optical turbulence in localized
environments is essential for estimating the performance of free-space optical
systems. Macro-meteorological models developed to predict turbulent effects in
one environment may fail when applied in new environments. However, existing
macro-meteorological models are expected to offer some predictive power.
Building a new model from locally-measured macro-meteorology and scintillometer
readings can require significant time and resources, as well as a large number
of observations. These challenges motivate the development of a
machine-learning informed hybrid model framework. By combining some baseline
macro-meteorological model with local observations, hybrid models were trained
to improve upon the predictive power of each baseline model. Comparisons
between the performance of the hybrid models, the selected baseline
macro-meteorological models, and machine-learning models trained only on local
observations highlight potential use cases for the hybrid model framework when
local data is expensive to collect. Both the hybrid and data-only models were
trained using the Gradient Boosted Decision Tree (GBDT) architecture with a
variable number of in-situ meteorological observations. The hybrid and
data-only models were found to outperform three baseline macro-meteorological
models, even for low numbers of observations, in some cases as little as one
day. For the first baseline macro-meteorological model investigated, the hybrid
model achieves an estimated 29% reduction in mean absolute error (MAE) using
only one days-equivalent of observation, growing to 41% after only two days,
and 68% after 180 days-equivalent training data. The number of days-equivalent
training data required is potentially indicative of the seasonal variation in
the local microclimate and its propagation environment.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17832" title="Abstract">arXiv:2310.17832</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.17832" title="Download PDF">pdf</a>, <a href="/format/2310.17832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlled density transport by microrotors in a Stokes flow using  linear transfer operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Buzhardt%2C+J">Jake Buzhardt</a>, 
<a href="/search/physics?searchtype=author&query=Tallapragada%2C+P">Phanindra Tallapragada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">We study the problem of transporting a distribution of fluid particles in a
Stokes flow to a desired final distribution in a fixed, finite time by
controlling the torques of a pair of microrotors at fixed positions in the
flow. Our approach is based on a finite dimensional approximation of the
Liouville operator, the infinitesimal generator of the semi-group of Perron
Frobenius operators, which describes the density transport dynamics associated
with the microrotor flow fields. Using this operator, we express the transport
problem as an optimal control problem in terms of the moments of the density
function of the particle distribution. The finite time optimal control problem
is then solved using differential dynamic programming, an iterative trajectory
optimization method. We apply this framework to the microrotor driven flow on
four related problems: transport using rotors in an unbounded flow, transport
near to an infinite plane wall, transport within a circular domain, and the
simultaneous transport of two particle distributions to a common final
distribution in an unbounded flow. These examples demonstrate the effectiveness
of the proposed framework and also allow us to better understand the effects of
boundaries on the ability to achieve a desired fluid transport using a
rotor-driven flow.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17848" title="Abstract">arXiv:2310.17848</a> (cross-list from stat.ML) [<a href="/pdf/2310.17848" title="Download PDF">pdf</a>, <a href="/format/2310.17848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Data Analytics With Synthetic Volume Expansion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Shen%2C+X">Xiaotong Shen</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+Y">Yifei Liu</a>, 
<a href="/search/stat?searchtype=author&query=Shen%2C+R">Rex Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Synthetic data generation, a cornerstone of Generative Artificial
Intelligence, signifies a paradigm shift in data science by addressing data
scarcity and privacy while enabling unprecedented performance. As synthetic
data gains prominence, questions arise concerning the accuracy of statistical
methods when applied to synthetic data compared to raw data. In this article,
we introduce the Synthetic Data Generation for Analytics framework. This
framework employs statistical methods on high-fidelity synthetic data generated
by advanced models such as tabular diffusion and Generative Pre-trained
Transformer models. These models, trained on raw data, are further enhanced
with insights from pertinent studies. A significant discovery within this
framework is the generational effect: the error of a statistical method on
synthetic data initially diminishes with added synthetic data but may
eventually increase or plateau. This phenomenon, rooted in the complexities of
replicating raw data distributions, highlights a "reflection point"--an optimal
threshold in the size of synthetic data determined by specific error metrics.
Through three illustrative case studies-sentiment analysis of texts, predictive
modeling of structured data, and inference in tabular data--we demonstrate the
effectiveness of this framework over traditional ones. We underline its
potential to amplify various statistical methods, including gradient boosting
for prediction and hypothesis testing, thereby underscoring the transformative
potential of synthetic data generation in data science.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17864" title="Abstract">arXiv:2310.17864</a> (cross-list from eess.AS) [<a href="/pdf/2310.17864" title="Download PDF">pdf</a>, <a href="/format/2310.17864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TorchAudio 2.1: Advancing speech recognition, self-supervised learning,  and audio processing components for PyTorch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hwang%2C+J">Jeff Hwang</a>, 
<a href="/search/eess?searchtype=author&query=Hira%2C+M">Moto Hira</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C">Caroline Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiaohui Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+Z">Zhaoheng Ni</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guangzhi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+P">Pingchuan Ma</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+R">Ruizhe Huang</a>, 
<a href="/search/eess?searchtype=author&query=Pratap%2C+V">Vineel Pratap</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yuekai Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Kumar%2C+A">Anurag Kumar</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+C">Chin-Yun Yu</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+C">Chuang Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chunxi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Kahn%2C+J">Jacob Kahn</a>, 
<a href="/search/eess?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+P">Peng Sun</a>, 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+Y">Yangyang Shi</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+Y">Yumeng Tao</a>, 
<a href="/search/eess?searchtype=author&query=Scheibler%2C+R">Robin Scheibler</a>, 
<a href="/search/eess?searchtype=author&query=Cornell%2C+S">Samuele Cornell</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+S">Sean Kim</a>, 
<a href="/search/eess?searchtype=author&query=Petridis%2C+S">Stavros Petridis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">TorchAudio is an open-source audio and speech processing library built for
PyTorch. It aims to accelerate the research and development of audio and speech
technologies by providing well-designed, easy-to-use, and performant PyTorch
components. Its contributors routinely engage with users to understand their
needs and fulfill them by developing impactful features. Here, we survey
TorchAudio's development principles and contents and highlight key features we
include in its latest version (2.1): self-supervised learning pre-trained
pipelines and training recipes, high-performance CTC decoders, speech
recognition models and training recipes, advanced media I/O capabilities, and
tools for performing forced alignment, multi-channel speech enhancement, and
reference-less speech assessment. For a selection of these features, through
empirical studies, we demonstrate their efficacy and show that they achieve
competitive or state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17867" title="Abstract">arXiv:2310.17867</a> (cross-list from stat.ML) [<a href="/pdf/2310.17867" title="Download PDF">pdf</a>, <a href="/format/2310.17867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducibility in Multiple Instance Learning: A Case For Algorithmic  Unit Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Raff%2C+E">Edward Raff</a>, 
<a href="/search/stat?searchtype=author&query=Holt%2C+J">James Holt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multiple Instance Learning (MIL) is a sub-domain of classification problems
with positive and negative labels and a "bag" of inputs, where the label is
positive if and only if a positive element is contained within the bag, and
otherwise is negative. Training in this context requires associating the
bag-wide label to instance-level information, and implicitly contains a causal
assumption and asymmetry to the task (i.e., you can't swap the labels without
changing the semantics). MIL problems occur in healthcare (one malignant cell
indicates cancer), cyber security (one malicious executable makes an infected
computer), and many other tasks. In this work, we examine five of the most
prominent deep-MIL models and find that none of them respects the standard MIL
assumption. They are able to learn anti-correlated instances, i.e., defaulting
to "positive" labels until seeing a negative counter-example, which should not
be possible for a correct MIL model. We suspect that enhancements and other
works derived from these models will share the same issue. In any context in
which these models are being used, this creates the potential for learning
incorrect models, which creates risk of operational failure. We identify and
demonstrate this problem via a proposed "algorithmic unit test", where we
create synthetic datasets that can be solved by a MIL respecting model, and
which clearly reveal learning that violates MIL assumptions. The five evaluated
methods each fail one or more of these tests. This provides a model-agnostic
way to identify violations of modeling assumptions, which we hope will be
useful for future development and evaluation of MIL models.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17954" title="Abstract">arXiv:2310.17954</a> (cross-list from eess.IV) [<a href="/pdf/2310.17954" title="Download PDF">pdf</a>, <a href="/format/2310.17954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivessel Coronary Artery Segmentation and Stenosis Localisation using  Ensemble Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bilal%2C+M">Muhammad Bilal</a>, 
<a href="/search/eess?searchtype=author&query=Martinho%2C+D">Dinis Martinho</a>, 
<a href="/search/eess?searchtype=author&query=Sim%2C+R">Reiner Sim</a>, 
<a href="/search/eess?searchtype=author&query=Qayyum%2C+A">Adnan Qayyum</a>, 
<a href="/search/eess?searchtype=author&query=Vohra%2C+H">Hunaid Vohra</a>, 
<a href="/search/eess?searchtype=author&query=Caputo%2C+M">Massimo Caputo</a>, 
<a href="/search/eess?searchtype=author&query=Akinosho%2C+T">Taofeek Akinosho</a>, 
<a href="/search/eess?searchtype=author&query=Abioye%2C+S">Sofiat Abioye</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+Z">Zaheer Khan</a>, 
<a href="/search/eess?searchtype=author&query=Niaz%2C+W">Waleed Niaz</a>, 
<a href="/search/eess?searchtype=author&query=Qadir%2C+J">Junaid Qadir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submission report for ARCADE challenge hosted at MICCAI2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Coronary angiography analysis is a common clinical task performed by
cardiologists to diagnose coronary artery disease (CAD) through an assessment
of atherosclerotic plaque's accumulation. This study introduces an end-to-end
machine learning solution developed as part of our solution for the MICCAI 2023
Automatic Region-based Coronary Artery Disease diagnostics using x-ray
angiography imagEs (ARCADE) challenge, which aims to benchmark solutions for
multivessel coronary artery segmentation and potential stenotic lesion
localisation from X-ray coronary angiograms. We adopted a robust baseline model
training strategy to progressively improve performance, comprising five
successive stages of binary class pretraining, multivessel segmentation,
fine-tuning using class frequency weighted dataloaders, fine-tuning using
F1-based curriculum learning strategy (F1-CLS), and finally multi-target
angiogram view classifier-based collective adaptation. Unlike many other
medical imaging procedures, this task exhibits a notable degree of
interobserver variability. %, making it particularly amenable to automated
analysis. Our ensemble model combines the outputs from six baseline models
using the weighted ensembling approach, which our analysis shows is found to
double the predictive accuracy of the proposed solution. The final prediction
was further refined, targeting the correction of misclassified blobs. Our
solution achieved a mean F1 score of $37.69\%$ for coronary artery
segmentation, and $39.41\%$ for stenosis localisation, positioning our team in
the 5th position on both leaderboards. This work demonstrates the potential of
automated tools to aid CAD diagnosis, guide interventions, and improve the
accuracy of stent injections in clinical settings.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17997" title="Abstract">arXiv:2310.17997</a> (cross-list from physics.optics) [<a href="/pdf/2310.17997" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Enables Large Depth-of-Field Images for  Sub-Diffraction-Limit Scanning Superlens Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sun%2C+H">Hui Sun</a>, 
<a href="/search/physics?searchtype=author&query=Luo%2C+H">Hao Luo</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+F">Feifei Wang</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+Q">Qingjiu Chen</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+M">Meng Chen</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+X">Xiaoduo Wang</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+H">Haibo Yu</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+G">Guanglie Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+L">Lianqing Liu</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+J">Jianping Wang</a>, 
<a href="/search/physics?searchtype=author&query=Wu%2C+D">Dapeng Wu</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+W+J">Wen Jung Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages,7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Scanning electron microscopy (SEM) is indispensable in diverse applications
ranging from microelectronics to food processing because it provides large
depth-of-field images with a resolution beyond the optical diffraction limit.
However, the technology requires coating conductive films on insulator samples
and a vacuum environment. We use deep learning to obtain the mapping
relationship between optical super-resolution (OSR) images and SEM domain
images, which enables the transformation of OSR images into SEM-like large
depth-of-field images. Our custom-built scanning superlens microscopy (SSUM)
system, which requires neither coating samples by conductive films nor a vacuum
environment, is used to acquire the OSR images with features down to ~80 nm.
The peak signal-to-noise ratio (PSNR) and structural similarity index measure
values indicate that the deep learning method performs excellently in
image-to-image translation, with a PSNR improvement of about 0.74 dB over the
optical super-resolution images. The proposed method provides a high level of
detail in the reconstructed results, indicating that it has broad applicability
to chip-level defect detection, biological sample analysis, forensics, and
various other fields.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18010" title="Abstract">arXiv:2310.18010</a> (cross-list from quant-ph) [<a href="/pdf/2310.18010" title="Download PDF">pdf</a>, <a href="/format/2310.18010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The 7 faces of quantum NP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Gharibian%2C+S">Sevag Gharibian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 5 figures. To appear as ACM SIGACT News guest column
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">When it comes to NP, its natural definition, its wide applicability across
scientific disciplines, and its timeless relevance, the writing is on the wall:
There can be only one. Quantum NP, on the other hand, is clearly the apple that
fell far from the tree of NP. Two decades since the first definitions of
quantum NP started rolling in, quantum complexity theorists face a stark
reality: There's QMA, QCMA, QMA1, QMA(2), StoqMA, and NQP. In this article
aimed at a general theoretical computer science audience, I survey these
various definitions of quantum NP, their strengths and weaknesses, and why most
of them, for better or worse, actually appear to fit naturally into the
complexity zoo.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18026" title="Abstract">arXiv:2310.18026</a> (cross-list from quant-ph) [<a href="/pdf/2310.18026" title="Download PDF">pdf</a>, <a href="/format/2310.18026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetry-Based Quantum Circuit Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Yu%2C+D">Di Yu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fang%2C+K">Kun Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures; comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Programming Languages (cs.PL); Combinatorics (math.CO)

</div>
<p class="mathjax">Quantum circuit mapping is a crucial process in the quantum circuit
compilation pipeline, facilitating the transformation of a logical quantum
circuit into a list of instructions directly executable on a target quantum
system. Recent research has introduced a post-compilation step known as
remapping, which seeks to reconfigure the initial circuit mapping to mitigate
quantum circuit errors arising from system variability. As quantum processors
continue to scale in size, the efficiency of quantum circuit mapping and the
overall compilation process has become of paramount importance. In this work,
we introduce a quantum circuit remapping algorithm that leverages the intrinsic
symmetries in quantum processors, making it well-suited for large-scale quantum
systems. This algorithm identifies all topologically equivalent circuit
mappings by constraining the search space using symmetries and accelerates the
scoring of each mapping using vector computation. Notably, this symmetry-based
circuit remapping algorithm exhibits linear scaling with the number of qubits
in the target quantum hardware and is proven to be optimal in terms of its time
complexity. Moreover, we conduct a comparative analysis against existing
methods in the literature, demonstrating the superior performance of our
symmetry-based method on state-of-the-art quantum hardware architectures and
highlighting the practical utility of our algorithm, particularly for quantum
processors with millions of qubits.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18057" title="Abstract">arXiv:2310.18057</a> (cross-list from math.OC) [<a href="/pdf/2310.18057" title="Download PDF">pdf</a>, <a href="/ps/2310.18057" title="Download PostScript">ps</a>, <a href="/format/2310.18057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduction of Sufficient Conditions in Variational Obstacle Avoidance  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goodman%2C+J+R">Jacob R. Goodman</a>, 
<a href="/search/math?searchtype=author&query=Colombo%2C+L+J">Leonardo J. Colombo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8th IFAC Workshop on Lagrangian and Hamiltonian Methods for Nonlinear Control. arXiv admin note: text overlap with <a href="/abs/2207.13574">arXiv:2207.13574</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Differential Geometry (math.DG); Dynamical Systems (math.DS)

</div>
<p class="mathjax">This paper studies sufficient conditions in a variational obstacle avoidance
problem on complete Riemannian manifolds. That is, we minimize an action
functional, among a set of admissible curves, which depends on an artificial
potential function used to avoid obstacles. We provide necessary and sufficient
conditions under which the resulting critical points, the so-called modified
Riemannian cubics, are local minimizers. We then study the theory of reduction
by symmetries of sufficient conditions for optimality in variational obstacle
avoidance problems on Lie groups endowed with a left-invariant metric. This
amounts to left-translating the Bi-Jacobi fields described to the Lie algebra,
and studying the corresponding bi-conjugate points. New conditions are provided
in terms of the invertibility of a certain matrix.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18078" title="Abstract">arXiv:2310.18078</a> (cross-list from math.FA) [<a href="/pdf/2310.18078" title="Download PDF">pdf</a>, <a href="/ps/2310.18078" title="Download PostScript">ps</a>, <a href="/format/2310.18078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lipschitz and H&#xf6;lder Continuity in Reproducing Kernel Hilbert Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fiedler%2C+C">Christian Fiedler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reproducing kernel Hilbert spaces (RKHSs) are very important function spaces,
playing an important role in machine learning, statistics, numerical analysis
and pure mathematics. Since Lipschitz and H\"older continuity are important
regularity properties, with many applications in interpolation, approximation
and optimization problems, in this work we investigate these continuity notion
in RKHSs. We provide several sufficient conditions as well as an in depth
investigation of reproducing kernels inducing prescribed Lipschitz or H\"older
continuity. Apart from new results, we also collect related known results from
the literature, making the present work also a convenient reference on this
topic.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18108" title="Abstract">arXiv:2310.18108</a> (cross-list from stat.ME) [<a href="/pdf/2310.18108" title="Download PDF">pdf</a>, <a href="/format/2310.18108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transductive conformal inference with adaptive scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gazin%2C+U">Ulysse Gazin</a>, 
<a href="/search/stat?searchtype=author&query=Blanchard%2C+G">Gilles Blanchard</a>, 
<a href="/search/stat?searchtype=author&query=Roquain%2C+E">Etienne Roquain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 6 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Conformal inference is a fundamental and versatile tool that provides
distribution-free guarantees for many machine learning tasks. We consider the
transductive setting, where decisions are made on a test sample of $m$ new
points, giving rise to $m$ conformal $p$-values. {While classical results only
concern their marginal distribution, we show that their joint distribution
follows a P\'olya urn model, and establish a concentration inequality for their
empirical distribution function.} The results hold for arbitrary exchangeable
scores, including {\it adaptive} ones that can use the covariates of the
test+calibration samples at training stage for increased accuracy. We
demonstrate the usefulness of these theoretical results through uniform,
in-probability guarantees for two machine learning tasks of current interest:
interval prediction for transductive transfer learning and novelty detection
based on two-class classification.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18138" title="Abstract">arXiv:2310.18138</a> (cross-list from quant-ph) [<a href="/pdf/2310.18138" title="Download PDF">pdf</a>, <a href="/ps/2310.18138" title="Download PostScript">ps</a>, <a href="/format/2310.18138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Single-Shot Decoding of Quantum Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cumitini%2C+A">Aldo Cumitini</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tinelli%2C+S">Stefano Tinelli</a>, 
<a href="/search/quant-ph?searchtype=author&query=Matuz%2C+B">Bal&#xe1;zs Matuz</a>, 
<a href="/search/quant-ph?searchtype=author&query=L%C3%A1zaro%2C+F">Francisco L&#xe1;zaro</a>, 
<a href="/search/quant-ph?searchtype=author&query=Barletta%2C+L">Luca Barletta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We discuss single-shot decoding of quantum Calderbank-Shor-Steane codes with
faulty syndrome measurements. We state the problem as a joint source-channel
coding problem. By adding redundant rows to the code's parity-check matrix we
obtain an additional syndrome error correcting code which addresses faulty
syndrome measurements. Thereby, the redundant rows are chosen to obtain good
syndrome error correcting capabilities while keeping the stabilizer weights
low. Optimal joint decoding rules are derived which, though too complex for
general codes, can be evaluated for short quantum codes.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18186" title="Abstract">arXiv:2310.18186</a> (cross-list from stat.ML) [<a href="/pdf/2310.18186" title="Download PDF">pdf</a>, <a href="/format/2310.18186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-free Posterior Sampling via Learning Rate Randomization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tiapkin%2C+D">Daniil Tiapkin</a>, 
<a href="/search/stat?searchtype=author&query=Belomestny%2C+D">Denis Belomestny</a>, 
<a href="/search/stat?searchtype=author&query=Calandriello%2C+D">Daniele Calandriello</a>, 
<a href="/search/stat?searchtype=author&query=Moulines%2C+E">Eric Moulines</a>, 
<a href="/search/stat?searchtype=author&query=Munos%2C+R">Remi Munos</a>, 
<a href="/search/stat?searchtype=author&query=Naumov%2C+A">Alexey Naumov</a>, 
<a href="/search/stat?searchtype=author&query=Perrault%2C+P">Pierre Perrault</a>, 
<a href="/search/stat?searchtype=author&query=Valko%2C+M">Michal Valko</a>, 
<a href="/search/stat?searchtype=author&query=Menard%2C+P">Pierre Menard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we introduce Randomized Q-learning (RandQL), a novel
randomized model-free algorithm for regret minimization in episodic Markov
Decision Processes (MDPs). To the best of our knowledge, RandQL is the first
tractable model-free posterior sampling-based algorithm. We analyze the
performance of RandQL in both tabular and non-tabular metric space settings. In
tabular MDPs, RandQL achieves a regret bound of order
$\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})$, where $H$ is the planning horizon,
$S$ is the number of states, $A$ is the number of actions, and $T$ is the
number of episodes. For a metric state-action space, RandQL enjoys a regret
bound of order $\widetilde{\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$, where
$d_z$ denotes the zooming dimension. Notably, RandQL achieves optimistic
exploration without using bonuses, relying instead on a novel idea of learning
rate randomization. Our empirical study shows that RandQL outperforms existing
approaches on baseline exploration environments.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18192" title="Abstract">arXiv:2310.18192</a> (cross-list from eess.IV) [<a href="/pdf/2310.18192" title="Download PDF">pdf</a>, <a href="/format/2310.18192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artifact-Robust Graph-Based Learning in Digital Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gheshlaghi%2C+S+H">Saba Heidari Gheshlaghi</a>, 
<a href="/search/eess?searchtype=author&query=Aryal%2C+M">Milan Aryal</a>, 
<a href="/search/eess?searchtype=author&query=Yahyasoltani%2C+N">Nasim Yahyasoltani</a>, 
<a href="/search/eess?searchtype=author&query=Ganji%2C+M">Masoud Ganji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Whole slide images~(WSIs) are digitized images of tissues placed in glass
slides using advanced scanners. The digital processing of WSIs is challenging
as they are gigapixel images and stored in multi-resolution format. A common
challenge with WSIs is that perturbations/artifacts are inevitable during
storing the glass slides and digitizing them. These perturbations include
motion, which often arises from slide movement during placement, and changes in
hue and brightness due to variations in staining chemicals and the quality of
digitizing scanners. In this work, a novel robust learning approach to account
for these artifacts is presented. Due to the size and resolution of WSIs and to
account for neighborhood information, graph-based methods are called for. We
use graph convolutional network~(GCN) to extract features from the graph
representing WSI. Through a denoiser {and pooling layer}, the effects of
perturbations in WSIs are controlled and the output is followed by a
transformer for the classification of different grades of prostate cancer. To
compare the efficacy of the proposed approach, the model without denoiser is
trained and tested with WSIs without any perturbation and then different
perturbations are introduced in WSIs and passed through the network with the
denoiser. The accuracy and kappa scores of the proposed model with prostate
cancer dataset compared with non-robust algorithms show significant improvement
in cancer diagnosis.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18198" title="Abstract">arXiv:2310.18198</a> (cross-list from quant-ph) [<a href="/pdf/2310.18198" title="Download PDF">pdf</a>, <a href="/ps/2310.18198" title="Download PostScript">ps</a>, <a href="/format/2310.18198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Fidelity Distribution of Link-level Entanglements under  Purification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Elsayed%2C+K">Karim Elsayed</a>, 
<a href="/search/quant-ph?searchtype=author&query=KhudaBukhsh%2C+W+R">Wasiur R. KhudaBukhsh</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rizk%2C+A">Amr Rizk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Quantum entanglement is the key to quantum communications over considerable
distances. The first step for entanglement distribution among quantum
communication nodes is to generate link-level Einstein-Podolsky-Rosen (EPR)
pairs between adjacent communication nodes. EPR pairs may be continuously
generated and stored in a few quantum memories to be ready for utilization by
quantum applications. A major challenge is that qubits suffer from unavoidable
noise due to their interaction with the environment, which is called
decoherence. This decoherence results in the known exponential decay model of
the fidelity of the qubits with time, thus, limiting the lifetime of a qubit in
a quantum memory and the performance of quantum applications.
<br />In this paper, we evaluate the fidelity of the stored EPR pairs under two
opposite dynamical and probabilistic phenomena, first, the aforementioned
decoherence and second purification, i.e. an operation to improve the fidelity
of an EPR pair at the expense of sacrificing another EPR pair. Instead of
applying the purification as soon as two EPR pairs are generated, we introduce
a Purification scheme Beyond the Generation time (PBG) of two EPR pairs. We
analytically show the probability distribution of the fidelity of stored
link-level EPR pairs in a system with two quantum memories at each node
allowing a maximum of two stored EPR pairs. In addition, we apply a PBG scheme
that purifies the two stored EPR pairs upon the generation of an additional
one. We finally provide numerical evaluations of the analytical approach and
show the fidelity-rate trade-off of the considered purification scheme.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18201" title="Abstract">arXiv:2310.18201</a> (cross-list from math.AP) [<a href="/pdf/2310.18201" title="Download PDF">pdf</a>, <a href="/format/2310.18201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Residual Minimization for PDEs: Failure of PINN, Modified Equation,  and Implicit Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+T">Tao Luo</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Q">Qixuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We study the failure phenomenon on the residual minimization (RM) methods, in
particular, the physics-informed neural network (PINN), for the elliptic
equations with discontinuous coefficients. To explain the failure phenomenon,
we conduct numerical experiments with insightful observations and propose a
modified equation to model the numerical solution via RM method. We then
investigate the solution to the modified equation and, in particular, show that
the modified solution deviates from the original exact solution. The proof uses
a necessary and sufficient condition on characterizing the singularity in the
coefficients. This equivalent condition is a useful by-product of this paper,
which can be extended to other types of equations in the future. Finally, we
prove that RM method implicitly biases the numerical solution against the exact
solution and towards a modified solution. Extensions of the results to the
quasilinear elliptic equations are also discussed.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18222" title="Abstract">arXiv:2310.18222</a> (cross-list from eess.IV) [<a href="/pdf/2310.18222" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TBDLNet: a network for classifying multidrug-resistant and  drug-sensitive tuberculosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Z">Ziquan Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+J">Jing Tao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shuihua Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yudong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes applying a novel deep-learning model, TBDLNet, to
recognize CT images to classify multidrug-resistant and drug-sensitive
tuberculosis automatically. The pre-trained ResNet50 is selected to extract
features. Three randomized neural networks are used to alleviate the
overfitting problem. The ensemble of three RNNs is applied to boost the
robustness via majority voting. The proposed model is evaluated by five-fold
cross-validation. Five indexes are selected in this paper, which are accuracy,
sensitivity, precision, F1-score, and specificity. The TBDLNet achieves 0.9822
accuracy, 0.9815 specificity, 0.9823 precision, 0.9829 sensitivity, and 0.9826
F1-score, respectively. The TBDLNet is suitable for classifying
multidrug-resistant tuberculosis and drug-sensitive tuberculosis. It can detect
multidrug-resistant pulmonary tuberculosis as early as possible, which helps to
adjust the treatment plan in time and improve the treatment effect.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18234" title="Abstract">arXiv:2310.18234</a> (cross-list from eess.IV) [<a href="/pdf/2310.18234" title="Download PDF">pdf</a>, <a href="/format/2310.18234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge AI-Based Vein Detector for Efficient Venipuncture in the  Antecubital Fossa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Salcedo%2C+E">Edwin Salcedo</a>, 
<a href="/search/eess?searchtype=author&query=Pe%C3%B1aloza%2C+P">Patricia Pe&#xf1;aloza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in MICAI 2023, Part II, LNCS 14392
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Assessing the condition and visibility of veins is a crucial step before
obtaining intravenous access in the antecubital fossa, which is a common
procedure to draw blood or administer intravenous therapies (IV therapies).
Even though medical practitioners are highly skilled at intravenous
cannulation, they usually struggle to perform the procedure in patients with
low visible veins due to fluid retention, age, overweight, dark skin tone, or
diabetes. Recently, several investigations proposed combining Near Infrared
(NIR) imaging and deep learning (DL) techniques for forearm vein segmentation.
Although they have demonstrated compelling results, their use has been rather
limited owing to the portability and precision requirements to perform
venipuncture. In this paper, we aim to contribute to bridging this gap using
three strategies. First, we introduce a new NIR-based forearm vein segmentation
dataset of 2,016 labelled images collected from 1,008 subjects with low visible
veins. Second, we propose a modified U-Net architecture that locates veins
specifically in the antecubital fossa region of the examined patient. Finally,
a compressed version of the proposed architecture was deployed inside a
bespoke, portable vein finder device after testing four common embedded
microcomputers and four common quantization modalities. Experimental results
showed that the model compressed with Dynamic Range Quantization and deployed
on a Raspberry Pi 4B card produced the best execution time and precision
balance, with 5.14 FPS and 0.957 of latency and Intersection over Union (IoU),
respectively. These results show promising performance inside a
resource-restricted low-cost device.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18238" title="Abstract">arXiv:2310.18238</a> (cross-list from math.CO) [<a href="/pdf/2310.18238" title="Download PDF">pdf</a>, <a href="/format/2310.18238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order-2 Delaunay Triangulations Optimize Angles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Edelsbrunner%2C+H">Herbert Edelsbrunner</a>, 
<a href="/search/math?searchtype=author&query=Garber%2C+A">Alexey Garber</a>, 
<a href="/search/math?searchtype=author&query=Saghafian%2C+M">Morteza Saghafian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Metric Geometry (math.MG)

</div>
<p class="mathjax">The local angle property of the (order-$1$) Delaunay triangulations of a
generic set in $\mathbb{R}^2$ asserts that the sum of two angles opposite a
common edge is less than $\pi$. This paper extends this property to higher
order and uses it to generalize two classic properties from order-$1$ to
order-$2$: (1) among the complete level-$2$ hypertriangulations of a generic
point set in $\mathbb{R}^2$, the order-$2$ Delaunay triangulation
lexicographically maximizes the sorted angle vector; (2) among the maximal
level-$2$ hypertriangulations of a generic point set in $\mathbb{R}^2$, the
order-$2$ Delaunay triangulation is the only one that has the local angle
property. For order-$1$, both properties have been instrumental in numerous
applications of Delaunay triangulations, and we expect that their
generalization will make order-$2$ Delaunay triangulations more attractive to
applications as well.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18281" title="Abstract">arXiv:2310.18281</a> (cross-list from quant-ph) [<a href="/pdf/2310.18281" title="Download PDF">pdf</a>, <a href="/format/2310.18281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Non-Linear Programming Formulations in QuantumCircuitOpt for  Optimal Circuit Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Henderson%2C+E+R">Elena R. Henderson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nagarajan%2C+H">Harsha Nagarajan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Coffrin%2C+C">Carleton Coffrin</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE/ACM Third International Workshop on Quantum Computing
  Software (QCS), SC22, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Given the limitations of current hardware, the theoretical gains promised by
quantum computing remain unrealized across practical applications. But the gap
between theory and hardware is closing, assisted by developments in quantum
algorithmic modeling. One such recent development is QuantumCircuitOpt (QCOpt),
an open-source software framework that leverages state-of-the-art
optimization-based solvers to find provably optimal compact circuit
decompositions, which are exact up to global phase and machine precision. The
quantum circuit design problem can be modeled using non-linear, non-convex
constraints. However, QCOpt reformulates these non-linear constraints using
well-known linearization techniques such that the resulting design problem is
solved as a Mixed-Integer Linear Programming (MILP) model. In this work, we
instead explore whether the QCOpt could also be effective with a continuous
Non-Linear Programming (NLP) model obtained via relaxation of the integer
variables in the non-linear constraints. We are able to present not only
multiple significant enhancements to QCOpt, with up to 11.3x speed-up in run
times on average, but also opportunities for more generally exploring the
behavior of gradient-based NLP solvers.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18306" title="Abstract">arXiv:2310.18306</a> (cross-list from stat.ML) [<a href="/pdf/2310.18306" title="Download PDF">pdf</a>, <a href="/format/2310.18306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised and Penalized Baseline Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nikzad-Langerodi%2C+E+A+R">Erik Andries Ramin Nikzad-Langerodi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages; 8 figure with a total of 18 subfigures; 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Spectroscopic measurements can show distorted spectra shapes arising from a
mixture of absorbing and scattering contributions. These distortions (or
baselines) often manifest themselves as non-constant offsets or low-frequency
oscillations. As a result, these baselines can adversely affect analytical and
quantitative results. Baseline correction is an umbrella term where one applies
pre-processing methods to obtain baseline spectra (the unwanted distortions)
and then remove the distortions by differencing. However, current state-of-the
art baseline correction methods do not utilize analyte concentrations even if
they are available, or even if they contribute significantly to the observed
spectral variability. We examine a class of state-of-the-art methods (penalized
baseline correction) and modify them such that they can accommodate a priori
analyte concentration such that prediction can be enhanced. Performance will be
access on two near infra-red data sets across both classical penalized baseline
correction methods (without analyte information) and modified penalized
baseline correction methods (leveraging analyte information).
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18309" title="Abstract">arXiv:2310.18309</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.18309" title="Download PDF">pdf</a>, <a href="/ps/2310.18309" title="Download PostScript">ps</a>, <a href="/format/2310.18309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social media battle for attention: opinion dynamics on competing  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Somazzi%2C+A">Andrea Somazzi</a>, 
<a href="/search/physics?searchtype=author&query=Ferro%2C+G+M">Giuseppe Maria Ferro</a>, 
<a href="/search/physics?searchtype=author&query=Garlaschelli%2C+D">Diego Garlaschelli</a>, 
<a href="/search/physics?searchtype=author&query=Levin%2C+S+A">Simon Asher Levin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In the age of information abundance, attention is a coveted resource. Social
media platforms vigorously compete for users' engagement, influencing the
evolution of their opinions on a variety of topics. With recommendation
algorithms often accused of creating "filter bubbles", where like-minded
individuals interact predominantly with one another, it's crucial to understand
the consequences of this unregulated attention market. To address this, we
present a model of opinion dynamics on a multiplex network. Each layer of the
network represents a distinct social media platform, each with its unique
characteristics. Users, as nodes in this network, share their opinions across
platforms and decide how much time to allocate in each platform depending on
its perceived quality. Our model reveals two key findings. i) When examining
two platforms - one with a neutral recommendation algorithm and another with a
homophily-based algorithm - we uncover that even if users spend the majority of
their time on the neutral platform, opinion polarization can persist. ii) By
allowing users to dynamically allocate their social energy across platforms in
accordance to their homophilic preferences, a further segregation of
individuals emerges. While network fragmentation is usually associated with
"echo chambers", the emergent multi-platform segregation leads to an increase
in users' satisfaction without the undesired increase in polarization. These
results underscore the significance of acknowledging how individuals gather
information from a multitude of sources. Furthermore, they emphasize that
policy interventions on a single social media platform may yield limited
impact.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Mon, 30 Oct 23</h3>
<dl>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.11880" title="Abstract">arXiv:1911.11880</a> (replaced) [<a href="/pdf/1911.11880" title="Download PDF">pdf</a>, <a href="/format/1911.11880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Framework on Enhancing Portfolio Management with Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Li%2C+Y">Yinheng Li</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+J">Junhao Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=Cao%2C+Y">Yijie Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1912.13490" title="Abstract">arXiv:1912.13490</a> (replaced) [<a href="/pdf/1912.13490" title="Download PDF">pdf</a>, <a href="/format/1912.13490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neurocomputational Account of Flexible Goal-directed Cognition and  Consciousness: The Goal-Aligning Representation Internal Manipulation Theory  (GARIM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Granato%2C+G">Giovanni Granato</a>, 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+G">Gianluca Baldassarre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.10294" title="Abstract">arXiv:2010.10294</a> (replaced) [<a href="/pdf/2010.10294" title="Download PDF">pdf</a>, <a href="/format/2010.10294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Webpage Fingerprinting from TLS Traces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mavroudis%2C+V">Vasilios Mavroudis</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+J">Jamie Hayes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.01161" title="Abstract">arXiv:2102.01161</a> (replaced) [<a href="/pdf/2102.01161" title="Download PDF">pdf</a>, <a href="/format/2102.01161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adjoint Rigid Transform Network: Task-conditioned Alignment of 3D Shapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Keyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+B+L">Bharat Lal Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+B">Bernt Schiele</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.10019" title="Abstract">arXiv:2102.10019</a> (replaced) [<a href="/pdf/2102.10019" title="Download PDF">pdf</a>, <a href="/format/2102.10019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Reich%2C+C+L">Claire Lazar Reich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.11439" title="Abstract">arXiv:2107.11439</a> (replaced) [<a href="/pdf/2107.11439" title="Download PDF">pdf</a>, <a href="/format/2107.11439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Is The Internet? Partial Connectivity in the Internet Core
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baltra%2C+G">Guillermo Baltra</a>, 
<a href="/search/cs?searchtype=author&query=Saluja%2C+T">Tarang Saluja</a>, 
<a href="/search/cs?searchtype=author&query=Pradkin%2C+Y">Yuri Pradkin</a>, 
<a href="/search/cs?searchtype=author&query=Heidemann%2C+J">John Heidemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.00914" title="Abstract">arXiv:2108.00914</a> (replaced) [<a href="/pdf/2108.00914" title="Download PDF">pdf</a>, <a href="/ps/2108.00914" title="Download PostScript">ps</a>, <a href="/format/2108.00914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hardness and Approximation of Submodular Minimum Linear Ordering  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farhadi%2C+M">Majid Farhadi</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Swati Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shengding Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tetali%2C+P">Prasad Tetali</a>, 
<a href="/search/cs?searchtype=author&query=Wigal%2C+M+C">Michael C. Wigal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.14053" title="Abstract">arXiv:2110.14053</a> (replaced) [<a href="/pdf/2110.14053" title="Download PDF">pdf</a>, <a href="/format/2110.14053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mohit Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Khurshid%2C+S">Sarfraz Khurshid</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+K">Kenneth McMillan</a>, 
<a href="/search/cs?searchtype=author&query=Miikkulainen%2C+R">Risto Miikkulainen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.00559" title="Abstract">arXiv:2111.00559</a> (replaced) [<a href="/pdf/2111.00559" title="Download PDF">pdf</a>, <a href="/format/2111.00559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacity of Noisy Permutation Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jennifer Tang</a>, 
<a href="/search/cs?searchtype=author&query=Polyanskiy%2C+Y">Yury Polyanskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> draft version updated to newer version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Transactions on Information Theory, vol. 69, no. 7, pp.
  4145-4162, July 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.04469" title="Abstract">arXiv:2111.04469</a> (replaced) [<a href="/pdf/2111.04469" title="Download PDF">pdf</a>, <a href="/format/2111.04469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed-Integer Optimization with Constraint Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Maragno%2C+D">Donato Maragno</a>, 
<a href="/search/math?searchtype=author&query=Wiberg%2C+H">Holly Wiberg</a>, 
<a href="/search/math?searchtype=author&query=Bertsimas%2C+D">Dimitris Bertsimas</a>, 
<a href="/search/math?searchtype=author&query=Birbil%2C+S+I">S. Ilker Birbil</a>, 
<a href="/search/math?searchtype=author&query=Hertog%2C+D+d">Dick den Hertog</a>, 
<a href="/search/math?searchtype=author&query=Fajemisin%2C+A">Adejuyigbe Fajemisin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.11988" title="Abstract">arXiv:2112.11988</a> (replaced) [<a href="/pdf/2112.11988" title="Download PDF">pdf</a>, <a href="/format/2112.11988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Programs to Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bugayenko%2C+Y">Yegor Bugayenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.14596" title="Abstract">arXiv:2203.14596</a> (replaced) [<a href="/pdf/2203.14596" title="Download PDF">pdf</a>, <a href="/format/2203.14596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recasting an operator splitting solver into a standard finite volume  flux-based algorithm. The case of a Lagrange-Projection-type method for gas  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bourgeois%2C+R">R&#xe9;mi Bourgeois</a>, 
<a href="/search/math?searchtype=author&query=Tremblin%2C+P">Pascal Tremblin</a>, 
<a href="/search/math?searchtype=author&query=Kokh%2C+S">Samuel Kokh</a>, 
<a href="/search/math?searchtype=author&query=Padioleau%2C+T">Thomas Padioleau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.00585" title="Abstract">arXiv:2204.00585</a> (replaced) [<a href="/pdf/2204.00585" title="Download PDF">pdf</a>, <a href="/format/2204.00585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Approach for Structuring and Analysing Knowledge  Provenance for Visual Analytics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christino%2C+L">Leonardo Christino</a>, 
<a href="/search/cs?searchtype=author&query=Rezaeipourfarsangi%2C+S">Sima Rezaeipourfarsangi</a>, 
<a href="/search/cs?searchtype=author&query=Milios%2C+E">Evangelos Milios</a>, 
<a href="/search/cs?searchtype=author&query=Paulovich%2C+F+V">Fernando V. Paulovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pgs, submitted to Computer Graphics Forum 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07982" title="Abstract">arXiv:2205.07982</a> (replaced) [<a href="/pdf/2205.07982" title="Download PDF">pdf</a>, <a href="/format/2205.07982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion  Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Keyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+B+L">Bharat Lal Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Lenssen%2C+J+E">Jan Eric Lenssen</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15802" title="Abstract">arXiv:2205.15802</a> (replaced) [<a href="/e-print/2205.15802" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaTask: Adaptive Multitask Online Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laforgue%2C+P">Pierre Laforgue</a>, 
<a href="/search/cs?searchtype=author&query=Della+Vecchia%2C+A">Andrea Della Vecchia</a>, 
<a href="/search/cs?searchtype=author&query=Cesa-Bianchi%2C+N">Nicol&#xf2; Cesa-Bianchi</a>, 
<a href="/search/cs?searchtype=author&query=Rosasco%2C+L">Lorenzo Rosasco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The proof of Theorem 3 is wrong: in the display equation below Equation (22), bottom of page 15, the gradient of $\phi_{t+1}$ is missing a factor $1/(\alpha\eta_t)$
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.02171" title="Abstract">arXiv:2207.02171</a> (replaced) [<a href="/pdf/2207.02171" title="Download PDF">pdf</a>, <a href="/format/2207.02171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical modeling for an industrial implementation of a Digital Twin  for electrical drives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cherifi%2C+K">Karim Cherifi</a>, 
<a href="/search/eess?searchtype=author&query=Schulze%2C+P">Philipp Schulze</a>, 
<a href="/search/eess?searchtype=author&query=Mehrmann%2C+V">Volker Mehrmann</a>, 
<a href="/search/eess?searchtype=author&query=Go%C3%9Flau%2C+L">Leo Go&#xdf;lau</a>, 
<a href="/search/eess?searchtype=author&query=L%C3%BCnnemann%2C+P">Pascal L&#xfc;nnemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05898" title="Abstract">arXiv:2207.05898</a> (replaced) [<a href="/pdf/2207.05898" title="Download PDF">pdf</a>, <a href="/ps/2207.05898" title="Download PostScript">ps</a>, <a href="/format/2207.05898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing and Learning Quantum Juntas Nearly Optimally
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+T">Thomas Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nadimpalli%2C+S">Shivam Nadimpalli</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yuen%2C+H">Henry Yuen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07051" title="Abstract">arXiv:2207.07051</a> (replaced) [<a href="/pdf/2207.07051" title="Download PDF">pdf</a>, <a href="/format/2207.07051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language models show human-like content effects on reasoning tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Lampinen%2C+A+K">Andrew K. Lampinen</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S+C+Y">Stephanie C. Y. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Creswell%2C+H+R+S+A">Hannah R. Sheahan Antonia Creswell</a>, 
<a href="/search/cs?searchtype=author&query=Kumaran%2C+D">Dharshan Kumaran</a>, 
<a href="/search/cs?searchtype=author&query=McClelland%2C+J+L">James L. McClelland</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+F">Felix Hill</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08189" title="Abstract">arXiv:2207.08189</a> (replaced) [<a href="/pdf/2207.08189" title="Download PDF">pdf</a>, <a href="/format/2207.08189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supplementing Recurrent Neural Networks with Annealing to Solve  Combinatorial Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Khandoker%2C+S+A">Shoummo Ahsan Khandoker</a>, 
<a href="/search/cond-mat?searchtype=author&query=Abedin%2C+J+M">Jawaril Munshad Abedin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hibat-Allah%2C+M">Mohamed Hibat-Allah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, 4 tables. Github code: <a href="https://github.com/RNN-VCA-CO/RNN-VCA-CO.">this https URL</a> Published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mach. Learn.: Sci. Technol. 4 015026, Feb 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08911" title="Abstract">arXiv:2207.08911</a> (replaced) [<a href="/pdf/2207.08911" title="Download PDF">pdf</a>, <a href="/format/2207.08911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deeply-Learned Generalized Linear Models with Missing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lim%2C+D+K">David K Lim</a>, 
<a href="/search/stat?searchtype=author&query=Rashid%2C+N+U">Naim U Rashid</a>, 
<a href="/search/stat?searchtype=author&query=Oliva%2C+J+B">Junier B Oliva</a>, 
<a href="/search/stat?searchtype=author&query=Ibrahim%2C+J+G">Joseph G Ibrahim</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computational and Graphical Statistics, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.02744" title="Abstract">arXiv:2208.02744</a> (replaced) [<a href="/pdf/2208.02744" title="Download PDF">pdf</a>, <a href="/format/2208.02744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Error Correction via Noise Guessing Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cruz%2C+D">Diogo Cruz</a>, 
<a href="/search/quant-ph?searchtype=author&query=Monteiro%2C+F+A">Francisco A. Monteiro</a>, 
<a href="/search/quant-ph?searchtype=author&query=Coutinho%2C+B+C">Bruno C. Coutinho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04627" title="Abstract">arXiv:2208.04627</a> (replaced) [<a href="/pdf/2208.04627" title="Download PDF">pdf</a>, <a href="/format/2208.04627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Effect Identification in Uncertain Causal Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akbari%2C+S">Sina Akbari</a>, 
<a href="/search/cs?searchtype=author&query=Jamshidi%2C+F">Fateme Jamshidi</a>, 
<a href="/search/cs?searchtype=author&query=Mokhtarian%2C+E">Ehsan Mokhtarian</a>, 
<a href="/search/cs?searchtype=author&query=Vowels%2C+M+J">Matthew J. Vowels</a>, 
<a href="/search/cs?searchtype=author&query=Etesami%2C+J">Jalal Etesami</a>, 
<a href="/search/cs?searchtype=author&query=Kiyavash%2C+N">Negar Kiyavash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 9 figures, NeurIPS 2023 conference, causal identification, causal discovery, probabilistic models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.08402" title="Abstract">arXiv:2208.08402</a> (replaced) [<a href="/pdf/2208.08402" title="Download PDF">pdf</a>, <a href="/ps/2208.08402" title="Download PostScript">ps</a>, <a href="/format/2208.08402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical analysis for electromagnetic scattering from nonlinear  boundary conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nick%2C+J">J&#xf6;rg Nick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12660" title="Abstract">arXiv:2209.12660</a> (replaced) [<a href="/pdf/2209.12660" title="Download PDF">pdf</a>, <a href="/format/2209.12660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARLUI: Multi-Agent Reinforcement Learning for Adaptive UIs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Langerak%2C+T">Thomas Langerak</a>, 
<a href="/search/cs?searchtype=author&query=Christen%2C+S">Sammy Christen</a>, 
<a href="/search/cs?searchtype=author&query=Albaba%2C+M">Mert Albaba</a>, 
<a href="/search/cs?searchtype=author&query=Gebhardt%2C+C">Christoph Gebhardt</a>, 
<a href="/search/cs?searchtype=author&query=Hilliges%2C+O">Otmar Hilliges</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01232" title="Abstract">arXiv:2210.01232</a> (replaced) [<a href="/pdf/2210.01232" title="Download PDF">pdf</a>, <a href="/ps/2210.01232" title="Download PostScript">ps</a>, <a href="/format/2210.01232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Split-Spectrum Based Distributed State Estimation for Linear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Lili Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Ji Liu</a>, 
<a href="/search/eess?searchtype=author&query=Anderson%2C+B+B+O">Brian B. O. Anderson</a>, 
<a href="/search/eess?searchtype=author&query=Morse%2C+A+S">A. Stephen Morse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures. arXiv admin note: substantial text overlap with <a href="/abs/1903.05486">arXiv:1903.05486</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04059" title="Abstract">arXiv:2210.04059</a> (replaced) [<a href="/pdf/2210.04059" title="Download PDF">pdf</a>, <a href="/format/2210.04059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selection and Ordering Policies for Hiring Pipelines via Linear  Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epstein%2C+B">Boris Epstein</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Will Ma</a> (Columbia University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07105" title="Abstract">arXiv:2210.07105</a> (replaced) [<a href="/pdf/2210.07105" title="Download PDF">pdf</a>, <a href="/format/2210.07105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CORL: Research-oriented Deep Offline Reinforcement Learning Library
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarasov%2C+D">Denis Tarasov</a>, 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Akimov%2C+D">Dmitry Akimov</a>, 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+V">Vladislav Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+S">Sergey Kolesnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Source code at <a href="https://github.com/corl-team/CORL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.10544" title="Abstract">arXiv:2210.10544</a> (replaced) [<a href="/pdf/2210.10544" title="Download PDF">pdf</a>, <a href="/ps/2210.10544" title="Download PostScript">ps</a>, <a href="/format/2210.10544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subtractive random forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Broutin%2C+N">Nicolas Broutin</a>, 
<a href="/search/math?searchtype=author&query=Devroye%2C+L">Luc Devroye</a>, 
<a href="/search/math?searchtype=author&query=Lugosi%2C+G">Gabor Lugosi</a>, 
<a href="/search/math?searchtype=author&query=Oliveira%2C+R+I">Roberto Imbuzeiro Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13389" title="Abstract">arXiv:2210.13389</a> (replaced) [<a href="/pdf/2210.13389" title="Download PDF">pdf</a>, <a href="/format/2210.13389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Regularized Conditional GAN for Posterior Sampling in Image Recovery  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bendel%2C+M">Matthew Bendel</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+R">Rizwan Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Schniter%2C+P">Philip Schniter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03952" title="Abstract">arXiv:2211.03952</a> (replaced) [<a href="/pdf/2211.03952" title="Download PDF">pdf</a>, <a href="/format/2211.03952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal design of large-scale nonlinear Bayesian inverse problems under  model uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alexanderian%2C+A">Alen Alexanderian</a>, 
<a href="/search/math?searchtype=author&query=Nicholson%2C+R">Ruanui Nicholson</a>, 
<a href="/search/math?searchtype=author&query=Petra%2C+N">Noemi Petra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages; revised version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04858" title="Abstract">arXiv:2212.04858</a> (replaced) [<a href="/pdf/2212.04858" title="Download PDF">pdf</a>, <a href="/format/2212.04858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit variance regularization in non-contrastive SSL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halvagal%2C+M+S">Manu Srinath Halvagal</a>, 
<a href="/search/cs?searchtype=author&query=Laborieux%2C+A">Axel Laborieux</a>, 
<a href="/search/cs?searchtype=author&query=Zenke%2C+F">Friedemann Zenke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06096" title="Abstract">arXiv:2212.06096</a> (replaced) [<a href="/pdf/2212.06096" title="Download PDF">pdf</a>, <a href="/format/2212.06096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Convolutional Kernels for Steerable CNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhdanov%2C+M">Maksim Zhdanov</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+N">Nico Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Cesa%2C+G">Gabriele Cesa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07056" title="Abstract">arXiv:2212.07056</a> (replaced) [<a href="/pdf/2212.07056" title="Download PDF">pdf</a>, <a href="/format/2212.07056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Probability of Necessity and Sufficiency of Explaining Graph  Neural Networks: A Lower Bound Optimization Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+R">Ruichu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuxuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuexin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+J">Jie Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhifeng Hao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09674" title="Abstract">arXiv:2212.09674</a> (replaced) [<a href="/pdf/2212.09674" title="Download PDF">pdf</a>, <a href="/format/2212.09674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LR-Sum: Summarization for Less-Resourced Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palen-Michel%2C+C">Chester Palen-Michel</a>, 
<a href="/search/cs?searchtype=author&query=Lignos%2C+C">Constantine Lignos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10015" title="Abstract">arXiv:2212.10015</a> (replaced) [<a href="/pdf/2212.10015" title="Download PDF">pdf</a>, <a href="/format/2212.10015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Spatial Relationships in Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gokhale%2C+T">Tejas Gokhale</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Nushi%2C+B">Besmira Nushi</a>, 
<a href="/search/cs?searchtype=author&query=Vineet%2C+V">Vibhav Vineet</a>, 
<a href="/search/cs?searchtype=author&query=Horvitz%2C+E">Eric Horvitz</a>, 
<a href="/search/cs?searchtype=author&query=Kamar%2C+E">Ece Kamar</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yezhou Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint; Code and Data at <a href="https://github.com/microsoft/VISOR">this https URL</a> and <a href="https://huggingface.co/datasets/tgokhale/sr2d_visor">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10767" title="Abstract">arXiv:2212.10767</a> (replaced) [<a href="/pdf/2212.10767" title="Download PDF">pdf</a>, <a href="/format/2212.10767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Does Beam Search improve Span-Level Confidence Estimation in  Generative Sequence Labeling?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+K">Kazuma Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Naim%2C+I">Iftekhar Naim</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+K">Karthik Raman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00457" title="Abstract">arXiv:2301.00457</a> (replaced) [<a href="/pdf/2301.00457" title="Download PDF">pdf</a>, <a href="/format/2301.00457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReSQueing Parallel and Private Stochastic Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carmon%2C+Y">Yair Carmon</a>, 
<a href="/search/math?searchtype=author&query=Jambulapati%2C+A">Arun Jambulapati</a>, 
<a href="/search/math?searchtype=author&query=Jin%2C+Y">Yujia Jin</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+Y+T">Yin Tat Lee</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+D">Daogao Liu</a>, 
<a href="/search/math?searchtype=author&query=Sidford%2C+A">Aaron Sidford</a>, 
<a href="/search/math?searchtype=author&query=Tian%2C+K">Kevin Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02428" title="Abstract">arXiv:2301.02428</a> (replaced) [<a href="/pdf/2301.02428" title="Download PDF">pdf</a>, <a href="/format/2301.02428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensitivity analysis using Physics-informed neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hanna%2C+J+M">John M. Hanna</a>, 
<a href="/search/math?searchtype=author&query=Aguado%2C+J+V">Jos&#xe9; V. Aguado</a>, 
<a href="/search/math?searchtype=author&query=Comas-Cardona%2C+S">Sebastien Comas-Cardona</a>, 
<a href="/search/math?searchtype=author&query=Askri%2C+R">Ramzi Askri</a>, 
<a href="/search/math?searchtype=author&query=Borzacchiello%2C+D">Domenico Borzacchiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12534" title="Abstract">arXiv:2301.12534</a> (replaced) [<a href="/pdf/2301.12534" title="Download PDF">pdf</a>, <a href="/format/2301.12534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vicarious Offense and Noise Audit of Offensive Speech Classifiers:  Unifying Human and Machine Disagreement on What is Offensive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weerasooriya%2C+T+C">Tharindu Cyril Weerasooriya</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Sujan Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Ranasinghe%2C+T">Tharindu Ranasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Zampieri%2C+M">Marcos Zampieri</a>, 
<a href="/search/cs?searchtype=author&query=Homan%2C+C+M">Christopher M. Homan</a>, 
<a href="/search/cs?searchtype=author&query=KhudaBukhsh%2C+A+R">Ashiqur R. KhudaBukhsh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12842" title="Abstract">arXiv:2301.12842</a> (replaced) [<a href="/pdf/2301.12842" title="Download PDF">pdf</a>, <a href="/format/2301.12842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Preference-based Policy Optimization without Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+G">Gaon An</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junhyeok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+X">Xingdong Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Kosaka%2C+N">Norio Kosaka</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyung-Min Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H+O">Hyun Oh Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00993" title="Abstract">arXiv:2302.00993</a> (replaced) [<a href="/pdf/2302.00993" title="Download PDF">pdf</a>, <a href="/format/2302.00993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unpaired Multi-Domain Causal Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sturma%2C+N">Nils Sturma</a>, 
<a href="/search/stat?searchtype=author&query=Squires%2C+C">Chandler Squires</a>, 
<a href="/search/stat?searchtype=author&query=Drton%2C+M">Mathias Drton</a>, 
<a href="/search/stat?searchtype=author&query=Uhler%2C+C">Caroline Uhler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08478" title="Abstract">arXiv:2302.08478</a> (replaced) [<a href="/pdf/2302.08478" title="Download PDF">pdf</a>, <a href="/format/2302.08478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernelized Back-Projection Networks for Blind Super Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yoshida%2C+T">Tomoki Yoshida</a>, 
<a href="/search/eess?searchtype=author&query=Kondo%2C+Y">Yuki Kondo</a>, 
<a href="/search/eess?searchtype=author&query=Maeda%2C+T">Takahiro Maeda</a>, 
<a href="/search/eess?searchtype=author&query=Akita%2C+K">Kazutoshi Akita</a>, 
<a href="/search/eess?searchtype=author&query=Ukita%2C+N">Norimichi Ukita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08631" title="Abstract">arXiv:2302.08631</a> (replaced) [<a href="/pdf/2302.08631" title="Download PDF">pdf</a>, <a href="/format/2302.08631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Contextual Bandits with Feedback Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vrousgou%2C+O">Olga Vrousgou</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Mineiro%2C+P">Paul Mineiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09887" title="Abstract">arXiv:2302.09887</a> (replaced) [<a href="/pdf/2302.09887" title="Download PDF">pdf</a>, <a href="/format/2302.09887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 90% F1 Score in Relational Triple Extraction: Is it Real ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saini%2C+P">Pratik Saini</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+S">Samiran Pal</a>, 
<a href="/search/cs?searchtype=author&query=Nayak%2C+T">Tapas Nayak</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+I">Indrajit Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in GenBench workshop @ EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10056" title="Abstract">arXiv:2302.10056</a> (replaced) [<a href="/pdf/2302.10056" title="Download PDF">pdf</a>, <a href="/format/2302.10056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bilevel learning of regularization models and their discretization for  image deblurring and super-resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bubba%2C+T+A">Tatiana A. Bubba</a>, 
<a href="/search/math?searchtype=author&query=Calatroni%2C+L">Luca Calatroni</a>, 
<a href="/search/math?searchtype=author&query=Catozzi%2C+A">Ambra Catozzi</a>, 
<a href="/search/math?searchtype=author&query=Crisci%2C+S">Serena Crisci</a>, 
<a href="/search/math?searchtype=author&query=Pock%2C+T">Thomas Pock</a>, 
<a href="/search/math?searchtype=author&query=Pragliola%2C+M">Monica Pragliola</a>, 
<a href="/search/math?searchtype=author&query=Rautio%2C+S">Siiri Rautio</a>, 
<a href="/search/math?searchtype=author&query=Riccio%2C+D">Danilo Riccio</a>, 
<a href="/search/math?searchtype=author&query=Sebastiani%2C+A">Andrea Sebastiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Acknowledgments corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10359" title="Abstract">arXiv:2302.10359</a> (replaced) [<a href="/pdf/2302.10359" title="Download PDF">pdf</a>, <a href="/format/2302.10359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replicable Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esfandiari%2C+H">Hossein Esfandiari</a>, 
<a href="/search/cs?searchtype=author&query=Karbasi%2C+A">Amin Karbasi</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Velegkas%2C+G">Grigoris Velegkas</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Felix Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10503" title="Abstract">arXiv:2302.10503</a> (replaced) [<a href="/pdf/2302.10503" title="Download PDF">pdf</a>, <a href="/format/2302.10503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reusable Slotwise Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Trang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+A">Amin Mansouri</a>, 
<a href="/search/cs?searchtype=author&query=Madan%2C+K">Kanika Madan</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khuong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dianbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11294" title="Abstract">arXiv:2302.11294</a> (replaced) [<a href="/pdf/2302.11294" title="Download PDF">pdf</a>, <a href="/format/2302.11294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Learning of Variational AutoEncoder: Application to  Synthetic Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=An%2C+S">Seunghwan An</a>, 
<a href="/search/stat?searchtype=author&query=Jeon%2C+J">Jong-June Jeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13934" title="Abstract">arXiv:2302.13934</a> (replaced) [<a href="/pdf/2302.13934" title="Download PDF">pdf</a>, <a href="/format/2302.13934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Learning under Heterogeneous Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Ajay%2C+A">Anurag Ajay</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Akshay Krishnamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02260" title="Abstract">arXiv:2303.02260</a> (replaced) [<a href="/pdf/2303.02260" title="Download PDF">pdf</a>, <a href="/format/2303.02260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to reason over visual objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+S+S">Shanka Subhra Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Webb%2C+T">Taylor Webb</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+J+D">Jonathan D. Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03169" title="Abstract">arXiv:2303.03169</a> (replaced) [<a href="/pdf/2303.03169" title="Download PDF">pdf</a>, <a href="/ps/2303.03169" title="Download PostScript">ps</a>, <a href="/format/2303.03169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Algebraic Perspective on Lipschitz Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Araujo%2C+A">Alexandre Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Havens%2C+A">Aaron Havens</a>, 
<a href="/search/cs?searchtype=author&query=Delattre%2C+B">Blaise Delattre</a>, 
<a href="/search/cs?searchtype=author&query=Allauzen%2C+A">Alexandre Allauzen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023. Spotlight paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06614" title="Abstract">arXiv:2303.06614</a> (replaced) [<a href="/pdf/2303.06614" title="Download PDF">pdf</a>, <a href="/format/2303.06614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Experience Replay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ball%2C+P+J">Philip J. Ball</a>, 
<a href="/search/cs?searchtype=author&query=Teh%2C+Y+W">Yee Whye Teh</a>, 
<a href="/search/cs?searchtype=author&query=Parker-Holder%2C+J">Jack Parker-Holder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06705" title="Abstract">arXiv:2303.06705</a> (replaced) [<a href="/pdf/2303.06705" title="Download PDF">pdf</a>, <a href="/format/2303.06705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retinexformer: One-stage Retinex-based Transformer for Low-light Image  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yuanhao Cai</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+H">Hao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jing Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Timofte%2C+R">Radu Timofte</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023; The first Transformer-based method for low-light image enhancement
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07230" title="Abstract">arXiv:2303.07230</a> (replaced) [<a href="/pdf/2303.07230" title="Download PDF">pdf</a>, <a href="/format/2303.07230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Evaluation of Deep Learning Models for Failure Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hadadi%2C+F">Fatemeh Hadadi</a>, 
<a href="/search/cs?searchtype=author&query=Dawes%2C+J+H">Joshua H. Dawes</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+D">Donghwan Shin</a>, 
<a href="/search/cs?searchtype=author&query=Bianculli%2C+D">Domenico Bianculli</a>, 
<a href="/search/cs?searchtype=author&query=Briand%2C+L">Lionel Briand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11205" title="Abstract">arXiv:2303.11205</a> (replaced) [<a href="/pdf/2303.11205" title="Download PDF">pdf</a>, <a href="/format/2303.11205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-dissipation Informed Neural Network for McKean-Vlasov Type PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shen%2C+Z">Zebang Shen</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Zhenfu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG); Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11403" title="Abstract">arXiv:2303.11403</a> (replaced) [<a href="/pdf/2303.11403" title="Download PDF">pdf</a>, <a href="/format/2303.11403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> eP-ALM: Efficient Perceptual Augmentation of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukor%2C+M">Mustafa Shukor</a>, 
<a href="/search/cs?searchtype=author&query=Dancette%2C+C">Corentin Dancette</a>, 
<a href="/search/cs?searchtype=author&query=Cord%2C+M">Matthieu Cord</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023. Project page: <a href="https://mshukor.github.io/eP-ALM.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15273" title="Abstract">arXiv:2303.15273</a> (replaced) [<a href="/pdf/2303.15273" title="Download PDF">pdf</a>, <a href="/format/2303.15273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modified Implicit Discretization of the Super-Twisting Controller
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Andritsch%2C+B">Benedikt Andritsch</a>, 
<a href="/search/eess?searchtype=author&query=Watermann%2C+L">Lars Watermann</a>, 
<a href="/search/eess?searchtype=author&query=Koch%2C+S">Stefan Koch</a>, 
<a href="/search/eess?searchtype=author&query=Reichhartinger%2C+M">Markus Reichhartinger</a>, 
<a href="/search/eess?searchtype=author&query=Reger%2C+J">Johann Reger</a>, 
<a href="/search/eess?searchtype=author&query=Horn%2C+M">Martin Horn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02247" title="Abstract">arXiv:2304.02247</a> (replaced) [<a href="/pdf/2304.02247" title="Download PDF">pdf</a>, <a href="/format/2304.02247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling Structure and Style: Political Bias Detection in News by  Inducing Document Hierarchy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jiwoo Hong</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y">Yejin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+J">Jaemin Jung</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiyoung Han</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02860" title="Abstract">arXiv:2304.02860</a> (replaced) [<a href="/pdf/2304.02860" title="Download PDF">pdf</a>, <a href="/format/2304.02860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an Effective and Efficient Transformer for Rain-by-snow Weather  Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yuanbo Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Ting Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> code is available at \url{<a href="https://github.com/chdwyb/RSFormer">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04282" title="Abstract">arXiv:2304.04282</a> (replaced) [<a href="/pdf/2304.04282" title="Download PDF">pdf</a>, <a href="/format/2304.04282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium --  Except When They Do
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toonsi%2C+S+A">Sarah A. Toonsi</a>, 
<a href="/search/cs?searchtype=author&query=Shamma%2C+J+S">Jeff S. Shamma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06706" title="Abstract">arXiv:2304.06706</a> (replaced) [<a href="/pdf/2304.06706" title="Download PDF">pdf</a>, <a href="/format/2304.06706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barron%2C+J+T">Jonathan T. Barron</a>, 
<a href="/search/cs?searchtype=author&query=Mildenhall%2C+B">Ben Mildenhall</a>, 
<a href="/search/cs?searchtype=author&query=Verbin%2C+D">Dor Verbin</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+P+P">Pratul P. Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Hedman%2C+P">Peter Hedman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://jonbarron.info/zipnerf/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09542" title="Abstract">arXiv:2304.09542</a> (replaced) [<a href="/pdf/2304.09542" title="Download PDF">pdf</a>, <a href="/format/2304.09542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT Good at Search? Investigating Large Language Models as  Re-Ranking Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lingyong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pengjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhumin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10149" title="Abstract">arXiv:2304.10149</a> (replaced) [<a href="/pdf/2304.10149" title="Download PDF">pdf</a>, <a href="/format/2304.10149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT a Good Recommender? A Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peilin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+R">Renjie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CIKM 2023 GenRec Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11527" title="Abstract">arXiv:2304.11527</a> (replaced) [<a href="/pdf/2304.11527" title="Download PDF">pdf</a>, <a href="/format/2304.11527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Pendulum-Driven Legless Rolling Jumping Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buzhardt%2C+J">Jake Buzhardt</a>, 
<a href="/search/cs?searchtype=author&query=Chivkula%2C+P">Prashanth Chivkula</a>, 
<a href="/search/cs?searchtype=author&query=Tallapragada%2C+P">Phanindra Tallapragada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version of paper in IROS 2023. View the supplemental video at <a href="https://youtu.be/9hKQilCpeaw">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12483" title="Abstract">arXiv:2304.12483</a> (replaced) [<a href="/pdf/2304.12483" title="Download PDF">pdf</a>, <a href="/format/2304.12483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Realistic Generative 3D Face Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rai%2C+A">Aashish Rai</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H">Hiresh Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+A">Ayush Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Carrasco%2C+F+V">Francisco Vicente Carrasco</a>, 
<a href="/search/cs?searchtype=author&query=Takagi%2C+S+J">Shingo Jason Takagi</a>, 
<a href="/search/cs?searchtype=author&query=Aubel%2C+A">Amaury Aubel</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Daeil Kim</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+A">Aayush Prakash</a>, 
<a href="/search/cs?searchtype=author&query=de+la+Torre%2C+F">Fernando de la Torre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01028" title="Abstract">arXiv:2305.01028</a> (replaced) [<a href="/pdf/2305.01028" title="Download PDF">pdf</a>, <a href="/format/2305.01028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Company classification using zero-shot learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rizinski%2C+M">Maryan Rizinski</a>, 
<a href="/search/cs?searchtype=author&query=Jankov%2C+A">Andrej Jankov</a>, 
<a href="/search/cs?searchtype=author&query=Sankaradas%2C+V">Vignesh Sankaradas</a>, 
<a href="/search/cs?searchtype=author&query=Pinsky%2C+E">Eugene Pinsky</a>, 
<a href="/search/cs?searchtype=author&query=Miskovski%2C+I">Igor Miskovski</a>, 
<a href="/search/cs?searchtype=author&query=Trajanov%2C+D">Dimitar Trajanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, 4 tables, conference paper, published in the 20th International Conference on Informatics and Information Technologies (CIIT 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01834" title="Abstract">arXiv:2305.01834</a> (replaced) [<a href="/pdf/2305.01834" title="Download PDF">pdf</a>, <a href="/format/2305.01834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous search of real-life environments combining dynamical  system-based path planning and unsupervised learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amadasun%2C+U+P">Uyiosa Philip Amadasun</a>, 
<a href="/search/cs?searchtype=author&query=McNamee%2C+P">Patrick McNamee</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadabadi%2C+Z+N">Zahra Nili Ahmadabadi</a>, 
<a href="/search/cs?searchtype=author&query=Naseradinmousavi%2C+P">Peiman Naseradinmousavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03129" title="Abstract">arXiv:2305.03129</a> (replaced) [<a href="/pdf/2305.03129" title="Download PDF">pdf</a>, <a href="/format/2305.03129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Program Synthesis for Robot Learning from Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patton%2C+N">Noah Patton</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+K">Kia Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Missula%2C+M">Meghana Missula</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Dillig%2C+I">I&#x15f;il Dillig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 Pages, Extended Version of POPL 2024 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05480" title="Abstract">arXiv:2305.05480</a> (replaced) [<a href="/pdf/2305.05480" title="Download PDF">pdf</a>, <a href="/format/2305.05480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effects of sub-word segmentation on performance of transformer language  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jue Hou</a>, 
<a href="/search/cs?searchtype=author&query=Katinskaia%2C+A">Anisia Katinskaia</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+A">Anh-Duc Vu</a>, 
<a href="/search/cs?searchtype=author&query=Yangarber%2C+R">Roman Yangarber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This submission published in EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> This submission published in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05993" title="Abstract">arXiv:2305.05993</a> (replaced) [<a href="/pdf/2305.05993" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Product Computation using Quantum Entanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Christensen%2C+R+B">Ren&#xe9; B&#xf8;dker Christensen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages. Identical to published journal paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Trans. Quant. Eng., Vol. 4 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06571" title="Abstract">arXiv:2305.06571</a> (replaced) [<a href="/pdf/2305.06571" title="Download PDF">pdf</a>, <a href="/format/2305.06571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Stability for Differential Equations with Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+G">Guihong Wang</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Y">Yuqing Li</a>, 
<a href="/search/math?searchtype=author&query=Luo%2C+T">Tao Luo</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+Z">Zheng Ma</a>, 
<a href="/search/math?searchtype=author&query=Yip%2C+N+K">Nung Kwan Yip</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+G">Guang Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07512" title="Abstract">arXiv:2305.07512</a> (replaced) [<a href="/pdf/2305.07512" title="Download PDF">pdf</a>, <a href="/format/2305.07512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn to Unlearn: A Survey on Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Youyang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+W">Wei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Rakotoarivelo%2C+T">Thierry Rakotoarivelo</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+D">David Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10547" title="Abstract">arXiv:2305.10547</a> (replaced) [<a href="/pdf/2305.10547" title="Download PDF">pdf</a>, <a href="/format/2305.10547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Multimodal Content Moderation from an Asymmetric Angle with  Mixed-modality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jialin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Ye Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+G">Gaurav Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+M">Matthew Hall</a>, 
<a href="/search/cs?searchtype=author&query=Sajeev%2C+S">Sandra Sajeev</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11129" title="Abstract">arXiv:2305.11129</a> (replaced) [<a href="/pdf/2305.11129" title="Download PDF">pdf</a>, <a href="/format/2305.11129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mLongT5: A Multilingual and Efficient Text-To-Text Transformer for  Longer Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uthus%2C+D">David Uthus</a>, 
<a href="/search/cs?searchtype=author&query=Onta%C3%B1%C3%B3n%2C+S">Santiago Onta&#xf1;&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Ainslie%2C+J">Joshua Ainslie</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mandy Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11165" title="Abstract">arXiv:2305.11165</a> (replaced) [<a href="/pdf/2305.11165" title="Download PDF">pdf</a>, <a href="/ps/2305.11165" title="Download PostScript">ps</a>, <a href="/format/2305.11165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The noise level in linear regression with dependent data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziemann%2C+I">Ingvar Ziemann</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+S">Stephen Tu</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Matni%2C+N">Nikolai Matni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11475" title="Abstract">arXiv:2305.11475</a> (replaced) [<a href="/pdf/2305.11475" title="Download PDF">pdf</a>, <a href="/format/2305.11475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curve Your Enthusiasm: Concurvity Regularization in Differentiable  Generalized Additive Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siems%2C+J">Julien Siems</a>, 
<a href="/search/cs?searchtype=author&query=Ditschuneit%2C+K">Konstantin Ditschuneit</a>, 
<a href="/search/cs?searchtype=author&query=Ripken%2C+W">Winfried Ripken</a>, 
<a href="/search/cs?searchtype=author&query=Lindborg%2C+A">Alma Lindborg</a>, 
<a href="/search/cs?searchtype=author&query=Schambach%2C+M">Maximilian Schambach</a>, 
<a href="/search/cs?searchtype=author&query=Otterbach%2C+J+S">Johannes S. Otterbach</a>, 
<a href="/search/cs?searchtype=author&query=Genzel%2C+M">Martin Genzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12029" title="Abstract">arXiv:2305.12029</a> (replaced) [<a href="/pdf/2305.12029" title="Download PDF">pdf</a>, <a href="/format/2305.12029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational  Transcript Cleanup
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zayats%2C+V">Vicky Zayats</a>, 
<a href="/search/cs?searchtype=author&query=Rocholl%2C+J+C">Johann C. Rocholl</a>, 
<a href="/search/cs?searchtype=author&query=Walker%2C+D+D">Daniel D. Walker</a>, 
<a href="/search/cs?searchtype=author&query=Padfield%2C+D">Dirk Padfield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference. Dataset: <a href="https://github.com/huashen218/MultiTurnCleanup">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12672" title="Abstract">arXiv:2305.12672</a> (replaced) [<a href="/pdf/2305.12672" title="Download PDF">pdf</a>, <a href="/format/2305.12672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block Coordinate Plug-and-Play Methods for Blind Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gan%2C+W">Weijie Gan</a>, 
<a href="/search/eess?searchtype=author&query=Shoushtari%2C+S">Shirin Shoushtari</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yuyang Hu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jiaming Liu</a>, 
<a href="/search/eess?searchtype=author&query=An%2C+H">Hongyu An</a>, 
<a href="/search/eess?searchtype=author&query=Kamilov%2C+U+S">Ulugbek S. Kamilov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13455" title="Abstract">arXiv:2305.13455</a> (replaced) [<a href="/pdf/2305.13455" title="Download PDF">pdf</a>, <a href="/format/2305.13455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as  Conversational Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chalamalasetti%2C+K">Kranti Chalamalasetti</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6tze%2C+J">Jana G&#xf6;tze</a>, 
<a href="/search/cs?searchtype=author&query=Hakimov%2C+S">Sherzod Hakimov</a>, 
<a href="/search/cs?searchtype=author&query=Madureira%2C+B">Brielen Madureira</a>, 
<a href="/search/cs?searchtype=author&query=Sadler%2C+P">Philipp Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Schlangen%2C+D">David Schlangen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13661" title="Abstract">arXiv:2305.13661</a> (replaced) [<a href="/pdf/2305.13661" title="Download PDF">pdf</a>, <a href="/format/2305.13661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Risk of Misinformation Pollution with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yikang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings; Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13788" title="Abstract">arXiv:2305.13788</a> (replaced) [<a href="/pdf/2305.13788" title="Download PDF">pdf</a>, <a href="/format/2305.13788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Capture Dissenting Human Voices?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Noah Lee</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+N+M">Na Min An</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13850" title="Abstract">arXiv:2305.13850</a> (replaced) [<a href="/pdf/2305.13850" title="Download PDF">pdf</a>, <a href="/format/2305.13850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Structure Knowledge-Guided Relation Extraction Method for  Visually-Rich Document
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangnan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Q">Qian Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juncheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+D">Duo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaozhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14215" title="Abstract">arXiv:2305.14215</a> (replaced) [<a href="/pdf/2305.14215" title="Download PDF">pdf</a>, <a href="/format/2305.14215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Chain-of-Thought Style Prompting for Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tai%2C+C">Chang-You Tai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziru Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianshu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main; long paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14218" title="Abstract">arXiv:2305.14218</a> (replaced) [<a href="/pdf/2305.14218" title="Download PDF">pdf</a>, <a href="/format/2305.14218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DUBLIN -- Document Understanding By Language-Image Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+K">Kriti Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+A">Aditi Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Tanmay%2C+K">Kumar Tanmay</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+O+M">Owais Mohammed Khan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+M">Monojit Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+H+H">Hardik Hansrajbhai Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Som%2C+S">Subhojit Som</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+V">Vishrav Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Tiwary%2C+S">Saurabh Tiwary</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14282" title="Abstract">arXiv:2305.14282</a> (replaced) [<a href="/pdf/2305.14282" title="Download PDF">pdf</a>, <a href="/format/2305.14282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhenqiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+M">Markus Freitag</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14695" title="Abstract">arXiv:2305.14695</a> (replaced) [<a href="/pdf/2305.14695" title="Download PDF">pdf</a>, <a href="/format/2305.14695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Causal View of Entity Bias in (Large) Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+W">Wenjie Mo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenxuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14800" title="Abstract">arXiv:2305.14800</a> (replaced) [<a href="/pdf/2305.14800" title="Download PDF">pdf</a>, <a href="/format/2305.14800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Diverse In-Context Configurations for Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongliang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingzhuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xin Geng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14815" title="Abstract">arXiv:2305.14815</a> (replaced) [<a href="/pdf/2305.14815" title="Download PDF">pdf</a>, <a href="/format/2305.14815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Reading Comprehension using Case-based Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thai%2C+D">Dung Thai</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+D">Dhruv Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhary%2C+M">Mudit Chaudhary</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+R">Rajarshi Das</a>, 
<a href="/search/cs?searchtype=author&query=Zaheer%2C+M">Manzil Zaheer</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jay-Yoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15328" title="Abstract">arXiv:2305.15328</a> (replaced) [<a href="/pdf/2305.15328" title="Download PDF">pdf</a>, <a href="/format/2305.15328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Programming for Text-to-Image Generation and Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jaemin Cho</a>, 
<a href="/search/cs?searchtype=author&query=Zala%2C+A">Abhay Zala</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; Project website: <a href="https://vp-t2i.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16379" title="Abstract">arXiv:2305.16379</a> (replaced) [<a href="/pdf/2305.16379" title="Download PDF">pdf</a>, <a href="/format/2305.16379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Better with Less: Effective Augmentation for Sample-Efficient  Visual Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guozheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17161" title="Abstract">arXiv:2305.17161</a> (replaced) [<a href="/pdf/2305.17161" title="Download PDF">pdf</a>, <a href="/format/2305.17161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow Matching for Scalable Simulation-Based Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dax%2C+M">Maximilian Dax</a>, 
<a href="/search/cs?searchtype=author&query=Wildberger%2C+J">Jonas Wildberger</a>, 
<a href="/search/cs?searchtype=author&query=Buchholz%2C+S">Simon Buchholz</a>, 
<a href="/search/cs?searchtype=author&query=Green%2C+S+R">Stephen R. Green</a>, 
<a href="/search/cs?searchtype=author&query=Macke%2C+J+H">Jakob H. Macke</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code available at <a href="https://github.com/dingo-gw/flow-matching-posterior-estimation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17975" title="Abstract">arXiv:2305.17975</a> (replaced) [<a href="/pdf/2305.17975" title="Download PDF">pdf</a>, <a href="/format/2305.17975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jigsaw: Learning to Assemble Multiple Fractured Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiaxin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yifan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qixing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18370" title="Abstract">arXiv:2305.18370</a> (replaced) [<a href="/pdf/2305.18370" title="Download PDF">pdf</a>, <a href="/format/2305.18370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Brain Age Prediction using coVariance Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sihag%2C+S">Saurabh Sihag</a>, 
<a href="/search/q-bio?searchtype=author&query=Mateos%2C+G">Gonzalo Mateos</a>, 
<a href="/search/q-bio?searchtype=author&query=McMillan%2C+C">Corey McMillan</a>, 
<a href="/search/q-bio?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready version for NeurIPS 2023. arXiv admin note: substantial text overlap with <a href="/abs/2305.01807">arXiv:2305.01807</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18499" title="Abstract">arXiv:2305.18499</a> (replaced) [<a href="/pdf/2305.18499" title="Download PDF">pdf</a>, <a href="/format/2305.18499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training Contextualized World Models with In-the-wild Videos for  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jialong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haoyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chaoyi Deng</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code is available at <a href="https://github.com/thuml/ContextWM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18728" title="Abstract">arXiv:2305.18728</a> (replaced) [<a href="/pdf/2305.18728" title="Download PDF">pdf</a>, <a href="/format/2305.18728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug-in Performative Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Licong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zrnic%2C+T">Tijana Zrnic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19068" title="Abstract">arXiv:2305.19068</a> (replaced) [<a href="/pdf/2305.19068" title="Download PDF">pdf</a>, <a href="/format/2305.19068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complex Query Answering on Eventuality Knowledge Graph with Implicit  Logical Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiaxin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19079" title="Abstract">arXiv:2305.19079</a> (replaced) [<a href="/pdf/2305.19079" title="Download PDF">pdf</a>, <a href="/format/2305.19079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Sample Complexity of Self-Supervised Image Reconstruction  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Klug%2C+T">Tobit Klug</a>, 
<a href="/search/eess?searchtype=author&query=Atik%2C+D">Dogukan Atik</a>, 
<a href="/search/eess?searchtype=author&query=Heckel%2C+R">Reinhard Heckel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19181" title="Abstract">arXiv:2305.19181</a> (replaced) [<a href="/pdf/2305.19181" title="Download PDF">pdf</a>, <a href="/ps/2305.19181" title="Download PostScript">ps</a>, <a href="/format/2305.19181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Table Detection for Visually Rich Document Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Bin Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Simsek%2C+M">Murat Simsek</a>, 
<a href="/search/cs?searchtype=author&query=Kantarci%2C+B">Burak Kantarci</a>, 
<a href="/search/cs?searchtype=author&query=Alkheir%2C+A+A">Ala Abu Alkheir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Knowledge-Based Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19478" title="Abstract">arXiv:2305.19478</a> (replaced) [<a href="/pdf/2305.19478" title="Download PDF">pdf</a>, <a href="/format/2305.19478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Permutation-Aware Action Segmentation via Unsupervised Frame-to-Segment  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+Q">Quoc-Huy Tran</a>, 
<a href="/search/cs?searchtype=author&query=Mehmood%2C+A">Ahmed Mehmood</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M">Muhammad Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Naufil%2C+M">Muhammad Naufil</a>, 
<a href="/search/cs?searchtype=author&query=Zafar%2C+A">Anas Zafar</a>, 
<a href="/search/cs?searchtype=author&query=Konin%2C+A">Andrey Konin</a>, 
<a href="/search/cs?searchtype=author&query=Zia%2C+M+Z">M. Zeeshan Zia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19742" title="Abstract">arXiv:2305.19742</a> (replaced) [<a href="/pdf/2305.19742" title="Download PDF">pdf</a>, <a href="/format/2305.19742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable Off-Policy Learning for Dosage Combinations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schweisthal%2C+J">Jonas Schweisthal</a>, 
<a href="/search/cs?searchtype=author&query=Frauen%2C+D">Dennis Frauen</a>, 
<a href="/search/cs?searchtype=author&query=Melnychuk%2C+V">Valentyn Melnychuk</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00006" title="Abstract">arXiv:2306.00006</a> (replaced) [<a href="/pdf/2306.00006" title="Download PDF">pdf</a>, <a href="/format/2306.00006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Truncated Affinity Maximization: One-class Homophily Modeling for Graph  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+H">Hezhe Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00323" title="Abstract">arXiv:2306.00323</a> (replaced) [<a href="/pdf/2306.00323" title="Download PDF">pdf</a>, <a href="/format/2306.00323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thought Cloning: Learning to Think while Acting by Imitating Human  Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengran Hu</a>, 
<a href="/search/cs?searchtype=author&query=Clune%2C+J">Jeff Clune</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 as a spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00777" title="Abstract">arXiv:2306.00777</a> (replaced) [<a href="/pdf/2306.00777" title="Download PDF">pdf</a>, <a href="/format/2306.00777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object pop-up: Can we infer 3D objects and their poses from human  interactions alone?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrov%2C+I+A">Ilya A. Petrov</a>, 
<a href="/search/cs?searchtype=author&query=Marin%2C+R">Riccardo Marin</a>, 
<a href="/search/cs?searchtype=author&query=Chibane%2C+J">Julian Chibane</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01243" title="Abstract">arXiv:2306.01243</a> (replaced) [<a href="/pdf/2306.01243" title="Download PDF">pdf</a>, <a href="/format/2306.01243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Reinforcement Learning with Impaired Observability: Learning  to Act with Delayed and Missing State Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minshuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+J">Jie Meng</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yinyu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01424" title="Abstract">arXiv:2306.01424</a> (replaced) [<a href="/pdf/2306.01424" title="Download PDF">pdf</a>, <a href="/format/2306.01424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial Counterfactual Identification of Continuous Outcomes with a  Curvature Sensitivity Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Melnychuk%2C+V">Valentyn Melnychuk</a>, 
<a href="/search/stat?searchtype=author&query=Frauen%2C+D">Dennis Frauen</a>, 
<a href="/search/stat?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 37th Conference on Neural Information
  Processing Systems (NeurIPS 2023), New Orleans, Louisiana, USA, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01708" title="Abstract">arXiv:2306.01708</a> (replaced) [<a href="/pdf/2306.01708" title="Download PDF">pdf</a>, <a href="/format/2306.01708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TIES-Merging: Resolving Interference When Merging Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yadav%2C+P">Prateek Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Tam%2C+D">Derek Tam</a>, 
<a href="/search/cs?searchtype=author&query=Choshen%2C+L">Leshem Choshen</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023, 23 Pages, 13 Figures, 14 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01859" title="Abstract">arXiv:2306.01859</a> (replaced) [<a href="/pdf/2306.01859" title="Download PDF">pdf</a>, <a href="/format/2306.01859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatially Resolved Gene Expression Prediction from H&amp;E Histology Images  via Bi-modal Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ronald Xie</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+K">Kuan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+S+W">Sai W. Chung</a>, 
<a href="/search/cs?searchtype=author&query=Perciani%2C+C+T">Catia T. Perciani</a>, 
<a href="/search/cs?searchtype=author&query=MacParland%2C+S+A">Sonya A. MacParland</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bader%2C+G+D">Gary D. Bader</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02531" title="Abstract">arXiv:2306.02531</a> (replaced) [<a href="/pdf/2306.02531" title="Download PDF">pdf</a>, <a href="/format/2306.02531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PLANNER: Generating Diversified Paragraph via Latent Language Diffusion  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiatao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhuofeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shuangfei Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J">Josh Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Jaitly%2C+N">Navdeep Jaitly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03266" title="Abstract">arXiv:2306.03266</a> (replaced) [<a href="/pdf/2306.03266" title="Download PDF">pdf</a>, <a href="/format/2306.03266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending the Design Space of Graph Neural Networks by Rethinking  Folklore Weisfeiler-Lehman
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiarui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lecheng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fuhai Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03783" title="Abstract">arXiv:2306.03783</a> (replaced) [<a href="/pdf/2306.03783" title="Download PDF">pdf</a>, <a href="/format/2306.03783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotics of Bayesian Uncertainty Estimation in Random Features  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Baek%2C+Y">Youngsoo Baek</a>, 
<a href="/search/stat?searchtype=author&query=Berchuck%2C+S+I">Samuel I. Berchuck</a>, 
<a href="/search/stat?searchtype=author&query=Mukherjee%2C+S">Sayan Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03792" title="Abstract">arXiv:2306.03792</a> (replaced) [<a href="/pdf/2306.03792" title="Download PDF">pdf</a>, <a href="/format/2306.03792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAMO: Fast Adaptive Multitask Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04220" title="Abstract">arXiv:2306.04220</a> (replaced) [<a href="/pdf/2306.04220" title="Download PDF">pdf</a>, <a href="/format/2306.04220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look Beneath the Surface: Exploiting Fundamental Symmetry for  Sample-Efficient Offline RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xianyuan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shoucheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youfang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Li Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023; The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04375" title="Abstract">arXiv:2306.04375</a> (replaced) [<a href="/pdf/2306.04375" title="Download PDF">pdf</a>, <a href="/ps/2306.04375" title="Download PostScript">ps</a>, <a href="/format/2306.04375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning via Wasserstein-Based High Probability Generalisation Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Viallard%2C+P">Paul Viallard</a>, 
<a href="/search/stat?searchtype=author&query=Haddouche%2C+M">Maxime Haddouche</a>, 
<a href="/search/stat?searchtype=author&query=%C5%9Eim%C5%9Fekli%2C+U">Umut &#x15e;im&#x15f;ekli</a>, 
<a href="/search/stat?searchtype=author&query=Guedj%2C+B">Benjamin Guedj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04406" title="Abstract">arXiv:2306.04406</a> (replaced) [<a href="/pdf/2306.04406" title="Download PDF">pdf</a>, <a href="/format/2306.04406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Teacher Forcing for Learning Chaotic Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hess%2C+F">Florian Hess</a>, 
<a href="/search/cs?searchtype=author&query=Monfared%2C+Z">Zahra Monfared</a>, 
<a href="/search/cs?searchtype=author&query=Brenner%2C+M">Manuel Brenner</a>, 
<a href="/search/cs?searchtype=author&query=Durstewitz%2C+D">Daniel Durstewitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the Proceedings of the 40th International Conference on Machine Learning (ICML 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PMLR 202:13017-13049, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS); Chaotic Dynamics (nlin.CD)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06546" title="Abstract">arXiv:2306.06546</a> (replaced) [<a href="/pdf/2306.06546" title="Download PDF">pdf</a>, <a href="/format/2306.06546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Fidelity Audio Compression with Improved RVQGAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Rithesh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Seetharaman%2C+P">Prem Seetharaman</a>, 
<a href="/search/cs?searchtype=author&query=Luebs%2C+A">Alejandro Luebs</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+I">Ishaan Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+K">Kundan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06836" title="Abstract">arXiv:2306.06836</a> (replaced) [<a href="/pdf/2306.06836" title="Download PDF">pdf</a>, <a href="/format/2306.06836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function  Approximation: Minimax Optimal and Instance-Dependent Regret Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiayi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L+F">Lin F. Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07280" title="Abstract">arXiv:2306.07280</a> (replaced) [<a href="/pdf/2306.07280" title="Download PDF">pdf</a>, <a href="/format/2306.07280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Text-to-Image Diffusion by Orthogonal Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zeju Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Haiwen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yuxuan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (43 pages, 34 figures, project page: <a href="https://oft.wyliu.com/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08445" title="Abstract">arXiv:2306.08445</a> (replaced) [<a href="/pdf/2306.08445" title="Download PDF">pdf</a>, <a href="/format/2306.08445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Gaussian Markov Random Fields for Graph-Structured Dynamical  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lippert%2C+F">Fiona Lippert</a>, 
<a href="/search/cs?searchtype=author&query=Kranstauber%2C+B">Bart Kranstauber</a>, 
<a href="/search/cs?searchtype=author&query=van+Loon%2C+E+E">E. Emiel van Loon</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08772" title="Abstract">arXiv:2306.08772</a> (replaced) [<a href="/pdf/2306.08772" title="Download PDF">pdf</a>, <a href="/format/2306.08772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Katakomba: Tools and Benchmarks for Data-Driven NetHack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+V">Vladislav Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Tarasov%2C+D">Denis Tarasov</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+S">Sergey Kolesnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Source code at <a href="https://github.com/corl-team/katakomba">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09312" title="Abstract">arXiv:2306.09312</a> (replaced) [<a href="/pdf/2306.09312" title="Download PDF">pdf</a>, <a href="/format/2306.09312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic HELM: A Human-Readable Memory for Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paischer%2C+F">Fabian Paischer</a>, 
<a href="/search/cs?searchtype=author&query=Adler%2C+T">Thomas Adler</a>, 
<a href="/search/cs?searchtype=author&query=Hofmarcher%2C+M">Markus Hofmarcher</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at NeurIPS 2023, 10 pages (+ references and appendix), Code: <a href="https://github.com/ml-jku/helm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09539" title="Abstract">arXiv:2306.09539</a> (replaced) [<a href="/pdf/2306.09539" title="Download PDF">pdf</a>, <a href="/format/2306.09539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block-State Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fathi%2C+M">Mahan Fathi</a>, 
<a href="/search/cs?searchtype=author&query=Pilault%2C+J">Jonathan Pilault</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+C">Christopher Pal</a>, 
<a href="/search/cs?searchtype=author&query=Firat%2C+O">Orhan Firat</a>, 
<a href="/search/cs?searchtype=author&query=Goroshin%2C+R">Ross Goroshin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'23 - Thirty-seventh Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09850" title="Abstract">arXiv:2306.09850</a> (replaced) [<a href="/pdf/2306.09850" title="Download PDF">pdf</a>, <a href="/format/2306.09850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Sharpness-Aware Minimization Cannot Converge All the Way to  Optima
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+D">Dongkuk Si</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+C">Chulhee Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages. v3 NeurIPS 2023 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10225" title="Abstract">arXiv:2306.10225</a> (replaced) [<a href="/pdf/2306.10225" title="Download PDF">pdf</a>, <a href="/format/2306.10225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Genes in Intelligent Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xin Geng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11551" title="Abstract">arXiv:2306.11551</a> (replaced) [<a href="/pdf/2306.11551" title="Download PDF">pdf</a>, <a href="/format/2306.11551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMP-MARL: a Suite of Environments for Large-scale Infrastructure  Management Planning via MARL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leroy%2C+P">Pascal Leroy</a>, 
<a href="/search/cs?searchtype=author&query=Morato%2C+P+G">Pablo G. Morato</a>, 
<a href="/search/cs?searchtype=author&query=Pisane%2C+J">Jonathan Pisane</a>, 
<a href="/search/cs?searchtype=author&query=Kolios%2C+A">Athanasios Kolios</a>, 
<a href="/search/cs?searchtype=author&query=Ernst%2C+D">Damien Ernst</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11641" title="Abstract">arXiv:2306.11641</a> (replaced) [<a href="/pdf/2306.11641" title="Download PDF">pdf</a>, <a href="/ps/2306.11641" title="Download PostScript">ps</a>, <a href="/format/2306.11641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALSA VERDE: a machine learning attack on Learning With Errors with  sparse small secrets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C+Y">Cathy Yuanchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+E">Emily Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Allen-Zhu%2C+Z">Zeyuan Allen-Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Charton%2C+F">Francois Charton</a>, 
<a href="/search/cs?searchtype=author&query=Lauter%2C+K">Kristin Lauter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11835" title="Abstract">arXiv:2306.11835</a> (replaced) [<a href="/pdf/2306.11835" title="Download PDF">pdf</a>, <a href="/format/2306.11835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Parallax: A Geometric Specification for Deep Perception  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+A+D">Abraham D. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+M+J">Michael J. Catanzaro</a>, 
<a href="/search/cs?searchtype=author&query=Angeloro%2C+G">Gabrielle Angeloro</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nirav Patel</a>, 
<a href="/search/cs?searchtype=author&query=Bendich%2C+P">Paul Bendich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures. Preprint submitted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13229" title="Abstract">arXiv:2306.13229</a> (replaced) [<a href="/pdf/2306.13229" title="Download PDF">pdf</a>, <a href="/format/2306.13229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TACO: Temporal Latent Action-Driven Contrastive Loss for Visual  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14884" title="Abstract">arXiv:2306.14884</a> (replaced) [<a href="/pdf/2306.14884" title="Download PDF">pdf</a>, <a href="/format/2306.14884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Modulate pre-trained Models in RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmied%2C+T">Thomas Schmied</a>, 
<a href="/search/cs?searchtype=author&query=Hofmarcher%2C+M">Markus Hofmarcher</a>, 
<a href="/search/cs?searchtype=author&query=Paischer%2C+F">Fabian Paischer</a>, 
<a href="/search/cs?searchtype=author&query=Pascanu%2C+R">Razvan Pascanu</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages (+ references and appendix), Code: <a href="https://github.com/ml-jku/L2M">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15162" title="Abstract">arXiv:2306.15162</a> (replaced) [<a href="/pdf/2306.15162" title="Download PDF">pdf</a>, <a href="/format/2306.15162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English  Parallel Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uthus%2C+D">David Uthus</a>, 
<a href="/search/cs?searchtype=author&query=Tanzer%2C+G">Garrett Tanzer</a>, 
<a href="/search/cs?searchtype=author&query=Georg%2C+M">Manfred Georg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15626" title="Abstract">arXiv:2306.15626</a> (replaced) [<a href="/pdf/2306.15626" title="Download PDF">pdf</a>, <a href="/format/2306.15626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LeanDojo: Theorem Proving with Retrieval-Augmented Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kaiyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Swope%2C+A+M">Aidan M. Swope</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+A">Alex Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chalamala%2C+R">Rahul Chalamala</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+P">Peiyang Song</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Godil%2C+S">Saad Godil</a>, 
<a href="/search/cs?searchtype=author&query=Prenger%2C+R">Ryan Prenger</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (Datasets and Benchmarks Track) as an oral presentation. Data, code, and models available at <a href="https://leandojo.org/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16564" title="Abstract">arXiv:2306.16564</a> (replaced) [<a href="/pdf/2306.16564" title="Download PDF">pdf</a>, <a href="/format/2306.16564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Calibration and Error Correction for Generative Large Language  Models via Pareto Optimal Self-Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Theodore Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Mu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Preston%2C+J+S">J. Samuel Preston</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+H">Hoifung Poon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16844" title="Abstract">arXiv:2306.16844</a> (replaced) [<a href="/pdf/2306.16844" title="Download PDF">pdf</a>, <a href="/format/2306.16844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Macro Placement by Wire-Mask-Guided Black-Box Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yunqi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+K">Ke Xue</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lei Song</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update NeurIPS'23 camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01616" title="Abstract">arXiv:2307.01616</a> (replaced) [<a href="/pdf/2307.01616" title="Download PDF">pdf</a>, <a href="/format/2307.01616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SageFormer: Series-Aware Framework for Long-term Multivariate Time  Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Linghang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuantao Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02318" title="Abstract">arXiv:2307.02318</a> (replaced) [<a href="/pdf/2307.02318" title="Download PDF">pdf</a>, <a href="/format/2307.02318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Contract Design via Discontinuous Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tonghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCtting%2C+P">Paul D&#xfc;tting</a>, 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+D">Dmitry Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Talgam-Cohen%2C+I">Inbal Talgam-Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03298" title="Abstract">arXiv:2307.03298</a> (replaced) [<a href="/pdf/2307.03298" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spherical CNN for Medical Imaging Applications: Importance of  Equivariance in image reconstruction and denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hashemi%2C+A">Amirreza Hashemi</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+Y">Yuemeng Feng</a>, 
<a href="/search/eess?searchtype=author&query=Sabet%2C+H">Hamid Sabet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03672" title="Abstract">arXiv:2307.03672</a> (replaced) [<a href="/pdf/2307.03672" title="Download PDF">pdf</a>, <a href="/format/2307.03672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation-free Schr&#xf6;dinger bridges via score and flow matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+A">Alexander Tong</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Fatras%2C+K">Kilian Fatras</a>, 
<a href="/search/cs?searchtype=author&query=Atanackovic%2C+L">Lazar Atanackovic</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanlei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huguet%2C+G">Guillaume Huguet</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+G">Guy Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A version of this paper appeared in the New Frontiers in Learning, Control, and Dynamical Systems workshop at ICML 2023. Code: <a href="https://github.com/atong01/conditional-flow-matching">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04090" title="Abstract">arXiv:2307.04090</a> (replaced) [<a href="/pdf/2307.04090" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roush%2C+A">Allen Roush</a>, 
<a href="/search/cs?searchtype=author&query=Mezzetti%2C+D">David Mezzetti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, Accepted to The 4th New Frontiers in Summarization Workshop (EMNLP 2023), System Demonstration paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05902" title="Abstract">arXiv:2307.05902</a> (replaced) [<a href="/pdf/2307.05902" title="Download PDF">pdf</a>, <a href="/format/2307.05902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Guarantees for Feature Attributions with Multiplicative  Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+A">Anton Xue</a>, 
<a href="/search/cs?searchtype=author&query=Alur%2C+R">Rajeev Alur</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06095" title="Abstract">arXiv:2307.06095</a> (replaced) [<a href="/pdf/2307.06095" title="Download PDF">pdf</a>, <a href="/format/2307.06095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Resource Allocation for Fair Wireless Relay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arribas%2C+E">Edgar Arribas</a>, 
<a href="/search/cs?searchtype=author&query=Cholvi%2C+V">Vicent Cholvi</a>, 
<a href="/search/cs?searchtype=author&query=Mancuso%2C+V">Vincenzo Mancuso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06947" title="Abstract">arXiv:2307.06947</a> (replaced) [<a href="/pdf/2307.06947" title="Download PDF">pdf</a>, <a href="/format/2307.06947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wasim%2C+S+T">Syed Talal Wasim</a>, 
<a href="/search/cs?searchtype=author&query=Khattak%2C+M+U">Muhammad Uzair Khattak</a>, 
<a href="/search/cs?searchtype=author&query=Naseer%2C+M">Muzammal Naseer</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Mubarak Shah</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV-2023. Camera-Ready version. Project page: <a href="https://TalalWasim.github.io/Video-FocalNets/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08623" title="Abstract">arXiv:2307.08623</a> (replaced) [<a href="/pdf/2307.08623" title="Download PDF">pdf</a>, <a href="/format/2307.08623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HYTREL: Hypergraph-enhanced Tabular Data Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumajyoti Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Lausen%2C+L">Leonard Lausen</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+S">Sheng Zha</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruihong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10088" title="Abstract">arXiv:2307.10088</a> (replaced) [<a href="/pdf/2307.10088" title="Download PDF">pdf</a>, <a href="/format/2307.10088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Android in the Wild: A Large-Scale Dataset for Android Device Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rawles%2C+C">Christopher Rawles</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Alice Li</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+D">Daniel Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Riva%2C+O">Oriana Riva</a>, 
<a href="/search/cs?searchtype=author&query=Lillicrap%2C+T">Timothy Lillicrap</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11957" title="Abstract">arXiv:2307.11957</a> (replaced) [<a href="/pdf/2307.11957" title="Download PDF">pdf</a>, <a href="/format/2307.11957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-performance real-world optical computing trained by in situ  model-free optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Zhao%2C+G">Guangyuan Zhao</a>, 
<a href="/search/physics?searchtype=author&query=Shu%2C+X">Xin Shu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Computer Vision and Pattern Recognition (cs.CV); Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12233" title="Abstract">arXiv:2307.12233</a> (replaced) [<a href="/pdf/2307.12233" title="Download PDF">pdf</a>, <a href="/format/2307.12233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Consensus-based Reference Generation for the Regulation of  Open-Channel Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fabris%2C+M">Marco Fabris</a>, 
<a href="/search/eess?searchtype=author&query=Bellinazzi%2C+M+D">Marco D. Bellinazzi</a>, 
<a href="/search/eess?searchtype=author&query=Furlanetto%2C+A">Andrea Furlanetto</a>, 
<a href="/search/eess?searchtype=author&query=Cenedese%2C+A">Angelo Cenedese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures, submitted to IEEE Access (version 2)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12868" title="Abstract">arXiv:2307.12868</a> (replaced) [<a href="/pdf/2307.12868" title="Download PDF">pdf</a>, <a href="/format/2307.12868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Latent Space of Diffusion Models through the Lens of  Riemannian Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yong-Hyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+M">Mingi Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaewoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+J">Junghyo Jo</a>, 
<a href="/search/cs?searchtype=author&query=Uh%2C+Y">Youngjung Uh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13613" title="Abstract">arXiv:2307.13613</a> (replaced) [<a href="/pdf/2307.13613" title="Download PDF">pdf</a>, <a href="/ps/2307.13613" title="Download PostScript">ps</a>, <a href="/format/2307.13613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eigenvalue Bounds for Sum-Rank-Metric Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Abiad%2C+A">Aida Abiad</a>, 
<a href="/search/math?searchtype=author&query=Khramova%2C+A+P">Antonina P. Khramova</a>, 
<a href="/search/math?searchtype=author&query=Ravagnani%2C+A">Alberto Ravagnani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13661" title="Abstract">arXiv:2307.13661</a> (replaced) [<a href="/pdf/2307.13661" title="Download PDF">pdf</a>, <a href="/format/2307.13661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Subtyping for Structural Parametric Polymorphism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeYoung%2C+H">Henry DeYoung</a>, 
<a href="/search/cs?searchtype=author&query=Mordido%2C+A">Andreia Mordido</a>, 
<a href="/search/cs?searchtype=author&query=Pfenning%2C+F">Frank Pfenning</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Ankush Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13755" title="Abstract">arXiv:2307.13755</a> (replaced) [<a href="/pdf/2307.13755" title="Download PDF">pdf</a>, <a href="/format/2307.13755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-based Model Refinement and Representation Disagreement for  Semi-Supervised Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marvasti-Zadeh%2C+S+M">Seyed Mojtaba Marvasti-Zadeh</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+N">Nilanjan Ray</a>, 
<a href="/search/cs?searchtype=author&query=Erbilgin%2C+N">Nadir Erbilgin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/CVF Winter Applications of Computer Vision (WACV) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14993" title="Abstract">arXiv:2307.14993</a> (replaced) [<a href="/pdf/2307.14993" title="Download PDF">pdf</a>, <a href="/format/2307.14993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thinker: Learning to Plan and Act
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+S">Stephen Chung</a>, 
<a href="/search/cs?searchtype=author&query=Anokhin%2C+I">Ivan Anokhin</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+D">David Krueger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15662" title="Abstract">arXiv:2307.15662</a> (replaced) [<a href="/pdf/2307.15662" title="Download PDF">pdf</a>, <a href="/ps/2307.15662" title="Download PostScript">ps</a>, <a href="/format/2307.15662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust data-driven learning and control of nonlinear systems. A Sontag&#x27;s  formula approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Becerra-Mora%2C+Y+A">Yeyson A. Becerra-Mora</a>, 
<a href="/search/eess?searchtype=author&query=Acosta%2C+J+%C3%81">Jos&#xe9; &#xc1;ngel Acosta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to journal (under review). 25 pages, 6 figures, font 12pt
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16821" title="Abstract">arXiv:2307.16821</a> (replaced) [<a href="/pdf/2307.16821" title="Download PDF">pdf</a>, <a href="/ps/2307.16821" title="Download PostScript">ps</a>, <a href="/format/2307.16821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Formal Verification of a TPM Software Stack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziani%2C+Y">Yani Ziani</a>, 
<a href="/search/cs?searchtype=author&query=Kosmatov%2C+N">Nikolai Kosmatov</a>, 
<a href="/search/cs?searchtype=author&query=Loulergue%2C+F">Fr&#xe9;d&#xe9;ric Loulergue</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+D+G">Daniel Gracia P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Bernier%2C+T">T&#xe9;o Bernier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00806" title="Abstract">arXiv:2308.00806</a> (replaced) [<a href="/pdf/2308.00806" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Uncertainty in Imbalanced Histopathology Image Classification  of HER2 Breast Cancer: An interpretable Ensemble Approach with Threshold  Filtered Single Instance Evaluation (SIE)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+S+H">Md Sakib Hossain Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>, 
<a href="/search/cs?searchtype=author&query=Hasib%2C+K+M">Khan Md Hasib</a>, 
<a href="/search/cs?searchtype=author&query=Alfarhood%2C+S">Sultan Alfarhood</a>, 
<a href="/search/cs?searchtype=author&query=Safran%2C+M">Mejdl Safran</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+D">Dunren Che</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02122" title="Abstract">arXiv:2308.02122</a> (replaced) [<a href="/pdf/2308.02122" title="Download PDF">pdf</a>, <a href="/format/2308.02122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned  Samples in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+G">Guanhong Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+G">Guangyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04585" title="Abstract">arXiv:2308.04585</a> (replaced) [<a href="/pdf/2308.04585" title="Download PDF">pdf</a>, <a href="/ps/2308.04585" title="Download PostScript">ps</a>, <a href="/format/2308.04585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel Single Proxy Control for Deterministic Confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+L">Liyuan Xu</a>, 
<a href="/search/stat?searchtype=author&query=Gretton%2C+A">Arthur Gretton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04808" title="Abstract">arXiv:2308.04808</a> (replaced) [<a href="/pdf/2308.04808" title="Download PDF">pdf</a>, <a href="/format/2308.04808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint-Relation Transformer for Multi-Person Motion Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qingyao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+W">Weibo Mao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jingze Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04904" title="Abstract">arXiv:2308.04904</a> (replaced) [<a href="/pdf/2308.04904" title="Download PDF">pdf</a>, <a href="/format/2308.04904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StableVQA: A Deep No-Reference Quality Assessment Model for Video  Stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kou%2C+T">Tengchuan Kou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+X">Xiongkuo Min</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05037" title="Abstract">arXiv:2308.05037</a> (replaced) [<a href="/pdf/2308.05037" title="Download PDF">pdf</a>, <a href="/format/2308.05037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separate Anything You Describe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xubo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+Q">Qiuqiang Kong</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Y">Yan Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Haohe Liu</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+Y">Yi Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yuzhuo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+R">Rui Xia</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yuxuan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Plumbley%2C+M+D">Mark D. Plumbley</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Wenwu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code, benchmark and pre-trained models: <a href="https://github.com/Audio-AGI/AudioSep">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05078" title="Abstract">arXiv:2308.05078</a> (replaced) [<a href="/pdf/2308.05078" title="Download PDF">pdf</a>, <a href="/format/2308.05078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Capacity of Low-Rank Dyadic Fading Channels in the Low-SNR Regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Kamal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Sumit Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07817" title="Abstract">arXiv:2308.07817</a> (replaced) [<a href="/pdf/2308.07817" title="Download PDF">pdf</a>, <a href="/format/2308.07817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying the Cost of Learning in Queueing Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freund%2C+D">Daniel Freund</a>, 
<a href="/search/cs?searchtype=author&query=Lykouris%2C+T">Thodoris Lykouris</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+W">Wentao Weng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A condensed version of this work was accepted for presentation at the Conference on Neural Information Processing Systems (NeurIPS 2023). Compared to the first version of the paper, the current version expands the comparison with related work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Performance (cs.PF); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08643" title="Abstract">arXiv:2308.08643</a> (replaced) [<a href="/pdf/2308.08643" title="Download PDF">pdf</a>, <a href="/format/2308.08643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Personalized Federated Learning via Heterogeneous Model  Reassembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xingyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Suhan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+L">Liwei Che</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongkuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08886" title="Abstract">arXiv:2308.08886</a> (replaced) [<a href="/pdf/2308.08886" title="Download PDF">pdf</a>, <a href="/format/2308.08886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Gauss-Newton Directions for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roulet%2C+V">Vincent Roulet</a>, 
<a href="/search/cs?searchtype=author&query=Blondel%2C+M">Mathieu Blondel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the Duality Principles for Modern Machine Learning Workshop at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10397" title="Abstract">arXiv:2308.10397</a> (replaced) [<a href="/pdf/2308.10397" title="Download PDF">pdf</a>, <a href="/format/2308.10397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes  and Biases in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yanhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiabao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jinxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tingjiang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xingjiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10702" title="Abstract">arXiv:2308.10702</a> (replaced) [<a href="/pdf/2308.10702" title="Download PDF">pdf</a>, <a href="/format/2308.10702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Optimal Experimental Design for Constitutive Model Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ricciardi%2C+D">Denielle Ricciardi</a>, 
<a href="/search/cs?searchtype=author&query=Seidl%2C+T">Tom Seidl</a>, 
<a href="/search/cs?searchtype=author&query=Lester%2C+B">Brian Lester</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+A">Amanda Jones</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+E">Elizabeth Jones</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12947" title="Abstract">arXiv:2308.12947</a> (replaced) [<a href="/pdf/2308.12947" title="Download PDF">pdf</a>, <a href="/ps/2308.12947" title="Download PostScript">ps</a>, <a href="/format/2308.12947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting Distinct Elements Under Person-Level Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knop%2C+A">Alexander Knop</a>, 
<a href="/search/cs?searchtype=author&query=Steinke%2C+T">Thomas Steinke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14263" title="Abstract">arXiv:2308.14263</a> (replaced) [<a href="/pdf/2308.14263" title="Download PDF">pdf</a>, <a href="/format/2308.14263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Modal Retrieval: A Systematic Review of Methods and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fengling Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingjing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H+T">Heng Tao Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00184" title="Abstract">arXiv:2309.00184</a> (replaced) [<a href="/pdf/2309.00184" title="Download PDF">pdf</a>, <a href="/format/2309.00184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Network Requirements for Enabling Effective Cyber Deception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M+A">Md Abu Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M">Moqsadur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+A+I">Mohammad Ariful Islam Khan</a>, 
<a href="/search/cs?searchtype=author&query=Tosh%2C+D">Deepak Tosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00591" title="Abstract">arXiv:2309.00591</a> (replaced) [<a href="/pdf/2309.00591" title="Download PDF">pdf</a>, <a href="/format/2309.00591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Regret Optimal Best Arm Identification: Fundamental Limits and  Low-Complexity Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qining Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+L">Lei Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01507" title="Abstract">arXiv:2309.01507</a> (replaced) [<a href="/pdf/2309.01507" title="Download PDF">pdf</a>, <a href="/format/2309.01507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory Efficient Optimizers with 4-bit States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v3: camera ready revisions for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01524" title="Abstract">arXiv:2309.01524</a> (replaced) [<a href="/pdf/2309.01524" title="Download PDF">pdf</a>, <a href="/format/2309.01524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Input Redundancy under Input and State Constraints (Extended version of  the submission accepted to Automatica)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tr%C3%A9gou%C3%ABt%2C+J">Jean-Fran&#xe7;ois Tr&#xe9;gou&#xeb;t</a>, 
<a href="/search/eess?searchtype=author&query=Kreiss%2C+J">J&#xe9;r&#xe9;mie Kreiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02898" title="Abstract">arXiv:2309.02898</a> (replaced) [<a href="/pdf/2309.02898" title="Download PDF">pdf</a>, <a href="/format/2309.02898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for Discovering Discrete Symmetries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karjol%2C+P">Pavan Karjol</a>, 
<a href="/search/cs?searchtype=author&query=Kashyap%2C+R">Rohan Kashyap</a>, 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+A">Aditya Gopalan</a>, 
<a href="/search/cs?searchtype=author&query=P%2C+P+A">Prathosh A.P</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04679" title="Abstract">arXiv:2309.04679</a> (replaced) [<a href="/pdf/2309.04679" title="Download PDF">pdf</a>, <a href="/format/2309.04679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding structure matters: Comparing methods to adapt multilingual  vocabularies to new languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Downey%2C+C+M">C.M. Downey</a>, 
<a href="/search/cs?searchtype=author&query=Blevins%2C+T">Terra Blevins</a>, 
<a href="/search/cs?searchtype=author&query=Goldfine%2C+N">Nora Goldfine</a>, 
<a href="/search/cs?searchtype=author&query=Steinert-Threlkeld%2C+S">Shane Steinert-Threlkeld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready for Proceedings of the 3rd Workshop on Multilingual Representation Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04810" title="Abstract">arXiv:2309.04810</a> (replaced) [<a href="/pdf/2309.04810" title="Download PDF">pdf</a>, <a href="/format/2309.04810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Latent Geometry Search: Product Manifold Inference via  Gromov-Hausdorff-Informed Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Ocariz+Borde%2C+H+S">Haitz Saez de Ocariz Borde</a>, 
<a href="/search/cs?searchtype=author&query=Arroyo%2C+A">Alvaro Arroyo</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+I">Ismael Morales</a>, 
<a href="/search/cs?searchtype=author&query=Posner%2C+I">Ingmar Posner</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaowen Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04917" title="Abstract">arXiv:2309.04917</a> (replaced) [<a href="/pdf/2309.04917" title="Download PDF">pdf</a>, <a href="/format/2309.04917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-driven Editing of 3D Scenes without Retraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Shuangkang Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yi-Hsuan Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenrui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuchang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://sk-fun.fun/DN2N">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04941" title="Abstract">arXiv:2309.04941</a> (replaced) [<a href="/pdf/2309.04941" title="Download PDF">pdf</a>, <a href="/ps/2309.04941" title="Download PostScript">ps</a>, <a href="/format/2309.04941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle  Counting Power
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Junru Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiarui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05133" title="Abstract">arXiv:2309.05133</a> (replaced) [<a href="/pdf/2309.05133" title="Download PDF">pdf</a>, <a href="/format/2309.05133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel RAM from Cyclic Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heath%2C+D">David Heath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06364" title="Abstract">arXiv:2309.06364</a> (replaced) [<a href="/pdf/2309.06364" title="Download PDF">pdf</a>, <a href="/format/2309.06364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Framework-Based Qualitative Analysis of Free Responses of Large Language  Models: Algorithmic Fidelity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirova%2C+A">Aliya Amirova</a>, 
<a href="/search/cs?searchtype=author&query=Fteropoulli%2C+T">Theodora Fteropoulli</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N">Nafiso Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Cowie%2C+M+R">Martin R. Cowie</a>, 
<a href="/search/cs?searchtype=author&query=Leibo%2C+J+Z">Joel Z. Leibo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 5 tables, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06520" title="Abstract">arXiv:2309.06520</a> (replaced) [<a href="/pdf/2309.06520" title="Download PDF">pdf</a>, <a href="/format/2309.06520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimum Bayes&#x27; Risk Decoding for System Combination of Grammatical Error  Correction Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raina%2C+V">Vyas Raina</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M">Mark Gales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07867" title="Abstract">arXiv:2309.07867</a> (replaced) [<a href="/pdf/2309.07867" title="Download PDF">pdf</a>, <a href="/format/2309.07867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beta Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhendong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Huangjie Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation (stat.CO); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10900" title="Abstract">arXiv:2309.10900</a> (replaced) [<a href="/pdf/2309.10900" title="Download PDF">pdf</a>, <a href="/format/2309.10900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incremental Multimodal Surface Mapping via Self-Organizing Gaussian  Mixture Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goel%2C+K">Kshitij Goel</a>, 
<a href="/search/cs?searchtype=author&query=Tabib%2C+W">Wennie Tabib</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, published in IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11139" title="Abstract">arXiv:2309.11139</a> (replaced) [<a href="/pdf/2309.11139" title="Download PDF">pdf</a>, <a href="/format/2309.11139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More complex encoder is not all you need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+W">Weibin Yang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+L">Longwei Xu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+P">Pengwei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Geng%2C+D">Dehua Geng</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yusong Li</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+M">Mingyuan Xu</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+Z">Zhiqi Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12095" title="Abstract">arXiv:2309.12095</a> (replaced) [<a href="/pdf/2309.12095" title="Download PDF">pdf</a>, <a href="/format/2309.12095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian sparsification for deep neural networks with Bayesian model  reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Markovi%C4%87%2C+D">Dimitrije Markovi&#x107;</a>, 
<a href="/search/stat?searchtype=author&query=Friston%2C+K+J">Karl J. Friston</a>, 
<a href="/search/stat?searchtype=author&query=Kiebel%2C+S+J">Stefan J. Kiebel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13135" title="Abstract">arXiv:2309.13135</a> (replaced) [<a href="/pdf/2309.13135" title="Download PDF">pdf</a>, <a href="/format/2309.13135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Response to Treatment with Global Deep Learning and  Patient-Specific Pharmacokinetic Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Potosnak%2C+W">Willa Potosnak</a>, 
<a href="/search/cs?searchtype=author&query=Challu%2C+C">Cristian Challu</a>, 
<a href="/search/cs?searchtype=author&query=Olivares%2C+K+G">Kin G. Olivares</a>, 
<a href="/search/cs?searchtype=author&query=Dubrawski%2C+A">Artur Dubrawski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14208" title="Abstract">arXiv:2309.14208</a> (replaced) [<a href="/pdf/2309.14208" title="Download PDF">pdf</a>, <a href="/format/2309.14208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Framework based on complex networks to model and mine patient pathways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Oliveira+Costa+Souza+Rosa%2C+C">Caroline de Oliveira Costa Souza Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Ito%2C+M">M&#xe1;rcia Ito</a>, 
<a href="/search/cs?searchtype=author&query=Vieira%2C+A+B">Alex Borges Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Wehmuth%2C+K">Klaus Wehmuth</a>, 
<a href="/search/cs?searchtype=author&query=Gomes%2C+A+T+A">Ant&#xf4;nio Tadeu Azevedo Gomes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 11 figures, 2 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14500" title="Abstract">arXiv:2309.14500</a> (replaced) [<a href="/pdf/2309.14500" title="Download PDF">pdf</a>, <a href="/format/2309.14500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of a new GeoAI foundation model for flood inundation mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyunho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chia-Yu Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Arundel%2C+S+T">Samantha T. Arundel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, Accepted for the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16318" title="Abstract">arXiv:2309.16318</a> (replaced) [<a href="/pdf/2309.16318" title="Download PDF">pdf</a>, <a href="/format/2309.16318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepPCR: Parallelizing Sequential Operations in Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Danieli%2C+F">Federico Danieli</a>, 
<a href="/search/cs?searchtype=author&query=Sarabia%2C+M">Miguel Sarabia</a>, 
<a href="/search/cs?searchtype=author&query=Suau%2C+X">Xavier Suau</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+P">Pau Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Zappella%2C+L">Luca Zappella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00656" title="Abstract">arXiv:2310.00656</a> (replaced) [<a href="/pdf/2310.00656" title="Download PDF">pdf</a>, <a href="/format/2310.00656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEGO-Prover: Neural Theorem Proving with Growing Libraries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+H">Huajian Xin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+H">Heng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01573" title="Abstract">arXiv:2310.01573</a> (replaced) [<a href="/pdf/2310.01573" title="Download PDF">pdf</a>, <a href="/format/2310.01573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Reality Environment and High-Dimensional Continuification Control  for Swarm Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maffettone%2C+G+C">Gian Carlo Maffettone</a>, 
<a href="/search/cs?searchtype=author&query=Liguori%2C+L">Lorenzo Liguori</a>, 
<a href="/search/cs?searchtype=author&query=Palermo%2C+E">Eduardo Palermo</a>, 
<a href="/search/cs?searchtype=author&query=di+Bernardo%2C+M">Mario di Bernardo</a>, 
<a href="/search/cs?searchtype=author&query=Porfiri%2C+M">Maurizio Porfiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01972" title="Abstract">arXiv:2310.01972</a> (replaced) [<a href="/pdf/2310.01972" title="Download PDF">pdf</a>, <a href="/format/2310.01972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epidemic Learning: Boosting Decentralized Learning with Randomized  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Vos%2C+M">Martijn de Vos</a>, 
<a href="/search/cs?searchtype=author&query=Farhadkhani%2C+S">Sadegh Farhadkhani</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Kermarrec%2C+A">Anne-Marie Kermarrec</a>, 
<a href="/search/cs?searchtype=author&query=Pires%2C+R">Rafael Pires</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rishi Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02810" title="Abstract">arXiv:2310.02810</a> (replaced) [<a href="/pdf/2310.02810" title="Download PDF">pdf</a>, <a href="/ps/2310.02810" title="Download PostScript">ps</a>, <a href="/format/2310.02810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circular external difference families, graceful labellings and cyclotomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Paterson%2C+M+B">Maura B. Paterson</a>, 
<a href="/search/math?searchtype=author&query=Stinson%2C+D+R">Douglas R. Stinson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04691" title="Abstract">arXiv:2310.04691</a> (replaced) [<a href="/pdf/2310.04691" title="Download PDF">pdf</a>, <a href="/format/2310.04691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMO: Earth Mover Distance Optimization for Auto-Regressive Language  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update experimental results of instruction-tuning and Github link
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05351" title="Abstract">arXiv:2310.05351</a> (replaced) [<a href="/pdf/2310.05351" title="Download PDF">pdf</a>, <a href="/format/2310.05351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Neural Collapse for a Large Number of Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiachen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>, 
<a href="/search/cs?searchtype=author&query=Mixon%2C+D">Dustin Mixon</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chong You</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihui Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05865" title="Abstract">arXiv:2310.05865</a> (replaced) [<a href="/pdf/2310.05865" title="Download PDF">pdf</a>, <a href="/format/2310.05865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Learning-Based Framework for Safe Human-Robot Collaboration with  Multiple Backup Control Barrier Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janwani%2C+N+C">Neil C. Janwani</a>, 
<a href="/search/cs?searchtype=author&query=Da%C5%9F%2C+E">Ersin Da&#x15f;</a>, 
<a href="/search/cs?searchtype=author&query=Touma%2C+T">Thomas Touma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S+X">Skylar X. Wei</a>, 
<a href="/search/cs?searchtype=author&query=Molnar%2C+T+G">Tamas G. Molnar</a>, 
<a href="/search/cs?searchtype=author&query=Burdick%2C+J+W">Joel W. Burdick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06822" title="Abstract">arXiv:2310.06822</a> (replaced) [<a href="/pdf/2310.06822" title="Download PDF">pdf</a>, <a href="/format/2310.06822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Bounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+M">Michael Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+P+D">Paul D. Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Ritschel%2C+T">Tobias Ritschel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07747" title="Abstract">arXiv:2310.07747</a> (replaced) [<a href="/pdf/2310.07747" title="Download PDF">pdf</a>, <a href="/format/2310.07747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accountability in Offline Reinforcement Learning: Explaining Decisions  with a Corpus of Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCy%C3%BCk%2C+A">Alihan H&#xfc;y&#xfc;k</a>, 
<a href="/search/cs?searchtype=author&query=Jarrett%2C+D">Daniel Jarrett</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08252" title="Abstract">arXiv:2310.08252</a> (replaced) [<a href="/pdf/2310.08252" title="Download PDF">pdf</a>, <a href="/format/2310.08252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zeyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongshu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiacheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+G">Guojun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yue-Jiao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yining Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NuerIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08282" title="Abstract">arXiv:2310.08282</a> (replaced) [<a href="/pdf/2310.08282" title="Download PDF">pdf</a>, <a href="/format/2310.08282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data driven modeling of self-similar dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Ru-yi Tao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+N">Ning-ning Tao</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yi-zhuang You</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages,5 figures,1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistical Mechanics (cond-mat.stat-mech)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08670" title="Abstract">arXiv:2310.08670</a> (replaced) [<a href="/pdf/2310.08670" title="Download PDF">pdf</a>, <a href="/format/2310.08670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Every Parameter Matters: Ensuring the Convergence of Federated Learning  with Dynamic Heterogeneous Models Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hanhan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Venkataramani%2C+G">Guru Venkataramani</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenbo Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09137" title="Abstract">arXiv:2310.09137</a> (replaced) [<a href="/pdf/2310.09137" title="Download PDF">pdf</a>, <a href="/format/2310.09137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Performance of Serverless Edge Networking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michalke%2C+M">Marc Michalke</a>, 
<a href="/search/cs?searchtype=author&query=Carpio%2C+F">Francisco Carpio</a>, 
<a href="/search/cs?searchtype=author&query=Jukan%2C+A">Admela Jukan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09520" title="Abstract">arXiv:2310.09520</a> (replaced) [<a href="/pdf/2310.09520" title="Download PDF">pdf</a>, <a href="/format/2310.09520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward-Augmented Decoding: Efficient Controlled Text Generation With a  Unidirectional Reward Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haikang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09727" title="Abstract">arXiv:2310.09727</a> (replaced) [<a href="/pdf/2310.09727" title="Download PDF">pdf</a>, <a href="/format/2310.09727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Fast Convergence of Independent Natural Policy Gradient for  Markov Potential Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Youbang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruida Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P+R">P. R. Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Shahrampour%2C+S">Shahin Shahrampour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Will appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10082" title="Abstract">arXiv:2310.10082</a> (replaced) [<a href="/pdf/2310.10082" title="Download PDF">pdf</a>, <a href="/format/2310.10082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simple uniformly optimal method without line search for convex  optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+T">Tianjiao Li</a>, 
<a href="/search/math?searchtype=author&query=Lan%2C+G">Guanghui Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11069" title="Abstract">arXiv:2310.11069</a> (replaced) [<a href="/pdf/2310.11069" title="Download PDF">pdf</a>, <a href="/format/2310.11069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waheed%2C+A">Abdul Waheed</a>, 
<a href="/search/cs?searchtype=author&query=Talafha%2C+B">Bashar Talafha</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+P">Peter Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Elmadany%2C+A">AbdelRahim Elmadany</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ArabicNLP conference co-located with EMNLP'23. First three authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11672" title="Abstract">arXiv:2310.11672</a> (replaced) [<a href="/pdf/2310.11672" title="Download PDF">pdf</a>, <a href="/format/2310.11672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-ended Commonsense Reasoning with Unrestricted Answer Scope
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xujiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Oishi%2C+M">Mika Oishi</a>, 
<a href="/search/cs?searchtype=author&query=Osaki%2C+T">Takao Osaki</a>, 
<a href="/search/cs?searchtype=author&query=Matsuda%2C+K">Katsushi Matsuda</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12298" title="Abstract">arXiv:2310.12298</a> (replaced) [<a href="/pdf/2310.12298" title="Download PDF">pdf</a>, <a href="/format/2310.12298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jorge: Approximate Preconditioning for GPU-efficient Second-order  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Siddharth Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sating%2C+Z">Zachary Sating</a>, 
<a href="/search/cs?searchtype=author&query=Bhatele%2C+A">Abhinav Bhatele</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12305" title="Abstract">arXiv:2310.12305</a> (replaced) [<a href="/pdf/2310.12305" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Random, Fair, and Verifiable Games on Blockchain. Raffle smart  contract designs on Sui Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Justa Liang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ray Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pierce Hung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Damien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+A">Ashley Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chalkias%2C+K">Konstantinos Chalkias</a>, 
<a href="/search/cs?searchtype=author&query=Pleros%2C+S">Stefanos Pleros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12692" title="Abstract">arXiv:2310.12692</a> (replaced) [<a href="/pdf/2310.12692" title="Download PDF">pdf</a>, <a href="/format/2310.12692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Learning via Consistent Assignment of Views over Random  Partitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+T">Thalles Silva</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+A+R">Ad&#xed;n Ram&#xed;rez Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023. Code available at <a href="https://github.com/sthalles/carp">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13167" title="Abstract">arXiv:2310.13167</a> (replaced) [<a href="/pdf/2310.13167" title="Download PDF">pdf</a>, <a href="/format/2310.13167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visualizing Causality in Mixed Reality for Manual Task Learning: An  Exploratory Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rahul Jain</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jingyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Benton%2C+A">Andrew Benton</a>, 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+M">Moiz Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Doh%2C+H">Hyungjun Doh</a>, 
<a href="/search/cs?searchtype=author&query=Chidambaram%2C+S">Subramanian Chidambaram</a>, 
<a href="/search/cs?searchtype=author&query=Ramani%2C+K">Karthik Ramani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13513" title="Abstract">arXiv:2310.13513</a> (replaced) [<a href="/pdf/2310.13513" title="Download PDF">pdf</a>, <a href="/format/2310.13513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Potential of Flexible 8-bit Format: Design and Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuoyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Gonglei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruihao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+X">Xiaoxu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lewei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13548" title="Abstract">arXiv:2310.13548</a> (replaced) [<a href="/pdf/2310.13548" title="Download PDF">pdf</a>, <a href="/format/2310.13548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Sycophancy in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mrinank Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+M">Meg Tong</a>, 
<a href="/search/cs?searchtype=author&query=Korbak%2C+T">Tomasz Korbak</a>, 
<a href="/search/cs?searchtype=author&query=Duvenaud%2C+D">David Duvenaud</a>, 
<a href="/search/cs?searchtype=author&query=Askell%2C+A">Amanda Askell</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Newton Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Durmus%2C+E">Esin Durmus</a>, 
<a href="/search/cs?searchtype=author&query=Hatfield-Dodds%2C+Z">Zac Hatfield-Dodds</a>, 
<a href="/search/cs?searchtype=author&query=Johnston%2C+S+R">Scott R. Johnston</a>, 
<a href="/search/cs?searchtype=author&query=Kravec%2C+S">Shauna Kravec</a>, 
<a href="/search/cs?searchtype=author&query=Maxwell%2C+T">Timothy Maxwell</a>, 
<a href="/search/cs?searchtype=author&query=McCandlish%2C+S">Sam McCandlish</a>, 
<a href="/search/cs?searchtype=author&query=Ndousse%2C+K">Kamal Ndousse</a>, 
<a href="/search/cs?searchtype=author&query=Rausch%2C+O">Oliver Rausch</a>, 
<a href="/search/cs?searchtype=author&query=Schiefer%2C+N">Nicholas Schiefer</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Da Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miranda Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13725" title="Abstract">arXiv:2310.13725</a> (replaced) [<a href="/pdf/2310.13725" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing drug and cell line representations via contrastive learning  for improved anti-cancer drug prioritization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lawrence%2C+P+J">Patrick J. Lawrence</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xia Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, 4 figures, 4 tables, 11 supplementary tables, 1 supplementary note, submitted to Nature Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13786" title="Abstract">arXiv:2310.13786</a> (replaced) [<a href="/pdf/2310.13786" title="Download PDF">pdf</a>, <a href="/ps/2310.13786" title="Download PostScript">ps</a>, <a href="/format/2310.13786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Membership Inference Attacks on Machine Learning  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Aubinais%2C+E">Eric Aubinais</a>, 
<a href="/search/stat?searchtype=author&query=Gassiat%2C+E">Elisabeth Gassiat</a>, 
<a href="/search/stat?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14088" title="Abstract">arXiv:2310.14088</a> (replaced) [<a href="/pdf/2310.14088" title="Download PDF">pdf</a>, <a href="/format/2310.14088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark  for Language Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+A">An Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+E+Y">Eric Y. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Gentili%2C+A">Amilcare Gentili</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chun-Nan Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023. Camera-ready version: added more evaluation results on LLMs such as GPT4, LLaMa2, and LLaMa2-chat
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14586" title="Abstract">arXiv:2310.14586</a> (replaced) [<a href="/pdf/2310.14586" title="Download PDF">pdf</a>, <a href="/format/2310.14586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Molaei%2C+S">Soheila Molaei</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14735" title="Abstract">arXiv:2310.14735</a> (replaced) [<a href="/pdf/2310.14735" title="Download PDF">pdf</a>, <a href="/format/2310.14735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the potential of prompt engineering in Large Language Models:  a comprehensive review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Banghao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Langren%C3%A9%2C+N">Nicolas Langren&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shengxin Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15169" title="Abstract">arXiv:2310.15169</a> (replaced) [<a href="/pdf/2310.15169" title="Download PDF">pdf</a>, <a href="/format/2310.15169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Haonan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menghan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yingqing He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="http://haonanqiu.com/projects/FreeNoise.html">this http URL</a> Code Repo: <a href="https://github.com/arthur-qiu/LongerCrafter">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15211" title="Abstract">arXiv:2310.15211</a> (replaced) [<a href="/pdf/2310.15211" title="Download PDF">pdf</a>, <a href="/format/2310.15211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Path Importance for Effective Alzheimer&#x27;s Disease Drug  Repurposing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xiang%2C+S">Shunian Xiang</a>, 
<a href="/search/q-bio?searchtype=author&query=Lawrence%2C+P+J">Patrick J. Lawrence</a>, 
<a href="/search/q-bio?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/q-bio?searchtype=author&query=Chiang%2C+C">ChienWei Chiang</a>, 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+D">Dokyoon Kim</a>, 
<a href="/search/q-bio?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/q-bio?searchtype=author&query=Ning%2C+X">Xia Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 2 tables, 1 supplementary figure, 5 supplementary tables, Preprint of an article accepted for publication in Pacific Symposium on Biocomputing \copyright 2023 World Scientific Publishing Co., Singapore, <a href="http://psb.stanford.edu/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Molecular Networks (q-bio.MN)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15390" title="Abstract">arXiv:2310.15390</a> (replaced) [<a href="/pdf/2310.15390" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEMPSEP III. A machine learning-oriented multivariate data set for  forecasting the Occurrence and Properties of Solar Energetic Particle Events  using a Multivariate Ensemble Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Moreland%2C+K">Kimberly Moreland</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dayeh%2C+M">Maher Dayeh</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bain%2C+H+M">Hazel M. Bain</a>, 
<a href="/search/astro-ph?searchtype=author&query=Chatterjee%2C+S">Subhamoy Chatterjee</a>, 
<a href="/search/astro-ph?searchtype=author&query=Munoz-Jaramillo%2C+A">Andres Munoz-Jaramillo</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hart%2C+S">Samuel Hart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Machine Learning (cs.LG); Space Physics (physics.space-ph)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15403" title="Abstract">arXiv:2310.15403</a> (replaced) [<a href="/pdf/2310.15403" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study between Silicon Carbide and Silicon Nitride based  Single Cell CMUT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kanjilal%2C+R">Rakesh Kanjilal</a>, 
<a href="/search/eess?searchtype=author&query=Maity%2C+R">Reshmi Maity</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> copyright (2023) Inventive Research Organization. This is an open access article under the Creative Commons Attribution-NonCommercial International (CC BY-NC 4.0) License. This is a "Preprint" version of the main publication, published copy of the article can be accessed from the publisher's directory (DOI: 10.36548/jei.2023.3.006)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Electronics and Informatics, September 2023, Volume 5,
  Issue 3, Pages 320-334
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15970" title="Abstract">arXiv:2310.15970</a> (replaced) [<a href="/pdf/2310.15970" title="Download PDF">pdf</a>, <a href="/format/2310.15970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accented Speech Recognition With Accent-specific Codebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+D">Darshan Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+P">Preethi Jyothi</a>, 
<a href="/search/cs?searchtype=author&query=Ganapathy%2C+S">Sriram Ganapathy</a>, 
<a href="/search/cs?searchtype=author&query=Unni%2C+V">Vinit Unni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16069" title="Abstract">arXiv:2310.16069</a> (replaced) [<a href="/pdf/2310.16069" title="Download PDF">pdf</a>, <a href="/format/2310.16069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought  Language Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16145" title="Abstract">arXiv:2310.16145</a> (replaced) [<a href="/pdf/2310.16145" title="Download PDF">pdf</a>, <a href="/ps/2310.16145" title="Download PostScript">ps</a>, <a href="/format/2310.16145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Positive Almost-Sure Termination -- Complexity and Proof Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+R">Rupak Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Sathiyanarayana%2C+V+R">V.R. Sathiyanarayana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16314" title="Abstract">arXiv:2310.16314</a> (replaced) [<a href="/pdf/2310.16314" title="Download PDF">pdf</a>, <a href="/format/2310.16314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Code Semantics: An Evaluation of Transformer Models in  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+D">Debanjan Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Lodha%2C+A">Abhilasha Lodha</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+A">Ankita Sahoo</a>, 
<a href="/search/cs?searchtype=author&query=Kumari%2C+B">Beena Kumari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at GenBench, EMNLP 2023. All authors are co-first authors and have equal contributions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16506" title="Abstract">arXiv:2310.16506</a> (replaced) [<a href="/pdf/2310.16506" title="Download PDF">pdf</a>, <a href="/ps/2310.16506" title="Download PostScript">ps</a>, <a href="/format/2310.16506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Reasons for Bias: An Argumentation-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waller%2C+M">Madeleine Waller</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+O">Odinaldo Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Cocarascu%2C+O">Oana Cocarascu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16546" title="Abstract">arXiv:2310.16546</a> (replaced) [<a href="/pdf/2310.16546" title="Download PDF">pdf</a>, <a href="/format/2310.16546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pitfall of Optimism: Distributional Reinforcement Learning by  Randomizing Risk Criterion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cho%2C+T">Taehyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Seungyub Han</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Heesoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyungjae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jungwoo Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16655" title="Abstract">arXiv:2310.16655</a> (replaced) [<a href="/pdf/2310.16655" title="Download PDF">pdf</a>, <a href="/format/2310.16655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Control-Centric Representations in Reinforcement Learning from  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+H">Hongyu Zang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+Y">Yong Heng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingzhong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16737" title="Abstract">arXiv:2310.16737</a> (replaced) [<a href="/pdf/2310.16737" title="Download PDF">pdf</a>, <a href="/format/2310.16737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Translating Universal Scene Descriptions into Knowledge Graphs for  Robotic Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+G+H">Giang Hoang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Bessler%2C+D">Daniel Bessler</a>, 
<a href="/search/cs?searchtype=author&query=Stelter%2C+S">Simon Stelter</a>, 
<a href="/search/cs?searchtype=author&query=Pomarlan%2C+M">Mihai Pomarlan</a>, 
<a href="/search/cs?searchtype=author&query=Beetz%2C+M">Michael Beetz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16779" title="Abstract">arXiv:2310.16779</a> (replaced) [<a href="/pdf/2310.16779" title="Download PDF">pdf</a>, <a href="/format/2310.16779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale Diffusion Denoised Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jongheon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at NeurIPS 2023; Code is available at <a href="https://github.com/jh-jeong/smoothing-multiscale">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16832" title="Abstract">arXiv:2310.16832</a> (replaced) [<a href="/pdf/2310.16832" title="Download PDF">pdf</a>, <a href="/format/2310.16832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightSpeed: Light and Fast Neural Light Fields on Mobile Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aarush Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Junli Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Ju Hu</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Jeni%2C+L+A">L&#xe1;szl&#xf3; A Jeni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="http://lightspeed-r2l.github.io/">this http URL</a> . Add camera ready version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16898" title="Abstract">arXiv:2310.16898</a> (replaced) [<a href="/pdf/2310.16898" title="Download PDF">pdf</a>, <a href="/format/2310.16898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited  Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yinan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiuwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yansong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiwen Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17004" title="Abstract">arXiv:2310.17004</a> (replaced) [<a href="/pdf/2310.17004" title="Download PDF">pdf</a>, <a href="/format/2310.17004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level  Compensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hanschke%2C+J">Jan-Hendrik Hanschke</a>, 
<a href="/search/eess?searchtype=author&query=Arteaga%2C+D">Daniel Arteaga</a>, 
<a href="/search/eess?searchtype=author&query=Cengarle%2C+G">Giulio Cengarle</a>, 
<a href="/search/eess?searchtype=author&query=Lando%2C+J">Joshua Lando</a>, 
<a href="/search/eess?searchtype=author&query=Thomas%2C+M+R+P">Mark R. P. Thomas</a>, 
<a href="/search/eess?searchtype=author&query=Seefeldt%2C+A">Alan Seefeldt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages. Accepted for presentation in AES Convention 155 (2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Audio Engineering Society Convention 155, New
  York, paper 10669 (October 2023).
  https://www.aes.org/e-lib/inst/browse.cfm?elib=22250
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17015" title="Abstract">arXiv:2310.17015</a> (replaced) [<a href="/pdf/2310.17015" title="Download PDF">pdf</a>, <a href="/format/2310.17015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation for Emotion Detection in Small Imbalanced Text Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koufakou%2C+A">Anna Koufakou</a>, 
<a href="/search/cs?searchtype=author&query=Grisales%2C+D">Diego Grisales</a>, 
<a href="/search/cs?searchtype=author&query=de+jesus%2C+R+C">Ragy Costa de jesus</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+O">Oscar Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Proceedings of IEEE International Conference on Machine Learning Applications IEEE (ICMLA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17069" title="Abstract">arXiv:2310.17069</a> (replaced) [<a href="/pdf/2310.17069" title="Download PDF">pdf</a>, <a href="/format/2310.17069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electric Vehicle Aggregation Review: Benefits and Vulnerabilities of  Managing a Growing EV Fleet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nelson%2C+K">Kelsey Nelson</a>, 
<a href="/search/eess?searchtype=author&query=Mohammadi%2C+J">Javad Mohammadi</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Blasch%2C+E">Erik Blasch</a>, 
<a href="/search/eess?searchtype=author&query=Aved%2C+A">Alex Aved</a>, 
<a href="/search/eess?searchtype=author&query=Ferris%2C+D">David Ferris</a>, 
<a href="/search/eess?searchtype=author&query=Cruz%2C+E+A">Erika Ardiles Cruz</a>, 
<a href="/search/eess?searchtype=author&query=Morrone%2C+P">Philip Morrone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17075" title="Abstract">arXiv:2310.17075</a> (replaced) [<a href="/pdf/2310.17075" title="Download PDF">pdf</a>, <a href="/format/2310.17075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperFields: Towards Zero-Shot Generation of NeRFs from Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babu%2C+S">Sudarshan Babu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Richard Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Avery Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Maire%2C+M">Michael Maire</a>, 
<a href="/search/cs?searchtype=author&query=Shakhnarovich%2C+G">Greg Shakhnarovich</a>, 
<a href="/search/cs?searchtype=author&query=Hanocka%2C+R">Rana Hanocka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://threedle.github.io/hyperfields/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17097" title="Abstract">arXiv:2310.17097</a> (replaced) [<a href="/pdf/2310.17097" title="Download PDF">pdf</a>, <a href="/format/2310.17097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Data Heterogeneity in Federated Learning A Semi-Supervised  Approach for Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+E">Eric Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+C">Christian Lau</a>, 
<a href="/search/cs?searchtype=author&query=Mugunthan%2C+V">Vaikkunth Mugunthan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17136" title="Abstract">arXiv:2310.17136</a> (replaced) [<a href="/pdf/2310.17136" title="Download PDF">pdf</a>, <a href="/format/2310.17136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Core Challenge 2023: Solver and Graph Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soh%2C+T">Takehide Soh</a>, 
<a href="/search/cs?searchtype=author&query=Tanjo%2C+T">Tomoya Tanjo</a>, 
<a href="/search/cs?searchtype=author&query=Okamoto%2C+Y">Yoshio Okamoto</a>, 
<a href="/search/cs?searchtype=author&query=Ito%2C+T">Takehiro Ito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2208.02495">arXiv:2208.02495</a>, <a href="/abs/2207.13959">arXiv:2207.13959</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17255" title="Abstract">arXiv:2310.17255</a> (replaced) [<a href="/pdf/2310.17255" title="Download PDF">pdf</a>, <a href="/format/2310.17255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing to Unseen Domains in Diabetic Retinopathy Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galappaththige%2C+C+J">Chamuditha Jayanga Galappaththige</a>, 
<a href="/search/cs?searchtype=author&query=Kuruppu%2C+G">Gayal Kuruppu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+H">Muhammad Haris Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17273" title="Abstract">arXiv:2310.17273</a> (replaced) [<a href="/pdf/2310.17273" title="Download PDF">pdf</a>, <a href="/format/2310.17273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Looping in the Human: Collaborative and Explainable Bayesian  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adachi%2C+M">Masaki Adachi</a>, 
<a href="/search/cs?searchtype=author&query=Planden%2C+B">Brady Planden</a>, 
<a href="/search/cs?searchtype=author&query=Howey%2C+D+A">David A. Howey</a>, 
<a href="/search/cs?searchtype=author&query=Maundet%2C+K">Krikamol Maundet</a>, 
<a href="/search/cs?searchtype=author&query=Osborne%2C+M+A">Michael A. Osborne</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+S+L">Siu Lun Chau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17341" title="Abstract">arXiv:2310.17341</a> (replaced) [<a href="/pdf/2310.17341" title="Download PDF">pdf</a>, <a href="/format/2310.17341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De-novo Chemical Reaction Generation by Means of Temporarily  Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buin%2C+A">Andrei Buin</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+H+Y">Hung Yi Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Gadsden%2C+S+A">S. Andrew Gadsden</a>, 
<a href="/search/cs?searchtype=author&query=Alderson%2C+F+A">Faraz A. Alderson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17408" title="Abstract">arXiv:2310.17408</a> (replaced) [<a href="/pdf/2310.17408" title="Download PDF">pdf</a>, <a href="/format/2310.17408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling the Matrix Multiplication Micro-kernel Generation with Exo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castell%C3%B3%2C+A">Adri&#xe1;n Castell&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Bellavita%2C+J">Julian Bellavita</a>, 
<a href="/search/cs?searchtype=author&query=Dinh%2C+G">Grace Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Ikarashi%2C+Y">Yuka Ikarashi</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+H">H&#xe9;ctor Mart&#xed;nez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 18 figures. Presented at CGO 2024. It includes a software artifact step-by-step execution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Computation and Language (cs.CL); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17513" title="Abstract">arXiv:2310.17513</a> (replaced) [<a href="/pdf/2310.17513" title="Download PDF">pdf</a>, <a href="/format/2310.17513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expressive Power of Low-Rank Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yuchen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17519" title="Abstract">arXiv:2310.17519</a> (replaced) [<a href="/pdf/2310.17519" title="Download PDF">pdf</a>, <a href="/format/2310.17519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLARE: Fast Learning of Animatable and Relightable Mesh Avatars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bharadwaj%2C+S">Shrisha Bharadwaj</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yufeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hilliges%2C+O">Otmar Hilliges</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+M+J">Michael J. Black</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez-Abrevaya%2C+V">Victoria Fernandez-Abrevaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, Accepted: ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Volume 42, article number 204, year 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17523" title="Abstract">arXiv:2310.17523</a> (replaced) [<a href="/pdf/2310.17523" title="Download PDF">pdf</a>, <a href="/ps/2310.17523" title="Download PostScript">ps</a>, <a href="/format/2310.17523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Resource Management for Edge Network Slicing using Incremental  Multi-Agent Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haiyuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yuelin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xueqing Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Vasilakos%2C+X">Xenofon Vasilakos</a>, 
<a href="/search/eess?searchtype=author&query=Nejabati%2C+R">Reza Nejabati</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+S">Shuangyi Yan</a>, 
<a href="/search/eess?searchtype=author&query=Simeonidou%2C+D">Dimitra Simeonidou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17526" title="Abstract">arXiv:2310.17526</a> (replaced) [<a href="/pdf/2310.17526" title="Download PDF">pdf</a>, <a href="/format/2310.17526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can large language models replace humans in the systematic review  process? Evaluating GPT-4&#x27;s efficacy in screening and extracting data from  peer-reviewed and grey literature in multiple languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khraisha%2C+Q">Qusai Khraisha</a>, 
<a href="/search/cs?searchtype=author&query=Put%2C+S">Sophie Put</a>, 
<a href="/search/cs?searchtype=author&query=Kappenberg%2C+J">Johanna Kappenberg</a>, 
<a href="/search/cs?searchtype=author&query=Warraitch%2C+A">Azza Warraitch</a>, 
<a href="/search/cs?searchtype=author&query=Hadfield%2C+K">Kristin Hadfield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17550" title="Abstract">arXiv:2310.17550</a> (replaced) [<a href="/pdf/2310.17550" title="Download PDF">pdf</a>, <a href="/format/2310.17550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Guided Complexity-Controlled Abstractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+A">Andi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tucker%2C+M">Mycal Tucker</a>, 
<a href="/search/cs?searchtype=author&query=Kenny%2C+E">Eoin Kenny</a>, 
<a href="/search/cs?searchtype=author&query=Zaslavsky%2C+N">Noga Zaslavsky</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J">Julie Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17577" title="Abstract">arXiv:2310.17577</a> (replaced) [<a href="/pdf/2310.17577" title="Download PDF">pdf</a>, <a href="/format/2310.17577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Structure-Aware Diffusion Process for Low-Light Image Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jinhui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Junhui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Huanqiang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hui Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17594" title="Abstract">arXiv:2310.17594</a> (replaced) [<a href="/pdf/2310.17594" title="Download PDF">pdf</a>, <a href="/format/2310.17594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPA: A Graph Spectral Alignment Perspective for Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhiqing Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haobo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Ying Jin</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junbo Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17651" title="Abstract">arXiv:2310.17651</a> (replaced) [<a href="/pdf/2310.17651" title="Download PDF">pdf</a>, <a href="/format/2310.17651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Dimensional Prediction for Sequential Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noarov%2C+G">Georgy Noarov</a>, 
<a href="/search/cs?searchtype=author&query=Ramalingam%2C+R">Ramya Ramalingam</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Stephan Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added references, Arxiv abstract edited
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item258">Cross-lists</a></li>
<li><a href="#item300">Replacements</a></li>
</ul>
<small>[ total of 546 entries:  <b>1-546</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
