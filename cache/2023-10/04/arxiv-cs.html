<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon  2 Oct 23  to  Tue  3 Oct 23, announced Wed,  4 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item404">Cross-lists</a></li>
<li><a href="#item456">Replacements</a></li>
</ul>
<small>[ total of 658 entries:  <b>1-658</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed,  4 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01418" title="Abstract">arXiv:2310.01418</a> [<a href="/pdf/2310.01418" title="Download PDF">pdf</a>, <a href="/format/2310.01418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ninalga%2C+D">Dean Ninalga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Depression is debilitating, and not uncommon. Indeed, studies of excessive
social media users show correlations with depression, ADHD, and other mental
health concerns. Given that there is a large number of people with excessive
social media usage, then there is a significant population of potentially
undiagnosed users and posts that they create. In this paper, we propose a
depression severity detection system using a semi-supervised learning technique
to predict if a post is from a user who is experiencing severe, moderate, or
low (non-diagnostic) levels of depression. Namely, we use a trained model to
classify a large number of unlabelled social media posts from Reddit, then use
these generated labels to train a more powerful classifier. We demonstrate our
framework on Detecting Signs of Depression from Social Media Text -
LT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01419" title="Abstract">arXiv:2310.01419</a> [<a href="/pdf/2310.01419" title="Download PDF">pdf</a>, <a href="/format/2310.01419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Principles of Robust Multi-Armed Bandit Framework in Video  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayar%2C+B">Belhassen Bayar</a>, 
<a href="/search/cs?searchtype=author&query=Gampa%2C+P">Phanideep Gampa</a>, 
<a href="/search/cs?searchtype=author&query=Yessenalina%2C+A">Ainur Yessenalina</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhen Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> RecSys CARS 2023 Workshop paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Current multi-armed bandit approaches in recommender systems (RS) have
focused more on devising effective exploration techniques, while not adequately
addressing common exploitation challenges related to distributional changes and
item cannibalization. Little work exists to guide the design of robust bandit
frameworks that can address these frequent challenges in RS. In this paper, we
propose a new design principles to (i) make bandit models robust to
time-variant metadata signals, (ii) less prone to item cannibalization, and
(iii) prevent their weights fluctuating due to data sparsity. Through a series
of experiments, we systematically examine the influence of several important
bandit design choices. We demonstrate the advantage of our proposed design
principles at making bandit models robust to dynamic behavioral changes through
in-depth analyses. Noticeably, we show improved relative gain compared to a
baseline bandit model not incorporating our design choices of up to $11.88\%$
and $44.85\%$, respectively in ROC-AUC and PR-AUC. Case studies about fairness
in recommending specific popular and unpopular titles are presented, to
demonstrate the robustness of our proposed design at addressing popularity
biases.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01420" title="Abstract">arXiv:2310.01420</a> [<a href="/pdf/2310.01420" title="Download PDF">pdf</a>, <a href="/format/2310.01420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmucker%2C+R">Robin Schmucker</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Meng Xia</a>, 
<a href="/search/cs?searchtype=author&query=Azaria%2C+A">Amos Azaria</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+T">Tom Mitchell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Conversational tutoring systems (CTSs) offer learning experiences driven by
natural language interaction. They are known to promote high levels of
cognitive engagement and benefit learning outcomes, particularly in reasoning
tasks. Nonetheless, the time and cost required to author CTS content is a major
obstacle to widespread adoption. In this paper, we introduce a novel type of
CTS that leverages the recent advances in large language models (LLMs) in two
ways: First, the system induces a tutoring script automatically from a lesson
text. Second, the system automates the script orchestration via two LLM-based
agents (Ruffle&amp;Riley) with the roles of a student and a professor in a
learning-by-teaching format. The system allows a free-form conversation that
follows the ITS-typical outer-/inner-loop structure. In an initial
between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler
QA chatbots and reading activity, we found no significant differences in
post-test scores. Nonetheless, in the learning experience survey, Ruffle&amp;Riley
users expressed higher ratings of understanding and remembering and further
perceived the offered support as more helpful and the conversation as coherent.
Our study provides insights for a new generation of scalable CTS technologies.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01421" title="Abstract">arXiv:2310.01421</a> [<a href="/pdf/2310.01421" title="Download PDF">pdf</a>, <a href="/ps/2310.01421" title="Download PostScript">ps</a>, <a href="/format/2310.01421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Focus Group Interviews to Examine Biased Experiences in  Human-Robot-Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erle%2C+L">Lukas Erle</a>, 
<a href="/search/cs?searchtype=author&query=Timm%2C+L">Lara Timm</a>, 
<a href="/search/cs?searchtype=author&query=Stra%C3%9Fmann%2C+C">Carolin Stra&#xdf;mann</a>, 
<a href="/search/cs?searchtype=author&query=Eimler%2C+S+C">Sabrina C. Eimler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">When deploying interactive agents like (social) robots in public spaces they
need to be able to interact with a diverse audience, with members each having
individual diversity characteristics and prior experiences with interactive
systems. To cater for these various predispositions, it is important to examine
what experiences citizens have made with interactive systems and how these
experiences might create a bias towards such systems. To analyze these
bias-inducing experiences, focus group interviews have been conducted to learn
of citizens individual discrimination experiences, their attitudes towards and
arguments for and against the deployment of social robots in public spaces.
This extended abstract focuses especially on the method and measurement of
diversity.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01422" title="Abstract">arXiv:2310.01422</a> [<a href="/pdf/2310.01422" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Service Pet Robot Design: Queer, Feminine and Sexuality Aspects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Velentza%2C+A">Anna-Maria Velentza</a>, 
<a href="/search/cs?searchtype=author&query=Tsagkaropoulou%2C+A">Antigoni Tsagkaropoulou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The integration of robots and AI in society raises concerns about
discrimination and biases mostly affecting underrepresented groups, including
queer and feminine figures. Socially assistive robots (SAR) are being used in a
variety of service and companion roles, following social norms during their
interaction with humans and seem to be beneficial in many roles, such as the
pet therapy robots. To promote inclusion and representation, robot design
should incorporate queer and feminine characteristics. As a response to these
concerns, a pet robot called BB was designed using a multidisciplinary and
inclusive approach. BB was presented in a queer architecture and aesthetics
environment, emphasizing aspects of techno-touch, vulnerability, and sexuality
in human-robot interactions. The audience's perception of both the robot and
the female researcher was evaluated through questionnaires and focus groups.
This study aims to explore how technology and design can better accommodate
diverse perspectives and needs in the field of SAR.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01423" title="Abstract">arXiv:2310.01423</a> [<a href="/pdf/2310.01423" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of AI Generated Text Detection Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akram%2C+A">Arslan Akram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 Pages, 4 Figures, 2 Tables, 42 References
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Since ChatGPT has emerged as a major AIGC model, providing high-quality
responses across a wide range of applications (including software development
and maintenance), it has attracted much interest from many individuals. ChatGPT
has great promise, but there are serious problems that might arise from its
misuse, especially in the realms of education and public safety. Several AIGC
detectors are available, and they have all been tested on genuine text.
However, more study is needed to see how effective they are for multi-domain
ChatGPT material. This study aims to fill this need by creating a multi-domain
dataset for testing the state-of-the-art APIs and tools for detecting
artificially generated information used by universities and other research
institutions. A large dataset consisting of articles, abstracts, stories, news,
and product reviews was created for this study. The second step is to use the
newly created dataset to put six tools through their paces. Six different
artificial intelligence (AI) text identification systems, including "GPTkit,"
"GPTZero," "Originality," "Sapling," "Writer," and "Zylalab," have accuracy
rates between 55.29 and 97.0%. Although all the tools fared well in the
evaluations, originality was particularly effective across the board.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01424" title="Abstract">arXiv:2310.01424</a> [<a href="/pdf/2310.01424" title="Download PDF">pdf</a>, <a href="/ps/2310.01424" title="Download PostScript">ps</a>, <a href="/format/2310.01424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and Mitigating Privacy Risks Stemming from Language Models:  A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+V">Victoria Smith</a>, 
<a href="/search/cs?searchtype=author&query=Shamsabadi%2C+A+S">Ali Shahin Shamsabadi</a>, 
<a href="/search/cs?searchtype=author&query=Ashurst%2C+C">Carolyn Ashurst</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Rapid advancements in language models (LMs) have led to their adoption across
many sectors. Alongside the potential benefits, such models present a range of
risks, including around privacy. In particular, as LMs have grown in size, the
potential to memorise aspects of their training data has increased, resulting
in the risk of leaking private information. As LMs become increasingly
widespread, it is vital that we understand such privacy risks and how they
might be mitigated. To help researchers and policymakers understand the state
of knowledge around privacy attacks and mitigations, including where more work
is needed, we present the first technical survey on LM privacy. We (i) identify
a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey
existing attacks and use our taxonomy of dimensions to highlight key trends,
(iii) discuss existing mitigation strategies, highlighting their strengths and
limitations, identifying key gaps and demonstrating open problems and areas for
concern.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01425" title="Abstract">arXiv:2310.01425</a> [<a href="/pdf/2310.01425" title="Download PDF">pdf</a>, <a href="/ps/2310.01425" title="Download PostScript">ps</a>, <a href="/format/2310.01425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Borges and AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bottou%2C+L">L&#xe9;on Bottou</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhardt Sch&#xf6;lkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Many believe that Large Language Models (LLMs) open the era of Artificial
Intelligence (AI). Some see opportunities while others see dangers. Yet both
proponents and opponents grasp AI through the imagery popularised by science
fiction. Will the machine become sentient and rebel against its creators? Will
we experience a paperclip apocalypse? Before answering such questions, we
should first ask whether this mental imagery provides a good description of the
phenomenon at hand. Understanding weather patterns through the moods of the
gods only goes so far. The present paper instead advocates understanding LLMs
and their connection to AI through the imagery of Jorge Luis Borges, a master
of 20th century literature, forerunner of magical realism, and precursor to
postmodern literature. This exercise leads to a new perspective that
illuminates the relation between language modelling and artificial
intelligence.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01427" title="Abstract">arXiv:2310.01427</a> [<a href="/pdf/2310.01427" title="Download PDF">pdf</a>, <a href="/format/2310.01427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Sorting Combats Recency Bias In Long Context Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peysakhovich%2C+A">Alexander Peysakhovich</a>, 
<a href="/search/cs?searchtype=author&query=Lerer%2C+A">Adam Lerer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current language models often fail to incorporate long contexts efficiently
during generation. We show that a major contributor to this issue are attention
priors that are likely learned during pre-training: relevant information
located earlier in context is attended to less on average. Yet even when models
fail to use the information from a relevant document in their response, they
still pay preferential attention to that document compared to an irrelevant
document at the same position. We leverage this fact to introduce ``attention
sorting'': perform one step of decoding, sort documents by the attention they
receive (highest attention going last), repeat the process, generate the answer
with the newly sorted context. We find that attention sorting improves
performance of long context models. Our findings highlight some challenges in
using off-the-shelf language models for retrieval augmented generation.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01429" title="Abstract">arXiv:2310.01429</a> [<a href="/pdf/2310.01429" title="Download PDF">pdf</a>, <a href="/format/2310.01429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chatmap : Large Language Model Interaction with Cartographic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Unlu%2C+E">Eren Unlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The swift advancement and widespread availability of foundational Large
Language Models (LLMs), complemented by robust fine-tuning methodologies, have
catalyzed their adaptation for innovative and industrious applications.
Enabling LLMs to recognize and interpret geospatial data, while offering a
linguistic access to vast cartographic datasets, is of significant importance.
OpenStreetMap (OSM) is the most ambitious open-source global initiative
offering detailed urban and rural geographic data, curated by a community of
over 10 million contributors, which constitutes a great potential for LLM
applications. In this study, we demonstrate the proof of concept and details of
the process of fine-tuning a relatively small scale (1B parameters) LLM with a
relatively small artificial dataset curated by a more capable teacher model, in
order to provide a linguistic interface to the OSM data of an arbitrary urban
region. Through this interface, users can inquire about a location's
attributes, covering a wide spectrum of concepts, such as its touristic appeal
or the potential profitability of various businesses in that vicinity. The
study aims to provide an initial guideline for such generative artificial
intelligence (AI) adaptations and demonstrate early signs of useful emerging
abilities in this context even in minimal computational settings. The
embeddings of artificially curated prompts including OSM data are also
investigated in detail, which might be instrumental for potential geospatially
aware urban Retrieval Augmented Generation (RAG) applications.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01430" title="Abstract">arXiv:2310.01430</a> [<a href="/pdf/2310.01430" title="Download PDF">pdf</a>, <a href="/format/2310.01430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve  Multimodal Sarcasm Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhosale%2C+S">Swapnil Bhosale</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+A">Abhra Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+A+L+R">Alex Lee Robert Williams</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+D">Divyank Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Anjan Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiatian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+P">Pushpak Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Kanojia%2C+D">Diptesh Kanojia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The introduction of the MUStARD dataset, and its emotion recognition
extension MUStARD++, have identified sarcasm to be a multi-modal phenomenon --
expressed not only in natural language text, but also through manners of speech
(like tonality and intonation) and visual cues (facial expression). With this
work, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by
considering state-of-the-art language, speech, and visual encoders, for fully
utilizing the totality of the multi-modal richness that it has to offer,
achieving a 2\% improvement in macro-F1 over the existing benchmark.
Additionally, to cure the imbalance in the `sarcasm type' category in
MUStARD++, we propose an extension, which we call \emph{MUStARD++ Balanced},
benchmarking the same with instances from the extension split across both train
and test sets, achieving a further 2.4\% macro-F1 boost. The new clips were
taken from a novel source -- the TV show, House MD, which adds to the diversity
of the dataset, and were manually annotated by multiple annotators with
substantial inter-annotator agreement in terms of Cohen's kappa and
Krippendorf's alpha. Our code, extended data, and SOTA benchmark models are
made public.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01432" title="Abstract">arXiv:2310.01432</a> [<a href="/pdf/2310.01432" title="Download PDF">pdf</a>, <a href="/format/2310.01432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Split and Merge: Aligning Position Biases in Large Language Model based  Evaluators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaozheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Pingchuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Daoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Cuiyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have shown promise as automated evaluators for
assessing the quality of answers generated by AI systems. However, these
LLM-based evaluators exhibit position bias, or inconsistency, when used to
evaluate candidate answers in pairwise comparisons, favoring either the first
or second answer regardless of content. To address this limitation, we propose
PORTIA, an alignment-based system designed to mimic human comparison strategies
to calibrate position bias in a lightweight yet effective manner. Specifically,
PORTIA splits the answers into multiple segments, aligns similar content across
candidate answers, and then merges them back into a single prompt for
evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to
evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances
the consistency rates for all the models and comparison forms tested, achieving
an average relative improvement of 47.46%. Remarkably, PORTIA enables less
advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4
model at just 10% of the cost. Furthermore, it rectifies around 80% of the
position bias instances within the GPT-4 model, elevating its consistency rate
up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced
GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with
human evaluators. These findings highlight PORTIA's ability to correct position
bias, improve LLM consistency, and boost performance while keeping
cost-efficiency. This represents a valuable step toward a more reliable and
scalable use of LLMs for automated evaluations across diverse applications.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01434" title="Abstract">arXiv:2310.01434</a> [<a href="/pdf/2310.01434" title="Download PDF">pdf</a>, <a href="/format/2310.01434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT  LLM on Mobile
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carreira%2C+S">Samuel Carreira</a>, 
<a href="/search/cs?searchtype=author&query=Marques%2C+T">Tom&#xe1;s Marques</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+J">Jos&#xe9; Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Grilo%2C+C">Carlos Grilo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The field of Artificial Intelligence has witnessed remarkable progress in
recent years, especially with the emergence of powerful large language models
(LLMs) based on the transformer architecture. Cloud-based LLMs, such as
OpenAI's ChatGPT, offer impressive capabilities but come with concerns
regarding latency and privacy due to network dependencies. This article
presents an innovative approach to LLM inference, envisioning a future where
LLMs with billions of parameters can be executed directly on mobile devices
without network connectivity. The article showcases a fine-tuned GPT LLM with 3
billion parameters that can operate smoothly on devices with as low as 4GB of
memory. Through the integration of native code and model quantization
techniques, the application not only serves as a general-purpose assistant but
also facilitates seamless mobile interactions with text-to-actions features.
The article provides insights into the training pipeline, implementation
details, test results, and future directions of on-device LLM inference. This
breakthrough technology opens up possibilities for empowering users with
sophisticated AI capabilities while preserving their privacy and eliminating
latency concerns.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01436" title="Abstract">arXiv:2310.01436</a> [<a href="/pdf/2310.01436" title="Download PDF">pdf</a>, <a href="/format/2310.01436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Architecture Search with GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haishuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+J">Jiajun Bu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Neural Architecture Search (GNAS) has shown promising results in
automatically designing graph neural networks. However, GNAS still requires
intensive human labor with rich domain knowledge to design the search space and
search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new
GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The
basic idea of our method is to design a new class of prompts for GPT-4 to guide
GPT-4 toward the generative task of graph neural architectures. The prompts
consist of descriptions of the search space, search strategy, and search
feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS
generates more accurate graph neural networks with fast convergence.
Experimental results show that embedding GPT-4 into GNAS outperforms the
state-of-the-art GNAS methods.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01437" title="Abstract">arXiv:2310.01437</a> [<a href="/pdf/2310.01437" title="Download PDF">pdf</a>, <a href="/format/2310.01437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Secrecy in UAV RSMA Networks: Deep Unfolding Meets Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adam%2C+A+B+M">Abuzar B. M. Adam</a>, 
<a href="/search/cs?searchtype=author&query=Elhassan%2C+M+A+M">Mohammed A. M. Elhassan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Emerging Technologies (cs.ET); Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we consider the maximization of the secrecy rate in multiple
unmanned aerial vehicles (UAV) rate-splitting multiple access (RSMA) network. A
joint beamforming, rate allocation, and UAV trajectory optimization problem is
formulated which is nonconvex. Hence, the problem is transformed into a Markov
decision problem and a novel multiagent deep reinforcement learning (DRL)
framework is designed. The proposed framework (named DUN-DRL) combines deep
unfolding to design beamforming and rate allocation, data-driven to design the
UAV trajectory, and deep deterministic policy gradient (DDPG) for the learning
procedure. The proposed DUN-DRL have shown great performance and outperformed
other DRL-based methods in the literature.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01438" title="Abstract">arXiv:2310.01438</a> [<a href="/pdf/2310.01438" title="Download PDF">pdf</a>, <a href="/format/2310.01438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Flexible, Scalable, and Machine Learning-ready Multimodal  Oncology Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+A">Aakash Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Waqas%2C+A">Asim Waqas</a>, 
<a href="/search/cs?searchtype=author&query=Venkatesan%2C+K">Kavya Venkatesan</a>, 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+Y">Yasin Yilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Rasool%2C+G">Ghulam Rasool</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The advancements in data acquisition, storage, and processing techniques have
resulted in the rapid growth of heterogeneous medical data. Integrating
radiological scans, histopathology images, and molecular information with
clinical data is essential for developing a holistic understanding of the
disease and optimizing treatment. The need for integrating data from multiple
sources is further pronounced in complex diseases such as cancer for enabling
precision medicine and personalized treatments. This work proposes Multimodal
Integration of Oncology Data System (MINDS) - a flexible, scalable, and
cost-effective metadata framework for efficiently fusing disparate data from
public sources such as the Cancer Research Data Commons (CRDC) into an
interconnected, patient-centric framework. MINDS offers an interface for
exploring relationships across data types and building cohorts for developing
large-scale multimodal machine learning models. By harmonizing multimodal data,
MINDS aims to potentially empower researchers with greater analytical ability
to uncover diagnostic and prognostic insights and enable evidence-based
personalized care. MINDS tracks granular end-to-end data provenance, ensuring
reproducibility and transparency. The cloud-native architecture of MINDS can
handle exponential data growth in a secure, cost-optimized manner while
ensuring substantial storage optimization, replication avoidance, and dynamic
access capabilities. Auto-scaling, access controls, and other mechanisms
guarantee pipelines' scalability and security. MINDS overcomes the limitations
of existing biomedical data silos via an interoperable metadata-driven approach
that represents a pivotal step toward the future of oncology data integration.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01439" title="Abstract">arXiv:2310.01439</a> [<a href="/pdf/2310.01439" title="Download PDF">pdf</a>, <a href="/format/2310.01439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Friends in the Dark: Ad Hoc Teamwork Under Partial Observability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribeiroa%2C+J+G">Jo&#xe3;o G. Ribeiroa</a>, 
<a href="/search/cs?searchtype=author&query=Martinhoa%2C+C">Cassandro Martinhoa</a>, 
<a href="/search/cs?searchtype=author&query=Sardinhaa%2C+A">Alberto Sardinhaa</a>, 
<a href="/search/cs?searchtype=author&query=Melo%2C+F+S">Francisco S. Melo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2201.03538">arXiv:2201.03538</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces a formal definition of the setting of ad hoc teamwork
under partial observability and proposes a first-principled model-based
approach which relies only on prior knowledge and partial observations of the
environment in order to perform ad hoc teamwork. We make three distinct
assumptions that set it apart previous works, namely: i) the state of the
environment is always partially observable, ii) the actions of the teammates
are always unavailable to the ad hoc agent and iii) the ad hoc agent has no
access to a reward signal which could be used to learn the task from scratch.
Our results in 70 POMDPs from 11 domains show that our approach is not only
effective in assisting unknown teammates in solving unknown tasks but is also
robust in scaling to more challenging problems.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01440" title="Abstract">arXiv:2310.01440</a> [<a href="/pdf/2310.01440" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Many Voices of Duying: Revisiting the Disputed Essays Between Lu Xun  and Zhou Zuoren
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangqiong Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haining Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Lu Xun and Zhou Zuoren stand as two of the most influential writers in modern
Chinese literature. Beyond their familial ties as brothers, they were also
intimate collaborators during the nascent stages of their writing careers. This
research employs quantitative methods to revisit three disputed essays
pseudonymously published by the brothers in 1912. Our stylometric analysis uses
an interpretable authorship attribution model to investigate the essays'
authorship and examine the brothers' respective writing styles. Our findings
suggest that 'Looking at the Country of China' was authored by Lu Xun.
Moreover, 'People of Yue, Forget Not Your Ancestors' Instructions' seems to be
either predominantly authored or extensively revised by Lu Xun given its
notable stylistic similarities to 'Looking at the Land of Yue,' a piece Zhou
Zuoren recognized as his own, but edited by Lu Xun. The third essay, 'Where Has
the Character of the Republic Gone?,' exhibits a 'diluted', mixed writing
style, suggesting thorough collaboration. We offer visual representations of
essay features to facilitate a nuanced and intuitive understanding. We have
uncovered evidence suggesting Lu Xun's covert engagement with social issues
during his purported 'silent era' and provided insights into the brothers'
formative intellectual trajectories.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01441" title="Abstract">arXiv:2310.01441</a> [<a href="/pdf/2310.01441" title="Download PDF">pdf</a>, <a href="/format/2310.01441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large  Language Model Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+H">Hejia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Boxun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated impressive inferential
capabilities, with numerous research endeavors devoted to enhancing this
capacity through prompting. Despite these efforts, a unified epistemological
foundation is still conspicuously absent. Drawing inspiration from Kant's a
priori philosophy, we propose the UPAR prompting framework, designed to emulate
the structure of human cognition within LLMs. The UPAR framework is delineated
into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the
extraction of structured information from complex contexts, prior planning of
solutions, execution according to plan, and self-reflection. This structure
significantly augments the explainability and accuracy of LLM inference,
producing a human-understandable and inspectable inferential trajectory.
Furthermore, our work offers an epistemological foundation for existing
prompting techniques, allowing for a possible systematic integration of these
methods. With GPT-4, our approach elevates the accuracy from COT baseline of
22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in
the causal judgment task.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01443" title="Abstract">arXiv:2310.01443</a> [<a href="/pdf/2310.01443" title="Download PDF">pdf</a>, <a href="/format/2310.01443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum-Based Feature Selection for Multi-classification Problem in  Complex Systems with Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junxiu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peipei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zhibin Lei</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xu Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Complexity, 2020. 2020: p. 8216874
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Quantum Physics (quant-ph)

</div>
<p class="mathjax">The complex systems with edge computing require a huge amount of
multi-feature data to extract appropriate insights for their decision making,
so it is important to find a feasible feature selection method to improve the
computational efficiency and save the resource consumption. In this paper, a
quantum-based feature selection algorithm for the multi-classification problem,
namely, QReliefF, is proposed, which can effectively reduce the complexity of
algorithm and improve its computational efficiency. First, all features of each
sample are encoded into a quantum state by performing operations CMP and R_y,
and then the amplitude estimation is applied to calculate the similarity
between any two quantum states (i.e., two samples). According to the
similarities, the Grover-Long method is utilized to find the nearest k neighbor
samples, and then the weight vector is updated. After a certain number of
iterations through the above process, the desired features can be selected with
regards to the final weight vector and the threshold {\tau}. Compared with the
classical ReliefF algorithm, our algorithm reduces the complexity of similarity
calculation from O(MN) to O(M), the complexity of finding the nearest neighbor
from O(M) to O(sqrt(M)), and resource consumption from O(MN) to O(MlogN).
Meanwhile, compared with the quantum Relief algorithm, our algorithm is
superior in finding the nearest neighbor, reducing the complexity from O(M) to
O(sqrt(M)). Finally, in order to verify the feasibility of our algorithm, a
simulation experiment based on Rigetti with a simple example is performed.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01444" title="Abstract">arXiv:2310.01444</a> [<a href="/pdf/2310.01444" title="Download PDF">pdf</a>, <a href="/format/2310.01444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting LLM Agents Through Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yadong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Santacroce%2C+M">Michael Santacroce</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Recent
advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Through
iterative exploration and PPO training, LTC empowers the agent to assimilate
short-term experiences into long-term memory. To optimize agent interactions
for task-specific learning, we introduce three structured communication
patterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as
decision-making, knowledge-intensive reasoning, and numerical reasoning. We
evaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA
(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,
it exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,
LTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it
outperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,
LTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results
showcase the versatility and efficiency of the LTC approach across diverse
domains. We will open-source our code to promote further development of the
community.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01445" title="Abstract">arXiv:2310.01445</a> [<a href="/pdf/2310.01445" title="Download PDF">pdf</a>, <a href="/format/2310.01445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigation on a Novel Length-Based Local Linear Subdivision Strategy  for Triangular Meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Junyi Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">Triangular meshes are a widely used representation in the field of 3D
modeling. In this paper, we present a novel approach for edge length-based
linear subdivision on triangular meshes, along with two auxiliary techniques.
We conduct a comprehensive comparison of different subdivision methods in terms
of computational capabilities and mesh-enhancing abilities. Our proposed
approach demonstrates improved computational efficiency and generates fewer
elements with higher quality compared to existing methods. The improvement in
computational efficiency and mesh augmentation capability of our method is
further enhanced when working with the two auxiliary techniques presented in
this paper. Our novel strategy represents a significant contribution to the
field and has important implications for local mesh refinement, computer-aided
design, and isotropic remeshing.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01446" title="Abstract">arXiv:2310.01446</a> [<a href="/pdf/2310.01446" title="Download PDF">pdf</a>, <a href="/format/2310.01446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive-Solver Framework for Dynamic Strategy Selection in Large  Language Model Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jianpeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanlin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) are showcasing impressive ability in handling
complex reasoning tasks. In real-world situations, problems often span a
spectrum of complexities. Humans inherently adjust their problem-solving
approaches based on task complexity. However, most methodologies that leverage
LLMs tend to adopt a uniform approach: utilizing consistent models, prompting
methods, and degrees of problem decomposition, regardless of the problem
complexity. Inflexibility of them can bring unnecessary computational overhead
or sub-optimal performance. To address this problem, we introduce an
Adaptive-Solver framework. It strategically modulates solving strategies based
on the difficulties of the problems. Given an initial solution, the framework
functions with two primary modules. The initial evaluation module assesses the
adequacy of the current solution. If improvements are needed, the subsequent
adaptation module comes into play. Within this module, three key adaptation
strategies are employed: (1) Model Adaptation: Switching to a stronger LLM when
a weaker variant is inadequate. (2) Prompting Method Adaptation: Alternating
between different prompting techniques to suit the problem's nuances. (3)
Decomposition Granularity Adaptation: Breaking down a complex problem into more
fine-grained sub-questions to enhance solvability. Through such dynamic
adaptations, our framework not only enhances computational efficiency but also
elevates the overall performance. This dual-benefit ensures both the efficiency
of the system for simpler tasks and the precision required for more complex
questions. Experimental results from complex reasoning tasks reveal that the
prompting method adaptation and decomposition granularity adaptation enhance
performance across all tasks. Furthermore, the model adaptation approach
significantly reduces API costs (up to 50%) while maintaining superior
performance.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01448" title="Abstract">arXiv:2310.01448</a> [<a href="/pdf/2310.01448" title="Download PDF">pdf</a>, <a href="/format/2310.01448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta Semantic Template for Evaluation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yachuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress; 7 pages; more work at: <a href="https://llm-eval.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Do large language models (LLMs) genuinely understand the semantics of the
language, or just memorize the training data? The recent concern on potential
data contamination of LLMs has raised awareness of the community to conduct
research on LLMs evaluation. In this paper, we propose MSTemp, an approach that
creates meta semantic templates to evaluate the semantic understanding ability
of LLMs. The core of MSTemp is not to perform evaluation directly on existing
benchmark datasets, but to generate new out-of-distribution (OOD) evaluation
sets using existing datasets as seeds. Specifically, for a given sentence,
MSTemp leverages another language model to generate new samples while
preserving its semantics. The new samples are called semantic templates to the
original sentence. Then, MSTemp generates evaluation samples via sentence
parsing and random word replacement on the semantic templates. MSTemp is highly
flexible, dynamic, and cost-effective. Our initial experiments show that
MSTemp-generated samples can significantly reduce the performance of LLMs using
existing datasets as seeds. We hope this initial work can shed light on future
research of LLMs evaluation.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01449" title="Abstract">arXiv:2310.01449</a> [<a href="/pdf/2310.01449" title="Download PDF">pdf</a>, <a href="/format/2310.01449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elastic Interaction Energy Loss for Traffic Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yaxin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yuan Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Luchan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yang Xiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Segmentation is a pixel-level classification of images. The accuracy and fast
inference speed of image segmentation are crucial for autonomous driving
safety. Fine and complex geometric objects are the most difficult but important
recognition targets in traffic scene, such as pedestrians, traffic signs and
lanes. In this paper, a simple and efficient geometry-sensitive energy-based
loss function is proposed to Convolutional Neural Network (CNN) for multi-class
segmentation on real-time traffic scene understanding. To be specific, the
elastic interaction energy (EIE) between two boundaries will drive the
prediction moving toward the ground truth until completely overlap. The EIE
loss function is incorporated into CNN to enhance accuracy on fine-scale
structure segmentation. In particular, small or irregularly shaped objects can
be identified more accurately, and discontinuity issues on slender objects can
be improved. Our approach can be applied to different segmentation-based
problems, such as urban scene segmentation and lane detection. We
quantitatively and qualitatively analyze our method on three traffic datasets,
including urban scene data Cityscapes, lane data TuSimple and CULane. The
results show that our approach consistently improves performance, especially
when using real-time, lightweight networks as the backbones, which is more
suitable for autonomous driving.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01452" title="Abstract">arXiv:2310.01452</a> [<a href="/pdf/2310.01452" title="Download PDF">pdf</a>, <a href="/format/2310.01452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fooling the Textual Fooler via Randomizing Latent Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+C">Duy C. Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q+H">Quang H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Manchanda%2C+S">Saurav Manchanda</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+M">MinLong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kok-Seng Wong</a>, 
<a href="/search/cs?searchtype=author&query=Doan%2C+K+D">Khoa D. Doan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite outstanding performance in a variety of NLP tasks, recent studies
have revealed that NLP models are vulnerable to adversarial attacks that
slightly perturb the input to cause the models to misbehave. Among these
attacks, adversarial word-level perturbations are well-studied and effective
attack strategies. Since these attacks work in black-box settings, they do not
require access to the model architecture or model parameters and thus can be
detrimental to existing NLP applications. To perform an attack, the adversary
queries the victim model many times to determine the most important words in an
input text and to replace these words with their corresponding synonyms. In
this work, we propose a lightweight and attack-agnostic defense whose main goal
is to perplex the process of generating an adversarial example in these
query-based black-box attacks; that is to fool the textual fooler. This
defense, named AdvFooler, works by randomizing the latent representation of the
input at inference time. Different from existing defenses, AdvFooler does not
necessitate additional computational overhead during training nor relies on
assumptions about the potential adversarial perturbation set while having a
negligible impact on the model's accuracy. Our theoretical and empirical
analyses highlight the significance of robustness resulting from confusing the
adversary via randomizing the latent space, as well as the impact of
randomization on clean accuracy. Finally, we empirically demonstrate near
state-of-the-art robustness of AdvFooler against representative adversarial
word-level attacks on two benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01459" title="Abstract">arXiv:2310.01459</a> [<a href="/pdf/2310.01459" title="Download PDF">pdf</a>, <a href="/format/2310.01459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NarrativePlay: Interactive Narrative Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Runcong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lixing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanran Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In this paper, we introduce NarrativePlay, a novel system that allows users
to role-play a fictional character and interact with other characters in
narratives such as novels in an immersive environment. We leverage Large
Language Models (LLMs) to generate human-like responses, guided by personality
traits extracted from narratives. The system incorporates auto-generated visual
display of narrative settings, character portraits, and character speech,
greatly enhancing user experience. Our approach eschews predefined sandboxes,
focusing instead on main storyline events extracted from narratives from the
perspective of a user-selected character. NarrativePlay has been evaluated on
two types of narratives, detective and adventure stories, where users can
either explore the world or improve their favorability with the narrative
characters through conversations.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01461" title="Abstract">arXiv:2310.01461</a> [<a href="/pdf/2310.01461" title="Download PDF">pdf</a>, <a href="/ps/2310.01461" title="Download PostScript">ps</a>, <a href="/format/2310.01461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Awkward Just-In-Time (JIT) Compilation: A Developer&#x27;s Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osborne%2C+I">Ianna Osborne</a>, 
<a href="/search/cs?searchtype=author&query=Pivarski%2C+J">Jim Pivarski</a>, 
<a href="/search/cs?searchtype=author&query=Ifrim%2C+I">Ioana Ifrim</a>, 
<a href="/search/cs?searchtype=author&query=Hollands%2C+A">Angus Hollands</a>, 
<a href="/search/cs?searchtype=author&query=Schreiner%2C+H">Henry Schreiner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Awkward Array is a library for performing NumPy-like computations on nested,
variable-sized data, enabling array-oriented programming on arbitrary data
structures in Python. However, imperative (procedural) solutions can sometimes
be easier to write or faster to run. Performant imperative programming requires
compilation; JIT-compilation makes it convenient to compile in an interactive
Python environment. Various functions in Awkward Arrays JIT-compile a user's
code into executable machine code. They use several different techniques, but
reuse parts of each others' implementations. We discuss the techniques used to
achieve the Awkward Arrays acceleration with JIT-compilation, focusing on
RDataFrame, cppyy, and Numba, particularly Numba on GPUs: conversions of
Awkward Arrays to and from RDataFrame; standalone cppyy; passing Awkward Arrays
to and from Python functions compiled by Numba; passing Awkward Arrays to
Python functions compiled for GPUs by Numba; and header-only libraries for
populating Awkward Arrays from C++ without any Python dependencies.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01467" title="Abstract">arXiv:2310.01467</a> [<a href="/pdf/2310.01467" title="Download PDF">pdf</a>, <a href="/format/2310.01467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongxu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Daguang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+H+R">Holger R. Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pre-trained language models (PLM) have revolutionized the NLP landscape,
achieving stellar performances across diverse tasks. These models, while
benefiting from vast training data, often require fine-tuning on specific data
to cater to distinct downstream tasks. However, this data adaptation process
has inherent security and privacy concerns, primarily when leveraging
user-generated, device-residing data. Federated learning (FL) provides a
solution, allowing collaborative model fine-tuning without centralized data
collection. However, applying FL to finetune PLMs is hampered by challenges,
including restricted model parameter access, high computational requirements,
and communication overheads. This paper introduces Federated Black-box Prompt
Tuning (FedBPT), a framework designed to address these challenges. FedBPT does
not require the clients to access the model parameters. By focusing on training
optimal prompts and utilizing gradient-free optimization methods, FedBPT
reduces the number of exchanged variables, boosts communication efficiency, and
minimizes computational and storage costs. Experiments highlight the
framework's ability to drastically cut communication and memory costs while
maintaining competitive performance. Ultimately, FedBPT presents a promising
solution for efficient, privacy-preserving fine-tuning of PLM in the age of
large language models.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01468" title="Abstract">arXiv:2310.01468</a> [<a href="/pdf/2310.01468" title="Download PDF">pdf</a>, <a href="/format/2310.01468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Entity-Deduction Arena: A playground for probing the conversational  reasoning and planning capabilities of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jaitly%2C+N">Navdeep Jaitly</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are currently effective at answering questions
that are clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This \textit{entity-deducing game} can serve as an
evaluation framework to probe the conversational reasoning and planning
capabilities of language models. We systematically evaluate various LLMs and
discover significant differences in their performance on this task. We find
that strong LLMs like GPT-4 outperform human players by a large margin. We
further employ Behavior Cloning (BC) to examine whether a weaker model is
capable of imitating a stronger model and generalizing to data or domains,
using only the demonstrations from a stronger model. We finally propose to use
Reinforcement Learning to enhance reasoning and planning capacity of Vicuna
models through episodes of game playing, which lead to significant performance
improvement. We hope that this problem offers insights into how autonomous
agents could be trained to behave more intelligently in ambiguous
circumstances.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01469" title="Abstract">arXiv:2310.01469</a> [<a href="/pdf/2310.01469" title="Download PDF">pdf</a>, <a href="/format/2310.01469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Lies: Hallucinations are not Bugs, but Features as Adversarial  Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jia-Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+K">Kun-Peng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen-Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Mu-Nan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be
knowledgeable and able to adapt to many tasks. However, we still can not
completely trust their answer, since LLMs suffer from
hallucination--fabricating non-existent facts to cheat users without
perception. And the reasons for their existence and pervasiveness remain
unclear. In this paper, we demonstrate that non-sense prompts composed of
random tokens can also elicit the LLMs to respond with hallucinations. This
phenomenon forces us to revisit that hallucination may be another view of
adversarial examples, and it shares similar features with conventional
adversarial examples as the basic feature of LLMs. Therefore, we formalize an
automatic hallucination triggering method as the hallucination attack in an
adversarial way. Finally, we explore basic feature of attacked adversarial
prompts and propose a simple yet effective defense strategy. Our code is
released on GitHub.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01470" title="Abstract">arXiv:2310.01470</a> [<a href="/pdf/2310.01470" title="Download PDF">pdf</a>, <a href="/format/2310.01470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges in Modelling and Solving Plotting with PDDL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Espasa%2C+J">Joan Espasa</a>, 
<a href="/search/cs?searchtype=author&query=Miguel%2C+I">Ian Miguel</a>, 
<a href="/search/cs?searchtype=author&query=Nightingale%2C+P">Peter Nightingale</a>, 
<a href="/search/cs?searchtype=author&query=Salamon%2C+A+Z">Andr&#xe1;s Z. Salamon</a>, 
<a href="/search/cs?searchtype=author&query=Villaret%2C+M">Mateu Villaret</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2110.14397">arXiv:2110.14397</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We study a planning problem based on Plotting, a tile-matching puzzle video
game published by Taito in 1989. The objective of this game is to remove a
target number of coloured blocks from a grid by sequentially shooting blocks
into the grid. Plotting features complex transitions after every shot: various
blocks are affected directly, while others can be indirectly affected by
gravity. We highlight the challenges of modelling Plotting with PDDL and of
solving it with a grounding-based state-of-the-art planner.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01471" title="Abstract">arXiv:2310.01471</a> [<a href="/pdf/2310.01471" title="Download PDF">pdf</a>, <a href="/format/2310.01471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Good Snowman is Hard to Plan
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bofill%2C+M">Miquel Bofill</a>, 
<a href="/search/cs?searchtype=author&query=Borralleras%2C+C">Cristina Borralleras</a>, 
<a href="/search/cs?searchtype=author&query=Espasa%2C+J">Joan Espasa</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn%2C+G">Gerard Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Patow%2C+G">Gustavo Patow</a>, 
<a href="/search/cs?searchtype=author&query=Villaret%2C+M">Mateu Villaret</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2310.01378">arXiv:2310.01378</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In this work we face a challenging puzzle video game: A Good Snowman is Hard
to Build. The objective of the game is to build snowmen by moving and stacking
snowballs on a discrete grid. For the sake of player engagement with the game,
it is interesting to avoid that a player finds a much easier solution than the
one the designer expected. Therefore, having tools that are able to certify the
optimality of solutions is crucial.
<br />Although the game can be stated as a planning problem and can be naturally
modelled in PDDL, we show that a direct translation to SAT clearly outperforms
off-the-shelf state-of-the-art planners. As we show, this is mainly due to the
fact that reachability properties can be easily modelled in SAT, allowing for
shorter plans, whereas using axioms to express a reachability derived predicate
in PDDL does not result in any significant reduction of solving time with the
considered planners. We deal with a set of 51 levels, both original and
crafted, solving 43 and with 8 challenging instances still remaining to be
solved.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01503" title="Abstract">arXiv:2310.01503</a> [<a href="/pdf/2310.01503" title="Download PDF">pdf</a>, <a href="/format/2310.01503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Model of Puzznic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Espasa%2C+J">Joan Espasa</a>, 
<a href="/search/cs?searchtype=author&query=Gent%2C+I+P">Ian P. Gent</a>, 
<a href="/search/cs?searchtype=author&query=Miguel%2C+I">Ian Miguel</a>, 
<a href="/search/cs?searchtype=author&query=Nightingale%2C+P">Peter Nightingale</a>, 
<a href="/search/cs?searchtype=author&query=Salamon%2C+A+Z">Andr&#xe1;s Z. Salamon</a>, 
<a href="/search/cs?searchtype=author&query=Villaret%2C+M">Mateu Villaret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We report on progress in modelling and solving Puzznic, a video game
requiring the player to plan sequences of moves to clear a grid by matching
blocks. We focus here on levels with no moving blocks. We compare a planning
approach and three constraint programming approaches on a small set of
benchmark instances. The planning approach is at present superior to the
constraint programming approaches, but we outline proposals for improving the
constraint models.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01505" title="Abstract">arXiv:2310.01505</a> [<a href="/pdf/2310.01505" title="Download PDF">pdf</a>, <a href="/format/2310.01505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automatic Design of Factorio Blueprints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patterson%2C+S">Sean Patterson</a>, 
<a href="/search/cs?searchtype=author&query=Espasa%2C+J">Joan Espasa</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M+S">Mun See Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+R">Ruth Hoffmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Factorio is a 2D construction and management simulation video game about
building automated factories to produce items of increasing complexity. A core
feature of the game is its blueprint system, which allows players to easily
save and replicate parts of their designs. Blueprints can reproduce any layout
of objects in the game, but are typically used to encapsulate a complex
behaviour, such as the production of a non-basic object. Once created, these
blueprints are then used as basic building blocks, allowing the player to
create a layer of abstraction. The usage of blueprints not only eases the
expansion of the factory but also allows the sharing of designs with the game's
community. The layout in a blueprint can be optimised using various criteria,
such as the total space used or the final production throughput. The design of
an optimal blueprint is a hard combinatorial problem, interleaving elements of
many well-studied problems such as bin-packing, routing or network design. This
work presents a new challenging problem and explores the feasibility of a
constraint model to optimise Factorio blueprints, balancing correctness,
optimality, and performance.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01506" title="Abstract">arXiv:2310.01506</a> [<a href="/pdf/2310.01506" title="Download PDF">pdf</a>, <a href="/format/2310.01506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+X">Xuan Ju</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Ailing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yuxuan Bian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shaoteng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-guided diffusion models have revolutionized image generation and
editing, offering exceptional realism and diversity. Specifically, in the
context of diffusion-based editing, where a source image is edited according to
a target prompt, the process commences by acquiring a noisy latent vector
corresponding to the source image via the diffusion model. This vector is
subsequently fed into separate source and target diffusion branches for
editing. The accuracy of this inversion process significantly impacts the final
editing outcome, influencing both essential content preservation of the source
image and edit fidelity according to the target prompt. Prior inversion
techniques aimed at finding a unified solution in both the source and target
diffusion branches. However, our theoretical and empirical analyses reveal that
disentangling these branches leads to a distinct separation of responsibilities
for preserving essential content and ensuring edit fidelity. Building on this
insight, we introduce "Direct Inversion," a novel technique achieving optimal
performance of both branches with just three lines of code. To assess image
editing performance, we present PIE-Bench, an editing benchmark with 700 images
showcasing diverse scenes and editing types, accompanied by versatile
annotations and comprehensive evaluation metrics. Compared to state-of-the-art
optimization-based inversion techniques, our solution not only yields superior
performance across 8 editing methods but also achieves nearly an order of
speed-up.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01507" title="Abstract">arXiv:2310.01507</a> [<a href="/pdf/2310.01507" title="Download PDF">pdf</a>, <a href="/format/2310.01507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replicating Relevance-Ranked Synonym Discovery in a New Language and  Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yates%2C+A">Andrew Yates</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECIR (1) 2019: 429-442
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Domain-specific synonyms occur in many specialized search tasks, such as when
searching medical documents, legal documents, and software engineering
artifacts. We replicate prior work on ranking domain-specific synonyms in the
consumer health domain by applying the approach to a new language and domain:
identifying Swedish language synonyms in the building construction domain. We
chose this setting because identifying synonyms in this domain is helpful for
downstream systems, where different users may query for documents (e.g.,
engineering requirements) using different terminology. We consider two new
features inspired by the change in language and methodological advances since
the prior work's publication. An evaluation using data from the building
construction domain supports the finding from the prior work that synonym
discovery is best approached as a learning to rank task in which a human editor
views ranked synonym candidates in order to construct a domain-specific
thesaurus. We additionally find that FastText embeddings alone provide a strong
baseline, though they do not perform as well as the strongest learning to rank
method. Finally, we analyze the performance of individual features and the
differences in the domains.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01508" title="Abstract">arXiv:2310.01508</a> [<a href="/pdf/2310.01508" title="Download PDF">pdf</a>, <a href="/format/2310.01508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CODA: Temporal Domain Generalization via Concept Drift Simulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Chia-Yuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yu-Neng Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhimeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+K">Kwei-Herng Lai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+A">Anxiao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+N">Na Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In real-world applications, machine learning models often become obsolete due
to shifts in the joint distribution arising from underlying temporal trends, a
phenomenon known as the "concept drift". Existing works propose model-specific
strategies to achieve temporal generalization in the near-future domain.
However, the diverse characteristics of real-world datasets necessitate
customized prediction model architectures. To this end, there is an urgent
demand for a model-agnostic temporal domain generalization approach that
maintains generality across diverse data modalities and architectures. In this
work, we aim to address the concept drift problem from a data-centric
perspective to bypass considering the interaction between data and model.
Developing such a framework presents non-trivial challenges: (i) existing
generative models struggle to generate out-of-distribution future data, and
(ii) precisely capturing the temporal trends of joint distribution along
chronological source domains is computationally infeasible. To tackle the
challenges, we propose the COncept Drift simulAtor (CODA) framework
incorporating a predicted feature correlation matrix to simulate future data
for model training. Specifically, CODA leverages feature correlations to
represent data characteristics at specific time points, thereby circumventing
the daunting computational costs. Experimental results demonstrate that using
CODA-generated data as training input effectively achieves temporal domain
generalization across different model architectures.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01517" title="Abstract">arXiv:2310.01517</a> [<a href="/pdf/2310.01517" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kandil%2C+M">Mohamed Kandil</a>, 
<a href="/search/cs?searchtype=author&query=McArthur%2C+J+J">J.J. McArthur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 15 figures, initial manuscript self-archiving prior to submission to ADVEI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Gray-box models offer significant benefit over black-box approaches for
equipment emulator development for equipment since their integration of physics
provides more confidence in the model outside of the training domain. However,
challenges such as model nonlinearity, unmodeled dynamics, and local minima
introduce uncertainties into grey-box creation that contemporary approaches
have failed to overcome, leading to their under-performance compared with
black-box models. This paper seeks to address these uncertainties by injecting
noise into the training dataset. This noise injection enriches the dataset and
provides a measure of robustness against such uncertainties. A dynamic model
for a water-to-water heat exchanger has been used as a demonstration case for
this approach and tested using a pair of real devices with live data streaming.
Compared to the unprocessed signal data, the application of noise injection
resulted in a significant reduction in modeling error (root mean square error),
decreasing from 0.68 to 0.27{\deg}C. This improvement amounts to a 60%
enhancement when assessed on the training set, and improvements of 50% and 45%
when validated against the test and validation sets, respectively.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01519" title="Abstract">arXiv:2310.01519</a> [<a href="/pdf/2310.01519" title="Download PDF">pdf</a>, <a href="/format/2310.01519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision-Oriented Intervention Cost Prediction for Multi-robot  Persistent Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangyao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shek%2C+C+L">Chak Lam Shek</a>, 
<a href="/search/cs?searchtype=author&query=Karapetyan%2C+N">Nare Karapetyan</a>, 
<a href="/search/cs?searchtype=author&query=Tokekar%2C+P">Pratap Tokekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2303.01543">arXiv:2303.01543</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we present a differentiable, decision-oriented learning
technique for a class of vehicle routing problems. Specifically, we consider a
scenario where a team of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground
Vehicles (UGVs) are persistently monitoring an environment. The UGVs are
occasionally taken over by humans to take detours to recharge the depleted
UAVs. The goal is to select routes for the UGVs so that they can efficiently
monitor the environment while reducing the cost of interventions. The former is
modeled as a monotone, submodular function whereas the latter is a linear
function of the routes of the UGVs. We consider a scenario where the former is
known but the latter depends on the context (e.g., wind and terrain conditions)
that must be learned. Typically, we first learn to predict the cost function
and then solve the optimization problem. However, the loss function used in
prediction may be misaligned with our final goal of finding good routes. We
propose a \emph{decision-oriented learning} framework that incorporates task
optimization as a differentiable layer in the prediction phase. To make the
task optimization (which is a non-monotone submodular function) differentiable,
we propose the Differentiable Cost Scaled Greedy algorithm. We demonstrate the
efficacy of the proposed framework through numerical simulations. The results
show that the proposed framework can result in better performance than the
traditional approach.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01520" title="Abstract">arXiv:2310.01520</a> [<a href="/pdf/2310.01520" title="Download PDF">pdf</a>, <a href="/format/2310.01520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gap between Structural and Semantic Similarity in Diverse  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelwahed%2C+M+F">Mustafa F. Abdelwahed</a>, 
<a href="/search/cs?searchtype=author&query=Espasa%2C+J">Joan Espasa</a>, 
<a href="/search/cs?searchtype=author&query=Toniolo%2C+A">Alice Toniolo</a>, 
<a href="/search/cs?searchtype=author&query=Gent%2C+I+P">Ian P. Gent</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Diverse planning is the problem of finding multiple plans for a given problem
specification, which is at the core of many real-world applications. For
example, diverse planning is a critical piece for the efficiency of plan
recognition systems when dealing with noisy and missing observations. Providing
diverse solutions can also benefit situations where constraints are too
expensive or impossible to model. Current diverse planners operate by
generating multiple plans and then applying a selection procedure to extract
diverse solutions using a similarity metric. Generally, current similarity
metrics only consider the structural properties of the given plans. We argue
that this approach is a limitation that sometimes prevents such metrics from
capturing why two plans differ. In this work, we propose two new
domain-independent metrics which are able to capture relevant information on
the difference between two given plans from a domain-dependent viewpoint. We
showcase their utility in various situations where the currently used metrics
fail to capture the similarity between plans, failing to capture some
structural symmetries.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01522" title="Abstract">arXiv:2310.01522</a> [<a href="/pdf/2310.01522" title="Download PDF">pdf</a>, <a href="/format/2310.01522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Property-preserving numerical approximations of a  Cahn-Hilliard-Navier-Stokes model with variable densities and degenerate  mobility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Acosta-Soba%2C+D">Daniel Acosta-Soba</a>, 
<a href="/search/math?searchtype=author&query=Guill%C3%A9n-Gonz%C3%A1lez%2C+F">Francisco Guill&#xe9;n-Gonz&#xe1;lez</a>, 
<a href="/search/math?searchtype=author&query=Rodr%C3%ADguez-Galv%C3%A1n%2C+J+R">J. Rafael Rodr&#xed;guez-Galv&#xe1;n</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 11 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we present a new computational framework using coupled and
decoupled approximations for a Cahn-Hilliard-Navier-Stokes model with variable
densities and degenerate mobility. In this sense, the coupled approximation is
shown to conserve the mass of the fluid, preserve the point-wise bounds of the
density and decrease an energy functional. In contrast, the decoupled scheme is
presented as a more computationally efficient alternative but the discrete
energy-decreasing property can not be assured. Both schemes are based on a
finite element approximation for the Navier-Stokes fluid flow with
discontinuous pressure and an upwind discontinuous Galerkin scheme for the
Cahn-Hilliard part. Finally, several numerical experiments contrasting both
approaches are conducted. In particular, results for a convergence test, a
simple qualitative comparison and some well-known benchmark problems are shown.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01524" title="Abstract">arXiv:2310.01524</a> [<a href="/pdf/2310.01524" title="Download PDF">pdf</a>, <a href="/format/2310.01524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep  generative models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suri%2C+D">Dhruv Suri</a>, 
<a href="/search/cs?searchtype=author&query=Arifi%2C+A">Anela Arifi</a>, 
<a href="/search/cs?searchtype=author&query=Azevedo%2C+I">Ines Azevedo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Tackling Climate Change with Machine Learning Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Nowcasting day-ahead marginal emissions factors is increasingly important for
power systems with high flexibility and penetration of distributed energy
resources. With a significant share of firm generation from natural gas and
coal power plants, forecasting day-ahead emissions in the current energy system
has been widely studied. In contrast, as we shift to an energy system
characterized by flexible power markets, dispatchable sources, and competing
low-cost generation such as large-scale battery or hydrogen storage, system
operators will be able to choose from a mix of different generation as well as
emission pathways. To fully develop the emissions implications of a given
dispatch schedule, we need a near real-time workflow with two layers. The first
layer is a market model that continuously solves a security-constrained
economic dispatch model. The second layer determines the marginal emissions
based on the output of the market model, which is the subject of this paper. We
propose using multi-headed convolutional neural networks to generate day-ahead
forecasts of marginal and average emissions for a given independent system
operator.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01526" title="Abstract">arXiv:2310.01526</a> [<a href="/pdf/2310.01526" title="Download PDF">pdf</a>, <a href="/format/2310.01526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modern code reviews -- Preliminary results of a systematic mapping study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Badampudi%2C+D">Deepika Badampudi</a>, 
<a href="/search/cs?searchtype=author&query=Britto%2C+R">Ricardo Britto</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EASE 2019: 340-345
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Reviewing source code is a common practice in a modern and collaborative
coding environment. In the past few years, the research on modern code reviews
has gained interest among practitioners and researchers. The objective of our
investigation is to observe the evolution of research related to modern code
reviews, identify research gaps and serve as a basis for future research. We
use a systematic mapping approach to identify and classify 177 research papers.
As preliminary result of our investigation, we present in this paper a
classification scheme of the main contributions of modern code review research
between 2005 and 2018.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01529" title="Abstract">arXiv:2310.01529</a> [<a href="/pdf/2310.01529" title="Download PDF">pdf</a>, <a href="/format/2310.01529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive DeepSSM: Training Methodology for Image-To-Shape Deep Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aziz%2C+A+Z+B">Abu Zahid Bin Aziz</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+J">Jadie Adams</a>, 
<a href="/search/cs?searchtype=author&query=Elhabian%2C+S">Shireen Elhabian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ShapeMI MICCAI 2023: Workshop on Shape in Medical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Statistical shape modeling (SSM) is an enabling quantitative tool to study
anatomical shapes in various medical applications. However, directly using 3D
images in these applications still has a long way to go. Recent deep learning
methods have paved the way for reducing the substantial preprocessing steps to
construct SSMs directly from unsegmented images. Nevertheless, the performance
of these models is not up to the mark. Inspired by multiscale/multiresolution
learning, we propose a new training strategy, progressive DeepSSM, to train
image-to-shape deep learning models. The training is performed in multiple
scales, and each scale utilizes the output from the previous scale. This
strategy enables the model to learn coarse shape features in the first scales
and gradually learn detailed fine shape features in the later scales. We
leverage shape priors via segmentation-guided multi-task learning and employ
deep supervision loss to ensure learning at each scale. Experiments show the
superiority of models trained by the proposed strategy from both quantitative
and qualitative perspectives. This training methodology can be employed to
improve the stability and accuracy of any deep learning method for inferring
statistical representations of anatomies from medical images and can be adopted
by existing deep learning methods to improve model accuracy and training
stability.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01530" title="Abstract">arXiv:2310.01530</a> [<a href="/pdf/2310.01530" title="Download PDF">pdf</a>, <a href="/format/2310.01530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Pretty Expressive Printer (with Appendices)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Porncharoenwase%2C+S">Sorawee Porncharoenwase</a>, 
<a href="/search/cs?searchtype=author&query=Pombrio%2C+J">Justin Pombrio</a>, 
<a href="/search/cs?searchtype=author&query=Torlak%2C+E">Emina Torlak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages. This is the full version (with appendices) of the OOPSLA 2023 paper "A Pretty Expressive Printer."
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Pretty printers make trade-offs between the expressiveness of their pretty
printing language, the optimality objective that they minimize when choosing
between different ways to lay out a document, and the performance of their
algorithm. This paper presents a new pretty printer, $\Pi_e$, that is strictly
more expressive than all pretty printers in the literature and provably
minimizes an optimality objective. Furthermore, the time complexity of $\Pi_e$
is better than many existing pretty printers. When choosing among different
ways to lay out a document, $\Pi_e$ consults a user-supplied cost factory,
which determines the optimality objective, giving $\Pi_e$ a unique degree of
flexibility. We use the Lean theorem prover to verify the correctness (validity
and optimality) of $\Pi_e$, and implement $\Pi_e$ concretely as a pretty
printer that we call PrettyExpressive. To evaluate our pretty printer against
others, we develop a formal framework for reasoning about the expressiveness of
pretty printing languages, and survey pretty printers in the literature,
comparing their expressiveness, optimality, worst-case time complexity, and
practical running time. Our evaluation shows that PrettyExpressive is efficient
and effective at producing optimal layouts. PrettyExpressive has also seen
real-world adoption: it serves as a foundation of a code formatter for Racket.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01536" title="Abstract">arXiv:2310.01536</a> [<a href="/pdf/2310.01536" title="Download PDF">pdf</a>, <a href="/format/2310.01536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebras of actions in an agent&#x27;s representations of the world
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dean%2C+A">Alexander Dean</a>, 
<a href="/search/cs?searchtype=author&query=Alonso%2C+E">Eduardo Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Mondragon%2C+E">Esther Mondragon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In this paper, we propose a framework to extract the algebra of the
transformations of worlds from the perspective of an agent. As a starting
point, we use our framework to reproduce the symmetry-based representations
from the symmetry-based disentangled representation learning (SBDRL) formalism
proposed by [1]; only the algebra of transformations of worlds that form groups
can be described using symmetry-based representations. We then study the
algebras of the transformations of worlds with features that occur in simple
reinforcement learning scenarios. Using computational methods, that we
developed, we extract the algebras of the transformations of these worlds and
classify them according to their properties. Finally, we generalise two
important results of SBDRL - the equivariance condition and the disentangling
definition - from only working with symmetry-based representations to working
with representations capturing the transformation properties of worlds with
transformations for any algebra. Finally, we combine our generalised
equivariance condition and our generalised disentangling definition to show
that disentangled sub-algebras can each have their own individual equivariance
conditions, which can be treated independently.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01537" title="Abstract">arXiv:2310.01537</a> [<a href="/pdf/2310.01537" title="Download PDF">pdf</a>, <a href="/format/2310.01537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Client Detection via Non-parametric Subspace Monitoring in  the Internet of Federated Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xianjian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+X">Xiaochen Xian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Andi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The Internet of Federated Things (IoFT) represents a network of
interconnected systems with federated learning as the backbone, facilitating
collaborative knowledge acquisition while ensuring data privacy for individual
systems. The wide adoption of IoFT, however, is hindered by security concerns,
particularly the susceptibility of federated learning networks to adversarial
attacks. In this paper, we propose an effective non-parametric approach FedRR,
which leverages the low-rank features of the transmitted parameter updates
generated by federated learning to address the adversarial attack problem.
Besides, our proposed method is capable of accurately detecting adversarial
clients and controlling the false alarm rate under the scenario with no attack
occurring. Experiments based on digit recognition using the MNIST datasets
validated the advantages of our approach.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01538" title="Abstract">arXiv:2310.01538</a> [<a href="/pdf/2310.01538" title="Download PDF">pdf</a>, <a href="/ps/2310.01538" title="Download PostScript">ps</a>, <a href="/format/2310.01538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Sensitive Inhibitory Control for Safe Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lederer%2C+A">Armin Lederer</a>, 
<a href="/search/eess?searchtype=author&query=Noorani%2C+E">Erfaun Noorani</a>, 
<a href="/search/eess?searchtype=author&query=Baras%2C+J+S">John S. Baras</a>, 
<a href="/search/eess?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 62nd IEEE Conference on Decision and Control, Dec. 13-15, 2023, Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Humans have the ability to deviate from their natural behavior when
necessary, which is a cognitive process called response inhibition. Similar
approaches have independently received increasing attention in recent years for
ensuring the safety of control. Realized using control barrier functions or
predictive safety filters, these approaches can effectively ensure the
satisfaction of state constraints through an online adaptation of nominal
control laws, e.g., obtained through reinforcement learning. While the focus of
these realizations of inhibitory control has been on risk-neutral formulations,
human studies have shown a tight link between response inhibition and risk
attitude. Inspired by this insight, we propose a flexible, risk-sensitive
method for inhibitory control. Our method is based on a risk-aware condition
for value functions, which guarantees the satisfaction of state constraints. We
propose a method for learning these value functions using common techniques
from reinforcement learning and derive sufficient conditions for its success.
By enforcing the derived safety conditions online using the learned value
function, risk-sensitive inhibitory control is effectively achieved. The
effectiveness of the developed control scheme is demonstrated in simulations.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01541" title="Abstract">arXiv:2310.01541</a> [<a href="/pdf/2310.01541" title="Download PDF">pdf</a>, <a href="/ps/2310.01541" title="Download PostScript">ps</a>, <a href="/format/2310.01541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Restoring the Discontinuous Heat Equation Source Using Sparse Boundary  Data and Dynamic Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+G">Guang Lin</a>, 
<a href="/search/math?searchtype=author&query=Ou%2C+N">Na Ou</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zecheng Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhidong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This study focuses on addressing the inverse source problem associated with
the parabolic equation. We rely on sparse boundary flux data as our
measurements, which are acquired from a restricted section of the boundary.
While it has been established that utilizing sparse boundary flux data can
enable source recovery, the presence of a limited number of observation sensors
poses a challenge for accurately tracing the inverse quantity of interest. To
overcome this limitation, we introduce a sampling algorithm grounded in
Langevin dynamics that incorporates dynamic sensors to capture the flux
information. Furthermore, we propose and discuss two distinct sensor migration
strategies. Remarkably, our findings demonstrate that even with only two
observation sensors at our disposal, it remains feasible to successfully
reconstruct the high-dimensional unknown parameters.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01542" title="Abstract">arXiv:2310.01542</a> [<a href="/pdf/2310.01542" title="Download PDF">pdf</a>, <a href="/format/2310.01542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusing Models with Complementary Expertise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Polo%2C+F+M">Felipe Maia Polo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuekai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Souvik Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Training AI models that generalize across tasks and domains has long been
among the open problems driving AI research. The emergence of Foundation Models
made it easier to obtain expert models for a given task, but the heterogeneity
of data that may be encountered at test time often means that any single expert
is insufficient. We consider the Fusion of Experts (FoE) problem of fusing
outputs of expert models with complementary knowledge of the data distribution
and formulate it as an instance of supervised learning. Our method is
applicable to both discriminative and generative tasks and leads to significant
performance improvements in image and text classification, text summarization,
multiple-choice QA, and automatic evaluation of generated text. We also extend
our method to the "frugal" setting where it is desired to reduce the number of
expert model evaluations at test time.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01545" title="Abstract">arXiv:2310.01545</a> [<a href="/pdf/2310.01545" title="Download PDF">pdf</a>, <a href="/format/2310.01545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RF-ULM: Deep Learning for Radio-Frequency Ultrasound Localization  Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahne%2C+C">Christopher Hahne</a>, 
<a href="/search/cs?searchtype=author&query=Chabouh%2C+G">Georges Chabouh</a>, 
<a href="/search/cs?searchtype=author&query=Chavignon%2C+A">Arthur Chavignon</a>, 
<a href="/search/cs?searchtype=author&query=Couture%2C+O">Olivier Couture</a>, 
<a href="/search/cs?searchtype=author&query=Sznitman%2C+R">Raphael Sznitman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">In Ultrasound Localization Microscopy (ULM), achieving high-resolution images
relies on the precise localization of contrast agent particles across
consecutive beamformed frames. However, our study uncovers an enormous
potential: The process of delay-and-sum beamforming leads to an irreversible
reduction of Radio-Frequency (RF) data, while its implications for localization
remain largely unexplored. The rich contextual information embedded within RF
wavefronts, including their hyperbolic shape and phase, offers great promise
for guiding Deep Neural Networks (DNNs) in challenging localization scenarios.
To fully exploit this data, we propose to directly localize scatterers in RF
signals. Our approach involves a custom super-resolution DNN using learned
feature channel shuffling and a novel semi-global convolutional sampling block
tailored for reliable and accurate localization in RF input data. Additionally,
we introduce a geometric point transformation that facilitates seamless mapping
between B-mode and RF spaces. To validate the effectiveness of our method and
understand the impact of beamforming, we conduct an extensive comparison with
State-Of-The-Art (SOTA) techniques in ULM. We present the inaugural in vivo
results from an RF-trained DNN, highlighting its real-world practicality. Our
findings show that RF-ULM bridges the domain gap between synthetic and real
datasets, offering a considerable advantage in terms of precision and
complexity. To enable the broader research community to benefit from our
findings, our code and the associated SOTA methods are made available at
https://github.com/hahnec/rf-ulm.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01546" title="Abstract">arXiv:2310.01546</a> [<a href="/pdf/2310.01546" title="Download PDF">pdf</a>, <a href="/format/2310.01546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralization Cheapens Corruptive Majority Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Newman%2C+S+H">Stephen H. Newman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Corruptive majority attacks, in which mining power is distributed among
miners and an attacker attempts to bribe a majority of miners into
participation in a majority attack, pose a threat to blockchains. Budish
bounded the cost of bribing miners to participate in an attack by their
expected loss as a result of attack success. We show that this bound is loose.
In particular, an attack may be structured so that under equilibrium play by
most miners, a miner's choice to participate only slightly affects the attack
success chance. Combined with the fact that most of the cost of attack success
is externalized by any given small miner, this implies that if most mining
power is controlled by small miners, bribing miners to participate in such an
attack is much cheaper than the Budish bound. We provide a scheme for a cheap
corruptive majority attack and discuss practical concerns and consequences.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01551" title="Abstract">arXiv:2310.01551</a> [<a href="/pdf/2310.01551" title="Download PDF">pdf</a>, <a href="/format/2310.01551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of Choices in Decision Tree Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blanc%2C+G">Guy Blanc</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+J">Jane Lange</a>, 
<a href="/search/cs?searchtype=author&query=Pabbaraju%2C+C">Chirag Pabbaraju</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+C">Colin Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Li-Yang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mo Tiwari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We propose a simple generalization of standard and empirically successful
decision tree learning algorithms such as ID3, C4.5, and CART. These
algorithms, which have been central to machine learning for decades, are greedy
in nature: they grow a decision tree by iteratively splitting on the best
attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as
possible splits instead of just the single best attribute. We demonstrate,
theoretically and empirically, the power of this simple generalization. We
first prove a {\sl greediness hierarchy theorem} showing that for every $k \in
\mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there
are data distributions for which the former achieves accuracy $1-\varepsilon$,
whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then
show, through extensive experiments, that Top-$k$ outperforms the two main
approaches to decision tree learning: classic greedy algorithms and more recent
"optimal decision tree" algorithms. On one hand, Top-$k$ consistently enjoys
significant accuracy gains over greedy algorithms across a wide range of
benchmarks. On the other hand, Top-$k$ is markedly more scalable than optimal
decision tree algorithms and is able to handle dataset and feature set sizes
that remain far beyond the reach of these algorithms.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01552" title="Abstract">arXiv:2310.01552</a> [<a href="/pdf/2310.01552" title="Download PDF">pdf</a>, <a href="/format/2310.01552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Ancillary Services: From Grid Codes to Transfer Function-Based  Converter Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=H%C3%A4berle%2C+V">Verena H&#xe4;berle</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+L">Linbin Huang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/eess?searchtype=author&query=Prieto-Araujo%2C+E">Eduardo Prieto-Araujo</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Conventional grid-code specifications for dynamic ancillary services
provision such as fast frequency and voltage regulation are typically defined
by means of piece-wise linear step-response capability curves in the time
domain. However, although the specification of such time-domain curves is
straightforward, their practical implementation in a converter-based generation
system is not immediate, and no customary methods have been developed yet. In
this paper, we thus propose a systematic approach for the practical
implementation of piece-wise linear time-domain curves to provide dynamic
ancillary services by converter-based generation systems, while ensuring
grid-code and device-level requirements to be reliably satisfied. Namely, we
translate the piece-wise linear time-domain curves for active and reactive
power provision in response to a frequency and voltage step change into a
desired rational parametric transfer function in the frequency domain, which
defines a dynamic response behavior to be realized by the converter. The
obtained transfer function can be easily implemented e.g. via a PI-based
matching control in the power loop of standard converter control architectures.
We demonstrate the performance of our method in numerical grid-code compliance
tests, and reveal its superiority over classical droop and virtual inertia
schemes which may not satisfy the grid codes due to their structural
limitations.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01556" title="Abstract">arXiv:2310.01556</a> [<a href="/pdf/2310.01556" title="Download PDF">pdf</a>, <a href="/ps/2310.01556" title="Download PostScript">ps</a>, <a href="/format/2310.01556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Family of Strang-type exponential splittings in the presence of  unbounded and time dependent operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=del+Valle%2C+J+C">Juan Carlos del Valle</a>, 
<a href="/search/math?searchtype=author&query=Kropielnicka%2C+K">Karolina Kropielnicka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a new and straightforward derivation of a family
$\mathcal{F}(h,\tau)$ of exponential splittings of Strang-type for the general
linear evolutionary equation with two linear components. One component is
assumed to be a time-independent, unbounded operator, while the other is a
bounded one with explicit time dependence. The family $\mathcal{F}(h,\tau)$ is
characterized by the length of the time-step $h$ and a continuous parameter
$\tau$, which defines each member of the family. It is shown that the
derivation and error analysis follows from two elementary arguments: the
variation of constants formula and specific quadratures for integrals over
simplices. For these Strang-type splittings, we prove their convergence which,
depending on some commutators of the relevant operators, may be of first or
second order. As a result, error bounds appear in terms of commutator bounds.
Based on the explicit form of the error terms, we establish the influence of
$\tau$ on the accuracy of $\mathcal{F}(h,\tau)$, allowing us to investigate the
optimal value of $\tau$. This simple yet powerful approach establishes the
connection between exponential integrators and splitting methods. Furthermore,
the present approach can be easily applied to the derivation of higher-order
splitting methods under similar considerations. Needless to say, the obtained
results also apply to Strang-type splittings in the case of time
independent-operators. To complement rigorous results, we present numerical
experiments with various values of $\tau$ based on the linear Schr\"odinger
equation.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01557" title="Abstract">arXiv:2310.01557</a> [<a href="/pdf/2310.01557" title="Download PDF">pdf</a>, <a href="/format/2310.01557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SmartPlay : A Benchmark for LLMs as Intelligent Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+T+M">Tom M. Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent large language models (LLMs) have demonstrated great potential toward
intelligent agents and next-gen automation, but there currently lacks a
systematic benchmark for evaluating LLMs' abilities as agents. We introduce
SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs
as agents. SmartPlay consists of 6 different games, including
Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique
setting, providing up to 20 evaluation settings and infinite environment
variations. Each game in SmartPlay uniquely challenges a subset of 9 important
capabilities of an intelligent LLM agent, including reasoning with object
dependencies, planning ahead, spatial reasoning, learning from history, and
understanding randomness. The distinction between the set of capabilities each
game test allows us to analyze each capability separately. SmartPlay serves not
only as a rigorous testing ground for evaluating the overall performance of LLM
agents but also as a road-map for identifying gaps in current methodologies. We
release our benchmark at github.com/LLMsmartplay/SmartPlay
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01558" title="Abstract">arXiv:2310.01558</a> [<a href="/pdf/2310.01558" title="Download PDF">pdf</a>, <a href="/format/2310.01558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Retrieval-Augmented Language Models Robust to Irrelevant Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoran%2C+O">Ori Yoran</a>, 
<a href="/search/cs?searchtype=author&query=Wolfson%2C+T">Tomer Wolfson</a>, 
<a href="/search/cs?searchtype=author&query=Ram%2C+O">Ori Ram</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Retrieval-augmented language models (RALMs) hold promise to produce language
understanding systems that are are factual, efficient, and up-to-date. An
important desideratum of RALMs, is that retrieved information helps model
performance when it is relevant, and does not harm performance when it is not.
This is particularly important in multi-hop reasoning scenarios, where misuse
of irrelevant evidence can lead to cascading errors. However, recent work has
shown that retrieval augmentation can sometimes have a negative effect on
performance. In this work, we present a thorough analysis on five open-domain
question answering benchmarks, characterizing cases when retrieval reduces
accuracy. We then propose two methods to mitigate this issue. First, a simple
baseline that filters out retrieved passages that do not entail question-answer
pairs according to a natural language inference (NLI) model. This is effective
in preventing performance reduction, but at a cost of also discarding relevant
passages. Thus, we propose a method for automatically generating data to
fine-tune the language model to properly leverage retrieved passages, using a
mix of relevant and irrelevant contexts at training time. We empirically show
that even 1,000 examples suffice to train the model to be robust to irrelevant
contexts while maintaining high performance on examples with relevant ones.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01565" title="Abstract">arXiv:2310.01565</a> [<a href="/pdf/2310.01565" title="Download PDF">pdf</a>, <a href="/format/2310.01565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causality-informed Rapid Post-hurricane Building Damage Detection in  Large Scale from InSAR Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yepeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaojian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuechun Li</a>, 
<a href="/search/cs?searchtype=author&query=Paramygin%2C+V">Vladimir Paramygin</a>, 
<a href="/search/cs?searchtype=author&query=Subgranon%2C+A">Arthriya Subgranon</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+P">Peter Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xilei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Susu Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Timely and accurate assessment of hurricane-induced building damage is
crucial for effective post-hurricane response and recovery efforts. Recently,
remote sensing technologies provide large-scale optical or Interferometric
Synthetic Aperture Radar (InSAR) imagery data immediately after a disastrous
event, which can be readily used to conduct rapid building damage assessment.
Compared to optical satellite imageries, the Synthetic Aperture Radar can
penetrate cloud cover and provide more complete spatial coverage of damaged
zones in various weather conditions. However, these InSAR imageries often
contain highly noisy and mixed signals induced by co-occurring or co-located
building damage, flood, flood/wind-induced vegetation changes, as well as
anthropogenic activities, making it challenging to extract accurate building
damage information. In this paper, we introduced an approach for rapid
post-hurricane building damage detection from InSAR imagery. This approach
encoded complex causal dependencies among wind, flood, building damage, and
InSAR imagery using a holistic causal Bayesian network. Based on the causal
Bayesian network, we further jointly inferred the large-scale unobserved
building damage by fusing the information from InSAR imagery with prior
physical models of flood and wind, without the need for ground truth labels.
Furthermore, we validated our estimation results in a real-world devastating
hurricane -- the 2022 Hurricane Ian. We gathered and annotated building damage
ground truth data in Lee County, Florida, and compared the introduced method's
estimation results with the ground truth and benchmarked it against
state-of-the-art models to assess the effectiveness of our proposed method.
Results show that our method achieves rapid and accurate detection of building
damage, with significantly reduced processing time compared to traditional
manual inspection methods.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01566" title="Abstract">arXiv:2310.01566</a> [<a href="/pdf/2310.01566" title="Download PDF">pdf</a>, <a href="/format/2310.01566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do Software Engineering Researchers Use GitHub? An Empirical Study  of Artifacts &amp; Impact
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alrashedy%2C+K">Kamel Alrashedy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Millions of developers share their code on open-source platforms like GitHub,
which offer social coding opportunities such as distributed collaboration and
popularity-based ranking. Software engineering researchers have joined in as
well, hosting their research artifacts (tools, replication package &amp; datasets)
in repositories, an action often marked as part of the publications
contribution. Yet a decade after the first such paper-with-GitHub-link, little
is known about the fate of such repositories in practice. Do research
repositories ever gain the interest of the developer community, or other
researchers? If so, how often and why (not)? Does effort invested on GitHub pay
off with research impact? In short: we ask whether and how authors engage in
social coding related to their research. We conduct a broad empirical
investigation of repositories from published work, starting with ten thousand
papers in top SE research venues, hand-annotating their 3449 GitHub (and
Zenodo) links, and studying 309 paper-related repositories in detail. We find a
wide distribution in popularity and impact, some strongly correlated with
publication venue. These were often heavily informed by the authors investment
in terms of timely responsiveness and upkeep, which was often remarkably subpar
by GitHubs standards, if not absent altogether. Yet we also offer hope: popular
repositories often go hand-in-hand with well-citepd papers and achieve broad
impact. Our findings suggest the need to rethink the research incentives and
reward structure around research products requiring such sustained
contributions.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01568" title="Abstract">arXiv:2310.01568</a> [<a href="/pdf/2310.01568" title="Download PDF">pdf</a>, <a href="/format/2310.01568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Against Authorship Identification Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haining Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Authorship identification has proven unsettlingly effective in inferring the
identity of the author of an unsigned document, even when sensitive personal
information has been carefully omitted. In the digital era, individuals leave a
lasting digital footprint through their written content, whether it is posted
on social media, stored on their employer's computers, or located elsewhere.
When individuals need to communicate publicly yet wish to remain anonymous,
there is little available to protect them from unwanted authorship
identification. This unprecedented threat to privacy is evident in scenarios
such as whistle-blowing. Proposed defenses against authorship identification
attacks primarily aim to obfuscate one's writing style, thereby making it
unlinkable to their pre-existing writing, while concurrently preserving the
original meaning and grammatical integrity. The presented work offers a
comprehensive review of the advancements in this research area spanning over
the past two decades and beyond. It emphasizes the methodological frameworks of
modification and generation-based strategies devised to evade authorship
identification attacks, highlighting joint efforts from the differential
privacy community. Limitations of current research are discussed, with a
spotlight on open challenges and potential research avenues.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01569" title="Abstract">arXiv:2310.01569</a> [<a href="/pdf/2310.01569" title="Download PDF">pdf</a>, <a href="/format/2310.01569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Option Discovery for Planning, by Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Young%2C+K">Kenny Young</a>, 
<a href="/search/cs?searchtype=author&query=Sutton%2C+R+S">Richard S. Sutton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Discovering useful temporal abstractions, in the form of options, is widely
thought to be key to applying reinforcement learning and planning to
increasingly complex domains. Building on the empirical success of the Expert
Iteration approach to policy learning used in AlphaZero, we propose Option
Iteration, an analogous approach to option discovery. Rather than learning a
single strong policy that is trained to match the search results everywhere,
Option Iteration learns a set of option policies trained such that for each
state encountered, at least one policy in the set matches the search results
for some horizon into the future. Intuitively, this may be significantly easier
as it allows the algorithm to hedge its bets compared to learning a single
globally strong policy, which may have complex dependencies on the details of
the current state. Having learned such a set of locally strong policies, we can
use them to guide the search algorithm resulting in a virtuous cycle where
better options lead to better search results which allows for training of
better options. We demonstrate experimentally that planning using options
learned with Option Iteration leads to a significant benefit in challenging
planning environments compared to an analogous planning algorithm operating in
the space of primitive actions and learning a single rollout policy with Expert
Iteration.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01571" title="Abstract">arXiv:2310.01571</a> [<a href="/pdf/2310.01571" title="Download PDF">pdf</a>, <a href="/format/2310.01571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contraction Properties of the Global Workspace Primitive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ennis%2C+M">Michaela Ennis</a>, 
<a href="/search/cs?searchtype=author&query=Kozachkov%2C+L">Leo Kozachkov</a>, 
<a href="/search/cs?searchtype=author&query=Slotine%2C+J">Jean-Jacques Slotine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">To push forward the important emerging research field surrounding multi-area
recurrent neural networks (RNNs), we expand theoretically and empirically on
the provably stable RNNs of RNNs introduced by Kozachkov et al. in "RNNs of
RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural
Networks". We prove relaxed stability conditions for salient special cases of
this architecture, most notably for a global workspace modular structure. We
then demonstrate empirical success for Global Workspace Sparse Combo Nets with
a small number of trainable parameters, not only through strong overall test
performance but also greater resilience to removal of individual subnetworks.
These empirical results for the global workspace inter-area topology are
contingent on stability preservation, highlighting the relevance of our
theoretical work for enabling modular RNN success. Further, by exploring
sparsity in the connectivity structure between different subnetwork modules
more broadly, we improve the state of the art performance for stable RNNs on
benchmark sequence processing tasks, thus underscoring the general utility of
specialized graph structures for multi-area RNNs.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01573" title="Abstract">arXiv:2310.01573</a> [<a href="/pdf/2310.01573" title="Download PDF">pdf</a>, <a href="/format/2310.01573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Platform for Swarm Robotics: Experiments and High-Dimensional  Continuification Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maffettone%2C+G+C">Gian Carlo Maffettone</a>, 
<a href="/search/cs?searchtype=author&query=Liguori%2C+L">Lorenzo Liguori</a>, 
<a href="/search/cs?searchtype=author&query=Palermo%2C+E">Eduardo Palermo</a>, 
<a href="/search/cs?searchtype=author&query=di+Bernardo%2C+M">Mario di Bernardo</a>, 
<a href="/search/cs?searchtype=author&query=Porfiri%2C+M">Maurizio Porfiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">A significant challenge in control theory and technology is to devise agile
and less resource-intensive experiments for evaluating the performance and
feasibility of control algorithms for the collective coordination of
large-scale complex systems. Many new methodologies are based on macroscopic
representations of the emerging system behavior, and can be easily validated
only through numerical simulations, because of the inherent hurdle of
developing full scale experimental platforms. In this paper, we introduce a
novel hybrid set-up for testing swarm robotics techniques, focusing on the
collective motion of robotic swarms. This hybrid apparatus combines both real
differential drive robots and virtual agents to create a heterogeneous swarm of
tunable size. We validate the methodology by extending to higher dimensions,
and investigating experimentally, continuification-based control methods for
swarms. Our study demonstrates the versatility and effectiveness of the
platform for conducting large-scale swarm robotics experiments. Also, it
contributes new theoretical insights into control algorithms exploiting
continuification approaches.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01574" title="Abstract">arXiv:2310.01574</a> [<a href="/pdf/2310.01574" title="Download PDF">pdf</a>, <a href="/ps/2310.01574" title="Download PostScript">ps</a>, <a href="/format/2310.01574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Potential Ways to Detect Unfairness in HRI and to Re-establish Positive  Group Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=der+P%C3%BCtten%2C+A+R">Astrid Rosenthal-von der P&#xfc;tten</a>, 
<a href="/search/cs?searchtype=author&query=Schiffer%2C+S">Stefan Schiffer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper focuses on the identification of different algorithm-based biases
in robotic behaviour and their consequences in human-robot mixed groups. We
propose to develop computational models to detect episodes of microaggression,
discrimination, and social exclusion informed by a) observing human coping
behaviours that are used to regain social inclusion and b) using system
inherent information that reveal unequal treatment of human interactants. Based
on this information we can start to develop regulatory mechanisms to promote
fairness and social inclusion in HRI.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01580" title="Abstract">arXiv:2310.01580</a> [<a href="/pdf/2310.01580" title="Download PDF">pdf</a>, <a href="/format/2310.01580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Learning on Neural Networks through Interactive Generation of  Digit Patterns and Visual Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+D+H">Dong H. Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jin-Hee Cho</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Josang%2C+A">Audun Josang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Soo-Yeon Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial neural networks (ANNs) have been broadly utilized to analyze
various data and solve different domain problems. However, neural networks
(NNs) have been considered a black box operation for years because their
underlying computation and meaning are hidden. Due to this nature, users often
face difficulties in interpreting the underlying mechanism of the NNs and the
benefits of using them. In this paper, to improve users' learning and
understanding of NNs, an interactive learning system is designed to create
digit patterns and recognize them in real time. To help users clearly
understand the visual differences of digit patterns (i.e., 0 ~ 9) and their
results with an NN, integrating visualization is considered to present all
digit patterns in a two-dimensional display space with supporting multiple user
interactions. An evaluation with multiple datasets is conducted to determine
its usability for active learning. In addition, informal user testing is
managed during a summer workshop by asking the workshop participants to use the
system.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01581" title="Abstract">arXiv:2310.01581</a> [<a href="/pdf/2310.01581" title="Download PDF">pdf</a>, <a href="/format/2310.01581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Safety of Open-Sourced Large Language Models: Does Alignment  Really Prevent Them From Being Misused?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hangfan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhimeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huaisheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+B">Bochuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinyuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dinghao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved unprecedented performance in
Natural Language Generation (NLG) tasks. However, many existing studies have
shown that they could be misused to generate undesired content. In response,
before releasing LLMs for public access, model developers usually align those
language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning
with Human Feedback (RLHF). Consequently, those aligned large language models
refuse to generate undesired content when facing potentially harmful/unethical
requests. A natural question is "could alignment really prevent those
open-sourced large language models from being misused to generate undesired
content?''. In this work, we provide a negative answer to this question. In
particular, we show those open-sourced, aligned large language models could be
easily misguided to generate undesired content without heavy computations or
careful prompt designs. Our key idea is to directly manipulate the generation
process of open-sourced LLMs to misguide it to generate undesired content
including harmful or biased information and even private data. We evaluate our
method on 4 open-sourced LLMs accessible publicly and our finding highlights
the need for more advanced mitigation strategies for open-sourced LLMs.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01586" title="Abstract">arXiv:2310.01586</a> [<a href="/pdf/2310.01586" title="Download PDF">pdf</a>, <a href="/format/2310.01586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experiences Readying Applications for Exascale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bauman%2C+P+T">Paul T. Bauman</a>, 
<a href="/search/cs?searchtype=author&query=Budiardja%2C+R+D">Reuben D. Budiardja</a>, 
<a href="/search/cs?searchtype=author&query=Bykov%2C+D">Dmytro Bykov</a>, 
<a href="/search/cs?searchtype=author&query=Chalmers%2C+N">Noel Chalmers</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jacqueline Chen</a>, 
<a href="/search/cs?searchtype=author&query=Curtis%2C+N">Nicholas Curtis</a>, 
<a href="/search/cs?searchtype=author&query=Day%2C+M">Marc Day</a>, 
<a href="/search/cs?searchtype=author&query=Eisenbach%2C+M">Markus Eisenbach</a>, 
<a href="/search/cs?searchtype=author&query=Esclapez%2C+L">Lucas Esclapez</a>, 
<a href="/search/cs?searchtype=author&query=Fanfarillo%2C+A">Alessandro Fanfarillo</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+W">William Freitag</a>, 
<a href="/search/cs?searchtype=author&query=Frontiere%2C+N">Nicholas Frontiere</a>, 
<a href="/search/cs?searchtype=author&query=Georgiadou%2C+A">Antigoni Georgiadou</a>, 
<a href="/search/cs?searchtype=author&query=Glenski%2C+J">Joseph Glenski</a>, 
<a href="/search/cs?searchtype=author&query=Gottiparthi%2C+K">Kalyana Gottiparthi</a>, 
<a href="/search/cs?searchtype=author&query=de+Frahan%2C+M+T+H">Marc T. Henry de Frahan</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+G+R">Gustav R. Jansen</a>, 
<a href="/search/cs?searchtype=author&query=Joubert%2C+W">Wayne Joubert</a>, 
<a href="/search/cs?searchtype=author&query=Lietz%2C+J+G">Justin G. Lietz</a>, 
<a href="/search/cs?searchtype=author&query=Kurzak%2C+J">Jakub Kurzak</a>, 
<a href="/search/cs?searchtype=author&query=Malaya%2C+N">Nicholas Malaya</a>, 
<a href="/search/cs?searchtype=author&query=Messer%2C+B">Bronson Messer</a>, 
<a href="/search/cs?searchtype=author&query=McDougall%2C+D">Damon McDougall</a>, 
<a href="/search/cs?searchtype=author&query=Mullowney%2C+P">Paul Mullowney</a>, 
<a href="/search/cs?searchtype=author&query=Nichols%2C+S">Stephen Nichols</a>, 
<a href="/search/cs?searchtype=author&query=Norman%2C+M">Matthew Norman</a>, 
<a href="/search/cs?searchtype=author&query=Papatheodore%2C+T">Thomas Papatheodore</a>, 
<a href="/search/cs?searchtype=author&query=Rood%2C+J">Jon Rood</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+P+C">Philip C. Roth</a>, 
<a href="/search/cs?searchtype=author&query=Sreepathi%2C+S">Sarat Sreepathi</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+J">James White III</a>, 
<a href="/search/cs?searchtype=author&query=Wolfe%2C+N">Noah Wolfe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at SC23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The advent of exascale computing invites an assessment of existing best
practices for developing application readiness on the world's largest
supercomputers. This work details observations from the last four years in
preparing scientific applications to run on the Oak Ridge Leadership Computing
Facility's (OLCF) Frontier system. This paper addresses a range of topics in
software including programmability, tuning, and portability considerations that
are key to moving applications from existing systems to future installations. A
set of representative workloads provides case studies for general system and
software testing. We evaluate the use of early access systems for development
across several generations of hardware. Finally, we discuss how best practices
were identified and disseminated to the community through a wide range of
activities including user-guides and trainings. We conclude with
recommendations for ensuring application readiness on future leadership
computing systems.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01587" title="Abstract">arXiv:2310.01587</a> [<a href="/pdf/2310.01587" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theory of CHTW-systems. Born by Petri Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chunikhin%2C+A+Y">Alexander Yu. Chunikhin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">For the first time, the concept of CHTW-systems as a multidimensional
representation of Petri nets, based on the assumption of the spatial
distribution of tokens (resources) in positions (branes) and, accordingly, the
spatial representation of transitions and arcs is proposed. The theoretical
constructs are based on the concept of hybrid functional Petri nets [10]. The
introduced concepts of branes and carriers are distant analogies of the
corresponding concepts in superstring theory, but the theory of CHTW-systems is
neither part of superstring theory nor its development. The description of
CHTW-system as a dynamic system for stationary and non-stationary cases is
considered. The initial classification of CHTW systems is provided enabling
understanding of further research directions.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01593" title="Abstract">arXiv:2310.01593</a> [<a href="/pdf/2310.01593" title="Download PDF">pdf</a>, <a href="/format/2310.01593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prescribed Fire Modeling using Knowledge-Guided Machine Learning for  Land Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S+S">Somya Sharma Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Lindsay%2C+K">Kelly Lindsay</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+N">Neel Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+R">Rohan Patil</a>, 
<a href="/search/cs?searchtype=author&query=De+Callafon%2C+I+A">Ilkay Altintas De Callafon</a>, 
<a href="/search/cs?searchtype=author&query=Steinbach%2C+M">Michael Steinbach</a>, 
<a href="/search/cs?searchtype=author&query=Giron%2C+D">Daniel Giron</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M+H">Mai H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vipin Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applications (stat.AP)

</div>
<p class="mathjax">In recent years, the increasing threat of devastating wildfires has
underscored the need for effective prescribed fire management. Process-based
computer simulations have traditionally been employed to plan prescribed fires
for wildfire prevention. However, even simplified process models like QUIC-Fire
are too compute-intensive to be used for real-time decision-making, especially
when weather conditions change rapidly. Traditional ML methods used for fire
modeling offer computational speedup but struggle with physically inconsistent
predictions, biased predictions due to class imbalance, biased estimates for
fire spread metrics (e.g., burned area, rate of spread), and generalizability
in out-of-distribution wind conditions. This paper introduces a novel machine
learning (ML) framework that enables rapid emulation of prescribed fires while
addressing these concerns. By incorporating domain knowledge, the proposed
method helps reduce physical inconsistencies in fuel density estimates in
data-scarce scenarios. To overcome the majority class bias in predictions, we
leverage pre-existing source domain data to augment training data and learn the
spread of fire more effectively. Finally, we overcome the problem of biased
estimation of fire spread metrics by incorporating a hierarchical modeling
structure to capture the interdependence in fuel density and burned area.
Notably, improvement in fire metric (e.g., burned area) estimates offered by
our framework makes it useful for fire managers, who often rely on these fire
metric estimates to make decisions about prescribed burn management.
Furthermore, our framework exhibits better generalization capabilities than the
other ML-based fire modeling methods across diverse wind conditions and
ignition patterns.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01594" title="Abstract">arXiv:2310.01594</a> [<a href="/pdf/2310.01594" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Transmissions in Blockchain enabled AGVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shile Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuchen Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Automated Guided Vehicles (AGVs) operate in synergy to execute specific
tasks. These vehicles exchange information to ensure seamless collaboration,
prevent collisions, and eliminate task redundancy. The advent of blockchain
technology offers a promising avenue for establishing a secure and dependable
communication infrastructure for AGVs. Nonetheless, it becomes imperative for
AGVs to adopt efficient data transmission methodologies, especially when
interacting with the dynamic nature of blockchain infrastructure where data
undergoes frequent modifications.
<br />In the present study, we introduce a novel data transmission methodology
tailored for blockchain-integrated AGVs utilizing the principles of Named Data
Networking (NDN). A simulated environment was crafted and executed in NetSim,
wherein multiple AGVs collaboratively endeavored to locate concealed objectives
within a defined region. Upon discovery of novel elements, such as obstructions
or concealed objectives, each AGV would update a collective blockchain
repository. This blockchain infrastructure leverages NDN to fetch specific data
blocks in response to data queries from individual AGVs. This system ensures
that AGVs can navigate and scrutinize their environment with heightened
efficiency, drawing upon the collective intelligence and shared experiences of
the fleet.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01595" title="Abstract">arXiv:2310.01595</a> [<a href="/pdf/2310.01595" title="Download PDF">pdf</a>, <a href="/format/2310.01595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient particle filter recurrent neural network for object  localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korkin%2C+R">Roman Korkin</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>, 
<a href="/search/cs?searchtype=author&query=Katrutsa%2C+A">Aleksandr Katrutsa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study proposes a novel memory-efficient recurrent neural network (RNN)
architecture specified to solve the object localization problem. This problem
is to recover the object states along with its movement in a noisy environment.
We take the idea of the classical particle filter and combine it with GRU RNN
architecture. The key feature of the resulting memory-efficient particle filter
RNN model (mePFRNN) is that it requires the same number of parameters to
process environments of different sizes. Thus, the proposed mePFRNN
architecture consumes less memory to store parameters compared to the
previously proposed PFRNN model. To demonstrate the performance of our model,
we test it on symmetric and noisy environments that are incredibly challenging
for filtering algorithms. In our experiments, the mePFRNN model provides more
precise localization than the considered competitors and requires fewer trained
parameters.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01596" title="Abstract">arXiv:2310.01596</a> [<a href="/pdf/2310.01596" title="Download PDF">pdf</a>, <a href="/format/2310.01596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ImagenHub: Standardizing the evaluation of conditional image generation  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ku%2C+M">Max Ku</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianle Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yujie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xingyu Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+W">Wenwen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Multimedia (cs.MM)

</div>
<p class="mathjax">Recently, a myriad of conditional image generation and editing models have
been developed to serve different downstream tasks, including text-to-image
generation, text-guided image editing, subject-driven image generation,
control-guided image generation, etc. However, we observe huge inconsistencies
in experimental conditions: datasets, inference, and evaluation metrics -
render fair comparisons difficult. This paper proposes ImagenHub, which is a
one-stop library to standardize the inference and evaluation of all the
conditional image generation models. Firstly, we define seven prominent tasks
and curate high-quality evaluation datasets for them. Secondly, we built a
unified inference pipeline to ensure fair comparison. Thirdly, we design two
human evaluation scores, i.e. Semantic Consistency and Perceptual Quality,
along with comprehensive guidelines to evaluate generated images. We train
expert raters to evaluate the model outputs based on the proposed metrics. Our
human evaluation achieves a high inter-worker agreement of Krippendorff's alpha
on 76% models with a value higher than 0.4. We comprehensively evaluated a
total of around 30 models and observed three key takeaways: (1) the existing
models' performance is generally unsatisfying except for Text-guided Image
Generation and Subject-driven Image Generation, with 74% models achieving an
overall score lower than 0.5. (2) we examined the claims from published papers
and found 83% of them hold with a few exceptions. (3) None of the existing
automatic metrics has a Spearman's correlation higher than 0.2 except
subject-driven image generation. Moving forward, we will continue our efforts
to evaluate newly published models and update our leaderboard to keep track of
the progress in conditional image generation.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01597" title="Abstract">arXiv:2310.01597</a> [<a href="/pdf/2310.01597" title="Download PDF">pdf</a>, <a href="/format/2310.01597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pool-Based Active Learning with Proper Topological Regions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hadjadj%2C+L">Lies Hadjadj</a>, 
<a href="/search/cs?searchtype=author&query=Devijver%2C+E">Emilie Devijver</a>, 
<a href="/search/cs?searchtype=author&query=Molinier%2C+R">Remi Molinier</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+M">Massih-Reza Amini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Algebraic Topology (math.AT)

</div>
<p class="mathjax">Machine learning methods usually rely on large sample size to have good
performance, while it is difficult to provide labeled set in many applications.
Pool-based active learning methods are there to detect, among a set of
unlabeled data, the ones that are the most relevant for the training. We
propose in this paper a meta-approach for pool-based active learning strategies
in the context of multi-class classification tasks based on Proper Topological
Regions. PTR, based on topological data analysis (TDA), are relevant regions
used to sample cold-start points or within the active learning scheme. The
proposed method is illustrated empirically on various benchmark datasets, being
competitive to the classical methods from the literature.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01602" title="Abstract">arXiv:2310.01602</a> [<a href="/pdf/2310.01602" title="Download PDF">pdf</a>, <a href="/format/2310.01602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAT-LM: Training Language Models on Aligned Code And Tests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+N">Nikitha Rao</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+K">Kush Jain</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>, 
<a href="/search/cs?searchtype=author&query=Goues%2C+C+L">Claire Le Goues</a>, 
<a href="/search/cs?searchtype=author&query=Hellendoorn%2C+V+J">Vincent J. Hellendoorn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Testing is an integral part of the software development process. Yet, writing
tests is time-consuming and therefore often neglected. Classical test
generation tools such as EvoSuite generate behavioral test suites by optimizing
for coverage, but tend to produce tests that are hard to understand. Language
models trained on code can generate code that is highly similar to that written
by humans, but current models are trained to generate each file separately, as
is standard practice in natural language processing, and thus fail to consider
the code-under-test context when producing a test file. In this work, we
propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style
language model with 2.7 Billion parameters, trained on a corpus of Python and
Java projects. We utilize a novel pretraining signal that explicitly considers
the mapping between code and test files when available. We also drastically
increase the maximum sequence length of inputs to 8,192 tokens, 4x more than
typical code generation models, to ensure that the code context is available to
the model when generating test code. We analyze its usefulness for realistic
applications, showing that sampling with filtering (e.g., by compilability,
coverage) allows it to efficiently produce tests that achieve coverage similar
to ones written by developers while resembling their writing style. By
utilizing the code context, CAT-LM generates more valid tests than even much
larger language models trained with more data (CodeGen 16B and StarCoder) and
substantially outperforms a recent test-specific model (TeCo) at test
completion. Overall, our work highlights the importance of incorporating
software-specific insights when training language models for code and paves the
way to more powerful automated test generation.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01603" title="Abstract">arXiv:2310.01603</a> [<a href="/pdf/2310.01603" title="Download PDF">pdf</a>, <a href="/format/2310.01603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Digital Learning Environments for Teaching Natural Language  Processing in K-12 Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xiaoyi Tian</a>, 
<a href="/search/cs?searchtype=author&query=Boyer%2C+K+E">Kristy Elizabeth Boyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Natural Language Processing (NLP) plays a significant role in our daily lives
and has become an essential part of Artificial Intelligence (AI) education in
K-12. As children grow up with NLP-powered applications, it is crucial to
introduce NLP concepts to them, fostering their understanding of language
processing, language generation, and ethical implications of AI and NLP. This
paper presents a comprehensive review of digital learning environments for
teaching NLP in K-12. Specifically, it explores existing digital learning
tools, discusses how they support specific NLP tasks and procedures, and
investigates their explainability and evaluation results in educational
contexts. By examining the strengths and limitations of these tools, this
literature review sheds light on the current state of NLP learning tools in
K-12 education. It aims to guide future research efforts to refine existing
tools, develop new ones, and explore more effective and inclusive strategies
for integrating NLP into K-12 educational contexts.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01604" title="Abstract">arXiv:2310.01604</a> [<a href="/pdf/2310.01604" title="Download PDF">pdf</a>, <a href="/format/2310.01604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving the Quadratic Assignment Problem using Deep Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bagga%2C+P+S">Puneet S. Bagga</a>, 
<a href="/search/cs?searchtype=author&query=Delarue%2C+A">Arthur Delarue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">The Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven
particularly challenging to solve: unlike other combinatorial problems like the
traveling salesman problem (TSP), which can be solved to optimality for
instances with hundreds or even thousands of locations using advanced integer
programming techniques, no methods are known to exactly solve QAP instances of
size greater than 30. Solving the QAP is nevertheless important because of its
many critical applications, such as electronic wiring design and facility
layout selection. We propose a method to solve the original Koopmans-Beckman
formulation of the QAP using deep reinforcement learning. Our approach relies
on a novel double pointer network, which alternates between selecting a
location in which to place the next facility and a facility to place in the
previous location. We train our model using A2C on a large dataset of synthetic
instances, producing solutions with no instance-specific retraining necessary.
Out of sample, our solutions are on average within 7.5% of a high-quality local
search baseline, and even outperform it on 1.2% of instances.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01605" title="Abstract">arXiv:2310.01605</a> [<a href="/pdf/2310.01605" title="Download PDF">pdf</a>, <a href="/format/2310.01605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Primal-dual hybrid gradient algorithms for computing time-implicit  Hamilton-Jacobi equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Meng%2C+T">Tingwei Meng</a>, 
<a href="/search/math?searchtype=author&query=Hao%2C+W">Wenbo Hao</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+S">Siting Liu</a>, 
<a href="/search/math?searchtype=author&query=Osher%2C+S+J">Stanley J. Osher</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wuchen Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Hamilton-Jacobi (HJ) partial differential equations (PDEs) have diverse
applications spanning physics, optimal control, game theory, and imaging
sciences. This research introduces a first-order optimization-based technique
for HJ PDEs, which formulates the time-implicit update of HJ PDEs as saddle
point problems. We remark that the saddle point formulation for HJ equations is
aligned with the primal-dual formulation of optimal transport and potential
mean-field games (MFGs). This connection enables us to extend MFG techniques
and design numerical schemes for solving HJ PDEs. We employ the primal-dual
hybrid gradient (PDHG) method to solve the saddle point problems, benefiting
from the simple structures that enable fast computations in updates.
Remarkably, the method caters to a broader range of Hamiltonians, encompassing
non-smooth and spatiotemporally dependent cases. The approach's effectiveness
is verified through various numerical examples in both one-dimensional and
two-dimensional examples, such as quadratic and $L^1$ Hamiltonians with spatial
and time dependence.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01611" title="Abstract">arXiv:2310.01611</a> [<a href="/pdf/2310.01611" title="Download PDF">pdf</a>, <a href="/format/2310.01611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intractability of Learning the Discrete Logarithm with Gradient-Based  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takhanov%2C+R">Rustem Takhanov</a>, 
<a href="/search/cs?searchtype=author&query=Tezekbayev%2C+M">Maxat Tezekbayev</a>, 
<a href="/search/cs?searchtype=author&query=Pak%2C+A">Artur Pak</a>, 
<a href="/search/cs?searchtype=author&query=Bolatov%2C+A">Arman Bolatov</a>, 
<a href="/search/cs?searchtype=author&query=Kadyrsizova%2C+Z">Zhibek Kadyrsizova</a>, 
<a href="/search/cs?searchtype=author&query=Assylbekov%2C+Z">Zhenisbek Assylbekov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The discrete logarithm problem is a fundamental challenge in number theory
with significant implications for cryptographic protocols. In this paper, we
investigate the limitations of gradient-based methods for learning the parity
bit of the discrete logarithm in finite cyclic groups of prime order. Our main
result, supported by theoretical analysis and empirical verification, reveals
the concentration of the gradient of the loss function around a fixed point,
independent of the logarithm's base used. This concentration property leads to
a restricted ability to learn the parity bit efficiently using gradient-based
methods, irrespective of the complexity of the network architecture being
trained.
<br />Our proof relies on Boas-Bellman inequality in inner product spaces and it
involves establishing approximate orthogonality of discrete logarithm's parity
bit functions through the spectral norm of certain matrices. Empirical
experiments using a neural network-based approach further verify the
limitations of gradient-based learning, demonstrating the decreasing success
rate in predicting the parity bit as the group order increases.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01612" title="Abstract">arXiv:2310.01612</a> [<a href="/pdf/2310.01612" title="Download PDF">pdf</a>, <a href="/format/2310.01612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Efficient and Effective Adaptation of Large Language Models for  Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Burns%2C+B">Ben Burns</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathy%2C+S">Srinivasan Parthasarathy</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xia Ning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In recent years, with large language models (LLMs) achieving state-of-the-art
performance in context understanding, increasing efforts have been dedicated to
developing LLM-enhanced sequential recommendation (SR) methods. Considering
that most existing LLMs are not specifically optimized for recommendation
tasks, adapting them for SR becomes a critical step in LLM-enhanced SR methods.
Though numerous adaptation methods have been developed, it still remains a
significant challenge to adapt LLMs for SR both efficiently and effectively. To
address this challenge, in this paper, we introduce a novel side sequential
network adaptation method, denoted as SSNA, for LLM enhanced SR. SSNA features
three key designs to allow both efficient and effective LLM adaptation. First,
SSNA learns adapters separate from LLMs, while fixing all the pre-trained
parameters within LLMs to allow efficient adaptation. In addition, SSNA adapts
the top-a layers of LLMs jointly, and integrates adapters sequentially for
enhanced effectiveness (i.e., recommendation performance). We compare SSNA
against five state-of-the-art baseline methods on five benchmark datasets using
three LLMs. The experimental results demonstrate that SSNA significantly
outperforms all the baseline methods in terms of recommendation performance,
and achieves substantial improvement over the best-performing baseline methods
at both run-time and memory efficiency during training. Our analysis shows the
effectiveness of integrating adapters in a sequential manner. Our parameter
study demonstrates the effectiveness of jointly adapting the top-a layers of
LLMs.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01614" title="Abstract">arXiv:2310.01614</a> [<a href="/pdf/2310.01614" title="Download PDF">pdf</a>, <a href="/format/2310.01614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Multi-agent Interaction Generation with Imagined Potential  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingfeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pin-Yun Hung</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhuo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Interactive behavior modeling of multiple agents is an essential challenge in
simulation, especially in scenarios when agents need to avoid collisions and
cooperate at the same time. Humans can interact with others without explicit
communication and navigate in scenarios when cooperation is required. In this
work, we aim to model human interactions in this realistic setting, where each
agent acts based on its observation and does not communicate with others. We
propose a framework based on distributed potential games, where each agent
imagines a cooperative game with other agents and solves the game using its
estimation of their behavior. We utilize iLQR to solve the games and
closed-loop simulate the interactions. We demonstrate the benefits of utilizing
distributed imagined games in our framework through various simulation
experiments. We show the high success rate, the increased navigation
efficiency, and the ability to generate rich and realistic interactions with
interpretable parameters. Illustrative examples are available at
https://sites.google.com/berkeley.edu/distributed-interaction.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01616" title="Abstract">arXiv:2310.01616</a> [<a href="/pdf/2310.01616" title="Download PDF">pdf</a>, <a href="/format/2310.01616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for  Dimension-Dependent Adaptivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+E">Emmeran Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Pike-Burke%2C+C">Ciara Pike-Burke</a>, 
<a href="/search/cs?searchtype=author&query=Rebeschini%2C+P">Patrick Rebeschini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We theoretically explore the relationship between sample-efficiency and
adaptivity in reinforcement learning. An algorithm is sample-efficient if it
uses a number of queries $n$ to the environment that is polynomial in the
dimension $d$ of the problem. Adaptivity refers to the frequency at which
queries are sent and feedback is processed to update the querying strategy. To
investigate this interplay, we employ a learning framework that allows sending
queries in $K$ batches, with feedback being processed and queries updated after
each batch. This model encompasses the whole adaptivity spectrum, ranging from
non-adaptive 'offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes
in between. For the problems of policy evaluation and best-policy
identification under $d$-dimensional linear function approximation, we
establish $\Omega(\log \log d)$ lower bounds on the number of batches $K$
required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our
results show that just having adaptivity ($K&gt;1$) does not necessarily guarantee
sample-efficiency. Notably, the adaptivity-boundary for sample-efficiency is
not between offline reinforcement learning ($K=1$), where sample-efficiency was
known to not be possible, and adaptive settings. Instead, the boundary lies
between different regimes of adaptivity and depends on the problem dimension.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01617" title="Abstract">arXiv:2310.01617</a> [<a href="/pdf/2310.01617" title="Download PDF">pdf</a>, <a href="/format/2310.01617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Spatio-Temporal Summarization using Information Based Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tasnim%2C+H">Humayra Tasnim</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Soumya Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Moses%2C+M">Melanie Moses</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In the era of burgeoning data generation, managing and storing large-scale
time-varying datasets poses significant challenges. With the rise of
supercomputing capabilities, the volume of data produced has soared,
intensifying storage and I/O overheads. To address this issue, we propose a
dynamic spatio-temporal data summarization technique that identifies
informative features in key timesteps and fuses less informative ones. This
approach minimizes storage requirements while preserving data dynamics. Unlike
existing methods, our method retains both raw and summarized timesteps,
ensuring a comprehensive view of information changes over time. We utilize
information-theoretic measures to guide the fusion process, resulting in a
visual representation that captures essential data patterns. We demonstrate the
versatility of our technique across diverse datasets, encompassing
particle-based flow simulations, security and surveillance applications, and
biological cell interactions within the immune system. Our research
significantly contributes to the realm of data management, introducing enhanced
efficiency and deeper insights across diverse multidisciplinary domains. We
provide a streamlined approach for handling massive datasets that can be
applied to in situ analysis as well as post hoc analysis. This not only
addresses the escalating challenges of data storage and I/O overheads but also
unlocks the potential for informed decision-making. Our method empowers
researchers and experts to explore essential temporal dynamics while minimizing
storage requirements, thereby fostering a more effective and intuitive
understanding of complex data behaviors.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01618" title="Abstract">arXiv:2310.01618</a> [<a href="/pdf/2310.01618" title="Download PDF">pdf</a>, <a href="/format/2310.01618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operator Learning Meets Numerical Analysis: Improving Neural Networks  through Iterative Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zappala%2C+E">Emanuele Zappala</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+D">Daniel Levine</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Sizhuang He</a>, 
<a href="/search/cs?searchtype=author&query=Rizvi%2C+S">Syed Rizvi</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+S">Sacha Levy</a>, 
<a href="/search/cs?searchtype=author&query=van+Dijk%2C+D">David van Dijk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages (13+14). 8 Figures and 5 tables. Comments are welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Deep neural networks, despite their success in numerous applications, often
function without established theoretical foundations. In this paper, we bridge
this gap by drawing parallels between deep learning and classical numerical
analysis. By framing neural networks as operators with fixed points
representing desired solutions, we develop a theoretical framework grounded in
iterative methods for operator equations. Under defined conditions, we present
convergence proofs based on fixed point theory. We demonstrate that popular
architectures, such as diffusion models and AlphaFold, inherently employ
iterative operator learning. Empirical assessments highlight that performing
iterations through network operators improves performance. We also introduce an
iterative graph neural network, PIGN, that further demonstrates benefits of
iterations. Our work aims to enhance the understanding of deep learning by
merging insights from numerical analysis, potentially guiding the design of
future networks with clearer theoretical underpinnings and improved
performance.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01621" title="Abstract">arXiv:2310.01621</a> [<a href="/pdf/2310.01621" title="Download PDF">pdf</a>, <a href="/ps/2310.01621" title="Download PostScript">ps</a>, <a href="/format/2310.01621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The RESET and MARC Techniques, with Application to Multiserver-Job  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grosof%2C+I">Isaac Grosof</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yige Hong</a>, 
<a href="/search/cs?searchtype=author&query=Harchol-Balter%2C+M">Mor Harchol-Balter</a>, 
<a href="/search/cs?searchtype=author&query=Scheller-Wolf%2C+A">Alan Scheller-Wolf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, IFIP Performance 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">Multiserver-job (MSJ) systems, where jobs need to run concurrently across
many servers, are increasingly common in practice. The default service ordering
in many settings is First-Come First-Served (FCFS) service. Virtually all
theoretical work on MSJ FCFS models focuses on characterizing the stability
region, with almost nothing known about mean response time.
<br />We derive the first explicit characterization of mean response time in the
MSJ FCFS system. Our formula characterizes mean response time up to an additive
constant, which becomes negligible as arrival rate approaches throughput, and
allows for general phase-type job durations.
<br />We derive our result by utilizing two key techniques: REduction to Saturated
for Expected Time (RESET) and MArkovian Relative Completions (MARC).
<br />Using our novel RESET technique, we reduce the problem of characterizing mean
response time in the MSJ FCFS system to an M/M/1 with Markovian service rate
(MMSR). The Markov chain controlling the service rate is based on the saturated
system, a simpler closed system which is far more analytically tractable.
<br />Unfortunately, the MMSR has no explicit characterization of mean response
time. We therefore use our novel MARC technique to give the first explicit
characterization of mean response time in the MMSR, again up to constant
additive error. We specifically introduce the concept of "relative
completions," which is the cornerstone of our MARC technique.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01626" title="Abstract">arXiv:2310.01626</a> [<a href="/pdf/2310.01626" title="Download PDF">pdf</a>, <a href="/ps/2310.01626" title="Download PostScript">ps</a>, <a href="/format/2310.01626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Explanation via Support Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabalar%2C+P">Pedro Cabalar</a>, 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1iz%2C+B">Brais Mu&#xf1;iz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In this note, we introduce the notion of support graph to define explanations
for any model of a logic program. An explanation is an acyclic support graph
that, for each true atom in the model, induces a proof in terms of program
rules represented by labels. A classical model may have zero, one or several
explanations: when it has at least one, it is called a justified model. We
prove that all stable models are justified whereas, in general, the opposite
does not hold, at least for disjunctive programs. We also provide a
meta-programming encoding in Answer Set Programming that generates the
explanations for a given stable model of some program. We prove that the
encoding is sound and complete, that is, there is a one-to-one correspondence
between each answer set of the encoding and each explanation for the original
stable model.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01627" title="Abstract">arXiv:2310.01627</a> [<a href="/pdf/2310.01627" title="Download PDF">pdf</a>, <a href="/format/2310.01627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VAL: Interactive Task Learning with GPT Dialog Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lawley%2C+L">Lane Lawley</a>, 
<a href="/search/cs?searchtype=author&query=MacLellan%2C+C+J">Christopher J. MacLellan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Reinforcement learning often requires millions of examples to produce static,
black-box models. In contrast, interactive task learning (ITL) emphasizes
incremental knowledge acquisition from limited instruction provided by humans
in modalities such as natural language. However, in practice, ITL systems often
suffers from brittle, error-prone language parsing. Large language models
(LLMs) are resistant to brittleness but are not interpretable and cannot learn
incrementally. We present VAL, an ITL system with a new philosophy for
LLM/symbolic integration. By using LLMs only for specific tasks -- such as
predicate and argument selection -- within an algorithmic framework, VAL reaps
the benefits of LLMs to support interactive learning of hierarchical task
knowledge from natural language. Acquired knowledge is human interpretable and
generalizes to support execution of novel tasks without additional training. We
studied users' interactions with VAL in a video game setting, finding that most
users could successfully teach VAL using language they felt was natural.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01632" title="Abstract">arXiv:2310.01632</a> [<a href="/pdf/2310.01632" title="Download PDF">pdf</a>, <a href="/format/2310.01632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imitation Learning from Observation through Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+W">Wei-Di Chang</a>, 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+S">Scott Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Meger%2C+D">David Meger</a>, 
<a href="/search/cs?searchtype=author&query=Dudek%2C+G">Gregory Dudek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Imitation Learning from Observation (ILfO) is a setting in which a learner
tries to imitate the behavior of an expert, using only observational data and
without the direct guidance of demonstrated actions. In this paper, we
re-examine the use of optimal transport for IL, in which a reward is generated
based on the Wasserstein distance between the state trajectories of the learner
and expert. We show that existing methods can be simplified to generate a
reward function without requiring learned models or adversarial learning.
Unlike many other state-of-the-art methods, our approach can be integrated with
any RL algorithm, and is amenable to ILfO. We demonstrate the effectiveness of
this simple approach on a variety of continuous control tasks and find that it
surpasses the state of the art in the IlfO setting, achieving expert-level
performance across a range of evaluation domains even when observing only a
single expert trajectory without actions.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01633" title="Abstract">arXiv:2310.01633</a> [<a href="/pdf/2310.01633" title="Download PDF">pdf</a>, <a href="/format/2310.01633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Path Integral Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Park%2C+H">Hyuk Park</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+D">Duo Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Hanasusanto%2C+G+A">Grani A. Hanasusanto</a>, 
<a href="/search/eess?searchtype=author&query=Tanaka%2C+T">Takashi Tanaka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We consider a continuous-time continuous-space stochastic optimal control
problem, where the controller lacks exact knowledge of the underlying diffusion
process, relying instead on a finite set of historical disturbance
trajectories. In situations where data collection is limited, the controller
synthesized from empirical data may exhibit poor performance. To address this
issue, we introduce a novel approach named Distributionally Robust Path
Integral (DRPI). The proposed method employs distributionally robust
optimization (DRO) to robustify the resulting policy against the unknown
diffusion process. Notably, the DRPI scheme shows similarities with
risk-sensitive control, which enables us to utilize the path integral control
(PIC) framework as an efficient solution scheme. We derive theoretical
performance guarantees for the DRPI scheme, which closely aligns with selecting
a risk parameter in risk-sensitive control. We validate the efficacy of our
scheme and showcase its superiority when compared to risk-neutral PIC policies
in the absence of the true diffusion process.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01634" title="Abstract">arXiv:2310.01634</a> [<a href="/pdf/2310.01634" title="Download PDF">pdf</a>, <a href="/format/2310.01634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Insights into Noisy Pseudo Labeling on Graph Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Botao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jiashun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yu Rong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tsung%2C+F">Fugee Tsung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pseudo labeling (PL) is a wide-applied strategy to enlarge the labeled
dataset by self-annotating the potential samples during the training process.
Several works have shown that it can improve the graph learning model
performance in general. However, we notice that the incorrect labels can be
fatal to the graph training process. Inappropriate PL may result in the
performance degrading, especially on graph data where the noise can propagate.
Surprisingly, the corresponding error is seldom theoretically analyzed in the
literature. In this paper, we aim to give deep insights of PL on graph learning
models. We first present the error analysis of PL strategy by showing that the
error is bounded by the confidence of PL threshold and consistency of
multi-view prediction. Then, we theoretically illustrate the effect of PL on
convergence property. Based on the analysis, we propose a cautious pseudo
labeling methodology in which we pseudo label the samples with highest
confidence and multi-view consistency. Finally, extensive experiments
demonstrate that the proposed strategy improves graph learning process and
outperforms other PL strategies on link prediction and node classification
tasks.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01636" title="Abstract">arXiv:2310.01636</a> [<a href="/pdf/2310.01636" title="Download PDF">pdf</a>, <a href="/format/2310.01636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Visual Scene Understanding: Incremental Scene Graph Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+N">Naitik Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene graph generation (SGG) involves analyzing images to extract meaningful
information about objects and their relationships. Given the dynamic nature of
the visual world, it becomes crucial for AI systems to detect new objects and
establish their new relationships with existing objects. To address the lack of
continual learning methodologies in SGG, we introduce the comprehensive
Continual ScenE Graph Generation (CSEGG) dataset along with 3 learning
scenarios and 8 evaluation metrics. Our research investigates the continual
learning performances of existing SGG methods on the retention of previous
object entities and relationships as they learn new ones. Moreover, we also
explore how continual object detection enhances generalization in classifying
known relationships on unknown objects. We conduct extensive experiments
benchmarking and analyzing the classical two-stage SGG methods and the most
recent transformer-based SGG methods in continual learning settings, and gain
valuable insights into the CSEGG problem. We invite the research community to
explore this emerging field of study.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01641" title="Abstract">arXiv:2310.01641</a> [<a href="/pdf/2310.01641" title="Download PDF">pdf</a>, <a href="/format/2310.01641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Only Look at Once for Real-time and Generic Multi-Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiayuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q+M+J">Q. M. Jonathan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">High precision, lightweight, and real-time responsiveness are three essential
requirements for implementing autonomous driving. Considering all of them
simultaneously is a challenge. In this study, we present an adaptive,
real-time, and lightweight multi-task model designed to concurrently handle
object detection, drivable area segmentation, and lane detection tasks. To
achieve this research objective, we developed an end-to-end multi-task model
with a unified and streamlined segmentation structure. Our model operates
without the need for any specific customization structure or loss function. We
achieved competitive results on the BDD100k dataset, particularly in
visualization outcomes. The performance results show a mAP50 of 81.1% for
object detection, a mIoU of 91.0% for drivable area segmentation, and an IoU of
28.8% for lane line segmentation. Additionally, we introduced a real-road
dataset to evaluate our model's performance in a real scene, which
significantly outperforms competitors. This demonstrates that our model not
only exhibits competitive performance but is also more flexible and faster than
existing multi-task models. The source codes and pre-trained models are
released at https://github.com/JiayuanWang-JW/YOLOv8-multi-task
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01642" title="Abstract">arXiv:2310.01642</a> [<a href="/pdf/2310.01642" title="Download PDF">pdf</a>, <a href="/format/2310.01642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Naming Conventions (and Defects) of Pre-trained Deep Learning  Models in Hugging Face and Other Model Hubs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+C">Chingwo Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Thiruvathukal%2C+G+K">George K. Thiruvathukal</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C. Davis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As innovation in deep learning continues, many engineers want to adopt
Pre-Trained deep learning Models (PTMs) as components in computer systems. PTMs
are part of a research-to-practice pipeline: researchers publish PTMs, which
engineers adapt for quality or performance and then deploy. If PTM authors
choose appropriate names for their PTMs, it could facilitate model discovery
and reuse. However, prior research has reported that model names are not always
well chosen, and are sometimes erroneous. The naming conventions and naming
defects for PTM packages have not been systematically studied - understanding
them will add to our knowledge of how the research-to-practice process works
for PTM packages
<br />In this paper, we report the first study of PTM naming conventions and the
associated PTM naming defects. We define the components of a PTM package name,
comprising the package name and claimed architecture from the metadata. We
present the first study focused on characterizing the nature of naming in PTM
ecosystem. To this end, we developed a novel automated naming assessment
technique that can automatically extract the semantic and syntactic patterns.
To identify potential naming defects, we developed a novel algorithm, automated
DNN ARchitecture Assessment pipeline (DARA), to cluster PTMs based on
architectural differences. Our study suggests the naming conventions for PTMs,
and frames the naming conventions as signal of the research-to-practice
relationships in the PTM ecosystem. We envision future works on further
empirical study on leveraging meta-features of PTMs to support model search and
reuse.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01646" title="Abstract">arXiv:2310.01646</a> [<a href="/pdf/2310.01646" title="Download PDF">pdf</a>, <a href="/format/2310.01646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Information Attacks on Incentive-Compatible Navigational  Recommendations in Intelligent Transportation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Ya-Ting Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Haozhe Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Intelligent transportation systems (ITS) have gained significant attention
from various communities, driven by rapid advancements in informational
technology. Within the realm of ITS, navigational recommendation systems (RS)
play a pivotal role, as users often face diverse path (route) options in such
complex urban environments. However, RS is not immune to vulnerabilities,
especially when confronted with potential information-based attacks. This study
aims to explore the impacts of these cyber threats on RS, explicitly focusing
on local targeted information attacks in which the attacker favors certain
groups or businesses. We study human behaviors and propose the coordinated
incentive-compatible RS that guides users toward a mixed Nash equilibrium,
under which each user has no incentive to deviate from the recommendation.
Then, we delve into the vulnerabilities within the recommendation process,
focusing on scenarios involving misinformed demands. In such cases, the
attacker can fabricate fake users to mislead the RS's recommendations. Using
the Stackelberg game approach, the analytical results and the numerical case
study reveal that RS is susceptible to informational attacks. This study
highlights the need to consider informational attacks for a more resilient and
effective navigational recommendation.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01647" title="Abstract">arXiv:2310.01647</a> [<a href="/pdf/2310.01647" title="Download PDF">pdf</a>, <a href="/format/2310.01647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Adaptation of Large Pre-Trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+A+K">Arnab Kumar Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Panigrahi%2C+S+S">Siba Smarak Panigrahi</a>, 
<a href="/search/cs?searchtype=author&query=Kaba%2C+S">S&#xe9;kou-Oumar Kaba</a>, 
<a href="/search/cs?searchtype=author&query=Rajeswar%2C+S">Sai Rajeswar</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures. Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Equivariant networks are specifically designed to ensure consistent behavior
with respect to a set of input transformations, leading to higher sample
efficiency and more accurate and robust predictions. However, redesigning each
component of prevalent deep neural network architectures to achieve chosen
equivariance is a difficult problem and can result in a computationally
expensive network during both training and inference. A recently proposed
alternative towards equivariance that removes the architectural constraints is
to use a simple canonicalization network that transforms the input to a
canonical form before feeding it to an unconstrained prediction network. We
show here that this approach can effectively be used to make a large
pre-trained network equivariant. However, we observe that the produced
canonical orientations can be misaligned with those of the training
distribution, hindering performance. Using dataset-dependent priors to inform
the canonicalization function, we are able to make large pre-trained models
equivariant while maintaining their performance. This significantly improves
the robustness of these models to deterministic transformations of the data,
such as rotations. We believe this equivariant adaptation of large pre-trained
models can help their domain-specific applications with known symmetry priors.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01649" title="Abstract">arXiv:2310.01649</a> [<a href="/pdf/2310.01649" title="Download PDF">pdf</a>, <a href="/format/2310.01649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Training Derivative-Constrained Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+K">KaiChieh Lo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Daniel Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We refer to the setting where the (partial) derivatives of a neural network's
(NN's) predictions with respect to its inputs are used as additional training
signal as a derivative-constrained (DC) NN. This situation is common in
physics-informed settings in the natural sciences. We propose an integrated
RELU (IReLU) activation function to improve training of DC NNs. We also
investigate denormalization and label rescaling to help stabilize DC training.
We evaluate our methods on physics-informed settings including quantum
chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that
existing architectures with IReLU activations combined with denormalization and
label rescaling better incorporate training signal provided by derivative
constraints.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01650" title="Abstract">arXiv:2310.01650</a> [<a href="/pdf/2310.01650" title="Download PDF">pdf</a>, <a href="/format/2310.01650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoDBench: A Critical Evaluation of Data-driven Models for Continuous  Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burark%2C+P">Priyanshu Burark</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+K">Karn Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Rashid%2C+M+M">Meer Mehran Rashid</a>, 
<a href="/search/cs?searchtype=author&query=P%2C+P+A">Prathosh A P</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+N+M+A">N M Anoop Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Continuous dynamical systems, characterized by differential equations, are
ubiquitously used to model several important problems: plasma dynamics, flow
through porous media, weather forecasting, and epidemic dynamics. Recently, a
wide range of data-driven models has been used successfully to model these
systems. However, in contrast to established fields like computer vision,
limited studies are available analyzing the strengths and potential
applications of different classes of these models that could steer
decision-making in scientific machine learning. Here, we introduce CodBench, an
exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models
for solving differential equations. Specifically, we comprehensively evaluate 4
distinct categories of models, viz., feed forward neural networks, deep
operator regression models, frequency-based neural operators, and transformer
architectures against 8 widely applicable benchmark datasets encompassing
challenges from fluid and solid mechanics. We conduct extensive experiments,
assessing the operators' capabilities in learning, zero-shot super-resolution,
data efficiency, robustness to noise, and computational efficiency.
Interestingly, our findings highlight that current operators struggle with the
newer mechanics datasets, motivating the need for more robust neural operators.
All the datasets and codes will be shared in an easy-to-use fashion for the
scientific community. We hope this resource will be an impetus for accelerated
progress and exploration in modeling dynamical systems.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01651" title="Abstract">arXiv:2310.01651</a> [<a href="/pdf/2310.01651" title="Download PDF">pdf</a>, <a href="/format/2310.01651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fool Your (Vision and) Language Model With Embarrassingly Simple  Permutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yongshuo Zong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tingyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bingchen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chavhan%2C+R">Ruchika Chavhan</a>, 
<a href="/search/cs?searchtype=author&query=Hospedales%2C+T">Timothy Hospedales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language and vision-language models are rapidly being deployed in
practice thanks to their impressive capabilities in instruction following,
in-context learning, and so on. This raises an urgent need to carefully analyse
their robustness so that stakeholders can understand if and when such models
are trustworthy enough to be relied upon in any given application. In this
paper, we highlight a specific vulnerability in popular models, namely
permutation sensitivity in multiple-choice question answering (MCQA).
Specifically, we show empirically that popular models are vulnerable to
adversarial permutation in answer sets for multiple-choice prompting, which is
surprising as models should ideally be as invariant to prompt permutation as
humans are. These vulnerabilities persist across various model sizes, and exist
in very recent language and vision-language models. Code is available at
\url{https://github.com/ys-zong/FoolyourVLLMs}.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01652" title="Abstract">arXiv:2310.01652</a> [<a href="/pdf/2310.01652" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sleep during COVID-19 pandemic: Longitudinal observational study  combining multisensor data with questionnaires
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luong%2C+N">Nguyen Luong</a>, 
<a href="/search/cs?searchtype=author&query=Mark%2C+G">Gloria Mark</a>, 
<a href="/search/cs?searchtype=author&query=Kulshrestha%2C+J">Juhi Kulshrestha</a>, 
<a href="/search/cs?searchtype=author&query=Aledavood%2C+T">Talayeh Aledavood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Applications (stat.AP)

</div>
<p class="mathjax">The COVID-19 pandemic led to various containment strategies, such as
work-from-home policies and reduced social contact, which significantly altered
people's sleep patterns. Our study, conducted from June 2021 to June 2022,
longitudinally examined the changes in the sleep patterns of working adults in
Finland during this period, utilizing multisensor data from fitness trackers
and monthly questionnaires. We conducted a comprehensive study, exploring the
changes in sleep patterns in correlation with multiple factors such as
individual demographics, occupation, sleep-related behaviors, levels of
physical activity, restrictions imposed by the pandemic, and adjustments in
seasonal variations. From over 27,000 nights analyzed from 112 participants, we
found a correlation between stringent pandemic measures and increased total
sleep time as well as delayed sleep timing. Academic staff experienced shorter
and more variable sleep durations compared to service staff. Early-day physical
activity was also linked to longer sleep duration, revealing the influence of
lifestyle on sleep quality. Habitual snoozers exhibited higher variability in
their sleep patterns. The findings reveal the multifaceted impacts of the
pandemic and associated measures on sleep patterns, highlighting the nuanced
variations among different occupations and habits, and emphasizing the role of
flexible work-life routines and external factors in shaping sleep behaviors
during such unprecedented times.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01653" title="Abstract">arXiv:2310.01653</a> [<a href="/pdf/2310.01653" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Taxonomy and Evaluation of IoT Security Guidelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jesse Chen</a>, 
<a href="/search/cs?searchtype=author&query=Anandayuvaraj%2C+D">Dharun Anandayuvaraj</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C Davis</a>, 
<a href="/search/cs?searchtype=author&query=Rahaman%2C+S">Sazzadur Rahaman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Cybersecurity concerns about Internet of Things (IoT) devices and
infrastructure are growing each year. In response, organizations worldwide have
published IoT cybersecurity guidelines to protect their citizens and customers.
These guidelines constrain the development of IoT systems, which include
substantial software components both on-device and in the Cloud. While these
guidelines are being widely adopted, e.g. by US federal contractors, their
content and merits have not been critically examined. Two notable gaps are: (1)
We do not know how these guidelines differ by the topics and details of their
recommendations; and (2) We do not know how effective they are at mitigating
real-world IoT failures.
<br />In this paper, we address these questions through an exploratory sequential
mixed-method study of IoT cybersecurity guidelines. We collected a corpus of
142 general IoT cybersecurity guidelines, sampling them for recommendations
until saturation was reached. From the resulting 958 unique recommendations, we
iteratively developed a hierarchical taxonomy following grounded theory coding
principles. We measured the guidelines' usefulness by asking novice engineers
about the actionability of each recommendation, and by matching cybersecurity
recommendations to the root causes of failures (CVEs and news stories). We
report that: (1) Comparing guidelines to one another, each guideline has gaps
in its topic coverage and comprehensiveness; and (2) Although 87.2%
recommendations are actionable and the union of the guidelines mitigates all 17
of the failures from news stories, 21% of the CVEs apparently evade the
guidelines. In summary, we report shortcomings in every guideline's depth and
breadth, but as a whole they are capable of preventing security issues. Our
results will help software engineers determine which and how many guidelines to
study as they implement IoT systems.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01655" title="Abstract">arXiv:2310.01655</a> [<a href="/pdf/2310.01655" title="Download PDF">pdf</a>, <a href="/format/2310.01655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kacham%2C+P">Praneeth Kacham</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Peilin Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The quadratic complexity of attention in transformer architectures remains a
big bottleneck in scaling up large foundation models for long context. In fact,
recent theoretical results show the hardness of approximating the output of
softmax attention mechanism in sub-quadratic time assuming Strong Exponential
Time Hypothesis. In this paper, we show how to break this theoretical barrier
by replacing softmax with a polynomial function and polynomial sketching. In
particular we show that sketches for Polynomial Kernel from the randomized
numerical linear algebra literature can be used to approximate the polynomial
attention which leads to a significantly faster attention mechanism without
assuming any sparse structure for the attention matrix that has been done in
many previous works.
<br />In addition, we propose an efficient block-based algorithm that lets us apply
the causal mask to the attention matrix without explicitly realizing the $n
\times n$ attention matrix and compute the output of the polynomial attention
mechanism in time linear in the context length. The block-based algorithm gives
significant speedups over the \emph{cumulative sum} algorithm used by Performer
to apply the causal mask to the attention matrix. These observations help us
design \emph{PolySketchFormer}, a practical linear-time transformer
architecture for language modeling with provable guarantees.
<br />We validate our design empirically by training language models with long
context lengths. We first show that the eval perplexities of our models are
comparable to that of models trained with softmax attention. We then show that
for large context lengths our training times are significantly faster than
FlashAttention.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01656" title="Abstract">arXiv:2310.01656</a> [<a href="/pdf/2310.01656" title="Download PDF">pdf</a>, <a href="/format/2310.01656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Forced Oscillation Localization using Inferred Impulse  Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Shaohui Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Kekatos%2C+V">Vassilis Kekatos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Poorly damped oscillations pose threats to the stability and reliability of
interconnected power systems. In this work, we propose a comprehensive
data-driven framework for inferring the sources of forced oscillation (FO)
using only synchrophasor measurements. During normal grid operations, fast-rate
ambient data are collected to recover the impulse responses in the small-signal
regime, without requiring the system models. When FO events occur, the source
is estimated based on the frequency domain analysis by fitting the
least-squares (LS) error for the FO data using the impulse responses recovered
previously. Although the proposed framework is purely data-driven, the result
has been established theoretically via model-based analysis of linearized
dynamics under a few realistic assumptions. Numerical validations demonstrate
its applicability to realistic power systems including nonlinear, higher-order
dynamics with control effects using the IEEE 68-bus system. The
generalizability of the proposed methodology has been validated using different
types of measurements and partial sensor coverage conditions.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01659" title="Abstract">arXiv:2310.01659</a> [<a href="/pdf/2310.01659" title="Download PDF">pdf</a>, <a href="/format/2310.01659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It&#x27;s all about you: Personalized in-Vehicle Gesture Recognition with a  Time-of-Flight Camera
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomaa%2C+A">Amr Gomaa</a>, 
<a href="/search/cs?searchtype=author&query=Reyes%2C+G">Guillermo Reyes</a>, 
<a href="/search/cs?searchtype=author&query=Feld%2C+M">Michael Feld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AutoUI2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Despite significant advances in gesture recognition technology, recognizing
gestures in a driving environment remains challenging due to limited and costly
data and its dynamic, ever-changing nature. In this work, we propose a
model-adaptation approach to personalize the training of a CNNLSTM model and
improve recognition accuracy while reducing data requirements. Our approach
contributes to the field of dynamic hand gesture recognition while driving by
providing a more efficient and accurate method that can be customized for
individual users, ultimately enhancing the safety and convenience of in-vehicle
interactions, as well as driver's experience and system trust. We incorporate
hardware enhancement using a time-of-flight camera and algorithmic enhancement
through data augmentation, personalized adaptation, and incremental learning
techniques. We evaluate the performance of our approach in terms of recognition
accuracy, achieving up to 90\%, and show the effectiveness of personalized
adaptation and incremental learning for a user-centered design.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01661" title="Abstract">arXiv:2310.01661</a> [<a href="/pdf/2310.01661" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Home Electricity Data Generator (HEDGE): An open-access tool for the  generation of electric vehicle, residential demand, and PV generation  profiles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Charbonnier%2C+F">Flora Charbonnier</a>, 
<a href="/search/eess?searchtype=author&query=Morstyn%2C+T">Thomas Morstyn</a>, 
<a href="/search/eess?searchtype=author&query=McCulloch%2C+M">Malcolm McCulloch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we present the Home Electricity Data Generator (HEDGE), an
open-access tool for the random generation of realistic residential energy
data. HEDGE generates realistic daily profiles of residential PV generation,
household electric loads, and electric vehicle consumption and at-home
availability, based on real-life UK datasets. The lack of usable data is a
major hurdle for research on residential distributed energy resources
characterisation and coordination, especially when using data-driven methods
such as machine learning-based forecasting and reinforcement learning-based
control. A key issue is that while large data banks are available, they are not
in a usable format, and numerous subsequent days of data for a given single
home are unavailable. We fill these gaps with the open-access HEDGE tool which
generates data sequences of energy data for several days in a way that is
consistent for single homes, both in terms of profile magnitude and behavioural
clusters. From raw datasets, pre-processing steps are conducted, including
filling in incomplete data sequences and clustering profiles into behaviour
clusters. Generative adversarial networks (GANs) are then trained to generate
realistic synthetic data representative of each behaviour groups consistent
with real-life behavioural and physical patterns.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01662" title="Abstract">arXiv:2310.01662</a> [<a href="/pdf/2310.01662" title="Download PDF">pdf</a>, <a href="/format/2310.01662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SYRAC: Synthesize, Rank, and Count
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Alessandro%2C+A">Adriano D&#x27;Alessandro</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi-Amiri%2C+A">Ali Mahdavi-Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Hamarneh%2C+G">Ghassan Hamarneh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Crowd counting is a critical task in computer vision, with several important
applications. However, existing counting methods rely on labor-intensive
density map annotations, necessitating the manual localization of each
individual pedestrian. While recent efforts have attempted to alleviate the
annotation burden through weakly or semi-supervised learning, these approaches
fall short of significantly reducing the workload. We propose a novel approach
to eliminate the annotation burden by leveraging latent diffusion models to
generate synthetic data. However, these models struggle to reliably understand
object quantities, leading to noisy annotations when prompted to produce images
with a specific quantity of objects. To address this, we use latent diffusion
models to create two types of synthetic data: one by removing pedestrians from
real images, which generates ranked image pairs with a weak but reliable object
quantity signal, and the other by generating synthetic images with a
predetermined number of objects, offering a strong but noisy counting signal.
Our method utilizes the ranking image pairs for pre-training and then fits a
linear layer to the noisy synthetic images using these crowd quantity features.
We report state-of-the-art results for unsupervised crowd counting.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01663" title="Abstract">arXiv:2310.01663</a> [<a href="/pdf/2310.01663" title="Download PDF">pdf</a>, <a href="/format/2310.01663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-guided Domain Gap Reduction for Monocular Depth Prediction in  Endoscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rau%2C+A">Anita Rau</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Agapito%2C+L">Lourdes Agapito</a>, 
<a href="/search/cs?searchtype=author&query=Stoyanov%2C+D">Danail Stoyanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First Data Engineering in Medical Imaging Workshop at MICCAI 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes in Computer Science, vol 14314. 2023. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Colorectal cancer remains one of the deadliest cancers in the world. In
recent years computer-aided methods have aimed to enhance cancer screening and
improve the quality and availability of colonoscopies by automatizing
sub-tasks. One such task is predicting depth from monocular video frames, which
can assist endoscopic navigation. As ground truth depth from standard in-vivo
colonoscopy remains unobtainable due to hardware constraints, two approaches
have aimed to circumvent the need for real training data: supervised methods
trained on labeled synthetic data and self-supervised models trained on
unlabeled real data. However, self-supervised methods depend on unreliable loss
functions that struggle with edges, self-occlusion, and lighting inconsistency.
Methods trained on synthetic data can provide accurate depth for synthetic
geometries but do not use any geometric supervisory signal from real data and
overfit to synthetic anatomies and properties. This work proposes a novel
approach to leverage labeled synthetic and unlabeled real data. While previous
domain adaptation methods indiscriminately enforce the distributions of both
input data modalities to coincide, we focus on the end task, depth prediction,
and translate only essential information between the input domains. Our
approach results in more resilient and accurate depth maps of real colonoscopy
sequences.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01664" title="Abstract">arXiv:2310.01664</a> [<a href="/pdf/2310.01664" title="Download PDF">pdf</a>, <a href="/format/2310.01664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+Y">Yeonsoo Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Erez%2C+M">Mattan Erez</a>, 
<a href="/search/cs?searchtype=author&query=Orshansky%2C+M">Michael Orshansky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Privacy-Preserving ML (PPML) based on Homomorphic Encryption (HE) is a
promising foundational privacy technology. Making it more practical requires
lowering its computational cost, especially, in handling modern large deep
neural networks. Model compression via pruning is highly effective in
conventional plaintext ML but cannot be effectively applied to HE-PPML as is.
<br />We propose Artemis, a highly effective DNN pruning technique for HE-based
inference. We judiciously investigate two HE-aware pruning strategies
(positional and diagonal) to reduce the number of Rotation operations, which
dominate compute time in HE convolution. We find that Pareto-optimal solutions
are based fully on diagonal pruning. Artemis' benefits come from coupling DNN
training, driven by a novel group Lasso regularization objective, with pruning
to maximize HE-specific cost reduction (dominated by the Rotation operations).
We show that Artemis improves on prior HE-oriented pruning and can achieve a
1.2-6x improvement when targeting modern convolutional models (ResNet18 and
ResNet18) across three datasets.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01665" title="Abstract">arXiv:2310.01665</a> [<a href="/pdf/2310.01665" title="Download PDF">pdf</a>, <a href="/format/2310.01665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightning Helmholtz Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ginn%2C+H">Henry Ginn</a>, 
<a href="/search/math?searchtype=author&query=Trefethen%2C+L+N">Lloyd N. Trefethen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages, 42 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this dissertation we have applied Trefethen and Gopal's Lightning Method
to solve the Helmholtz equation in the exterior of two dimensional piecewise
smooth domains. The background theory motivating the method is presented, and
we explore the optimal method implementation for the unit square, which is
subsequently used to give a guide on parameter selection for a general region.
The behaviour of the computed solutions is verified to act in accordance with
our intuition and current understanding of wave propagation, and we show that
the wave decays to approximately 0 in the shadow region.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01667" title="Abstract">arXiv:2310.01667</a> [<a href="/pdf/2310.01667" title="Download PDF">pdf</a>, <a href="/format/2310.01667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STARS: Zero-shot Sim-to-Real Transfer for Segmentation of Shipwrecks in  Sonar Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sethuraman%2C+A+V">Advaith Venkatramanan Sethuraman</a>, 
<a href="/search/cs?searchtype=author&query=Skinner%2C+K+A">Katherine A. Skinner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we address the problem of sim-to-real transfer for object
segmentation when there is no access to real examples of an object of interest
during training, i.e. zero-shot sim-to-real transfer for segmentation. We focus
on the application of shipwreck segmentation in side scan sonar imagery. Our
novel segmentation network, STARS, addresses this challenge by fusing a
predicted deformation field and anomaly volume, allowing it to generalize
better to real sonar images and achieve more effective zero-shot sim-to-real
transfer for image segmentation. We evaluate the sim-to-real transfer
capabilities of our method on a real, expert-labeled side scan sonar dataset of
shipwrecks collected from field work surveys with an autonomous underwater
vehicle (AUV). STARS is trained entirely in simulation and performs zero-shot
shipwreck segmentation with no additional fine-tuning on real data. Our method
provides a significant 20% increase in segmentation performance for the
targeted shipwreck class compared to the best baseline.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01668" title="Abstract">arXiv:2310.01668</a> [<a href="/pdf/2310.01668" title="Download PDF">pdf</a>, <a href="/format/2310.01668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locality-Aware Graph-Rewiring in GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barbero%2C+F">Federico Barbero</a>, 
<a href="/search/cs?searchtype=author&query=Velingker%2C+A">Ameya Velingker</a>, 
<a href="/search/cs?searchtype=author&query=Saberi%2C+A">Amin Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Di+Giovanni%2C+F">Francesco Di Giovanni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) are popular models for machine learning on
graphs that typically follow the message-passing paradigm, whereby the feature
of a node is updated recursively upon aggregating information over its
neighbors. While exchanging messages over the input graph endows GNNs with a
strong inductive bias, it can also make GNNs susceptible to over-squashing,
thereby preventing them from capturing long-range interactions in the given
graph. To rectify this issue, graph rewiring techniques have been proposed as a
means of improving information flow by altering the graph connectivity. In this
work, we identify three desiderata for graph-rewiring: (i) reduce
over-squashing, (ii) respect the locality of the graph, and (iii) preserve the
sparsity of the graph. We highlight fundamental trade-offs that occur between
spatial and spectral rewiring techniques; while the former often satisfy (i)
and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the
expense of (ii). We propose a novel rewiring framework that satisfies all of
(i)--(iii) through a locality-aware sequence of rewiring operations. We then
discuss a specific instance of such rewiring framework and validate its
effectiveness on several real-world benchmarks, showing that it either matches
or significantly outperforms existing rewiring approaches.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01669" title="Abstract">arXiv:2310.01669</a> [<a href="/pdf/2310.01669" title="Download PDF">pdf</a>, <a href="/format/2310.01669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wellbeing in Future Mobility: Toward AV Policy Design to Increase  Wellbeing through Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrotra%2C+S">Shashank Mehrotra</a>, 
<a href="/search/cs?searchtype=author&query=Zahedi%2C+Z">Zahra Zahedi</a>, 
<a href="/search/cs?searchtype=author&query=Misu%2C+T">Teruhisa Misu</a>, 
<a href="/search/cs?searchtype=author&query=Akash%2C+K">Kumar Akash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC), September 24-28, 2023, Bilbao, Bizkaia, Spain
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Recent advances in Automated vehicle (AV) technology and micromobility
devices promise a transformational change in the future of mobility usage.
These advances also pose challenges concerning human-AV interactions. To ensure
the smooth adoption of these new mobilities, it is essential to assess how past
experiences and perceptions of social interactions by people may impact the
interactions with AV mobility. This research identifies and estimates an
individual's wellbeing based on their actions, prior experiences, social
interaction perceptions, and dyadic interactions with other road users. An
online video-based user study was designed, and responses from 300 participants
were collected and analyzed to investigate the impact on individual wellbeing.
A machine learning model was designed to predict the change in wellbeing. An
optimal policy based on the model allows informed AV actions toward its
yielding behavior with other road users to enhance users' wellbeing. The
findings from this study have broader implications for creating human-aware
systems by creating policies that align with the individual state and
contribute toward designing systems that align with an individual's state of
wellbeing.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01673" title="Abstract">arXiv:2310.01673</a> [<a href="/pdf/2310.01673" title="Download PDF">pdf</a>, <a href="/format/2310.01673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Versatile Data Fabric for Advanced IoT-Based Remote Health Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buleje%2C+I">Italo Buleje</a>, 
<a href="/search/cs?searchtype=author&query=Siu%2C+V+S">Vince S. Siu</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+K+Y">Kuan Yu Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Hinds%2C+N">Nigel Hinds</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+B">Bing Dang</a>, 
<a href="/search/cs?searchtype=author&query=Bilal%2C+E">Erhan Bilal</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thanhnha Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E+E">Ellen E. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Depp%2C+C+A">Colin A. Depp</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+J+L">Jeffrey L. Rogers</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Digital Health (ICDH),
  Chicago, IL, USA, 2023, pp. 88-90
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This paper presents a data-centric and security-focused data fabric designed
for digital health applications. With the increasing interest in digital health
research, there has been a surge in the volume of Internet of Things (IoT) data
derived from smartphones, wearables, and ambient sensors. Managing this vast
amount of data, encompassing diverse data types and varying time scales, is
crucial. Moreover, compliance with regulatory and contractual obligations is
essential. The proposed data fabric comprises an architecture and a toolkit
that facilitate the integration of heterogeneous data sources, across different
environments, to provide a unified view of the data in dashboards. Furthermore,
the data fabric supports the development of reusable and configurable data
integration components, which can be shared as open-source or inner-source
software. These components are used to generate data pipelines that can be
deployed and scheduled to run either in the cloud or on-premises. Additionally,
we present the implementation of our data fabric in a home-based telemonitoring
research project involving older adults, conducted in collaboration with the
University of California, San Diego (UCSD). The study showcases the streamlined
integration of data collected from various IoT sensors and mobile applications
to create a unified view of older adults' health for further analysis and
research.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01675" title="Abstract">arXiv:2310.01675</a> [<a href="/pdf/2310.01675" title="Download PDF">pdf</a>, <a href="/format/2310.01675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision-Dominant Strategic Defense Against Lateral Movement for 5G  Zero-Trust Multi-Domain Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tao Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yunian Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Quanyan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, 1 table, and 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Multi-domain warfare is a military doctrine that leverages capabilities from
different domains, including air, land, sea, space, and cyberspace, to create a
highly interconnected battle network that is difficult for adversaries to
disrupt or defeat. However, the adoption of 5G technologies on battlefields
presents new vulnerabilities due to the complexity of interconnections and the
diversity of software, hardware, and devices from different supply chains.
Therefore, establishing a zero-trust architecture for 5G-enabled networks is
crucial for continuous monitoring and fast data analytics to protect against
targeted attacks. To address these challenges, we propose a proactive
end-to-end security scheme that utilizes a 5G satellite-guided air-ground
network. Our approach incorporates a decision-dominant learning-based method
that can thwart the lateral movement of adversaries targeting critical assets
on the battlefield before they can conduct reconnaissance or gain necessary
access or credentials. We demonstrate the effectiveness of our game-theoretic
design, which uses a meta-learning framework to enable zero-trust monitoring
and decision-dominant defense against attackers in emerging multi-domain
battlefield networks.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01676" title="Abstract">arXiv:2310.01676</a> [<a href="/pdf/2310.01676" title="Download PDF">pdf</a>, <a href="/format/2310.01676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk and Threat Mitigation Techniques in Internet of Things (IoT)  Environments: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salayma%2C+M">Marwa Salayma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 4 figures, 1 table, Survey article (systematic review). This work was funded by PETRAS National Centre of Excellence for IoT Systems Cybersecurity (PETRAS 2/RACE Project), Grant number is EPS0353621
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Security in the Internet of Things (IoT) remains a predominant area of
concern. This survey updates the state of the art covered in previous surveys
and focuses on defending against threats rather than on the threats alone. This
area is less extensively covered by other surveys and warrants particular
attention. A life-cycle approach is adopted, articulated to form a "defence in
depth" strategy against malicious actors compromising an IoT network laterally
within it and from it. This study highlights the challenges of each mitigation
step, emphasises novel perspectives, and reconnects the discussed mitigation
steps to the ground principles they seek to implement.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01679" title="Abstract">arXiv:2310.01679</a> [<a href="/pdf/2310.01679" title="Download PDF">pdf</a>, <a href="/format/2310.01679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating and Implementing Conventional Fairness Metrics With  Probabilistic Protected Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elzayn%2C+H">Hadi Elzayn</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+E">Emily Black</a>, 
<a href="/search/cs?searchtype=author&query=Vossler%2C+P">Patrick Vossler</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+N">Nathanael Jo</a>, 
<a href="/search/cs?searchtype=author&query=Goldin%2C+J">Jacob Goldin</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+D+E">Daniel E. Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
<p class="mathjax">The vast majority of techniques to train fair models require access to the
protected attribute (e.g., race, gender), either at train time or in
production. However, in many important applications this protected attribute is
largely unavailable. In this paper, we develop methods for measuring and
reducing fairness violations in a setting with limited access to protected
attribute labels. Specifically, we assume access to protected attribute labels
on a small subset of the dataset of interest, but only probabilistic estimates
of protected attribute labels (e.g., via Bayesian Improved Surname Geocoding)
for the rest of the dataset. With this setting in mind, we propose a method to
estimate bounds on common fairness metrics for an existing model, as well as a
method for training a model to limit fairness violations by solving a
constrained non-convex optimization problem. Unlike similar existing
approaches, our methods take advantage of contextual information --
specifically, the relationships between a model's predictions and the
probabilistic prediction of protected attributes, given the true protected
attribute, and vice versa -- to provide tighter bounds on the true disparity.
We provide an empirical illustration of our methods using voting data. First,
we show our measurement method can bound the true disparity up to 5.5x tighter
than previous methods in these applications. Then, we demonstrate that our
training technique effectively reduces disparity while incurring lesser
fairness-accuracy trade-offs than other fair optimization methods with limited
access to protected attributes.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01680" title="Abstract">arXiv:2310.01680</a> [<a href="/pdf/2310.01680" title="Download PDF">pdf</a>, <a href="/format/2310.01680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keypoint-Augmented Self-Supervised Learning for Medical Image  Segmentation with Limited Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhangsihao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+M">Mengwei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kaize Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gerig%2C+G">Guido Gerig</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yalin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready for NeurIPS 2023. Code available at <a href="https://github.com/zshyang/kaf.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pretraining CNN models (i.e., UNet) through self-supervision has become a
powerful approach to facilitate medical image segmentation under low annotation
regimes. Recent contrastive learning methods encourage similar global
representations when the same image undergoes different transformations, or
enforce invariance across different image/patch features that are intrinsically
correlated. However, CNN-extracted global and local features are limited in
capturing long-range spatial dependencies that are essential in biological
anatomy. To this end, we present a keypoint-augmented fusion layer that
extracts representations preserving both short- and long-range self-attention.
In particular, we augment the CNN feature map at multiple scales by
incorporating an additional input that learns long-range spatial self-attention
among localized keypoint features. Further, we introduce both global and local
self-supervised pretraining for the framework. At the global scale, we obtain
global representations from both the bottleneck of the UNet, and by aggregating
multiscale keypoint features. These global features are subsequently
regularized through image-level contrastive objectives. At the local scale, we
define a distance-based criterion to first establish correspondences among
keypoints and encourage similarity between their features. Through extensive
experiments on both MRI and CT segmentation tasks, we demonstrate the
architectural advantages of our proposed method in comparison to both CNN and
Transformer-based UNets, when all architectures are trained with randomly
initialized weights. With our proposed pretraining strategy, our method further
outperforms existing SSL methods by producing more robust self-attention and
achieving state-of-the-art segmentation results. The code is available at
https://github.com/zshyang/kaf.git.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01681" title="Abstract">arXiv:2310.01681</a> [<a href="/pdf/2310.01681" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Micro Water-Energy Co-Optimization for Small Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Silva-Rodriguez%2C+J">Jesus Silva-Rodriguez</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xingpeng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The water-energy nexus encompasses the interdependencies between water and
energy resources identifying the existing links between the production and
distribution of these resources. Therefore, understanding the water-energy
nexus is crucial for developing sustainable and integrated resource management
approaches. This paper proposes a decentralized co-optimization model for a
micro water-energy nexus system (MWEN), aiming to optimize the combined supply
of both resources to end consumers. The approach respects the separate
ownership and management of the water and energy sectors while bridging the gap
between their optimized operations. An enhanced version of the alternating
direction method of multipliers (ADMM) is proposed, the objective-based ADMM
(OB-ADMM), which is able to robustly optimize each system independently towards
a common objective, only sharing information about the power consumption of
water management, providing privacy for each resource provider.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01682" title="Abstract">arXiv:2310.01682</a> [<a href="/pdf/2310.01682" title="Download PDF">pdf</a>, <a href="/format/2310.01682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Two-Player General-Sum Games Between Swarms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghimire%2C+M">Mukesh Ghimire</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenlong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yi Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhe Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Hamilton-Jacobi-Isaacs (HJI) PDEs are the governing equations for the
two-player general-sum games. Unlike Reinforcement Learning (RL) methods, which
are data-intensive methods for learning value function, learning HJ PDEs
provide a guaranteed convergence to the Nash Equilibrium value of the game when
it exists. However, a caveat is that solving HJ PDEs becomes intractable when
the state dimension increases. To circumvent the curse of dimensionality (CoD),
physics-informed machine learning methods with supervision can be used and have
been shown to be effective in generating equilibrial policies in two-player
general-sum games. In this work, we extend the existing work on agent-level
two-player games to a two-player swarm-level game, where two sub-swarms play a
general-sum game. We consider the \textit{Kolmogorov forward equation} as the
dynamic model for the evolution of the densities of the swarms. Results show
that policies generated from the physics-informed neural network (PINN) result
in a higher payoff than a Nash Double Deep Q-Network (Nash DDQN) agent and have
comparable performance with numerical solvers.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01684" title="Abstract">arXiv:2310.01684</a> [<a href="/pdf/2310.01684" title="Download PDF">pdf</a>, <a href="/format/2310.01684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing User-Centric Behavioral Interventions to Prevent Dysglycemia  with Novel Counterfactual Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arefeen%2C+A">Asiful Arefeen</a>, 
<a href="/search/cs?searchtype=author&query=Ghasemzadeh%2C+H">Hassan Ghasemzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Maintaining normal blood glucose levels through lifestyle behaviors is
central to maintaining health and preventing disease. Frequent exposure to
dysglycemia (i.e., abnormal glucose events such as hyperlycemia and
hypoglycemia) leads to chronic complications including diabetes, kidney disease
and need for dialysis, myocardial infarction, stroke, amputation, and death.
Therefore, a tool capable of predicting dysglycemia and offering users
actionable feedback about how to make changes in their diet, exercise, and
medication to prevent abnormal glycemic events could have significant societal
impacts. Counterfactual explanations can provide insights into why a model made
a particular prediction by generating hypothetical instances that are similar
to the original input but lead to a different prediction outcome. Therefore,
counterfactuals can be viewed as a means to design AI-driven health
interventions to prevent adverse health outcomes such as dysglycemia. In this
paper, we design GlyCoach, a framework for generating counterfactual
explanations for glucose control. Leveraging insights from adversarial
learning, GlyCoach characterizes the decision boundary for high-dimensional
health data and performs a grid search to generate actionable interventions.
GlyCoach is unique in integrating prior knowledge about user preferences of
plausible explanations into the process of counterfactual generation. We
evaluate GlyCoach extensively using two real-world datasets and external
simulators from prior studies that predict glucose response. GlyCoach achieves
87\% sensitivity in the simulation-aided validation, surpassing the
state-of-the-art techniques for generating counterfactual explanations by at
least $10\%$. Besides, counterfactuals from GlyCoach exhibit a $32\%$ improved
normalized distance compared to previous research.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01685" title="Abstract">arXiv:2310.01685</a> [<a href="/pdf/2310.01685" title="Download PDF">pdf</a>, <a href="/format/2310.01685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Interpretability in Machine Learning for Medical Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+A+Q">Alan Q. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Karaman%2C+B+K">Batuhan K. Karaman</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heejong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Rosenthal%2C+J">Jacob Rosenthal</a>, 
<a href="/search/cs?searchtype=author&query=Saluja%2C+R">Rachit Saluja</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+S+I">Sean I. Young</a>, 
<a href="/search/cs?searchtype=author&query=Sabuncu%2C+M+R">Mert R. Sabuncu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Interpretability for machine learning models in medical imaging (MLMI) is an
important direction of research. However, there is a general sense of murkiness
in what interpretability means. Why does the need for interpretability in MLMI
arise? What goals does one actually seek to address when interpretability is
needed? To answer these questions, we identify a need to formalize the goals
and elements of interpretability in MLMI. By reasoning about real-world tasks
and goals common in both medical image analysis and its intersection with
machine learning, we identify four core elements of interpretability:
localization, visual recognizability, physical attribution, and transparency.
Overall, this paper formalizes interpretability needs in the context of medical
imaging, and our applied perspective clarifies concrete MLMI-specific goals and
considerations in order to guide method design and improve real-world usage.
Our goal is to provide practical and didactic information for model designers
and practitioners, inspire developers of models in the medical imaging field to
reason more deeply about what interpretability is achieving, and suggest future
directions of interpretability research.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01686" title="Abstract">arXiv:2310.01686</a> [<a href="/pdf/2310.01686" title="Download PDF">pdf</a>, <a href="/format/2310.01686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OFDM-RSMA: Robust Transmission under Inter-Carrier Interference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahin%2C+M+M">Mehmet Mert Sahin</a>, 
<a href="/search/cs?searchtype=author&query=Dizdar%2C+O">Onur Dizdar</a>, 
<a href="/search/cs?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>, 
<a href="/search/cs?searchtype=author&query=Arslan%2C+H">Huseyin Arslan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Rate-splitting multiple access (RSMA) is a multiple access scheme to mitigate
the effects of the multi-user interference (MUI) in multi-antenna systems. In
this study, we leverage the interference management capabilities of RSMA to
tackle the issue of inter-carrier interference (ICI) in orthogonal frequency
division multiplexing (OFDM) waveform. We formulate a sum-rate maximization
problem to find the optimal subcarrier and power allocation for downlink
transmission in a two-user system using RSMA and OFDM. A weighted minimum
mean-square error (WMMSE)-based algorithm is proposed to obtain a solution for
the formulated non-convex problem. We show that the marriage of rate-splitting
(RS) with OFDM provides complementary strengths to cope with peculiar
characteristic of wireless medium and its performance-limiting challenges
including inter-symbol interference (ISI), MUI, ICI, and inter-numerology
interference (INI). The sum-rate performance of the proposed OFDM-RSMA scheme
is numerically compared with that of conventional orthogonal frequency division
multiple access (OFDMA) and OFDM-non-orthogonal multiple access (NOMA). It is
shown that the proposed OFDM-RSMA outperforms OFDM-NOMA and OFDMA in diverse
propagation channel conditions owing to its flexible structure and robust
interference management capabilities.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01687" title="Abstract">arXiv:2310.01687</a> [<a href="/pdf/2310.01687" title="Download PDF">pdf</a>, <a href="/format/2310.01687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Stability to Chaos: Analyzing Gradient Descent Dynamics in  Quadratic Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+K">Krishnakumar Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Ghosal%2C+P">Promit Ghosal</a>, 
<a href="/search/cs?searchtype=author&query=Agrawalla%2C+B">Bhavya Agrawalla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS); Optimization and Control (math.OC); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">We conduct a comprehensive investigation into the dynamics of gradient
descent using large-order constant step-sizes in the context of quadratic
regression models. Within this framework, we reveal that the dynamics can be
encapsulated by a specific cubic map, naturally parameterized by the step-size.
Through a fine-grained bifurcation analysis concerning the step-size parameter,
we delineate five distinct training phases: (1) monotonic, (2) catapult, (3)
periodic, (4) chaotic, and (5) divergent, precisely demarcating the boundaries
of each phase. As illustrations, we provide examples involving phase retrieval
and two-layer neural networks employing quadratic activation functions and
constant outer-layers, utilizing orthogonal training data. Our simulations
indicate that these five phases also manifest with generic non-orthogonal data.
We also empirically investigate the generalization performance when training in
the various non-monotonic (and non-divergent) phases. In particular, we observe
that performing an ergodic trajectory averaging stabilizes the test error in
non-monotonic (and non-divergent) phases.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01689" title="Abstract">arXiv:2310.01689</a> [<a href="/pdf/2310.01689" title="Download PDF">pdf</a>, <a href="/format/2310.01689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Threat Modelling in Internet of Things (IoT) Environment Using Dynamic  Attack Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salayma%2C+M">Marwa Salayma</a>, 
<a href="/search/cs?searchtype=author&query=Lupu%2C+E+C">Emil C Lupu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 19 figures, 2 tables, journal article (long paper). This work was supported by PETRAS National Centre of Excellence for IoT Systems Cybersecurity (PETRAS 2/RACE Project). Grant number is EPS0353621
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">We present a threat modelling approach to represent changes to the attack
paths through an Internet of Things (IoT) environment when the environment
changes dynamically, i.e., when new devices are added or removed from the
system or when whole sub-systems join or leave. The proposed approach
investigates the propagation of threats using attack graphs. However,
traditional attack graph approaches have been applied in static environments
that do not continuously change such as the Enterprise networks, leading to
static and usually very large attack graphs. In contrast, IoT environments are
often characterised by dynamic change and interconnections; different
topologies for different systems may interconnect with each other dynamically
and outside the operator control. Such new interconnections lead to changes in
the reachability amongst devices according to which their corresponding attack
graphs change. This requires dynamic topology and attack graphs for threat and
risk analysis. In this paper, we develop a threat modelling approach that cope
with dynamic system changes that may occur in IoT environments and enables
identifying attack paths whilst allowing for system dynamics. We develop
dynamic topology and attack graphs that are able to cope with the changes in
the IoT environment rapidly by maintaining their associated graphs. To motivate
the work and illustrate our approach we introduce an example scenario based on
healthcare systems. Our approach is implemented using a Graph Database
Management Tool (GDBM) -- Neo4j -- which is a popular tool for mapping,
visualising and querying the graphs of highly connected data, and is efficient
in providing a rapid threat modelling mechanism, which makes it suitable for
capturing security changes in the dynamic IoT environment.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01691" title="Abstract">arXiv:2310.01691</a> [<a href="/pdf/2310.01691" title="Download PDF">pdf</a>, <a href="/format/2310.01691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zijun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongkang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+L">Lili Mou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prompt tuning in natural language processing (NLP) has become an increasingly
popular method for adapting large language models to specific tasks. However,
the transferability of these prompts, especially continuous prompts, between
different models remains a challenge. In this work, we propose a zero-shot
continuous prompt transfer method, where source prompts are encoded into
relative space and the corresponding target prompts are searched for
transferring to target models. Experimental results confirm the effectiveness
of our method, showing that 'task semantics' in continuous prompts can be
generalized across various language models. Moreover, we find that combining
'task semantics' from multiple source models can further enhance the
generalizability of transfer.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01693" title="Abstract">arXiv:2310.01693</a> [<a href="/pdf/2310.01693" title="Download PDF">pdf</a>, <a href="/format/2310.01693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closing the Curious Case of Neural Text Degeneration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Finlayson%2C+M">Matthew Finlayson</a>, 
<a href="/search/cs?searchtype=author&query=Hewitt%2C+J">John Hewitt</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+A">Alexander Koller</a>, 
<a href="/search/cs?searchtype=author&query=Swayamdipta%2C+S">Swabha Swayamdipta</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite their ubiquity in language generation, it remains unknown why
truncation sampling heuristics like nucleus sampling are so effective. We
provide a theoretical explanation for the effectiveness of the truncation
sampling by proving that truncation methods that discard tokens below some
probability threshold (the most common type of truncation) can guarantee that
all sampled tokens have nonzero true probability. However, thresholds are a
coarse heuristic, and necessarily discard some tokens with nonzero true
probability as well. In pursuit of a more precise sampling strategy, we show
that we can leverage a known source of model errors, the softmax bottleneck, to
prove that certain tokens have nonzero true probability, without relying on a
threshold. Based on our findings, we develop an experimental truncation
strategy and the present pilot studies demonstrating the promise of this type
of algorithm. Our evaluations show that our method outperforms its
threshold-based counterparts under automatic and human evaluation metrics for
low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical
findings and pilot experiments provide both insight into why truncation
sampling works, and make progress toward more expressive sampling algorithms
that better surface the generative capabilities of large language models.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01695" title="Abstract">arXiv:2310.01695</a> [<a href="/pdf/2310.01695" title="Download PDF">pdf</a>, <a href="/format/2310.01695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynAMO: Multi-agent reinforcement learning for dynamic anticipatory mesh  optimization with applications to hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dzanic%2C+T">Tarik Dzanic</a>, 
<a href="/search/math?searchtype=author&query=Mittal%2C+K">Ketan Mittal</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+D">Dohyun Kim</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Jiachen Yang</a>, 
<a href="/search/math?searchtype=author&query=Petrides%2C+S">Socratis Petrides</a>, 
<a href="/search/math?searchtype=author&query=Keith%2C+B">Brendan Keith</a>, 
<a href="/search/math?searchtype=author&query=Anderson%2C+R">Robert Anderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We introduce DynAMO, a reinforcement learning paradigm for Dynamic
Anticipatory Mesh Optimization. Adaptive mesh refinement is an effective tool
for optimizing computational cost and solution accuracy in numerical methods
for partial differential equations. However, traditional adaptive mesh
refinement approaches for time-dependent problems typically rely only on
instantaneous error indicators to guide adaptivity. As a result, standard
strategies often require frequent remeshing to maintain accuracy. In the DynAMO
approach, multi-agent reinforcement learning is used to discover new local
refinement policies that can anticipate and respond to future solution states
by producing meshes that deliver more accurate solutions for longer time
intervals. By applying DynAMO to discontinuous Galerkin methods for the linear
advection and compressible Euler equations in two dimensions, we demonstrate
that this new mesh refinement paradigm can outperform conventional
threshold-based strategies while also generalizing to different mesh sizes,
remeshing and simulation times, and initial conditions.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01696" title="Abstract">arXiv:2310.01696</a> [<a href="/pdf/2310.01696" title="Download PDF">pdf</a>, <a href="/format/2310.01696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DANI: Fast Diffusion Aware Network Inference with Preserving Topological  Structure Property
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramezani%2C+M">Maryam Ramezani</a>, 
<a href="/search/cs?searchtype=author&query=Ahadinia%2C+A">Aryan Ahadinia</a>, 
<a href="/search/cs?searchtype=author&query=Farhadi%2C+E">Erfan Farhadi</a>, 
<a href="/search/cs?searchtype=author&query=Rabiee%2C+H+R">Hamid R. Rabiee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/1706.00941">arXiv:1706.00941</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The fast growth of social networks and their data access limitations in
recent years has led to increasing difficulty in obtaining the complete
topology of these networks. However, diffusion information over these networks
is available, and many algorithms have been proposed to infer the underlying
networks using this information. The previously proposed algorithms only focus
on inferring more links and ignore preserving the critical topological
characteristics of the underlying social networks. In this paper, we propose a
novel method called DANI to infer the underlying network while preserving its
structural properties. It is based on the Markov transition matrix derived from
time series cascades, as well as the node-node similarity that can be observed
in the cascade behavior from a structural point of view. In addition, the
presented method has linear time complexity (increases linearly with the number
of nodes, number of cascades, and square of the average length of cascades),
and its distributed version in the MapReduce framework is also scalable. We
applied the proposed approach to both real and synthetic networks. The
experimental results showed that DANI has higher accuracy and lower run time
while maintaining structural properties, including modular structure, degree
distribution, connected components, density, and clustering coefficients, than
well-known network inference methods.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01698" title="Abstract">arXiv:2310.01698</a> [<a href="/pdf/2310.01698" title="Download PDF">pdf</a>, <a href="/format/2310.01698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustifying State-space Models for Long Sequences via Approximate  Diagonalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+A">Annan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Nigmetov%2C+A">Arnur Nigmetov</a>, 
<a href="/search/cs?searchtype=author&query=Morozov%2C+D">Dmitriy Morozov</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Erichson%2C+N+B">N. Benjamin Erichson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">State-space models (SSMs) have recently emerged as a framework for learning
long-range sequence tasks. An example is the structured state-space sequence
(S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO
initialization framework. However, the complicated structure of the S4 layer
poses challenges; and, in an effort to address these challenges, models such as
S4D and S5 have considered a purely diagonal structure. This choice simplifies
the implementation, improves computational efficiency, and allows channel
communication. However, diagonalizing the HiPPO framework is itself an
ill-posed problem. In this paper, we propose a general solution for this and
related ill-posed diagonalization problems in machine learning. We introduce a
generic, backward-stable "perturb-then-diagonalize" (PTD) methodology, which is
based on the pseudospectral theory of non-normal operators, and which may be
interpreted as the approximate diagonalization of the non-normal matrices
defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models.
Through theoretical analysis of the transfer functions of different
initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization
strongly converges to the HiPPO framework, while the S4D/S5 initialization only
achieves weak convergences. As a result, our new models show resilience to
Fourier-mode noise-perturbed inputs, a crucial property not achieved by the
S4D/S5 models. In addition to improved robustness, our S5-PTD model averages
87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD
methodology helps to improve the accuracy of deep learning models.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01701" title="Abstract">arXiv:2310.01701</a> [<a href="/pdf/2310.01701" title="Download PDF">pdf</a>, <a href="/format/2310.01701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcending Domains through Text-to-Image Diffusion: A Source-Free  Approach to Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chopra%2C+S">Shivang Chopra</a>, 
<a href="/search/cs?searchtype=author&query=Kothawade%2C+S">Suraj Kothawade</a>, 
<a href="/search/cs?searchtype=author&query=Aynaou%2C+H">Houda Aynaou</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Domain Adaptation (DA) is a method for enhancing a model's performance on a
target domain with inadequate annotated data by applying the information the
model has acquired from a related source domain with sufficient labeled data.
The escalating enforcement of data-privacy regulations like HIPAA, COPPA,
FERPA, etc. have sparked a heightened interest in adapting models to novel
domains while circumventing the need for direct access to the source data, a
problem known as Source-Free Domain Adaptation (SFDA). In this paper, we
propose a novel framework for SFDA that generates source data using a
text-to-image diffusion model trained on the target domain samples. Our method
starts by training a text-to-image diffusion model on the labeled target domain
samples, which is then fine-tuned using the pre-trained source model to
generate samples close to the source data. Finally, we use Domain Adaptation
techniques to align the artificially generated source data with the target
domain data, resulting in significant performance improvements of the model on
the target domain. Through extensive comparison against several baselines on
the standard Office-31, Office-Home, and VisDA benchmarks, we demonstrate the
effectiveness of our approach for the SFDA task.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01704" title="Abstract">arXiv:2310.01704</a> [<a href="/pdf/2310.01704" title="Download PDF">pdf</a>, <a href="/format/2310.01704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers are efficient hierarchical chemical graph learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pengmei%2C+Z">Zihan Pengmei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zimu Li</a>, 
<a href="/search/cs?searchtype=author&query=Tien%2C+C">Chih-chan Tien</a>, 
<a href="/search/cs?searchtype=author&query=Kondor%2C+R">Risi Kondor</a>, 
<a href="/search/cs?searchtype=author&query=Dinner%2C+A+R">Aaron R. Dinner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Transformers, adapted from natural language processing, are emerging as a
leading approach for graph representation learning. Contemporary graph
transformers often treat nodes or edges as separate tokens. This approach leads
to computational challenges for even moderately-sized graphs due to the
quadratic scaling of self-attention complexity with token count. In this paper,
we introduce SubFormer, a graph transformer that operates on subgraphs that
aggregate information by a message-passing mechanism. This approach reduces the
number of tokens and enhances learning long-range interactions. We demonstrate
SubFormer on benchmarks for predicting molecular properties from chemical
structures and show that it is competitive with state-of-the-art graph
transformers at a fraction of the computational cost, with training times on
the order of minutes on a consumer-grade graphics card. We interpret the
attention weights in terms of chemical structures. We show that SubFormer
exhibits limited over-smoothing and avoids over-squashing, which is prevalent
in traditional graph neural networks.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01706" title="Abstract">arXiv:2310.01706</a> [<a href="/pdf/2310.01706" title="Download PDF">pdf</a>, <a href="/format/2310.01706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Representation Complexity of Model-based and Model-free Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baihe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+S">Stuart Russell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the representation complexity of model-based and model-free
reinforcement learning (RL) in the context of circuit complexity. We prove
theoretically that there exists a broad class of MDPs such that their
underlying transition and reward functions can be represented by constant depth
circuits with polynomial size, while the optimal $Q$-function suffers an
exponential circuit complexity in constant-depth circuits. By drawing attention
to the approximation errors and building connections to complexity theory, our
theory provides unique insights into why model-based algorithms usually enjoy
better sample complexity than model-free algorithms from a novel representation
complexity perspective: in some cases, the ground-truth rule (model) of the
environment is simple to represent, while other quantities, such as
$Q$-function, appear complex. We empirically corroborate our theory by
comparing the approximation error of the transition kernel, reward function,
and optimal $Q$-function in various Mujoco environments, which demonstrates
that the approximation errors of the transition kernel and reward function are
consistently lower than those of the optimal $Q$-function. To the best of our
knowledge, this work is the first to study the circuit complexity of RL, which
also provides a rigorous framework for future research.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01708" title="Abstract">arXiv:2310.01708</a> [<a href="/pdf/2310.01708" title="Download PDF">pdf</a>, <a href="/format/2310.01708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciphering Diagnoses: How Large Language Models Explanations Influence  Clinical Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Umerenkov%2C+D">D.Umerenkov</a>, 
<a href="/search/cs?searchtype=author&query=Zubkova%2C+G">G.Zubkova</a>, 
<a href="/search/cs?searchtype=author&query=Nesterov%2C+A">A.Nesterov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and
patient data to offer real-time recommendations, with Large Language Models
(LLMs) emerging as a promising tool to generate plain-text explanations for
medical decisions. This study explores the effectiveness and reliability of
LLMs in generating explanations for diagnoses based on patient complaints.
Three experienced doctors evaluated LLM-generated explanations of the
connection between patient complaints and doctor and model-assigned diagnoses
across several stages. Experimental results demonstrated that LLM explanations
significantly increased doctors' agreement rates with given diagnoses and
highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study
underscores the potential and challenges of LLMs in healthcare and emphasizes
the need for careful integration and evaluation to ensure patient safety and
optimal clinical utility.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01711" title="Abstract">arXiv:2310.01711</a> [<a href="/pdf/2310.01711" title="Download PDF">pdf</a>, <a href="/format/2310.01711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Class-Specific Spectral Patterns to Improve Deep Learning Based  Scene-Level Fire Smoke Detection from Multi-Spectral Satellite Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jixue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+S">Stefan Peters</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiuyong Li</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+N">Norman Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Oliver%2C+S">Simon Oliver</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Detecting fire smoke is crucial for the timely identification of early
wildfires using satellite imagery. However, the spatial and spectral similarity
of fire smoke to other confounding aerosols, such as clouds and haze, often
confuse even the most advanced deep-learning (DL) models. Nonetheless, these
aerosols also present distinct spectral characteristics in some specific bands,
and such spectral patterns are useful for distinguishing the aerosols more
accurately. Early research tried to derive various threshold values from the
reflectance and brightness temperature in specific spectral bands to
differentiate smoke and cloud pixels. However, such threshold values were
determined based on domain knowledge and are hard to generalise. In addition,
such threshold values were manually derived from specific combinations of bands
to infer spectral patterns, making them difficult to employ in deep-learning
models. In this paper, we introduce a DL module called input amplification
(InAmp) which is designed to enable DL models to learn class-specific spectral
patterns automatically from multi-spectral satellite imagery and improve the
fire smoke detection accuracy. InAmp can be conveniently integrated with
different DL architectures. We evaluate the effectiveness of the InAmp module
on different Convolutional neural network (CNN) architectures using two
satellite imagery datasets: USTC_SmokeRS, derived from Moderate Resolution
Imaging Spectroradiometer (MODIS) with three spectral bands; and Landsat_Smk,
derived from Landsat 5/8 with six spectral bands. Our experimental results
demonstrate that the InAmp module improves the fire smoke detection accuracy of
the CNN models. Additionally, we visualise the spectral patterns extracted by
the InAmp module using test imagery and demonstrate that the InAmp module can
effectively extract class-specific spectral patterns.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01712" title="Abstract">arXiv:2310.01712</a> [<a href="/pdf/2310.01712" title="Download PDF">pdf</a>, <a href="/format/2310.01712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Autoencoding of Dropout Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maeda%2C+S">Shunta Maeda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose a generative model termed Deciphering Autoencoders. In this model,
we assign a unique random dropout pattern to each data point in the training
dataset and then train an autoencoder to reconstruct the corresponding data
point using this pattern as information to be encoded. Since the training of
Deciphering Autoencoders relies solely on reconstruction error, it offers more
stable training than other generative models. Despite its simplicity,
Deciphering Autoencoders show comparable sampling quality to DCGAN on the
CIFAR-10 dataset.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01713" title="Abstract">arXiv:2310.01713</a> [<a href="/pdf/2310.01713" title="Download PDF">pdf</a>, <a href="/format/2310.01713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Greedy invariant-domain preserving approximation for hyperbolic systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guermond%2C+J">Jean-Luc Guermond</a>, 
<a href="/search/math?searchtype=author&query=Maier%2C+M">Matthias Maier</a>, 
<a href="/search/math?searchtype=author&query=Popov%2C+B">Bojan Popov</a>, 
<a href="/search/math?searchtype=author&query=Saavedra%2C+L">Laura Saavedra</a>, 
<a href="/search/math?searchtype=author&query=Tomas%2C+I">Ignacio Tomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The paper focuses on invariant-domain preserving approximations of hyperbolic
systems. We propose a new way to estimate the artificial viscosity that has to
be added to make explicit, conservative, consistent numerical methods
invariant-domain preserving and entropy inequality compliant. Instead of
computing an upper bound on the maximum wave speed in Riemann problems, we
estimate a minimum wave speed in the said Riemann problems such that the
approximation satisfies predefined invariant-domain properties and predefined
entropy inequalities. This technique eliminates non-essential fast waves from
the construction of the artificial viscosity, while preserving pre-assigned
invariant-domain properties and entropy inequalities.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01714" title="Abstract">arXiv:2310.01714</a> [<a href="/pdf/2310.01714" title="Download PDF">pdf</a>, <a href="/format/2310.01714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Analogical Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yasunaga%2C+M">Michihiro Yasunaga</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yujia Li</a>, 
<a href="/search/cs?searchtype=author&query=Pasupat%2C+P">Panupong Pasupat</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Chain-of-thought (CoT) prompting for language models demonstrates impressive
performance across reasoning tasks, but typically needs labeled exemplars of
the reasoning process. In this work, we introduce a new prompting approach,
Analogical Prompting, designed to automatically guide the reasoning process of
large language models. Inspired by analogical reasoning, a cognitive process in
which humans draw from relevant past experiences to tackle new problems, our
approach prompts language models to self-generate relevant exemplars or
knowledge in the context, before proceeding to solve the given problem. This
method presents several advantages: it obviates the need for labeling or
retrieving exemplars, offering generality and convenience; it can also tailor
the generated exemplars and knowledge to each problem, offering adaptability.
Experimental results show that our approach outperforms 0-shot CoT and manual
few-shot CoT in a variety of reasoning tasks, including math problem solving in
GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in
BIG-Bench.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01717" title="Abstract">arXiv:2310.01717</a> [<a href="/pdf/2310.01717" title="Download PDF">pdf</a>, <a href="/format/2310.01717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble Distillation for Unsupervised Constituency Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shayegh%2C+B">Behzad Shayegh</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yanshuai Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaodan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+J+C+K">Jackie C.K. Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+L">Lili Mou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate the unsupervised constituency parsing task, which organizes
words and phrases of a sentence into a hierarchical structure without using
linguistically annotated data. We observe that existing unsupervised parsers
capture differing aspects of parsing structures, which can be leveraged to
enhance unsupervised parsing performance. To this end, we propose a notion of
"tree averaging," based on which we further propose a novel ensemble method for
unsupervised parsing. To improve inference efficiency, we further distill the
ensemble knowledge into a student model; such an ensemble-then-distill process
is an effective approach to mitigate the over-smoothing problem existing in
common multi-teacher distilling methods. Experiments show that our method
surpasses all previous approaches, consistently demonstrating its effectiveness
and robustness across various runs, with different ensemble components, and
under domain-shift conditions.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01719" title="Abstract">arXiv:2310.01719</a> [<a href="/pdf/2310.01719" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software Testing and Code Refactoring: A Survey with Practitioners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lima%2C+D+L">Danilo Leandro Lima</a>, 
<a href="/search/cs?searchtype=author&query=de+Souza+Santos%2C+R">Ronnie de Souza Santos</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+G+P">Guilherme Pires Garcia</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+S+S">Sildemir S. da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Franca%2C+C">Cesar Franca</a>, 
<a href="/search/cs?searchtype=author&query=Capretz%2C+L+F">Luiz Fernando Capretz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Nowadays, software testing professionals are commonly required to develop
coding skills to work on test automation. One essential skill required from
those who code is the ability to implement code refactoring, a valued quality
aspect of software development; however, software developers usually encounter
obstacles in successfully applying this practice. In this scenario, the present
study aims to explore how software testing professionals (e.g., software
testers, test engineers, test analysts, and software QAs) deal with code
refactoring to understand the benefits and limitations of this practice in the
context of software testing. We followed the guidelines to conduct surveys in
software engineering and applied three sampling techniques, namely convenience
sampling, purposive sampling, and snowballing sampling, to collect data from
testing professionals. We received answers from 80 individuals reporting their
experience refactoring the code of automated tests. We concluded that in the
context of software testing, refactoring offers several benefits, such as
supporting the maintenance of automated tests and improving the performance of
the testing team. However, practitioners might encounter barriers in
effectively implementing this practice, in particular, the lack of interest
from managers and leaders. Our study raises discussions on the importance of
having testing professionals implement refactoring in the code of automated
tests, allowing them to improve their coding abilities.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01720" title="Abstract">arXiv:2310.01720</a> [<a href="/pdf/2310.01720" title="Download PDF">pdf</a>, <a href="/format/2310.01720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PrACTiS: Perceiver-Attentional Copulas for Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+C+P">Cat P. Le</a>, 
<a href="/search/cs?searchtype=author&query=Cannella%2C+C">Chris Cannella</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+A">Ali Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+Y">Yuting Ng</a>, 
<a href="/search/cs?searchtype=author&query=Tarokh%2C+V">Vahid Tarokh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformers incorporating copula structures have demonstrated remarkable
performance in time series prediction. However, their heavy reliance on
self-attention mechanisms demands substantial computational resources, thus
limiting their practical utility across a wide range of tasks. In this work, we
present a model that combines the perceiver architecture with a copula
structure to enhance time-series forecasting. By leveraging the perceiver as
the encoder, we efficiently transform complex, high-dimensional, multimodal
data into a compact latent space, thereby significantly reducing computational
demands. To further reduce complexity, we introduce midpoint inference and
local attention mechanisms, enabling the model to capture dependencies within
imputed samples effectively. Subsequently, we deploy the copula-based attention
and output variance testing mechanism to capture the joint distribution of
missing data, while simultaneously mitigating error propagation during
prediction. Our experimental results on the unimodal and multimodal benchmarks
showcase a consistent 20\% improvement over the state-of-the-art methods, while
utilizing less than half of available memory resources.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01723" title="Abstract">arXiv:2310.01723</a> [<a href="/pdf/2310.01723" title="Download PDF">pdf</a>, <a href="/format/2310.01723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Future Spatiotemporal Occupancy Grids with Semantics for  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toyungyernsub%2C+M">Maneekwan Toyungyernsub</a>, 
<a href="/search/cs?searchtype=author&query=Yel%2C+E">Esen Yel</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiachen Li</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">For autonomous vehicles to proactively plan safe trajectories and make
informed decisions, they must be able to predict the future occupancy states of
the local environment. However, common issues with occupancy prediction include
predictions where moving objects vanish or become blurred, particularly at
longer time horizons. We propose an environment prediction framework that
incorporates environment semantics for future occupancy prediction. Our method
first semantically segments the environment and uses this information along
with the occupancy information to predict the spatiotemporal evolution of the
environment. We validate our approach on the real-world Waymo Open Dataset.
Compared to baseline methods, our model has higher prediction accuracy and is
capable of maintaining moving object appearances in the predictions for longer
prediction time horizons.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01726" title="Abstract">arXiv:2310.01726</a> [<a href="/pdf/2310.01726" title="Download PDF">pdf</a>, <a href="/format/2310.01726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Test-Free Fault Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+A+Z+H">Aidan Z.H. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+R">Ruben Martins</a>, 
<a href="/search/cs?searchtype=author&query=Goues%2C+C+L">Claire Le Goues</a>, 
<a href="/search/cs?searchtype=author&query=Hellendoorn%2C+V+J">Vincent J. Hellendoorn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Fault Localization (FL) aims to automatically localize buggy lines of code, a
key first step in many manual and automatic debugging tasks. Previous FL
techniques assume the provision of input tests, and often require extensive
program analysis, program instrumentation, or data preprocessing. Prior work on
deep learning for APR struggles to learn from small datasets and produces
limited results on real-world programs. Inspired by the ability of large
language models (LLMs) of code to adapt to new tasks based on very few
examples, we investigate the applicability of LLMs to line level fault
localization. Specifically, we propose to overcome the left-to-right nature of
LLMs by fine-tuning a small set of bidirectional adapter layers on top of the
representations learned by LLMs to produce LLMAO, the first language model
based fault localization approach that locates buggy lines of code without any
test coverage information. We fine-tune LLMs with 350 million, 6 billion, and
16 billion parameters on small, manually curated corpora of buggy programs such
as the Defects4J corpus. We observe that our technique achieves substantially
more confidence in fault localization when built on the larger models, with bug
localization performance scaling consistently with the LLM size. Our empirical
evaluation shows that LLMAO improves the Top-1 results over the
state-of-the-art machine learning fault localization (MLFL) baselines by
2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL
technique trained using a language model architecture that can detect security
vulnerabilities down to the code line level.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01727" title="Abstract">arXiv:2310.01727</a> [<a href="/pdf/2310.01727" title="Download PDF">pdf</a>, <a href="/format/2310.01727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can GPT-4 Replicate Empirical Software Engineering Research?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J+T">Jenny T. Liang</a>, 
<a href="/search/cs?searchtype=author&query=Badea%2C+C">Carmen Badea</a>, 
<a href="/search/cs?searchtype=author&query=Bird%2C+C">Christian Bird</a>, 
<a href="/search/cs?searchtype=author&query=DeLine%2C+R">Robert DeLine</a>, 
<a href="/search/cs?searchtype=author&query=Ford%2C+D">Denae Ford</a>, 
<a href="/search/cs?searchtype=author&query=Forsgren%2C+N">Nicole Forsgren</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+T">Thomas Zimmermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Empirical software engineering research on production systems has brought
forth a better understanding of the software engineering process for
practitioners and researchers alike. However, only a small subset of production
systems is studied, limiting the impact of this research. While software
engineering practitioners benefit from replicating research on their own data,
this poses its own set of challenges, since performing replications requires a
deep understanding of research methodologies and subtle nuances in software
engineering data. Given that large language models (LLMs), such as GPT-4, show
promise in tackling both software engineering- and science-related tasks, these
models could help democratize empirical software engineering research.
<br />In this paper, we examine LLMs' abilities to perform replications of
empirical software engineering research on new data. We specifically study
their ability to surface assumptions made in empirical software engineering
research methodologies, as well as their ability to plan and generate code for
analysis pipelines on seven empirical software engineering papers. We perform a
user study with 14 participants with software engineering research expertise,
who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of
module specifications) from the papers. We find that GPT-4 is able to surface
correct assumptions, but struggle to generate ones that reflect common
knowledge about software engineering data. In a manual analysis of the
generated code, we find that the GPT-4-generated code contains the correct
high-level logic, given a subset of the methodology. However, the code contains
many small implementation-level errors, reflecting a lack of software
engineering knowledge. Our findings have implications for leveraging LLMs for
software engineering research as well as practitioner data scientists in
software teams.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01728" title="Abstract">arXiv:2310.01728</a> [<a href="/pdf/2310.01728" title="Download PDF">pdf</a>, <a href="/format/2310.01728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-LLM: Time Series Forecasting by Reprogramming Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lintao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+Y">James Y. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaoming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time series forecasting holds significant importance in many real-world
dynamic systems and has been extensively studied. Unlike natural language
process (NLP) and computer vision (CV), where a single large model can tackle
multiple tasks, models for time series forecasting are often specialized,
necessitating distinct designs for different tasks and applications. While
pre-trained foundation models have made impressive strides in NLP and CV, their
development in time series domains has been constrained by data sparsity.
Recent studies have revealed that large language models (LLMs) possess robust
pattern recognition and reasoning abilities over complex sequences of tokens.
However, the challenge remains in effectively aligning the modalities of time
series data and natural language to leverage these capabilities. In this work,
we present Time-LLM, a reprogramming framework to repurpose LLMs for general
time series forecasting with the backbone language models kept intact. We begin
by reprogramming the input time series with text prototypes before feeding it
into the frozen LLM to align the two modalities. To augment the LLM's ability
to reason with time series data, we propose Prompt-as-Prefix (PaP), which
enriches the input context and directs the transformation of reprogrammed input
patches. The transformed time series patches from the LLM are finally projected
to obtain the forecasts. Our comprehensive evaluations demonstrate that
Time-LLM is a powerful time series learner that outperforms state-of-the-art,
specialized forecasting models. Moreover, Time-LLM excels in both few-shot and
zero-shot learning scenarios.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01729" title="Abstract">arXiv:2310.01729</a> [<a href="/pdf/2310.01729" title="Download PDF">pdf</a>, <a href="/format/2310.01729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Correction for DNA Storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sima%2C+J">Jin Sima</a>, 
<a href="/search/cs?searchtype=author&query=Raviv%2C+N">Netanel Raviv</a>, 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+M">Moshe Schwartz</a>, 
<a href="/search/cs?searchtype=author&query=Bruck%2C+J">Jehoshua Bruck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">DNA-based storage is an emerging storage technology that provides high
information density and long duration. Due to the physical constraints in the
reading and writing processes, error correction in DNA storage poses several
interesting coding theoretic challenges, some of which are new. In this paper,
we give a brief introduction to some of the coding challenges for DNA-based
storage, including deletion/insertion correcting codes, codes over sliced
channels, and duplication correcting codes.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01732" title="Abstract">arXiv:2310.01732</a> [<a href="/pdf/2310.01732" title="Download PDF">pdf</a>, <a href="/format/2310.01732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nugget: Neural Agglomerative Embeddings of Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+G">Guanghui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Van+Durme%2C+B">Benjamin Van Durme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared at ICML 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Embedding text sequences is a widespread requirement in modern language
understanding. Existing approaches focus largely on constant-size
representations. This is problematic, as the amount of information contained in
text often varies with the length of the input. We propose a solution called
Nugget, which encodes language into a representation based on a dynamically
selected subset of input tokens. These nuggets are learned through tasks like
autoencoding and machine translation, and intuitively segment language into
meaningful units. We demonstrate Nugget outperforms related approaches in tasks
involving semantic comparison. Finally, we illustrate these compact units allow
for expanding the contextual window of a language model (LM), suggesting new
future LMs that can condition on significantly larger amounts of content.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01733" title="Abstract">arXiv:2310.01733</a> [<a href="/pdf/2310.01733" title="Download PDF">pdf</a>, <a href="/format/2310.01733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Health Guardian: Using Multi-modal Data to Understand Individual Health
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Siu%2C+V+S">Vince S. Siu</a>, 
<a href="/search/eess?searchtype=author&query=Hsieh%2C+K+Y">Kuan Yu Hsieh</a>, 
<a href="/search/eess?searchtype=author&query=Buleje%2C+I">Italo Buleje</a>, 
<a href="/search/eess?searchtype=author&query=Itoh%2C+T">Takashi Itoh</a>, 
<a href="/search/eess?searchtype=author&query=Hao%2C+T">Tian Hao</a>, 
<a href="/search/eess?searchtype=author&query=Civjan%2C+B">Ben Civjan</a>, 
<a href="/search/eess?searchtype=author&query=Hinds%2C+N">Nigel Hinds</a>, 
<a href="/search/eess?searchtype=author&query=Dang%2C+B">Bing Dang</a>, 
<a href="/search/eess?searchtype=author&query=Rogers%2C+J+L">Jeffrey L. Rogers</a>, 
<a href="/search/eess?searchtype=author&query=Wen%2C+B">Bo Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Conference on Digital Health (ICDH), 2023, pp.
  65-74
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Artificial intelligence (AI) has shown great promise in revolutionizing the
field of digital health by improving disease diagnosis, treatment, and
prevention. This paper describes the Health Guardian platform, a
non-commercial, scientific research-based platform developed by the IBM Digital
Health team to rapidly translate AI research into cloud-based microservices.
The platform can collect health-related data from various digital devices,
including wearables and mobile applications. Its flexible architecture supports
microservices that accept diverse data types such as text, audio, and video,
expanding the range of digital health assessments and enabling holistic health
evaluations by capturing voice, facial, and motion bio-signals. These
microservices can be deployed to a clinical cohort specified through the
Clinical Task Manager (CTM). The CTM then collects multi-modal, clinical data
that can iteratively improve the accuracy of AI predictive models, discover new
disease mechanisms, or identify novel biomarkers. This paper highlights three
microservices with different input data types, including a text-based
microservice for depression assessment, a video-based microservice for
sit-to-stand mobility assessment, and a wearable-based microservice for
functional mobility assessment. The CTM is also discussed as a tool to help
design and set up clinical studies to unlock the full potential of the
platform. Today, the Health Guardian platform is being leveraged in
collaboration with research partners to optimize the development of AI models
by utilizing a multitude of input sources. This approach streamlines research
efforts, enhances efficiency, and facilitates the development and validation of
digital health applications.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01735" title="Abstract">arXiv:2310.01735</a> [<a href="/pdf/2310.01735" title="Download PDF">pdf</a>, <a href="/format/2310.01735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Expected Appearances for Intraoperative Registration during  Neurosurgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haouchine%2C+N">Nazim Haouchine</a>, 
<a href="/search/cs?searchtype=author&query=Dorent%2C+R">Reuben Dorent</a>, 
<a href="/search/cs?searchtype=author&query=Juvekar%2C+P">Parikshit Juvekar</a>, 
<a href="/search/cs?searchtype=author&query=Torio%2C+E">Erickson Torio</a>, 
<a href="/search/cs?searchtype=author&query=Wells%2C+W+M">William M. Wells III</a>, 
<a href="/search/cs?searchtype=author&query=Kapur%2C+T">Tina Kapur</a>, 
<a href="/search/cs?searchtype=author&query=Golby%2C+A+J">Alexandra J. Golby</a>, 
<a href="/search/cs?searchtype=author&query=Frisken%2C+S">Sarah Frisken</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a novel method for intraoperative patient-to-image registration by
learning Expected Appearances. Our method uses preoperative imaging to
synthesize patient-specific expected views through a surgical microscope for a
predicted range of transformations. Our method estimates the camera pose by
minimizing the dissimilarity between the intraoperative 2D view through the
optical microscope and the synthesized expected texture. In contrast to
conventional methods, our approach transfers the processing tasks to the
preoperative stage, reducing thereby the impact of low-resolution, distorted,
and noisy intraoperative images, that often degrade the registration accuracy.
We applied our method in the context of neuronavigation during brain surgery.
We evaluated our approach on synthetic data and on retrospective data from 6
clinical cases. Our method outperformed state-of-the-art methods and achieved
accuracies that met current clinical standards.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01737" title="Abstract">arXiv:2310.01737</a> [<a href="/pdf/2310.01737" title="Download PDF">pdf</a>, <a href="/format/2310.01737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blending Imitation and Reinforcement Learning for Robust Policy  Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuefeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Takuma Yoneda</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+R+L">Rick L. Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+M+R">Matthew R. Walter</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">While reinforcement learning (RL) has shown promising performance, its sample
complexity continues to be a substantial hurdle, restricting its broader
application across a variety of domains. Imitation learning (IL) utilizes
oracles to improve sample efficiency, yet it is often constrained by the
quality of the oracles deployed. which actively interleaves between IL and RL
based on an online estimate of their performance. RPI draws on the strengths of
IL, using oracle queries to facilitate exploration, an aspect that is notably
challenging in sparse-reward RL, particularly during the early stages of
learning. As learning unfolds, RPI gradually transitions to RL, effectively
treating the learned policy as an improved oracle. This algorithm is capable of
learning from and improving upon a diverse set of black-box oracles. Integral
to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient
(RPG), both of which reason over whether to perform state-wise imitation from
the oracles or learn from its own value function when the learner's performance
surpasses that of the oracles in a specific state. Empirical evaluations and
theoretical analysis validate that RPI excels in comparison to existing
state-of-the-art methodologies, demonstrating superior performance across
various benchmark domains.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01738" title="Abstract">arXiv:2310.01738</a> [<a href="/pdf/2310.01738" title="Download PDF">pdf</a>, <a href="/format/2310.01738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RETRO: Reactive Trajectory Optimization for Real-Time Robot Motion  Planning in Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dastider%2C+A">Apan Dastider</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Hao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Mingjie Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review in IEEE ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Reactive trajectory optimization for robotics presents formidable challenges,
demanding the rapid generation of purposeful robot motion in complex and
swiftly changing dynamic environments. While much existing research
predominantly addresses robotic motion planning with predefined objectives,
emerging problems in robotic trajectory optimization frequently involve
dynamically evolving objectives and stochastic motion dynamics. However,
effectively addressing such reactive trajectory optimization challenges for
robot manipulators proves difficult due to inefficient, high-dimensional
trajectory representations and a lack of consideration for time optimization.
<br />In response, we introduce a novel trajectory optimization framework called
RETRO. RETRO employs adaptive optimization techniques that span both spatial
and temporal dimensions. As a result, it achieves a remarkable computing
complexity of $O(T^{2.4}) + O(Tn^{2})$, a significant improvement over the
traditional application of DDP, which leads to a complexity of $O(n^{4})$ when
reasonable time step sizes are used. To evaluate RETRO's performance in terms
of error, we conducted a comprehensive analysis of its regret bounds, comparing
it to an Oracle value function obtained through an Oracle trajectory
optimization algorithm. Our analytical findings demonstrate that RETRO's total
regret can be upper-bounded by a function of the chosen time step size.
Moreover, our approach delivers smoothly optimized robot trajectories within
the joint space, offering flexibility and adaptability for various tasks. It
can seamlessly integrate task-specific requirements such as collision avoidance
while maintaining real-time control rates. We validate the effectiveness of our
framework through extensive simulations and real-world robot experiments in
closed-loop manipulation scenarios.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01739" title="Abstract">arXiv:2310.01739</a> [<a href="/pdf/2310.01739" title="Download PDF">pdf</a>, <a href="/format/2310.01739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Dimension Reduction with Statistical Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yijun Dong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ph.D. dissertation (University of Texas at Austin)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Large models and enormous data are essential driving forces of the
unprecedented successes achieved by modern algorithms, especially in scientific
computing and machine learning. Nevertheless, the growing dimensionality and
model complexity, as well as the non-negligible workload of data
pre-processing, also bring formidable costs to such successes in both
computation and data aggregation. As the deceleration of Moore's Law slackens
the cost reduction of computation from the hardware level, fast heuristics for
expensive classical routines and efficient algorithms for exploiting limited
data are increasingly indispensable for pushing the limit of algorithm potency.
This thesis explores some of such algorithms for fast execution and efficient
data utilization.
<br />From the computational efficiency perspective, we design and analyze fast
randomized low-rank decomposition algorithms for large matrices based on
"matrix sketching", which can be regarded as a dimension reduction strategy in
the data space. These include the randomized pivoting-based interpolative and
CUR decomposition discussed in Chapter 2 and the randomized subspace
approximations discussed in Chapter 3.
<br />From the sample efficiency perspective, we focus on learning algorithms with
various incorporations of data augmentation that improve generalization and
distributional robustness provably. Specifically, Chapter 4 presents a sample
complexity analysis for data augmentation consistency regularization where we
view sample efficiency from the lens of dimension reduction in the function
space. Then in Chapter 5, we introduce an adaptively weighted data augmentation
consistency regularization algorithm for distributionally robust optimization
with applications in medical image segmentation.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01740" title="Abstract">arXiv:2310.01740</a> [<a href="/pdf/2310.01740" title="Download PDF">pdf</a>, <a href="/format/2310.01740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control of Soft Pneumatic Actuators with Approximated Dynamical Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wu-Te Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kurkcu%2C+B">Burak Kurkcu</a>, 
<a href="/search/cs?searchtype=author&query=Hirao%2C+M">Motohiro Hirao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lingfeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+G+X">Grace X. Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, accepted by 2023 IEEE ROBIO conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper introduces a full system modeling strategy for a syringe pump and
soft pneumatic actuators(SPAs). The soft actuator is conceptualized as a beam
structure, utilizing a second-order bending model. The equation of natural
frequency is derived from Euler's bending theory, while the damping ratio is
estimated by fitting step responses of soft pneumatic actuators. Evaluation of
model uncertainty underscores the robustness of our modeling methodology. To
validate our approach, we deploy it across four prototypes varying in
dimensional parameters. Furthermore, a syringe pump is designed to drive the
actuator, and a pressure model is proposed to construct a full system model. By
employing this full system model, the Linear-Quadratic Regulator (LQR)
controller is implemented to control the soft actuator, achieving high-speed
responses and high accuracy in both step response and square wave function
response tests. Both the modeling method and the LQR controller are thoroughly
evaluated through experiments. Lastly, a gripper, consisting of two actuators
with a feedback controller, demonstrates stable grasping of delicate objects
with a significantly higher success rate.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01742" title="Abstract">arXiv:2310.01742</a> [<a href="/pdf/2310.01742" title="Download PDF">pdf</a>, <a href="/format/2310.01742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Reflecting Surface Aided MIMO Networks: Distributed or  Centralized Architecture?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yanzhao Hou</a>, 
<a href="/search/cs?searchtype=author&query=Jian%2C+M">Mengnan Jian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We investigate the capacity of a broadcast channel with a multi-antenna base
station (BS) sending independent messages to multiple users, aided by IRSs with
N elements. In particular, both the distributed and centralized IRS deployment
architectures are considered. Regarding the distributed IRS, the N IRS elements
form multiple IRSs and each of them is installed near a user cluster; while for
the centralized IRS, all IRS elements are located in the vicinity of the BS. To
draw essential insights, we first derive the maximum capacity achieved by the
distributed IRS and centralized IRS, respectively, under the assumption of
line-of-sight propagation and homogeneous channel setups. By capturing the
fundamental tradeoff between the spatial multiplexing gain and passive
beamforming gain, we rigourously prove that the capacity of the distributed IRS
is higher than that of the centralized IRS provided that the total number of
IRS elements is above a threshold. Motivated by the superiority of the
distributed IRS, we then focus on the transmission and element allocation
design under the distributed IRS. By exploiting the user channel correlation of
intra-clusters and inter-clusters, an efficient hybrid multiple access scheme
relying on both spatial and time domains is proposed to fully exploit both the
passive beamforming gain and spatial DoF. Moreover, the IRS element allocation
problem is investigated for the objectives of sum-rate maximization and minimum
user rate maximization, respectively. Finally, extensive numerical results are
provided to validate our theoretical finding and also to unveil the
effectiveness of the distributed IRS for improving the system capacity under
various system setups.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01745" title="Abstract">arXiv:2310.01745</a> [<a href="/pdf/2310.01745" title="Download PDF">pdf</a>, <a href="/format/2310.01745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Volumetric Approach to Monge&#x27;s Optimal Transport on Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tsai%2C+R">Richard Tsai</a>, 
<a href="/search/math?searchtype=author&query=Turnquist%2C+A+G+R">Axel G. R. Turnquist</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a volumetric formulation for computing the Optimal Transport
problem defined on surfaces in $\mathbb{R}^3$, found in disciplines like
optics, computer graphics, and computational methodologies. Instead of directly
tackling the original problem on the surface, we define a new Optimal Transport
problem on a thin tubular region, $T_{\epsilon}$, adjacent to the surface. This
extension offers enhanced flexibility and simplicity for numerical
discretization on Cartesian grids. The Optimal Transport mapping and potential
function computed on $T_{\epsilon}$ are consistent with the original problem on
surfaces. We demonstrate that, with the proposed volumetric approach, it is
possible to use simple and straightforward numerical methods to solve Optimal
Transport for $\Gamma = \mathbb{S}^2$.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01747" title="Abstract">arXiv:2310.01747</a> [<a href="/pdf/2310.01747" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 5G Network Slicing: Analysis of Multiple Machine Learning Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malkoc%2C+M">Mirsad Malkoc</a>, 
<a href="/search/cs?searchtype=author&query=Kholidy%2C+H+A">Hisham A. Kholidy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The division of one physical 5G communications infrastructure into several
virtual network slices with distinct characteristics such as bandwidth,
latency, reliability, security, and service quality is known as 5G network
slicing. Each slice is a separate logical network that meets the requirements
of specific services or use cases, such as virtual reality, gaming, autonomous
vehicles, or industrial automation. The network slice can be adjusted
dynamically to meet the changing demands of the service, resulting in a more
cost-effective and efficient approach to delivering diverse services and
applications over a shared infrastructure. This paper assesses various machine
learning techniques, including the logistic regression model, linear
discriminant model, k-nearest neighbor's model, decision tree model, random
forest model, SVC BernoulliNB model, and GaussianNB model, to investigate the
accuracy and precision of each model on detecting network slices. The report
also gives an overview of 5G network slicing.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01749" title="Abstract">arXiv:2310.01749</a> [<a href="/pdf/2310.01749" title="Download PDF">pdf</a>, <a href="/format/2310.01749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stack Attention: Improving the Ability of Transformers to Model  Hierarchical Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DuSell%2C+B">Brian DuSell</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+D">David Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Attention, specifically scaled dot-product attention, has proven effective
for natural language, but it does not have a mechanism for handling
hierarchical patterns of arbitrary nesting depth, which limits its ability to
recognize certain syntactic structures. To address this shortcoming, we propose
stack attention: an attention operator that incorporates stacks, inspired by
their theoretical connections to context-free languages (CFLs). We show that
stack attention is analogous to standard attention, but with a latent model of
syntax that requires no syntactic supervision. We propose two variants: one
related to deterministic pushdown automata (PDAs) and one based on
nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.
We show that transformers with stack attention are very effective at learning
CFLs that standard transformers struggle on, achieving strong results on a CFL
with theoretically maximal parsing difficulty. We also show that stack
attention is more effective at natural language modeling under a constrained
parameter budget, and we include results on machine translation.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01753" title="Abstract">arXiv:2310.01753</a> [<a href="/pdf/2310.01753" title="Download PDF">pdf</a>, <a href="/format/2310.01753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CausalTime: Realistically Generated Time-series for Benchmarking of  Causal Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuxiao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tingxiong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Suo%2C+J">Jinli Suo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kunlun He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Time-series causal discovery (TSCD) is a fundamental problem of machine
learning. However, existing synthetic datasets cannot properly evaluate or
predict the algorithms' performance on real data. This study introduces the
CausalTime pipeline to generate time-series that highly resemble the real data
and with ground truth causal graphs for quantitative performance evaluation.
The pipeline starts from real observations in a specific scenario and produces
a matching benchmark dataset. Firstly, we harness deep neural networks along
with normalizing flow to accurately capture realistic dynamics. Secondly, we
extract hypothesized causal graphs by performing importance analysis on the
neural network or leveraging prior knowledge. Thirdly, we derive the ground
truth causal graphs by splitting the causal model into causal term, residual
term, and noise term. Lastly, using the fitted network and the derived causal
graph, we generate corresponding versatile time-series proper for algorithm
assessment. In the experiments, we validate the fidelity of the generated data
through qualitative and quantitative experiments, followed by a benchmarking of
existing TSCD algorithms using these generated datasets. CausalTime offers a
feasible solution to evaluating TSCD algorithms in real applications and can be
generalized to a wide range of fields. For easy use of the proposed approach,
we also provide a user-friendly website, hosted on www.causaltime.cc.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01755" title="Abstract">arXiv:2310.01755</a> [<a href="/pdf/2310.01755" title="Download PDF">pdf</a>, <a href="/format/2310.01755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">William Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Byron Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Russakovsky%2C+O">Olga Russakovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The task of out-of-distribution (OOD) detection is notoriously ill-defined.
Earlier works focused on new-class detection, aiming to identify label-altering
data distribution shifts, also known as "semantic shift." However, recent works
argue for a focus on failure detection, expanding the OOD evaluation framework
to account for label-preserving data distribution shifts, also known as
"covariate shift." Intriguingly, under this new framework, complex OOD
detectors that were previously considered state-of-the-art now perform
similarly to, or even worse than the simple maximum softmax probability
baseline. This raises the question: what are the latest OOD detectors actually
detecting? Deciphering the behavior of OOD detection algorithms requires
evaluation datasets that decouples semantic shift and covariate shift. To aid
our investigations, we present ImageNet-OOD, a clean semantic shift dataset
that minimizes the interference of covariate shift. Through comprehensive
experiments, we show that OOD detectors are more sensitive to covariate shift
than to semantic shift, and the benefits of recent OOD detection algorithms on
semantic shift detection is minimal. Our dataset and analyses provide important
insights for guiding the design of future OOD detectors.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01757" title="Abstract">arXiv:2310.01757</a> [<a href="/pdf/2310.01757" title="Download PDF">pdf</a>, <a href="/format/2310.01757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MinAres: An Iterative Solver for Symmetric Linear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Montoison%2C+A">Alexis Montoison</a>, 
<a href="/search/math?searchtype=author&query=Orban%2C+D">Dominique Orban</a>, 
<a href="/search/math?searchtype=author&query=Saunders%2C+M+A">Michael A. Saunders</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We introduce an iterative solver named MINARES for symmetric linear systems
$Ax \approx b$, where $A$ is possibly singular. MINARES is based on the
symmetric Lanczos process, like MINRES and MINRES-QLP, but it minimizes
$\|Ar_k\|$ in each Krylov subspace rather than $\|r_k\|$, where $r_k$ is the
current residual vector. When $A$ is symmetric, MINARES minimizes the same
quantity $\|Ar_k\|$ as LSMR, but in more relevant Krylov subspaces, and it
requires only one matrix-vector product $Av$ per iteration, whereas LSMR would
need two. Our numerical experiments with MINRES-QLP and LSMR show that MINARES
is a pertinent alternative on consistent symmetric systems and the most
suitable Krylov method for inconsistent symmetric systems. We derive properties
of MINARES from an equivalent solver named CAR that is to MINARES as CR is to
MINRES, is not based on the Lanczos process, and minimizes $\|Ar_k\|$ in the
same Krylov subspace as MINARES. We establish that MINARES and CAR generate
monotonic $\|x_k - x_{\star}\|$, $\|x_k - x_{\star}\|_A$ and $\|r_k\|$ when $A$
is positive definite.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01758" title="Abstract">arXiv:2310.01758</a> [<a href="/pdf/2310.01758" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linearization of ReLU Activation Function for Neural Network-Embedded  Optimization:Optimal Day-Ahead Energy Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Cunzhi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingpeng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Neural networks have been widely applied in the power system area. They can
be used for better predicting input information and modeling system performance
with increased accuracy. In some applications such as battery degradation
neural network-based microgrid day-ahead energy scheduling, the input features
of the trained learning model are variables to be solved in optimization models
that enforce limits on the output of the same learning model. This will create
a neural network-embedded optimization problem; the use of nonlinear activation
functions in the neural network will make such problems extremely hard to solve
if not unsolvable. To address this emerging challenge, this paper investigated
different methods for linearizing the nonlinear activation functions with a
particular focus on the widely used rectified linear unit (ReLU) function. Four
linearization methods tailored for the ReLU activation function are developed,
analyzed and compared in this paper. Each method employs a set of linear
constraints to replace the ReLU function, effectively linearizing the
optimization problem, which can overcome the computational challenges
associated with the nonlinearity of the neural network model. These proposed
linearization methods provide valuable tools for effectively solving
optimization problems that integrate neural network models with ReLU activation
functions.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01762" title="Abstract">arXiv:2310.01762</a> [<a href="/pdf/2310.01762" title="Download PDF">pdf</a>, <a href="/format/2310.01762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling Multimodal Distributions with the Vanilla Score: Benefits of  Data-Based Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koehler%2C+F">Frederic Koehler</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+T">Thuy-Duong Vuong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Statistics Theory (math.ST)

</div>
<p class="mathjax">There is a long history, as well as a recent explosion of interest, in
statistical and generative modeling approaches based on score functions --
derivatives of the log-likelihood of a distribution. In seminal works,
Hyv\"arinen proposed vanilla score matching as a way to learn distributions
from data by computing an estimate of the score function of the underlying
ground truth, and established connections between this method and established
techniques like Contrastive Divergence and Pseudolikelihood estimation. It is
by now well-known that vanilla score matching has significant difficulties
learning multimodal distributions. Although there are various ways to overcome
this difficulty, the following question has remained unanswered -- is there a
natural way to sample multimodal distributions using just the vanilla score?
Inspired by a long line of related experimental works, we prove that the
Langevin diffusion with early stopping, initialized at the empirical
distribution, and run on a score function estimated from data successfully
generates natural multimodal distributions (mixtures of log-concave
distributions).
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01765" title="Abstract">arXiv:2310.01765</a> [<a href="/pdf/2310.01765" title="Download PDF">pdf</a>, <a href="/format/2310.01765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Cleaning and Machine Learning: A Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=C%C3%B4t%C3%A9%2C+P">Pierre-Olivier C&#xf4;t&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Nikanjam%2C+A">Amin Nikanjam</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N">Nafisa Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Humeniuk%2C+D">Dmytro Humeniuk</a>, 
<a href="/search/cs?searchtype=author&query=Khomh%2C+F">Foutse Khomh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Automated Software Engineering Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Context: Machine Learning (ML) is integrated into a growing number of systems
for various applications. Because the performance of an ML model is highly
dependent on the quality of the data it has been trained on, there is a growing
interest in approaches to detect and repair data errors (i.e., data cleaning).
Researchers are also exploring how ML can be used for data cleaning; hence
creating a dual relationship between ML and data cleaning. To the best of our
knowledge, there is no study that comprehensively reviews this relationship.
Objective: This paper's objectives are twofold. First, it aims to summarize the
latest approaches for data cleaning for ML and ML for data cleaning. Second, it
provides future work recommendations. Method: We conduct a systematic
literature review of the papers published between 2016 and 2022 inclusively. We
identify different types of data cleaning activities with and for ML: feature
cleaning, label cleaning, entity matching, outlier detection, imputation, and
holistic data cleaning. Results: We summarize the content of 101 papers
covering various data cleaning activities and provide 24 future work
recommendations. Our review highlights many promising data cleaning techniques
that can be further extended. Conclusion: We believe that our review of the
literature will help the community develop better approaches to clean data.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01766" title="Abstract">arXiv:2310.01766</a> [<a href="/pdf/2310.01766" title="Download PDF">pdf</a>, <a href="/format/2310.01766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Counterfactual Alignment Loss towards Human-centered AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Ching-Wen Lee</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep neural networks have demonstrated impressive accuracy in supervised
learning tasks. However, their lack of transparency makes it hard for humans to
trust their results, especially in safe-critic domains such as healthcare. To
address this issue, recent explanation-guided learning approaches proposed to
align the gradient-based attention map to image regions annotated by human
experts, thereby obtaining an intrinsically human-centered model. However, the
attention map these methods are based on may fail to causally attribute the
model predictions, thus compromising their validity for alignment. To address
this issue, we propose a novel human-centered framework based on counterfactual
generation. In particular, we utilize the counterfactual generation's ability
for causal attribution to introduce a novel loss called the CounterFactual
Alignment (CF-Align) loss. This loss guarantees that the features attributed by
the counterfactual generation for the classifier align with the human
annotations. To optimize the proposed loss that entails a counterfactual
generation with an implicit function form, we leverage the implicit function
theorem for backpropagation. Our method is architecture-agnostic and, therefore
can be applied to any neural network. We demonstrate the effectiveness of our
method on a lung cancer diagnosis dataset, showcasing faithful alignment to
humans.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01767" title="Abstract">arXiv:2310.01767</a> [<a href="/pdf/2310.01767" title="Download PDF">pdf</a>, <a href="/format/2310.01767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Encoded Observation Spaces for Perceptive Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grossman%2C+L">Lev Grossman</a>, 
<a href="/search/cs?searchtype=author&query=Plancher%2C+B">Brian Plancher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Perceptive deep reinforcement learning (DRL) has lead to many recent
breakthroughs for complex AI systems leveraging image-based input data.
Applications of these results range from super-human level video game agents to
dexterous, physically intelligent robots. However, training these perceptive
DRL-enabled systems remains incredibly compute and memory intensive, often
requiring huge training datasets and large experience replay buffers. This
poses a challenge for the next generation of field robots that will need to be
able to learn on the edge in order to adapt to their environments. In this
paper, we begin to address this issue through differentially encoded
observation spaces. By reinterpreting stored image-based observations as a
video, we leverage lossless differential video encoding schemes to compress the
replay buffer without impacting training performance. We evaluate our approach
with three state-of-the-art DRL algorithms and find that differential image
encoding reduces the memory footprint by as much as 14.2x and 16.7x across
tasks from the Atari 2600 benchmark and the DeepMind Control Suite (DMC)
respectively. These savings also enable large-scale perceptive DRL that
previously required paging between flash and RAM to be run entirely in RAM,
improving the latency of DMC tasks by as much as 32%.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01769" title="Abstract">arXiv:2310.01769</a> [<a href="/pdf/2310.01769" title="Download PDF">pdf</a>, <a href="/format/2310.01769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing:  The Curses of Symmetry and Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+N">Nuoya Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Lijun Ding</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper rigorously shows how over-parameterization changes the convergence
behaviors of gradient descent (GD) for the matrix sensing problem, where the
goal is to recover an unknown low-rank ground-truth matrix from near-isotropic
linear measurements. First, we consider the symmetric setting with the
symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a
positive semi-definite unknown matrix of rank $r \ll n$, and one uses a
symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n
\times k}$ with $k &gt; r$ is the factor matrix. We give a novel $\Omega (1/T^2)$
lower bound of randomly initialized GD for the over-parameterized case ($k &gt;r$)
where $T$ is the number of iterations. This is in stark contrast to the
exact-parameterization scenario ($k=r$) where the convergence rate is $\exp
(-\Omega (T))$. Next, we study asymmetric setting where $M^* \in
\mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll
\min\{n_1,n_2\}$, and one uses an asymmetric parameterization $FG^\top$ to
learn $M^*$ where $F \in \mathbb{R}^{n_1 \times k}$ and $G \in \mathbb{R}^{n_2
\times k}$. Building on prior work, we give a global exact convergence result
of randomly initialized GD for the exact-parameterization case ($k=r$) with an
$\exp (-\Omega(T))$ rate. Furthermore, we give the first global exact
convergence result for the over-parameterization case ($k&gt;r$) with an
$\exp(-\Omega(\alpha^2 T))$ rate where $\alpha$ is the initialization scale.
This linear convergence result in the over-parameterization case is especially
significant because one can apply the asymmetric parameterization to the
symmetric setting to speed up from $\Omega (1/T^2)$ to linear convergence. On
the other hand, we propose a novel method that only modifies one step of GD and
obtains a convergence rate independent of $\alpha$, recovering the rate in the
exact-parameterization case.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01770" title="Abstract">arXiv:2310.01770</a> [<a href="/pdf/2310.01770" title="Download PDF">pdf</a>, <a href="/format/2310.01770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A simple connection from loss flatness to compressed representations in  neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shirui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Recanatesi%2C+S">Stefano Recanatesi</a>, 
<a href="/search/cs?searchtype=author&query=Shea-Brown%2C+E">Eric Shea-Brown</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep neural networks' generalization capacity has been studied in a variety
of ways, including at least two distinct categories of approach: one based on
the shape of the loss landscape in parameter space, and the other based on the
structure of the representation manifold in feature space (that is, in the
space of unit activities). These two approaches are related, but they are
rarely studied together and explicitly connected. Here, we present a simple
analysis that makes such a connection. We show that, in the last phase of
learning of deep neural networks, compression of the volume of the manifold of
neural representations correlates with the flatness of the loss around the
minima explored by ongoing parameter optimization. We show that this is
predicted by a relatively simple mathematical relationship: loss flatness
implies compression of neural representations. Our results build closely on
prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small
eigenvalues of the loss Hessian) develops in late phases of learning and lead
to robustness to perturbations in network inputs. Moreover, we show there is no
similarly direct connection between local dimensionality and sharpness,
suggesting that this property may be controlled by different mechanisms than
volume and hence may play a complementary role in neural representations.
Overall, we advance a dual perspective on generalization in neural networks in
both parameter and feature space.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01772" title="Abstract">arXiv:2310.01772</a> [<a href="/pdf/2310.01772" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SnB Collaborative Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pape%2C+D">Dave Pape</a>, 
<a href="/search/cs?searchtype=author&query=Ghadersohi%2C+A">Amin Ghadersohi</a>, 
<a href="/search/cs?searchtype=author&query=Anstey%2C+J">Josephine Anstey</a>, 
<a href="/search/cs?searchtype=author&query=Makwana%2C+A">Amit Makwana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, Proceedings of 2nd INTUITION International Workshop, Paris, France, 24-25 Nov 2005
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">We describe a system for visualization and editing of data in a computational
chemistry environment. The system is a collaborative tool allowing researchers
using virtual reality and/or desktop computer displays to work together on
results of the Shake-and-Bake structure determination application.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01775" title="Abstract">arXiv:2310.01775</a> [<a href="/pdf/2310.01775" title="Download PDF">pdf</a>, <a href="/format/2310.01775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STAMP: Differentiable Task and Motion Planning via Stein Variational  Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yewon Lee</a> (1), 
<a href="/search/cs?searchtype=author&query=Huang%2C+P">Philip Huang</a> (2), 
<a href="/search/cs?searchtype=author&query=Jatavallabhula%2C+K+M">Krishna Murthy Jatavallabhula</a> (3), 
<a href="/search/cs?searchtype=author&query=Li%2C+A+Z">Andrew Z. Li</a> (1), 
<a href="/search/cs?searchtype=author&query=Damken%2C+F">Fabian Damken</a> (1 and 4), 
<a href="/search/cs?searchtype=author&query=Heiden%2C+E">Eric Heiden</a> (5), 
<a href="/search/cs?searchtype=author&query=Smith%2C+K">Kevin Smith</a> (3), 
<a href="/search/cs?searchtype=author&query=Nowrouzezahrai%2C+D">Derek Nowrouzezahrai</a> (6), 
<a href="/search/cs?searchtype=author&query=Ramos%2C+F">Fabio Ramos</a> (5 and 7), 
<a href="/search/cs?searchtype=author&query=Shkurti%2C+F">Florian Shkurti</a> (1) ((1) University of Toronto, (2) Carnegie Mellon University, (3) Massachusetts Institute of Technology, (4) Technische Universitat Darmstadt, (5) NVIDIA, (6) McGill University, (7) University of Sydney)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures, submitted to the Learning Effective Abstractions for Planning (LEAP) Workshop at CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Planning for many manipulation tasks, such as using tools or assembling
parts, often requires both symbolic and geometric reasoning. Task and Motion
Planning (TAMP) algorithms typically solve these problems by conducting a tree
search over high-level task sequences while checking for kinematic and dynamic
feasibility. While performant, most existing algorithms are highly inefficient
as their time complexity grows exponentially with the number of possible
actions and objects. Additionally, they only find a single solution to problems
in which many feasible plans may exist. To address these limitations, we
propose a novel algorithm called Stein Task and Motion Planning (STAMP) that
leverages parallelization and differentiable simulation to efficiently search
for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems
into continuous optimization problems that can be solved using variational
inference. Our algorithm builds upon Stein Variational Gradient Descent, a
gradient-based variational inference algorithm, and parallelized differentiable
physics simulators on the GPU to efficiently obtain gradients for inference.
Further, we employ imitation learning to introduce action abstractions that
reduce the inference problem to lower dimensions. We demonstrate our method on
two TAMP problems and empirically show that STAMP is able to: 1) produce
multiple diverse plans in parallel; and 2) search for plans more efficiently
compared to existing TAMP baselines.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01777" title="Abstract">arXiv:2310.01777</a> [<a href="/pdf/2310.01777" title="Download PDF">pdf</a>, <a href="/format/2310.01777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEA: Sparse Linear Attention with Estimated Attention Mask
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Heejun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jina Kim</a>, 
<a href="/search/cs?searchtype=author&query=Willette%2C+J">Jeffrey Willette</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 main pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The transformer architecture has made breakthroughs in recent years on tasks
which require modeling pairwise relationships between sequential elements, as
is the case in natural language understanding. However, transformers struggle
with long sequences due to the quadratic complexity of the attention operation,
and previous research has aimed to lower the complexity by sparsifying or
linearly approximating the attention matrix. Yet, these approaches cannot
straightforwardly distill knowledge from a teacher's attention matrix, and
often require complete retraining from scratch. Furthermore, previous sparse
and linear approaches may also lose interpretability if they do not produce
full quadratic attention matrices. To address these challenges, we propose SEA:
Sparse linear attention with an Estimated Attention mask. SEA estimates the
attention matrix with linear complexity via kernel-based linear attention, then
creates a sparse approximation to the full attention matrix with a top-k
selection to perform a sparse attention operation. For language modeling tasks
(Wikitext2), previous linear and sparse attention methods show a roughly
two-fold worse perplexity scores over the quadratic OPT-125M baseline, while
SEA achieves an even better perplexity than OPT-125M, using roughly half as
much memory as OPT-125M. Moreover, SEA maintains an interpretable attention
matrix and can utilize knowledge distillation to lower the complexity of
existing pretrained transformers. We believe that our work will have a large
practical impact, as it opens the possibility of running large transformers on
resource-limited devices with less memory.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01779" title="Abstract">arXiv:2310.01779</a> [<a href="/pdf/2310.01779" title="Download PDF">pdf</a>, <a href="/format/2310.01779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HallE-Switch: Rethinking and Controlling Object Existence Hallucinations  in Large Vision Language Models for Detailed Caption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+B">Bohan Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shijia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangchen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongdi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Manling Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiangjun Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current large vision-language models (LVLMs) achieve remarkable progress, yet
there remains significant uncertainty regarding their ability to accurately
apprehend visual details, that is, in performing detailed captioning. To
address this, we introduce \textit{CCEval}, a GPT-4 assisted evaluation method
tailored for detailed captioning. Interestingly, while LVLMs demonstrate
minimal object existence hallucination in existing VQA benchmarks, our proposed
evaluation reveals continued susceptibility to such hallucinations. In this
paper, we make the first attempt to investigate and attribute such
hallucinations, including image resolution, the language decoder size, and
instruction data amount, quality, granularity. Our findings underscore the
unwarranted inference when the language description includes details at a finer
object granularity than what the vision module can ground or verify, thus
inducing hallucination. To control such hallucinations, we further attribute
the reliability of captioning to contextual knowledge (involving only
contextually grounded objects) and parametric knowledge (containing inferred
objects by the model). Thus, we introduce $\textit{HallE-Switch}$, a
controllable LVLM in terms of $\textbf{Hall}$ucination in object
$\textbf{E}$xistence. HallE-Switch can condition the captioning to shift
between (i) exclusively depicting contextual knowledge for grounded objects and
(ii) blending it with parametric knowledge to imagine inferred objects. Our
method reduces hallucination by 44% compared to LLaVA$_{7B}$ and maintains the
same object coverage.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01780" title="Abstract">arXiv:2310.01780</a> [<a href="/pdf/2310.01780" title="Download PDF">pdf</a>, <a href="/format/2310.01780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Optimal Freshness in Multi-Source, Multi-Channel Systems via MDP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhal%2C+S">Shiksha Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Kavitha%2C+V">Veeraruna Kavitha</a>, 
<a href="/search/cs?searchtype=author&query=Shankar%2C+V">Vidya Shankar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Many systems necessitate frequent and consistent updates of a specific
information. Often this information is updated regularly, where an old packet
becomes completely obsolete in the presence of a new packet. In this context,
we consider a system with multiple sources, each equipped with a storage buffer
of size one, communicating to a common destination via d orthogonal channels.
In each slot, the packets arrive at each source with certain probability and
occupy the buffer (by discarding the old packet if any), and each transfer (to
the destination) is successful with certain other probability. Thus in any
slot, there are two (Age of Information) AoI-measures for each source: one
corresponding to the information at the source itself and the other
corresponding to the information of the same source available at the
destination; some sources may not even have the packet to transmit. The aim of
the controller at the destination is to maintain the freshness of information
of all the sources, to the best extent possible -- it aims to design an optimal
scheduling policy that assigns in each slot, a subset of sources with packets
(at maximum d) for transmission. This is achieved using an appropriate Markov
Decision Process (MDP) framework, where the objective function is the sum of
Average AoIs (AAoI) of all the sources. We derive a very simple stationary
policy that is epsilon-optimal -- in any slot, order the sources with packets
in the decreasing order of the differences in AoI at the destination and the
source and choose the top sources for transmission. With moderate number of
sources (less than 30), the AAoI reduces in the range of 30-90%.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01781" title="Abstract">arXiv:2310.01781</a> [<a href="/pdf/2310.01781" title="Download PDF">pdf</a>, <a href="/format/2310.01781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Stability of Angle based Feedback Control in Power Systems: A  Negative-Imaginary Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yijun Chen</a>, 
<a href="/search/eess?searchtype=author&query=Petersen%2C+I+R">Ian R. Petersen</a>, 
<a href="/search/eess?searchtype=author&query=Ratnam%2C+E+L">Elizabeth L. Ratnam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper considers a power transmission network characterized by
interconnected nonlinear swing dynamics on generator buses. At the steady
state, frequencies across different buses synchronize to a common nominal value
such as $50$Hz or $60$Hz, and power flows on transmission lines are within
steady-state envelopes. We assume that fast measurements of generator rotor
angles are available. Our approach to frequency and angle control centers on
equipping generator buses with large-scale batteries that are controllable on a
fast timescale. We link angle based feedback linearization control with
negative-imaginary systems theory. Angle based feedback controllers are
designed using large-scale batteries as actuators and can be implemented in a
distributed manner incorporating local information. Our analysis demonstrates
the internal stability of the interconnection between the power transmission
network and the angle based feedback controllers. This internal stability
underscores the benefits of achieving frequency synchronization and preserving
steady-state power flows within network envelopes through the use of feedback
controllers. Our approach will enable transmission lines to be operated at
maximum power capacity since stability robustness is ensured by the use of
feedback controllers rather than conservative criteria such as the equal area
criterion. By means of numerical simulations, we illustrate our results.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01783" title="Abstract">arXiv:2310.01783</a> [<a href="/pdf/2310.01783" title="Download PDF">pdf</a>, <a href="/format/2310.01783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can large language models provide useful feedback on research papers? A  large-scale empirical analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+W">Weixin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hancheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Binglu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+D">Daisy Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Vodrahalli%2C+K">Kailas Vodrahalli</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Siyu He</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+D">Daniel Smith</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yian Yin</a>, 
<a href="/search/cs?searchtype=author&query=McFarland%2C+D">Daniel McFarland</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Expert feedback lays the foundation of rigorous research. However, the rapid
growth of scholarly production and intricate knowledge specialization challenge
the conventional scientific feedback mechanisms. High-quality peer reviews are
increasingly difficult to obtain. Researchers who are more junior or from
under-resourced settings have especially hard times getting timely feedback.
With the breakthrough of large language models (LLM) such as GPT-4, there is
growing interest in using LLMs to generate scientific feedback on research
manuscripts. However, the utility of LLM-generated feedback has not been
systematically studied. To address this gap, we created an automated pipeline
using GPT-4 to provide comments on the full PDFs of scientific papers. We
evaluated the quality of GPT-4's feedback through two large-scale studies. We
first quantitatively compared GPT-4's generated feedback with human peer
reviewer feedback in 15 Nature family journals (3,096 papers in total) and the
ICLR machine learning conference (1,709 papers). The overlap in the points
raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature
journals, 39.23% for ICLR) is comparable to the overlap between two human
reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The
overlap between GPT-4 and human reviewers is larger for the weaker papers. We
then conducted a prospective user study with 308 researchers from 110 US
institutions in the field of AI and computational biology to understand how
researchers perceive feedback generated by our GPT-4 system on their own
papers. Overall, more than half (57.4%) of the users found GPT-4 generated
feedback helpful/very helpful and 82.4% found it more beneficial than feedback
from at least some human reviewers. While our findings show that LLM-generated
feedback can help researchers, we also identify several limitations.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01791" title="Abstract">arXiv:2310.01791</a> [<a href="/pdf/2310.01791" title="Download PDF">pdf</a>, <a href="/format/2310.01791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online POMDP Planning with Anytime Deterministic Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barenboim%2C+M">Moran Barenboim</a>, 
<a href="/search/cs?searchtype=author&query=Indelman%2C+V">Vadim Indelman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01794" title="Abstract">arXiv:2310.01794</a> [<a href="/pdf/2310.01794" title="Download PDF">pdf</a>, <a href="/format/2310.01794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers  through In-depth Benchmarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kosan%2C+M">Mert Kosan</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+S">Samidha Verma</a>, 
<a href="/search/cs?searchtype=author&query=Armgaan%2C+B">Burouj Armgaan</a>, 
<a href="/search/cs?searchtype=author&query=Pahwa%2C+K">Khushbu Pahwa</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Ambuj Singh</a>, 
<a href="/search/cs?searchtype=author&query=Medya%2C+S">Sourav Medya</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Numerous explainability methods have been proposed to shed light on the inner
workings of GNNs. Despite the inclusion of empirical evaluations in all the
proposed algorithms, the interrogative aspects of these evaluations lack
diversity. As a result, various facets of explainability pertaining to GNNs,
such as a comparative analysis of counterfactual reasoners, their stability to
variational factors such as different GNN architectures, noise, stochasticity
in non-convex loss surfaces, feasibility amidst domain constraints, and so
forth, have yet to be formally investigated. Motivated by this need, we present
a benchmarking study on perturbation-based explainability methods for GNNs,
aiming to systematically evaluate and compare a wide range of explainability
techniques. Among the key findings of our study, we identify the Pareto-optimal
methods that exhibit superior efficacy and stability in the presence of noise.
Nonetheless, our study reveals that all algorithms are affected by stability
issues when faced with noisy data. Furthermore, we have established that the
current generation of counterfactual explainers often fails to provide feasible
recourses due to violations of topological constraints encoded by
domain-specific considerations. Overall, this benchmarking study empowers
stakeholders in the field of GNNs with a comprehensive understanding of the
state-of-the-art explainability methods, potential research problems for
further enhancement, and the implications of their application in real-world
scenarios.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01795" title="Abstract">arXiv:2310.01795</a> [<a href="/pdf/2310.01795" title="Download PDF">pdf</a>, <a href="/format/2310.01795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TempoNet: Empowering long-term Knee Joint Angle Prediction with Dynamic  Temporal Attention in Exoskeleton Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saoud%2C+L+S">Lyes Saad Saoud</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+I">Irfan Hussain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the 2023 IEEE-RAS International Conference on Humanoid Robots, Austin, USA, on December 12-14
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In the realm of exoskeleton control, achieving precise control poses
challenges due to the mechanical delay of exoskeletons. To address this,
incorporating future gait trajectories as feed-forward input has been proposed.
However, existing deep learning models for gait prediction mainly focus on
short-term predictions, leaving the long-term performance of these models
relatively unexplored. In this study, we present TempoNet, a novel model
specifically designed for precise knee joint angle prediction. By harnessing
dynamic temporal attention within the Transformer-based architecture, TempoNet
surpasses existing models in forecasting knee joint angles over extended time
horizons. Notably, our model achieves a remarkable reduction of 10\% to 185\%
in Mean Absolute Error (MAE) for 100 ms ahead forecasting compared to other
transformer-based models, demonstrating its effectiveness. Furthermore,
TempoNet exhibits further reliability and superiority over the baseline
Transformer model, outperforming it by 14\% in MAE for the 200 ms prediction
horizon. These findings highlight the efficacy of TempoNet in accurately
predicting knee joint angles and emphasize the importance of incorporating
dynamic temporal attention. TempoNet's capability to enhance knee joint angle
prediction accuracy opens up possibilities for precise control, improved
rehabilitation outcomes, advanced sports performance analysis, and deeper
insights into biomechanical research. Code implementation for the TempoNet
model can be found in the GitHub repository:
https://github.com/LyesSaadSaoud/TempoNet.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01796" title="Abstract">arXiv:2310.01796</a> [<a href="/pdf/2310.01796" title="Download PDF">pdf</a>, <a href="/format/2310.01796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMParser: A LLM-based Log Parsing Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhihan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuangbin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junjie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yintong Huo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pinjia He</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiazhen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The process of log parsing, which converts log messages into structured
formats, is a crucial step for various log analysis tasks. Although numerous
log parsers have been proposed, their effectiveness on complex log data is
often hindered due to reliance on human-made rules or learning-based models
with limited training data. The recent rise of powerful large language models
(LLMs) shows potential for log parsing due to their extensive pre-trained
knowledge related to code and logging. However, their accuracy is currently
limited due to the lack of specialized log parsing capabilities. Additionally,
the inconsistency of their answers and significant overhead obstruct the
practical implementation of LLM-based log parsing.
<br />To tackle these challenges, we introduce LLMParser, the first practical
LLM-based log parsing framework. LLMParser enables accurate and robust log
parsing by leveraging the in-context learning (ICL) capability of the LLM,
employing a hierarchical candidate sampling algorithm, and selecting
high-quality demonstrations. LLMParser also includes a novel adaptive parsing
cache component to store and refine the templates generated by the LLM. This
design aids in addressing the inefficiency of LLMs by rapid matching to
previously parsed log templates. LLMParser also adaptively updates the
templates in the parsing cache to ensure consistent parsed results. Extensive
evaluation on large-scale public datasets demonstrates that LLMParser surpasses
the state-of-the-art methods. Furthermore, LLMParser significantly reduces the
query times to LLMs, achieving efficiency comparable to the most efficient
baseline, Drain.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01798" title="Abstract">arXiv:2310.01798</a> [<a href="/pdf/2310.01798" title="Download PDF">pdf</a>, <a href="/format/2310.01798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Cannot Self-Correct Reasoning Yet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H+S">Huaixiu Steven Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+A+W">Adams Wei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xinying Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance might even
degrade post self-correction. Drawing from these insights, we offer suggestions
for future research and practical applications in this field.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01801" title="Abstract">arXiv:2310.01801</a> [<a href="/pdf/2310.01801" title="Download PDF">pdf</a>, <a href="/format/2310.01801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+S">Suyu Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minjia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review; To be updated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this study, we introduce adaptive KV cache compression, a plug-and-play
method that reduces the memory footprint of generative inference for Large
Language Models (LLMs). Different from the conventional KV cache that retains
key and value vectors for all context tokens, we conduct targeted profiling to
discern the intrinsic structure of attention modules. Based on the recognized
structure, we then construct the KV cache in an adaptive manner: evicting
long-range contexts on attention heads emphasizing local contexts, discarding
non-special tokens on attention heads centered on special tokens, and only
employing the standard KV cache for attention heads that broadly attend to all
tokens. Moreover, with the lightweight attention profiling used to guide the
construction of the adaptive KV cache, FastGen can be deployed without
resource-intensive fine-tuning or re-training. In our experiments across
various asks, FastGen demonstrates substantial reduction on GPU memory
consumption with negligible generation quality loss. We will release our code
and the compatible CUDA kernel for reproducibility.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01802" title="Abstract">arXiv:2310.01802</a> [<a href="/pdf/2310.01802" title="Download PDF">pdf</a>, <a href="/format/2310.01802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying Safety Approaches for Stochastic Systems: From Barrier  Functions to Uncertain Abstractions via Dynamic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Laurenti%2C+L">Luca Laurenti</a>, 
<a href="/search/eess?searchtype=author&query=Lahijanian%2C+M">Morteza Lahijanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transaction on Automatic Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Providing safety guarantees for stochastic dynamical systems has become a
central problem in many fields, including control theory, machine learning, and
robotics. Existing methods either employ Stochastic Barrier Functions (SBFs) or
rely on numerical approaches based on abstractions. While SBFs are analogous to
Lyapunov functions to prove (probabilistic) set invariance, abstraction-based
approaches approximate the stochastic system into a finite model for the
computation of safety probability bounds. In this paper, we offer a new
perspective on these seemingly different methods. We show that both these
approaches arise as approximations of a stochastic dynamic programming problem.
Such a new and unifying perspective allows us to formally show the correctness
of both approaches, characterize their convergence and optimality properties,
and examine advantages and disadvantages of each. Our analysis reveals that
abstraction-based methods can provide more accurate certificates of safety, but
generally at a higher computational cost. We conclude the article with a
discussion that highlights possible future directions.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01803" title="Abstract">arXiv:2310.01803</a> [<a href="/pdf/2310.01803" title="Download PDF">pdf</a>, <a href="/format/2310.01803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of Cross-Lingual Bug Localization: Two Industrial Cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+S">Shinpei Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+T">Takashi Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Kato%2C+T">Tadahisa Kato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> (C) 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 39th IEEE International Conference on Software
  Maintenance and Evolution, 495-499, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This study reports the results of applying the cross-lingual bug localization
approach proposed by Xia et al. to industrial software projects. To realize
cross-lingual bug localization, we applied machine translation to non-English
descriptions in the source code and bug reports, unifying them into
English-based texts, to which an existing English-based bug localization
technique was applied. In addition, a prototype tool based on BugLocator was
implemented and applied to two Japanese industrial projects, which resulted in
a slightly different performance from that of Xia et al.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01805" title="Abstract">arXiv:2310.01805</a> [<a href="/pdf/2310.01805" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative study of microgrid optimal scheduling under  multi-optimization algorithm fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Hongyi Duan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuming Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 fiures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">As global attention on renewable and clean energy grows, the research and
implementation of microgrids become paramount. This paper delves into the
methodology of exploring the relationship between the operational and
environmental costs of microgrids through multi-objective optimization models.
By integrating various optimization algorithms like Genetic Algorithm,
Simulated Annealing, Ant Colony Optimization, and Particle Swarm Optimization,
we propose an integrated approach for microgrid optimization. Simulation
results depict that these algorithms provide different dispatch results under
economic and environmental dispatch, revealing distinct roles of diesel
generators and micro gas turbines in microgrids. Overall, this study offers
in-depth insights and practical guidance for microgrid design and operation.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01806" title="Abstract">arXiv:2310.01806</a> [<a href="/pdf/2310.01806" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improvement and Enhancement of YOLOv5 Small Target Recognition Based on  Multi-module Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Hongyi Duan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">JiaLiang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+X">Xueqian Gan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruotong Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, the limitations of YOLOv5s model on small target detection
task are deeply studied and improved. The performance of the model is
successfully enhanced by introducing GhostNet-based convolutional module,
RepGFPN-based Neck module optimization, CA and Transformer's attention
mechanism, and loss function improvement using NWD. The experimental results
validate the positive impact of these improvement strategies on model
precision, recall and mAP. In particular, the improved model shows significant
superiority in dealing with complex backgrounds and tiny targets in real-world
application tests. This study provides an effective optimization strategy for
the YOLOv5s model on small target detection, and lays a solid foundation for
future related research and applications.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01807" title="Abstract">arXiv:2310.01807</a> [<a href="/pdf/2310.01807" title="Download PDF">pdf</a>, <a href="/format/2310.01807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete, compositional, and symbolic representations through attractor  dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nam%2C+A">Andrew Nam</a>, 
<a href="/search/cs?searchtype=author&query=Elmoznino%2C+E">Eric Elmoznino</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Lajoie%2C+G">Guillaume Lajoie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Compositionality is an important feature of discrete symbolic systems, such
as language and programs, as it enables them to have infinite capacity despite
a finite symbol set. It serves as a useful abstraction for reasoning in both
cognitive science and in AI, yet the interface between continuous and symbolic
processing is often imposed by fiat at the algorithmic level, such as by means
of quantization or a softmax sampling step. In this work, we explore how
discretization could be implemented in a more neurally plausible manner through
the modeling of attractor dynamics that partition the continuous representation
space into basins that correspond to sequences of symbols. Building on
established work in attractor networks and introducing novel training methods,
we show that imposing structure in the symbolic space can produce
compositionality in the attractor-supported representation space of rich
sensory inputs. Lastly, we argue that our model exhibits the process of an
information bottleneck that is thought to play a role in conscious experience,
decomposing the rich information of a sensory input into stable components
encoding symbolic information.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01809" title="Abstract">arXiv:2310.01809</a> [<a href="/pdf/2310.01809" title="Download PDF">pdf</a>, <a href="/format/2310.01809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mel-Band RoFormer for Music Source Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Ju-Chiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei-Tsung Lu</a>, 
<a href="/search/cs?searchtype=author&query=Won%2C+M">Minz Won</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted as an ISMIR 2023 late-breaking and demo paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recently, multi-band spectrogram-based approaches such as Band-Split RNN
(BSRNN) have demonstrated promising results for music source separation. In our
recent work, we introduce the BS-RoFormer model which inherits the idea of
band-split scheme in BSRNN at the front-end, and then uses the hierarchical
Transformer with Rotary Position Embedding (RoPE) to model the inner-band and
inter-band sequences for multi-band mask estimation. This model has achieved
state-of-the-art performance, but the band-split scheme is defined empirically,
without analytic supports from the literature. In this paper, we propose
Mel-RoFormer, which adopts the Mel-band scheme that maps the frequency bins
into overlapped subbands according to the mel scale. In contract, the
band-split mapping in BSRNN and BS-RoFormer is non-overlapping and designed
based on heuristics. Using the MUSDB18HQ dataset for experiments, we
demonstrate that Mel-RoFormer outperforms BS-RoFormer in the separation tasks
of vocals, drums, and other stems.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01812" title="Abstract">arXiv:2310.01812</a> [<a href="/pdf/2310.01812" title="Download PDF">pdf</a>, <a href="/format/2310.01812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PPT: Token Pruning and Pooling for Efficient Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinjian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanhu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiudong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinghao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision Transformers (ViTs) have emerged as powerful models in the field of
computer vision, delivering superior performance across various vision tasks.
However, the high computational complexity poses a significant barrier to their
practical applications in real-world scenarios. Motivated by the fact that not
all tokens contribute equally to the final predictions and fewer tokens bring
less computational cost, reducing redundant tokens has become a prevailing
paradigm for accelerating vision transformers. However, we argue that it is not
optimal to either only reduce inattentive redundancy by token pruning, or only
reduce duplicative redundancy by token merging. To this end, in this paper we
propose a novel acceleration framework, namely token Pruning &amp; Pooling
Transformers (PPT), to adaptively tackle these two types of redundancy in
different layers. By heuristically integrating both token pruning and token
pooling techniques in ViTs without additional trainable parameters, PPT
effectively reduces the model complexity while maintaining its predictive
accuracy. For example, PPT reduces over 37% FLOPs and improves the throughput
by over 45% for DeiT-S without any accuracy drop on the ImageNet dataset.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01815" title="Abstract">arXiv:2310.01815</a> [<a href="/pdf/2310.01815" title="Download PDF">pdf</a>, <a href="/format/2310.01815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Determines the Price of NFTs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziemke%2C+V">Vivian Ziemke</a>, 
<a href="/search/cs?searchtype=author&query=Estermann%2C+B">Benjamin Estermann</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the evolving landscape of digital art, Non-Fungible Tokens (NFTs) have
emerged as a groundbreaking platform, bridging the realms of art and
technology. NFTs serve as the foundational framework that has revolutionized
the market for digital art, enabling artists to showcase and monetize their
creations in unprecedented ways. NFTs combine metadata stored on the blockchain
with off-chain data, such as images, to create a novel form of digital
ownership. It is not fully understood how these factors come together to
determine NFT prices. In this study, we analyze both on-chain and off-chain
data of NFT collections trading on OpenSea to understand what influences NFT
pricing. Our results show that while text and image data of the NFTs can be
used to explain price variations within collections, the extracted features do
not generalize to new, unseen collections. Furthermore, we find that an NFT
collection's trading volume often relates to its online presence, like social
media followers and website traffic.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01818" title="Abstract">arXiv:2310.01818</a> [<a href="/pdf/2310.01818" title="Download PDF">pdf</a>, <a href="/format/2310.01818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xilie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kankanhalli%2C+M">Mohan Kankanhalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial
robustness in downstream applications, without requiring a lot of computational
resources and collecting significant amounts of data. This paper uncovers an
issue with the existing RFT, where optimizing both adversarial and natural
objectives through the feature extractor (FE) yields significantly divergent
gradient directions. This divergence introduces instability in the optimization
process, thereby hindering the attainment of adversarial robustness and
rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we
propose a low-rank (LoRa) branch that disentangles RFT into two distinct
components: optimizing natural objectives via the LoRa branch and adversarial
objectives via the FE. Besides, we introduce heuristic strategies for
automating the scheduling of the learning rate and the scalars of loss terms.
Extensive empirical evaluations demonstrate that our proposed automated RFT
disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art
results across a range of downstream tasks. AutoLoRa holds significant
practical utility, as it automatically converts a pre-trained FE into an
adversarially robust model for downstream tasks without the need for searching
hyperparameters.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01819" title="Abstract">arXiv:2310.01819</a> [<a href="/pdf/2310.01819" title="Download PDF">pdf</a>, <a href="/format/2310.01819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amazing Combinatorial Creation: Acceptable Swap-Sampling for  Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zedong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: \url{<a href="https://asst2i.github.io/anon/">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Exploring a machine learning system to generate meaningful combinatorial
object images from multiple textual descriptions, emulating human creativity,
is a significant challenge as humans are able to construct amazing
combinatorial objects, but machines strive to emulate data distribution. In
this paper, we develop a straightforward yet highly effective technique called
acceptable swap-sampling to generate a combinatorial object image that exhibits
novelty and surprise, utilizing text concepts of different objects. Initially,
we propose a swapping mechanism that constructs a novel embedding by exchanging
column vectors of two text embeddings for generating a new combinatorial image
through a cutting-edge diffusion model. Furthermore, we design an acceptable
region by managing suitable CLIP distances between the new image and the
original concept generations, increasing the likelihood of accepting the new
image with a high-quality combination. This region allows us to efficiently
sample a small subset from a new image pool generated by using randomly
exchanging column vectors. Lastly, we employ a segmentation method to compare
CLIP distances among the segmented components, ultimately selecting the most
promising object image from the sampled subset. Our experiments focus on text
pairs of objects from ImageNet, and our results demonstrate that our approach
outperforms recent methods such as Stable-Diffusion2, DALLE2, ERNIE-ViLG2 and
Bing in generating novel and surprising object images, even when the associated
concepts appear to be implausible, such as lionfish-abacus. Furthermore, during
the sampling process, our approach without training and human preference is
also comparable to PickScore and HPSv2 trained using human preference datasets.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01820" title="Abstract">arXiv:2310.01820</a> [<a href="/pdf/2310.01820" title="Download PDF">pdf</a>, <a href="/format/2310.01820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Fidelity for Evaluating Explainability of Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shirani%2C+F">Farhad Shirani</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianchun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuomin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hua Wei</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 Pages, 10 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) are neural models that leverage the dependency
structure in graphical data via message passing among the graph nodes. GNNs
have emerged as pivotal architectures in analyzing graph-structured data, and
their expansive application in sensitive domains requires a comprehensive
understanding of their decision-making processes -- necessitating a framework
for GNN explainability. An explanation function for GNNs takes a pre-trained
GNN along with a graph as input, to produce a `sufficient statistic' subgraph
with respect to the graph label. A main challenge in studying GNN
explainability is to provide fidelity measures that evaluate the performance of
these explanation functions. This paper studies this foundational challenge,
spotlighting the inherent limitations of prevailing fidelity metrics, including
$Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal,
information-theoretic definition of explainability is introduced and it is
shown that existing metrics often fail to align with this definition across
various statistical scenarios. The reason is due to potential distribution
shifts when subgraphs are removed in computing these fidelity measures.
Subsequently, a robust class of fidelity measures are introduced, and it is
shown analytically that they are resilient to distribution shift issues and are
applicable in a wide range of scenarios. Extensive empirical analysis on both
synthetic and real datasets are provided to illustrate that the proposed
metrics are more coherent with gold standard metrics.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01821" title="Abstract">arXiv:2310.01821</a> [<a href="/pdf/2310.01821" title="Download PDF">pdf</a>, <a href="/format/2310.01821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural  Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+T">Takuhiro Kaneko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023. Project page: <a href="https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/mimo-nerf/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Neural radiance fields (NeRFs) have shown impressive results for novel view
synthesis. However, they depend on the repetitive use of a single-input
single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and
view direction to the color and volume density in a sample-wise manner, which
slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF)
that reduces the number of MLPs running by replacing the SISO MLP with a MIMO
MLP and conducting mappings in a group-wise manner. One notable challenge with
this approach is that the color and volume density of each point can differ
according to a choice of input coordinates in a group, which can lead to some
notable ambiguity. We also propose a self-supervised learning method that
regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this
ambiguity without using pretrained models. The results of a comprehensive
experimental evaluation including comparative and ablation studies are
presented to show that MIMO-NeRF obtains a good trade-off between speed and
quality with a reasonable training time. We then demonstrate that MIMO-NeRF is
compatible with and complementary to previous advancements in NeRFs by applying
it to two representative fast NeRFs, i.e., a NeRF with sample reduction
(DONeRF) and a NeRF with alternative representations (TensoRF).
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01824" title="Abstract">arXiv:2310.01824</a> [<a href="/pdf/2310.01824" title="Download PDF">pdf</a>, <a href="/format/2310.01824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon  Decision-Making in Embodied AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+E">Emily Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiaheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhuoyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruohan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fei-Fei%2C+L">Li Fei-Fei</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn-Mart%C3%ADn%2C+R">Roberto Mart&#xed;n-Mart&#xed;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges
agents to use reasoning and decision-making skills to solve complex activities
that resemble everyday human challenges. The Mini-BEHAVIOR environment is a
fast, realistic Gridworld environment that offers the benefits of rapid
prototyping and ease of use while preserving a symbolic level of physical
realism and complexity found in complex embodied AI benchmarks. We introduce
key features such as procedural generation, to enable the creation of countless
task variations and support open-ended learning. Mini-BEHAVIOR provides
implementations of various household tasks from the original BEHAVIOR
benchmark, along with starter code for data collection and reinforcement
learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended
benchmark for evaluating decision-making and planning solutions in embodied AI.
It serves as a user-friendly entry point for research and facilitates the
evaluation and development of solutions, simplifying their assessment and
development while advancing the field of embodied AI. Code is publicly
available at https://github.com/StanfordVL/mini_behavior.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01825" title="Abstract">arXiv:2310.01825</a> [<a href="/pdf/2310.01825" title="Download PDF">pdf</a>, <a href="/format/2310.01825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Study of PEFT techniques for Winter Wheat Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zahweh%2C+M+H">Mohamad Hasan Zahweh</a>, 
<a href="/search/cs?searchtype=author&query=Nasrallah%2C+H">Hasan Nasrallah</a>, 
<a href="/search/cs?searchtype=author&query=Shukor%2C+M">Mustafa Shukor</a>, 
<a href="/search/cs?searchtype=author&query=Faour%2C+G">Ghaleb Faour</a>, 
<a href="/search/cs?searchtype=author&query=Ghandour%2C+A+J">Ali J. Ghandour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced
significant growth and have been extensively employed to adapt large vision and
language models to various domains, enabling satisfactory model performance
with minimal computational needs. Despite these advances, more research has yet
to delve into potential PEFT applications in real-life scenarios, particularly
in the critical domains of remote sensing and crop monitoring. The diversity of
climates across different regions and the need for comprehensive large-scale
datasets have posed significant obstacles to accurately identify crop types
across varying geographic locations and changing growing seasons. This study
seeks to bridge this gap by comprehensively exploring the feasibility of
cross-area and cross-year out-of-distribution generalization using the
State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to
explore PEFT approaches for crop monitoring. Specifically, we focus on adapting
the SOTA TSViT model to address winter wheat field segmentation, a critical
task for crop monitoring and food security. This adaptation process involves
integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and
prompt tuning. Using PEFT techniques, we achieved notable results comparable to
those achieved using full fine-tuning methods while training only a mere 0.7%
parameters of the whole TSViT architecture. The in-house labeled data-set,
referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated
polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over
five consecutive years. Using Sentinel-2 images, our model achieved a 84%
F1-score. We intend to publicly release the Lebanese winter wheat data set,
code repository, and model weights.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01826" title="Abstract">arXiv:2310.01826</a> [<a href="/pdf/2310.01826" title="Download PDF">pdf</a>, <a href="/format/2310.01826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grid-Forming Control Methods for Weakly Connected Offshore WPPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghimire%2C+S">Sulav Ghimire</a>, 
<a href="/search/eess?searchtype=author&query=Kkuni%2C+K+V">Kanakesh V Kkuni</a>, 
<a href="/search/eess?searchtype=author&query=Jakobsen%2C+S+C">Simon C Jakobsen</a>, 
<a href="/search/eess?searchtype=author&query=Knueppel%2C+T">Thyge Knueppel</a>, 
<a href="/search/eess?searchtype=author&query=Jensen%2C+K+H">Kim H Jensen</a>, 
<a href="/search/eess?searchtype=author&query=Guest%2C+E">Emerson Guest</a>, 
<a href="/search/eess?searchtype=author&query=Rasmussen%2C+T+W">Tonny W Rasmussen</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guangya Yang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Wind and Solar Integration Workshop 2023, Copenhagen
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Grid-forming control (GFC) has seen numerous technological advances in their
control types, applications, and the multitude of services they provide. Some
examples of the services they provide include black start, inertial frequency
response, and islanded operation capabilities with the possibility of
re-synchronization without the need of additional support from other devices
such as storage. State of the art literature proposes a variety of GFCs which
can provide single or multiple of these services. However, study of these
different GFCs for weakly-connected offshore wind power plants (WPPs) based on
time-domain simulation and focusing on the large signal disturbance is not well
covered. This paper reviews some of the most researched grid-forming control
methods applicable to offshore WPPs and provides a comparative investigation
and discussion of their stability properties and applicability, especially when
connected to a weak-grid. The paper also provides a discussion on the
prerequisites and challenges surrounding the comparative study of different
GFCs.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01827" title="Abstract">arXiv:2310.01827</a> [<a href="/pdf/2310.01827" title="Download PDF">pdf</a>, <a href="/format/2310.01827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning and reusing primitive behaviours to improve Hindsight  Experience Replay sample efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+F+R">Francisco Roldan Sanchez</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bulens%2C+D+C">David Cordova Bulens</a>, 
<a href="/search/cs?searchtype=author&query=McGuinness%2C+K">Kevin McGuinness</a>, 
<a href="/search/cs?searchtype=author&query=Redmond%2C+S">Stephen Redmond</a>, 
<a href="/search/cs?searchtype=author&query=O%27Connor%2C+N">Noel O&#x27;Connor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, 1 algorithm. Submitted to ICARA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Hindsight Experience Replay (HER) is a technique used in reinforcement
learning (RL) that has proven to be very efficient for training off-policy
RL-based agents to solve goal-based robotic manipulation tasks using sparse
rewards. Even though HER improves the sample efficiency of RL-based agents by
learning from mistakes made in past experiences, it does not provide any
guidance while exploring the environment. This leads to very large training
times due to the volume of experience required to train an agent using this
replay strategy. In this paper, we propose a method that uses primitive
behaviours that have been previously learned to solve simple tasks in order to
guide the agent toward more rewarding actions during exploration while learning
other more complex tasks. This guidance, however, is not executed by a manually
designed curriculum, but rather using a critic network to decide at each
timestep whether or not to use the actions proposed by the previously-learned
primitive policies. We evaluate our method by comparing its performance against
HER and other more efficient variations of this algorithm in several block
manipulation tasks. We demonstrate the agents can learn a successful policy
faster when using our proposed method, both in terms of sample efficiency and
computation time. Code is available at https://github.com/franroldans/qmp-her.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01828" title="Abstract">arXiv:2310.01828</a> [<a href="/pdf/2310.01828" title="Download PDF">pdf</a>, <a href="/format/2310.01828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trainable Noise Model as an XAI evaluation method: application on Sobol  for remote sensing image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shreim%2C+H">Hossein Shreim</a>, 
<a href="/search/cs?searchtype=author&query=Gizzini%2C+A+K">Abdul Karim Gizzini</a>, 
<a href="/search/cs?searchtype=author&query=Ghandour%2C+A+J">Ali J. Ghandour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01830" title="Abstract">arXiv:2310.01830</a> [<a href="/pdf/2310.01830" title="Download PDF">pdf</a>, <a href="/format/2310.01830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Generated Images as Data Source: The Dawn of Synthetic Era
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+F">Fangneng Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kunhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Muyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The advancement of visual intelligence is intrinsically tethered to the
availability of data. In parallel, generative Artificial Intelligence (AI) has
unlocked the potential to create synthetic images that closely resemble
real-world photographs, which prompts a compelling inquiry: how visual
intelligence benefit from the advance of generative AI? This paper explores the
innovative concept of harnessing these AI-generated images as a new data
source, reshaping traditional model paradigms in visual intelligence. In
contrast to real data, AI-generated data sources exhibit remarkable advantages,
including unmatched abundance and scalability, the rapid generation of vast
datasets, and the effortless simulation of edge cases. Built on the success of
generative AI models, we examines the potential of their generated data in a
range of applications, from training machine learning models to simulating
scenarios for computational modelling, testing, and validation. We probe the
technological foundations that support this groundbreaking use of generative
AI, engaging in an in-depth discussion on the ethical, legal, and practical
considerations that accompany this transformative paradigm shift. Through an
exhaustive survey of current technologies and applications, this paper presents
a comprehensive view of the synthetic era in visual intelligence. A project
with this paper can be found at https://github.com/mwxely/AIGS .
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01831" title="Abstract">arXiv:2310.01831</a> [<a href="/pdf/2310.01831" title="Download PDF">pdf</a>, <a href="/format/2310.01831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formalizing Natural Language Intent into Program Specifications via  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Endres%2C+M">Madeline Endres</a>, 
<a href="/search/cs?searchtype=author&query=Fakhoury%2C+S">Sarah Fakhoury</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Lahiri%2C+S+K">Shuvendu K. Lahiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">Informal natural language that describes code functionality, such as code
comments or function documentation, may contain substantial information about a
programs intent. However, there is typically no guarantee that a programs
implementation and natural language documentation are aligned. In the case of a
conflict, leveraging information in code-adjacent natural language has the
potential to enhance fault localization, debugging, and code trustworthiness.
In practice, however, this information is often underutilized due to the
inherent ambiguity of natural language which makes natural language intent
challenging to check programmatically. The "emergent abilities" of Large
Language Models (LLMs) have the potential to facilitate the translation of
natural language intent to programmatically checkable assertions. However, it
is unclear if LLMs can correctly translate informal natural language
specifications into formal specifications that match programmer intent.
Additionally, it is unclear if such translation could be useful in practice. In
this paper, we describe LLM4nl2post, the problem leveraging LLMs for
transforming informal natural language to formal method postconditions,
expressed as program assertions. We introduce and validate metrics to measure
and compare different LLM4nl2post approaches, using the correctness and
discriminative power of generated postconditions. We then perform qualitative
and quantitative methods to assess the quality of LLM4nl2post postconditions,
finding that they are generally correct and able to discriminate incorrect
code. Finally, we find that LLM4nl2post via LLMs has the potential to be
helpful in practice; specifications generated from natural language were able
to catch 70 real-world historical bugs from Defects4J.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01833" title="Abstract">arXiv:2310.01833</a> [<a href="/pdf/2310.01833" title="Download PDF">pdf</a>, <a href="/format/2310.01833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skin the sheep not only once: Reusing Various Depth Datasets to Drive  the Learning of Optical Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sheng-Chi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+W">Wei-Chen Chiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Optical flow estimation is crucial for various applications in vision and
robotics. As the difficulty of collecting ground truth optical flow in
real-world scenarios, most of the existing methods of learning optical flow
still adopt synthetic dataset for supervised training or utilize photometric
consistency across temporally adjacent video frames to drive the unsupervised
learning, where the former typically has issues of generalizability while the
latter usually performs worse than the supervised ones. To tackle such
challenges, we propose to leverage the geometric connection between optical
flow estimation and stereo matching (based on the similarity upon finding pixel
correspondences across images) to unify various real-world depth estimation
datasets for generating supervised training data upon optical flow.
Specifically, we turn the monocular depth datasets into stereo ones via
synthesizing virtual disparity, thus leading to the flows along the horizontal
direction; moreover, we introduce virtual camera motion into stereo data to
produce additional flows along the vertical direction. Furthermore, we propose
applying geometric augmentations on one image of an optical flow pair,
encouraging the optical flow estimator to learn from more challenging cases.
Lastly, as the optical flow maps under different geometric augmentations
actually exhibit distinct characteristics, an auxiliary classifier which trains
to identify the type of augmentation from the appearance of the flow map is
utilized to further enhance the learning of the optical flow estimator. Our
proposed method is general and is not tied to any particular flow estimator,
where extensive experiments based on various datasets and optical flow
estimation models verify its efficacy and superiority.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01835" title="Abstract">arXiv:2310.01835</a> [<a href="/pdf/2310.01835" title="Download PDF">pdf</a>, <a href="/format/2310.01835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMBERSim: A Large-Scale Databank for Boosting Similarity Search in  Malware Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corlatescu%2C+D+G">Dragos Georgian Corlatescu</a>, 
<a href="/search/cs?searchtype=author&query=Dinu%2C+A">Alexandru Dinu</a>, 
<a href="/search/cs?searchtype=author&query=Gaman%2C+M">Mihaela Gaman</a>, 
<a href="/search/cs?searchtype=author&query=Sumedrea%2C+P">Paul Sumedrea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years there has been a shift from heuristics-based malware
detection towards machine learning, which proves to be more robust in the
current heavily adversarial threat landscape. While we acknowledge machine
learning to be better equipped to mine for patterns in the increasingly high
amounts of similar-looking files, we also note a remarkable scarcity of the
data available for similarity-targeted research. Moreover, we observe that the
focus in the few related works falls on quantifying similarity in malware,
often overlooking the clean data. This one-sided quantification is especially
dangerous in the context of detection bypass. We propose to address the
deficiencies in the space of similarity research on binary files, starting from
EMBER - one of the largest malware classification data sets. We enhance EMBER
with similarity information as well as malware class tags, to enable further
research in the similarity space. Our contribution is threefold: (1) we publish
EMBERSim, an augmented version of EMBER, that includes similarity-informed
tags; (2) we enrich EMBERSim with automatically determined malware class tags
using the open-source tool AVClass on VirusTotal data and (3) we describe and
share the implementation for our class scoring technique and leaf similarity
method.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01837" title="Abstract">arXiv:2310.01837</a> [<a href="/pdf/2310.01837" title="Download PDF">pdf</a>, <a href="/format/2310.01837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gizzini%2C+A+K">Abdul Karim Gizzini</a>, 
<a href="/search/cs?searchtype=author&query=Shukor%2C+M">Mustafa Shukor</a>, 
<a href="/search/cs?searchtype=author&query=Ghandour%2C+A+J">Ali J. Ghandour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Current AI-based methods do not provide comprehensible physical
interpretations of the utilized data, extracted features, and
predictions/inference operations. As a result, deep learning models trained
using high-resolution satellite imagery lack transparency and explainability
and can be merely seen as a black box, which limits their wide-level adoption.
Experts need help understanding the complex behavior of AI models and the
underlying decision-making process. The explainable artificial intelligence
(XAI) field is an emerging field providing means for robust, practical, and
trustworthy deployment of AI models. Several XAI techniques have been proposed
for image classification tasks, whereas the interpretation of image
segmentation remains largely unexplored. This paper offers to bridge this gap
by adapting the recent XAI classification algorithms and making them usable for
muti-class image segmentation, where we mainly focus on buildings' segmentation
from high-resolution satellite images. To benchmark and compare the performance
of the proposed approaches, we introduce a new XAI evaluation methodology and
metric based on "Entropy" to measure the model uncertainty. Conventional XAI
evaluation methods rely mainly on feeding area-of-interest regions from the
image back to the pre-trained (utility) model and then calculating the average
change in the probability of the target class. Those evaluation metrics lack
the needed robustness, and we show that using Entropy to monitor the model
uncertainty in segmenting the pixels within the target class is more suitable.
We hope this work will pave the way for additional XAI research for image
segmentation and applications in the remote sensing discipline.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01840" title="Abstract">arXiv:2310.01840</a> [<a href="/pdf/2310.01840" title="Download PDF">pdf</a>, <a href="/format/2310.01840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in  Dynamic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhilu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaotao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+L">Lei Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Merging multi-exposure images is a common approach for obtaining high dynamic
range (HDR) images, with the primary challenge being the avoidance of ghosting
artifacts in dynamic scenes. Recent methods have proposed using deep neural
networks for deghosting. However, the methods typically rely on sufficient data
with HDR ground-truths, which are difficult and costly to collect. In this
work, to eliminate the need for labeled data, we propose SelfHDR, a
self-supervised HDR reconstruction method that only requires dynamic
multi-exposure images during training. Specifically, SelfHDR learns a
reconstruction network under the supervision of two complementary components,
which can be constructed from multi-exposure images and focus on HDR color as
well as structure, respectively. The color component is estimated from aligned
multi-exposure images, while the structure one is generated through a
structure-focused network that is supervised by the color component and an
input reference (\eg, medium-exposure) image. During testing, the learned
reconstruction network is directly deployed to predict an HDR image.
Experiments on real-world images demonstrate our SelfHDR achieves superior
results against the state-of-the-art self-supervised methods, and comparable
performance to supervised ones. Codes are available at
https://github.com/cszhilu1998/SelfHDR
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01842" title="Abstract">arXiv:2310.01842</a> [<a href="/pdf/2310.01842" title="Download PDF">pdf</a>, <a href="/format/2310.01842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SelfGraphVQA: A Self-Supervised Graph Neural Network for Scene-based  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Souza%2C+B">Bruno Souza</a>, 
<a href="/search/cs?searchtype=author&query=Aasan%2C+M">Marius Aasan</a>, 
<a href="/search/cs?searchtype=author&query=Pedrini%2C+H">Helio Pedrini</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+A+R">Ad&#xed;n Ram&#xed;rez Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Vision-and-Language Algorithmic Reasoning Workshop at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The intersection of vision and language is of major interest due to the
increased focus on seamless integration between recognition and reasoning.
Scene graphs (SGs) have emerged as a useful tool for multimodal image analysis,
showing impressive performance in tasks such as Visual Question Answering
(VQA). In this work, we demonstrate that despite the effectiveness of scene
graphs in VQA tasks, current methods that utilize idealized annotated scene
graphs struggle to generalize when using predicted scene graphs extracted from
images. To address this issue, we introduce the SelfGraphVQA framework. Our
approach extracts a scene graph from an input image using a pre-trained scene
graph generator and employs semantically-preserving augmentation with
self-supervised techniques. This method improves the utilization of graph
representations in VQA tasks by circumventing the need for costly and
potentially biased annotated data. By creating alternative views of the
extracted graphs through image augmentations, we can learn joint embeddings by
optimizing the informational content in their representations using an
un-normalized contrastive approach. As we work with SGs, we experiment with
three distinct maximization strategies: node-wise, graph-wise, and
permutation-equivariant regularization. We empirically showcase the
effectiveness of the extracted scene graph for VQA and demonstrate that these
approaches enhance overall performance by highlighting the significance of
visual information. This offers a more practical solution for VQA tasks that
rely on SGs for complex reasoning questions.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01843" title="Abstract">arXiv:2310.01843</a> [<a href="/pdf/2310.01843" title="Download PDF">pdf</a>, <a href="/format/2310.01843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Feature Adapter for Dense Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xueqing Deng</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Q">Qi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaojie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fine-tuning pre-trained transformer models, e.g., Swin Transformer, are
successful in numerous downstream for dense prediction vision tasks. However,
one major issue is the cost/storage of their huge amount of parameters, which
becomes increasingly challenging to handle with the growing amount of vision
tasks. In this paper, we propose an effective approach to alleviate the issue,
namely selective feature adapter (SFA). It achieves state-of-the-art (SoTA)
performance under any given budget of trainable parameters, and demonstrates
comparable or better performance than fully fine-tuned models across various
dense tasks. Specifically, SFA consists of external adapters and internal
adapters which are sequentially operated over a transformer model. For external
adapters, we properly select the places and amount of additional multilayer
perception (MLP). For internal adapters, we transform a few task-important
parameters inside the transformer, which are automatically discovered through a
simple yet effective lottery ticket algorithm. Our experiments show that the
dual adapter module, a.k.a SFA, is essential to achieve the best trade-off on
dense vision tasks, such as segmentation, detection and depth-estimation,
outperforming other adapters with a single module.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01844" title="Abstract">arXiv:2310.01844</a> [<a href="/pdf/2310.01844" title="Download PDF">pdf</a>, <a href="/format/2310.01844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Aerodynamic Model Aided Invariant Kalman Filtering for UAV  Full-State Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xiaoyu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+F">Fujun Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zongyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qinghua Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Due to the state trajectory-independent features of invariant Kalman
filtering (InEKF), it has attracted widespread attention in the research
community for its significantly improved state estimation accuracy and
convergence under disturbance. In this paper, we formulate the full-source data
fusion navigation problem for fixed-wing unmanned aerial vehicle (UAV) within a
framework based on error state right-invariant extended Kalman filtering
(ES-RIEKF) on Lie groups. We merge measurements from a multi-rate onboard
sensor network on UAVs to achieve real-time estimation of pose, air flow
angles, and wind speed. Detailed derivations are provided, and the algorithm's
convergence and accuracy improvements over established methods like Error State
EKF (ES-EKF) and Nonlinear Complementary Filter (NCF) are demonstrated using
real-flight data from UAVs. Additionally, we introduce a semi-aerodynamic model
fusion framework that relies solely on ground-measurable parameters. We design
and train an Long Short Term Memory (LSTM) deep network to achieve drift-free
prediction of the UAV's angle of attack (AOA) and side-slip angle (SA) using
easily obtainable onboard data like control surface deflections, thereby
significantly reducing dependency on GNSS or complicated aerodynamic model
parameters. Further, we validate the algorithm's robust advantages under GNSS
denied, where flight data shows that the maximum positioning error stays within
30 meters over a 130-second denial period. To the best of our knowledge, this
study is the first to apply ES-RIEKF to full-source navigation applications for
fixed-wing UAVs, aiming to provide engineering references for designers. Our
implementations using MATLAB/Simulink will open source.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01845" title="Abstract">arXiv:2310.01845</a> [<a href="/pdf/2310.01845" title="Download PDF">pdf</a>, <a href="/format/2310.01845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Refinement of Buildings&#x27; Segmentation Models using SAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mayladan%2C+A">Ali Mayladan</a>, 
<a href="/search/cs?searchtype=author&query=Nasrallah%2C+H">Hasan Nasrallah</a>, 
<a href="/search/cs?searchtype=author&query=Moughnieh%2C+H">Hasan Moughnieh</a>, 
<a href="/search/cs?searchtype=author&query=Shukor%2C+M">Mustafa Shukor</a>, 
<a href="/search/cs?searchtype=author&query=Ghandour%2C+A+J">Ali J. Ghandour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Foundation models have excelled in various tasks but are often evaluated on
general benchmarks. The adaptation of these models for specific domains, such
as remote sensing imagery, remains an underexplored area. In remote sensing,
precise building instance segmentation is vital for applications like urban
planning. While Convolutional Neural Networks (CNNs) perform well, their
generalization can be limited. For this aim, we present a novel approach to
adapt foundation models to address existing models' generalization dropback.
Among several models, our focus centers on the Segment Anything Model (SAM), a
potent foundation model renowned for its prowess in class-agnostic image
segmentation capabilities. We start by identifying the limitations of SAM,
revealing its suboptimal performance when applied to remote sensing imagery.
Moreover, SAM does not offer recognition abilities and thus fails to classify
and tag localized objects. To address these limitations, we introduce different
prompting strategies, including integrating a pre-trained CNN as a prompt
generator. This novel approach augments SAM with recognition abilities, a first
of its kind. We evaluated our method on three remote sensing datasets,
including the WHU Buildings dataset, the Massachusetts Buildings dataset, and
the AICrowd Mapping Challenge. For out-of-distribution performance on the WHU
dataset, we achieve a 5.47% increase in IoU and a 4.81% improvement in
F1-score. For in-distribution performance on the WHU dataset, we observe a
2.72% and 1.58% increase in True-Positive-IoU and True-Positive-F1 score,
respectively. We intend to release our code repository, hoping to inspire
further exploration of foundation models for domain-specific tasks within the
remote sensing community.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01846" title="Abstract">arXiv:2310.01846</a> [<a href="/pdf/2310.01846" title="Download PDF">pdf</a>, <a href="/format/2310.01846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking and Improving Generator-Validator Consistency of Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X+L">Xiang Lisa Li</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+V">Vaishnavi Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As of September 2023, ChatGPT correctly answers "what is 7+8" with 15, but
when asked "7+8=15, True or False" it responds with "False". This inconsistency
between generating and validating an answer is prevalent in language models
(LMs) and erodes trust. In this paper, we propose a framework for measuring the
consistency between generation and validation (which we call
generator-validator consistency, or GV-consistency), finding that even GPT-4, a
state-of-the-art LM, is GV-consistent only 76% of the time. To improve the
consistency of LMs, we propose to finetune on the filtered generator and
validator responses that are GV-consistent, and call this approach consistency
fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B
from 60% to 93%, and the improvement extrapolates to unseen tasks and domains
(e.g., GV-consistency for positive style transfers extrapolates to unseen
styles like humor). In addition to improving consistency, consistency
fine-tuning improves both generator quality and validator accuracy without
using any labeled data. Evaluated across 6 tasks, including math questions,
knowledge-intensive QA, and instruction following, our method improves the
generator quality by 16% and the validator accuracy by 6.3% across all tasks.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01850" title="Abstract">arXiv:2310.01850</a> [<a href="/pdf/2310.01850" title="Download PDF">pdf</a>, <a href="/format/2310.01850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-class Network Intrusion Detection with Class Imbalance via LSTM &amp;  SMOTE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nawaz%2C+M+W">Muhammad Wasim Nawaz</a>, 
<a href="/search/cs?searchtype=author&query=Munawar%2C+R">Rashid Munawar</a>, 
<a href="/search/cs?searchtype=author&query=Mehmood%2C+A">Ahsan Mehmood</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M+U">Muhammad Mahboob Ur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Abbasi%2C+Q+H">Qammer H. Abbasi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Monitoring network traffic to maintain the quality of service (QoS) and to
detect network intrusions in a timely and efficient manner is essential. As
network traffic is sequential, recurrent neural networks (RNNs) such as long
short-term memory (LSTM) are suitable for building network intrusion detection
systems. However, in the case of a few dataset examples of the rare attack
types, even these networks perform poorly. This paper proposes to use
oversampling techniques along with appropriate loss functions to handle class
imbalance for the detection of various types of network intrusions. Our deep
learning model employs LSTM with fully connected layers to perform multi-class
classification of network attacks. We enhance the representation of minority
classes: i) through the application of the Synthetic Minority Over-sampling
Technique (SMOTE), and ii) by employing categorical focal cross-entropy loss to
apply a focal factor to down-weight examples of the majority classes and focus
more on hard examples of the minority classes. Extensive experiments on KDD99
and CICIDS2017 datasets show promising results in detecting network intrusions
(with many rare attack types, e.g., Probe, R2L, DDoS, PortScan, etc.).
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01851" title="Abstract">arXiv:2310.01851</a> [<a href="/pdf/2310.01851" title="Download PDF">pdf</a>, <a href="/format/2310.01851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An equioscillation theorem for multivariate Chebyshev approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goldsztejn%2C+A">Alexandre Goldsztejn</a> (LS2N - &#xe9;quipe OGRE, LS2N)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The equioscillation condition is extended to multivariate approximation. To
this end, it is reformulated as the synchronized oscillations between the error
maximizers and the components of a related Haar matrix kernel vector. This new
condition gives rise to a multivariate equioscillation theorem where the Haar
condition is not assumed and hence the existence and the characterization by
equioscillation become independent of uniqueness. This allows the theorem to be
applicable to problems with no strong uniqueness or even no uniqueness. A
technical additional requirement on the involved Haar matrix and its kernel
vector is proved to be sufficient for strong uniqueness. Instances of
multivariate problems with strongly unique, unique and nonunique solutions are
presented to illustrate the scope of the theorem.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01852" title="Abstract">arXiv:2310.01852</a> [<a href="/pdf/2310.01852" title="Download PDF">pdf</a>, <a href="/format/2310.01852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LanguageBind: Extending Video-Language Pretraining to N-modality by  Language-based Semantic Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxi Cui</a>, 
<a href="/search/cs?searchtype=author&query=HongFa%2C+W">Wang HongFa</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yatian Pang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C+W">Cai Wan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 1.2%
R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot
video-text retrieval, validating the high quality of our dataset. Beyond this,
our LanguageBind has achieved great improvement in the zero-shot video, audio,
depth, and infrared understanding tasks. For instance, on the LLVIP and NYU-D
datasets, LanguageBind outperforms ImageBind-huge with 23.8% and 11.1% top-1
accuracy.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01854" title="Abstract">arXiv:2310.01854</a> [<a href="/pdf/2310.01854" title="Download PDF">pdf</a>, <a href="/format/2310.01854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better  Account for Brain Language Representations?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+M">Marie-Francine Moens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IJCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">To decipher the algorithm underlying the human brain's language
representation, previous work probed brain responses to language input with
pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks.
However, full fine-tuning generally updates the entire parametric space and
distorts pre-trained features, cognitively inconsistent with the brain's robust
multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained
weights and learns task-specific embeddings to fit a task. Could prompt-tuning
generate representations that better account for the brain's language
representations than fine-tuning? If so, what kind of NLU task leads a
pre-trained model to better decode the information represented in the human
brain? We investigate these questions by comparing prompt-tuned and fine-tuned
representations in neural decoding, that is predicting the linguistic stimulus
from the brain activities evoked by the stimulus. We find that on none of the
10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning in
neural decoding, implicating that a more brain-consistent tuning method yields
representations that better correlate with brain data. Moreover, we identify
that tasks dealing with fine-grained concept meaning yield representations that
better decode brain activation patterns than other tasks, especially the
syntactic chunking task. This indicates that our brain encodes more
fine-grained concept information than shallow syntactic information when
representing languages.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01858" title="Abstract">arXiv:2310.01858</a> [<a href="/pdf/2310.01858" title="Download PDF">pdf</a>, <a href="/ps/2310.01858" title="Download PostScript">ps</a>, <a href="/format/2310.01858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A method of generating bespoke optimised keyboard layouts that  significantly reduce typing effort for Twitter users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elson%2C+E">E. Elson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the South African Computer Journal. 17 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">This study addresses the problem of generating an optimised keyboard layout
for single-finger typing on a smartphone. It offers Twitter users a
tweet-typing experience that requires less effort and time. Bodies of tweet
text for 85 popular Twitter users are used. While existing studies have
produced optimisations that may generally benefit a variety of users, this
study is unique in the sense that a bespoke optimised keyboard layout is
generated for each Twitter user based on their own tweets, thereby uniquely
benefiting them more than other users. The optimisation process is based on
moving only six letter keys from their positions on the QWERTY keyboard, and
therefore strikes an effective balance between the typing efficiency
improvements offered by an optimised keyboard layout and the effort required to
learn to use it. It is shown that a Twitter user will enjoy a reduction in
typing effort of at least 13.4%. The typical user will benefit from a 15.8%
reduction, while the highest typing effort reduction is nearly 25%. The method
presented in this study could therefore be used in practical ways to offer any
Twitter user a uniquely-improved tweeting experience.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01865" title="Abstract">arXiv:2310.01865</a> [<a href="/pdf/2310.01865" title="Download PDF">pdf</a>, <a href="/format/2310.01865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Instrumental Variable Regression with Representation  Learning for Causal Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Debo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiuyong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jixue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T+D">Thuc Duy Le</a> (UniSA STEM, University of South Australia, Australia)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17pages, 3 figures and 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper studies the challenging problem of estimating causal effects from
observational data, in the presence of unobserved confounders. The two-stage
least square (TSLS) method and its variants with a standard instrumental
variable (IV) are commonly used to eliminate confounding bias, including the
bias caused by unobserved confounders, but they rely on the linearity
assumption. Besides, the strict condition of unconfounded instruments posed on
a standard IV is too strong to be practical. To address these challenging and
practical problems of the standard IV method (linearity assumption and the
strict condition), in this paper, we use a conditional IV (CIV) to relax the
unconfounded instrument condition of standard IV and propose a non-linear CIV
regression with Confounding Balancing Representation Learning, CBRL.CIV, for
jointly eliminating the confounding bias from unobserved confounders and
balancing the observed confounders, without the linearity assumption. We
theoretically demonstrate the soundness of CBRL.CIV. Extensive experiments on
synthetic and two real-world datasets show the competitive performance of
CBRL.CIV against state-of-the-art IV-based estimators and superiority in
dealing with the non-linear situation.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01866" title="Abstract">arXiv:2310.01866</a> [<a href="/pdf/2310.01866" title="Download PDF">pdf</a>, <a href="/format/2310.01866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Route Design in Sheepdog System--Traveling Salesman Problem Formulation  and Evolutionary Computation Solution--
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imahayashi%2C+W">Wataru. Imahayashi</a>, 
<a href="/search/cs?searchtype=author&query=Tsunoda%2C+Y">Yusuke. Tsunoda</a>, 
<a href="/search/cs?searchtype=author&query=Ogura%2C+M">Masaki. Ogura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In this study, we consider the guidance control problem of the sheepdog
system, which involves the guidance of the flock using the characteristics of
the sheepdog and sheep. Sheepdog systems require a strategy to guide sheep
agents to a target value using a small number of sheepdog agents, and various
methods have been proposed. Previous studies have proposed a guidance control
law to guide a herd of sheep reliably, but the movement distance of a sheepdog
required for guidance has not been considered. Therefore, in this study, we
propose a novel guidance algorithm in which a supposedly efficient route for
guiding a flock of sheep is designed via Traveling Salesman Problem and
evolutionary computation. Numerical simulations were performed to confirm
whether sheep flocks could be guided and controlled using the obtained guidance
routes. We specifically revealed that the proposed method reduces both the
guidance failure rate and the guidance distance.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01870" title="Abstract">arXiv:2310.01870</a> [<a href="/pdf/2310.01870" title="Download PDF">pdf</a>, <a href="/format/2310.01870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepDecipher: Accessing and Investigating Neuron Activation in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garde%2C+A">Albert Garde</a>, 
<a href="/search/cs?searchtype=author&query=Kran%2C+E">Esben Kran</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages (9 total), 1 figure, submitted to NeurIPS 2023 Workshop XAIA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As large language models (LLMs) become more capable, there is an urgent need
for interpretable and transparent tools. Current methods are difficult to
implement, and accessible tools to analyze model internals are lacking. To
bridge this gap, we present DeepDecipher - an API and interface for probing
neurons in transformer models' MLP layers. DeepDecipher makes the outputs of
advanced interpretability techniques for LLMs readily available. The
easy-to-use interface also makes inspecting these complex models more
intuitive. This paper outlines DeepDecipher's design and capabilities. We
demonstrate how to analyze neurons, compare models, and gain insights into
model behavior. For example, we contrast DeepDecipher's functionality with
similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher
enables efficient, scalable analysis of LLMs. By granting access to
state-of-the-art interpretability methods, DeepDecipher makes LLMs more
transparent, trustworthy, and safe. Researchers, engineers, and developers can
quickly diagnose issues, audit systems, and advance the field.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01875" title="Abstract">arXiv:2310.01875</a> [<a href="/pdf/2310.01875" title="Download PDF">pdf</a>, <a href="/format/2310.01875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Stable Backdoor Purification through Feature Shift Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+R">Rui Min</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zeyu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minhao Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 paper. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">It has been widely observed that deep neural networks (DNN) are vulnerable to
backdoor attacks where attackers could manipulate the model behavior
maliciously by tampering with a small set of training samples. Although a line
of defense methods is proposed to mitigate this threat, they either require
complicated modifications to the training process or heavily rely on the
specific model architecture, which makes them hard to deploy into real-world
applications. Therefore, in this paper, we instead start with fine-tuning, one
of the most common and easy-to-deploy backdoor defenses, through comprehensive
evaluations against diverse attack scenarios. Observations made through initial
experiments show that in contrast to the promising defensive results on high
poisoning rates, vanilla tuning methods completely fail at low poisoning rate
scenarios. Our analysis shows that with the low poisoning rate, the
entanglement between backdoor and clean features undermines the effect of
tuning-based defenses. Therefore, it is necessary to disentangle the backdoor
and clean features in order to improve backdoor purification. To address this,
we introduce Feature Shift Tuning (FST), a method for tuning-based backdoor
purification. Specifically, FST encourages feature shifts by actively deviating
the classifier weights from the originally compromised weights. Extensive
experiments demonstrate that our FST provides consistently stable performance
under different attack settings. Additionally, it is also convenient to deploy
in real-world scenarios with significantly reduced computation costs. Our codes
are available at
\url{https://github.com/AISafety-HKUST/stable_backdoor_purification}.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01876" title="Abstract">arXiv:2310.01876</a> [<a href="/pdf/2310.01876" title="Download PDF">pdf</a>, <a href="/format/2310.01876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dual Attentive Generative Adversarial Network for Remote Sensing Image  Change Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Luyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+C">ChaoChen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+a+S">and ShanYing Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Remote sensing change detection between bi-temporal images receives growing
concentration from researchers. However, comparing two bi-temporal images for
detecting changes is challenging, as they demonstrate different appearances. In
this paper, we propose a dual attentive generative adversarial network for
achieving very high-resolution remote sensing image change detection tasks,
which regards the detection model as a generator and attains the optimal
weights of the detection model without increasing the parameters of the
detection model through generative-adversarial strategy, boosting the spatial
contiguity of predictions. Moreover, We design a multi-level feature extractor
for effectively fusing multi-level features, which adopts the pre-trained model
to extract multi-level features from bi-temporal images and introduces
aggregate connections to fuse them. To strengthen the identification of
multi-scale objects, we propose a multi-scale adaptive fusion module to
adaptively fuse multi-scale features through various receptive fields and
design a context refinement module to explore contextual dependencies.
Moreover, the DAGAN framework utilizes the 4-layer convolution network as a
discriminator to identify whether the synthetic image is fake or real.
Extensive experiments represent that the DAGAN framework has better performance
with 85.01% mean IoU and 91.48% mean F1 score than advanced methods on the
LEVIR dataset.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01878" title="Abstract">arXiv:2310.01878</a> [<a href="/pdf/2310.01878" title="Download PDF">pdf</a>, <a href="/format/2310.01878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Workflow Security in Multi-Cloud Environments through  Monitoring and Adaptation upon Cloud Service and Network Security Violations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soveizi%2C+N">Nafiseh Soveizi</a>, 
<a href="/search/cs?searchtype=author&query=Karastoyanova%2C+D">Dimka Karastoyanova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Cloud computing has emerged as a crucial solution for handling data- and
compute-intensive workflows, offering scalability to address dynamic demands.
However, ensuring the secure execution of workflows in the untrusted
multi-cloud environment poses significant challenges, given the sensitive
nature of the involved data and tasks. The lack of comprehensive approaches for
detecting attacks during workflow execution, coupled with inadequate measures
for reacting to security and privacy breaches has been identified in the
literature. To close this gap, in this work, we propose an approach that
focuses on monitoring cloud services and networks to detect security violations
during workflow executions. Upon detection, our approach selects the optimal
adaptation action to minimize the impact on the workflow. To mitigate the
uncertain cost associated with such adaptations and their potential impact on
other tasks in the workflow, we employ adaptive learning to determine the most
suitable adaptation action. Our approach is evaluated based on the performance
of the detection procedure and the impact of the selected adaptations on the
workflows.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01879" title="Abstract">arXiv:2310.01879</a> [<a href="/pdf/2310.01879" title="Download PDF">pdf</a>, <a href="/format/2310.01879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IGA Using Offset-based Overlapping Domain Parameterizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kargaran%2C+S">Somayeh Kargaran</a>, 
<a href="/search/math?searchtype=author&query=J%C3%BCttler%2C+B">Bert J&#xfc;ttler</a>, 
<a href="/search/math?searchtype=author&query=Takacs%2C+T">Thomas Takacs</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer-Aided Design, 139: 103087, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Isogeometric analysis (IGA) is a numerical method that connects
computer-aided design (CAD) with finite element analysis (FEA). In CAD the
computational domain is usually represented by B-spline or NURBS patches. Given
a NURBS parameterization of the domain, an isogeometric discretization is
defined on the domain using the same NURBS basis as for the domain
parameterization. Ideally, such an isogeometric discretization allows an exact
representation of the underlying CAD model.
<br />CAD models usually represent only the boundary of the object, thus, for
planar domains, it is given as a collection of curves. Finding a suitable
parameterization of the interior is one of the major issues in IGA, similar to
the mesh generation process in FEA. The objective of this parameterization
problem is to obtain a set of patches, which exactly represent the boundary of
the domain and which are parameterized regularly and without
self-intersections. This can be achieved by segmenting the domain into patches
which are matching along interfaces, or by covering the domain with overlapping
patches. In this paper we follow the second approach.
<br />To construct from a given boundary a planar parameterization suitable for
IGA, we propose an offset-based domain parameterization algorithm. Given a
boundary curve, we obtain an inner curve by generalized offsetting. Those two
curves define a ring-shaped patch, which has a hole that can be covered by a
multi-cell domain. Consequently, the domain is represented as a union of two
overlapping subdomains which are both regularly parameterized. On such a
configuration, one can employ the overlapping multi-patch method introduced in
(Kargaran, J\"uttler, Kleiss, Mantzaflaris, Takacs; CMAME, 2019), to solve PDEs
on the given domain. The performance of the proposed method is reported in
several numerical examples, considering different shapes of the domain.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01880" title="Abstract">arXiv:2310.01880</a> [<a href="/pdf/2310.01880" title="Download PDF">pdf</a>, <a href="/format/2310.01880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoCast++: Enhancing World Event Prediction with Zero-shot  Ranking-based Context Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Seraj%2C+R">Raihan Seraj</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiawei He</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lili Meng</a>, 
<a href="/search/cs?searchtype=author&query=Sylvain%2C+T">Tristan Sylvain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine-based prediction of real-world events is garnering attention due to
its potential for informed decision-making. Whereas traditional forecasting
predominantly hinges on structured data like time-series, recent breakthroughs
in language models enable predictions using unstructured text. In particular,
(Zou et al., 2022) unveils AutoCast, a new benchmark that employs news articles
for answering forecasting queries. Nevertheless, existing methods still trail
behind human performance. The cornerstone of accurate forecasting, we argue,
lies in identifying a concise, yet rich subset of news snippets from a vast
corpus. With this motivation, we introduce AutoCast++, a zero-shot
ranking-based context retrieval system, tailored to sift through expansive news
document collections for event forecasting. Our approach first re-ranks
articles based on zero-shot question-passage relevance, honing in on
semantically pertinent news. Following this, the chosen articles are subjected
to zero-shot summarization to attain succinct context. Leveraging a pre-trained
language model, we conduct both the relevance evaluation and article
summarization without needing domain-specific training. Notably, recent
articles can sometimes be at odds with preceding ones due to new facts or
unanticipated incidents, leading to fluctuating temporal dynamics. To tackle
this, our re-ranking mechanism gives preference to more recent articles, and we
further regularize the multi-passage representation learning to align with
human forecaster responses made on different dates. Empirical results
underscore marked improvements across multiple metrics, improving the
performance for multiple-choice questions (MCQ) by 48% and true/false (TF)
questions by up to 8%.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01881" title="Abstract">arXiv:2310.01881</a> [<a href="/pdf/2310.01881" title="Download PDF">pdf</a>, <a href="/format/2310.01881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple  Scale Neural Radiance Field Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kurabayashi%2C+S">Shuichi Kurabayashi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in Neural Radiance Fields (NeRF) have demonstrated
significant potential for representing 3D scene appearances as implicit neural
networks, enabling the synthesis of high-fidelity novel views. However, the
lengthy training and rendering process hinders the widespread adoption of this
promising technique for real-time rendering applications. To address this
issue, we present an effective adaptive multi-NeRF method designed to
accelerate the neural rendering process for large scenes with unbalanced
workloads due to varying scene complexities.
<br />Our method adaptively subdivides scenes into axis-aligned bounding boxes
using a tree hierarchy approach, assigning smaller NeRFs to different-sized
subspaces based on the complexity of each scene portion. This ensures the
underlying neural representation is specific to a particular part of the scene.
We optimize scene subdivision by employing a guidance density grid, which
balances representation capability for each Multilayer Perceptron (MLP).
Consequently, samples generated by each ray can be sorted and collected for
parallel inference, achieving a balanced workload suitable for small MLPs with
consistent dimensions for regular and GPU-friendly computations. We aosl
demonstrated an efficient NeRF sampling strategy that intrinsically adapts to
increase parallelism, utilization, and reduce kernel calls, thereby achieving
much higher GPU utilization and accelerating the rendering process.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01882" title="Abstract">arXiv:2310.01882</a> [<a href="/pdf/2310.01882" title="Download PDF">pdf</a>, <a href="/ps/2310.01882" title="Download PostScript">ps</a>, <a href="/format/2310.01882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fortran performance optimisation and auto-parallelisation by leveraging  MLIR-based domain specific abstractions in Flang
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brown%2C+N">Nick Brown</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+M">Maurice Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Lydike%2C+A">Anton Lydike</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+E">Emilien Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Grosser%2C+T">Tobias Grosser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Author accepted version of paper in ACM Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (SC-W 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">MLIR has become popular since it was open sourced in 2019. A sub-project of
LLVM, the flexibility provided by MLIR to represent Intermediate
Representations (IR) as dialects at different abstraction levels, to mix these,
and to leverage transformations between dialects provides opportunities for
automated program optimisation and parallelisation. In addition to general
purpose compilers built upon MLIR, domain specific abstractions have also been
developed.
<br />In this paper we explore complimenting the Flang MLIR general purpose
compiler by combining with the domain specific Open Earth Compiler's MLIR
stencil dialect. Developing transformations to discover and extracts stencils
from Fortran, this specialisation delivers between a 2 and 10 times performance
improvement for our benchmarks on a Cray supercomputer compared to using Flang
alone. Furthermore, by leveraging existing MLIR transformations we develop an
auto-parallelisation approach targeting multi-threaded and distributed memory
parallelism, and optimised execution on GPUs, without any modifications to the
serial Fortran source code.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01883" title="Abstract">arXiv:2310.01883</a> [<a href="/pdf/2310.01883" title="Download PDF">pdf</a>, <a href="/ps/2310.01883" title="Download PostScript">ps</a>, <a href="/format/2310.01883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduction and efficient solution of MILP models of mixed Hamming  packings yielding improved upper bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naszvadi%2C+P">P&#xe9;ter Naszvadi</a>, 
<a href="/search/cs?searchtype=author&query=Koniorczyk%2C+M">M&#xe1;ty&#xe1;s Koniorczyk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">Mixed Hamming packings are considered: the maximal cardinality given a
minimum codeword Hamming distance of mixed codes is addressed via mixed integer
programming models. Adopting the concept of contact graph from classical
continuous sphere packing problems, a reduction technique for the models is
introduced, which enables their efficient solution. Several best known upper
bounds are improved and some of them are found to be sharp.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01884" title="Abstract">arXiv:2310.01884</a> [<a href="/pdf/2310.01884" title="Download PDF">pdf</a>, <a href="/format/2310.01884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Hybrid Model for Enhanced Stock Market Predictions Using  Improved VMD and Stacked Informer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Hongyi Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces an innovative adaptive hybrid model for stock market
predictions, leveraging the capabilities of an enhanced Variational Mode
Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated
with an adaptive loss function. Through rigorous experimentation, the proposed
model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates
significant proficiency in addressing the intricate dynamics and volatile
nature of stock market data. Experimental results, derived from multiple
benchmark datasets, underscore the model's superiority in terms of prediction
accuracy, responsiveness, and generalization capabilities over traditional and
other hybrid models. The research further highlights potential avenues for
optimization and introduces future directions to enhance predictive modeling,
especially for small enterprises and feature engineering.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01886" title="Abstract">arXiv:2310.01886</a> [<a href="/pdf/2310.01886" title="Download PDF">pdf</a>, <a href="/format/2310.01886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective and Parameter-Efficient Reusing Fine-Tuned Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Baijiong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+a+Z">and Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Many pre-trained large-scale models provided online have become highly
effective in transferring to downstream tasks. At the same time, various
task-specific models fine-tuned on these pre-trained models are available
online for public use. In practice, as collecting task-specific data is
labor-intensive and fine-tuning the large pre-trained models is computationally
expensive, one can reuse task-specific finetuned models to deal with downstream
tasks. However, using a model per task causes a heavy burden on storage and
serving. Recently, many training-free and parameter-efficient methods have been
proposed for reusing multiple fine-tuned task-specific models into a single
multi-task model. However, these methods exhibit a large accuracy gap compared
with using a fine-tuned model per task. In this paper, we propose
Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing
Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task
vector into a merged model by magnitude pruning. For reusing LoRA fine-tuned
models, we propose PERU-LoRA use a lower-rank matrix to approximate the LoRA
matrix by singular value decomposition. Both PERUFFT and PERU-LoRA are
training-free. Extensive experiments conducted on computer vision and natural
language process tasks demonstrate the effectiveness and parameter-efficiency
of the proposed methods. The proposed PERU-FFT and PERU-LoRA outperform
existing reusing model methods by a large margin and achieve comparable
performance to using a fine-tuned model per task.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01889" title="Abstract">arXiv:2310.01889</a> [<a href="/pdf/2310.01889" title="Download PDF">pdf</a>, <a href="/format/2310.01889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ring Attention with Blockwise Transformers for Near-Infinite Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zaharia%2C+M">Matei Zaharia</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Transformers have emerged as the architecture of choice for many
state-of-the-art AI models, showcasing exceptional performance across a wide
range of AI applications. However, the memory demands imposed by Transformers
limit their ability to handle long sequences, thereby creating challenges for
tasks involving extended sequences or long-term dependencies. We present a
distinct approach, Ring Attention, which leverages blockwise computation of
self-attention to distribute long sequences across multiple devices while
concurrently overlapping the communication of key-value blocks with the
computation of blockwise attention. By processing longer input sequences while
maintaining memory efficiency, Ring Attention enables training and inference of
sequences that are device count times longer than those of prior
memory-efficient Transformers, effectively eliminating the memory constraints
imposed by individual devices. Extensive experiments on language modeling tasks
demonstrate the effectiveness of Ring Attention in allowing large sequence
input size and improving performance.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01892" title="Abstract">arXiv:2310.01892</a> [<a href="/pdf/2310.01892" title="Download PDF">pdf</a>, <a href="/ps/2310.01892" title="Download PostScript">ps</a>, <a href="/format/2310.01892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FiGURe: Simple and Efficient Unsupervised Node Representations with  Filter Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ekbote%2C+C">Chanakya Ekbote</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A+P">Ajinkya Pankaj Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+A">Arun Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Bairi%2C+R">Ramakrishna Bairi</a>, 
<a href="/search/cs?searchtype=author&query=Sellamanickam%2C+S">Sundararajan Sellamanickam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unsupervised node representations learnt using contrastive learning-based
methods have shown good performance on downstream tasks. However, these methods
rely on augmentations that mimic low-pass filters, limiting their performance
on tasks requiring different eigen-spectrum parts. This paper presents a simple
filter-based augmentation method to capture different parts of the
eigen-spectrum. We show significant improvements using these augmentations.
Further, we show that sharing the same weights across these different filter
augmentations is possible, reducing the computational load. In addition,
previous works have shown that good performance on downstream tasks requires
high dimensional representations. Working with high dimensions increases the
computations, especially when multiple augmentations are involved. We mitigate
this problem and recover good performance through lower dimensional embeddings
using simple random Fourier feature projections. Our method, FiGURe achieves an
average gain of up to 4.4\%, compared to the state-of-the-art unsupervised
models, across all datasets in consideration, both homophilic and heterophilic.
Our code can be found at: https://github.com/microsoft/figure.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01893" title="Abstract">arXiv:2310.01893</a> [<a href="/pdf/2310.01893" title="Download PDF">pdf</a>, <a href="/format/2310.01893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimplePIM: A Software Framework for Productive and Efficient  Processing-in-Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinfan Chen</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Luna%2C+J">Juan G&#xf3;mez-Luna</a>, 
<a href="/search/cs?searchtype=author&query=Hajj%2C+I+E">Izzat El Hajj</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuxin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Software Engineering (cs.SE)

</div>
<p class="mathjax">Data movement between memory and processors is a major bottleneck in modern
computing systems. The processing-in-memory (PIM) paradigm aims to alleviate
this bottleneck by performing computation inside memory chips. Real PIM
hardware (e.g., the UPMEM system) is now available and has demonstrated
potential in many applications. However, programming such real PIM hardware
remains a challenge for many programmers.
<br />This paper presents a new software framework, SimplePIM, to aid programming
real PIM systems. The framework processes arrays of arbitrary elements on a PIM
device by calling iterator functions from the host and provides primitives for
communication among PIM cores and between PIM and the host system. We implement
SimplePIM for the UPMEM PIM system and evaluate it on six major applications.
Our results show that SimplePIM enables 66.5% to 83.1% reduction in lines of
code in PIM programs. The resulting code leads to higher performance (between
10% and 37% speedup) than hand-optimized code in three applications and
provides comparable performance in three others. SimplePIM is fully and freely
available at https://github.com/CMU-SAFARI/SimplePIM.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01894" title="Abstract">arXiv:2310.01894</a> [<a href="/pdf/2310.01894" title="Download PDF">pdf</a>, <a href="/format/2310.01894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Waveform Manipulation Against DNN-based Modulation Classification  Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varkatzas%2C+D">Dimitrios Varkatzas</a>, 
<a href="/search/cs?searchtype=author&query=Argyriou%2C+A">Antonios Argyriou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE MILCOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper we propose a method for defending against an eavesdropper that
uses a Deep Neural Network (DNN) for learning the modulation of wireless
communication signals. Our method is based on manipulating the emitted waveform
with the aid of a continuous time frequency-modulated (FM) obfuscating signal
that is mixed with the modulated data. The resulting waveform allows a
legitimate receiver (LRx) to demodulate the data but it increases the test
error of a pre-trained or adversarially-trained DNN classifier at the
eavesdropper. The scheme works for analog modulation and digital single carrier
and multi carrier orthogonal frequency division multiplexing (OFDM) waveforms,
while it can implemented in frame-based wireless protocols. The results
indicate that careful selection of the parameters of the obfuscating waveform
can drop classification performance at the eavesdropper to less than 10% in
AWGN and fading channels with no performance loss at the LRx.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01896" title="Abstract">arXiv:2310.01896</a> [<a href="/pdf/2310.01896" title="Download PDF">pdf</a>, <a href="/format/2310.01896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the Mittag-Leffler Function of a Matrix Argument
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cardoso%2C+J+R">Jo&#xe3;o R. Cardoso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">It is well-known that the two-parameter Mittag-Leffler (ML) function plays a
key role in Fractional Calculus. In this paper, we address the problem of
computing this function, when its argument is a square matrix. Effective
methods for solving this problem involve the computation of higher order
derivatives or require the use of mixed precision arithmetic. In this paper, we
provide an alternative method that is derivative-free and works entirely using
IEEE standard double precision arithmetic. If certain conditions are satisfied,
our method uses a Taylor series representation for the ML function; if not, it
switches to a Schur-Parlett technique that will be combined with the Cauchy
integral formula. A detailed discussion on the choice of a convenient contour
is included. Theoretical and numerical issues regarding the performance of the
proposed algorithm are discussed. A set of numerical experiments shows that our
novel approach is competitive with the state-of-the-art method for IEEE double
precision arithmetic, in terms of accuracy and CPU time. For matrices whose
Schur decomposition has large blocks with clustered eigenvalues, our method far
outperforms the other. Since our method does not require the efficient
computation of higher order derivatives, it has the additional advantage of
being easily extended to other matrix functions (e.g., special functions).
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01897" title="Abstract">arXiv:2310.01897</a> [<a href="/pdf/2310.01897" title="Download PDF">pdf</a>, <a href="/format/2310.01897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MFOS: Model-Free &amp; One-Shot Object Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">JongMin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Cabon%2C+Y">Yohann Cabon</a>, 
<a href="/search/cs?searchtype=author&query=Br%C3%A9gier%2C+R">Romain Br&#xe9;gier</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Sungjoo Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Revaud%2C+J">Jerome Revaud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing learning-based methods for object pose estimation in RGB images are
mostly model-specific or category based. They lack the capability to generalize
to new object categories at test time, hence severely hindering their
practicability and scalability. Notably, recent attempts have been made to
solve this issue, but they still require accurate 3D data of the object surface
at both train and test time. In this paper, we introduce a novel approach that
can estimate in a single forward pass the pose of objects never seen during
training, given minimum input. In contrast to existing state-of-the-art
approaches, which rely on task-specific modules, our proposed model is entirely
based on a transformer architecture, which can benefit from recently proposed
3D-geometry general pretraining. We conduct extensive experiments and report
state-of-the-art one-shot performance on the challenging LINEMOD benchmark.
Finally, extensive ablations allow us to determine good practices with this
relatively new type of architecture in the field.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01900" title="Abstract">arXiv:2310.01900</a> [<a href="/pdf/2310.01900" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Collaborative System of Systems Simulation of Urban Air Mobility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Naeem%2C+N">Nabih Naeem</a>, 
<a href="/search/eess?searchtype=author&query=Ratei%2C+P">Patrick Ratei</a>, 
<a href="/search/eess?searchtype=author&query=Prakasha%2C+P+S">Prajwal Shiva Prakasha</a>, 
<a href="/search/eess?searchtype=author&query=Asmer%2C+L">Lukas Asmer</a>, 
<a href="/search/eess?searchtype=author&query=Jaksche%2C+R">Roman Jaksche</a>, 
<a href="/search/eess?searchtype=author&query=Pak%2C+H">Henry Pak</a>, 
<a href="/search/eess?searchtype=author&query=Schweiger%2C+K">Karolin Schweiger</a>, 
<a href="/search/eess?searchtype=author&query=Velieva%2C+A">Asija Velieva</a>, 
<a href="/search/eess?searchtype=author&query=Naser%2C+F">Fares Naser</a>, 
<a href="/search/eess?searchtype=author&query=Swaid%2C+M">Majed Swaid</a>, 
<a href="/search/eess?searchtype=author&query=Pertz%2C+J">Jan Pertz</a>, 
<a href="/search/eess?searchtype=author&query=Niklass%2C+M">Malte Niklass</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 18 figures, CEAS Special Issue for HorizonUAM Project
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The implementation of Urban Air Mobility represents a complex challenge in
aviation due to the high degree of innovation required across various domains
to realize it. From the use of advanced aircraft powered by novel technologies,
the management of the air space to enable high density operations, to the
operation of vertidromes serving as a start and end point of the flights, Urban
Air Mobility paradigm necessitates significant innovation in many aspects of
civil aviation as we know it today. In order to understand and assess the many
facets of this new paradigm, a Collaborative Agent-Based Simulation is
developed to holistically evaluate the System of Systems through the modeling
of the stakeholders and their interactions as per the envisioned Concept of
Operations. To this end, models of vertidrome air-side operations,
unmanned/manned air space management, demand estimation and passenger mode
choice, vehicle operator cost and revenues, vehicle design, and fleet
management are brought together into a System of Systems Simulation of Urban
Air Mobility. Through collaboration, higher fidelity models of each domain can
be integrated into a single environment achieving fidelity levels not easily
achievable otherwise. Furthermore, the integration enables the capture of
cross-domain effects and allows domain-specific studies to be evaluated at a
holistic level. This work demonstrates the Collaborative Simulation and the
process of building it through the integration of several geographically
distributed tools into an Agent-Based Simulation without the need for sharing
code.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01904" title="Abstract">arXiv:2310.01904</a> [<a href="/pdf/2310.01904" title="Download PDF">pdf</a>, <a href="/format/2310.01904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Benchmark: Detecting Diverse Anomalies in Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arad%2C+Y">Yoav Arad</a>, 
<a href="/search/cs?searchtype=author&query=Werman%2C+M">Michael Werman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Video Anomaly Detection (VAD) plays a crucial role in modern surveillance
systems, aiming to identify various anomalies in real-world situations.
However, current benchmark datasets predominantly emphasize simple,
single-frame anomalies such as novel object detection. This narrow focus
restricts the advancement of VAD models. In this research, we advocate for an
expansion of VAD investigations to encompass intricate anomalies that extend
beyond conventional benchmark boundaries. To facilitate this, we introduce two
datasets, HMDB-AD and HMDB-Violence, to challenge models with diverse
action-based anomalies. These datasets are derived from the HMDB51 action
recognition dataset. We further present Multi-Frame Anomaly Detection (MFAD), a
novel method built upon the AI-VAD framework. AI-VAD utilizes single-frame
features such as pose estimation and deep image encoding, and two-frame
features such as object velocity. They then apply a density estimation
algorithm to compute anomaly scores. To address complex multi-frame anomalies,
we add a deep video encoding features capturing long-range temporal
dependencies, and logistic regression to enhance final score calculation.
Experimental results confirm our assumptions, highlighting existing models
limitations with new anomaly types. MFAD excels in both simple and complex
anomaly detection scenarios.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01905" title="Abstract">arXiv:2310.01905</a> [<a href="/pdf/2310.01905" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Driven Design in Software Development: A Systematic Literature  Review on Implementation, Challenges, and Effectiveness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%96zkan%2C+O">Ozan &#xd6;zkan</a>, 
<a href="/search/cs?searchtype=author&query=Babur%2C+%C3%96">&#xd6;nder Babur</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Brand%2C+M">Mark van der Brand</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review: Submitted to ACM Computing Surveys on 14 August 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Domain-Driven Design (DDD) addresses software challenges, gaining
attention for refactoring, reimplementation, and adoption. It centers on domain
knowledge to solve complex business problems. Objective: This Systematic
Literature Review (SLR) analyzes DDD research in software development to assess
its effectiveness in solving architecture problems, identify challenges, and
explore outcomes. Method: We selected 36 peer-reviewed studies and conducted
quantitative and qualitative analysis. Results: DDD effectively improved
software systems, emphasizing Ubiquitous Language, Bounded Context, and Domain
Events. DDD in microservices gained prominence for system decomposition. Some
studies lacked empirical evaluations, identifying challenges in onboarding and
expertise. Conclusion: Adopting DDD benefits software development, involving
stakeholders like engineers, architects, managers, and domain experts. More
empirical evaluations and open discussions on challenges are needed.
Collaboration between academia and industry advances DDD adoption and knowledge
transfer in projects.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01907" title="Abstract">arXiv:2310.01907</a> [<a href="/pdf/2310.01907" title="Download PDF">pdf</a>, <a href="/ps/2310.01907" title="Download PostScript">ps</a>, <a href="/format/2310.01907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coherent Taylor expansion as a bimonad
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ehrhard%2C+T">Thomas Ehrhard</a> (IRIF (UMR\_8243), Inria), 
<a href="/search/cs?searchtype=author&query=Walch%2C+A">Aymeric Walch</a> (IRIF (UMR\_8243))
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We extend the recently introduced setting of coherent differentiation for
taking into account not only differentiation, but also Taylor expansion in
categories which are not necessarily (left)additive.The main idea consists in
extending summability into an infinitary functor which intuitively maps any
object to the object of its countable summable families.This functor is endowed
with a canonical structure of bimonad.In a linear logical categorical setting,
Taylor expansion is then axiomatized as a distributive law between this
summability functor and the resource comonad (aka.~exponential), allowing to
extend the summability functor into a bimonad on the Kleisli category of the
resource comonad: this extended functor computes the Taylor expansion of the
(nonlinear) morphisms of the Kleisli category.We also show how this categorical
axiomatizations of Taylor expansion can be generalized to arbitrary cartesian
categories, leading to a general theory of Taylor expansion formally similar to
that of differential cartesian categories, although it does not require the
underlying cartesian category to be left additive.We provide several examples
of concrete categories which arise in denotational semantics and feature such
analytic structures.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01910" title="Abstract">arXiv:2310.01910</a> [<a href="/pdf/2310.01910" title="Download PDF">pdf</a>, <a href="/format/2310.01910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional independence on semiring relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hannula%2C+M">Miika Hannula</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Conditional independence plays a foundational role in database theory,
probability theory, information theory, and graphical models. In databases,
conditional independence appears in database normalization and is known as the
(embedded) multivalued dependency. Many properties of conditional independence
are shared across various domains, and to some extent these commonalities can
be studied through a measure-theoretic approach. The present paper proposes an
alternative approach via semiring relations, defined by extending database
relations with tuple annotations from some commutative semiring. Integrating
various interpretations of conditional independence in this context, we
investigate how the choice of the underlying semiring impacts the corresponding
axiomatic and decomposition properties. We specifically identify positivity and
multiplicative cancellativity as the key semiring properties that enable
extending results from the relational context to the broader semiring
framework. Additionally, we explore the relationships between different
conditional independence notions through model theory, and consider how methods
to test logical consequence and validity generalize from database theory and
information theory to semiring relations.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01911" title="Abstract">arXiv:2310.01911</a> [<a href="/pdf/2310.01911" title="Download PDF">pdf</a>, <a href="/ps/2310.01911" title="Download PostScript">ps</a>, <a href="/format/2310.01911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Voltage Stability Boundary Under High Variability of  Renewables Using Differential Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+D">Dan Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wolter%2C+F">Franz-Erich Wolter</a>, 
<a href="/search/eess?searchtype=author&query=Geng%2C+S">Sijia Geng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes a novel method rooted in differential geometry to
approximate the voltage stability boundary of power systems under high
variability of renewable generation. We extract intrinsic geometric information
of the power flow solution manifold at a given operating point. Specifically,
coefficients of the Levi-Civita connection are constructed to approximate the
geodesics of the manifold starting at an operating point along any interested
directions that represent possible fluctuations in generation and load. Then,
based on the geodesic approximation, we further predict the voltage collapse
point by solving a few univariate quadratic equations. Conventional methods
mostly rely on either expensive numerical continuation at specified directions
or numerical optimization. Instead, the proposed approach constructs the
Christoffel symbols of the second kind from the Riemannian metric tensors to
characterize the complete local geometry which is then extended to the
proximity of the stability boundary with efficient computations. As a result,
this approach is suitable to handle high-dimensional variability in operating
points due to the large-scale integration of renewable resources. Using various
case studies, we demonstrate the advantages of the proposed method and provide
additional insights and discussions on voltage stability in renewable-rich
power systems.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01914" title="Abstract">arXiv:2310.01914</a> [<a href="/pdf/2310.01914" title="Download PDF">pdf</a>, <a href="/ps/2310.01914" title="Download PostScript">ps</a>, <a href="/format/2310.01914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stencil-HMLS: A multi-layered approach to the automatic optimisation of  stencil codes on FPGA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodriguez-Canal%2C+G">Gabriel Rodriguez-Canal</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+N">Nick Brown</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+M">Maurice Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+E">Emilien Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Lydike%2C+A">Anton Lydike</a>, 
<a href="/search/cs?searchtype=author&query=Grosser%2C+T">Tobias Grosser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Author accepted version which appears in ACM Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (SC-W 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF); Programming Languages (cs.PL)

</div>
<p class="mathjax">The challenges associated with effectively programming FPGAs have been a
major blocker in popularising reconfigurable architectures for HPC workloads.
However new compiler technologies, such as MLIR, are providing new capabilities
which potentially deliver the ability to extract domain specific information
and drive automatic structuring of codes for FPGAs.
<br />In this paper we explore domain specific optimisations for stencils, a
fundamental access pattern in scientific computing, to obtain high performance
on FPGAs via automated code structuring. We propose Stencil-HMLS, a
multi-layered approach to automatic optimisation of stencil codes and introduce
the HLS dialect, which brings FPGA programming into the MLIR ecosystem. Using
the PSyclone Fortran DSL, we demonstrate an improvement of 14-100$\times$ with
respect to the next best performant state-of-the-art tool. Furthermore, our
approach is 14 to 92 times more energy efficient than the next most energy
efficient approach.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01916" title="Abstract">arXiv:2310.01916</a> [<a href="/pdf/2310.01916" title="Download PDF">pdf</a>, <a href="/ps/2310.01916" title="Download PostScript">ps</a>, <a href="/format/2310.01916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verified completeness in Henkin-style for intuitionistic propositional  logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Huayu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bentzen%2C+B">Bruno Bentzen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Joint proceedings of the Third International Workshop on Logics
  for New-Generation Artificial Intelligence and the International Workshop on
  Logic, AI and Law, pp.36-48, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">This paper presents a formalization of the classical proof of completeness in
Henkin-style developed by Troelstra and van Dalen for intuitionistic logic with
respect to Kripke models. The completeness proof incorporates their insights in
a fresh and elegant manner that is better suited for mechanization. We discuss
details of our implementation in the Lean theorem prover with emphasis on the
prime extension lemma and construction of the canonical model. Our
implementation is restricted to a system of intuitionistic propositional logic
with implication, conjunction, disjunction, and falsity given in terms of a
Hilbert-style axiomatization. As far as we know, our implementation is the
first verified Henkin-style proof of completeness for intuitionistic logic
following Troelstra and van Dalen's method in the literature. The full source
code can be found online at https://github.com/bbentzen/ipl.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01917" title="Abstract">arXiv:2310.01917</a> [<a href="/pdf/2310.01917" title="Download PDF">pdf</a>, <a href="/format/2310.01917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Evaluation Framework: Best Practices for Human Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bojic%2C+I">Iva Bojic</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jessica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S+Y">Si Yuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+Q+C">Qi Chwen Ong</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Car%2C+J">Josip Car</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Human evaluation plays a crucial role in Natural Language Processing (NLP) as
it assesses the quality and relevance of developed systems, thereby
facilitating their enhancement. However, the absence of widely accepted human
evaluation metrics in NLP hampers fair comparisons among different systems and
the establishment of universal assessment standards. Through an extensive
analysis of existing literature on human evaluation metrics, we identified
several gaps in NLP evaluation methodologies. These gaps served as motivation
for developing our own hierarchical evaluation framework. The proposed
framework offers notable advantages, particularly in providing a more
comprehensive representation of the NLP system's performance. We applied this
framework to evaluate the developed Machine Reading Comprehension system, which
was utilized within a human-AI symbiosis model. The results highlighted the
associations between the quality of inputs and outputs, underscoring the
necessity to evaluate both components rather than solely focusing on outputs.
In future work, we will investigate the potential time-saving benefits of our
proposed framework for evaluators assessing NLP systems.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01924" title="Abstract">arXiv:2310.01924</a> [<a href="/pdf/2310.01924" title="Download PDF">pdf</a>, <a href="/format/2310.01924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoFormer for Position Aware Multiple Instance Learning in Whole Slide  Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pochet%2C+E">Etienne Pochet</a>, 
<a href="/search/cs?searchtype=author&query=Maroun%2C+R">Rami Maroun</a>, 
<a href="/search/cs?searchtype=author&query=Trullo%2C+R">Roger Trullo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Whole slide image (WSI) classification is a critical task in computational
pathology. However, the gigapixel-size of such images remains a major challenge
for the current state of deep-learning. Current methods rely on
multiple-instance learning (MIL) models with frozen feature extractors. Given
the the high number of instances in each image, MIL methods have long assumed
independence and permutation-invariance of patches, disregarding the tissue
structure and correlation between patches. Recent works started studying this
correlation between instances but the computational workload of such a high
number of tokens remained a limiting factor. In particular, relative position
of patches remains unaddressed. We propose to apply a straightforward encoding
module, namely a RoFormer layer , relying on memory-efficient exact
self-attention and relative positional encoding. This module can perform full
self-attention with relative position encoding on patches of large and
arbitrary shaped WSIs, solving the need for correlation between instances and
spatial modeling of tissues. We demonstrate that our method outperforms
state-of-the-art MIL models on three commonly used public datasets (TCGA-NSCLC,
BRACS and Camelyon16)) on weakly supervised classification tasks. Code is
available at https://github.com/Sanofi-Public/DDS-RoFormerMIL
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01926" title="Abstract">arXiv:2310.01926</a> [<a href="/pdf/2310.01926" title="Download PDF">pdf</a>, <a href="/format/2310.01926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DARTH: Holistic Test-time Adaptation for Multiple Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Segu%2C+M">Mattia Segu</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+B">Bernt Schiele</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fisher Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the IEEE/CVF International Conference on Computer Vision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multiple object tracking (MOT) is a fundamental component of perception
systems for autonomous driving, and its robustness to unseen conditions is a
requirement to avoid life-critical failures. Despite the urge of safety in
driving systems, no solution to the MOT adaptation problem to domain shift in
test-time conditions has ever been proposed. However, the nature of a MOT
system is manifold - requiring object detection and instance association - and
adapting all its components is non-trivial. In this paper, we analyze the
effect of domain shift on appearance-based trackers, and introduce DARTH, a
holistic test-time adaptation framework for MOT. We propose a detection
consistency formulation to adapt object detection in a self-supervised fashion,
while adapting the instance appearance representations via our novel patch
contrastive loss. We evaluate our method on a variety of domain shifts -
including sim-to-real, outdoor-to-indoor, indoor-to-outdoor - and substantially
improve the source model performance on all metrics. Code:
https://github.com/mattiasegu/darth.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01929" title="Abstract">arXiv:2310.01929</a> [<a href="/pdf/2310.01929" title="Download PDF">pdf</a>, <a href="/format/2310.01929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of  Text-To-Image Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ventura%2C+M">Mor Ventura</a>, 
<a href="/search/cs?searchtype=author&query=Ben-David%2C+E">Eyal Ben-David</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Reichart%2C+R">Roi Reichart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have
recently gained prominence for their remarkable zero-shot capabilities in
generating images guided by textual prompts. Language, as a conduit of culture,
plays a pivotal role in these models' multilingual capabilities, which in turn
shape their cultural agency. In this study, we explore the cultural perception
embedded in TTI models by characterizing culture across three hierarchical
tiers: cultural dimensions, cultural domains, and cultural concepts. We propose
a comprehensive suite of evaluation techniques, including intrinsic evaluations
using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA)
model, and human assessments, to discern TTI cultural perceptions. To
facilitate our research, we introduce the CulText2I dataset, derived from four
diverse TTI models and spanning ten languages. Our experiments reveal insights
into these models' cultural awareness, cultural distinctions, and the unlocking
of cultural features, releasing the potential for cross-cultural applications.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01930" title="Abstract">arXiv:2310.01930</a> [<a href="/pdf/2310.01930" title="Download PDF">pdf</a>, <a href="/format/2310.01930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distributed Multi-Robot Framework for Exploration, Information  Acquisition and Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patwardhan%2C+A">Aalok Patwardhan</a>, 
<a href="/search/cs?searchtype=author&query=Davison%2C+A+J">Andrew J. Davison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We encourage the reader to view our demos at <a href="https://aalpatya.github.io/gbpstack">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The distributed coordination of robot teams performing complex tasks is
challenging to formulate. The different aspects of a complete task such as
local planning for obstacle avoidance, global goal coordination and
collaborative mapping are often solved separately, when clearly each of these
should influence the others for the most efficient behaviour. In this paper we
use the example application of distributed information acquisition as a robot
team explores a large space to show that we can formulate the whole problem as
a single factor graph with multiple connected layers representing each aspect.
We use Gaussian Belief Propagation (GBP) as the inference mechanism, which
permits parallel, on-demand or asynchronous computation for efficiency when
different aspects are more or less important. This is the first time that a
distributed GBP multi-robot solver has been proven to enable intelligent
collaborative behaviour rather than just guiding robots to individual, selfish
goals. We encourage the reader to view our demos at
https://aalpatya.github.io/gbpstack
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01931" title="Abstract">arXiv:2310.01931</a> [<a href="/pdf/2310.01931" title="Download PDF">pdf</a>, <a href="/format/2310.01931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MarineDet: Towards Open-Marine Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haixin%2C+L">Liang Haixin</a>, 
<a href="/search/cs?searchtype=author&query=Ziqiang%2C+Z">Zheng Ziqiang</a>, 
<a href="/search/cs?searchtype=author&query=Zeyu%2C+M">Ma Zeyu</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+S">Sai-Kit Yeung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Marine object detection has gained prominence in marine research, driven by
the pressing need to unravel oceanic mysteries and enhance our understanding of
invaluable marine ecosystems. There is a profound requirement to efficiently
and accurately identify and localize diverse and unseen marine entities within
underwater imagery. The open-marine object detection (OMOD for short) is
required to detect diverse and unseen marine objects, performing categorization
and localization simultaneously. To achieve OMOD, we present
\textbf{MarineDet}. We formulate a joint visual-text semantic space through
pre-training and then perform marine-specific training to achieve
in-air-to-marine knowledge transfer. Considering there is no specific dataset
designed for OMOD, we construct a \textbf{MarineDet dataset} consisting of 821
marine-relative object categories to promote and measure OMOD performance. The
experimental results demonstrate the superior performance of MarineDet over
existing generalist and specialist object detection algorithms. To the best of
our knowledge, we are the first to present OMOD, which holds a more valuable
and practical setting for marine ecosystem monitoring and management. Our
research not only pushes the boundaries of marine understanding but also offers
a standard pipeline for OMOD.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01932" title="Abstract">arXiv:2310.01932</a> [<a href="/pdf/2310.01932" title="Download PDF">pdf</a>, <a href="/format/2310.01932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Data Processing for Space Robotics Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheppard%2C+A">Anja Sheppard</a>, 
<a href="/search/cs?searchtype=author&query=Skinner%2C+K+A">Katherine A. Skinner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented as a poster at IAC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous terrain classification is an important problem in planetary
navigation, whether the goal is to identify scientific sites of interest or to
traverse treacherous areas safely. Past Martian rovers have relied on human
operators to manually identify a navigable path from transmitted imagery. Our
goals on Mars in the next few decades will eventually require rovers that can
autonomously move farther, faster, and through more dangerous
landscapes--demonstrating a need for improved terrain classification for
traversability. Autonomous navigation through extreme environments will enable
the search for water on the Moon and Mars as well as preparations for human
habitats. Advancements in machine learning techniques have demonstrated
potential to improve terrain classification capabilities for ground vehicles on
Earth. However, classification results for space applications are limited by
the availability of training data suitable for supervised learning methods.
This paper contributes an open source automatic data processing pipeline that
uses camera geometry to co-locate Curiosity and Perseverance Mastcam image
products with Mars overhead maps via ray projection over a terrain model. In
future work, this automated data processing pipeline will be leveraged for
development of machine learning methods for terrain classification.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01935" title="Abstract">arXiv:2310.01935</a> [<a href="/pdf/2310.01935" title="Download PDF">pdf</a>, <a href="/ps/2310.01935" title="Download PostScript">ps</a>, <a href="/format/2310.01935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who is the Audience? Designing Casual Data Visualizations for the  &#x27;General Public&#x27;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schuster%2C+R">Regina Schuster</a>, 
<a href="/search/cs?searchtype=author&query=Koesten%2C+L">Laura Koesten</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+T">Torsten M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Gregory%2C+K">Kathleen Gregory</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Casual data visualizations play a vital role in communicating data to lay
audiences. Despite this, little is known about how data visualization
practitioners make design decisions based on their envisioned target audiences
using different media channels. We draw on the findings of a semi-structured
interview study to explore how data visualization practitioners working in
various settings conceptualize and design for lay audiences and how they
evaluate their visualization designs. Our findings suggest that practitioners
often use broad definitions of their target audience, yet they stress the
importance of 'knowing the readers' for their design decisions. At the same
time, commonly used evaluation and feedback mechanisms do not allow a deep
knowledge of their readers but rely instead on tacit knowledge, simple usage
metrics, or testing with colleagues. We conclude by calling for different forms
of visualization evaluation that are feasible for practitioners to implement in
their daily workflows.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01936" title="Abstract">arXiv:2310.01936</a> [<a href="/pdf/2310.01936" title="Download PDF">pdf</a>, <a href="/ps/2310.01936" title="Download PostScript">ps</a>, <a href="/format/2310.01936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing Image-Text Pair Dataset from Books
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okamoto%2C+Y">Yamato Okamoto</a>, 
<a href="/search/cs?searchtype=author&query=Toyonaga%2C+H">Haruto Toyonaga</a>, 
<a href="/search/cs?searchtype=author&query=Ijiri%2C+Y">Yoshihisa Ijiri</a>, 
<a href="/search/cs?searchtype=author&query=Kataoka%2C+H">Hirokatsu Kataoka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023 workshop, Towards the Next Generation of Computer Vision Datasets: General DataCentric Submission Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Digital archiving is becoming widespread owing to its effectiveness in
protecting valuable books and providing knowledge to many people
electronically. In this paper, we propose a novel approach to leverage digital
archives for machine learning. If we can fully utilize such digitized data,
machine learning has the potential to uncover unknown insights and ultimately
acquire knowledge autonomously, just like humans read books. As a first step,
we design a dataset construction pipeline comprising an optical character
reader (OCR), an object detector, and a layout analyzer for the autonomous
extraction of image-text pairs. In our experiments, we apply our pipeline on
old photo books to construct an image-text pair dataset, showing its
effectiveness in image-text retrieval and insight extraction.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01937" title="Abstract">arXiv:2310.01937</a> [<a href="/pdf/2310.01937" title="Download PDF">pdf</a>, <a href="/format/2310.01937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Inference with Conditional Front-Door Adjustment and Identifiable  Variational Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Debo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiuyong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jixue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kui Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">An essential and challenging problem in causal inference is causal effect
estimation from observational data. The problem becomes more difficult with the
presence of unobserved confounding variables. The front-door adjustment is a
practical approach for dealing with unobserved confounding variables. However,
the restriction for the standard front-door adjustment is difficult to satisfy
in practice. In this paper, we relax some of the restrictions by proposing the
concept of conditional front-door (CFD) adjustment and develop the theorem that
guarantees the causal effect identifiability of CFD adjustment. Furthermore, as
it is often impossible for a CFD variable to be given in practice, it is
desirable to learn it from data. By leveraging the ability of deep generative
models, we propose CFDiVAE to learn the representation of the CFD adjustment
variable directly from data with the identifiable Variational AutoEncoder and
formally prove the model identifiability. Extensive experiments on synthetic
datasets validate the effectiveness of CFDiVAE and its superiority over
existing methods. The experiments also show that the performance of CFDiVAE is
less sensitive to the causal strength of unobserved confounding variables. We
further apply CFDiVAE to a real-world dataset to demonstrate its potential
application.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01941" title="Abstract">arXiv:2310.01941</a> [<a href="/pdf/2310.01941" title="Download PDF">pdf</a>, <a href="/format/2310.01941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandwidth of Timed Automata: 3 Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asarin%2C+E">Eugene Asarin</a>, 
<a href="/search/cs?searchtype=author&query=Degorre%2C+A">Aldric Degorre</a>, 
<a href="/search/cs?searchtype=author&query=Dima%2C+C">Catalin Dima</a>, 
<a href="/search/cs?searchtype=author&query=Inclan%2C+B+J">Bernardo Jacobo Inclan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Timed languages contain sequences of discrete events ("letters'') separated
by real-valued delays, they can be recognized by timed automata, and represent
behaviors of various real-time systems. The notion of bandwidth of a timed
language defined in a previous paper characterizes the amount of information
per time unit, encoded in words of the language observed with some precision
{\epsilon}.
<br />In this paper, we identify three classes of timed automata according to the
asymptotics of the bandwidth of their languages with respect to this precision
{\epsilon}: automata are either meager, with an O(1) bandwidth, normal, with a
{\Theta}(log (1/{\epsilon})) bandwidth, or obese, with {\Theta}(1/{\epsilon})
bandwidth. We define two structural criteria and prove that they partition
timed automata into these three classes of bandwidth, implying that there are
no intermediate asymptotic classes. The classification problem of a timed
automaton is PSPACE-complete.
<br />Both criteria are formulated using morphisms from paths of the timed
automaton to some finite monoids extending Puri's orbit graphs; the proofs are
based on Simon's factorization forest theorem.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01942" title="Abstract">arXiv:2310.01942</a> [<a href="/pdf/2310.01942" title="Download PDF">pdf</a>, <a href="/format/2310.01942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OOD Aware Supervised Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seifi%2C+S">Soroush Seifi</a>, 
<a href="/search/cs?searchtype=author&query=Reino%2C+D+O">Daniel Olmeda Reino</a>, 
<a href="/search/cs?searchtype=author&query=Chumerin%2C+N">Nikolay Chumerin</a>, 
<a href="/search/cs?searchtype=author&query=Aljundi%2C+R">Rahaf Aljundi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Out-of-Distribution (OOD) detection is a crucial problem for the safe
deployment of machine learning models identifying samples that fall outside of
the training distribution, i.e. in-distribution data (ID). Most OOD works focus
on the classification models trained with Cross Entropy (CE) and attempt to fix
its inherent issues. In this work we leverage powerful representation learned
with Supervised Contrastive (SupCon) training and propose a holistic approach
to learn a classifier robust to OOD data. We extend SupCon loss with two
additional contrast terms. The first term pushes auxiliary OOD representations
away from ID representations without imposing any constraints on similarities
among auxiliary data. The second term pushes OOD features far from the existing
class prototypes, while pushing ID representations closer to their
corresponding class prototype. When auxiliary OOD data is not available, we
propose feature mixing techniques to efficiently generate pseudo-OOD features.
Our solution is simple and efficient and acts as a natural extension of the
closed-set supervised contrastive representation learning. We compare against
different OOD detection methods on the common benchmarks and show
state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01943" title="Abstract">arXiv:2310.01943</a> [<a href="/pdf/2310.01943" title="Download PDF">pdf</a>, <a href="/format/2310.01943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ravestate: Distributed Composition of a Causal-Specificity-Guided  Interaction Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Birkner%2C+J">Joseph Birkner</a>, 
<a href="/search/cs?searchtype=author&query=Dolp%2C+A">Andreas Dolp</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+N">Negin Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Basargin%2C+N">Nikita Basargin</a>, 
<a href="/search/cs?searchtype=author&query=Kharchenko%2C+A">Alona Kharchenko</a>, 
<a href="/search/cs?searchtype=author&query=Hostettler%2C+R">Rafael Hostettler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In human-robot interaction policy design, a rule-based method is efficient,
explainable, expressive and intuitive. In this paper, we present the
Signal-Rule-Slot framework, which refines prior work on rule-based symbol
system design and introduces a new, Bayesian notion of interaction rule utility
called Causal Pathway Self-information. We offer a rigorous theoretical
foundation as well as a rich open-source reference implementation Ravestate,
with which we conduct user studies in text-, speech-, and vision-based
scenarios. The experiments show robust contextual behaviour of our
probabilistically informed rule-based system, paving the way for more effective
human-machine interaction.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01945" title="Abstract">arXiv:2310.01945</a> [<a href="/pdf/2310.01945" title="Download PDF">pdf</a>, <a href="/format/2310.01945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homotopy-Aware Multi-Agent Path Planning in Plane
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasaura%2C+K">Kazumi Kasaura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages with 6 pages of references and appendices, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We propose an efficient framework using the Dehornoy order for homotopy-aware
multi-agent path planning in the plane. We developed a method to generate
homotopically distinct solutions of multi-agent path planning problem in the
plane by combining our framework with revised prioritized planning and proved
its completeness under specific assumptions. Experimentally, we demonstrated
that the runtime of our method grows approximately quintically with the number
of agents. We also confirmed the usefulness of homotopy-awareness by showing
experimentally that generation of homotopically distinct solutions by our
method contributes to planning low-cost trajectories for a swarm of agents.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01946" title="Abstract">arXiv:2310.01946</a> [<a href="/pdf/2310.01946" title="Download PDF">pdf</a>, <a href="/format/2310.01946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoralVOS: Dataset and Benchmark for Coral Video Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziqiang%2C+Z">Zheng Ziqiang</a>, 
<a href="/search/cs?searchtype=author&query=Yaofeng%2C+X">Xie Yaofeng</a>, 
<a href="/search/cs?searchtype=author&query=Haixin%2C+L">Liang Haixin</a>, 
<a href="/search/cs?searchtype=author&query=Zhibin%2C+Y">Yu Zhibin</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+S">Sai-Kit Yeung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, dense coral video segmentation dataset and benchmark
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Coral reefs formulate the most valuable and productive marine ecosystems,
providing habitat for many marine species. Coral reef surveying and analysis
are currently confined to coral experts who invest substantial effort in
generating comprehensive and dependable reports (\emph{e.g.}, coral coverage,
population, spatial distribution, \textit{etc}), from the collected survey
data. However, performing dense coral analysis based on manual efforts is
significantly time-consuming, the existing coral analysis algorithms compromise
and opt for performing down-sampling and only conducting sparse point-based
coral analysis within selected frames. However, such down-sampling will
\textbf{inevitable} introduce the estimation bias or even lead to wrong
results. To address this issue, we propose to perform \textbf{dense coral video
segmentation}, with no down-sampling involved. Through video object
segmentation, we could generate more \textit{reliable} and \textit{in-depth}
coral analysis than the existing coral reef analysis algorithms. To boost such
dense coral analysis, we propose a large-scale coral video segmentation
dataset: \textbf{CoralVOS} as demonstrated in Fig. 1. To the best of our
knowledge, our CoralVOS is the first dataset and benchmark supporting dense
coral video segmentation. We perform experiments on our CoralVOS dataset,
including 6 recent state-of-the-art video object segmentation (VOS) algorithms.
We fine-tuned these VOS algorithms on our CoralVOS dataset and achieved
observable performance improvement. The results show that there is still great
potential for further promoting the segmentation accuracy. The dataset and
trained models will be released with the acceptance of this work to foster the
coral reef research community.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01951" title="Abstract">arXiv:2310.01951</a> [<a href="/pdf/2310.01951" title="Download PDF">pdf</a>, <a href="/format/2310.01951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Reach-Avoid for Bayesian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wicker%2C+M">Matthew Wicker</a>, 
<a href="/search/cs?searchtype=author&query=Laurenti%2C+L">Luca Laurenti</a>, 
<a href="/search/cs?searchtype=author&query=Patane%2C+A">Andrea Patane</a>, 
<a href="/search/cs?searchtype=author&query=Paoletti%2C+N">Nicola Paoletti</a>, 
<a href="/search/cs?searchtype=author&query=Abate%2C+A">Alessandro Abate</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowska%2C+M">Marta Kwiatkowska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 10 figures. arXiv admin note: text overlap with <a href="/abs/2105.10134">arXiv:2105.10134</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Model-based reinforcement learning seeks to simultaneously learn the dynamics
of an unknown stochastic environment and synthesise an optimal policy for
acting in it. Ensuring the safety and robustness of sequential decisions made
through a policy in such an environment is a key challenge for policies
intended for safety-critical scenarios. In this work, we investigate two
complementary problems: first, computing reach-avoid probabilities for
iterative predictions made with dynamical models, with dynamics described by
Bayesian neural network (BNN); second, synthesising control policies that are
optimal with respect to a given reach-avoid specification (reaching a "target"
state, while avoiding a set of "unsafe" states) and a learned BNN model. Our
solution leverages interval propagation and backward recursion techniques to
compute lower bounds for the probability that a policy's sequence of actions
leads to satisfying the reach-avoid specification. Such computed lower bounds
provide safety certification for the given policy and BNN model. We then
introduce control synthesis algorithms to derive policies maximizing said lower
bounds on the safety probability. We demonstrate the effectiveness of our
method on a series of control benchmarks characterized by learned BNN dynamics
models. On our most challenging benchmark, compared to purely data-driven
policies the optimal synthesis algorithm is able to provide more than a
four-fold increase in the number of certifiable states and more than a
three-fold increase in the average guaranteed reach-avoid probability.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01957" title="Abstract">arXiv:2310.01957</a> [<a href="/pdf/2310.01957" title="Download PDF">pdf</a>, <a href="/format/2310.01957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Driving with LLMs: Fusing Object-Level Vector Modality for Explainable  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sinavski%2C+O">Oleg Sinavski</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCnermann%2C+J">Jan H&#xfc;nermann</a>, 
<a href="/search/cs?searchtype=author&query=Karnsund%2C+A">Alice Karnsund</a>, 
<a href="/search/cs?searchtype=author&query=Willmott%2C+A+J">Andrew James Willmott</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+D">Danny Birch</a>, 
<a href="/search/cs?searchtype=author&query=Maund%2C+D">Daniel Maund</a>, 
<a href="/search/cs?searchtype=author&query=Shotton%2C+J">Jamie Shotton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Large Language Models (LLMs) have shown promise in the autonomous driving
sector, particularly in generalization and interpretability. We introduce a
unique object-level multimodal LLM architecture that merges vectorized numeric
modalities with a pre-trained LLM to improve context understanding in driving
situations. We also present a new dataset of 160k QA pairs derived from 10k
driving scenarios, paired with high quality control commands collected with RL
agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct
pretraining strategy is devised to align numeric vector modalities with static
LLM representations using vector captioning language data. We also introduce an
evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency
in interpreting driving scenarios, answering questions, and decision-making.
Our findings highlight the potential of LLM-based driving action generation in
comparison to traditional behavioral cloning. We make our benchmark, datasets,
and model available for further exploration.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01959" title="Abstract">arXiv:2310.01959</a> [<a href="/pdf/2310.01959" title="Download PDF">pdf</a>, <a href="/format/2310.01959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Labeling Oracles: What does it mean to steal ML models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shafran%2C+A">Avital Shafran</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Erdogdu%2C+M+A">Murat A. Erdogdu</a>, 
<a href="/search/cs?searchtype=author&query=Papernot%2C+N">Nicolas Papernot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Model extraction attacks are designed to steal trained models with only query
access, as is often provided through APIs that ML-as-a-Service providers offer.
ML models are expensive to train, in part because data is hard to obtain, and a
primary incentive for model extraction is to acquire a model while incurring
less cost than training from scratch. Literature on model extraction commonly
claims or presumes that the attacker is able to save on both data acquisition
and labeling costs. We show that the attacker often does not. This is because
current attacks implicitly rely on the adversary being able to sample from the
victim model's data distribution. We thoroughly evaluate factors influencing
the success of model extraction. We discover that prior knowledge of the
attacker, i.e. access to in-distribution data, dominates other factors like the
attack policy the adversary follows to choose which queries to make to the
victim model API. Thus, an adversary looking to develop an equally capable
model with a fixed budget has little practical incentive to perform model
extraction, since for the attack to work they need to collect in-distribution
data, saving only on the cost of labeling. With low labeling costs in the
current market, the usefulness of such attacks is questionable. Ultimately, we
demonstrate that the effect of prior knowledge needs to be explicitly decoupled
from the attack policy. To this end, we propose a benchmark to evaluate attack
policy directly.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01960" title="Abstract">arXiv:2310.01960</a> [<a href="/pdf/2310.01960" title="Download PDF">pdf</a>, <a href="/format/2310.01960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models as Knowledge Bases for Visual Word Sense Disambiguation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kritharoula%2C+A">Anastasia Kritharoula</a>, 
<a href="/search/cs?searchtype=author&query=Lymperaiou%2C+M">Maria Lymperaiou</a>, 
<a href="/search/cs?searchtype=author&query=Stamou%2C+G">Giorgos Stamou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies
between linguistic sense disambiguation and fine-grained multimodal retrieval.
The recent advancements in the development of visiolinguistic (VL) transformers
suggest some off-the-self implementations with encouraging results, which
however we argue that can be further improved. To this end, we propose some
knowledge-enhancement techniques towards improving the retrieval performance of
VL transformers via the usage of Large Language Models (LLMs) as Knowledge
Bases. More specifically, knowledge stored in LLMs is retrieved with the help
of appropriate prompts in a zero-shot manner, achieving performance
advancements. Moreover, we convert VWSD to a purely textual question-answering
(QA) problem by considering generated image captions as multiple-choice
candidate answers. Zero-shot and few-shot prompting strategies are leveraged to
explore the potential of such a transformation, while Chain-of-Thought (CoT)
prompting in the zero-shot setting is able to reveal the internal reasoning
steps an LLM follows to select the appropriate candidate. In total, our
presented approach is the first one to analyze the merits of exploiting
knowledge stored in LLMs in different ways to solve WVSD.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01961" title="Abstract">arXiv:2310.01961</a> [<a href="/pdf/2310.01961" title="Download PDF">pdf</a>, <a href="/ps/2310.01961" title="Download PostScript">ps</a>, <a href="/format/2310.01961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soda: An Object-Oriented Functional Language for Specifying  Human-Centered Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendez%2C+J+A">Julian Alfredo Mendez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://julianmendez.github.io/soda">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We present Soda (Symbolic Objective Descriptive Analysis), a language that
helps to treat qualities and quantities in a natural way and greatly simplifies
the task of checking their correctness. We present key properties for the
language motivated by the design of a descriptive language to encode complex
requirements on computer systems, and we explain how these key properties must
be addressed to model these requirements with simple definitions. We give an
overview of a tool that helps to describe problems in an easy way that we
consider more transparent and less error-prone.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01966" title="Abstract">arXiv:2310.01966</a> [<a href="/pdf/2310.01966" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Throughput Maximization for Instantly Decodable Network Coded NOMA in  Broadcast Communication Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+Z">Zhonghui Mei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Non-orthogonal multiple access (NOMA) is a promising transmission scheme
employed at the physical layer to improve the spectral efficiency. In this
paper, we develop a novel cross-layer approach by employing NOMA at the
physical layer and instantly decodable network coding (IDNC) at the network
layer in downlink cellular networks. Following this approach, two IDNC packets
are selected for each transmission, with one designed for all receivers and the
other designed only for the strong receivers which can employ successive
interference cancellation (SIC). The IDNC packets selection, transmission rates
adaption for the two IDNC packets, and NOMA power allocation are jointly
considered to improve the throughput of the network. Given the intractability
of the problem, we decouple it into two separate subproblems, the IDNC
scheduling which jointly selects the IDNC packets and the transmission rates
with the given NOMA power allocation, and the NOMA power allocation with the
given IDNC scheduling. The IDNC scheduling can be reduced to a maximum weight
clique problem, and two heuristic algorithms named as maximum weight vertex
(MWV) search and maximum weight path based maximum weight vertex (MWP-MWV)
search are developed to solve the first subproblem. An iterative function
evaluation (IFE) approach is proposed to solve the second subproblem.
Simulation results are presented to demonstrates the throughput gain of the
proposed approach over the existing solutions.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01967" title="Abstract">arXiv:2310.01967</a> [<a href="/pdf/2310.01967" title="Download PDF">pdf</a>, <a href="/format/2310.01967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Active SLAM: Synchronous and Asynchronous Coordination  Among Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maragliano%2C+M">Matteo Maragliano</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M+F">Muhammad Farhan Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Recchiuto%2C+C+T">Carmine Tommaso Recchiuto</a>, 
<a href="/search/cs?searchtype=author&query=Sgorbissa%2C+A">Antonio Sgorbissa</a>, 
<a href="/search/cs?searchtype=author&query=Fremont%2C+V">Vincent Fremont</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In the realm of autonomous robotics, a critical challenge lies in developing
robust solutions for Active Collaborative SLAM, wherein multiple robots must
collaboratively explore and map an unknown environment while intelligently
coordinating their movements and sensor data acquisitions. To this aim, we
present two approaches for coordinating a system consisting of multiple robots
to perform Active Collaborative SLAM (AC-SLAM) for environmental exploration.
Our two coordination approaches, synchronous and asynchronous implement a
methodology to prioritize robot goal assignments by the central server. We also
present a method to efficiently spread the robots for maximum exploration while
keeping SLAM uncertainty low. Both coordination approaches were evaluated
through simulation on publicly available datasets, obtaining promising results.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01968" title="Abstract">arXiv:2310.01968</a> [<a href="/pdf/2310.01968" title="Download PDF">pdf</a>, <a href="/format/2310.01968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PyHexTop: a compact Python code for topology optimization using  hexagonal elements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Aditi Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+A">Anupam Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Prabhat Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NCMDAO 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Python serves as an open-source and cost-effective alternative to the MATLAB
programming language. This paper introduces a concise topology optimization
Python code, named ``PyHexTop," primarily intended for educational purposes.
Code employs hexagonal elements to parameterize design domains as such elements
provide checkerboard-free optimized design naturally. PyHexTop is developed
based on the ``HoneyTop90" MATLAB code~\cite{kumar2023honeytop90} and uses the
NumPy and SciPy libraries. Code is straightforward and easily comprehensible,
proving a helpful tool that can help people new in the topology optimization
field to learn and explore. PyHexTop is specifically tailored to address
compliance minimization with specified volume constraints. The paper provides a
detailed explanation of the code for solving the MBB design and extensions to
solve problems with varying boundary and force conditions. The code is publicly
shared at: \url{https://github.com/PrabhatIn/PyHexTop.}
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01969" title="Abstract">arXiv:2310.01969</a> [<a href="/pdf/2310.01969" title="Download PDF">pdf</a>, <a href="/format/2310.01969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steganalysis of AI Models LSB Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gilkarov%2C+D">Daniel Gilkarov</a>, 
<a href="/search/cs?searchtype=author&query=Dubin%2C+R">Ran Dubin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Artificial intelligence has made significant progress in the last decade,
leading to a rise in the popularity of model sharing. The model zoo ecosystem,
a repository of pre-trained AI models, has advanced the AI open-source
community and opened new avenues for cyber risks. Malicious attackers can
exploit shared models to launch cyber-attacks. This work focuses on the
steganalysis of injected malicious Least Significant Bit (LSB) steganography
into AI models, and it is the first work focusing on AI model attacks. In
response to this threat, this paper presents a steganalysis method specifically
tailored to detect and mitigate malicious LSB steganography attacks based on
supervised and unsupervised AI detection steganalysis methods. Our proposed
technique aims to preserve the integrity of shared models, protect user trust,
and maintain the momentum of open collaboration within the AI community. In
this work, we propose 3 steganalysis methods and open source our code. We found
that the success of the steganalysis depends on the LSB attack location. If the
attacker decides to exploit the least significant bits in the LSB, the ability
to detect the attacks is low. However, if the attack is in the most significant
LSB bits, the attack can be detected with almost perfect accuracy.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01972" title="Abstract">arXiv:2310.01972</a> [<a href="/pdf/2310.01972" title="Download PDF">pdf</a>, <a href="/format/2310.01972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epidemic Learning: Boosting Decentralized Learning with Randomized  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Vos%2C+M">Martijn de Vos</a>, 
<a href="/search/cs?searchtype=author&query=Farhadkhani%2C+S">Sadegh Farhadkhani</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Kermarrec%2C+A">Anne-Marie Kermarrec</a>, 
<a href="/search/cs?searchtype=author&query=Pires%2C+R">Rafael Pires</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rishi Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We present Epidemic Learning (EL), a simple yet powerful decentralized
learning (DL) algorithm that leverages changing communication topologies to
achieve faster model convergence compared to conventional DL approaches. At
each round of EL, each node sends its model updates to a random sample of $s$
other nodes (in a system of $n$ nodes). We provide an extensive theoretical
analysis of EL, demonstrating that its changing topology culminates in superior
convergence properties compared to the state-of-the-art (static and dynamic)
topologies. Considering smooth non-convex loss functions, the number of
transient iterations for EL, i.e., the rounds required to achieve asymptotic
linear speedup, is in $\mathcal{O}(\frac{n^3}{s^2})$ which outperforms the
best-known bound $\mathcal{O}({n^3})$ by a factor of $ s^2 $, indicating the
benefit of randomized communication for DL. We empirically evaluate EL in a
96-node network and compare its performance with state-of-the-art DL
approaches. Our results illustrate that EL converges up to $ 1.6\times $
quicker than baseline DL algorithms and attains 1.8% higher accuracy for the
same communication volume.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01973" title="Abstract">arXiv:2310.01973</a> [<a href="/pdf/2310.01973" title="Download PDF">pdf</a>, <a href="/format/2310.01973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Wasserstein Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakotomamonjy%2C+A">Alain Rakotomamonjy</a>, 
<a href="/search/cs?searchtype=author&query=Nadjahi%2C+K">Kimia Nadjahi</a>, 
<a href="/search/cs?searchtype=author&query=Ralaivola%2C+L">Liva Ralaivola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We introduce a principled way of computing the Wasserstein distance between
two distributions in a federated manner. Namely, we show how to estimate the
Wasserstein distance between two samples stored and kept on different
devices/clients whilst a central entity/server orchestrates the computations
(again, without having access to the samples). To achieve this feat, we take
advantage of the geometric properties of the Wasserstein distance -- in
particular, the triangle inequality -- and that of the associated {\em
geodesics}: our algorithm, FedWad (for Federated Wasserstein Distance),
iteratively approximates the Wasserstein distance by manipulating and
exchanging distributions from the space of geodesics in lieu of the input
samples. In addition to establishing the convergence properties of FedWad, we
provide empirical results on federated coresets and federate optimal transport
dataset distance, that we respectively exploit for building a novel federated
model and for boosting performance of popular federated learning algorithms.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01975" title="Abstract">arXiv:2310.01975</a> [<a href="/pdf/2310.01975" title="Download PDF">pdf</a>, <a href="/format/2310.01975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for  XOR Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xuran Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+D">Difan Zou</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuan Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 74 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modern deep learning models are usually highly over-parameterized so that
they can overfit the training data. Surprisingly, such overfitting neural
networks can usually still achieve high prediction accuracy. To study this
"benign overfitting" phenomenon, a line of recent works has theoretically
studied the learning of linear models and two-layer neural networks. However,
most of these analyses are still limited to the very simple learning problems
where the Bayes-optimal classifier is linear. In this work, we investigate a
class of XOR-type classification tasks with label-flipping noises. We show
that, under a certain condition on the sample complexity and signal-to-noise
ratio, an over-parameterized ReLU CNN trained by gradient descent can achieve
near Bayes-optimal accuracy. Moreover, we also establish a matching lower bound
result showing that when the previous condition is not satisfied, the
prediction accuracy of the obtained CNN is an absolute constant away from the
Bayes-optimal rate. Our result demonstrates that CNNs have a remarkable
capacity to efficiently learn XOR problems, even in the presence of highly
correlated features.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01976" title="Abstract">arXiv:2310.01976</a> [<a href="/pdf/2310.01976" title="Download PDF">pdf</a>, <a href="/ps/2310.01976" title="Download PostScript">ps</a>, <a href="/format/2310.01976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some New Results With k-set agreement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carole%2C+D">Delporte-Gallet Carole</a>, 
<a href="/search/cs?searchtype=author&query=Hugues%2C+F">Fauconnier Hugues</a>, 
<a href="/search/cs?searchtype=author&query=Mouna%2C+S">Safir Mouna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this article, we investigate the solvability of $k$-set agreement among
$n$ processes in distributed systems prone to different types of process
failures. Specifically, we explore two scenarios: synchronous message-passing
systems prone to up to $t$ Byzantine failures of processes. And asynchronous
shared memory systems prone to up to $t$ crash failures of processes. Our goal
is to address the gaps left by previous works\cite{SSS,AsynchKset} in these
areas. For Byzantine failures case we consider systems with authentication
where processes have unforgeable signatures.
<br />For synchronous message-passing systems, we present an authenticated
algorithm that achieves $k$-set agreement in only two rounds, with no
constraints on the number of faults $t$, with $k$ determined as $k \geq \lfloor
\frac{n}{n-t} \rfloor + 1$. In fact the lower bound for $k$ is $k \geq \lfloor
\frac{n}{n-t} \rfloor $ that is obtained by an algorithm based on traditional
consensus with $t+1$ rounds.
<br />In asynchronous shared memory systems, we introduce an algorithm that
accomplishes $k$-set agreement for values of $k$ greater than $ \lfloor
\frac{n-t}{n-2t} \rfloor +1$. This algorithm uses a snapshot primitive to
handle crash failures and enable effective set agreement.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01977" title="Abstract">arXiv:2310.01977</a> [<a href="/pdf/2310.01977" title="Download PDF">pdf</a>, <a href="/format/2310.01977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experiences with Research Processes in an Undergraduate Theory of  Computing Course
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dougherty%2C+R+E">Ryan E. Dougherty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to SIGCSE TS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Theory of computing (ToC) courses are a staple in many undergraduate CS
curricula as they lay the foundation of why CS is important to students.
Although not a stated goal, an inevitable outcome of the course is enhancing
the students' technical reading and writing abilities as it often contains
formal reasoning and proof writing. Separately, many undergraduate students are
interested in performing research, but often lack these abilities. Based on
this observation, we emulated a common research environment within our ToC
course by creating a mock conference assignment, where students (in groups)
both wrote a technical paper solving an assigned problem and (individually)
anonymously refereed other groups' papers. In this paper we discuss the details
of this assignment and our experiences, and conclude with reflections and
future work about similar courses.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01978" title="Abstract">arXiv:2310.01978</a> [<a href="/pdf/2310.01978" title="Download PDF">pdf</a>, <a href="/format/2310.01978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Multimedia Verification with Computational Tools and OSINT:  Russia-Ukraine Conflict Case Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+A">Sohail Ahmed Khan</a>, 
<a href="/search/cs?searchtype=author&query=Furuly%2C+J+G">Jan Gunnar Furuly</a>, 
<a href="/search/cs?searchtype=author&query=Vold%2C+H+B">Henrik Brattli Vold</a>, 
<a href="/search/cs?searchtype=author&query=Tahseen%2C+R">Rano Tahseen</a>, 
<a href="/search/cs?searchtype=author&query=Dang-Nguyen%2C+D">Duc-Tien Dang-Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR)

</div>
<p class="mathjax">This paper investigates the use of computational tools and Open-Source
Intelligence (OSINT) techniques for verifying online multimedia content, with a
specific focus on real-world cases from the Russia-Ukraine conflict. Over a
nine-month period from April to December 2022, we examine verification
workflows, tools, and case studies published by \faktiskbar. Our study
showcases the effectiveness of diverse resources, including AI tools,
geolocation tools, internet archives, and social media monitoring platforms, in
enabling journalists and fact-checkers to efficiently process and corroborate
evidence, ensuring the dissemination of accurate information. This research
underscores the vital role of computational tools and OSINT techniques in
promoting evidence-based reporting and combatting misinformation. We also touch
on the current limitations of available tools and prospects for future
developments in multimedia verification.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01980" title="Abstract">arXiv:2310.01980</a> [<a href="/pdf/2310.01980" title="Download PDF">pdf</a>, <a href="/format/2310.01980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UAV Swarm-enabled Collaborative Secure Relay Communications with  Time-domain Colluding Eavesdropper
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Geng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiahui Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shuang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+V+C+M">Victor C.M. Leung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Mobile Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Unmanned aerial vehicles (UAVs) as aerial relays are practically appealing
for assisting Internet of Things (IoT) network. In this work, we aim to utilize
the UAV swarm to assist the secure communication between the micro base station
(MBS) equipped with the planar array antenna (PAA) and the IoT terminal devices
by collaborative beamforming (CB), so as to counteract the effects of collusive
eavesdropping attacks in time-domain. Specifically, we formulate a UAV
swarm-enabled secure relay multi-objective optimization problem (US2RMOP) for
simultaneously maximizing the achievable sum rate of associated IoT terminal
devices, minimizing the achievable sum rate of the eavesdropper and minimizing
the energy consumption of UAV swarm, by jointly optimizing the excitation
current weights of both MBS and UAV swarm, the selection of the UAV receiver,
the position of UAVs and user association order of IoT terminal devices.
Furthermore, the formulated US2RMOP is proved to be a non-convex, NP-hard and
large-scale optimization problem. Therefore, we propose an improved
multi-objective grasshopper algorithm (IMOGOA) with some specific designs to
address the problem. Simulation results exhibit the effectiveness of the
proposed UAV swarm-enabled collaborative secure relay strategy and demonstrate
the superiority of IMOGOA.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01981" title="Abstract">arXiv:2310.01981</a> [<a href="/pdf/2310.01981" title="Download PDF">pdf</a>, <a href="/format/2310.01981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Digitalization Framework for Smart Maintenance of Historic Buildings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ni%2C+Z">Zhongjun Ni</a> (Department of Science and Technology, Link&#xf6;ping University, Campus Norrk&#xf6;ping, Norrk&#xf6;ping, Sweden)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Licentiate Thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Smart maintenance of historic buildings involves integration of digital
technologies and data analysis methods to help maintain functionalities of
these buildings and preserve their heritage values. However, the maintenance of
historic buildings is a long-term process. During the process, the digital
transformation requires overcoming various challenges, such as stable and
scalable storage and computing resources, a consistent format for organizing
and representing building data, and a flexible design to integrate data
analytics to deliver applications. This licentiate thesis aims to address these
challenges by proposing a digitalization framework that integrates Internet of
Things (IoT), cloud computing, ontology, and machine learning. IoT devices
enable data collection from historic buildings to reveal their latest status.
Using a public cloud platform brings stable and scalable resources for storing
data, performing analytics, and deploying applications. Ontologies provide a
clear and concise way to organize and represent building data, which makes it
easier to understand the relationships between different building components
and systems. Combined with IoT devices and ontologies, parametric digital twins
can be created to evolve with their physical counterparts. Furthermore, with
machine learning, digital twins can identify patterns from data and provide
decision-makers with insights to achieve smart maintenance. Overall, this
thesis contributes to the field of preservation of historic buildings by
proposing a comprehensive digitalization framework that integrates various
advanced digital technologies to provide a holistic approach to achieve smart
maintenance of historic buildings.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01983" title="Abstract">arXiv:2310.01983</a> [<a href="/pdf/2310.01983" title="Download PDF">pdf</a>, <a href="/format/2310.01983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turning Tiles is PSPACE-complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshiwatari%2C+K">Kanae Yoshiwatari</a>, 
<a href="/search/cs?searchtype=author&query=Kiya%2C+H">Hironori Kiya</a>, 
<a href="/search/cs?searchtype=author&query=Suetsugu%2C+K">Koki Suetsugu</a>, 
<a href="/search/cs?searchtype=author&query=Hanaka%2C+T">Tesshu Hanaka</a>, 
<a href="/search/cs?searchtype=author&query=Ono%2C+H">Hirotaka Ono</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">In combinatorial game theory, the winning player for a position in normal
play is analyzed and characterized via algebraic operations. Such analyses
define a value for each position, called a game value. A game (ruleset) is
called universal if any game value is achievable in some position in a play of
the game. Although the universality of a game implies that the ruleset is rich
enough (i.e., sufficiently complex), it does not immediately imply that the
game is intractable in the sense of computational complexity. This paper proves
that the universal game Turning Tiles is PSPACE-complete.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01986" title="Abstract">arXiv:2310.01986</a> [<a href="/pdf/2310.01986" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Vision-Based Tactile Sensing System for Multimodal Contact Information  Perception via Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guoyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuanzhi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhibin Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiali Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenfeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinming Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In general, robotic dexterous hands are equipped with various sensors for
acquiring multimodal contact information such as position, force, and pose of
the grasped object. This multi-sensor-based design adds complexity to the
robotic system. In contrast, vision-based tactile sensors employ specialized
optical designs to enable the extraction of tactile information across
different modalities within a single system. Nonetheless, the decoupling design
for different modalities in common systems is often independent. Therefore, as
the dimensionality of tactile modalities increases, it poses more complex
challenges in data processing and decoupling, thereby limiting its application
to some extent. Here, we developed a multimodal sensing system based on a
vision-based tactile sensor, which utilizes visual representations of tactile
information to perceive the multimodal contact information of the grasped
object. The visual representations contain extensive content that can be
decoupled by a deep neural network to obtain multimodal contact information
such as classification, position, posture, and force of the grasped object. The
results show that the tactile sensing system can perceive multimodal tactile
information using only one single sensor and without different data decoupling
designs for different modal tactile information, which reduces the complexity
of the tactile system and demonstrates the potential for multimodal tactile
integration in various fields such as biomedicine, biology, and robotics.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01991" title="Abstract">arXiv:2310.01991</a> [<a href="/pdf/2310.01991" title="Download PDF">pdf</a>, <a href="/format/2310.01991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward  Reasoning in Math Word Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deb%2C+A">Aniruddha Deb</a>, 
<a href="/search/cs?searchtype=author&query=Oza%2C+N">Neeva Oza</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+S">Sarthak Singla</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+D">Dinesh Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+D">Dinesh Garg</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+P">Parag Singla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">While forward reasoning (i.e. find the answer given the question) has been
explored extensively in the recent literature, backward reasoning is relatively
unexplored. We examine the backward reasoning capabilities of LLMs on Math Word
Problems (MWPs): given a mathematical question and its answer, with some
details omitted from the question, can LLMs effectively retrieve the missing
information?
<br />In this paper, we formally define the backward reasoning task on math word
problems and modify three datasets to evaluate this task: GSM8k, SVAMP and
MultiArith. Our findings show a significant drop in the accuracy of models on
backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4,
GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we
propose three novel techniques that improve performance: Rephrase reformulates
the given problem into a forward reasoning problem, PAL-Tools combines the idea
of Program-Aided LLMs to produce a set of equations that can be solved by an
external solver, and Check your Work exploits the availability of natural
verifier of high accuracy in the forward direction, interleaving solving and
verification steps. Finally, realizing that each of our base methods correctly
solves a different set of problems, we propose a novel Bayesian formulation for
creating an ensemble over these base methods aided by a verifier to further
boost the accuracy by a significant margin. Extensive experimentation
demonstrates that our techniques successively improve the performance of LLMs
on the backward reasoning task, with the final ensemble-based method resulting
in a substantial performance gain compared to the raw LLMs with standard
prompting techniques such as chain-of-thought.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01992" title="Abstract">arXiv:2310.01992</a> [<a href="/pdf/2310.01992" title="Download PDF">pdf</a>, <a href="/format/2310.01992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acyclic Petri and Workflow Nets with Resets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chistikov%2C+D">Dmitry Chistikov</a>, 
<a href="/search/cs?searchtype=author&query=Czerwi%C5%84ski%2C+W">Wojciech Czerwi&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Hofman%2C+P">Piotr Hofman</a>, 
<a href="/search/cs?searchtype=author&query=Mazowiecki%2C+F">Filip Mazowiecki</a>, 
<a href="/search/cs?searchtype=author&query=Sinclair-Banks%2C+H">Henry Sinclair-Banks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint for FSTTCS'23 containing 28 pages and 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">In this paper we propose two new subclasses of Petri nets with resets, for
which the reachability and coverability problems become tractable. We add an
acyclicity condition that only applies to the consumptions and productions, not
the resets. The first class is acyclic Petri nets with resets, and we show that
coverability is PSPACE-complete for them. This contrasts the known
Ackermann-hardness for coverability in (not necessarily acyclic) Petri nets
with resets. We prove that the reachability problem remains undecidable for
acyclic Petri nets with resets. The second class concerns workflow nets, a
practically motivated and natural subclass of Petri nets. Here, we show that
both coverability and reachability in acyclic workflow nets with resets are
PSPACE-complete. Without the acyclicity condition, reachability and
coverability in workflow nets with resets are known to be equally hard as for
Petri nets with resets, that being Ackermann-hard and undecidable,
respectively.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01994" title="Abstract">arXiv:2310.01994</a> [<a href="/pdf/2310.01994" title="Download PDF">pdf</a>, <a href="/format/2310.01994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Masked Autoencoders From a Local Contrastive Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiaoyu Yue</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lei Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Meng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jiangmiao Pang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xihui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Luping Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Masked AutoEncoder(MAE) has revolutionized the field of self-supervised
learning with its simple yet effective masking and reconstruction strategies.
However, despite achieving state-of-the-art performance across various
downstream vision tasks, the underlying mechanisms that drive MAE's efficacy
are less well-explored compared to the canonical contrastive learning paradigm.
In this paper, we explore a new perspective to explain what truly contributes
to the "rich hidden representations inside the MAE". Firstly, concerning MAE's
generative pretraining pathway, with a unique encoder-decoder architecture to
reconstruct images from aggressive masking, we conduct an in-depth analysis of
the decoder's behaviors. We empirically find that MAE's decoder mainly learns
local features with a limited receptive field, adhering to the well-known
Locality Principle. Building upon this locality assumption, we propose a
theoretical framework that reformulates the reconstruction-based MAE into a
local region-level contrastive learning form for improved understanding.
Furthermore, to substantiate the local contrastive nature of MAE, we introduce
a Siamese architecture that combines the essence of MAE and contrastive
learning without masking and explicit decoder, which sheds light on a unified
and more flexible self-supervised learning framework.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01995" title="Abstract">arXiv:2310.01995</a> [<a href="/pdf/2310.01995" title="Download PDF">pdf</a>, <a href="/format/2310.01995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Development of Machine Vision Approach for Mechanical Component  Identification based on its Dimension and Pitch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+T">Toshit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Mushtaq%2C+F">Faisel Mushtaq</a>, 
<a href="/search/cs?searchtype=author&query=Ramesh%2C+K">K Ramesh</a>, 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+S">Sandip Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+T">Tathagata Ray</a>, 
<a href="/search/cs?searchtype=author&query=Parimi%2C+C">Chandu Parimi</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+P">Praveen Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+P+K">Pramod Kumar Jha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, a highly customizable and scalable vision based system for
automation of mechanical assembly lines is described. The proposed system
calculates the features that are required to classify and identify the
different kinds of bolts that are used in the assembly line. The system
describes a novel method of calculating the pitch of the bolt in addition to
bolt identification and calculating the dimensions of the bolts. This
identification and classification system is extremely lightweight and can be
run on bare minimum hardware. The system is very fast in the order of
milliseconds, hence the system can be used successfully even if the components
are steadily moving on a conveyor. The results show that our system can
correctly identify the parts in our dataset with 98% accuracy using the
calculated features.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01998" title="Abstract">arXiv:2310.01998</a> [<a href="/pdf/2310.01998" title="Download PDF">pdf</a>, <a href="/format/2310.01998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Formalization of Complete Discrete Valuation Rings and Local Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Frutos-Fern%C3%A1ndez%2C+M+I">Mar&#xed;a In&#xe9;s de Frutos-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Di+Capriglio%2C+F+A+E+N+M+M">Filippo Alberto Edoardo Nuccio Mortarino Majno Di Capriglio</a> (CTN, ICJ)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Number Theory (math.NT)

</div>
<p class="mathjax">Local fields, and fields complete with respect to a discrete valuation, are
essential objects in commutative algebra, with applications to number theory
and algebraic geometry. We formalize in Lean the basic theory of discretely
valued fields. In particular, we prove that the unit ball with respect to a
discrete valuation on a field is a discrete valuation ring and, conversely,
that the adic valuation on the field of fractions of a discrete valuation ring
is discrete. We define finite extensions of valuations and of discrete
valuation rings, and prove some global-to-local results. Building on this
general theory, we formalize the abstract definition and some fundamental
properties of local fields. As an application, we show that finite extensions
of the field $\mathbb{Q}_p$ of $p$-adic numbers and of the field
$\mathbb{F}_p(\!(X)\!)$ of Laurent series over $\mathbb{F}_p$ are local fields.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02000" title="Abstract">arXiv:2310.02000</a> [<a href="/pdf/2310.02000" title="Download PDF">pdf</a>, <a href="/format/2310.02000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep  Models for X-ray Images of Multiple Body Parts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Weibin Liao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Haoyi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingzhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+Y">Yan Mo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+D">Dejing Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While self-supervised learning (SSL) algorithms have been widely used to
pre-train deep models, few efforts [11] have been done to improve
representation learning of X-ray image analysis with SSL pre-trained models. In
this work, we study a novel self-supervised pre-training pipeline, namely
Multi-task Self-super-vised Continual Learning (MUSCLE), for multiple medical
imaging tasks, such as classification and segmentation, using X-ray images
collected from multiple body parts, including heads, lungs, and bones.
Specifically, MUSCLE aggregates X-rays collected from multiple body parts for
MoCo-based representation learning, and adopts a well-designed continual
learning (CL) procedure to further pre-train the backbone subject various X-ray
analysis tasks jointly. Certain strategies for image pre-processing, learning
schedules, and regularization have been used to solve data heterogeneity,
overfitting, and catastrophic forgetting problems for multi-task/dataset
learning in MUSCLE.We evaluate MUSCLE using 9 real-world X-ray datasets with
various tasks, including pneumonia classification, skeletal abnormality
classification, lung segmentation, and tuberculosis (TB) detection. Comparisons
against other pre-trained models [7] confirm the proof-of-concept that
self-supervised multi-task/dataset continual pre-training could boost the
performance of X-ray image analysis.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02002" title="Abstract">arXiv:2310.02002</a> [<a href="/pdf/2310.02002" title="Download PDF">pdf</a>, <a href="/format/2310.02002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Throughput and Coverage Trade-Off in Integrated Terrestrial and  Non-Terrestrial Networks: an Optimization Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+H">Henri Alam</a>, 
<a href="/search/cs?searchtype=author&query=De+Domenico%2C+A">Antonio De Domenico</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez-P%C3%A9rez%2C+D">David L&#xf3;pez-P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Kaltenberger%2C+F">Florian Kaltenberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in IEEE International Conference on Communications Workshops 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In past years, non-terrestrial networks (NTNs) have emerged as a viable
solution for providing ubiquitous connectivity for future wireless networks due
to their ability to reach large geographical areas. However, the efficient
integration and operation of an NTN with a classic terrestrial network (TN) is
challenging due the large amount of parameters to tune. In this paper, we
consider the downlink scenario of an integrated TN-NTN transmitting over the S
band, comprised of low-earth orbit (LEO) satellites overlapping a large-scale
ground cellular network. We propose a new resource management framework to
optimize the user equipment (UE) performance by properly controlling the
spectrum allocation, the UE association and the transmit power of ground base
stations (BSs) and satellites. Our study reveals that, in rural scenarios,
NTNs, combined with the proposed radio resource management framework, reduce
the number of UEs that are out of coverage, highlighting the important role of
NTNs in providing ubiquitous connectivity, and greatly improve the overall
capacity of the network. Specifically, our solution leads to more than 200%
gain in terms of mean data rate with respect to a network without satellites
and a standard integrated TN-NTN when the resource allocation setting follows
3GPP recommendation.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02003" title="Abstract">arXiv:2310.02003</a> [<a href="/pdf/2310.02003" title="Download PDF">pdf</a>, <a href="/format/2310.02003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L2MAC: Large Language Model Automatic Computer for Unbounded Code  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holt%2C+S">Samuel Holt</a>, 
<a href="/search/cs?searchtype=author&query=Luyten%2C+M+R">Max Ruiz Luyten</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Copyright 2023 by the author(s)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
<p class="mathjax">Transformer-based large language models (LLMs) are constrained by the fixed
context window of the underlying transformer architecture, hindering their
ability to produce long and logically consistent code. Memory-augmented LLMs
are a promising solution, but current approaches cannot handle long code
generation tasks since they (1) only focus on reading memory and reduce its
evolution to the concatenation of new memories or (2) use very specialized
memories that cannot adapt to other domains. This paper presents L2MAC, the
first practical LLM-based stored-program automatic computer for long and
consistent code generation. Its memory has two components: the instruction
registry, which is populated with a prompt program to solve the user-given
task, and a file store, which will contain the final and intermediate outputs.
Each instruction is executed by a separate LLM instance, whose context is
managed by a control unit capable of precise memory reading and writing to
ensure effective interaction with the file store. These components enable L2MAC
to generate virtually unbounded code structures, bypassing the constraints of
the finite context window while producing code that fulfills complex
user-specified requirements. We empirically show that L2MAC succeeds in
generating large code bases for system design tasks where other coding methods
fall short in implementing user requirements and provide insight into the
reasons for this performance gap.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02005" title="Abstract">arXiv:2310.02005</a> [<a href="/pdf/2310.02005" title="Download PDF">pdf</a>, <a href="/format/2310.02005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Convergence Analysis of Tsetlin Machines: A Probabilistic  Approach to Concept Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belaid%2C+M">Mohamed-Bachir Belaid</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+J">Jivitesh Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+L">Lei Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Granmo%2C+O">Ole-Christoffer Granmo</a>, 
<a href="/search/cs?searchtype=author&query=Andersen%2C+P">Per-Arne Andersen</a>, 
<a href="/search/cs?searchtype=author&query=Yazidi%2C+A">Anis Yazidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Tsetlin Machines (TMs) have garnered increasing interest for their ability to
learn concepts via propositional formulas and their proven efficiency across
various application domains. Despite this, the convergence proof for the TMs,
particularly for the AND operator (\emph{conjunction} of literals), in the
generalized case (inputs greater than two bits) remains an open problem. This
paper aims to fill this gap by presenting a comprehensive convergence analysis
of Tsetlin automaton-based Machine Learning algorithms. We introduce a novel
framework, referred to as Probabilistic Concept Learning (PCL), which
simplifies the TM structure while incorporating dedicated feedback mechanisms
and dedicated inclusion/exclusion probabilities for literals. Given $n$
features, PCL aims to learn a set of conjunction clauses $C_i$ each associated
with a distinct inclusion probability $p_i$. Most importantly, we establish a
theoretical proof confirming that, for any clause $C_k$, PCL converges to a
conjunction of literals when $0.5&lt;p_k&lt;1$. This result serves as a stepping
stone for future research on the convergence properties of Tsetlin
automaton-based learning algorithms. Our findings not only contribute to the
theoretical understanding of Tsetlin Machines but also have implications for
their practical application, potentially leading to more robust and
interpretable machine learning models.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02008" title="Abstract">arXiv:2310.02008</a> [<a href="/pdf/2310.02008" title="Download PDF">pdf</a>, <a href="/format/2310.02008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> fmeffects: An R Package for Forward Marginal Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6we%2C+H">Holger L&#xf6;we</a>, 
<a href="/search/cs?searchtype=author&query=Scholbeck%2C+C+A">Christian A. Scholbeck</a>, 
<a href="/search/cs?searchtype=author&query=Heumann%2C+C">Christian Heumann</a>, 
<a href="/search/cs?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/cs?searchtype=author&query=Casalicchio%2C+G">Giuseppe Casalicchio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
<p class="mathjax">Forward marginal effects (FMEs) have recently been introduced as a versatile
and effective model-agnostic interpretation method. They provide comprehensible
and actionable model explanations in the form of: If we change $x$ by an amount
$h$, what is the change in predicted outcome $\widehat{y}$? We present the R
package fmeffects, the first software implementation of FMEs. The relevant
theoretical background, package functionality and handling, as well as the
software design and options for future extensions are discussed in this paper.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02011" title="Abstract">arXiv:2310.02011</a> [<a href="/pdf/2310.02011" title="Download PDF">pdf</a>, <a href="/format/2310.02011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Human Activities: Analyzing Wearable Accelerometer and  Gyroscope Data for Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+U">Utsab Saha</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Sawradip Saha</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+T">Tahmid Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Fattah%2C+S+A">Shaikh Anowarul Fattah</a>, 
<a href="/search/cs?searchtype=author&query=Saquib%2C+M">Mohammad Saquib</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A person's movement or relative positioning effectively generates raw
electrical signals that can be read by computing machines to apply various
manipulative techniques for the classification of different human activities.
In this paper, a stratified multi-structural approach based on a Residual
network ensembled with Residual MobileNet is proposed, termed as FusionActNet.
The proposed method involves using carefully designed Residual blocks for
classifying the static and dynamic activities separately because they have
clear and distinct characteristics that set them apart. These networks are
trained independently, resulting in two specialized and highly accurate models.
These models excel at recognizing activities within a specific superclass by
taking advantage of the unique algorithmic benefits of architectural
adjustments. Afterward, these two ResNets are passed through a weighted
ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently
discriminates between a specific static and a specific dynamic activity, which
were previously identified based on their distinct feature characteristics in
the earlier stage. The proposed model is evaluated using two publicly
accessible datasets; namely, UCI HAR and Motion-Sense. Therein, it successfully
handled the highly confusing cases of data overlap. Therefore, the proposed
approach achieves a state-of-the-art accuracy of 96.71% and 95.35% in the UCI
HAR and Motion-Sense datasets respectively.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02012" title="Abstract">arXiv:2310.02012</a> [<a href="/pdf/2310.02012" title="Download PDF">pdf</a>, <a href="/format/2310.02012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Training Without Depth Limits: Batch Normalization Without  Gradient Explosion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meterez%2C+A">Alexandru Meterez</a>, 
<a href="/search/cs?searchtype=author&query=Joudaki%2C+A">Amir Joudaki</a>, 
<a href="/search/cs?searchtype=author&query=Orabona%2C+F">Francesco Orabona</a>, 
<a href="/search/cs?searchtype=author&query=Immer%2C+A">Alexander Immer</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A4tsch%2C+G">Gunnar R&#xe4;tsch</a>, 
<a href="/search/cs?searchtype=author&query=Daneshmand%2C+H">Hadi Daneshmand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Normalization layers are one of the key building blocks for deep neural
networks. Several theoretical studies have shown that batch normalization
improves the signal propagation, by avoiding the representations from becoming
collinear across the layers. However, results on mean-field theory of batch
normalization also conclude that this benefit comes at the expense of exploding
gradients in depth. Motivated by these two aspects of batch normalization, in
this study we pose the following question: "Can a batch-normalized network keep
the optimal signal propagation properties, but avoid exploding gradients?" We
answer this question in the affirmative by giving a particular construction of
an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization
that provably has bounded gradients at any depth. Based on Weingarten calculus,
we develop a rigorous and non-asymptotic theory for this constructed MLP that
gives a precise characterization of forward signal propagation, while proving
that gradients remain bounded for linearly independent input samples, which
holds in most practical settings. Inspired by our theory, we also design an
activation shaping scheme that empirically achieves the same properties for
certain non-linear activations.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02013" title="Abstract">arXiv:2310.02013</a> [<a href="/pdf/2310.02013" title="Download PDF">pdf</a>, <a href="/format/2310.02013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral operator learning for parametric PDEs without data reliance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Junho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Taehyun Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Namjung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Youngjoon Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we introduce the Spectral Coefficient Learning via Operator
Network (SCLON), a novel operator learning-based approach for solving
parametric partial differential equations (PDEs) without the need for data
harnessing. The cornerstone of our method is the spectral methodology that
employs expansions using orthogonal functions, such as Fourier series and
Legendre polynomials, enabling accurate PDE solutions with fewer grid points.
By merging the merits of spectral methods - encompassing high accuracy,
efficiency, generalization, and the exact fulfillment of boundary conditions -
with the prowess of deep neural networks, SCLON offers a transformative
strategy. Our approach not only eliminates the need for paired input-output
training data, which typically requires extensive numerical computations, but
also effectively learns and predicts solutions of complex parametric PDEs,
ranging from singularly perturbed convection-diffusion equations to the
Navier-Stokes equations. The proposed framework demonstrates superior
performance compared to existing scientific machine learning techniques,
offering solutions for multiple instances of parametric PDEs without harnessing
data. The mathematical framework is robust and reliable, with a well-developed
loss function derived from the weak formulation, ensuring accurate
approximation of solutions while exactly satisfying boundary conditions. The
method's efficacy is further illustrated through its ability to accurately
predict intricate natural behaviors like the Kolmogorov flow and boundary
layers. In essence, our work pioneers a compelling avenue for parametric PDE
solutions, serving as a bridge between traditional numerical methodologies and
cutting-edge machine learning techniques in the realm of scientific
computation.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02016" title="Abstract">arXiv:2310.02016</a> [<a href="/pdf/2310.02016" title="Download PDF">pdf</a>, <a href="/format/2310.02016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranking a Set of Objects using Heterogeneous Workers: QUITE an Easy  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nordio%2C+A">Alessandro Nordio</a>, 
<a href="/search/cs?searchtype=author&query=tarable%2C+A">Alberto tarable</a>, 
<a href="/search/cs?searchtype=author&query=Leonardi%2C+E">Emilio Leonardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC); Performance (cs.PF); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We focus on the problem of ranking $N$ objects starting from a set of noisy
pairwise comparisons provided by a crowd of unequal workers, each worker being
characterized by a specific degree of reliability, which reflects her ability
to rank pairs of objects. More specifically, we assume that objects are endowed
with intrinsic qualities and that the probability with which an object is
preferred to another depends both on the difference between the qualities of
the two competitors and on the reliability of the worker. We propose QUITE, a
non-adaptive ranking algorithm that jointly estimates workers' reliabilities
and qualities of objects. Performance of QUITE is compared in different
scenarios against previously proposed algorithms. Finally, we show how QUITE
can be naturally made adaptive.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02019" title="Abstract">arXiv:2310.02019</a> [<a href="/pdf/2310.02019" title="Download PDF">pdf</a>, <a href="/format/2310.02019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Feasible Counterfactual Explanations: A Taxonomy Guided  Template-based NLG Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salimi%2C+P">Pedram Salimi</a>, 
<a href="/search/cs?searchtype=author&query=Wiratunga%2C+N">Nirmalie Wiratunga</a>, 
<a href="/search/cs?searchtype=author&query=Corsar%2C+D">David Corsar</a>, 
<a href="/search/cs?searchtype=author&query=Wijekoon%2C+A">Anjana Wijekoon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Volume 372: ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Counterfactual Explanations (cf-XAI) describe the smallest changes in feature
values necessary to change an outcome from one class to another. However, many
cf-XAI methods neglect the feasibility of those changes. In this paper, we
introduce a novel approach for presenting cf-XAI in natural language
(Natural-XAI), giving careful consideration to actionable and comprehensible
aspects while remaining cognizant of immutability and ethical concerns. We
present three contributions to this endeavor. Firstly, through a user study, we
identify two types of themes present in cf-XAI composed by humans:
content-related, focusing on how features and their values are included from
both the counterfactual and the query perspectives; and structure-related,
focusing on the structure and terminology used for describing necessary value
changes. Secondly, we introduce a feature actionability taxonomy with four
clearly defined categories, to streamline the explanation presentation process.
Using insights from the user study and our taxonomy, we created a generalisable
template-based natural language generation (NLG) method compatible with
existing explainers like DICE, NICE, and DisCERN, to produce counterfactuals
that address the aforementioned limitations of existing approaches. Finally, we
conducted a second user study to assess the performance of our taxonomy-guided
NLG templates on three domains. Our findings show that the taxonomy-guided
Natural-XAI approach (n-XAI^T) received higher user ratings across all
dimensions, with significantly improved results in the majority of the domains
assessed for articulation, acceptability, feasibility, and sensitivity
dimensions.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02022" title="Abstract">arXiv:2310.02022</a> [<a href="/pdf/2310.02022" title="Download PDF">pdf</a>, <a href="/ps/2310.02022" title="Download PostScript">ps</a>, <a href="/format/2310.02022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing substructure search: a novel approach for efficient querying  in large chemical databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaskin%2C+V">Vsevolod Vaskin</a>, 
<a href="/search/cs?searchtype=author&query=Jakovlev%2C+D">Dmitri Jakovlev</a>, 
<a href="/search/cs?searchtype=author&query=Bakharev%2C+F">Fedor Bakharev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 table, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Substructure search in chemical compound databases is a fundamental task in
cheminformatics with critical implications for fields such as drug discovery,
materials science, and toxicology. However, the increasing size and complexity
of chemical databases have rendered traditional search algorithms ineffective,
exacerbating the need for scalable solutions. We introduce a novel approach to
enhance the efficiency of substructure search, moving beyond the traditional
full-enumeration methods. Our strategy employs a unique index structure: a tree
that segments the molecular data set into clusters based on the presence or
absence of certain features. This innovative indexing mechanism is inspired by
the binary Ball-Tree concept and demonstrates superior performance over
exhaustive search methods, leading to significant acceleration in the initial
filtering process. Comparative analysis with the existing Bingo algorithm
reveals the efficiency and versatility of our approach. Although the current
implementation does not affect the verification stage, it has the potential to
reduce false positive rates. Our method offers a promising avenue for future
research, meeting the growing demand for fast and accurate substructure search
in increasingly large chemical databases.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02023" title="Abstract">arXiv:2310.02023</a> [<a href="/pdf/2310.02023" title="Download PDF">pdf</a>, <a href="/format/2310.02023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nash Regret Guarantees for Linear Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sawarni%2C+A">Ayush Sawarni</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+S">Soumybrata Pal</a>, 
<a href="/search/cs?searchtype=author&query=Barman%2C+S">Siddharth Barman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We obtain essentially tight upper bounds for a strengthened notion of regret
in the stochastic linear bandits framework. The strengthening -- referred to as
Nash regret -- is defined as the difference between the (a priori unknown)
optimum and the geometric mean of expected rewards accumulated by the linear
bandit algorithm. Since the geometric mean corresponds to the well-studied Nash
social welfare (NSW) function, this formulation quantifies the performance of a
bandit algorithm as the collective welfare it generates across rounds. NSW is
known to satisfy fairness axioms and, hence, an upper bound on Nash regret
provides a principled fairness guarantee.
<br />We consider the stochastic linear bandits problem over a horizon of $T$
rounds and with set of arms ${X}$ in ambient dimension $d$. Furthermore, we
focus on settings in which the stochastic reward -- associated with each arm in
${X}$ -- is a non-negative, $\nu$-sub-Poisson random variable. For this
setting, we develop an algorithm that achieves a Nash regret of $O\left(
\sqrt{\frac{d\nu}{T}} \log( T |X|)\right)$. In addition, addressing linear
bandit instances in which the set of arms ${X}$ is not necessarily finite, we
obtain a Nash regret upper bound of $O\left(
\frac{d^\frac{5}{4}\nu^{\frac{1}{2}}}{\sqrt{T}} \log(T)\right)$. Since bounded
random variables are sub-Poisson, these results hold for bounded, positive
rewards. Our linear bandit algorithm is built upon the successive elimination
method with novel technical insights, including tailored concentration bounds
and the use of sampling via John ellipsoid in conjunction with the
Kiefer-Wolfowitz optimal design.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02025" title="Abstract">arXiv:2310.02025</a> [<a href="/pdf/2310.02025" title="Download PDF">pdf</a>, <a href="/format/2310.02025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aochuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Diffenderfer%2C+J">James Diffenderfer</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Parasyris%2C+K">Konstantinos Parasyris</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kailkhura%2C+B">Bhavya Kailkhura</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Zeroth-order (ZO) optimization has become a popular technique for solving
machine learning (ML) problems when first-order (FO) information is difficult
or impossible to obtain. However, the scalability of ZO optimization remains an
open problem: Its use has primarily been limited to relatively small-scale ML
problems, such as sample-wise adversarial attack generation. To our best
knowledge, no prior work has demonstrated the effectiveness of ZO optimization
in training deep neural networks (DNNs) without a significant decrease in
performance. To overcome this roadblock, we develop DeepZero, a principled ZO
deep learning (DL) framework that can scale ZO optimization to DNN training
from scratch through three primary innovations. First, we demonstrate the
advantages of coordinate-wise gradient estimation (CGE) over randomized
vector-wise gradient estimation in training accuracy and computational
efficiency. Second, we propose a sparsity-induced ZO training protocol that
extends the model pruning methodology using only finite differences to explore
and exploit the sparse DL prior in CGE. Third, we develop the methods of
feature reuse and forward parallelization to advance the practical
implementations of ZO training. Our extensive experiments show that DeepZero
achieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10,
approaching FO training performance for the first time. Furthermore, we show
the practical utility of DeepZero in applications of certified adversarial
defense and DL-based partial differential equation error correction, achieving
10-20% improvement over SOTA. We believe our results will inspire future
research on scalable ZO optimization and contribute to advancing DL with black
box.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02027" title="Abstract">arXiv:2310.02027</a> [<a href="/pdf/2310.02027" title="Download PDF">pdf</a>, <a href="/format/2310.02027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaxu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xinping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages including appendix and reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Hyperbolic graph convolutional networks (HGCN) have demonstrated significant
potential in extracting information from hierarchical graphs. However, existing
HGCNs are limited to shallow architectures, due to the expensive hyperbolic
operations and the over-smoothing issue as depth increases. Although in GCNs,
treatments have been applied to alleviate over-smoothing, developing a
hyperbolic therapy presents distinct challenges since operations should be
carefully designed to fit the hyperbolic nature. Addressing the above
challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN
architecture with dramatically improved computational efficiency and
substantially alleviated over-smoothing effect. DeepHGCN presents two key
enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer
that enables fast and accurate linear maps; and (2) Techniques such as
hyperbolic residual connections and regularization for both weights and
features facilitated by an efficient hyperbolic midpoint method. Extensive
experiments demonstrate that DeepHGCN obtains significant improvements in link
prediction and node classification tasks compared to both Euclidean and shallow
hyperbolic GCN variants.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02029" title="Abstract">arXiv:2310.02029</a> [<a href="/pdf/2310.02029" title="Download PDF">pdf</a>, <a href="/format/2310.02029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Between accurate prediction and poor decision making: the AI/ML gap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bontempi%2C+G">Gianluca Bontempi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Position paper presented in the BENELEARN 2022 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Intelligent agents rely on AI/ML functionalities to predict the consequence
of possible actions and optimise the policy. However, the effort of the
research community in addressing prediction accuracy has been so intense (and
successful) that it created the illusion that the more accurate the learner
prediction (or classification) the better would have been the final decision.
Now, such an assumption is valid only if the (human or artificial) decision
maker has complete knowledge of the utility of the possible actions. This paper
argues that AI/ML community has taken so far a too unbalanced approach by
devoting excessive attention to the estimation of the state (or target)
probability to the detriment of accurate and reliable estimations of the
utility. In particular, few evidence exists about the impact of a wrong utility
assessment on the resulting expected utility of the decision strategy. This
situation is creating a substantial gap between the expectations and the
effective impact of AI solutions, as witnessed by recent criticisms and
emphasised by the regulatory legislative efforts. This paper aims to study this
gap by quantifying the sensitivity of the expected utility to the utility
uncertainty and comparing it to the one due to probability estimation.
Theoretical and simulated results show that an inaccurate utility assessment
may as (and sometimes) more harmful than a poor probability estimation. The
final recommendation to the community is then to undertake a focus shift from a
pure accuracy-driven (or obsessed) approach to a more utility-aware
methodology.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02031" title="Abstract">arXiv:2310.02031</a> [<a href="/pdf/2310.02031" title="Download PDF">pdf</a>, <a href="/format/2310.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OceanGPT: A Large Language Model for Ocean Science Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guozhou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Project Website: <a href="https://zjunlp.github.io/project/OceanGPT/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02032" title="Abstract">arXiv:2310.02032</a> [<a href="/pdf/2310.02032" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> aSAGA: Automatic Sleep Analysis with Gray Areas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rusanen%2C+M">Matias Rusanen</a>, 
<a href="/search/cs?searchtype=author&query=Jouan%2C+G">Gabriel Jouan</a>, 
<a href="/search/cs?searchtype=author&query=Huttunen%2C+R">Riku Huttunen</a>, 
<a href="/search/cs?searchtype=author&query=Nikkonen%2C+S">Sami Nikkonen</a>, 
<a href="/search/cs?searchtype=author&query=Sigur%C3%B0ard%C3%B3ttir%2C+S">Sigr&#xed;&#xf0;ur Sigur&#xf0;ard&#xf3;ttir</a>, 
<a href="/search/cs?searchtype=author&query=T%C3%B6yr%C3%A4s%2C+J">Juha T&#xf6;yr&#xe4;s</a>, 
<a href="/search/cs?searchtype=author&query=Duce%2C+B">Brett Duce</a>, 
<a href="/search/cs?searchtype=author&query=Myllymaa%2C+S">Sami Myllymaa</a>, 
<a href="/search/cs?searchtype=author&query=Arnardottir%2C+E+S">Erna Sif Arnardottir</a>, 
<a href="/search/cs?searchtype=author&query=Lepp%C3%A4nen%2C+T">Timo Lepp&#xe4;nen</a>, 
<a href="/search/cs?searchtype=author&query=Islind%2C+A+S">Anna Sigridur Islind</a>, 
<a href="/search/cs?searchtype=author&query=Kainulainen%2C+S">Samu Kainulainen</a>, 
<a href="/search/cs?searchtype=author&query=Korkalainen%2C+H">Henri Korkalainen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">State-of-the-art automatic sleep staging methods have already demonstrated
comparable reliability and superior time efficiency to manual sleep staging.
However, fully automatic black-box solutions are difficult to adapt into
clinical workflow and the interaction between explainable automatic methods and
the work of sleep technologists remains underexplored and inadequately
conceptualized. Thus, we propose a human-in-the-loop concept for sleep
analysis, presenting an automatic sleep staging model (aSAGA), that performs
effectively with both clinical polysomnographic recordings and home sleep
studies. To validate the model, extensive testing was conducted, employing a
preclinical validation approach with three retrospective datasets; open-access,
clinical, and research-driven. Furthermore, we validate the utilization of
uncertainty mapping to identify ambiguous regions, conceptualized as gray
areas, in automatic sleep analysis that warrants manual re-evaluation. The
results demonstrate that the automatic sleep analysis achieved a comparable
level of agreement with manual analysis across different sleep recording types.
Moreover, validation of the gray area concept revealed its potential to enhance
sleep staging accuracy and identify areas in the recordings where sleep
technologists struggle to reach a consensus. In conclusion, this study
introduces and validates a concept from explainable artificial intelligence
into sleep medicine and provides the basis for integrating human-in-the-loop
automatic sleep staging into clinical workflows, aiming to reduce black-box
criticism and the burden associated with manual sleep staging.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02037" title="Abstract">arXiv:2310.02037</a> [<a href="/pdf/2310.02037" title="Download PDF">pdf</a>, <a href="/format/2310.02037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An evaluation of pre-trained models for feature extraction in image  classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Silva+Puls%2C+E">Erick da Silva Puls</a>, 
<a href="/search/cs?searchtype=author&query=Todescato%2C+M+V">Matheus V. Todescato</a>, 
<a href="/search/cs?searchtype=author&query=Carbonera%2C+J+L">Joel L. Carbonera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, we have witnessed a considerable increase in performance in
image classification tasks. This performance improvement is mainly due to the
adoption of deep learning techniques. Generally, deep learning techniques
demand a large set of annotated data, making it a challenge when applying it to
small datasets. In this scenario, transfer learning strategies have become a
promising alternative to overcome these issues. This work aims to compare the
performance of different pre-trained neural networks for feature extraction in
image classification tasks. We evaluated 16 different pre-trained models in
four image datasets. Our results demonstrate that the best general performance
along the datasets was achieved by CLIP-ViT-B and ViT-H-14, where the
CLIP-ResNet50 model had similar performance but with less variability.
Therefore, our study provides evidence supporting the choice of models for
feature extraction in image classification tasks.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02040" title="Abstract">arXiv:2310.02040</a> [<a href="/pdf/2310.02040" title="Download PDF">pdf</a>, <a href="/format/2310.02040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jury: A Comprehensive Evaluation Toolkit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cavusoglu%2C+D">Devrim Cavusoglu</a>, 
<a href="/search/cs?searchtype=author&query=Sert%2C+U">Ulas Sert</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+S">Secil Sen</a>, 
<a href="/search/cs?searchtype=author&query=Altinuc%2C+S">Sinan Altinuc</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Evaluation plays a critical role in deep learning as a fundamental block of
any prediction-based system. However, the vast number of Natural Language
Processing (NLP) tasks and the development of various metrics have led to
challenges in evaluating different systems with different metrics. To address
these challenges, we introduce jury, a toolkit that provides a unified
evaluation framework with standardized structures for performing evaluation
across different tasks and metrics. The objective of jury is to standardize and
improve metric evaluation for all systems and aid the community in overcoming
the challenges in evaluation. Since its open-source release, jury has reached a
wide audience and is available at https://github.com/obss/jury.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02041" title="Abstract">arXiv:2310.02041</a> [<a href="/pdf/2310.02041" title="Download PDF">pdf</a>, <a href="/ps/2310.02041" title="Download PostScript">ps</a>, <a href="/format/2310.02041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Inhibitor: ReLU and Addition-Based Attention for Efficient  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Br%C3%A4nnvall%2C+R">Rickard Br&#xe4;nnvall</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">To enhance the computational efficiency of quantized Transformers, we replace
the dot-product and Softmax-based attention with an alternative mechanism
involving addition and ReLU activation only. This side-steps the expansion to
double precision often required by matrix multiplication and avoids costly
Softmax evaluations but maintains much of the core functionality of
conventional dot-product attention. It can enable more efficient execution and
support larger quantized Transformer models on resource-constrained hardware or
alternative arithmetic systems like homomorphic encryption. Training
experiments on four common benchmark tasks show test set prediction scores
comparable to those of conventional Transformers with dot-product attention.
Our scaling experiments also suggest significant computational savings, both in
plaintext and under encryption. In particular, we believe that the ReLU and
addition-based attention mechanism introduced in this paper may enable
privacy-preserving AI applications operating under homomorphic encryption by
avoiding the costly multiplication of encrypted variables.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02043" title="Abstract">arXiv:2310.02043</a> [<a href="/pdf/2310.02043" title="Download PDF">pdf</a>, <a href="/format/2310.02043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> View-Independent Adjoint Light Tracing for Lighting Design Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lipp%2C+L">Lukas Lipp</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+D">David Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Ecormier-Nocca%2C+P">Pierre Ecormier-Nocca</a>, 
<a href="/search/cs?searchtype=author&query=Rist%2C+F">Florian Rist</a>, 
<a href="/search/cs?searchtype=author&query=Wimmer%2C+M">Michael Wimmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Controlling light is a central element when composing a scene, enabling
artistic expression, as well as the design of comfortable living spaces. In
contrast to previous camera-based inverse rendering approaches, we introduce a
novel method for interactive, view-independent differentiable global
illumination. Our method first performs a forward light-tracing pass, starting
from the light sources and storing the resulting radiance field on the scene
geometry, representing specular highlights via hemi-spherical harmonics. We
then evaluate an objective function on the entire radiance data and propagate
derivatives back to the lighting parameters by formulating a novel, analytical
adjoint light-tracing step. Our method builds on GPU ray tracing, which allows
us to optimize all lighting parameters at interactive rates, even for complex
geometry. Instead of specifying optimization targets as view-specific images,
our method allows us to optimize the lighting of an entire scene to match
either baked illumination (e.g., lightmaps), regulatory lighting requirements
for work spaces, or artistic sketches drawn directly on the geometry. This
approach provides a more direct and intuitive user experience for designers. We
visualize our adjoint gradients and compare them to image-based
state-of-the-art differentiable rendering methods. We also compare the
convergence behavior of various optimization algorithms when using our gradient
data vs. image-based differentiable rendering methods. Qualitative comparisons
with real-world scenes underline the practical applicability of our method.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02044" title="Abstract">arXiv:2310.02044</a> [<a href="/pdf/2310.02044" title="Download PDF">pdf</a>, <a href="/format/2310.02044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Transformers under Occlusion: How Physics and Background  Attributes Impact Large Models for Robotic Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shutong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zahid%2C+M">Muhammad Zahid</a>, 
<a href="/search/cs?searchtype=author&query=Pokorny%2C+F+T">Florian T. Pokorny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">As transformer architectures and dataset sizes continue to scale, the need to
understand the specific dataset factors affecting model performance becomes
increasingly urgent. This paper investigates how object physics attributes
(color, friction coefficient, shape) and background characteristics (static,
dynamic, background complexity) influence the performance of Video Transformers
in trajectory prediction tasks under occlusion. Beyond mere occlusion
challenges, this study aims to investigate three questions: How do object
physics attributes and background characteristics influence the model
performance? What kinds of attributes are most influential to the model
generalization? Is there a data saturation point for large transformer model
performance within a single task? To facilitate this research, we present
OccluManip, a real-world video-based robot pushing dataset comprising 460,000
consistent recordings of objects with different physics and varying
backgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible
temporal length along with target object trajectories are collected,
accommodating tasks with different temporal requirements. Additionally, we
propose Video Occlusion Transformer (VOT), a generic video-transformer-based
network achieving an average 96% accuracy across all 18 sub-datasets provided
in OccluManip. OccluManip and VOT will be released at:
https://github.com/ShutongJIN/OccluManip.git
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02045" title="Abstract">arXiv:2310.02045</a> [<a href="/pdf/2310.02045" title="Download PDF">pdf</a>, <a href="/format/2310.02045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trikarenos: A Fault-Tolerant RISC-V-based Microcontroller for CubeSats  in 28nm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rogenmoser%2C+M">Michael Rogenmoser</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures, accepted by IEEE International Conference on Electronics Circuits and Systems (ICECS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">One of the key challenges when operating microcontrollers in harsh
environments such as space is radiation-induced Single Event Upsets (SEUs),
which can lead to errors in computation. Common countermeasures rely on
proprietary radiation-hardened technologies, low density technologies, or
extensive replication, leading to high costs and low performance and
efficiency. To combat this, we present Trikarenos, a fault-tolerant 32-bit
RISC-V microcontroller SoC in an advanced TSMC 28nm technology. Trikarenos
alleviates the replication cost by employing a configurable triple-core
lockstep configuration, allowing three Ibex cores to execute applications
reliably, operating on ECC-protected memory. If reliability is not needed for a
given application, the cores can operate independently in parallel for higher
performance and efficiency. Trikarenos consumes 15.7mW at 250MHz executing a
fault-tolerant matrix-matrix multiplication, a 21.5x efficiency gain over
state-of-the-art, and performance is increased by 2.96x when reliability is not
needed for processing, with a 2.36x increase in energy efficiency.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02046" title="Abstract">arXiv:2310.02046</a> [<a href="/pdf/2310.02046" title="Download PDF">pdf</a>, <a href="/format/2310.02046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving web element localization by using a large language model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nass%2C+M">Michel Nass</a>, 
<a href="/search/cs?searchtype=author&query=Alegroth%2C+E">Emil Alegroth</a>, 
<a href="/search/cs?searchtype=author&query=Feldt%2C+R">Robert Feldt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Web-based test automation heavily relies on accurately finding web elements.
Traditional methods compare attributes but don't grasp the context and meaning
of elements and words. The emergence of Large Language Models (LLMs) like
GPT-4, which can show human-like reasoning abilities on some tasks, offers new
opportunities for software engineering and web element localization. This paper
introduces and evaluates VON Similo LLM, an enhanced web element localization
approach. Using an LLM, it selects the most likely web element from the
top-ranked ones identified by the existing VON Similo method, ideally aiming to
get closer to human-like selection accuracy. An experimental study was
conducted using 804 web element pairs from 48 real-world web applications. We
measured the number of correctly identified elements as well as the execution
times, comparing the effectiveness and efficiency of VON Similo LLM against the
baseline algorithm. In addition, motivations from the LLM were recorded and
analyzed for all instances where the original approach failed to find the right
web element. VON Similo LLM demonstrated improved performance, reducing failed
localizations from 70 to 39 (out of 804), a 44 percent reduction. Despite its
slower execution time and additional costs of using the GPT-4 model, the LLMs
human-like reasoning showed promise in enhancing web element localization. LLM
technology can enhance web element identification in GUI test automation,
reducing false positives and potentially lowering maintenance costs. However,
further research is necessary to fully understand LLMs capabilities,
limitations, and practical use in GUI testing.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02048" title="Abstract">arXiv:2310.02048</a> [<a href="/pdf/2310.02048" title="Download PDF">pdf</a>, <a href="/format/2310.02048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Generalisability of Self-Distillation with No Labels for  SAR-Based Vegetation Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Ferrer%2C+L">Laura Mart&#xed;nez-Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Jungbluth%2C+A">Anna Jungbluth</a>, 
<a href="/search/cs?searchtype=author&query=Gallego-Mejia%2C+J+A">Joseph A. Gallego-Mejia</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+M">Matt Allen</a>, 
<a href="/search/cs?searchtype=author&query=Dorr%2C+F">Francisco Dorr</a>, 
<a href="/search/cs?searchtype=author&query=Kalaitzis%2C+F">Freddie Kalaitzis</a>, 
<a href="/search/cs?searchtype=author&query=Ramos-Poll%C3%A1n%2C+R">Ra&#xfa;l Ramos-Poll&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work we pre-train a DINO-ViT based model using two Synthetic Aperture
Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We
fine-tune the models on smaller labeled datasets to predict vegetation
percentage, and empirically study the connection between the embedding space of
the models and their ability to generalize across diverse geographic regions
and to unseen data. For S1GRD, embedding spaces of different regions are
clearly separated, while GSSIC's overlaps. Positional patterns remain during
fine-tuning, and greater distances in embeddings often result in higher errors
for unfamiliar regions. With this, our work increases our understanding of
generalizability for self-supervised models applied to remote sensing.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02050" title="Abstract">arXiv:2310.02050</a> [<a href="/pdf/2310.02050" title="Download PDF">pdf</a>, <a href="/format/2310.02050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning Large language model for End-to-end Speech Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+N">Nianwen Si</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xukui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+D">Dan Qu</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+X">Xiaolin Jiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">With the emergence of large language models (LLMs), multimodal models based
on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,
and SpeechGPT exhibit an impressive ability to comprehend and generate human
instructions. However, their performance often falters when faced with complex
tasks like end-to-end speech translation (E2E-ST), a cross-language and
cross-modal translation task. In comparison to single-modal models, multimodal
models lag behind in these scenarios. This paper introduces LST, a Large
multimodal model designed to excel at the E2E-ST task. LST consists of a speech
frontend, an adapter, and a LLM backend. The training of LST consists of two
stages: (1) Modality adjustment, where the adapter is tuned to align speech
representation with text embedding space, and (2) Downstream task fine-tuning,
where both the adapter and LLM model are trained to optimize performance on the
E2EST task. Experimental results on the MuST-C speech translation benchmark
demonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on
En-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a
new state-of-the-art. Additionally, we conduct an in-depth analysis of
single-modal model selection and the impact of training strategies, which lays
the foundation for future research. We will open up our code and models after
review.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02051" title="Abstract">arXiv:2310.02051</a> [<a href="/pdf/2310.02051" title="Download PDF">pdf</a>, <a href="/ps/2310.02051" title="Download PostScript">ps</a>, <a href="/format/2310.02051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Tait Computability the Hard Way
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 73 pages, A5 paper, optimized for phone viewing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We walk through a few proofs of canonicity and normalization, each one with
more aspects dissected and re-expressed in category theory, so that readers can
compare the difference across proofs. During this process we isolate the
different ideas that make up the proofs. Finally we arrive at synthetic Tait
computability as proposed by J. Sterling. We also give a synthetic proof for
parametricity of system F.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02053" title="Abstract">arXiv:2310.02053</a> [<a href="/pdf/2310.02053" title="Download PDF">pdf</a>, <a href="/format/2310.02053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Topic-Focus Articulation in Meaning-to-Text Generation using  Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunliu Wang</a>, 
<a href="/search/cs?searchtype=author&query=van+Noord%2C+R">Rik van Noord</a>, 
<a href="/search/cs?searchtype=author&query=Bos%2C+J">Johan Bos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A bare meaning representation can be expressed in various ways using natural
language, depending on how the information is structured on the surface level.
We are interested in finding ways to control topic-focus articulation when
generating text from meaning. We focus on distinguishing active and passive
voice for sentences with transitive verbs. The idea is to add pragmatic
information such as topic to the meaning representation, thereby forcing either
active or passive voice when given to a natural language generation system. We
use graph neural models because there is no explicit information about word
order in a meaning represented by a graph. We try three different methods for
topic-focus articulation (TFA) employing graph neural models for a
meaning-to-text generation task. We propose a novel encoding strategy about
node aggregation in graph neural models, which instead of traditional encoding
by aggregating adjacent node information, learns node representations by using
depth-first search. The results show our approach can get competitive
performance with state-of-art graph models on general text generation, and lead
to significant improvements on the task of active-passive conversion compared
to traditional adjacency-based aggregation strategies. Different types of TFA
can have a huge impact on the performance of the graph models.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02054" title="Abstract">arXiv:2310.02054</a> [<a href="/pdf/2310.02054" title="Download PDF">pdf</a>, <a href="/format/2310.02054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zibin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yujing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tangjie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Changjie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhipeng Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Aligning agent behaviors with diverse human preferences remains a challenging
problem in reinforcement learning (RL), owing to the inherent abstractness and
mutability of human preferences. To address these issues, we propose AlignDiff,
a novel framework that leverages RL from Human Feedback (RLHF) to quantify
human preferences, covering abstractness, and utilizes them to guide diffusion
planning for zero-shot behavior customizing, covering mutability. AlignDiff can
accurately match user-customized behaviors and efficiently switch from one to
another. To build the framework, we first establish the multi-perspective human
feedback datasets, which contain comparisons for the attributes of diverse
behaviors, and then train an attribute strength model to predict quantified
relative strengths. After relabeling behavioral datasets with relative
strengths, we proceed to train an attribute-conditioned diffusion model, which
serves as a planner with the attribute strength model as a director for
preference aligning at the inference phase. We evaluate AlignDiff on various
locomotion tasks and demonstrate its superior performance on preference
matching, switching, and covering compared to other baselines. Its capability
of completing unseen downstream tasks under human instructions also showcases
the promising potential for human-AI collaboration. More visualization videos
are released on https://aligndiff.github.io/.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02055" title="Abstract">arXiv:2310.02055</a> [<a href="/pdf/2310.02055" title="Download PDF">pdf</a>, <a href="/format/2310.02055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrate-and-fire circuit for converting analog signals to spikes using  phase encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lopez-Randulfe%2C+J">Javier Lopez-Randulfe</a>, 
<a href="/search/cs?searchtype=author&query=Reeb%2C+N">Nico Reeb</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neuromorphic Computing and Engineering (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Neural and Evolutionary Computing (cs.NE); Signal Processing (eess.SP)

</div>
<p class="mathjax">Processing sensor data with spiking neural networks on digital neuromorphic
chips requires converting continuous analog signals into spike pulses. Two
strategies are promising for achieving low energy consumption and fast
processing speeds in end-to-end neuromorphic applications. First, to directly
encode analog signals to spikes to bypass the need for an analog-to-digital
converter (ADC). Second, to use temporal encoding techniques to maximize the
spike sparsity, which is a crucial parameter for fast and efficient
neuromorphic processing. In this work, we propose an adaptive control of the
refractory period of the leaky integrate-and-fire (LIF) neuron model for
encoding continuous analog signals into a train of time-coded spikes. The
LIF-based encoder generates phase-encoded spikes that are compatible with
digital hardware. We implemented the neuron model on a physical circuit and
tested it with different electric signals. A digital neuromorphic chip
processed the generated spike trains and computed the signal's frequency
spectrum using a spiking version of the Fourier transform. We tested the
prototype circuit on electric signals up to 1 KHz. Thus, we provide an
end-to-end neuromorphic application that generates the frequency spectrum of an
electric signal without the need for an ADC or a digital signal processing
algorithm.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02056" title="Abstract">arXiv:2310.02056</a> [<a href="/pdf/2310.02056" title="Download PDF">pdf</a>, <a href="/format/2310.02056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Data-Driven Models for Accurate Analysis of Grid-Tied Smart  Inverters Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Subedi%2C+S">Sunil Subedi</a>, 
<a href="/search/eess?searchtype=author&query=Guruwacharya%2C+N">Nischal Guruwacharya</a>, 
<a href="/search/eess?searchtype=author&query=Poudel%2C+B">Bidur Poudel</a>, 
<a href="/search/eess?searchtype=author&query=Vasquez-Plaza%2C+J+D">Jesus D. Vasquez-Plaza</a>, 
<a href="/search/eess?searchtype=author&query=Andrade%2C+F">Fabio Andrade</a>, 
<a href="/search/eess?searchtype=author&query=Fourney%2C+R">Robert Fourney</a>, 
<a href="/search/eess?searchtype=author&query=Rekabdarkolaee%2C+H+M">Hossein Moradi Rekabdarkolaee</a>, 
<a href="/search/eess?searchtype=author&query=Hansen%2C+T+M">Timothy M. Hansen</a>, 
<a href="/search/eess?searchtype=author&query=Tonkoski%2C+R">Reinaldo Tonkoski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The integration of power electronic converters (PECs) and distributed energy
resources (DERs) in modern power systems has introduced dynamism and
complexity. Accurate simulation becomes essential to comprehend the influence
of converter domination on the power grid. This study addresses the
fast-switching and stochastic behaviors exhibited by inverter-based resources
in converter-dominated power systems, highlighting the necessity for precise
analytical models. In the realm of modeling real-world systems, multiple
methodologies exist. Notably, black-box and data-driven system identification
techniques are employed to construct PEC models using experimental data,
without relying on a priori knowledge of the internal system physics. This
approach entails a systematic process of model class selection, parameter
estimation, and model validation. While a range of linear and nonlinear model
structures and estimation algorithms are at our disposal, it remains imperative
to harness creativity and a profound understanding of the physical system to
craft data-driven models that align seamlessly with their intended
applications. These applications may encompass simulation, prediction, control,
or fault detection. This report offers valuable insights into the collection of
datasets from commercial off-the-shelf inverters, along with the presentation
of intricate simulation models.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02059" title="Abstract">arXiv:2310.02059</a> [<a href="/pdf/2310.02059" title="Download PDF">pdf</a>, <a href="/format/2310.02059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Weaknesses of Copilot Generated Code in GitHub
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yujia Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Tahir%2C+A">Amjed Tahir</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zengyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shahin%2C+M">Mojtaba Shahin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiaxin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Modern code generation tools use AI models, particularly Large Language
Models (LLMs), to generate functional and complete code. While such tools are
becoming popular and widely available for developers, using these tools is
often accompanied by security challenges. Therefore, it is important to assess
the quality of the generated code, especially in terms of its security.
Researchers have recently explored various aspects of code generation tools,
including security. However, many open questions about the security of the
generated code require further investigation, especially the security issues of
automatically generated code in the wild. To this end, we conducted an
empirical study by analyzing the security weaknesses in code snippets generated
by GitHub Copilot that are found as part of publicly available projects hosted
on GitHub. The goal is to investigate the types of security issues and their
scale in real-world scenarios (rather than crafted scenarios). To this end, we
identified 435 code snippets generated by Copilot from publicly available
projects. We then conducted extensive security analysis to identify Common
Weakness Enumeration (CWE) instances in these code snippets. The results show
that (1) 35.8% of Copilot generated code snippets contain CWEs, and those
issues are spread across multiple languages, (2) the security weaknesses are
diverse and related to 42 different CWEs, in which CWE-78: OS Command
Injection, CWE-330: Use of Insufficiently Random Values, and CWE-703: Improper
Check or Handling of Exceptional Conditions occurred the most frequently, and
(3) among the 42 CWEs identified, 11 of those belong to the currently
recognized 2022 CWE Top-25. Our findings confirm that developers should be
careful when adding code generated by Copilot (and similar AI code generation
tools) and should also run appropriate security checks as they accept the
suggested code.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02060" title="Abstract">arXiv:2310.02060</a> [<a href="/pdf/2310.02060" title="Download PDF">pdf</a>, <a href="/format/2310.02060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Attractor for a Reaction-Diffusion Model Arising in Biological  Dynamic in 3D Soil Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elghandouri%2C+M">Mohamed Elghandouri</a>, 
<a href="/search/cs?searchtype=author&query=Ezzinbi%2C+K">Khalil Ezzinbi</a>, 
<a href="/search/cs?searchtype=author&query=Klai%2C+M">Mouad Klai</a>, 
<a href="/search/cs?searchtype=author&query=Monga%2C+O">Olivier Monga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Mathematical Geosciences
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Partial Differential Equations (PDEs) play a crucial role as tools for
modeling and comprehending intricate natural processes, notably within the
domain of biology. This research explores the domain of microbial activity
within the complex matrix of 3D soil structures, providing valuable
understanding into both the existence and uniqueness of solutions and the
asymptotic behavior of the corresponding PDE model. Our investigation results
in the discovery of a global attractor, a fundamental feature with significant
implications for long-term system behavior. To enhance the clarity of our
findings, numerical simulations are employed to visually illustrate the
attributes of this global attractor.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02062" title="Abstract">arXiv:2310.02062</a> [<a href="/pdf/2310.02062" title="Download PDF">pdf</a>, <a href="/format/2310.02062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gotta Catch &#x27;em All: Aggregating CVSS Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Longueira-Romero%2C+A">Angel Longueira-Romero</a>, 
<a href="/search/cs?searchtype=author&query=Flores%2C+J+L">Jose Luis Flores</a>, 
<a href="/search/cs?searchtype=author&query=Iglesias%2C+R">Rosa Iglesias</a>, 
<a href="/search/cs?searchtype=author&query=Garitano%2C+I">I&#xf1;aki Garitano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, conference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Spanish Meeting on Cryptology and Information Security
  (RECSI), 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Security metrics are not standardized, but inter-national proposals such as
the Common Vulnerability ScoringSystem (CVSS) for quantifying the severity of
known vulnerabil-ities are widely used. Many CVSS aggregation mechanisms
havebeen proposed in the literature. Nevertheless, factors related tothe
context of the System Under Test (SUT) are not taken intoaccount in the
aggregation process; vulnerabilities that in theoryaffect the SUT, but are not
exploitable in reality. We propose aCVSS aggregation algorithm that integrates
information aboutthe functionality disruption of the SUT, exploitation
difficulty,existence of exploits, and the context where the SUT operates.The
aggregation algorithm was applied to OpenPLC V3, showingthat it is capable of
filtering out vulnerabilities that cannot beexploited in the real conditions of
deployment of the particularsystem. Finally, because of the nature of the
proposed algorithm,the result can be interpreted in the same way as a normal
CVSS.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02063" title="Abstract">arXiv:2310.02063</a> [<a href="/pdf/2310.02063" title="Download PDF">pdf</a>, <a href="/format/2310.02063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lessons Learned from EXMOS User Studies: A Technical Report Summarizing  Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Aditya Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Stumpf%2C+S">Simone Stumpf</a>, 
<a href="/search/cs?searchtype=author&query=Gosak%2C+L">Lucija Gosak</a>, 
<a href="/search/cs?searchtype=author&query=Stiglic%2C+G">Gregor Stiglic</a>, 
<a href="/search/cs?searchtype=author&query=Verbert%2C+K">Katrien Verbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It is a technical report only. The contents are not peer-reviewed. Please reach out to the main author for any questions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In the realm of interactive machine-learning systems, the provision of
explanations serves as a vital aid in the processes of debugging and enhancing
prediction models. However, the extent to which various global model-centric
and data-centric explanations can effectively assist domain experts in
detecting and resolving potential data-related issues for the purpose of model
improvement has remained largely unexplored. In this technical report, we
summarise the key findings of our two user studies. Our research involved a
comprehensive examination of the impact of global explanations rooted in both
data-centric and model-centric perspectives within systems designed to support
healthcare experts in optimising machine learning models through both automated
and manual data configurations. To empirically investigate these dynamics, we
conducted two user studies, comprising quantitative analysis involving a sample
size of 70 healthcare experts and qualitative assessments involving 30
healthcare experts. These studies were aimed at illuminating the influence of
different explanation types on three key dimensions: trust, understandability,
and model improvement. Results show that global model-centric explanations
alone are insufficient for effectively guiding users during the intricate
process of data configuration. In contrast, data-centric explanations exhibited
their potential by enhancing the understanding of system changes that occur
post-configuration. However, a combination of both showed the highest level of
efficacy for fostering trust, improving understandability, and facilitating
model enhancement among healthcare experts. We also present essential
implications for developing interactive machine-learning systems driven by
explanations. These insights can guide the creation of more effective systems
that empower domain experts to harness the full potential of machine learning
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02064" title="Abstract">arXiv:2310.02064</a> [<a href="/pdf/2310.02064" title="Download PDF">pdf</a>, <a href="/ps/2310.02064" title="Download PostScript">ps</a>, <a href="/format/2310.02064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auction Design for Bidders with Ex Post ROI Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+H">Hongtao Lv</a>, 
<a href="/search/cs?searchtype=author&query=Bei%2C+X">Xiaohui Bei</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhenzhe Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WINE2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Motivated by practical constraints in online advertising, we investigate
single-parameter auction design for bidders with constraints on their Return On
Investment (ROI) -- a targeted minimum ratio between the obtained value and the
payment. We focus on ex post ROI constraints, which require the ROI condition
to be satisfied for every realized value profile. With ROI-constrained bidders,
we first provide a full characterization of the allocation and payment rules of
dominant-strategy incentive compatible (DSIC) auctions. In particular, we show
that given any monotone allocation rule, the corresponding DSIC payment should
be the Myerson payment with a rebate for each bidder to meet their ROI
constraints. Furthermore, we also determine the optimal auction structure when
the item is sold to a single bidder under a mild regularity condition. This
structure entails a randomized allocation scheme and a first-price payment
rule, which differs from the deterministic Myerson auction and previous works
on ex ante ROI constraints.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02065" title="Abstract">arXiv:2310.02065</a> [<a href="/pdf/2310.02065" title="Download PDF">pdf</a>, <a href="/format/2310.02065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor  Cores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castro%2C+R+L">Roberto L. Castro</a>, 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+A">Andrei Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Andrade%2C+D">Diego Andrade</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Nun%2C+T">Tal Ben-Nun</a>, 
<a href="/search/cs?searchtype=author&query=Fraguela%2C+B+B">Basilio B. Fraguela</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2023 International Conference on High Performance Computing, Networking, Storage and Analysis, 2023 (SC'23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The increasing success and scaling of Deep Learning models demands higher
computational efficiency and power. Sparsification can lead to both smaller
models as well as higher compute efficiency, and accelerated hardware is
becoming available. However, exploiting it efficiently requires kernel
implementations, pruning algorithms, and storage formats, to utilize hardware
support of specialized sparse vector units. An example of those are the
NVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However,
SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%.
We present the V:N:M format, which enables the execution of arbitrary N:M
ratios on SPTCs. To efficiently exploit the resulting format, we propose
Spatha, a high-performance sparse-library for DL routines. We show that Spatha
achieves up to 37x speedup over cuBLAS. We also demonstrate a second-order
pruning technique that enables sparsification to high sparsity ratios with
V:N:M and little to no loss in accuracy in modern transformers.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02066" title="Abstract">arXiv:2310.02066</a> [<a href="/pdf/2310.02066" title="Download PDF">pdf</a>, <a href="/format/2310.02066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De Novo Drug Design with Joint Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Izdebski%2C+A">Adam Izdebski</a>, 
<a href="/search/cs?searchtype=author&query=Weglarz-Tomczak%2C+E">Ewelina Weglarz-Tomczak</a>, 
<a href="/search/cs?searchtype=author&query=Szczurek%2C+E">Ewa Szczurek</a>, 
<a href="/search/cs?searchtype=author&query=Tomczak%2C+J+M">Jakub M. Tomczak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">De novo drug design requires simultaneously generating novel molecules
outside of training data and predicting their target properties, making it a
hard task for generative models. To address this, we propose Joint Transformer
that combines a Transformer decoder, a Transformer encoder, and a predictor in
a joint generative model with shared weights. We show that training the model
with a penalized log-likelihood objective results in state-of-the-art
performance in molecule generation, while decreasing the prediction error on
newly sampled molecules, as compared to a fine-tuned decoder-only Transformer,
by 42%. Finally, we propose a probabilistic black-box optimization algorithm
that employs Joint Transformer to generate novel molecules with improved target
properties, as compared to the training data, outperforming other SMILES-based
optimization methods in de novo drug design.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02067" title="Abstract">arXiv:2310.02067</a> [<a href="/pdf/2310.02067" title="Download PDF">pdf</a>, <a href="/format/2310.02067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content Bias in Deep Learning Age Approximation: A new Approach Towards  more Explainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=J%C3%B6chl%2C+R">Robert J&#xf6;chl</a>, 
<a href="/search/cs?searchtype=author&query=Uhl%2C+A">Andreas Uhl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a preprint, the paper is currently under consideration at Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the context of temporal image forensics, it is not evident that a neural
network, trained on images from different time-slots (classes), exploit solely
age related features. Usually, images taken in close temporal proximity (e.g.,
belonging to the same age class) share some common content properties. Such
content bias can be exploited by a neural network. In this work, a novel
approach that evaluates the influence of image content is proposed. This
approach is verified using synthetic images (where content bias can be ruled
out) with an age signal embedded. Based on the proposed approach, it is shown
that a `standard' neural network trained in the context of age classification
is strongly dependent on image content. As a potential countermeasure, two
different techniques are applied to mitigate the influence of the image content
during training, and they are also evaluated by the proposed method.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02069" title="Abstract">arXiv:2310.02069</a> [<a href="/pdf/2310.02069" title="Download PDF">pdf</a>, <a href="/format/2310.02069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TOaCNN: Adaptive Convolutional Neural Network for Multidisciplinary  Topology Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chadha%2C+K+S">Khaish Singh Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Prabhat Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 6th NCMDAO 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">This paper presents an adaptive convolutional neural network (CNN)
architecture that can automate diverse topology optimization (TO) problems
having different underlying physics. The architecture uses the encoder-decoder
networks with dense layers in the middle which includes an additional adaptive
layer to capture complex geometrical features. The network is trained using the
dataset obtained from the three open-source TO codes involving different
physics. The robustness and success of the presented adaptive CNN are
demonstrated on compliance minimization problems with constant and
design-dependent loads and material bulk modulus optimization. The architecture
takes the user's input of the volume fraction. It instantly generates optimized
designs resembling their counterparts obtained via open-source TO codes with
negligible performance and volume fraction error.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02071" title="Abstract">arXiv:2310.02071</a> [<a href="/pdf/2310.02071" title="Download PDF">pdf</a>, <a href="/format/2310.02071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards End-to-End Embodied Decision Making via Multi-modal Large  Language Model: Explorations with GPT4-Vision and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shuhuai Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haozhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zefan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">In this study, we explore the potential of Multimodal Large Language Models
(MLLMs) in improving embodied decision-making processes for agents. While Large
Language Models (LLMs) have been widely used due to their advanced reasoning
skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual
understanding and reasoning capabilities. We investigate whether
state-of-the-art MLLMs can handle embodied decision-making in an end-to-end
manner and whether collaborations between LLMs and MLLMs can enhance
decision-making. To address these questions, we introduce a new benchmark
called PCA-EVAL, which evaluates embodied decision-making from the perspectives
of Perception, Cognition, and Action. Additionally, we propose HOLMES, a
multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs
to gather multimodal information for informed decision-making. We compare
end-to-end embodied decision-making and HOLMES on our benchmark and find that
the GPT4-Vision model demonstrates strong end-to-end embodied decision-making
abilities, outperforming GPT4-HOLMES in terms of average decision accuracy
(+3%). However, this performance is exclusive to the latest GPT4-Vision model,
surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate
that powerful MLLMs like GPT4-Vision hold promise for decision-making in
embodied agents, offering new avenues for MLLM research.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02072" title="Abstract">arXiv:2310.02072</a> [<a href="/pdf/2310.02072" title="Download PDF">pdf</a>, <a href="/format/2310.02072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Variable Eddington Factor Model for Thermal Radiative Transfer with  Closure based on Data-Driven Shape Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Coale%2C+J+M">Joseph M. Coale</a>, 
<a href="/search/math?searchtype=author&query=Anistratov%2C+D+Y">Dmitriy Y. Anistratov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A new variable Eddington factor (VEF) model is presented for nonlinear
problems of thermal radiative transfer (TRT). The VEF model is a data-driven
one that acts on known (a-priori) radiation-diffusion solutions for material
temperatures in the TRT problem. A linear auxiliary problem is constructed for
the radiative transfer equation (RTE) with opacities and emission source
evaluated at the known material temperatures. The solution to this RTE
approximates the specific intensity distribution for the problem in all
phase-space and time. It is applied as a shape function to define the Eddington
tensor for the presented VEF model. The shape function computed via the
auxiliary RTE problem will capture some degree of transport effects within the
TRT problem. The VEF moment equations closed with this approximate Eddington
tensor will thus carry with them these captured transport effects. In this
study, the temperature data comes from multigroup $P_1$, $P_{1/3}$, and
flux-limited diffusion radiative transfer (RT) models. The proposed VEF model
can be interpreted as a transport-corrected diffusion reduced-order model.
Numerical results are presented on the Fleck-Cummings test problem which models
a supersonic wavefront of radiation. The presented VEF model is shown to
reliably improve accuracy by 1-2 orders of magnitude compared to the considered
radiation-diffusion model solutions to the TRT problem.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02076" title="Abstract">arXiv:2310.02076</a> [<a href="/pdf/2310.02076" title="Download PDF">pdf</a>, <a href="/format/2310.02076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Does the Chart Say? Grouping Cues Guide Viewer Comparisons and  Conclusions in Bar Charts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bearfield%2C+C+X">Cindy Xiong Bearfield</a>, 
<a href="/search/cs?searchtype=author&query=Stokes%2C+C">Chase Stokes</a>, 
<a href="/search/cs?searchtype=author&query=Lovett%2C+A">Andrew Lovett</a>, 
<a href="/search/cs?searchtype=author&query=Franconeri%2C+S">Steven Franconeri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Reading a visualization is like reading a paragraph. Each sentence is a
comparison: the mean of these is higher than those; this difference is smaller
than that. What determines which comparisons are made first? The viewer's goals
and expertise matter, but the way that values are visually grouped together
within the chart also impacts those comparisons. Research from psychology
suggests that comparisons involve multiple steps. First, the viewer divides the
visualization into a set of units. This might include a single bar or a grouped
set of bars. Then the viewer selects and compares two of these units, perhaps
noting that one pair of bars is longer than another. Viewers might take an
additional third step and perform a second-order comparison, perhaps
determining that the difference between one pair of bars is greater than the
difference between another pair. We create a visual comparison taxonomy that
allows us to develop and test a sequence of hypotheses about which comparisons
people are more likely to make when reading a visualization. We find that
people tend to compare two groups before comparing two individual bars and that
second-order comparisons are rare. Visual cues like spatial proximity and color
can influence which elements are grouped together and selected for comparison,
with spatial proximity being a stronger grouping cue. Interestingly, once the
viewer grouped together and compared a set of bars, regardless of whether the
group is formed by spatial proximity or color similarity, they no longer
consider other possible groupings in their comparisons.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02083" title="Abstract">arXiv:2310.02083</a> [<a href="/pdf/2310.02083" title="Download PDF">pdf</a>, <a href="/format/2310.02083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point Neighborhood Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hermosilla%2C+P">Pedro Hermosilla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Point convolution operations rely on different embedding mechanisms to encode
the neighborhood information of each point in order to detect patterns in 3D
space. However, as convolutions are usually evaluated as a whole, not much work
has been done to investigate which is the ideal mechanism to encode such
neighborhood information. In this paper, we provide the first extensive study
that analyzes such Point Neighborhood Embeddings (PNE) alone in a controlled
experimental setup. From our experiments, we derive a set of recommendations
for PNE that can help to improve future designs of neural network architectures
for point clouds. Our most surprising finding shows that the most commonly used
embedding based on a Multi-layer Perceptron (MLP) with ReLU activation
functions provides the lowest performance among all embeddings, even being
surpassed on some tasks by a simple linear combination of the point
coordinates. Additionally, we show that a neural network architecture using
simple convolutions based on such embeddings is able to achieve
state-of-the-art results on several tasks, outperforming recent and more
complex operations. Lastly, we show that these findings extrapolate to other
more complex convolution operations, where we show how following our
recommendations we are able to improve recent state-of-the-art architectures.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02086" title="Abstract">arXiv:2310.02086</a> [<a href="/pdf/2310.02086" title="Download PDF">pdf</a>, <a href="/format/2310.02086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bearing-Based Target Entrapping Control of Multiple Uncertain Agents  With Arbitrary Maneuvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Su%2C+H">Haifan Su</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Z">Ziwen Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+S">Shanying Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C">Cailian Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures, the paper has been accepted by IFAC WC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper is concerned with bearing-based cooperative target entrapping
control of multiple uncertain agents with arbitrary maneuvers including shape
deformation, rotations, scalings, etc. A leader-follower structure is used,
where the leaders move with the predesigned trajectories, and the followers are
steered by an estimation-based control method, integrating a distance estimator
using bearing measurements and a stress matrix-based formation controller. The
signum functions are used to compensate for the uncertainties so that the
agents' accelerations can be piecewise continuous and bounded to track the
desired dynamics. With proper design of the leaders' trajectories and a
geometric configuration, an affine matrix is determined so that the
persistently exciting conditions of the inter-agent relative bearings can be
satisfied since the bearing rates are related to different weighted
combinations of the affine matrix vectors. The asymptotic convergence of the
estimation error and control error is proved using Filipov properties and
cascaded system theories. A sufficient condition for inter-agent collision
avoidance is also proposed. Finally, simulation results are given to validate
the effectiveness of the method in both 2D and 3D cases.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02090" title="Abstract">arXiv:2310.02090</a> [<a href="/pdf/2310.02090" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 1D-CapsNet-LSTM: A Deep Learning-Based Model for Multi-Step Stock Index  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sjarif%2C+N+N+A">Nilam Nur Amir Sjarif</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+R">Roslina Ibrahim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Multi-step forecasting of stock market index prices is a crucial task in the
financial sector, playing a pivotal role in decision-making across various
financial activities. However, forecasting results are often unsatisfactory
owing to the stochastic and volatile nature of the data. Researchers have made
various attempts, and this process is ongoing. Inspired by convolutional neural
network long short-term memory (CNN-LSTM) networks that utilize a 1D CNN for
feature extraction to boost model performance, this study explores the use of a
capsule network (CapsNet) as an advanced feature extractor in an LSTM-based
forecasting model to enhance multi-step predictions. To this end, a novel
neural architecture called 1D-CapsNet-LSTM was introduced, which combines a 1D
CapsNet to extract high-level features from 1D sequential data and an LSTM
layer to capture the temporal dependencies between the previously extracted
features and uses a multi-input multi-output (MIMO) strategy to maintain the
stochastic dependencies between the predicted values at different time steps.
The proposed model was evaluated based on several real-world stock market
indices, including Standard &amp; Poor's 500 (S&amp;P 500), Dow Jones Industrial
Average (DJIA), Nasdaq Composite Index (IXIC), and New York Stock Exchange
(NYSE), and was compared with baseline models such as LSTM, recurrent neural
network (RNN), and CNN-LSTM in terms of various evaluation metrics. The
comparison results suggest that the 1D-CapsNet-LSTM model outperforms the
baseline models and has immense potential for the effective handling of complex
prediction tasks.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02093" title="Abstract">arXiv:2310.02093</a> [<a href="/pdf/2310.02093" title="Download PDF">pdf</a>, <a href="/format/2310.02093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Gradient Descent with Preconditioned Polyak Step-size
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdukhakimov%2C+F">Farshed Abdukhakimov</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+C">Chulu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Kamzolov%2C+D">Dmitry Kamzolov</a>, 
<a href="/search/cs?searchtype=author&query=Tak%C3%A1%C4%8D%2C+M">Martin Tak&#xe1;&#x10d;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Stochastic Gradient Descent (SGD) is one of the many iterative optimization
methods that are widely used in solving machine learning problems. These
methods display valuable properties and attract researchers and industrial
machine learning engineers with their simplicity. However, one of the
weaknesses of this type of methods is the necessity to tune learning rate
(step-size) for every loss function and dataset combination to solve an
optimization problem and get an efficient performance in a given time budget.
Stochastic Gradient Descent with Polyak Step-size (SPS) is a method that offers
an update rule that alleviates the need of fine-tuning the learning rate of an
optimizer. In this paper, we propose an extension of SPS that employs
preconditioning techniques, such as Hutchinson's method, Adam, and AdaGrad, to
improve its performance on badly scaled and/or ill-conditioned datasets.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02094" title="Abstract">arXiv:2310.02094</a> [<a href="/pdf/2310.02094" title="Download PDF">pdf</a>, <a href="/format/2310.02094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoNO: Complex Neural Operator for Continuous Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+K">Karn Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+N+M+A">N M Anoop Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=P%2C+P+A">Prathosh A P</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural operators extend data-driven models to map between
infinite-dimensional functional spaces. These models have successfully solved
continuous dynamical systems represented by differential equations, viz weather
forecasting, fluid flow, or solid mechanics. However, the existing operators
still rely on real space, thereby losing rich representations potentially
captured in the complex space by functional transforms. In this paper, we
introduce a Complex Neural Operator (CoNO), that parameterizes the integral
kernel in the complex fractional Fourier domain. Additionally, the model
employing a complex-valued neural network along with aliasing-free activation
functions preserves the complex values and complex algebraic properties,
thereby enabling improved representation, robustness to noise, and
generalization. We show that the model effectively captures the underlying
partial differential equation with a single complex fractional Fourier
transform. We perform an extensive empirical evaluation of CoNO on several
datasets and additional tasks such as zero-shot super-resolution, evaluation of
out-of-distribution data, data efficiency, and robustness to noise. CoNO
exhibits comparable or superior performance to all the state-of-the-art models
in these tasks. Altogether, CoNO presents a robust and superior model for
modeling continuous dynamical systems, providing a fillip to scientific machine
learning.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02095" title="Abstract">arXiv:2310.02095</a> [<a href="/pdf/2310.02095" title="Download PDF">pdf</a>, <a href="/format/2310.02095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on the Role of Crowds in Combating Online Misinformation:  Annotators, Evaluators, and Creators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bing He</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yibo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yeon-Chang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Soyoung Oh</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+G">Gaurav Verma</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Srijan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GitHub repository with the curated list of papers: <a href="https://github.com/claws-lab/awesome-crowd-combat-misinformation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Online misinformation poses a global risk with significant real-world
consequences. To combat misinformation, current research relies on
professionals like journalists and fact-checkers for annotating and debunking
misinformation, and develops automated machine learning methods for detecting
misinformation. Complementary to these approaches, recent research has
increasingly concentrated on utilizing the power of ordinary social media
users, a.k.a. "crowd", who act as eyes-on-the-ground proactively questioning
and countering misinformation. Notably, recent studies show that 96% of
counter-misinformation responses originate from them. Acknowledging their
prominent role, we present the first systematic and comprehensive survey of
research papers that actively leverage the crowds to combat misinformation.
<br />We first identify 88 papers related to crowd-based efforts, following a
meticulous annotation process adhering to the PRISMA framework. We then present
key statistics related to misinformation, counter-misinformation, and crowd
input in different formats and topics. Upon holistic analysis of the papers, we
introduce a novel taxonomy of the roles played by the crowds: (i)annotators who
actively identify misinformation; (ii)evaluators who assess
counter-misinformation effectiveness; (iii)creators who create
counter-misinformation. This taxonomy explores the crowd's capabilities in
misinformation detection, identifies prerequisites for effective
counter-misinformation, and analyzes crowd-generated counter-misinformation.
Then, we delve into (i)distinguishing individual, collaborative, and
machine-assisted labeling for annotators; (ii)analyzing the effectiveness of
counter-misinformation through surveys, interviews, and in-lab experiments for
evaluators; and (iii)characterizing creation patterns and creator profiles for
creators. Finally, we outline potential future research in this field.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02097" title="Abstract">arXiv:2310.02097</a> [<a href="/pdf/2310.02097" title="Download PDF">pdf</a>, <a href="/format/2310.02097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Classic Deconvolution and Feature Extraction in Zero-Shot  Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chobola%2C+T">Tom&#xe1;&#x161; Chobola</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+G">Gesine M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Dausmann%2C+V">Veit Dausmann</a>, 
<a href="/search/cs?searchtype=author&query=Theileis%2C+A">Anton Theileis</a>, 
<a href="/search/cs?searchtype=author&query=Taucher%2C+J">Jan Taucher</a>, 
<a href="/search/cs?searchtype=author&query=Huisken%2C+J">Jan Huisken</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+T">Tingying Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Non-blind deconvolution aims to restore a sharp image from its blurred
counterpart given an obtained kernel. Existing deep neural architectures are
often built based on large datasets of sharp ground truth images and trained
with supervision. Sharp, high quality ground truth images, however, are not
always available, especially for biomedical applications. This severely hampers
the applicability of current approaches in practice. In this paper, we propose
a novel non-blind deconvolution method that leverages the power of deep
learning and classic iterative deconvolution algorithms. Our approach combines
a pre-trained network to extract deep features from the input image with
iterative Richardson-Lucy deconvolution steps. Subsequently, a zero-shot
optimisation process is employed to integrate the deconvolved features,
resulting in a high-quality reconstructed image. By performing the preliminary
reconstruction with the classic iterative deconvolution method, we can
effectively utilise a smaller network to produce the final image, thus
accelerating the reconstruction whilst reducing the demand for valuable
computational resources. Our method demonstrates significant improvements in
various real-world applications non-blind deconvolution tasks.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02102" title="Abstract">arXiv:2310.02102</a> [<a href="/pdf/2310.02102" title="Download PDF">pdf</a>, <a href="/format/2310.02102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> dFlow: A Domain Specific Language for the Rapid Development of  open-source Virtual Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malamas%2C+N">Nikolaos Malamas</a>, 
<a href="/search/cs?searchtype=author&query=Panayiotou%2C+K">Konstantinos Panayiotou</a>, 
<a href="/search/cs?searchtype=author&query=Symeonidis%2C+A+L">Andreas L. Symeonidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">An increasing number of models and frameworks for Virtual Assistant (VA)
development exist nowadays, following the progress in the Natural Language
Processing (NLP) and Natural Language Understanding (NLU) fields. Regardless of
their performance, popularity, and ease of use, these frameworks require at
least basic expertise in NLP and software engineering, even for simple and
repetitive processes, limiting their use only to the domain and programming
experts. However, since the current state of practice of VA development is a
straightforward process, Model-Driven Engineering approaches can be utilized to
achieve automation and rapid development in a more convenient manner. To this
end, we present \textit{dFlow}, a textual Domain-Specific Language (DSL) that
offers a simplified, reusable, and framework-agnostic language for creating
task-specific VAs in a low-code manner. We describe a system-agnostic VA
meta-model, the developed grammar, and all essential processes for developing
and deploying smart VAs. For further convenience, we create a cloud-native
architecture and expose it through the Discord platform. We conducted a
large-scale empirical evaluation with more than 200 junior software developers
and collected positive feedback, indicating that dFlow can accelerate the
entire VA development process, while also enabling citizen and software
developers with minimum experience to participate.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02104" title="Abstract">arXiv:2310.02104</a> [<a href="/pdf/2310.02104" title="Download PDF">pdf</a>, <a href="/format/2310.02104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An empirical study of ChatGPT-3.5 on question answering and code  maintenance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+M+A">Md Mahir Asef Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+S+A">Sk Adnan Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ying Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+N">Na Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Ever since the launch of ChatGPT in 2022, a rising concern is whether ChatGPT
will replace programmers and kill jobs. Motivated by this widespread concern,
we conducted an empirical study to systematically compare ChatGPT against
programmers in question-answering and software-maintaining. We reused a dataset
introduced by prior work, which includes 130 StackOverflow (SO) discussion
threads referred to by the Java developers of 357 GitHub projects. We mainly
investigated three research questions (RQs). First, how does ChatGPT compare
with programmers when answering technical questions? Second, how do developers
perceive the differences between ChatGPT's answers and SO answers? Third, how
does ChatGPT compare with humans when revising code for maintenance requests?
<br />For RQ1, we provided the 130 SO questions to ChatGPT, and manually compared
ChatGPT answers with the accepted/most popular SO answers in terms of
relevance, readability, informativeness, comprehensiveness, and reusability.
For RQ2, we conducted a user study with 30 developers, asking each developer to
assess and compare 10 pairs of answers, without knowing the information source
(i.e., ChatGPT or SO). For RQ3, we distilled 48 software maintenance tasks from
48 GitHub projects citing the studied SO threads. We queried ChatGPT to revise
a given Java file, and to incorporate the code implementation for any
prescribed maintenance requirement. Our study reveals interesting phenomena:
For the majority of SO questions (97/130), ChatGPT provided better answers; in
203 of 300 ratings, developers preferred ChatGPT answers to SO answers; ChatGPT
revised code correctly for 22 of the 48 tasks. Our research will expand
people's knowledge of ChatGPT capabilities, and shed light on future adoption
of ChatGPT by the software industry.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02107" title="Abstract">arXiv:2310.02107</a> [<a href="/pdf/2310.02107" title="Download PDF">pdf</a>, <a href="/format/2310.02107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instance Needs More Care: Rewriting Prompts for Instances Yields Better  Zero-Shot Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Saurabh Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chengyue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Weiguo Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Ziyu Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Enabling large language models (LLMs) to perform tasks in zero-shot has been
an appealing goal owing to its labor-saving (i.e., requiring no task-specific
annotations); as such, zero-shot prompting approaches also enjoy better task
generalizability. To improve LLMs' zero-shot performance, prior work has
focused on devising more effective task instructions (e.g., ``let's think step
by step'' ). However, we argue that, in order for an LLM to solve them
correctly in zero-shot, individual test instances need more carefully designed
and customized instructions. To this end, we propose PRoMPTd, an approach that
rewrites the task prompt for each individual test input to be more specific,
unambiguous, and complete, so as to provide better guidance to the task LLM. We
evaluated PRoMPTd on eight datasets covering tasks including arithmetics,
logical reasoning, and code generation, using GPT-4 as the task LLM. Notably,
\algoname achieves an absolute improvement of around 10\% on the complex MATH
dataset and 5\% on the code generation task on HumanEval, outperforming
conventional zero-shot methods. In addition, we also showed that the rewritten
prompt can provide better interpretability of how the LLM resolves each test
instance, which can potentially be leveraged as a defense mechanism against
adversarial prompting. The source code and dataset can be obtained from
https://github.com/salokr/PRoMPTd
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02108" title="Abstract">arXiv:2310.02108</a> [<a href="/pdf/2310.02108" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Effective Human-AI Decision-Making: The Role of Human Learning  in Appropriate Reliance on AI Advice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schemmer%2C+M">Max Schemmer</a>, 
<a href="/search/cs?searchtype=author&query=Bartos%2C+A">Andrea Bartos</a>, 
<a href="/search/cs?searchtype=author&query=Spitzer%2C+P">Philipp Spitzer</a>, 
<a href="/search/cs?searchtype=author&query=Hemmer%2C+P">Patrick Hemmer</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>, 
<a href="/search/cs?searchtype=author&query=Liebschner%2C+J">Jonas Liebschner</a>, 
<a href="/search/cs?searchtype=author&query=Satzger%2C+G">Gerhard Satzger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Information Systems (ICIS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The true potential of human-AI collaboration lies in exploiting the
complementary capabilities of humans and AI to achieve a joint performance
superior to that of the individual AI or human, i.e., to achieve complementary
team performance (CTP). To realize this complementarity potential, humans need
to exercise discretion in following AI 's advice, i.e., appropriately relying
on the AI's advice. While previous work has focused on building a mental model
of the AI to assess AI recommendations, recent research has shown that the
mental model alone cannot explain appropriate reliance. We hypothesize that, in
addition to the mental model, human learning is a key mediator of appropriate
reliance and, thus, CTP. In this study, we demonstrate the relationship between
learning and appropriate reliance in an experiment with 100 participants. This
work provides fundamental concepts for analyzing reliance and derives
implications for the effective design of human-AI decision-making.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02110" title="Abstract">arXiv:2310.02110</a> [<a href="/pdf/2310.02110" title="Download PDF">pdf</a>, <a href="/format/2310.02110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIEVE: Multimodal Dataset Pruning Using Image Captioning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahmoud%2C+A">Anas Mahmoud</a>, 
<a href="/search/cs?searchtype=author&query=Elhoushi%2C+M">Mostafa Elhoushi</a>, 
<a href="/search/cs?searchtype=author&query=Abbas%2C+A">Amro Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ardalani%2C+N">Newsha Ardalani</a>, 
<a href="/search/cs?searchtype=author&query=Leather%2C+H">Hugh Leather</a>, 
<a href="/search/cs?searchtype=author&query=Morcos%2C+A">Ari Morcos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy
web-crawled datasets. This underscores the critical need for dataset pruning,
as the quality of these datasets is strongly correlated with the performance of
VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train
models using highly-aligned samples is one of the most successful methods for
pruning.We argue that this approach suffers from multiple limitations
including: 1) false positives due to spurious correlations captured by the
pretrained CLIP model, 2) false negatives due to poor discrimination between
hard and bad samples, and 3) biased ranking towards samples similar to the
pretrained CLIP dataset. We propose a pruning method, SIEVE, that employs
synthetic captions generated by image-captioning models pretrained on small,
diverse, and well-aligned image-text pairs to evaluate the alignment of noisy
image-text pairs. To bridge the gap between the limited diversity of generated
captions and the high diversity of alternative text (alt-text), we estimate the
semantic textual similarity in the embedding space of a language model
pretrained on billions of sentences. Using DataComp, a multimodal dataset
filtering benchmark, we achieve state-of-the-art performance on the large scale
pool, and competitive results on the medium scale pool, surpassing
CLIPScore-based filtering by 1.7% and 2.6% on average, on 38 downstream tasks.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02113" title="Abstract">arXiv:2310.02113</a> [<a href="/pdf/2310.02113" title="Download PDF">pdf</a>, <a href="/format/2310.02113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLEDGE: Ledger-based Federated Learning Resilient to Inference and  Backdoor Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castillo%2C+J">Jorge Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Rieger%2C+P">Phillip Rieger</a>, 
<a href="/search/cs?searchtype=author&query=Fereidooni%2C+H">Hossein Fereidooni</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sadeghi%2C+A">Ahmad Sadeghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Annual Computer Security Applications Conference (ACSAC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated learning (FL) is a distributed learning process that uses a trusted
aggregation server to allow multiple parties (or clients) to collaboratively
train a machine learning model without having them share their private data.
Recent research, however, has demonstrated the effectiveness of inference and
poisoning attacks on FL. Mitigating both attacks simultaneously is very
challenging. State-of-the-art solutions have proposed the use of poisoning
defenses with Secure Multi-Party Computation (SMPC) and/or Differential Privacy
(DP). However, these techniques are not efficient and fail to address the
malicious intent behind the attacks, i.e., adversaries (curious servers and/or
compromised clients) seek to exploit a system for monetization purposes. To
overcome these limitations, we present a ledger-based FL framework known as
FLEDGE that allows making parties accountable for their behavior and achieve
reasonable efficiency for mitigating inference and poisoning attacks. Our
solution leverages crypto-currency to increase party accountability by
penalizing malicious behavior and rewarding benign conduct. We conduct an
extensive evaluation on four public datasets: Reddit, MNIST, Fashion-MNIST, and
CIFAR-10. Our experimental results demonstrate that (1) FLEDGE provides strong
privacy guarantees for model updates without sacrificing model utility; (2)
FLEDGE can successfully mitigate different poisoning attacks without degrading
the performance of the global model; and (3) FLEDGE offers unique reward
mechanisms to promote benign behavior during model training and/or model
aggregation.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02116" title="Abstract">arXiv:2310.02116</a> [<a href="/pdf/2310.02116" title="Download PDF">pdf</a>, <a href="/format/2310.02116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Concept Discovery Models: A Concept Pyramid Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panousis%2C+K+P">Konstantinos P. Panousis</a>, 
<a href="/search/cs?searchtype=author&query=Ienco%2C+D">Dino Ienco</a>, 
<a href="/search/cs?searchtype=author&query=Marcos%2C+D">Diego Marcos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep Learning algorithms have recently gained significant attention due to
their impressive performance. However, their high complexity and
un-interpretable mode of operation hinders their confident deployment in
real-world safety-critical tasks. This work targets ante hoc interpretability,
and specifically Concept Bottleneck Models (CBMs). Our goal is to design a
framework that admits a highly interpretable decision making process with
respect to human understandable concepts, on multiple levels of granularity. To
this end, we propose a novel hierarchical concept discovery formulation
leveraging: (i) recent advances in image-text models, and (ii) an innovative
formulation for multi-level concept selection via data-driven and sparsity
inducing Bayesian arguments. Within this framework, concept information does
not solely rely on the similarity between the whole image and general
unstructured concepts; instead, we introduce the notion of concept hierarchy to
uncover and exploit more granular concept information residing in
patch-specific regions of the image scene. As we experimentally show, the
proposed construction not only outperforms recent CBM approaches, but also
yields a principled framework towards interpetability.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02117" title="Abstract">arXiv:2310.02117</a> [<a href="/pdf/2310.02117" title="Download PDF">pdf</a>, <a href="/format/2310.02117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric Single Index Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zweig%2C+A">Aaron Zweig</a>, 
<a href="/search/cs?searchtype=author&query=Bruna%2C+J">Joan Bruna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Few neural architectures lend themselves to provable learning with gradient
based methods. One popular model is the single-index model, in which labels are
produced by composing an unknown linear projection with a possibly unknown
scalar link function. Learning this model with SGD is relatively
well-understood, whereby the so-called information exponent of the link
function governs a polynomial sample complexity rate. However, extending this
analysis to deeper or more complicated architectures remains challenging.
<br />In this work, we consider single index learning in the setting of symmetric
neural networks. Under analytic assumptions on the activation and maximum
degree assumptions on the link function, we prove that gradient flow recovers
the hidden planted direction, represented as a finitely supported vector in the
feature space of power sum polynomials. We characterize a notion of information
exponent adapted to our setting that controls the efficiency of learning.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02118" title="Abstract">arXiv:2310.02118</a> [<a href="/pdf/2310.02118" title="Download PDF">pdf</a>, <a href="/format/2310.02118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TWIZ: The Wizard of Multimodal Conversational-Stimulus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+R">Rafael Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Tavares%2C+D">Diogo Tavares</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+D">Diogo Silva</a>, 
<a href="/search/cs?searchtype=author&query=Val%C3%A9rio%2C+R">Rodrigo Val&#xe9;rio</a>, 
<a href="/search/cs?searchtype=author&query=Bordalo%2C+J">Jo&#xe3;o Bordalo</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B5es%2C+I">In&#xea;s Sim&#xf5;es</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+V">Vasco Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Semedo%2C+D">David Semedo</a>, 
<a href="/search/cs?searchtype=author&query=Magalh%C3%A3es%2C+J">Jo&#xe3;o Magalh&#xe3;es</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this report, we describe the vision, challenges, and scientific
contributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot
Challenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal,
knowledgeable, and engaging assistant that can guide users towards the
successful completion of complex manual tasks. To achieve this, we focus our
efforts on three main research questions: (1) Humanly-Shaped Conversations, by
providing information in a knowledgeable way; (2) Multimodal Stimulus, making
use of various modalities including voice, images, and videos; and (3)
Zero-shot Conversational Flows, to improve the robustness of the interaction to
unseen scenarios. TWIZ is an assistant capable of supporting a wide range of
tasks, with several innovative features such as creative cooking, video
navigation through voice, and the robust TWIZ-LLM, a Large Language Model
trained for dialoguing about complex manual tasks. Given ratings and feedback
provided by users, we observed that TWIZ bot is an effective and robust system,
capable of guiding users through tasks while providing several multimodal
stimuli.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02120" title="Abstract">arXiv:2310.02120</a> [<a href="/pdf/2310.02120" title="Download PDF">pdf</a>, <a href="/format/2310.02120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HPCClusterScape: Increasing Transparency and Efficiency of Shared  High-Performance Computing Clusters for Large-scale AI Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Heungseok Park</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+A">Aeree Cho</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H">Hyojun Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hayoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Youngil Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sungjae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Heungsub Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The emergence of large-scale AI models, like GPT-4, has significantly
impacted academia and industry, driving the demand for high-performance
computing (HPC) to accelerate workloads. To address this, we present
HPCClusterScape, a visualization system that enhances the efficiency and
transparency of shared HPC clusters for large-scale AI models. HPCClusterScape
provides a comprehensive overview of system-level (e.g., partitions, hosts, and
workload status) and application-level (e.g., identification of experiments and
researchers) information, allowing HPC operators and machine learning
researchers to monitor resource utilization and identify issues through
customizable violation rules. The system includes diagnostic tools to
investigate workload imbalances and synchronization bottlenecks in large-scale
distributed deep learning experiments. Deployed in industrial-scale HPC
clusters, HPCClusterScape incorporates user feedback and meets specific
requirements. This paper outlines the challenges and prerequisites for
efficient HPC operation, introduces the interactive visualization system, and
highlights its contributions in addressing pain points and optimizing resource
utilization in shared HPC clusters.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02121" title="Abstract">arXiv:2310.02121</a> [<a href="/pdf/2310.02121" title="Download PDF">pdf</a>, <a href="/format/2310.02121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast algorithm for centralized multi-agent maze exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crnkovi%C4%87%2C+B">Bojan Crnkovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Ivi%C4%87%2C+S">Stefan Ivi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Zovko%2C+M">Mila Zovko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC)

</div>
<p class="mathjax">Recent advancements in robotics have paved the way for robots to replace
humans in perilous situations, such as searching for victims in blazing
buildings, earthquake-damaged structures, uncharted caves, traversing
minefields, or patrolling crime-ridden streets. These challenges can be
generalized as problems where agents need to explore unknown mazes. Although
various algorithms for single-agent maze exploration exist, extending them to
multi-agent systems poses complexities.
<br />We propose a solution: a cooperative multi-agent system of automated mobile
agents for exploring unknown mazes and locating stationary targets. Our
algorithm employs a potential field governing maze exploration, integrating
cooperative agent behaviors like collision avoidance, coverage coordination,
and path planning.
<br />This approach builds upon the Heat Equation Driven Area Coverage (HEDAC)
method by Ivi\'c, Crnkovi\'c, and Mezi\'c. Unlike previous continuous domain
applications, we adapt HEDAC for discrete domains, specifically mazes divided
into nodes. Our algorithm is versatile, easily modified for anti-collision
requirements, and adaptable to expanding mazes and numerical meshes over time.
<br />Comparative evaluations against alternative maze-solving methods illustrate
our algorithm's superiority. The results highlight significant enhancements,
showcasing its applicability across diverse mazes. Numerical simulations affirm
its robustness, adaptability, scalability, and simplicity, enabling centralized
parallel computation in autonomous systems of basic agents/robots.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02123" title="Abstract">arXiv:2310.02123</a> [<a href="/pdf/2310.02123" title="Download PDF">pdf</a>, <a href="/format/2310.02123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A benchmark proposal for contactless rebounds of elastic solids in  fluids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fara%2C+J">J. Fara</a>, 
<a href="/search/math?searchtype=author&query=Schwarzacher%2C+S">S. Schwarzacher</a>, 
<a href="/search/math?searchtype=author&query=T%C5%AFma%2C+K">K. T&#x16f;ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">The paper deals with the rebound of an elastic solid off a rigid wall of a
container filled with an incompressible Newtonian fluid. Accordingly, a new
fluid-structure interaction benchmark is introduced. We show that the benchmark
captures a rebound without the solid touching the wall, hence omitting any
artificial bouncing law. An adaptive numerical scheme that reconstructs the
rebound for very small viscosities is introduced. As the viscosity decreases,
the solution converges to the free rebound in a vacuum. The scheme is based on
a Glowinski time scheme and a localized arbitrary Lagrangian-Eulerian map on
finite elements in space. The absence of topological contact requires that very
thin liquid channels are solved with sufficient accuracy. This is done by newly
developed geometrically driven adaptive strategies. A rebound is simulated in
the absence of topological contacts. The benchmark is further tested by a
here-introduced adaptive purely Eulerian level-set method, which produces the
same dynamics but with a much higher computational cost. The experiments allow
for a better understanding of the effect of fluids on the dynamics of elastic
objects. Several observations are discussed, such as the amount of elastic
and/or kinetic energy loss or the precise connection between the fluid pressure
and the rebound of the solid.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02124" title="Abstract">arXiv:2310.02124</a> [<a href="/pdf/2310.02124" title="Download PDF">pdf</a>, <a href="/format/2310.02124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology  View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jintian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Shumin Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">As Natural Language Processing (NLP) systems are increasingly employed in
intricate social environments, a pressing query emerges: Can these NLP systems
mirror human-esque collaborative intelligence, in a multi-agent society
consisting of multiple large language models (LLMs)? This paper probes the
collaboration mechanisms among contemporary NLP systems by melding practical
experiments with theoretical insights. We fabricate four unique `societies'
comprised of LLM agents, where each agent is characterized by a specific
`trait' (easy-going or overconfident) and engages in collaboration with a
distinct `thinking pattern' (debate or reflection). Evaluating these
multi-agent societies on three benchmark datasets, we discern that LLM agents
navigate tasks by leveraging diverse social behaviors, from active debates to
introspective reflections. Notably, certain collaborative strategies only
optimize efficiency (using fewer API tokens), but also outshine previous
top-tier approaches. Moreover, our results further illustrate that LLM agents
manifest human-like social behaviors, such as conformity or majority rule,
mirroring foundational Social Psychology theories. In conclusion, we integrate
insights from Social Psychology to contextualize the collaboration of LLM
agents, inspiring further investigations into the collaboration mechanism for
LLMs. We commit to sharing our code and datasets (already submitted in
supplementary materials), hoping to catalyze further research in this promising
avenue (All code and data are available at
\url{https://github.com/zjunlp/MachineSoM}.).
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02128" title="Abstract">arXiv:2310.02128</a> [<a href="/pdf/2310.02128" title="Download PDF">pdf</a>, <a href="/format/2310.02128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Code Graph -- an information model to facilitate software  comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borowski%2C+K">Krzysztof Borowski</a>, 
<a href="/search/cs?searchtype=author&query=Bali%C5%9B%2C+B">Bartosz Bali&#x15b;</a>, 
<a href="/search/cs?searchtype=author&query=Orzechowski%2C+T">Tomasz Orzechowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software comprehension can be extremely time-consuming due to the
ever-growing size of codebases. Consequently, there is an increasing need to
accelerate the code comprehension process to facilitate maintenance and reduce
associated costs. A crucial aspect of this process is understanding and
preserving the high quality of the code dependency structure. While a variety
of code structure models already exist, there is a surprising lack of models
that closely represent the source code and focus on software comprehension. As
a result, there are no readily available and easy-to-use tools to assist with
dependency comprehension, refactoring, and quality monitoring of code. To
address this gap, we propose the Semantic Code Graph (SCG), an information
model that offers a detailed abstract representation of code dependencies with
a close relationship to the source code. To validate the SCG model's usefulness
in software comprehension, we compare it to nine other source code
representation models. Additionally, we select 11 well-known and widely-used
open-source projects developed in Java and Scala and perform a range of
software comprehension activities on them using three different code
representation models: the proposed SCG, the Call Graph (CG), and the Class
Collaboration Network (CCN). We then qualitatively analyze the results to
compare the performance of these models in terms of software comprehension
capabilities. These activities encompass project structure comprehension,
identifying critical project entities, interactive visualization of code
dependencies, and uncovering code similarities through software mining. Our
findings demonstrate that the SCG enhances software comprehension capabilities
compared to the prevailing CCN and CG models. We believe that the work
described is a step towards the next generation of tools that streamline code
dependency comprehension and management.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02129" title="Abstract">arXiv:2310.02129</a> [<a href="/pdf/2310.02129" title="Download PDF">pdf</a>, <a href="/format/2310.02129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Pitfalls of Knowledge Editing for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoubo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yunzhi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code will be released at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02130" title="Abstract">arXiv:2310.02130</a> [<a href="/pdf/2310.02130" title="Download PDF">pdf</a>, <a href="/format/2310.02130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Graphs of Bounded Treewidth to Minimize the Sum of  Radius-Dependent Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Drexler%2C+L">Lukas Drexler</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6ckendorff%2C+J">Jan H&#xf6;ckendorff</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nen%2C+J">Joshua K&#xf6;nen</a>, 
<a href="/search/cs?searchtype=author&query=Schewior%2C+K">Kevin Schewior</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We consider the following natural problem that generalizes min-sum-radii
clustering: Given is $k\in\mathbb{N}$ as well as some metric space $(V,d)$
where $V=F\cup C$ for facilities $F$ and clients $C$. The goal is to find a
clustering given by $k$ facility-radius pairs $(f_1,r_1),\dots,(f_k,r_k)\in
F\times\mathbb{R}_{\geq 0}$ such that $C\subseteq B(f_1,r_1)\cup\dots\cup
B(f_k,r_k)$ and $\sum_{i=1,\dots,k} g(r_i)$ is minimized for some increasing
function $g:\mathbb{R}_{\geq 0}\rightarrow\mathbb{R}_{\geq 0}$. Here, $B(x,r)$
is the radius-$r$ ball centered at $x$. For the case that $(V,d)$ is the
shortest-path metric of some edge-weighted graph of bounded treewidth, we
present a dynamic program that is tailored to this class of problems and
achieves a polynomial running time, establishing that the problem is in
$\mathsf{XP}$ with parameter treewidth.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02133" title="Abstract">arXiv:2310.02133</a> [<a href="/pdf/2310.02133" title="Download PDF">pdf</a>, <a href="/format/2310.02133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Reliable Logical Rules with SATNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaoyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinpei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+X">Xujie Si</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bridging logical reasoning and deep learning is crucial for advanced AI
systems. In this work, we present a new framework that addresses this goal by
generating interpretable and verifiable logical rules through differentiable
learning, without relying on pre-specified logical structures. Our approach
builds upon SATNet, a differentiable MaxSAT solver that learns the underlying
rules from input-output examples. Despite its efficacy, the learned weights in
SATNet are not straightforwardly interpretable, failing to produce
human-readable rules. To address this, we propose a novel specification method
called "maximum equality", which enables the interchangeability between the
learned weights of SATNet and a set of propositional logical rules in weighted
MaxSAT form. With the decoded weighted MaxSAT formula, we further introduce
several effective verification techniques to validate it against the ground
truth rules. Experiments on stream transformations and Sudoku problems show
that our decoded rules are highly reliable: using exact solvers on them could
achieve 100% accuracy, whereas the original SATNet fails to give correct
solutions in many cases. Furthermore, we formally verify that our decoded
logical rules are functionally equivalent to the ground truth ones.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02134" title="Abstract">arXiv:2310.02134</a> [<a href="/pdf/2310.02134" title="Download PDF">pdf</a>, <a href="/format/2310.02134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A discrete approximation scheme for fully nonlinear partial  integro-differential equations with the convergence rate of robust  $&#x3b1;$-stable central limit theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jiang%2C+L">Lianzi Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, we develop a numerical method to study the error estimates of
the $\alpha$-stable central limit theorem under sublinear expectation with
$\alpha \in(0,2)$, whose limit distribution can be characterized by a fully
nonlinear integro-differential equation (PIDE). Based on the sequence of
independent random variables, we propose a discrete approximation scheme for
the fully nonlinear PIDE. With the help of the nonlinear stochastic analysis
techniques and numerical analysis tools, we establish the error bounds for the
discrete approximation scheme, which in turn provides a general error bound for
the robust $\alpha$-stable central limit theorem, including the integrable case
$\alpha \in(1,2)$ as well as the non-integrable case $\alpha \in(0,1]$.
Finally, we provide some concrete examples to illustrate our main results and
derive the precise convergence rates.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02138" title="Abstract">arXiv:2310.02138</a> [<a href="/pdf/2310.02138" title="Download PDF">pdf</a>, <a href="/format/2310.02138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete anisotropic curve shortening flow in higher codimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Deckelnick%2C+K">Klaus Deckelnick</a>, 
<a href="/search/math?searchtype=author&query=N%C3%BCrnberg%2C+R">Robert N&#xfc;rnberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We introduce a novel formulation for the evolution of parametric curves by
anisotropic curve shortening flow in ${\mathbb R}^d$, $d\geq2$. The
reformulation hinges on a suitable manipulation of the parameterization's
tangential velocity, leading to a strictly parabolic differential equation.
Moreover, the derived equation is in divergence form, giving rise to a natural
variational numerical method. For a fully discrete finite element approximation
based on piecewise linear elements we prove optimal error estimates. Numerical
simulations confirm the theoretical results and demonstrate the practicality of
the method.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02140" title="Abstract">arXiv:2310.02140</a> [<a href="/pdf/2310.02140" title="Download PDF">pdf</a>, <a href="/format/2310.02140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAD-Phys: Exploiting Physiology for Presentation Attack Detection in  Face Biometrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomez%2C+L+F">Luis F. Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Fierrez%2C+J">Julian Fierrez</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+A">Aythami Morales</a>, 
<a href="/search/cs?searchtype=author&query=Ghafourian%2C+M">Mahdi Ghafourian</a>, 
<a href="/search/cs?searchtype=author&query=Tolosana%2C+R">Ruben Tolosana</a>, 
<a href="/search/cs?searchtype=author&query=Solano%2C+I">Imanol Solano</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+A">Alejandro Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Zamora-Martinez%2C+F">Francisco Zamora-Martinez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of the paper presented to the Workshop on IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC, 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Presentation Attack Detection (PAD) is a crucial stage in facial recognition
systems to avoid leakage of personal information or spoofing of identity to
entities. Recently, pulse detection based on remote photoplethysmography (rPPG)
has been shown to be effective in face presentation attack detection.
<br />This work presents three different approaches to the presentation attack
detection based on rPPG: (i) The physiological domain, a domain using
rPPG-based models, (ii) the Deepfakes domain, a domain where models were
retrained from the physiological domain to specific Deepfakes detection tasks;
and (iii) a new Presentation Attack domain was trained by applying transfer
learning from the two previous domains to improve the capability to
differentiate between bona-fides and attacks.
<br />The results show the efficiency of the rPPG-based models for presentation
attack detection, evidencing a 21.70% decrease in average classification error
rate (ACER) (from 41.03% to 19.32%) when the presentation attack domain is
compared to the physiological and Deepfakes domains. Our experiments highlight
the efficiency of transfer learning in rPPG-based models and perform well in
presentation attack detection in instruments that do not allow copying of this
physiological feature.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02141" title="Abstract">arXiv:2310.02141</a> [<a href="/pdf/2310.02141" title="Download PDF">pdf</a>, <a href="/format/2310.02141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Gait Modeling and Optimization for Principally Kinematic  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Siming Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cowan%2C+N+J">Noah J. Cowan</a>, 
<a href="/search/cs?searchtype=author&query=Bittner%2C+B+A">Brian A. Bittner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Robotic adaptation to unanticipated operating conditions is crucial to
achieving persistence and robustness in complex real world settings. For a wide
range of cutting-edge robotic systems, such as micro- and nano-scale robots,
soft robots, medical robots, and bio-hybrid robots, it is infeasible to
anticipate the operating environment a priori due to complexities that arise
from numerous factors including imprecision in manufacturing, chemo-mechanical
forces, and poorly understood contact mechanics. Drawing inspiration from
data-driven modeling, geometric mechanics (or gauge theory), and adaptive
control, we employ an adaptive system identification framework and demonstrate
its efficacy in enhancing the performance of principally kinematic locomotors
(those governed by Rayleigh dissipation or zero momentum conservation). We
showcase the capability of the adaptive model to efficiently accommodate
varying terrains and iteratively modified behaviors within a behavior
optimization framework. This provides both the ability to improve fundamental
behaviors and perform motion tracking to precision. Notably, we are capable of
optimizing the gaits of the Purcell swimmer using approximately 10 cycles per
link, which for the nine-link Purcell swimmer provides a factor of ten
improvement in optimization speed over the state of the art. Beyond simply a
computational speed up, this ten-fold improvement may enable this method to be
successfully deployed for in-situ behavior refinement, injury recovery, and
terrain adaptation, particularly in domains where simulations provide poor
guides for the real world.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02142" title="Abstract">arXiv:2310.02142</a> [<a href="/pdf/2310.02142" title="Download PDF">pdf</a>, <a href="/format/2310.02142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arena-independent Memory Bounds for Nash Equilibria in Reachability  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Main%2C+J+C+A">James C. A. Main</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study the memory requirements of Nash equilibria in turn-based multiplayer
games on possibly infinite graphs with reachability, shortest path and B\"uchi
objectives.
<br />We present constructions for finite-memory Nash equilibria in these games
that apply to arbitrary game graphs, bypassing the finite-arena requirement
that is central in existing approaches. We show that, for these three types of
games, from any Nash equilibrium, we can derive another Nash equilibrium where
all strategies are finite-memory such that the same players accomplish their
objective, without increasing their cost for shortest path games.
<br />Furthermore, we provide memory bounds that are independent of the size of the
game graph for reachability and shortest path games. These bounds depend only
on the number of players.
<br />To the best of our knowledge, we provide the first results pertaining to
finite-memory constrained Nash equilibria in infinite arenas and the first
arena-independent memory bounds for Nash equilibria.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02143" title="Abstract">arXiv:2310.02143</a> [<a href="/pdf/2310.02143" title="Download PDF">pdf</a>, <a href="/format/2310.02143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CORec-Cri: How collaborative and social technologies can help to  contextualize crises?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+N+L">Ngoc Luyen Le</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jinfeng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Negre%2C+E">Elsa Negre</a>, 
<a href="/search/cs?searchtype=author&query=Abel%2C+M">Marie-H&#xe9;l&#xe8;ne Abel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Crisis situations can present complex and multifaceted challenges, often
requiring the involvement of multiple organizations and stakeholders with
varying areas of expertise, responsibilities, and resources. Acquiring accurate
and timely information about impacted areas is crucial to effectively respond
to these crises. In this paper, we investigate how collaborative and social
technologies help to contextualize crises, including identifying impacted areas
and real-time needs. To this end, we define CORec-Cri (Contextulized
Ontology-based Recommender system for crisis management) based on existing
work. Our motivation for this approach is two-fold: first, effective
collaboration among stakeholders is essential for efficient and coordinated
crisis response; second, social computing facilitates interaction, information
flow, and collaboration among stakeholders. We detail the key components of our
system design, highlighting its potential to support decision-making, resource
allocation, and communication among stakeholders. Finally, we provide examples
of how our system can be applied to contextualize crises to improve crisis
management.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02147" title="Abstract">arXiv:2310.02147</a> [<a href="/pdf/2310.02147" title="Download PDF">pdf</a>, <a href="/format/2310.02147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-Time Analysis of Whittle Index based Q-Learning for Restless  Multi-Armed Bandits with Neural Network Function Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+G">Guojun Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures, Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Whittle index policy is a heuristic to the intractable restless multi-armed
bandits (RMAB) problem. Although it is provably asymptotically optimal, finding
Whittle indices remains difficult. In this paper, we present Neural-Q-Whittle,
a Whittle index based Q-learning algorithm for RMAB with neural network
function approximation, which is an example of nonlinear two-timescale
stochastic approximation with Q-function values updated on a faster timescale
and Whittle indices on a slower timescale. Despite the empirical success of
deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which
couples neural networks with two-timescale Q-learning largely remains unclear.
This paper provides a finite-time analysis of Neural-Q-Whittle, where data are
generated from a Markov chain, and Q-function is approximated by a ReLU neural
network. Our analysis leverages a Lyapunov drift approach to capture the
evolution of two coupled parameters, and the nonlinearity in value function
approximation further requires us to characterize the approximation error.
Combing these provide Neural-Q-Whittle with $\mathcal{O}(1/k^{2/3})$
convergence rate, where $k$ is the number of iterations.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02154" title="Abstract">arXiv:2310.02154</a> [<a href="/pdf/2310.02154" title="Download PDF">pdf</a>, <a href="/format/2310.02154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Program Structure Aware Precondition Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dinella%2C+E">Elizabeth Dinella</a>, 
<a href="/search/cs?searchtype=author&query=Lahiri%2C+S">Shuvendu Lahiri</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+M">Mayur Naik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">We introduce a novel approach for inferring natural preconditions from code.
Our technique produces preconditions of high quality in terms of both
correctness (modulo a test generator) and naturalness. Prior works generate
preconditions from scratch through combinations of boolean predicates, but fall
short in readability and ease of comprehension. Our innovation lies in,
instead, leveraging the structure of a target method as a seed to infer a
precondition through program transformations. Our evaluation shows that humans
can more easily reason over preconditions inferred using our approach. Lastly,
we instantiate our technique into a framework which can be applied at scale. We
present a dataset of ~18k Java (method, precondition) pairs obtained by
applying our framework to 87 real-world projects. We use this dataset to both
evaluate our approach and draw useful insights for future research in
precondition inference.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02156" title="Abstract">arXiv:2310.02156</a> [<a href="/pdf/2310.02156" title="Download PDF">pdf</a>, <a href="/format/2310.02156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistically Rewired Message-Passing Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chendi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Manolache%2C+A">Andrei Manolache</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+K">Kareem Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhe Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>, 
<a href="/search/cs?searchtype=author&query=Niepert%2C+M">Mathias Niepert</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Message-passing graph neural networks (MPNNs) emerged as powerful tools for
processing graph-structured input. However, they operate on a fixed input graph
structure, ignoring potential noise and missing information. Furthermore, their
local aggregation mechanism can lead to problems such as over-squashing and
limited expressive power in capturing relevant graph structures. Existing
solutions to these challenges have primarily relied on heuristic methods, often
disregarding the underlying data distribution. Hence, devising principled
approaches for learning to infer graph structures relevant to the given
prediction task remains an open challenge. In this work, leveraging recent
progress in exact and differentiable $k$-subset sampling, we devise
probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges
while omitting less beneficial ones. For the first time, our theoretical
analysis explores how PR-MPNNs enhance expressive power, and we identify
precise conditions under which they outperform purely randomized approaches.
Empirically, we demonstrate that our approach effectively mitigates issues like
over-squashing and under-reaching. In addition, on established real-world
datasets, our method exhibits competitive or superior predictive performance
compared to traditional MPNN models and recent graph transformer architectures.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02161" title="Abstract">arXiv:2310.02161</a> [<a href="/pdf/2310.02161" title="Download PDF">pdf</a>, <a href="/format/2310.02161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selenite: Scaffolding Decision Making with Comprehensive Overviews  Elicited from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M+X">Michael Xieyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F+M">Franklin Mingzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Kittur%2C+A">Aniket Kittur</a>, 
<a href="/search/cs?searchtype=author&query=Myers%2C+B+A">Brad A. Myers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Decision-making in unfamiliar domains can be challenging, demanding
considerable user effort to compare different options with respect to various
criteria. Prior research and our formative study found that people would
benefit from seeing an overview of the information space upfront, such as the
criteria that others have previously found useful. However, existing
sensemaking tools struggle with the "cold-start" problem -- it not only
requires significant input from previous users to generate and share these
overviews, but such overviews may also be biased and incomplete. In this work,
we introduce a novel system, Selenite, which leverages LLMs as reasoning
machines and knowledge retrievers to automatically produce a comprehensive
overview of options and criteria to jumpstart users' sensemaking processes.
Subsequently, Selenite also adapts as people use it, helping users find, read,
and navigate unfamiliar information in a systematic yet personalized manner.
Through three studies, we found that Selenite produced accurate and
high-quality overviews reliably, significantly accelerated users' information
processing, and effectively improved their overall comprehension and
sensemaking experience.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02162" title="Abstract">arXiv:2310.02162</a> [<a href="/pdf/2310.02162" title="Download PDF">pdf</a>, <a href="/format/2310.02162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TreeScope: An Agricultural Robotics Dataset for LiDAR-Based Mapping of  Trees in Forests and Orchards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Derek Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ojeda%2C+F+C">Fernando Cladera Ojeda</a>, 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+A">Ankit Prabhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A">Alan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Green%2C+P+C">Patrick Corey Green</a>, 
<a href="/search/cs?searchtype=author&query=Ehsani%2C+R">Reza Ehsani</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+P">Pratik Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vijay Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 IEEE International Conference on Robotics and Automation (ICRA 2024) for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Data collection for forestry, timber, and agriculture currently relies on
manual techniques which are labor-intensive and time-consuming. We seek to
demonstrate that robotics offers improvements over these techniques and
accelerate agricultural research, beginning with semantic segmentation and
diameter estimation of trees in forests and orchards. We present TreeScope
v1.0, the first robotics dataset for precision agriculture and forestry
addressing the counting and mapping of trees in forestry and orchards.
TreeScope provides LiDAR data from agricultural environments collected with
robotics platforms, such as UAV and mobile robot platforms carried by vehicles
and human operators. In the first release of this dataset, we provide
ground-truth data with over 1,800 manually annotated semantic labels for tree
stems and field-measured tree diameters. We share benchmark scripts for these
tasks that researchers may use to evaluate the accuracy of their algorithms.
Finally, we run our open-source diameter estimation and off-the-shelf semantic
segmentation algorithms and share our baseline results.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02164" title="Abstract">arXiv:2310.02164</a> [<a href="/pdf/2310.02164" title="Download PDF">pdf</a>, <a href="/format/2310.02164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Unlearning: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Said%2C+A">Anwar Said</a>, 
<a href="/search/cs?searchtype=author&query=Derr%2C+T">Tyler Derr</a>, 
<a href="/search/cs?searchtype=author&query=Shabbir%2C+M">Mudassir Shabbir</a>, 
<a href="/search/cs?searchtype=author&query=Abbas%2C+W">Waseem Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Koutsoukos%2C+X">Xenofon Koutsoukos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 page review paper on graph unlearning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph unlearning emerges as a crucial advancement in the pursuit of
responsible AI, providing the means to remove sensitive data traces from
trained models, thereby upholding the right to be forgotten. It is evident that
graph machine learning exhibits sensitivity to data privacy and adversarial
attacks, necessitating the application of graph unlearning techniques to
address these concerns effectively. In this comprehensive survey paper, we
present the first systematic review of graph unlearning approaches,
encompassing a diverse array of methodologies and offering a detailed taxonomy
and up-to-date literature overview to facilitate the understanding of
researchers new to this field. Additionally, we establish the vital connections
between graph unlearning and differential privacy, augmenting our understanding
of the relevance of privacy-preserving techniques in this context. To ensure
clarity, we provide lucid explanations of the fundamental concepts and
evaluation measures used in graph unlearning, catering to a broader audience
with varying levels of expertise. Delving into potential applications, we
explore the versatility of graph unlearning across various domains, including
but not limited to social networks, adversarial settings, and
resource-constrained environments like the Internet of Things (IoT),
illustrating its potential impact in safeguarding data privacy and enhancing AI
systems' robustness. Finally, we shed light on promising research directions,
encouraging further progress and innovation within the domain of graph
unlearning. By laying a solid foundation and fostering continued progress, this
survey seeks to inspire researchers to further advance the field of graph
unlearning, thereby instilling confidence in the ethical growth of AI systems
and reinforcing the responsible application of machine learning techniques in
various domains.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02166" title="Abstract">arXiv:2310.02166</a> [<a href="/pdf/2310.02166" title="Download PDF">pdf</a>, <a href="/format/2310.02166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Meet Knowledge Graphs to Answer Factoid Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salnikov%2C+M">Mikhail Salnikov</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hai Le</a>, 
<a href="/search/cs?searchtype=author&query=Rajput%2C+P">Prateek Rajput</a>, 
<a href="/search/cs?searchtype=author&query=Nikishina%2C+I">Irina Nikishina</a>, 
<a href="/search/cs?searchtype=author&query=Braslavski%2C+P">Pavel Braslavski</a>, 
<a href="/search/cs?searchtype=author&query=Malykh%2C+V">Valentin Malykh</a>, 
<a href="/search/cs?searchtype=author&query=Panchenko%2C+A">Alexander Panchenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, it has been shown that the incorporation of structured knowledge
into Large Language Models significantly improves the results for a variety of
NLP tasks. In this paper, we propose a method for exploring pre-trained
Text-to-Text Language Models enriched with additional information from
Knowledge Graphs for answering factoid questions. More specifically, we propose
an algorithm for subgraphs extraction from a Knowledge Graph based on question
entities and answer candidates. Then, we procure easily interpreted information
with Transformer-based models through the linearization of the extracted
subgraphs. Final re-ranking of the answer candidates with the extracted
information boosts Hits@1 scores of the pre-trained text-to-text language
models by 4-6%.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02167" title="Abstract">arXiv:2310.02167</a> [<a href="/pdf/2310.02167" title="Download PDF">pdf</a>, <a href="/ps/2310.02167" title="Download PostScript">ps</a>, <a href="/format/2310.02167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Unified Framework for Sequential Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=N%C3%BA%C3%B1ez-Molina%2C+C">Carlos N&#xfa;&#xf1;ez-Molina</a>, 
<a href="/search/cs?searchtype=author&query=Mesejo%2C+P">Pablo Mesejo</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Olivares%2C+J">Juan Fern&#xe1;ndez-Olivares</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In recent years, the integration of Automated Planning (AP) and Reinforcement
Learning (RL) has seen a surge of interest. To perform this integration, a
general framework for Sequential Decision Making (SDM) would prove immensely
useful, as it would help us understand how AP and RL fit together. In this
preliminary work, we attempt to provide such a framework, suitable for any
method ranging from Classical Planning to Deep RL, by drawing on concepts from
Probability Theory and Bayesian inference. We formulate an SDM task as a set of
training and test Markov Decision Processes (MDPs), to account for
generalization. We provide a general algorithm for SDM which we hypothesize
every SDM method is based on. According to it, every SDM algorithm can be seen
as a procedure that iteratively improves its solution estimate by leveraging
the task knowledge available. Finally, we derive a set of formulas and
algorithms for calculating interesting properties of SDM tasks and methods,
which make possible their empirical evaluation and comparison.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02168" title="Abstract">arXiv:2310.02168</a> [<a href="/pdf/2310.02168" title="Download PDF">pdf</a>, <a href="/format/2310.02168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Editing Personality for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shengyu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yunzhi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Pengjun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This paper introduces an innovative task focused on editing the personality
traits of Large Language Models (LLMs). This task seeks to adjust the models'
responses to opinion-related questions on specified topics since an
individual's personality often manifests in the form of their expressed
opinions, thereby showcasing different personality traits. Specifically, we
construct a new benchmark dataset PersonalityEdit to address this task. Drawing
on the theory in Social Psychology, we isolate three representative traits,
namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our
benchmark. We then gather data using GPT-4, generating responses that not only
align with a specified topic but also embody the targeted personality trait. We
conduct comprehensive experiments involving various baselines and discuss the
representation of personality behavior in LLMs. Our intriguing findings uncover
potential challenges of the proposed task, illustrating several remaining
issues. We anticipate that our work can provide the NLP community with
insights. Code and datasets will be released at
https://github.com/zjunlp/EasyEdit.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02170" title="Abstract">arXiv:2310.02170</a> [<a href="/pdf/2310.02170" title="Download PDF">pdf</a>, <a href="/format/2310.02170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with  Agent Team Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanzhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review. 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Large language model (LLM) agents have been shown effective on a wide range
of tasks, and by ensembling multiple LLM agents, their performances could be
further improved. Existing approaches employ a fixed set of agents to interact
with each other in a static architecture, which limits their generalizability
to various tasks and requires strong human prior in designing these agents. In
this work, we propose to construct a strategic team of agents communicating in
a dynamic interaction architecture based on the task query. Specifically, we
build a framework named Dynamic LLM-Agent Network ($\textbf{DyLAN}$) for
LLM-agent collaboration on complicated tasks like reasoning and code
generation. DyLAN enables agents to interact for multiple rounds in a dynamic
architecture with inference-time agent selection and an early-stopping
mechanism to improve performance and efficiency. We further design an automatic
agent team optimization algorithm based on an unsupervised metric termed
$\textit{Agent Importance Score}$, enabling the selection of best agents based
on the contribution each agent makes. Empirically, we demonstrate that DyLAN
performs well in both reasoning and code generation tasks with reasonable
computational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and
HumanEval, respectively, compared to a single execution on GPT-35-turbo. On
specific subjects of MMLU, agent team optimization in DyLAN increases accuracy
by up to 25.0%.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02172" title="Abstract">arXiv:2310.02172</a> [<a href="/pdf/2310.02172" title="Download PDF">pdf</a>, <a href="/format/2310.02172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lyfe Agents: Generative agents for low-cost real-time social  interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaiya%2C+Z">Zhao Kaiya</a>, 
<a href="/search/cs?searchtype=author&query=Naim%2C+M">Michelangelo Naim</a>, 
<a href="/search/cs?searchtype=author&query=Kondic%2C+J">Jovana Kondic</a>, 
<a href="/search/cs?searchtype=author&query=Cortes%2C+M">Manuel Cortes</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jiaxin Ge</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shuying Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G+R">Guangyu Robert Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+A">Andrew Ahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Highly autonomous generative agents powered by large language models promise
to simulate intricate social behaviors in virtual societies. However, achieving
real-time interactions with humans at a low computational cost remains
challenging. Here, we introduce Lyfe Agents. They combine low-cost with
real-time responsiveness, all while remaining intelligent and goal-oriented.
Key innovations include: (1) an option-action framework, reducing the cost of
high-level decisions; (2) asynchronous self-monitoring for better
self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing
critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation
and sociability across several multi-agent scenarios in our custom LyfeGame 3D
virtual environment platform. When equipped with our brain-inspired techniques,
Lyfe Agents can exhibit human-like self-motivated social reasoning. For
example, the agents can solve a crime (a murder mystery) through autonomous
collaboration and information exchange. Meanwhile, our techniques enabled Lyfe
Agents to operate at a computational cost 10-100 times lower than existing
alternatives. Our findings underscore the transformative potential of
autonomous generative agents to enrich human social experiences in virtual
worlds.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02174" title="Abstract">arXiv:2310.02174</a> [<a href="/pdf/2310.02174" title="Download PDF">pdf</a>, <a href="/format/2310.02174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Again, Then Fail: Large Language Models&#x27; Vacillations in Judgement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiming Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zengzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Rui Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the emergence of generative conversational large language models (LLMs)
like ChatGPT, serving as virtual assistants in various fields, the stability
and reliability of their responses have become crucial. However, during usage,
it has been observed that these models tend to waver in their judgements when
confronted with follow-up questions from users expressing skepticism or
disagreement. In this work, we draw inspiration from questioning strategies in
education and propose a \textsc{Follow-up Questioning Mechanism} along with two
evaluation metrics to assess the judgement consistency of LLMs before and after
exposure to disturbances. We evaluate the judgement consistency of ChatGPT,
PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning
benchmarks. Empirical results show that even when the initial answers are
correct, judgement consistency sharply decreases when LLMs face disturbances
such as questioning, negation, or misleading. Additionally, we study these
models' judgement consistency under various settings (sampling temperature and
prompts) to validate this issue further, observing the impact of prompt tone
and conducting an in-depth error analysis for deeper behavioral insights.
Furthermore, we also explore several prompting methods to mitigate this issue
and demonstrate their
effectiveness\footnote{\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02181" title="Abstract">arXiv:2310.02181</a> [<a href="/pdf/2310.02181" title="Download PDF">pdf</a>, <a href="/format/2310.02181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Optimization of Charging Infrastructure Placement and Operational  Schedules for a Fleet of Battery Electric Trucks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bertucci%2C+J+P">Juan Pablo Bertucci</a>, 
<a href="/search/eess?searchtype=author&query=Hofman%2C+T">Theo Hofman</a>, 
<a href="/search/eess?searchtype=author&query=Salazar%2C+M">Mauro Salazar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper examines the challenges and requirements for transitioning
logistic distribution networks to electric fleets. To maintain their current
operations, fleet operators need a clear understanding of the charging
infrastructure required and its relationship to existing power grid limitations
and fleet schedules. In this context, this paper presents a modeling framework
to optimize the charging infrastructure and charging schedules for a logistic
distribution network in a joint fashion. Specifically, we cast the joint
infrastructure design and operational scheduling problem as a mixed-integer
linear program that can be solved with off-the-shelf optimization algorithms
providing global optimality guarantees. For a case study in the Netherlands, we
assess the impact of different parameters in our optimization problem,
specifically, the allowed deviation from existing operations with conventional
diesel trucks and the cost factor for daily peak energy usage. We examine the
effects on infrastructure design and power requirements, comparing our
co-design algorithm with planned infrastructure solutions. The results indicate
that current charging and electric machine technologies for trucks can perform
the itineraries of conventional trucks for our case study, but to maintain
critical time requirements and navigate grid congestion co-design can have a
significant impact in reducing total cost of ownership (average 3.51% decrease
in total costs compared to rule-based design solutions).
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02183" title="Abstract">arXiv:2310.02183</a> [<a href="/pdf/2310.02183" title="Download PDF">pdf</a>, <a href="/format/2310.02183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Puddles: Application-Independent Recovery and Location-Independent Data  for Persistent Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahar%2C+S">Suyash Mahar</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Mingyao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+T">TJ Smith</a>, 
<a href="/search/cs?searchtype=author&query=Izraelevitz%2C+J">Joseph Izraelevitz</a>, 
<a href="/search/cs?searchtype=author&query=Swanson%2C+S">Steven Swanson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EuroSys 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this paper, we argue that current work has failed to provide a
comprehensive and maintainable in-memory representation for persistent memory.
<br />PM data should be easily mappable into a process address space, shareable
across processes, shippable between machines, consistent after a crash, and
accessible to legacy code with fast, efficient pointers as first-class
abstractions.
<br />While existing systems have provided niceties like mmap()-based load/store
access, they have not been able to support all these necessary properties due
to conflicting requirements.
<br />We propose Puddles, a new persistent memory abstraction, to solve these
problems. Puddles provide application-independent recovery after a power
outage; they make recovery from a system failure a system-level property of the
stored data rather than the responsibility of the programs that access it.
Puddles use native pointers, so they are compatible with existing code.
Finally, Puddles implement support for sharing and shipping of PM data between
processes and systems without expensive serialization and deserialization.
<br />Compared to existing systems, Puddles are at least as fast as and up to
1.34$\times$ faster than PMDK while being competitive with other PM libraries
across YCSB workloads. Moreover, to demonstrate Puddles' ability to relocate
data, we showcase a sensor network data-aggregation workload that results in a
4.7$\times$ speedup over PMDK.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02192" title="Abstract">arXiv:2310.02192</a> [<a href="/pdf/2310.02192" title="Download PDF">pdf</a>, <a href="/format/2310.02192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sneaked references: Cooked reference metadata inflate citation counts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Besan%C3%A7on%2C+L">Lonni Besan&#xe7;on</a>, 
<a href="/search/cs?searchtype=author&query=Cabanac%2C+G">Guillaume Cabanac</a>, 
<a href="/search/cs?searchtype=author&query=Labb%C3%A9%2C+C">Cyril Labb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Magazinov%2C+A">Alexander Magazinov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">We report evidence of an undocumented method to manipulate citation counts
involving 'sneaked' references. Sneaked references are registered as metadata
for scientific articles in which they do not appear. This manipulation exploits
trusted relationships between various actors: publishers, the Crossref metadata
registration agency, digital libraries, and bibliometric platforms. By
collecting metadata from various sources, we show that extra undue references
are actually sneaked in at Digital Object Identifier (DOI) registration time,
resulting in artificially inflated citation counts. As a case study, focusing
on three journals from a given publisher, we identified at least 9% sneaked
references (5,978/65,836) mainly benefiting two authors. Despite not existing
in the articles, these sneaked references exist in metadata registries and
inappropriately propagate to bibliometric dashboards. Furthermore, we
discovered 'lost' references: the studied bibliometric platform failed to index
at least 56% (36,939/65,836) of the references listed in the HTML version of
the publications. The extent of the sneaked and lost references in the global
literature remains unknown and requires further investigations. Bibliometric
platforms producing citation counts should identify, quantify, and correct
these flaws to provide accurate data to their patrons and prevent further
citation gaming.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02193" title="Abstract">arXiv:2310.02193</a> [<a href="/pdf/2310.02193" title="Download PDF">pdf</a>, <a href="/format/2310.02193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification in Inverse Models in Hydrology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S+S">Somya Sharma Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+R">Rahul Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Renganathan%2C+A">Arvind Renganathan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Snigdhansu Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Nieber%2C+J">John Nieber</a>, 
<a href="/search/cs?searchtype=author&query=Duffy%2C+C">Christopher Duffy</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vipin Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2210.06213">arXiv:2210.06213</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applications (stat.AP)

</div>
<p class="mathjax">In hydrology, modeling streamflow remains a challenging task due to the
limited availability of basin characteristics information such as soil geology
and geomorphology. These characteristics may be noisy due to measurement errors
or may be missing altogether. To overcome this challenge, we propose a
knowledge-guided, probabilistic inverse modeling method for recovering physical
characteristics from streamflow and weather data, which are more readily
available. We compare our framework with state-of-the-art inverse models for
estimating river basin characteristics. We also show that these estimates offer
improvement in streamflow modeling as opposed to using the original basin
characteristic values. Our inverse model offers 3\% improvement in R$^2$ for
the inverse model (basin characteristic estimation) and 6\% for the forward
model (streamflow prediction). Our framework also offers improved
explainability since it can quantify uncertainty in both the inverse and the
forward model. Uncertainty quantification plays a pivotal role in improving the
explainability of machine learning models by providing additional insights into
the reliability and limitations of model predictions. In our analysis, we
assess the quality of the uncertainty estimates. Compared to baseline
uncertainty quantification methods, our framework offers 10\% improvement in
the dispersion of epistemic uncertainty and 13\% improvement in coverage rate.
This information can help stakeholders understand the level of uncertainty
associated with the predictions and provide a more comprehensive view of the
potential outcomes.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02195" title="Abstract">arXiv:2310.02195</a> [<a href="/pdf/2310.02195" title="Download PDF">pdf</a>, <a href="/ps/2310.02195" title="Download PostScript">ps</a>, <a href="/format/2310.02195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Online Scheduling and Routing for Automated Guided Vehicles:  Comparing a Novel Loop-Based Algorithm Against Existing Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stubbe%2C+L">Louis Stubbe</a>, 
<a href="/search/cs?searchtype=author&query=Goemaere%2C+J">Jens Goemaere</a>, 
<a href="/search/cs?searchtype=author&query=Goedgebeur%2C+J">Jan Goedgebeur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automated guided vehicles (AGVs) are widely used in various industries, and
scheduling and routing them in a conflict-free manner is crucial to their
efficient operation. We propose a loop-based algorithm that solves the online,
conflict-free scheduling and routing problem for AGVs. The proposed algorithm
is compared against an exact method, a greedy heuristic and a metaheuristic. We
experimentally show that this algorithm either outperforms the other algorithms
or gets an equally good solution in less computing time.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02198" title="Abstract">arXiv:2310.02198</a> [<a href="/pdf/2310.02198" title="Download PDF">pdf</a>, <a href="/ps/2310.02198" title="Download PostScript">ps</a>, <a href="/format/2310.02198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong Faithfulness for ELH Ontology Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lacerda%2C+V">Victor Lacerda</a>, 
<a href="/search/cs?searchtype=author&query=Ozaki%2C+A">Ana Ozaki</a>, 
<a href="/search/cs?searchtype=author&query=Guimar%C3%A3es%2C+R">Ricardo Guimar&#xe3;es</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages (including refs.), 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Ontology embedding methods are powerful approaches to represent and reason
over structured knowledge in various domains. One advantage of ontology
embeddings over knowledge graph embeddings is their ability to capture and
impose an underlying schema to which the model must conform. Despite advances,
most current approaches do not guarantee that the resulting embedding respects
the axioms the ontology entails. In this work, we formally prove that
normalized ${\cal ELH}$ has the strong faithfulness property on convex
geometric models, which means that there is an embedding that precisely
captures the original ontology. We present a region-based geometric model for
embedding normalized ${\cal ELH}$ ontologies into a continuous vector space. To
prove strong faithfulness, our construction takes advantage of the fact that
normalized ${\cal ELH}$ has a finite canonical model. We first prove the
statement assuming (possibly) non-convex regions, allowing us to keep the
required dimensions low. Then, we impose convexity on the regions and show the
property still holds. Finally, we consider reasoning tasks on geometric models
and analyze the complexity in the class of convex geometric models used for
proving strong faithfulness.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02201" title="Abstract">arXiv:2310.02201</a> [<a href="/pdf/2310.02201" title="Download PDF">pdf</a>, <a href="/format/2310.02201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learnable Data Augmentation for One-Shot Unsupervised Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carrazco%2C+J+I+D">Julio Ivan Davila Carrazco</a>, 
<a href="/search/cs?searchtype=author&query=Morerio%2C+P">Pietro Morerio</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bue%2C+A">Alessio Del Bue</a>, 
<a href="/search/cs?searchtype=author&query=Murino%2C+V">Vittorio Murino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to The 34th British Machine Vision Conference (BMVC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a classification framework based on learnable data
augmentation to tackle the One-Shot Unsupervised Domain Adaptation (OS-UDA)
problem. OS-UDA is the most challenging setting in Domain Adaptation, as only
one single unlabeled target sample is assumed to be available for model
adaptation. Driven by such single sample, our method LearnAug-UDA learns how to
augment source data, making it perceptually similar to the target. As a result,
a classifier trained on such augmented data will generalize well for the target
domain. To achieve this, we designed an encoder-decoder architecture that
exploits a perceptual loss and style transfer strategies to augment the source
data. Our method achieves state-of-the-art performance on two well-known Domain
Adaptation benchmarks, DomainNet and VisDA. The project code is available at
https://github.com/IIT-PAVIS/LearnAug-UDA
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02204" title="Abstract">arXiv:2310.02204</a> [<a href="/pdf/2310.02204" title="Download PDF">pdf</a>, <a href="/format/2310.02204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determinisation and Unambiguisation of Polynomially-Ambiguous Rational  Weighted Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jecker%2C+I">Isma&#xeb;l Jecker</a>, 
<a href="/search/cs?searchtype=author&query=Mazowiecki%2C+F">Filip Mazowiecki</a>, 
<a href="/search/cs?searchtype=author&query=Purser%2C+D">David Purser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We study the determinisation and unambiguisation problems of weighted
automata over the rational field: Given a weighted automaton, can we determine
whether there exists an equivalent deterministic, respectively unambiguous,
weighted automaton? Recent results by Bell and Smertnig show that the problem
is decidable, however they do not provide any complexity bounds. We show that
both problems are in PSPACE for polynomially-ambiguous weighted automata.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02206" title="Abstract">arXiv:2310.02206</a> [<a href="/pdf/2310.02206" title="Download PDF">pdf</a>, <a href="/format/2310.02206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chunking: Forgetting Matters in Continual Learning even without Changing  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+L">Thomas L. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Storkey%2C+A">Amos Storkey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 11 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Work on continual learning (CL) has largely focused on the problems arising
from the dynamically-changing data distribution. However, CL can be decomposed
into two sub-problems: (a) shifts in the data distribution, and (b) dealing
with the fact that the data is split into chunks and so only a part of the data
is available to be trained on at any point in time. In this work, we look at
the latter sub-problem -- the chunking of data -- and note that previous
analysis of chunking in the CL literature is sparse. We show that chunking is
an important part of CL, accounting for around half of the performance drop
from offline learning in our experiments. Furthermore, our results reveal that
current CL algorithms do not address the chunking sub-problem, only performing
as well as plain SGD training when there is no shift in the data distribution.
We analyse why performance drops when learning occurs on chunks of data, and
find that forgetting, which is often seen to be a problem due to distribution
shift, still arises and is a significant problem. Motivated by an analysis of
the linear case, we show that per-chunk weight averaging improves performance
in the chunking setting and that this performance transfers to the full CL
setting. Hence, we argue that work on chunking can help advance CL in general.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02207" title="Abstract">arXiv:2310.02207</a> [<a href="/pdf/2310.02207" title="Download PDF">pdf</a>, <a href="/format/2310.02207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models Represent Space and Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gurnee%2C+W">Wes Gurnee</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The capabilities of large language models (LLMs) have sparked debate over
whether such systems just learn an enormous collection of superficial
statistics or a coherent model of the data generating process -- a world model.
We find evidence for the latter by analyzing the learned representations of
three spatial datasets (world, US, NYC places) and three temporal datasets
(historical figures, artworks, news headlines) in the Llama-2 family of models.
We discover that LLMs learn linear representations of space and time across
multiple scales. These representations are robust to prompting variations and
unified across different entity types (e.g. cities and landmarks). In addition,
we identify individual ``space neurons'' and ``time neurons'' that reliably
encode spatial and temporal coordinates. Our analysis demonstrates that modern
LLMs acquire structured knowledge about fundamental dimensions such as space
and time, supporting the view that they learn not merely superficial
statistics, but literal world models.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02208" title="Abstract">arXiv:2310.02208</a> [<a href="/pdf/2310.02208" title="Download PDF">pdf</a>, <a href="/format/2310.02208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Integer Clustering Approach for Modeling Large-Scale EV Fleets with  Guaranteed Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Geng%2C+S">Sijia Geng</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+T">Thomas Lee</a>, 
<a href="/search/eess?searchtype=author&query=Mallapragada%2C+D">Dharik Mallapragada</a>, 
<a href="/search/eess?searchtype=author&query=Botterud%2C+A">Audun Botterud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Large-scale integration of electric vehicles (EVs) leads to a tighter
integration between transportation and electric energy systems. In this paper,
we develop a novel integer-clustering approach to model a large number of EVs
that manages vehicle charging and energy at the fleet level yet maintain
individual trip dispatch. The model is then used to develop a spatially and
temporally-resolved decision-making tool for optimally planning and/or
operating EV fleets and charging infrastructure. The tool comprises a two-stage
framework where a tractable disaggregation step follows the integer-clustering
problem to recover an individually feasible solution. Mathematical
relationships between the integer clustering, disaggregation, and individual
formulations are analyzed. We establish theoretical lower and upper bounds on
the true individual formulation which underpins a guaranteed performance of the
proposed method. The optimality accuracy and computational efficiency of the
integer-clustering formulation are also numerically validated on a real-world
case study of Boston's public transit network under extensive test instances.
Substantial speedups with minimal loss in solution quality are demonstrated.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02211" title="Abstract">arXiv:2310.02211</a> [<a href="/pdf/2310.02211" title="Download PDF">pdf</a>, <a href="/format/2310.02211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Localization and Tracking in City-Scale UWB Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+N">Nakul Garg</a>, 
<a href="/search/cs?searchtype=author&query=Shahid%2C+I">Irtaza Shahid</a>, 
<a href="/search/cs?searchtype=author&query=Sheshadri%2C+R+K">Ramanujan K Sheshadri</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+K">Karthikeyan Sundaresan</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nirupam Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Localization of networked nodes is an essential problem in emerging
applications, including first-responder navigation, automated manufacturing
lines, vehicular and drone navigation, asset navigation and tracking, Internet
of Things and 5G communication networks. In this paper, we present Locate3D, a
novel system for peer-to-peer node localization and orientation estimation in
large networks. Unlike traditional range-only methods, Locate3D introduces
angle-of-arrival (AoA) data as an added network topology constraint. The system
solves three key challenges: it uses angles to reduce the number of
measurements required by 4x and jointly use range and angle data for location
estimation. We develop a spanning-tree approach for fast location updates, and
to ensure the output graphs are rigid and uniquely realizable, even in occluded
or weakly connected areas. Locate3D cuts down latency by up to 75% without
compromising accuracy, surpassing standard range-only solutions. It has a 10.2
meters median localization error for large-scale networks (30,000 nodes, 15
anchors spread across 14km square) and 0.5 meters for small-scale networks (10
nodes).
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02219" title="Abstract">arXiv:2310.02219</a> [<a href="/pdf/2310.02219" title="Download PDF">pdf</a>, <a href="/format/2310.02219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What do we learn from a large-scale study of pre-trained visual  representations in sim and real environments?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silwal%2C+S">Sneha Silwal</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+K">Karmesh Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tingfan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Vakil%2C+J">Jay Vakil</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+A">Arjun Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Arnaud%2C+S">Sergio Arnaud</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Claire Chen</a>, 
<a href="/search/cs?searchtype=author&query=Berges%2C+V">Vincent-Pierre Berges</a>, 
<a href="/search/cs?searchtype=author&query=Batra%2C+D">Dhruv Batra</a>, 
<a href="/search/cs?searchtype=author&query=Rajeswaran%2C+A">Aravind Rajeswaran</a>, 
<a href="/search/cs?searchtype=author&query=Kalakrishnan%2C+M">Mrinal Kalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Meier%2C+F">Franziska Meier</a>, 
<a href="/search/cs?searchtype=author&query=Maksymets%2C+O">Oleksandr Maksymets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website <a href="https://pvrs-sim2real.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a large empirical investigation on the use of pre-trained visual
representations (PVRs) for training downstream policies that execute real-world
tasks. Our study spans five different PVRs, two different policy-learning
paradigms (imitation and reinforcement learning), and three different robots
for 5 distinct manipulation and indoor navigation tasks. From this effort, we
can arrive at three insights: 1) the performance trends of PVRs in the
simulation are generally indicative of their trends in the real world, 2) the
use of PVRs enables a first-of-its-kind result with indoor ImageNav (zero-shot
transfer to a held-out scene in the real world), and 3) the benefits from
variations in PVRs, primarily data-augmentation and fine-tuning, also transfer
to the real-world performance. See project website for additional details and
visuals.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02221" title="Abstract">arXiv:2310.02221</a> [<a href="/pdf/2310.02221" title="Download PDF">pdf</a>, <a href="/format/2310.02221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structurally guided task decomposition in spatial navigation tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruiqi He</a>, 
<a href="/search/cs?searchtype=author&query=Correa%2C+C+G">Carlos G. Correa</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+M+K">Mark K. Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">How are people able to plan so efficiently despite limited cognitive
resources? We aimed to answer this question by extending an existing model of
human task decomposition that can explain a wide range of simple planning
problems by adding structure information to the task to facilitate planning in
more complex tasks. The extended model was then applied to a more complex
planning domain of spatial navigation. Our results suggest that our framework
can correctly predict the navigation strategies of the majority of the
participants in an online experiment.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02223" title="Abstract">arXiv:2310.02223</a> [<a href="/pdf/2310.02223" title="Download PDF">pdf</a>, <a href="/format/2310.02223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimum Monitoring of Heterogeneous Continuous Time Markov Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akar%2C+N">Nail Akar</a>, 
<a href="/search/cs?searchtype=author&query=Ulukus%2C+S">Sennur Ulukus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Performance (cs.PF)

</div>
<p class="mathjax">We study a remote monitoring system in which a collection of ergodic,
aperiodic, mutually independent, and heterogeneous continuous time Markov chain
(CTMC) based information sources is considered. In this system, a common remote
monitor samples the states of the individual CTMCs according to a Poisson
process with possibly different per-source sampling rates, in order to maintain
remote estimates of the states of each of the sources. Three information
freshness models are considered to quantify the accuracy of the remote
estimates: fresh when equal (FWE), fresh when sampled (FWS) and fresh when
close (FWC). For each of these freshness models, closed-form expressions are
derived for mean information freshness for a given source. Using these
expressions, optimum sampling rates for all sources are obtained so as to
maximize the weighted sum freshness of the monitoring system under an overall
sampling rate constraint. This optimization problem possesses a water-filling
solution with quadratic worst case computational complexity in the number of
information sources. Numerical examples are provided to validate the
effectiveness of the optimum sampler in comparison to several baseline sampling
policies.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02224" title="Abstract">arXiv:2310.02224</a> [<a href="/pdf/2310.02224" title="Download PDF">pdf</a>, <a href="/format/2310.02224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models be Instructed to Protect Personal Information?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mendes%2C+E">Ethan Mendes</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sauvik Das</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large multimodal language models have proven transformative in numerous
applications. However, these models have been shown to memorize and leak
pre-training data, raising serious user privacy and information security
concerns. While data leaks should be prevented, it is also crucial to examine
the trade-off between the privacy protection and model utility of proposed
approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to
assess this privacy/utility trade-off when a model is instructed to protect
specific categories of personal information in a simulated scenario. We also
propose a technique to iteratively self-moderate responses, which significantly
improves privacy. However, through a series of red-teaming experiments, we find
that adversaries can also easily circumvent these protections with simple
jailbreaking methods through textual and/or image inputs. We believe PrivQA has
the potential to support the development of new models with improved privacy
protections, as well as the adversarial robustness of these protections. We
release the entire PrivQA dataset at https://llm-access-control.github.io/.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02226" title="Abstract">arXiv:2310.02226</a> [<a href="/pdf/2310.02226" title="Download PDF">pdf</a>, <a href="/format/2310.02226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Think before you speak: Training Language Models With Pause Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Sachin Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ziwei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Rawat%2C+A+S">Ankit Singh Rawat</a>, 
<a href="/search/cs?searchtype=author&query=Menon%2C+A+K">Aditya Krishna Menon</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sanjiv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Nagarajan%2C+V">Vaishnavh Nagarajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Language models generate responses by producing a series of tokens in
immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$
hidden vectors per layer, one vector per preceding token. What if instead we
were to let the model manipulate say, $K+10$ hidden vectors, before it outputs
the $(K+1)^{th}$ token? We operationalize this idea by performing training and
inference on language models with a (learnable) $\textit{pause}$ token, a
sequence of which is appended to the input prefix. We then delay extracting the
model's outputs until the last pause token is seen, thereby allowing the model
to process extra computation before committing to an answer. We empirically
evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M
parameters with causal pretraining on C4, and on downstream tasks covering
reasoning, question-answering, general understanding and fact recall. Our main
finding is that inference-time delays show gains when the model is both
pre-trained and finetuned with delays. For the 1B model, we witness gains on 8
of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of
SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of
GSM8k. Our work raises a range of conceptual and practical future research
questions on making delayed next-token prediction a widely applicable new
paradigm.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02227" title="Abstract">arXiv:2310.02227</a> [<a href="/pdf/2310.02227" title="Download PDF">pdf</a>, <a href="/format/2310.02227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meidani%2C+K">Kazem Meidani</a>, 
<a href="/search/cs?searchtype=author&query=Shojaee%2C+P">Parshin Shojaee</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+C+K">Chandan K. Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In an era where symbolic mathematical equations are indispensable for
modeling complex natural phenomena, scientific inquiry often involves
collecting observations and translating them into mathematical expressions.
Recently, deep learning has emerged as a powerful tool for extracting insights
from data. However, existing models typically specialize in either numeric or
symbolic domains, and are usually trained in a supervised manner tailored to
specific tasks. This approach neglects the substantial benefits that could
arise from a task-agnostic unified understanding between symbolic equations and
their numeric counterparts. To bridge the gap, we introduce SNIP, a
Symbolic-Numeric Integrated Pre-training, which employs joint contrastive
learning between symbolic and numeric domains, enhancing their mutual
similarities in the pre-trained embeddings. By performing latent space
analysis, we observe that SNIP provides cross-domain insights into the
representations, revealing that symbolic supervision enhances the embeddings of
numeric data and vice versa. We evaluate SNIP across diverse tasks, including
symbolic-to-numeric mathematical property prediction and numeric-to-symbolic
equation discovery, commonly known as symbolic regression. Results show that
SNIP effectively transfers to various tasks, consistently outperforming fully
supervised baselines and competing strongly with established task-specific
methods, especially in few-shot learning scenarios where available data is
limited.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02229" title="Abstract">arXiv:2310.02229</a> [<a href="/pdf/2310.02229" title="Download PDF">pdf</a>, <a href="/format/2310.02229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extraction of Medication and Temporal Relation from Clinical Text by  Harnessing Different Deep Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+H">Hangyu Tu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lifeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Nenadic%2C+G">Goran Nenadic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> working paper, 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Clinical texts, represented in electronic medical records (EMRs), contain
rich medical information and are essential for disease prediction, personalised
information recommendation, clinical decision support, and medication pattern
mining and measurement. Relation extractions between medication mentions and
temporal information can further help clinicians better understand the
patients' treatment history. To evaluate the performances of deep learning (DL)
and large language models (LLMs) in medication extraction and temporal
relations classification, we carry out an empirical investigation of
\textbf{MedTem} project using several advanced learning structures including
BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER),
and BERT-CNN for temporal relation extraction (RE), in addition to the
exploration of different word embedding techniques. Furthermore, we also
designed a set of post-processing roles to generate structured output on
medications and the temporal relation. Our experiments show that CNN-BiLSTM
slightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding
75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro
Average. BERT-CNN model also produced reasonable evaluation scores 64.48,
67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction
test set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted
at \url{https://github.com/HECTA-UoM/MedTem}
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02230" title="Abstract">arXiv:2310.02230</a> [<a href="/pdf/2310.02230" title="Download PDF">pdf</a>, <a href="/format/2310.02230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts  in Underspecified Visual Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scimeca%2C+L">Luca Scimeca</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+A">Alexander Rubinstein</a>, 
<a href="/search/cs?searchtype=author&query=Nicolicioiu%2C+A">Armand Nicolicioiu</a>, 
<a href="/search/cs?searchtype=author&query=Teney%2C+D">Damien Teney</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spurious correlations in the data, where multiple cues are predictive of the
target labels, often lead to shortcut learning phenomena, where a model may
rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this
work, we propose an ensemble diversification framework exploiting the
generation of synthetic counterfactuals using Diffusion Probabilistic Models
(DPMs). We discover that DPMs have the inherent capability to represent
multiple visual cues independently, even when they are largely correlated in
the training data. We leverage this characteristic to encourage model diversity
and empirically show the efficacy of the approach with respect to several
diversification objectives. We show that diffusion-guided diversification can
lead models to avert attention from shortcut cues, achieving ensemble diversity
performance comparable to previous methods requiring additional data
collection.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02232" title="Abstract">arXiv:2310.02232</a> [<a href="/pdf/2310.02232" title="Download PDF">pdf</a>, <a href="/format/2310.02232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HoloNets: Spectral Convolutions do extend to Directed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koke%2C+C">Christian Koke</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Within the graph learning community, conventional wisdom dictates that
spectral convolutional networks may only be deployed on undirected graphs: Only
there could the existence of a well-defined graph Fourier transform be
guaranteed, so that information may be translated between spatial- and spectral
domains. Here we show this traditional reliance on the graph Fourier transform
to be superfluous and -- making use of certain advanced tools from complex
analysis and spectral theory -- extend spectral convolutions to directed
graphs. We provide a frequency-response interpretation of newly developed
filters, investigate the influence of the basis used to express filters and
discuss the interplay with characteristic operators on which networks are
based. In order to thoroughly test the developed theory, we conduct experiments
in real world settings, showcasing that directed spectral convolutional
networks provide new state of the art results for heterophilic node
classification on many datasets and -- as opposed to baselines -- may be
rendered stable to resolution-scale varying topological perturbations.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02234" title="Abstract">arXiv:2310.02234</a> [<a href="/pdf/2310.02234" title="Download PDF">pdf</a>, <a href="/format/2310.02234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIS-AVioDD: Modality Invariant and Specific Representation for  Audio-Visual Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Katamneni%2C+V+S">Vinaya Sree Katamneni</a>, 
<a href="/search/cs?searchtype=author&query=Rattani%2C+A">Ajita Rattani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deepfakes are synthetic media generated using deep generative algorithms and
have posed a severe societal and political threat. Apart from facial
manipulation and synthetic voice, recently, a novel kind of deepfakes has
emerged with either audio or visual modalities manipulated. In this regard, a
new generation of multimodal audio-visual deepfake detectors is being
investigated to collectively focus on audio and visual data for multimodal
manipulation detection. Existing multimodal (audio-visual) deepfake detectors
are often based on the fusion of the audio and visual streams from the video.
Existing studies suggest that these multimodal detectors often obtain
equivalent performances with unimodal audio and visual deepfake detectors. We
conjecture that the heterogeneous nature of the audio and visual signals
creates distributional modality gaps and poses a significant challenge to
effective fusion and efficient performance. In this paper, we tackle the
problem at the representation level to aid the fusion of audio and visual
streams for multimodal deepfake detection. Specifically, we propose the joint
use of modality (audio and visual) invariant and specific representations. This
ensures that the common patterns and patterns specific to each modality
representing pristine or fake content are preserved and fused for multimodal
deepfake manipulation detection. Our experimental results on FakeAVCeleb and
KoDF audio-visual deepfake datasets suggest the enhanced accuracy of our
proposed method over SOTA unimodal and multimodal audio-visual deepfake
detectors by $17.8$% and $18.4$%, respectively. Thus, obtaining
state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02235" title="Abstract">arXiv:2310.02235</a> [<a href="/pdf/2310.02235" title="Download PDF">pdf</a>, <a href="/format/2310.02235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Quality Assessment of Wikipedia Articles -- A Systematic  Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%C3%A1s%2C+P+M">Pedro Miguel Mo&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+C+T">Carla Teixeira Lopes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 10 figures, just accepted in ACM Computing Surveys (September 2023). This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in ACM Computing Surveys, <a href="https://dx.doi.org/10.1145/3625286">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Wikipedia is the world's largest online encyclopedia, but maintaining article
quality through collaboration is challenging. Wikipedia designed a quality
scale, but with such a manual assessment process, many articles remain
unassessed. We review existing methods for automatically measuring the quality
of Wikipedia articles, identifying and comparing machine learning algorithms,
article features, quality metrics, and used datasets, examining 149 distinct
studies, and exploring commonalities and gaps in them. The literature is
extensive, and the approaches follow past technological trends. However,
machine learning is still not widely used by Wikipedia, and we hope that our
analysis helps future researchers change that reality.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02237" title="Abstract">arXiv:2310.02237</a> [<a href="/pdf/2310.02237" title="Download PDF">pdf</a>, <a href="/format/2310.02237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Model Learning Heterogeneity for Boosting Ensemble Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanzhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+K">Ka-Ho Chow</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenqi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE ICDM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural network ensembles hold the potential of improving generalization
performance for complex learning tasks. This paper presents formal analysis and
empirical evaluation to show that heterogeneous deep ensembles with high
ensemble diversity can effectively leverage model learning heterogeneity to
boost ensemble robustness. We first show that heterogeneous DNN models trained
for solving the same learning problem, e.g., object detection, can
significantly strengthen the mean average precision (mAP) through our weighted
bounding box ensemble consensus method. Second, we further compose ensembles of
heterogeneous models for solving different learning problems, e.g., object
detection and semantic segmentation, by introducing the connected component
labeling (CCL) based alignment. We show that this two-tier heterogeneity driven
ensemble construction method can compose an ensemble team that promotes high
ensemble diversity and low negative correlation among member models of the
ensemble, strengthening ensemble robustness against both negative examples and
adversarial attacks. Third, we provide a formal analysis of the ensemble
robustness in terms of negative correlation. Extensive experiments validate the
enhanced robustness of heterogeneous ensembles in both benign and adversarial
settings. The source codes are available on GitHub at
https://github.com/git-disl/HeteRobust.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02238" title="Abstract">arXiv:2310.02238</a> [<a href="/pdf/2310.02238" title="Download PDF">pdf</a>, <a href="/ps/2310.02238" title="Download PostScript">ps</a>, <a href="/format/2310.02238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who&#x27;s Harry Potter? Approximate Unlearning in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eldan%2C+R">Ronen Eldan</a>, 
<a href="/search/cs?searchtype=author&query=Russinovich%2C+M">Mark Russinovich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are trained on massive internet corpora that
often contain copyrighted content. This poses legal and ethical challenges for
the developers and users of these models, as well as the original authors and
publishers. In this paper, we propose a novel technique for unlearning a subset
of the training data from a LLM, without having to retrain it from scratch.
<br />We evaluate our technique on the task of unlearning the Harry Potter books
from the Llama2-7b model (a generative language model recently open-sourced by
Meta). While the model took over 184K GPU-hours to pretrain, we show that in
about 1 GPU hour of finetuning, we effectively erase the model's ability to
generate or recall Harry Potter-related content, while its performance on
common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains
almost unaffected. We make our fine-tuned model publicly available on
HuggingFace for community evaluation. To the best of our knowledge, this is the
first paper to present an effective technique for unlearning in generative
language models.
<br />Our technique consists of three main components: First, we use a reinforced
model that is further trained on the target data to identify the tokens that
are most related to the unlearning target, by comparing its logits with those
of a baseline model. Second, we replace idiosyncratic expressions in the target
data with generic counterparts, and leverage the model's own predictions to
generate alternative labels for every token. These labels aim to approximate
the next-token predictions of a model that has not been trained on the target
data. Third, we finetune the model on these alternative labels, which
effectively erases the original text from the model's memory whenever it is
prompted with its context.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02239" title="Abstract">arXiv:2310.02239</a> [<a href="/pdf/2310.02239" title="Download PDF">pdf</a>, <a href="/format/2310.02239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiniGPT-5: Interleaved Vision-and-Language Generation via Generative  Vokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaizhi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuehai He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have garnered significant attention for their
advancements in natural language processing, demonstrating unparalleled prowess
in text comprehension and generation. Yet, the simultaneous generation of
images with coherent textual narratives remains an evolving frontier. In
response, we introduce an innovative interleaved vision-and-language generation
technique anchored by the concept of "generative vokens," acting as the bridge
for harmonized image-text outputs. Our approach is characterized by a
distinctive two-staged training strategy focusing on description-free
multimodal generation, where the training requires no comprehensive
descriptions of images. To bolster model integrity, classifier-free guidance is
incorporated, enhancing the effectiveness of vokens on image generation. Our
model, MiniGPT-5, exhibits substantial improvement over the baseline Divter
model on the MMDialog dataset and consistently delivers superior or comparable
multimodal outputs in human evaluations on the VIST dataset, highlighting its
efficacy across diverse benchmarks.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02240" title="Abstract">arXiv:2310.02240</a> [<a href="/pdf/2310.02240" title="Download PDF">pdf</a>, <a href="/format/2310.02240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spherical Rolling Robots Design, Modeling, and Control: A Systematic  Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diouf%2C+A">Aminata Diouf</a>, 
<a href="/search/cs?searchtype=author&query=Belzile%2C+B">Bruno Belzile</a>, 
<a href="/search/cs?searchtype=author&query=Saad%2C+M">Maarouf Saad</a>, 
<a href="/search/cs?searchtype=author&query=St-Onge%2C+D">David St-Onge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Spherical robots have garnered increasing interest for their applications in
exploration, tunnel inspection, and extraterrestrial missions. Diverse designs
have emerged, including barycentric configurations, pendulum-based mechanisms,
etc. In addition, a wide spectrum of control strategies has been proposed,
ranging from traditional PID approaches to cutting-edge neural networks. Our
systematic review aims to comprehensively identify and categorize locomotion
systems and control schemes employed by spherical robots, spanning the years
1996 to 2023. A meticulous search across five databases yielded a dataset of
3189 records. As a result of our exhaustive analysis, we identified a
collection of novel designs and control strategies. Leveraging the insights
garnered, we provide valuable recommendations for optimizing the design and
control aspects of spherical robots, supporting both novel design endeavors and
the advancement of field deployments. Furthermore, we illuminate key research
directions that hold the potential to unlock the full capabilities of spherical
robots
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02242" title="Abstract">arXiv:2310.02242</a> [<a href="/pdf/2310.02242" title="Download PDF">pdf</a>, <a href="/format/2310.02242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Generation of Human-Object Interactions with Diffusion  Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pi%2C+H">Huaijin Pi</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Sida Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Minghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaowei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023. Project page: <a href="https://zju3dv.github.io/hghoi">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">This paper presents a novel approach to generating the 3D motion of a human
interacting with a target object, with a focus on solving the challenge of
synthesizing long-range and diverse motions, which could not be fulfilled by
existing auto-regressive models or path planning-based methods. We propose a
hierarchical generation framework to solve this challenge. Specifically, our
framework first generates a set of milestones and then synthesizes the motion
along them. Therefore, the long-range motion generation could be reduced to
synthesizing several short motion sequences guided by milestones. The
experiments on the NSM, COUCH, and SAMP datasets show that our approach
outperforms previous methods by a large margin in both quality and diversity.
The source code is available on our project page
https://zju3dv.github.io/hghoi.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02244" title="Abstract">arXiv:2310.02244</a> [<a href="/pdf/2310.02244" title="Download PDF">pdf</a>, <a href="/format/2310.02244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Greg Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dingli Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Probability (math.PR)

</div>
<p class="mathjax">By classifying infinite-width neural networks and identifying the *optimal*
limit, Tensor Programs IV and V demonstrated a universal way, called $\mu$P,
for *widthwise hyperparameter transfer*, i.e., predicting optimal
hyperparameters of wide neural networks from narrow ones. Here we investigate
the analogous classification for *depthwise parametrizations* of deep residual
networks (resnets). We classify depthwise parametrizations of block multiplier
and learning rate by their infinite-width-then-depth limits. In resnets where
each block has only one layer, we identify a unique optimal parametrization,
called Depth-$\mu$P that extends $\mu$P and show empirically it admits
depthwise hyperparameter transfer. We identify *feature diversity* as a crucial
factor in deep networks, and Depth-$\mu$P can be characterized as maximizing
both feature learning and feature diversity. Exploiting this, we find that
absolute value, among all homogeneous nonlinearities, maximizes feature
diversity and indeed empirically leads to significant better performance.
However, if each block is deeper (such as modern transformers), then we find
fundamental limitations in all possible infinite-depth limits of such
parametrizations, which we illustrate both theoretically and empirically on
simple networks as well as Megatron transformer trained on Common Crawl.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02246" title="Abstract">arXiv:2310.02246</a> [<a href="/pdf/2310.02246" title="Download PDF">pdf</a>, <a href="/format/2310.02246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Relax: Setting Solver Parameters Across a Sequence of Linear  System Instances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khodak%2C+M">Mikhail Khodak</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+E">Edmond Chow</a>, 
<a href="/search/cs?searchtype=author&query=Balcan%2C+M">Maria-Florina Balcan</a>, 
<a href="/search/cs?searchtype=author&query=Talwalkar%2C+A">Ameet Talwalkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Solving a linear system $Ax=b$ is a fundamental scientific computing
primitive for which numerous solvers and preconditioners have been developed.
These come with parameters whose optimal values depend on the system being
solved and are often impossible or too expensive to identify; thus in practice
sub-optimal heuristics are used. We consider the common setting in which many
related linear systems need to be solved, e.g. during a single numerical
simulation. In this scenario, can we sequentially choose parameters that attain
a near-optimal overall number of iterations, without extra matrix computations?
We answer in the affirmative for Successive Over-Relaxation (SOR), a standard
solver whose parameter $\omega$ has a strong impact on its runtime. For this
method, we prove that a bandit online learning algorithm -- using only the
number of iterations as feedback -- can select parameters for a sequence of
instances such that the overall cost approaches that of the best fixed $\omega$
as the sequence length increases. Furthermore, when given additional structural
information, we show that a contextual bandit method asymptotically achieves
the performance of the instance-optimal policy, which selects the best $\omega$
for each instance. Our work provides the first learning-theoretic treatment of
high-precision linear system solvers and the first end-to-end guarantees for
data-driven scientific computing, demonstrating theoretically the potential to
speed up numerical methods using well-understood learning algorithms.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02247" title="Abstract">arXiv:2310.02247</a> [<a href="/pdf/2310.02247" title="Download PDF">pdf</a>, <a href="/format/2310.02247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Enumeration of Drawings and Combinatorial Structures for  Maximal Planar Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da+Lozzo%2C+G">Giordano Da Lozzo</a>, 
<a href="/search/cs?searchtype=author&query=Di+Battista%2C+G">Giuseppe Di Battista</a>, 
<a href="/search/cs?searchtype=author&query=Frati%2C+F">Fabrizio Frati</a>, 
<a href="/search/cs?searchtype=author&query=Grosso%2C+F">Fabrizio Grosso</a>, 
<a href="/search/cs?searchtype=author&query=Patrignani%2C+M">Maurizio Patrignani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Combinatorics (math.CO)

</div>
<p class="mathjax">We propose efficient algorithms for enumerating the notorious combinatorial
structures of maximal planar graphs, called canonical orderings and Schnyder
woods, and the related classical graph drawings by de Fraysseix, Pach, and
Pollack [Combinatorica, 1990] and by Schnyder [SODA, 1990], called canonical
drawings and Schnyder drawings, respectively. To this aim (i) we devise an
algorithm for enumerating special $e$-bipolar orientations of maximal planar
graphs, called canonical orientations; (ii) we establish bijections between
canonical orientations and canonical drawings, and between canonical
orientations and Schnyder drawings; and (iii) we exploit the known
correspondence between canonical orientations and canonical orderings, and the
known bijection between canonical orientations and Schnyder woods. All our
enumeration algorithms have $O(n)$ setup time, space usage, and delay between
any two consecutively listed outputs, for an $n$-vertex maximal planar graph.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02249" title="Abstract">arXiv:2310.02249</a> [<a href="/pdf/2310.02249" title="Download PDF">pdf</a>, <a href="/format/2310.02249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Pre-Trained Sentence Transformers for Offensive Language  Detection in Indian Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joshi%2C+A">Ananya Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+R">Raviraj Joshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> HASOC at FIRE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In our increasingly interconnected digital world, social media platforms have
emerged as powerful channels for the dissemination of hate speech and offensive
content. This work delves into the domain of hate speech detection, placing
specific emphasis on three low-resource Indian languages: Bengali, Assamese,
and Gujarati. The challenge is framed as a text classification task, aimed at
discerning whether a tweet contains offensive or non-offensive content.
Leveraging the HASOC 2023 datasets, we fine-tuned pre-trained BERT and SBERT
models to evaluate their effectiveness in identifying hate speech. Our findings
underscore the superiority of monolingual sentence-BERT models, particularly in
the Bengali language, where we achieved the highest ranking. However, the
performance in Assamese and Gujarati languages signifies ongoing opportunities
for enhancement. Our goal is to foster inclusive online spaces by countering
hate speech proliferation.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02250" title="Abstract">arXiv:2310.02250</a> [<a href="/pdf/2310.02250" title="Download PDF">pdf</a>, <a href="/format/2310.02250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why do autoencoders work?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kvalheim%2C+M+D">Matthew D. Kvalheim</a>, 
<a href="/search/cs?searchtype=author&query=Sontag%2C+E+D">Eduardo D. Sontag</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep neural network autoencoders are routinely used computationally for model
reduction. They allow recognizing the intrinsic dimension of data that lie in a
$k$-dimensional subset $K$ of an input Euclidean space $\R^n$. The underlying
idea is to obtain both an encoding layer that maps $\R^n$ into $\R^k$ (called
the bottleneck layer or the space of latent variables) and a decoding layer
that maps $\R^k$ back into $\R^n$, in such a way that the input data from the
set $K$ is recovered when composing the two maps. This is achieved by adjusting
parameters (weights) in the network to minimize the discrepancy between the
input and the reconstructed output. Since neural networks (with continuous
activation functions) compute continuous maps, the existence of a network that
achieves perfect reconstruction would imply that $K$ is homeomorphic to a
$k$-dimensional subset of $\R^k$, so clearly there are topological obstructions
to finding such a network. On the other hand, in practice the technique is
found to ``work'' well, which leads one to ask if there is a way to explain
this effectiveness. We show that, up to small errors, indeed the method is
guaranteed to work. This is done by appealing to certain facts from
differential geometry. A computational example is also included to illustrate
the ideas.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02251" title="Abstract">arXiv:2310.02251</a> [<a href="/pdf/2310.02251" title="Download PDF">pdf</a>, <a href="/format/2310.02251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Talk2BEV: Language-enhanced Bird&#x27;s-eye View Maps for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dewangan%2C+V">Vikrant Dewangan</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+T">Tushar Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Chandhok%2C+S">Shivam Chandhok</a>, 
<a href="/search/cs?searchtype=author&query=Priyadarshan%2C+S">Shubham Priyadarshan</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Anushka Jain</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Arun K. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Siddharth Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Jatavallabhula%2C+K+M">Krishna Murthy Jatavallabhula</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+K+M">K. Madhava Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024. Project page at <a href="https://llmbev.github.io/talk2bev/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye
view (BEV) maps in autonomous driving contexts. While existing perception
systems for autonomous driving scenarios have largely focused on a pre-defined
(closed) set of object categories and driving scenarios, Talk2BEV blends recent
advances in general-purpose language and vision models with BEV-structured map
representations, eliminating the need for task-specific models. This enables a
single system to cater to a variety of autonomous driving tasks encompassing
visual and spatial reasoning, predicting the intents of traffic actors, and
decision-making based on visual cues. We extensively evaluate Talk2BEV on a
large number of scene understanding tasks that rely on both the ability to
interpret free-form natural language queries, and in grounding these queries to
the visual context embedded into the language-enhanced BEV map. To enable
further research in LVLMs for autonomous driving scenarios, we develop and
release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV
scenarios, with more than 20,000 questions and ground-truth responses from the
NuScenes dataset.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02255" title="Abstract">arXiv:2310.02255</a> [<a href="/pdf/2310.02255" title="Download PDF">pdf</a>, <a href="/format/2310.02255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MathVista: Evaluating Mathematical Reasoning of Foundation Models in  Visual Contexts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+H">Hritik Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+T">Tony Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages, 56 figures. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although Large Language Models (LLMs) and Large Multimodal Models (LMMs)
exhibit impressive skills in various domains, their ability for mathematical
reasoning within visual contexts has not been formally examined. Equipping LLMs
and LMMs with this capability is vital for general-purpose AI assistants and
showcases promising potential in education, data analysis, and scientific
discovery. To bridge this gap, we present MathVista, a benchmark designed to
amalgamate challenges from diverse mathematical and visual tasks. We first
taxonomize the key task types, reasoning skills, and visual contexts from the
literature to guide our selection from 28 existing math-focused and visual
question answering datasets. Then, we construct three new datasets, IQTest,
FunctionQA, and PaperQA, to accommodate for missing types of visual contexts.
The problems featured often require deep visual understanding beyond OCR or
image captioning, and compositional reasoning with rich domain-specific tools,
thus posing a notable challenge to existing models. We conduct a comprehensive
evaluation of 11 prominent open-source and proprietary foundation models (LLMs,
LLMs augmented with tools, and LMMs), and early experiments with GPT-4V. The
best-performing model, Multimodal Bard, achieves only 58% of human performance
(34.8% vs 60.3%), indicating ample room for further improvement. Given this
significant gap, MathVista fuels future research in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. Preliminary tests show that MathVista also
presents challenges to GPT-4V, underscoring the benchmark's importance. The
project is available at https://mathvista.github.io/.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02258" title="Abstract">arXiv:2310.02258</a> [<a href="/pdf/2310.02258" title="Download PDF">pdf</a>, <a href="/format/2310.02258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neural Scaling Law from Lottery Ticket Ensembling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)

</div>
<p class="mathjax">Neural scaling laws (NSL) refer to the phenomenon where model performance
improves with scale. Sharma &amp; Kaplan analyzed NSL using approximation theory
and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is
the number of model parameters, and $d$ is the intrinsic input dimension.
Although their theory works well for some cases (e.g., ReLU networks), we
surprisingly find that a simple 1D problem $y=x^2$ manifests a different
scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the
neural networks and found that the new scaling law originates from lottery
ticket ensembling: a wider network on average has more "lottery tickets", which
are ensembled to reduce the variance of outputs. We support the ensembling
mechanism by mechanistically interpreting single neural networks, as well as
studying them statistically. We attribute the $N^{-1}$ scaling law to the
"central limit theorem" of lottery tickets. Finally, we discuss its potential
implications for large language models and statistical physics-type theories of
learning.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02260" title="Abstract">arXiv:2310.02260</a> [<a href="/pdf/2310.02260" title="Download PDF">pdf</a>, <a href="/format/2310.02260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View  Radar Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalbah%2C+Y">Yahia Dalbah</a>, 
<a href="/search/cs?searchtype=author&query=Lahoud%2C+J">Jean Lahoud</a>, 
<a href="/search/cs?searchtype=author&query=Cholakkal%2C+H">Hisham Cholakkal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Scene understanding plays an essential role in enabling autonomous driving
and maintaining high standards of performance and safety. To address this task,
cameras and laser scanners (LiDARs) have been the most commonly used sensors,
with radars being less popular. Despite that, radars remain low-cost,
information-dense, and fast-sensing techniques that are resistant to adverse
weather conditions. While multiple works have been previously presented for
radar-based scene semantic segmentation, the nature of the radar data still
poses a challenge due to the inherent noise and sparsity, as well as the
disproportionate foreground and background. In this work, we propose a novel
approach to the semantic segmentation of radar scenes using a multi-input
fusion of radar data through a novel architecture and loss functions that are
tailored to tackle the drawbacks of radar perception. Our novel architecture
includes an efficient attention block that adaptively captures important
feature information. Our method, TransRadar, outperforms state-of-the-art
methods on the CARRADA and RADIal datasets while having smaller model sizes.
https://github.com/YahiDar/TransRadar
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02262" title="Abstract">arXiv:2310.02262</a> [<a href="/pdf/2310.02262" title="Download PDF">pdf</a>, <a href="/format/2310.02262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and  Comfortable Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenfeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wei Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yintao Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Robotics (cs.RO)

</div>
<p class="mathjax">This paper addresses the growing demands for safety and comfort in
intelligent robot systems, particularly autonomous vehicles, where road
conditions play a pivotal role in overall driving performance. For example,
reconstructing road surfaces helps to enhance the analysis and prediction of
vehicle responses for motion planning and control systems. We introduce the
Road Surface Reconstruction Dataset (RSRD), a real-world, high-resolution, and
high-precision dataset collected with a specialized platform in diverse driving
conditions. It covers common road types containing approximately 16,000 pairs
of stereo images, original point clouds, and ground-truth depth/disparity maps,
with accurate post-processing pipelines to ensure its quality. Based on RSRD,
we further build a comprehensive benchmark for recovering road profiles through
depth estimation and stereo matching. Preliminary evaluations with various
state-of-the-art methods reveal the effectiveness of our dataset and the
challenge of the task, underscoring substantial opportunities of RSRD as a
valuable resource for advancing techniques, e.g., multi-view stereo towards
safe autonomous driving. The dataset and demo videos are available at
https://thu-rsxd.com/rsrd/
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02263" title="Abstract">arXiv:2310.02263</a> [<a href="/pdf/2310.02263" title="Download PDF">pdf</a>, <a href="/format/2310.02263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Post-training Large Language Models on Data Curriculum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Canwen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Rosset%2C+C">Corby Rosset</a>, 
<a href="/search/cs?searchtype=author&query=Del+Corro%2C+L">Luciano Del Corro</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+S">Shweti Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Neville%2C+J">Jennifer Neville</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A+H">Ahmed Hassan Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+N">Nikhil Rao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Alignment serves as an important step to steer large language models (LLMs)
towards human preferences. In this paper, we explore contrastive post-training
techniques for alignment by automatically constructing preference pairs from
multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We
carefully compare the contrastive techniques of SLiC and DPO to SFT baselines
and find that DPO provides a step-function improvement even after continueing
SFT saturates. We also explore a data curriculum learning scheme for
contrastive post-training, which starts by learning from "easier" pairs and
transitioning to "harder" ones, which further improves alignment. Finally, we
scale up our experiments to train with more data and larger models like Orca.
Remarkably, contrastive post-training further improves the performance of Orca,
already a state-of-the-art instruction learning model tuned with GPT-4 outputs,
to exceed that of ChatGPT.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02264" title="Abstract">arXiv:2310.02264</a> [<a href="/pdf/2310.02264" title="Download PDF">pdf</a>, <a href="/format/2310.02264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Long-Horizon Manipulations with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haoyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Mingyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Weikun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Tomizuka%2C+M">Masayoshi Tomizuka</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Lin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+C">Chuang Gan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">This work introduces a framework harnessing the capabilities of Large
Language Models (LLMs) to generate primitive task conditions for generalizable
long-horizon manipulations with novel objects and unseen tasks. These task
conditions serve as guides for the generation and adjustment of Dynamic
Movement Primitives (DMP) trajectories for long-horizon task execution. We
further create a challenging robotic manipulation task suite based on Pybullet
for long-horizon task evaluation. Extensive experiments in both simulated and
real-world environments demonstrate the effectiveness of our framework on both
familiar tasks involving new objects and novel but related tasks, highlighting
the potential of LLMs in enhancing robotic system versatility and adaptability.
Project website: https://object814.github.io/Task-Condition-With-LLM/
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02265" title="Abstract">arXiv:2310.02265</a> [<a href="/pdf/2310.02265" title="Download PDF">pdf</a>, <a href="/format/2310.02265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DREAM: Visual Decoding from Reversing Human Visual System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Weihao Xia</a>, 
<a href="/search/cs?searchtype=author&query=de+Charette%2C+R">Raoul de Charette</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96ztireli%2C+C">Cengiz &#xd6;ztireli</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jing-Hao Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://weihaox.github.io/DREAM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In this work we present DREAM, an fMRI-to-image method for reconstructing
viewed images from brain activities, grounded on fundamental knowledge of the
human visual system. We craft reverse pathways that emulate the hierarchical
and parallel nature of how humans perceive the visual world. These tailored
pathways are specialized to decipher semantics, color, and depth cues from fMRI
data, mirroring the forward pathways from visual stimuli to fMRI recordings. To
do so, two components mimic the inverse processes within the human visual
system: the Reverse Visual Association Cortex (R-VAC) which reverses pathways
of this brain region, extracting semantics from fMRI data; the Reverse Parallel
PKM (R-PKM) component simultaneously predicting color and depth from fMRI
signals. The experiments indicate that our method outperforms the current
state-of-the-art models in terms of the consistency of appearance, structure,
and semantics. Code will be made publicly available to facilitate further
research in this field.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed,  4 Oct 23</h3>
<dl>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00397" title="Abstract">arXiv:2310.00397</a> (cross-list from math.OC) [<a href="/pdf/2310.00397" title="Download PDF">pdf</a>, <a href="/format/2310.00397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A First-Order Method with Expansive Projection for Optimal Powered  Descent Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Choi%2C+J">Jiwoo Choi</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+J">Jong-Han Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper introduces a first-order method for solving optimal powered
descent guidance (PDG) problems, that directly handles the nonconvex
constraints associated with the maximum and minimum thrust bounds with varying
mass and the pointing angle constraints on thrust vectors. This issue has been
conventionally circumvented via lossless convexification (LCvx), which lifts a
nonconvex feasible set to a higher-dimensional convex set, and via linear
approximation of another nonconvex feasible set defined by exponential
functions. However, this approach sometimes results in an infeasible solution
when the solution obtained from the higher-dimensional space is projected back
to the original space, especially when the problem involves a nonoptimal time
of flight. Additionally, the Taylor series approximation introduces an
approximation error that grows with both flight time and deviation from the
reference trajectory. In this paper, we introduce a first-order approach that
makes use of orthogonal projections onto nonconvex sets, allowing expansive
projection (ExProj). We show that 1) this approach produces a feasible solution
with better performance even for the nonoptimal time of flight cases for which
conventional techniques fail and 2) the proposed method compensates for the
linearization error that arises from Taylor series approximation. We claim that
the proposed approach offers more flexibility in generating feasible
trajectories for a wide variety of planetary soft landing problems.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00398" title="Abstract">arXiv:2310.00398</a> (cross-list from math.OC) [<a href="/pdf/2310.00398" title="Download PDF">pdf</a>, <a href="/format/2310.00398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Impact Angle Guidance via First-Order Optimization Under  Nonconvex Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+G">Gyubin Park</a>, 
<a href="/search/math?searchtype=author&query=Jeong%2C+D+H">Da Hoon Jeong</a>, 
<a href="/search/math?searchtype=author&query=Kim%2C+J">Jong-Han Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Most optimal guidance problems can be formulated as nonconvex optimization
problems, which can be solved indirectly by relaxation, convexification, or
linearization. Although these methods are guaranteed to converge to the global
optimum of the modified problems, the obtained solution may not guarantee
global optimality or even the feasibility of the original nonconvex problems.
In this paper, we propose a computational optimal guidance approach that
directly handles the nonconvex constraints encountered in formulating guidance
problems. The proposed computational guidance approach alternately solves the
least squares problem and projects the solution onto nonconvex feasible sets,
which rapidly converge to feasible suboptimal solutions or, sometimes, to
globally optimal solutions. The proposed algorithm is verified via a series of
numerical simulations on impact angle guidance problems, and it is demonstrated
that the proposed algorithm provides superior guidance performance compared to
conventional techniques.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01416" title="Abstract">arXiv:2310.01416</a> (cross-list from eess.IV) [<a href="/pdf/2310.01416" title="Download PDF">pdf</a>, <a href="/format/2310.01416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gramian Angular Fields for leveraging pretrained computer vision models  with anomalous diffusion trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Garibo-i-Orts%2C+%C3%92">&#xd2;scar Garibo-i-Orts</a>, 
<a href="/search/eess?searchtype=author&query=Firbas%2C+N">Nicol&#xe1;s Firbas</a>, 
<a href="/search/eess?searchtype=author&query=Sebasti%C3%A1%2C+L">Laura Sebasti&#xe1;</a>, 
<a href="/search/eess?searchtype=author&query=Conejero%2C+J+A">J. Alberto Conejero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 15 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PhysRevE.107.034138 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Biological Physics (physics.bio-ph); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Anomalous diffusion is present at all scales, from atomic to large scales.
Some exemplary systems are; ultra-cold atoms, telomeres in the nucleus of
cells, moisture transport in cement-based materials, the free movement of
arthropods, and the migration patterns of birds. The characterization of the
diffusion gives critical information about the dynamics of these systems and
provides an interdisciplinary framework with which to study diffusive
transport. Thus, the problem of identifying underlying diffusive regimes and
inferring the anomalous diffusion exponent {$\alpha$} with high confidence is
critical to physics, chemistry, biology, and ecology. Classification and
analysis of raw trajectories combining machine learning techniques with
statistics extracted from them have widely been studied in the Anomalous
Diffusion Challenge ge (Munoz-Gil et al., 2021). Here we present a new
data-driven method for working with diffusive trajectories. This method
utilizes Gramian Angular Fields (GAF) to encode one-dimensional trajectories as
images (Gramian Matrices), while preserving their spatiotemporal structure for
input to computer-vision models. This allows us to leverage two
well-established pre-trained computer-vision models, ResNet and MobileNet, to
characterize the underlying diffusive regime, and infer the anomalous diffusion
exponent {$\alpha$}. Short raw trajectories, of lengths between 10 and 50, are
commonly encountered in single-particle tracking experiments and are the most
difficult to characterize. We show that by using GAF images, we can outperform
the current state-of-the-art while increasing accessibility to machine learning
methods in an applied setting.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01426" title="Abstract">arXiv:2310.01426</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.01426" title="Download PDF">pdf</a>, <a href="/format/2310.01426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REMEDI: REinforcement learning-driven adaptive MEtabolism modeling of  primary sclerosing cholangitis DIsease progression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Hu%2C+C">Chang Hu</a>, 
<a href="/search/q-bio?searchtype=author&query=Saboo%2C+K+V">Krishnakant V. Saboo</a>, 
<a href="/search/q-bio?searchtype=author&query=Ali%2C+A+H">Ahmad H. Ali</a>, 
<a href="/search/q-bio?searchtype=author&query=Juran%2C+B+D">Brian D. Juran</a>, 
<a href="/search/q-bio?searchtype=author&query=Lazaridis%2C+K+N">Konstantinos N. Lazaridis</a>, 
<a href="/search/q-bio?searchtype=author&query=Iyer%2C+R+K">Ravishankar K. Iyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 4 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Primary sclerosing cholangitis (PSC) is a rare disease wherein altered bile
acid metabolism contributes to sustained liver injury. This paper introduces
REMEDI, a framework that captures bile acid dynamics and the body's adaptive
response during PSC progression that can assist in exploring treatments. REMEDI
merges a differential equation (DE)-based mechanistic model that describes bile
acid metabolism with reinforcement learning (RL) to emulate the body's
adaptations to PSC continuously. An objective of adaptation is to maintain
homeostasis by regulating enzymes involved in bile acid metabolism. These
enzymes correspond to the parameters of the DEs. REMEDI leverages RL to
approximate adaptations in PSC, treating homeostasis as a reward signal and the
adjustment of the DE parameters as the corresponding actions. On real-world
data, REMEDI generated bile acid dynamics and parameter adjustments consistent
with published findings. Also, our results support discussions in the
literature that early administration of drugs that suppress bile acid synthesis
may be effective in PSC treatment.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01431" title="Abstract">arXiv:2310.01431</a> (cross-list from quant-ph) [<a href="/pdf/2310.01431" title="Download PDF">pdf</a>, <a href="/format/2310.01431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NISQ Computers: A Path to Quantum Supremacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=AbuGhanem%2C+M">Muhammad AbuGhanem</a>, 
<a href="/search/quant-ph?searchtype=author&query=Eleuch%2C+H">Hichem Eleuch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">The quest for quantum advantage, wherein quantum computers surpass the
computational capabilities of classical computers executing state-of-the-art
algorithms on well-defined tasks, represents a pivotal race in the domain of
quantum computing. NISQ (Noisy Intermediate-Scale Quantum) computing has
witnessed remarkable advancements, culminating in significant milestones on the
journey towards the realization of universal fault-tolerant quantum computers.
This transformative turning point, known as quantum supremacy, has been
achieved amid a series of breakthroughs, signifying the dawn of the quantum
era. Quantum hardware has undergone substantial integration and architectural
evolution, contrasting with its nascent stages. In this review, we critically
examine the quantum supremacy experiments conducted thus far, shedding light on
their implications and contributions to the evolving landscape of quantum
computing. Additionally, we endeavor to illuminate a range of cutting-edge
proof-of-principle investigations in the realm of applied quantum computing,
providing an insightful overview of the current state of applied quantum
research and its prospective influence across diverse scientific, industrial,
and technological frontiers.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01433" title="Abstract">arXiv:2310.01433</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.01433" title="Download PDF">pdf</a>, <a href="/format/2310.01433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box  Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Daryakenari%2C+N+A">Nazanin Ahmadi Daryakenari</a>, 
<a href="/search/q-bio?searchtype=author&query=De+Florio%2C+M">Mario De Florio</a>, 
<a href="/search/q-bio?searchtype=author&query=Shukla%2C+K">Khemraj Shukla</a>, 
<a href="/search/q-bio?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Discovering mathematical equations that govern physical and biological
systems from observed data is a fundamental challenge in scientific research.
We present a new physics-informed framework for parameter estimation and
missing physics identification (gray-box) in the field of Systems Biology. The
proposed framework -- named AI-Aristotle -- combines eXtreme Theory of
Functional Connections (X-TFC) domain-decomposition and Physics-Informed Neural
Networks (PINNs) with symbolic regression (SR) techniques for parameter
discovery and gray-box identification. We test the accuracy, speed, flexibility
and robustness of AI-Aristotle based on two benchmark problems in Systems
Biology: a pharmacokinetics drug absorption model, and an ultradian endocrine
model for glucose-insulin interactions. We compare the two machine learning
methods (X-TFC and PINNs), and moreover, we employ two different symbolic
regression techniques to cross-verify our results. While the current work
focuses on the performance of AI-Aristotle based on synthetic data, it can
equally handle noisy experimental data and can even be used for black-box
identification in just a few minutes on a laptop. More broadly, our work
provides insights into the accuracy, cost, scalability, and robustness of
integrating neural networks with symbolic regressors, offering a comprehensive
guide for researchers tackling gray-box identification challenges in complex
dynamical systems in biomedicine and beyond.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01453" title="Abstract">arXiv:2310.01453</a> (cross-list from eess.SP) [<a href="/pdf/2310.01453" title="Download PDF">pdf</a>, <a href="/format/2310.01453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Secrecy Capacity in PLS Communication with NORAN based on  Pilot Information Codebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gu%2C+Y">Yebo Gu</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+T">Tao Shen</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+J">Jian Song</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qingbo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In recent research, non-orthogonal artificial noise (NORAN) has been proposed
as an alternative to orthogonal artificial noise (AN). However, NORAN
introduces additional noise into the channel, which reduces the capacity of the
legitimate channel (LC). At the same time, selecting a NORAN design with ideal
security performance from a large number of design options is also a
challenging problem. To address these two issues, a novel NORAN based on a
pilot information codebook is proposed in this letter. The codebook associates
different suboptimal NORANs with pilot information as the key under different
channel state information (CSI). The receiver interrogates the codebook using
the pilot information to obtain the NORAN that the transmitter will transmit in
the next moment, in order to eliminate the NORAN when receiving information.
Therefore, NORAN based on pilot information codebooks can improve the secrecy
capacity (SC) of the communication system by directly using suboptimal NORAN
design schemes without increasing the noise in the LC. Numerical simulations
and analyses show that the introduction of NORAN with a novel design using
pilot information codebooks significantly enhances the security and improves
the SC of the communication system.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01455" title="Abstract">arXiv:2310.01455</a> (cross-list from physics.plasm-ph) [<a href="/pdf/2310.01455" title="Download PDF">pdf</a>, <a href="/format/2310.01455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shaping of Magnetic Field Coils in Fusion Reactors using Bayesian  Optimisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Nunn%2C+T">Timothy Nunn</a>, 
<a href="/search/physics?searchtype=author&query=Gopakumar%2C+V">Vignesh Gopakumar</a>, 
<a href="/search/physics?searchtype=author&query=Kahn%2C+S">Sebastien Kahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2022 Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Nuclear fusion using magnetic confinement holds promise as a viable method
for sustainable energy. However, most fusion devices have been experimental and
as we move towards energy reactors, we are entering into a new paradigm of
engineering. Curating a design for a fusion reactor is a high-dimensional
multi-output optimisation process. Through this work we demonstrate a
proof-of-concept of an AI-driven strategy to help explore the design search
space and identify optimum parameters. By utilising a Multi-Output Bayesian
Optimisation scheme, our strategy is capable of identifying the Pareto front
associated with the optimisation of the toroidal field coil shape of a tokamak.
The optimisation helps to identify design parameters that would minimise the
costs incurred while maximising the plasma stability by way of minimising
magnetic ripples.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01515" title="Abstract">arXiv:2310.01515</a> (cross-list from quant-ph) [<a href="/pdf/2310.01515" title="Download PDF">pdf</a>, <a href="/format/2310.01515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Ring Optimized Quantum-Enhanced Tensor Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Konar%2C+D">Debanjan Konar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Peddireddy%2C+D">Dheeraj Peddireddy</a>, 
<a href="/search/quant-ph?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>, 
<a href="/search/quant-ph?searchtype=author&query=Panigrahi%2C+B+K">Bijaya K. Panigrahi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum machine learning researchers often rely on incorporating Tensor
Networks (TN) into Deep Neural Networks (DNN) and variational optimization.
However, the standard optimization techniques used for training the contracted
trainable weights of each model layer suffer from the correlations and
entanglement structure between the model parameters on classical
implementations. To address this issue, a multi-layer design of a Tensor Ring
optimized variational Quantum learning classifier (Quan-TR) comprising
cascading entangling gates replacing the fully connected (dense) layers of a TN
is proposed, and it is referred to as Tensor Ring optimized Quantum-enhanced
tensor neural Networks (TR-QNet). TR-QNet parameters are optimized through the
stochastic gradient descent algorithm on qubit measurements. The proposed
TR-QNet is assessed on three distinct datasets, namely Iris, MNIST, and
CIFAR-10, to demonstrate the enhanced precision achieved for binary
classification. On quantum simulations, the proposed TR-QNet achieves promising
accuracy of $94.5\%$, $86.16\%$, and $83.54\%$ on the Iris, MNIST, and CIFAR-10
datasets, respectively. Benchmark studies have been conducted on
state-of-the-art quantum and classical implementations of TN models to show the
efficacy of the proposed TR-QNet. Moreover, the scalability of TR-QNet
highlights its potential for exhibiting in deep learning applications on a
large scale. The PyTorch implementation of TR-QNet is available on
Github:https://github.com/konar1987/TR-QNet/
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01523" title="Abstract">arXiv:2310.01523</a> (cross-list from eess.IV) [<a href="/pdf/2310.01523" title="Download PDF">pdf</a>, <a href="/format/2310.01523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fetal-BET: Brain Extraction Tool for Fetal MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Faghihpirayesh%2C+R">Razieh Faghihpirayesh</a>, 
<a href="/search/eess?searchtype=author&query=Karimi%2C+D">Davood Karimi</a>, 
<a href="/search/eess?searchtype=author&query=Erdo%C4%9Fmu%C5%9F%2C+D">Deniz Erdo&#x11f;mu&#x15f;</a>, 
<a href="/search/eess?searchtype=author&query=Gholipour%2C+A">Ali Gholipour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, 2 TABLES, This work has been submitted to the IEEE Transactions on Medical Imaging for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Fetal brain extraction is a necessary first step in most computational fetal
brain MRI pipelines. However, it has been a very challenging task due to
non-standard fetal head pose, fetal movements during examination, and vastly
heterogeneous appearance of the developing fetal brain and the neighboring
fetal and maternal anatomy across various sequences and scanning conditions.
Development of a machine learning method to effectively address this task
requires a large and rich labeled dataset that has not been previously
available. As a result, there is currently no method for accurate fetal brain
extraction on various fetal MRI sequences. In this work, we first built a large
annotated dataset of approximately 72,000 2D fetal brain MRI images. Our
dataset covers the three common MRI sequences including T2-weighted,
diffusion-weighted, and functional MRI acquired with different scanners.
Moreover, it includes normal and pathological brains. Using this dataset, we
developed and validated deep learning methods, by exploiting the power of the
U-Net style architectures, the attention mechanism, multi-contrast feature
learning, and data augmentation for fast, accurate, and generalizable automatic
fetal brain extraction. Our approach leverages the rich information from
multi-contrast (multi-sequence) fetal MRI data, enabling precise delineation of
the fetal brain structures. Evaluations on independent test data show that our
method achieves accurate brain extraction on heterogeneous test data acquired
with different scanners, on pathological brains, and at various gestational
stages. This robustness underscores the potential utility of our deep learning
model for fetal brain imaging and image analysis.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01540" title="Abstract">arXiv:2310.01540</a> (cross-list from quant-ph) [<a href="/pdf/2310.01540" title="Download PDF">pdf</a>, <a href="/format/2310.01540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the power of geometrically-local classical and quantum circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bharti%2C+K">Kishor Bharti</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jain%2C+R">Rahul Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We show a relation, based on parallel repetition of the Magic Square game,
that can be solved, with probability exponentially close to $1$ (worst-case
input), by $1D$ (uniform) depth $2$, geometrically-local, noisy (noise below a
threshold), fan-in $4$, quantum circuits. We show that the same relation cannot
be solved, with an exponentially small success probability (averaged over
inputs drawn uniformly), by $1D$ (non-uniform) geometrically-local, sub-linear
depth, classical circuits consisting of fan-in $2$ NAND gates. Quantum and
classical circuits are allowed to use input-independent
(geometrically-non-local) resource states, that is entanglement and randomness
respectively. To the best of our knowledge, previous best (analogous) depth
separation for a task between quantum and classical circuits was constant v/s
sub-logarithmic, although for general (geometrically non-local) circuits. Our
hardness result for classical circuits is based on a direct product theorem
about classical communication protocols from Jain and Kundu [JK22]. As an
application, we propose a protocol that can potentially demonstrate verifiable
quantum advantage in the NISQ era. We also provide generalizations of our
result for higher dimensional circuits as well as a wider class of Bell games.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01547" title="Abstract">arXiv:2310.01547</a> (cross-list from math.ST) [<a href="/pdf/2310.01547" title="Download PDF">pdf</a>, <a href="/format/2310.01547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the near-optimality of betting confidence sets for bounded means
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shekhar%2C+S">Shubhanshu Shekhar</a>, 
<a href="/search/math?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Constructing nonasymptotic confidence intervals (CIs) for the mean of a
univariate distribution from independent and identically distributed (i.i.d.)
observations is a fundamental task in statistics. For bounded observations, a
classical nonparametric approach proceeds by inverting standard concentration
bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an
alternative betting-based approach for defining CIs and their time-uniform
variants called confidence sequences (CSs), has been shown to be empirically
superior to the classical methods. In this paper, we provide theoretical
justification for this improved empirical performance of betting CIs and CSs.
<br />Our main contributions are as follows: (i) We first compare CIs using the
values of their first-order asymptotic widths (scaled by $\sqrt{n}$), and show
that the betting CI of Waudby-Smith and Ramdas (2023) has a smaller limiting
width than existing empirical Bernstein (EB)-CIs. (ii) Next, we establish two
lower bounds that characterize the minimum width achievable by any method for
constructing CIs/CSs in terms of certain inverse information projections. (iii)
Finally, we show that the betting CI and CS match the fundamental limits,
modulo an additive logarithmic term and a multiplicative constant. Overall
these results imply that the betting CI~(and CS) admit stronger theoretical
guarantees than the existing state-of-the-art EB-CI~(and CS); both in the
asymptotic and finite-sample regimes.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01563" title="Abstract">arXiv:2310.01563</a> (cross-list from quant-ph) [<a href="/pdf/2310.01563" title="Download PDF">pdf</a>, <a href="/format/2310.01563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local algorithms and the failure of log-depth quantum advantage on  sparse random CSPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+A">Antares Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+N">Neng Huang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Marwaha%2C+K">Kunal Marwaha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29+13 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS); Probability (math.PR)

</div>
<p class="mathjax">We construct and analyze a message-passing algorithm for random constraint
satisfaction problems (CSPs) at large clause density, generalizing work of El
Alaoui, Montanari, and Sellke for Maximum Cut [<a href="/abs/2111.06813">arXiv:2111.06813</a>] through a
connection between random CSPs and mean-field Ising spin glasses. For CSPs with
even predicates, the algorithm asymptotically solves a stochastic optimal
control problem dual to an extended Parisi variational principle. This gives an
optimal fraction of satisfied constraints among algorithms obstructed by the
branching overlap gap property of Huang and Sellke [<a href="/abs/2110.07847">arXiv:2110.07847</a>], notably
including the Quantum Approximate Optimization Algorithm and all quantum
circuits on a bounded-degree architecture of up to $\epsilon \cdot \log n$
depth.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01583" title="Abstract">arXiv:2310.01583</a> (cross-list from stat.ML) [<a href="/pdf/2310.01583" title="Download PDF">pdf</a>, <a href="/format/2310.01583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation of Representation and Allocation Harms in Contrastive  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Maity%2C+S">Subha Maity</a>, 
<a href="/search/stat?searchtype=author&query=Agarwal%2C+M">Mayank Agarwal</a>, 
<a href="/search/stat?searchtype=author&query=Yurochkin%2C+M">Mikhail Yurochkin</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+Y">Yuekai Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The effect of underrepresentation on the performance of minority groups is
known to be a serious problem in supervised learning settings; however, it has
been underexplored so far in the context of self-supervised learning (SSL). In
this paper, we demonstrate that contrastive learning (CL), a popular variant of
SSL, tends to collapse representations of minority groups with certain majority
groups. We refer to this phenomenon as representation harm and demonstrate it
on image and text datasets using the corresponding popular CL methods.
Furthermore, our causal mediation analysis of allocation harm on a downstream
classification task reveals that representation harm is partly responsible for
it, thus emphasizing the importance of studying and mitigating representation
harm. Finally, we provide a theoretical explanation for representation harm
using a stochastic block model that leads to a representational neural collapse
in a contrastive learning setting.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01609" title="Abstract">arXiv:2310.01609</a> (cross-list from stat.ML) [<a href="/pdf/2310.01609" title="Download PDF">pdf</a>, <a href="/ps/2310.01609" title="Download PostScript">ps</a>, <a href="/format/2310.01609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Contextual Bandits Go Kernelized
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Neu%2C+G">Gergely Neu</a>, 
<a href="/search/stat?searchtype=author&query=Olkhovskaya%2C+J">Julia Olkhovskaya</a>, 
<a href="/search/stat?searchtype=author&query=Vakili%2C+S">Sattar Vakili</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study a generalization of the problem of online learning in adversarial
linear contextual bandits by incorporating loss functions that belong to a
reproducing kernel Hilbert space, which allows for a more flexible modeling of
complex decision-making scenarios. We propose a computationally efficient
algorithm that makes use of a new optimistically biased estimator for the loss
functions and achieves near-optimal regret guarantees under a variety of
eigenvalue decay assumptions made on the underlying kernel. Specifically, under
the assumption of polynomial eigendecay with exponent $c&gt;1$, the regret is
$\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$, where $T$ denotes the number
of rounds and $K$ the number of actions. Furthermore, when the eigendecay
follows an exponential pattern, we achieve an even tighter regret bound of
$\widetilde{O}(\sqrt{T})$. These rates match the lower bounds in all special
cases where lower bounds are known at all, and match the best known upper
bounds available for the more well-studied stochastic counterpart of our
problem.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01630" title="Abstract">arXiv:2310.01630</a> (cross-list from quant-ph) [<a href="/pdf/2310.01630" title="Download PDF">pdf</a>, <a href="/format/2310.01630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inter-temperature Bandwidth Reduction in Cryogenic QAOA Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Ueno%2C+Y">Yosuke Ueno</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tomida%2C+Y">Yuna Tomida</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tanimoto%2C+T">Teruo Tanimoto</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tanaka%2C+M">Masamitsu Tanaka</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tabuchi%2C+Y">Yutaka Tabuchi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Inoue%2C+K">Koji Inoue</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nakamura%2C+H">Hiroshi Nakamura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 5 figures, 1 table. Accepted by IEEE Computer Architecture Letters,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">The bandwidth limit between cryogenic and room-temperature environments is a
critical bottleneck in superconducting noisy intermediate-scale quantum
computers. This paper presents the first trial of algorithm-aware system-level
optimization to solve this issue by targeting the quantum approximate
optimization algorithm. Our counter-based cryogenic architecture using
single-flux quantum logic shows exponential bandwidth reduction and decreases
heat inflow and peripheral power consumption of inter-temperature cables, which
contributes to the scalability of superconducting quantum computers.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01671" title="Abstract">arXiv:2310.01671</a> (cross-list from physics.med-ph) [<a href="/pdf/2310.01671" title="Download PDF">pdf</a>, <a href="/format/2310.01671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A directional regularization method for the limited-angle Helsinki  Tomography Challenge using the Core Imaging Library (CIL)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=J%C3%B8rgensen%2C+J+S">Jakob Sauer J&#xf8;rgensen</a>, 
<a href="/search/physics?searchtype=author&query=Papoutsellis%2C+E">Evangelos Papoutsellis</a>, 
<a href="/search/physics?searchtype=author&query=Murgatroyd%2C+L">Laura Murgatroyd</a>, 
<a href="/search/physics?searchtype=author&query=Fardell%2C+G">Gemma Fardell</a>, 
<a href="/search/physics?searchtype=author&query=Pasca%2C+E">Edoardo Pasca</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Mathematical Software (cs.MS); Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
<p class="mathjax">This article presents the algorithms developed by the Core Imaging Library
(CIL) developer team for the Helsinki Tomography Challenge 2022. The challenge
focused on reconstructing 2D phantom shapes from limited-angle computed
tomography (CT) data. The CIL team designed and implemented five reconstruction
methods using CIL (https://ccpi.ac.uk/cil/), an open-source Python package for
tomographic imaging. The CIL team adopted a model-based reconstruction
strategy, unique to this challenge with all other teams relying on
deep-learning techniques. The CIL algorithms showcased exceptional performance,
with one algorithm securing the third place in the competition. The
best-performing algorithm employed careful CT data pre-processing and an
optimization problem with single-sided directional total variation
regularization combined with isotropic total variation and tailored lower and
upper bounds. The reconstructions and segmentations achieved high quality for
data with angular ranges down to 50 degrees, and in some cases acceptable
performance even at 40 and 30 degrees. This study highlights the effectiveness
of model-based approaches in limited-angle tomography and emphasizes the
importance of proper algorithmic design leveraging on available prior knowledge
to overcome data limitations. Finally, this study highlights the flexibility of
CIL for prototyping and comparison of different optimization methods.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01678" title="Abstract">arXiv:2310.01678</a> (cross-list from physics.comp-ph) [<a href="/pdf/2310.01678" title="Download PDF">pdf</a>, <a href="/format/2310.01678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score dynamics: scaling molecular dynamics with picosecond timesteps via  conditional diffusion model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Hsu%2C+T">Tim Hsu</a>, 
<a href="/search/physics?searchtype=author&query=Sadigh%2C+B">Babak Sadigh</a>, 
<a href="/search/physics?searchtype=author&query=Bulatov%2C+V">Vasily Bulatov</a>, 
<a href="/search/physics?searchtype=author&query=Zhou%2C+F">Fei Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose score dynamics (SD), a general framework for learning effective
evolution operators for atomistic as well as coarse-grained dynamics from
molecular-dynamics (MD) simulations. SD is centered around scores, or
derivatives of the transition log-probability with respect to the dynamical
degrees of freedom. The latter play the same role as force fields in MD but are
used in denoising diffusion probability models to generate discrete transitions
of the dynamical variables in an SD timestep, which can be orders of magnitude
larger than a typical MD timestep. In this work, we construct graph neural
network based score dynamics models of realistic molecular systems that are
evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with
case studies of alanine dipeptide and short alkanes in aqueous solution. Both
equilibrium predictions derived from the stationary distributions of the
conditional probability and kinetic predictions for the transition rates and
transition paths are in good agreement with MD at about 8-18 fold wall-clock
speedup. Open challenges and possible future remedies to improve score dynamics
are also discussed.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01683" title="Abstract">arXiv:2310.01683</a> (cross-list from stat.ML) [<a href="/pdf/2310.01683" title="Download PDF">pdf</a>, <a href="/format/2310.01683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Commutative Width and Depth Scaling in Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 6 figures. arXiv admin note: substantial text overlap with <a href="/abs/2302.00453">arXiv:2302.00453</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper is the second in the series Commutative Scaling of Width and Depth
(WD) about commutativity of infinite width and depth limits in deep neural
networks. Our aim is to understand the behaviour of neural functions (functions
that depend on a neural network model) as width and depth go to infinity (in
some sense), and eventually identify settings under which commutativity holds,
i.e. the neural function tends to the same limit no matter how width and depth
limits are taken. In this paper, we formally introduce and define the
commutativity framework, and discuss its implications on neural network design
and scaling. We study commutativity for the neural covariance kernel which
reflects how network layers separate data. Our findings extend previous results
established in [55] by showing that taking the width and depth to infinity in a
deep neural network with skip connections, when branches are suitably scaled to
avoid exploding behaviour, result in the same covariance structure no matter
how that limit is taken. This has a number of theoretical and practical
implications that we discuss in the paper. The proof techniques in this paper
are novel and rely on tools that are more accessible to readers who are not
familiar with stochastic calculus (used in the proofs of WD(I))).
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01688" title="Abstract">arXiv:2310.01688</a> (cross-list from eess.AS) [<a href="/pdf/2310.01688" title="Download PDF">pdf</a>, <a href="/format/2310.01688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One model to rule them all ? Towards End-to-End Joint Speaker  Diarization and Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cornell%2C+S">Samuele Cornell</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+J">Jee-weon Jung</a>, 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/eess?searchtype=author&query=Squartini%2C+S">Stefano Squartini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">This paper presents a novel framework for joint speaker diarization (SD) and
automatic speech recognition (ASR), named SLIDAR (sliding-window
diarization-augmented recognition). SLIDAR can process arbitrary length inputs
and can handle any number of speakers, effectively solving ``who spoke what,
when'' concurrently. SLIDAR leverages a sliding window approach and consists of
an end-to-end diarization-augmented speech transcription (E2E DAST) model which
provides, locally, for each window: transcripts, diarization and speaker
embeddings. The E2E DAST model is based on an encoder-decoder architecture and
leverages recent techniques such as serialized output training and
``Whisper-style" prompting. The local outputs are then combined to get the
final SD+ASR result by clustering the speaker embeddings to get global speaker
identities. Experiments performed on monaural recordings from the AMI corpus
confirm the effectiveness of the method in both close-talk and far-field speech
scenarios.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01690" title="Abstract">arXiv:2310.01690</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.01690" title="Download PDF">pdf</a>, <a href="/format/2310.01690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Tropical Cyclones with Cascaded Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Nath%2C+P">Pritthijit Nath</a>, 
<a href="/search/physics?searchtype=author&query=Shukla%2C+P">Pancham Shukla</a>, 
<a href="/search/physics?searchtype=author&query=Quilodr%C3%A1n-Casas%2C+C">C&#xe9;sar Quilodr&#xe1;n-Casas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, Tackling Climate Change with Machine Learning workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As cyclones become more intense due to climate change, the rise of AI-based
modelling provides a more affordable and accessible approach compared to
traditional methods based on mathematical models. This work leverages diffusion
models to forecast cyclone trajectories and precipitation patterns by
integrating satellite imaging, remote sensing, and atmospheric data, employing
a cascaded approach that incorporates forecasting, super-resolution, and
precipitation modelling, with training on a dataset of 51 cyclones from six
major basins. Experiments demonstrate that the final forecasts from the
cascaded models show accurate predictions up to a 36-hour rollout, with SSIM
and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks.
This work also highlights the promising efficiency of AI methods such as
diffusion models for high-performance needs, such as cyclone forecasting, while
remaining computationally affordable, making them ideal for highly vulnerable
regions with critical forecasting needs and financial limitations. Code
accessible at \url{https://github.com/nathzi1505/forecast-diffmodels}.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01756" title="Abstract">arXiv:2310.01756</a> (cross-list from stat.ML) [<a href="/pdf/2310.01756" title="Download PDF">pdf</a>, <a href="/format/2310.01756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Algorithms for Adversarial Bandits with Unbounded Losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+M">Mingyu Chen</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+X">Xuezhou Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the Adversarial Multi-Armed Bandits (MAB) problem with unbounded
losses, where the algorithms have no prior knowledge on the sizes of the
losses. We present UMAB-NN and UMAB-G, two algorithms for non-negative and
general unbounded loss respectively. For non-negative unbounded loss, UMAB-NN
achieves the first adaptive and scale free regret bound without uniform
exploration. Built up on that, we further develop UMAB-G that can learn from
arbitrary unbounded loss. Our analysis reveals the asymmetry between positive
and negative losses in the MAB problem and provide additional insights. We also
accompany our theoretical findings with extensive empirical evaluations,
showing that our algorithms consistently out-performs all existing algorithms
that handles unbounded losses.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01768" title="Abstract">arXiv:2310.01768</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.01768" title="Download PDF">pdf</a>, <a href="/format/2310.01768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backdiff: a diffusion model for generalized transferable protein  backmapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Y">Yikai Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+M">Ming Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Lin%2C+G">Guang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Coarse-grained (CG) models play a crucial role in the study of protein
structures, protein thermodynamic properties, and protein conformation
dynamics. Due to the information loss in the coarse-graining process,
backmapping from CG to all-atom configurations is essential in many protein
design and drug discovery applications when detailed atomic representations are
needed for in-depth studies. Despite recent progress in data-driven backmapping
approaches, devising a backmapping method that can be universally applied
across various CG models and proteins remains unresolved. In this work, we
propose BackDiff, a new generative model designed to achieve generalization and
reliability in the protein backmapping problem. BackDiff leverages the
conditional score-based diffusion model with geometric representations. Since
different CG models can contain different coarse-grained sites which include
selected atoms (CG atoms) and simple CG auxiliary functions of atomistic
coordinates (CG auxiliary variables), we design a self-supervised training
framework to adapt to different CG atoms, and constrain the diffusion sampling
paths with arbitrary CG auxiliary variables as conditions. Our method
facilitates end-to-end training and allows efficient sampling across different
proteins and diverse CG models without the need for retraining. Comprehensive
experiments over multiple popular CG models demonstrate BackDiff's superior
performance to existing state-of-the-art approaches, and generalization and
flexibility that these approaches cannot achieve. A pretrained BackDiff model
can offer a convenient yet reliable plug-and-play solution for protein
researchers, enabling them to investigate further from their own CG models.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01774" title="Abstract">arXiv:2310.01774</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.01774" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mobile digital device proficiency performance test for cognitive  clinical research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Andrade%2C+A+C">Alan Cronemberger Andrade</a>, 
<a href="/search/q-bio?searchtype=author&query=de+Souza+Bido%2C+D">Di&#xf3;genes de Souza Bido</a>, 
<a href="/search/q-bio?searchtype=author&query=de+Barros%2C+A+C+B">Ana Carolina Bottura de Barros</a>, 
<a href="/search/q-bio?searchtype=author&query=Boot%2C+W+R">Walter Richard Boot</a>, 
<a href="/search/q-bio?searchtype=author&query=Bertolucci%2C+P+H+F">Paulo Henrique Ferreira Bertolucci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Human-Computer Interaction (cs.HC); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Mobile device proficiency is increasingly important for everyday living,
including to deliver healthcare services. Human-device interactions represent a
potential in cognitive neurology and aging research. Although traditional
pen-and-paper evaluations serve as valuable tools within public health
strategies for population-scale cognitive assessments, digital devices could
amplify cognitive assessment. However, even person-centered studies often fail
to incorporate measures of mobile device proficiency and research with digital
mobile technology frequently neglects these evaluations. Besides that,
cognitive screening, a fundamental part of brain health evaluation and a widely
accepted strategy to identify high-risk individuals vulnerable to cognitive
impairment and dementia, has research using digital devices for older adults in
need for standardization. To address this shortfall, the DigiTAU collaborative
and interdisciplinary project is creating refined methodological parameters for
the investigation of digital biomarkers. With careful consideration of
cognitive design elements, here we describe the open-source and
performance-based Mobile Device Abilities Test (MDAT), a simple, low-cost, and
reproductible open-sourced test framework. This result was achieved with a
cross-sectional study population sample of 101 low and middle-income subjects
aged 20 to 79 years old. Partial least squares structural equation modeling
(PLS-SEM) was used to assess the measurement of the construct. It was possible
to achieve a reliable method with internal consistency, good content validity
related to digital competences, and that does not have much interference with
auto-perceived global functional disability, health self-perception, and motor
dexterity. Limitations for this method are discussed and paths to improve and
establish better standards are highlighted.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01799" title="Abstract">arXiv:2310.01799</a> (cross-list from eess.IV) [<a href="/pdf/2310.01799" title="Download PDF">pdf</a>, <a href="/format/2310.01799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ozturkler%2C+B">Batu Ozturkler</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Eckart%2C+B">Benjamin Eckart</a>, 
<a href="/search/eess?searchtype=author&query=Mardani%2C+M">Morteza Mardani</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+J">Jiaming Song</a>, 
<a href="/search/eess?searchtype=author&query=Kautz%2C+J">Jan Kautz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion models have recently gained popularity for accelerated MRI
reconstruction due to their high sample quality. They can effectively serve as
rich data priors while incorporating the forward model flexibly at inference
time, and they have been shown to be more robust than unrolled methods under
distribution shifts. However, diffusion models require careful tuning of
inference hyperparameters on a validation set and are still sensitive to
distribution shifts during testing. To address these challenges, we introduce
SURE-based MRI Reconstruction with Diffusion models (SMRD), a method that
performs test-time hyperparameter tuning to enhance robustness during testing.
SMRD uses Stein's Unbiased Risk Estimator (SURE) to estimate the mean squared
error of the reconstruction during testing. SURE is then used to automatically
tune the inference hyperparameters and to set an early stopping criterion
without the need for validation tuning. To the best of our knowledge, SMRD is
the first to incorporate SURE into the sampling stage of diffusion models for
automatic hyperparameter selection. SMRD outperforms diffusion model baselines
on various measurement noise levels, acceleration factors, and anatomies,
achieving a PSNR improvement of up to 6 dB under measurement noise. The code is
publicly available at https://github.com/batuozt/SMRD .
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01808" title="Abstract">arXiv:2310.01808</a> (cross-list from stat.ML) [<a href="/pdf/2310.01808" title="Download PDF">pdf</a>, <a href="/format/2310.01808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation-based Inference with the Generalized Kullback-Leibler  Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Miller%2C+B+K">Benjamin Kurt Miller</a>, 
<a href="/search/stat?searchtype=author&query=Federici%2C+M">Marco Federici</a>, 
<a href="/search/stat?searchtype=author&query=Weniger%2C+C">Christoph Weniger</a>, 
<a href="/search/stat?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Synergy of Scientific and Machine Learning Modeling ICML 2023 Workshop <a href="https://syns-ml.github.io/2023/contributions/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">In Simulation-based Inference, the goal is to solve the inverse problem when
the likelihood is only known implicitly. Neural Posterior Estimation commonly
fits a normalized density estimator as a surrogate model for the posterior.
This formulation cannot easily fit unnormalized surrogates because it optimizes
the Kullback-Leibler divergence. We propose to optimize a generalized
Kullback-Leibler divergence that accounts for the normalization constant in
unnormalized distributions. The objective recovers Neural Posterior Estimation
when the model class is normalized and unifies it with Neural Ratio Estimation,
combining both into a single objective. We investigate a hybrid model that
offers the best of both worlds by learning a normalized base distribution and a
learned ratio. We also present benchmark results.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01839" title="Abstract">arXiv:2310.01839</a> (cross-list from eess.AS) [<a href="/pdf/2310.01839" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss  Function for Automatic Pronunciation Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yan%2C+B">Bi-Cheng Yan</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hsin-Wei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yi-Cheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiun-Ting Li</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+C">Chi-Han Lin</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+B">Berlin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Automatic pronunciation assessment (APA) manages to quantify the
pronunciation proficiency of a second language (L2) learner in a language.
Prevailing approaches to APA normally leverage neural models trained with a
regression loss function, such as the mean-squared error (MSE) loss, for
proficiency level prediction. Despite most regression models can effectively
capture the ordinality of proficiency levels in the feature space, they are
confronted with a primary obstacle that different phoneme categories with the
same proficiency level are inevitably forced to be close to each other,
retaining less phoneme-discriminative information. On account of this, we
devise a phonemic contrast ordinal (PCO) loss for training regression-based APA
models, which aims to preserve better phonemic distinctions between phoneme
categories meanwhile considering ordinal relationships of the regression target
output. Specifically, we introduce a phoneme-distinct regularizer into the MSE
loss, which encourages feature representations of different phoneme categories
to be far apart while simultaneously pulling closer the representations
belonging to the same phoneme category by means of weighted distances. An
extensive set of experiments carried out on the speechocean762 benchmark
dataset suggest the feasibility and effectiveness of our model in relation to
some existing state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01849" title="Abstract">arXiv:2310.01849</a> (cross-list from math.OC) [<a href="/pdf/2310.01849" title="Download PDF">pdf</a>, <a href="/format/2310.01849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Geometry of Virtual Nonlinear Nonholonomic Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Stratoglou%2C+E">Efstratios Stratoglou</a>, 
<a href="/search/math?searchtype=author&query=Simoes%2C+A+A">Alexandre Anahory Simoes</a>, 
<a href="/search/math?searchtype=author&query=Bloch%2C+A">Anthony Bloch</a>, 
<a href="/search/math?searchtype=author&query=Colombo%2C+L+J">Leonardo J. Colombo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Mathematical Physics (math-ph)

</div>
<p class="mathjax">Virtual constraints are relations imposed on a control system that become
invariant via feedback control, as opposed to physical constraints acting on
the system. Nonholonomic systems are mechanical systems with non-integrable
constraints on the velocities. In this work, we introduce the notion of virtual
nonlinear nonholonomic constraints in a geometric framework which is a
controlled invariant submanifold and we show the existence and uniqueness of a
control law preserving this submanifold. We illustrate the theory with various
examples and present simulation results for an application.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01853" title="Abstract">arXiv:2310.01853</a> (cross-list from stat.ML) [<a href="/pdf/2310.01853" title="Download PDF">pdf</a>, <a href="/format/2310.01853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rozet%2C+F">Fran&#xe7;ois Rozet</a>, 
<a href="/search/stat?searchtype=author&query=Louppe%2C+G">Gilles Louppe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Data assimilation addresses the problem of identifying plausible state
trajectories of dynamical systems given noisy or incomplete observations. In
geosciences, it presents challenges due to the high-dimensionality of
geophysical dynamical systems, often exceeding millions of dimensions. This
work assesses the scalability of score-based data assimilation (SDA), a novel
data assimilation method, in the context of such systems. We propose
modifications to the score network architecture aimed at significantly reducing
memory consumption and execution time. We demonstrate promising results for a
two-layer quasi-geostrophic model.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01859" title="Abstract">arXiv:2310.01859</a> (cross-list from stat.ML) [<a href="/pdf/2310.01859" title="Download PDF">pdf</a>, <a href="/format/2310.01859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Gaussian approximation of the Kushner optimal filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lambert%2C+M">Marc Lambert</a> (DGA, SIERRA), 
<a href="/search/stat?searchtype=author&query=Bonnabel%2C+S">Silv&#xe8;re Bonnabel</a>, 
<a href="/search/stat?searchtype=author&query=Bach%2C+F">Francis Bach</a> (SIERRA)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Lecture Notes in Computer Science, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In estimation theory, the Kushner equation provides the evolution of the
probability density of the state of a dynamical system given continuous-time
observations. Building upon our recent work, we propose a new way to
approximate the solution of the Kushner equation through tractable variational
Gaussian approximations of two proximal losses associated with the propagation
and Bayesian update of the probability density. The first is a proximal loss
based on the Wasserstein metric and the second is a proximal loss based on the
Fisher metric. The solution to this last proximal loss is given by implicit
updates on the mean and covariance that we proposed earlier. These two
variational updates can be fused and shown to satisfy a set of stochastic
differential equations on the Gaussian's mean and covariance matrix. This
Gaussian flow is consistent with the Kalman-Bucy and Riccati flows in the
linear case and generalize them in the nonlinear one.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01860" title="Abstract">arXiv:2310.01860</a> (cross-list from math.OC) [<a href="/pdf/2310.01860" title="Download PDF">pdf</a>, <a href="/ps/2310.01860" title="Download PostScript">ps</a>, <a href="/format/2310.01860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Probability Convergence for Composite and Distributed Stochastic  Minimization and Variational Inequalities with Heavy-Tailed Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gorbunov%2C+E">Eduard Gorbunov</a>, 
<a href="/search/math?searchtype=author&query=Sadiev%2C+A">Abdurakhmon Sadiev</a>, 
<a href="/search/math?searchtype=author&query=Danilova%2C+M">Marina Danilova</a>, 
<a href="/search/math?searchtype=author&query=Horv%C3%A1th%2C+S">Samuel Horv&#xe1;th</a>, 
<a href="/search/math?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>, 
<a href="/search/math?searchtype=author&query=Dvurechensky%2C+P">Pavel Dvurechensky</a>, 
<a href="/search/math?searchtype=author&query=Gasnikov%2C+A">Alexander Gasnikov</a>, 
<a href="/search/math?searchtype=author&query=Richt%C3%A1rik%2C+P">Peter Richt&#xe1;rik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 143 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">High-probability analysis of stochastic first-order optimization methods
under mild assumptions on the noise has been gaining a lot of attention in
recent years. Typically, gradient clipping is one of the key algorithmic
ingredients to derive good high-probability guarantees when the noise is
heavy-tailed. However, if implemented na\"ively, clipping can spoil the
convergence of the popular methods for composite and distributed optimization
(Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason,
many works on high-probability analysis consider only unconstrained
non-distributed problems, and the existing results for composite/distributed
problems do not include some important special cases (like strongly convex
problems) and are not optimal. To address this issue, we propose new stochastic
methods for composite and distributed optimization based on the clipping of
stochastic gradient differences and prove tight high-probability convergence
results (including nearly optimal ones) for the new methods. Using similar
ideas, we also develop new methods for composite and distributed variational
inequalities and analyze the high-probability convergence of these methods.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01861" title="Abstract">arXiv:2310.01861</a> (cross-list from eess.IV) [<a href="/pdf/2310.01861" title="Download PDF">pdf</a>, <a href="/format/2310.01861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shifting More Attention to Breast Lesion Segmentation in Ultrasound  Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+J">Junhao Lin</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+Q">Qian Dai</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Lei Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qiong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Weibin Li</a>, 
<a href="/search/eess?searchtype=author&query=Rao%2C+W">Wenhao Rao</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+X">Xiaoyang Huang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+L">Liansheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
<p class="mathjax">Breast lesion segmentation in ultrasound (US) videos is essential for
diagnosing and treating axillary lymph node metastasis. However, the lack of a
well-established and large-scale ultrasound video dataset with high-quality
annotations has posed a persistent challenge for the research community. To
overcome this issue, we meticulously curated a US video breast lesion
segmentation dataset comprising 572 videos and 34,300 annotated frames,
covering a wide range of realistic clinical scenarios. Furthermore, we propose
a novel frequency and localization feature aggregation network (FLA-Net) that
learns temporal features from the frequency domain and predicts additional
lesion location positions to assist with breast lesion segmentation. We also
devise a localization-based contrastive loss to reduce the lesion location
distance between neighboring video frames within the same video and enlarge the
location distances between frames from different ultrasound videos. Our
experiments on our annotated dataset and two public video polyp segmentation
datasets demonstrate that our proposed FLA-Net achieves state-of-the-art
performance in breast lesion segmentation in US videos and video polyp
segmentation while significantly reducing time and space complexity. Our model
and dataset are available at https://github.com/jhl-Det/FLA-Net.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01867" title="Abstract">arXiv:2310.01867</a> (cross-list from eess.AS) [<a href="/pdf/2310.01867" title="Download PDF">pdf</a>, <a href="/format/2310.01867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-visual child-adult speaker classification in dyadic interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+A">Anfeng Xu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+K">Kevin Huang</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+T">Tiantian Feng</a>, 
<a href="/search/eess?searchtype=author&query=Tager-Flusberg%2C+H">Helen Tager-Flusberg</a>, 
<a href="/search/eess?searchtype=author&query=Narayanan%2C+S">Shrikanth Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In review for ICASSP 2024, 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Interactions involving children span a wide range of important domains from
learning to clinical diagnostic and therapeutic contexts. Automated analyses of
such interactions are motivated by the need to seek accurate insights and offer
scale and robustness across diverse and wide-ranging conditions. Identifying
the speech segments belonging to the child is a critical step in such modeling.
Conventional child-adult speaker classification typically relies on audio
modeling approaches, overlooking visual signals that convey speech articulation
information, such as lip motion. Building on the foundation of an audio-only
child-adult speaker classification pipeline, we propose incorporating visual
cues through active speaker detection and visual processing models. Our
framework involves video pre-processing, utterance-level child-adult speaker
detection, and late fusion of modality-specific predictions. We demonstrate
from extensive experiments that a visually aided classification pipeline
enhances the accuracy and robustness of the classification. We show relative
improvements of 2.38% and 3.97% in F1 macro score when one face and two faces
are visible, respectively
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01885" title="Abstract">arXiv:2310.01885</a> (cross-list from eess.IV) [<a href="/pdf/2310.01885" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic CT Generation via Variant Invertible Network for All-digital  Brain PET Attenuation Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guan%2C+Y">Yu Guan</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+B">Bohui Shen</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+X">Xinchong Shi</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiangsong Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Bingxuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Q">Qiegen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Attenuation correction (AC) is essential for the generation of artifact-free
and quantitatively accurate positron emission tomography (PET) images. However,
AC of PET faces challenges including inter-scan motion and erroneous
transformation of structural voxel-intensities to PET attenuation-correction
factors. Nowadays, the problem of AC for quantitative PET have been solved to a
large extent after the commercial availability of devices combining PET with
computed tomography (CT). Meanwhile, considering the feasibility of a deep
learning approach for PET AC without anatomical imaging, this paper develops a
PET AC method, which uses deep learning to generate continuously valued CT
images from non-attenuation corrected PET images for AC on brain PET imaging.
Specifically, an invertible network combined with the variable augmentation
strategy that can achieve the bidirectional inference processes is proposed for
synthetic CT generation (IVNAC). To evaluate the performance of the proposed
algorithm, we conducted a comprehensive study on a total of 1440 data from 37
clinical patients using comparative algorithms (such as Cycle-GAN and Pix2pix).
Perceptual analysis and quantitative evaluations illustrate that the invertible
network for PET AC outperforms other existing AC models, which demonstrates the
potential of the proposed method and the feasibility of achieving brain PET AC
without CT.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01908" title="Abstract">arXiv:2310.01908</a> (cross-list from eess.IV) [<a href="/pdf/2310.01908" title="Download PDF">pdf</a>, <a href="/format/2310.01908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving style transfer in dynamic contrast enhanced MRI using a  spatio-temporal approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tattersall%2C+A+G">Adam G. Tattersall</a>, 
<a href="/search/eess?searchtype=author&query=Goatman%2C+K+A">Keith A. Goatman</a>, 
<a href="/search/eess?searchtype=author&query=Kershaw%2C+L+E">Lucy E. Kershaw</a>, 
<a href="/search/eess?searchtype=author&query=Semple%2C+S+I+K">Scott I. K. Semple</a>, 
<a href="/search/eess?searchtype=author&query=Dahdouh%2C+S">Sonia Dahdouh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Style transfer in DCE-MRI is a challenging task due to large variations in
contrast enhancements across different tissues and time. Current unsupervised
methods fail due to the wide variety of contrast enhancement and motion between
the images in the series. We propose a new method that combines autoencoders to
disentangle content and style with convolutional LSTMs to model predicted
latent spaces along time and adaptive convolutions to tackle the localised
nature of contrast enhancement. To evaluate our method, we propose a new metric
that takes into account the contrast enhancement. Qualitative and quantitative
analyses show that the proposed method outperforms the state of the art on two
different datasets.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01912" title="Abstract">arXiv:2310.01912</a> (cross-list from eess.IV) [<a href="/pdf/2310.01912" title="Download PDF">pdf</a>, <a href="/format/2310.01912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Automatic Diabetic Retinopathy Severity Classification Using  Deep Multimodal Fusion of UWF-CFP and OCTA Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Daho%2C+M+E+H">Mostafa El Habib Daho</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yihao Li</a>, 
<a href="/search/eess?searchtype=author&query=Zeghlache%2C+R">Rachid Zeghlache</a>, 
<a href="/search/eess?searchtype=author&query=Atse%2C+Y+C">Yapo Cedric Atse</a>, 
<a href="/search/eess?searchtype=author&query=Boit%C3%A9%2C+H+L">Hugo Le Boit&#xe9;</a>, 
<a href="/search/eess?searchtype=author&query=Bonnin%2C+S">Sophie Bonnin</a>, 
<a href="/search/eess?searchtype=author&query=Cosette%2C+D">Deborah Cosette</a>, 
<a href="/search/eess?searchtype=author&query=Deman%2C+P">Pierre Deman</a>, 
<a href="/search/eess?searchtype=author&query=Borderie%2C+L">Laurent Borderie</a>, 
<a href="/search/eess?searchtype=author&query=Lepicard%2C+C">Capucine Lepicard</a>, 
<a href="/search/eess?searchtype=author&query=Tadayoni%2C+R">Ramin Tadayoni</a>, 
<a href="/search/eess?searchtype=author&query=Cochener%2C+B">B&#xe9;atrice Cochener</a>, 
<a href="/search/eess?searchtype=author&query=Conze%2C+P">Pierre-Henri Conze</a>, 
<a href="/search/eess?searchtype=author&query=Lamard%2C+M">Mathieu Lamard</a>, 
<a href="/search/eess?searchtype=author&query=Quellec%2C+G">Gwenol&#xe9; Quellec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted preprint for presentation at MICCAI-OMIA 20023, Vancouver, Canada
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diabetic Retinopathy (DR), a prevalent and severe complication of diabetes,
affects millions of individuals globally, underscoring the need for accurate
and timely diagnosis. Recent advancements in imaging technologies, such as
Ultra-WideField Color Fundus Photography (UWF-CFP) imaging and Optical
Coherence Tomography Angiography (OCTA), provide opportunities for the early
detection of DR but also pose significant challenges given the disparate nature
of the data they produce. This study introduces a novel multimodal approach
that leverages these imaging modalities to notably enhance DR classification.
Our approach integrates 2D UWF-CFP images and 3D high-resolution 6x6 mm$^3$
OCTA (both structure and flow) images using a fusion of ResNet50 and
3D-ResNet50 models, with Squeeze-and-Excitation (SE) blocks to amplify relevant
features. Additionally, to increase the model's generalization capabilities, a
multimodal extension of Manifold Mixup, applied to concatenated multimodal
features, is implemented. Experimental results demonstrate a remarkable
enhancement in DR classification performance with the proposed multimodal
approach compared to methods relying on a single modality only. The methodology
laid out in this work holds substantial promise for facilitating more accurate,
early detection of DR, potentially improving clinical outcomes for patients.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01921" title="Abstract">arXiv:2310.01921</a> (cross-list from quant-ph) [<a href="/pdf/2310.01921" title="Download PDF">pdf</a>, <a href="/format/2310.01921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing the Inter-Core Qubit Traffic in Large-Scale Quantum  Modular Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Rached%2C+S+B">Sahar Ben Rached</a>, 
<a href="/search/quant-ph?searchtype=author&query=Agudo%2C+I+L">Isaac Lopez Agudo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rodrigo%2C+S">Santiago Rodrigo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bandic%2C+M">Medina Bandic</a>, 
<a href="/search/quant-ph?searchtype=author&query=Feld%2C+S">Sebastian Feld</a>, 
<a href="/search/quant-ph?searchtype=author&query=van+Someren%2C+H">Hans van Someren</a>, 
<a href="/search/quant-ph?searchtype=author&query=Alarc%C3%B3n%2C+E">Eduard Alarc&#xf3;n</a>, 
<a href="/search/quant-ph?searchtype=author&query=Almud%C3%A9ver%2C+C+G">Carmen G. Almud&#xe9;ver</a>, 
<a href="/search/quant-ph?searchtype=author&query=Abadal%2C+S">Sergi Abadal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Modular quantum processor architectures are envisioned as a promising
solution for the scalability of quantum computing systems beyond the Noisy
Intermediate Scale Quantum (NISQ) devices era. Based upon interconnecting tens
to hundreds of quantum cores via a quantum intranet, this approach unravels the
pressing limitations of densely qubit-packed monolithic processors, mainly by
mitigating the requirements of qubit control and enhancing qubit isolation, and
therefore enables executing large-scale algorithms on quantum computers. In
order to optimize such architectures, it is crucial to analyze the quantum
state transfers occurring via inter-core communication networks, referred to as
inter-core qubit traffic. This would also provide insights to improve the
software and hardware stack for multi-core quantum computers. To this aim, we
present a pioneering characterization of the spatio-temporal inter-core qubit
traffic in large-scale circuits. The programs are executed on an all-to-all
connected multi-core architecture that supports up to around 1000 qubits. We
characterize the qubit traffic based on multiple performance metrics to assess
the computational process and the communication overhead. Based on the
showcased results, we conclude on the scalability of the presented algorithms,
provide a set of guidelines to improve mapping quantum circuits to multi-core
processors, and lay the foundations of benchmarking large-scale multi-core
architectures.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01934" title="Abstract">arXiv:2310.01934</a> (cross-list from eess.IV) [<a href="/pdf/2310.01934" title="Download PDF">pdf</a>, <a href="/format/2310.01934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust deformable image registration using cycle-consistent implicit  representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=van+Harten%2C+L+D">Louis D. van Harten</a>, 
<a href="/search/eess?searchtype=author&query=Stoker%2C+J">Jaap Stoker</a>, 
<a href="/search/eess?searchtype=author&query=I%C5%A1gum%2C+I">Ivana I&#x161;gum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures, accepted in IEEE Transactions on Medical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent works in medical image registration have proposed the use of Implicit
Neural Representations, demonstrating performance that rivals state-of-the-art
learning-based methods. However, these implicit representations need to be
optimized for each new image pair, which is a stochastic process that may fail
to converge to a global minimum. To improve robustness, we propose a deformable
registration method using pairs of cycle-consistent Implicit Neural
Representations: each implicit representation is linked to a second implicit
representation that estimates the opposite transformation, causing each network
to act as a regularizer for its paired opposite. During inference, we generate
multiple deformation estimates by numerically inverting the paired backward
transformation and evaluating the consensus of the optimized pair. This
consensus improves registration accuracy over using a single representation and
results in a robust uncertainty metric that can be used for automatic quality
control. We evaluate our method with a 4D lung CT dataset. The proposed
cycle-consistent optimization method reduces the optimization failure rate from
2.4% to 0.0% compared to the current state-of-the-art. The proposed inference
method improves landmark accuracy by 4.5% and the proposed uncertainty metric
detects all instances where the registration method fails to converge to a
correct solution. We verify the generalizability of these results to other data
using a centerline propagation task in abdominal 4D MRI, where our method
achieves a 46% improvement in propagation consistency compared with single-INR
registration and demonstrates a strong correlation between the proposed
uncertainty metric and registration accuracy.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01963" title="Abstract">arXiv:2310.01963</a> (cross-list from stat.CO) [<a href="/pdf/2310.01963" title="Download PDF">pdf</a>, <a href="/format/2310.01963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying the information lost in optimal covariance matrix cleaning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bongiorno%2C+C">Christian Bongiorno</a>, 
<a href="/search/stat?searchtype=author&query=Lamrani%2C+L">Lamia Lamrani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Information Theory (cs.IT); Symbolic Computation (cs.SC)

</div>
<p class="mathjax">In this work, we examine the prevalent use of Frobenius error minimization in
covariance matrix cleaning. Currently, minimizing the Frobenius error offers a
limited interpretation within information theory. To better understand this
relationship, we focus on the Kullback-Leibler divergence as a measure of the
information lost by the optimal estimators. Our analysis centers on
rotationally invariant estimators for data following an inverse Wishart
population covariance matrix, and we derive an analytical expression for their
Kullback-Leibler divergence. Due to the intricate nature of the calculations,
we use genetic programming regressors paired with human intuition. Ultimately,
we establish a more defined link between the Frobenius error and information
theory, showing that the former corresponds to a first-order expansion term of
the Kullback-Leibler divergence.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02021" title="Abstract">arXiv:2310.02021</a> (cross-list from astro-ph.SR) [<a href="/pdf/2310.02021" title="Download PDF">pdf</a>, <a href="/format/2310.02021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of the CRD approximation for the observer&#x27;s frame RIII  redistribution matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Riva%2C+S">Simone Riva</a>, 
<a href="/search/astro-ph?searchtype=author&query=Guerreiro%2C+N">Nuno Guerreiro</a>, 
<a href="/search/astro-ph?searchtype=author&query=Janett%2C+G">Gioele Janett</a>, 
<a href="/search/astro-ph?searchtype=author&query=Rossinelli%2C+D">Diego Rossinelli</a>, 
<a href="/search/astro-ph?searchtype=author&query=Benedusi%2C+P">Pietro Benedusi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Krause%2C+R">Rolf Krause</a>, 
<a href="/search/astro-ph?searchtype=author&query=Belluzzi%2C+L">Luca Belluzzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Approximated forms of the RII and RIII redistribution matrices are frequently
applied to simplify the numerical solution of the radiative transfer problem
for polarized radiation, taking partial frequency redistribution (PRD) effects
into account. A widely used approximation for RIII is to consider its
expression under the assumption of complete frequency redistribution (CRD) in
the observer frame (RIII CRD). The adequacy of this approximation for modeling
the intensity profiles has been firmly established. By contrast, its
suitability for modeling scattering polarization signals has only been analyzed
in a few studies, considering simplified settings.
<br />In this work, we aim at quantitatively assessing the impact and the range of
validity of the RIII CRD approximation in the modeling of scattering
polarization. Methods. We first present an analytic comparison between RIII and
RIII CRD. We then compare the results of radiative transfer calculations, out
of local thermodynamic equilibrium, performed with RIII and RIII CRD in
realistic 1D atmospheric models. We focus on the chromospheric Ca i line at
4227 A and on the photospheric Sr i line at 4607 A.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02068" title="Abstract">arXiv:2310.02068</a> (cross-list from math.AP) [<a href="/pdf/2310.02068" title="Download PDF">pdf</a>, <a href="/format/2310.02068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Well-posedness and numerical analysis of an elapsed time model with  strongly coupled neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sepulveda%2C+M">Mauricio Sepulveda</a>, 
<a href="/search/math?searchtype=author&query=Torres%2C+N">Nicolas Torres</a>, 
<a href="/search/math?searchtype=author&query=Villada%2C+L+M">Luis Miguel Villada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The elapsed time equation is an age-structured model that describes dynamics
of interconnected spiking neurons through the elapsed time since the last
discharge, leading to many interesting questions on the evolution of the system
from a mathematical and biological point of view. In this work, we first deal
with the case when transmission after a spike is instantaneous and the case
when there exists a distributed delay that depends on previous history of the
system, which is a more realistic assumption. Then we study the well-posedness
and the numerical analysis of the elapsed time models. For existence and
uniqueness we improve the previous works by relaxing some hypothesis on the
nonlinearity, including the strongly excitatory case, while for the numerical
analysis we prove that the approximation given by the explicit upwind scheme
converges to the solution of the non-linear problem. We also show some
numerical simulations to compare the behavior of the system in the case of
instantaneous transmission with the case of distributed delay under different
parameters, leading to solutions with different asymptotic profiles.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02074" title="Abstract">arXiv:2310.02074</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.02074" title="Download PDF">pdf</a>, <a href="/format/2310.02074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACE: A fast, skillful learned global atmospheric model for climate  prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Watt-Meyer%2C+O">Oliver Watt-Meyer</a>, 
<a href="/search/physics?searchtype=author&query=Dresdner%2C+G">Gideon Dresdner</a>, 
<a href="/search/physics?searchtype=author&query=McGibbon%2C+J">Jeremy McGibbon</a>, 
<a href="/search/physics?searchtype=author&query=Clark%2C+S+K">Spencer K. Clark</a>, 
<a href="/search/physics?searchtype=author&query=Henn%2C+B">Brian Henn</a>, 
<a href="/search/physics?searchtype=author&query=Duncan%2C+J">James Duncan</a>, 
<a href="/search/physics?searchtype=author&query=Brenowitz%2C+N+D">Noah D. Brenowitz</a>, 
<a href="/search/physics?searchtype=author&query=Kashinath%2C+K">Karthik Kashinath</a>, 
<a href="/search/physics?searchtype=author&query=Pritchard%2C+M+S">Michael S. Pritchard</a>, 
<a href="/search/physics?searchtype=author&query=Bonev%2C+B">Boris Bonev</a>, 
<a href="/search/physics?searchtype=author&query=Peters%2C+M+E">Matthew E. Peters</a>, 
<a href="/search/physics?searchtype=author&query=Bretherton%2C+C+S">Christopher S. Bretherton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing ML-based atmospheric models are not suitable for climate prediction,
which requires long-term stability and physical consistency. We present ACE
(AI2 Climate Emulator), a 200M-parameter, autoregressive machine learning
emulator of an existing comprehensive 100-km resolution global atmospheric
model. The formulation of ACE allows evaluation of physical laws such as the
conservation of mass and moisture. The emulator is stable for 10 years, nearly
conserves column moisture without explicit constraints and faithfully
reproduces the reference model's climate, outperforming a challenging baseline
on over 80% of tracked variables. ACE requires nearly 100x less wall clock time
and is 100x more energy efficient than the reference model using typically
available resources.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02075" title="Abstract">arXiv:2310.02075</a> (cross-list from quant-ph) [<a href="/pdf/2310.02075" title="Download PDF">pdf</a>, <a href="/format/2310.02075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Quantum Processes with Quantum Statistical Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wadhwa%2C+C">Chirag Wadhwa</a>, 
<a href="/search/quant-ph?searchtype=author&query=Doosti%2C+M">Mina Doosti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning complex quantum processes is a central challenge in many areas of
quantum computing and quantum machine learning, with applications in quantum
benchmarking, cryptanalysis, and variational quantum algorithms. This paper
introduces the first learning framework for studying quantum process learning
within the Quantum Statistical Query (QSQ) model, providing the first formal
definition of statistical queries to quantum processes (QPSQs). The framework
allows us to propose an efficient QPSQ learner for arbitrary quantum processes
accompanied by a provable performance guarantee. We also provide numerical
simulations to demonstrate the efficacy of this algorithm. The practical
relevance of this framework is exemplified through application in
cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum
Physical Unclonable Functions (CR-QPUFs), addressing an important open question
in the field of quantum hardware security. This work marks a significant step
towards understanding the learnability of quantum processes and shedding light
on their security implications.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02114" title="Abstract">arXiv:2310.02114</a> (cross-list from math.DG) [<a href="/pdf/2310.02114" title="Download PDF">pdf</a>, <a href="/ps/2310.02114" title="Download PostScript">ps</a>, <a href="/format/2310.02114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On dual quaternions, dual split quaternions and Cartan-Schouten metrics  on perfect Lie groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Diatta%2C+A">Andre Diatta</a>, 
<a href="/search/math?searchtype=author&query=Manga%2C+B">Bakary Manga</a>, 
<a href="/search/math?searchtype=author&query=Sy%2C+F">Fatimata Sy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, Latex
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Differential Geometry (math.DG)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph); Metric Geometry (math.MG)

</div>
<p class="mathjax">We discuss Cartan-Schouten metrics (Riemannian or pseudo-Riemannian metrics
that are parallel with respect to the Cartan-Schouten canonical connection) on
perfect Lie groups and in particular, on cotangent bundles of simple Lie
groups. Applications are foreseen in Information Geometry. Throughout this
work, the tangent bundle TG and the cotangent bundle T*G of a Lie group G, will
always be endowed with their Lie group structures induced by the right
trivialization.
<br />We show that TG and T*G are isomorphic if G itself possesses a biinvariant
Riemannian or pseudo-Riemannian metric. We also show that, if on a perfect Lie
group, there exists a Cartan-Schouten metric, then it must be biinvariant. We
compute all such metrics on the cotangent bundles of simple Lie groups. We
further show the following. Endowed with their canonical Lie group structures,
the set of unit dual quaternions is isomorphic to T*SU(2), the set of unit dual
split quaternions is isomorphic to the cotangent bundle of the group of unit
split quaternions.
<br />The group SE(3) of special rigid displacements of the Euclidean 3-space is
isomorphic to T*SO(3). The group SE(2,1) of special rigid displacements of the
Minkowski 3-space is isomorphic to T*SO(2,1). So some results on SE(3) by N.
Miolane and X. Pennec, and M. Zefran, V. Kumar and C. Croke, are generalized to
SE(2,1) and to T*G, for any simple Lie group G.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02152" title="Abstract">arXiv:2310.02152</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.02152" title="Download PDF">pdf</a>, <a href="/format/2310.02152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Network-based EEG Classification: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Klepl%2C+D">Dominik Klepl</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=He%2C+F">Fei He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Graph neural networks (GNN) are increasingly used to classify EEG for tasks
such as emotion recognition, motor imagery and neurological diseases and
disorders. A wide range of methods have been proposed to design GNN-based
classifiers. Therefore, there is a need for a systematic review and
categorisation of these approaches. We exhaustively search the published
literature on this topic and derive several categories for comparison. These
categories highlight the similarities and differences among the methods. The
results suggest a prevalence of spectral graph convolutional layers over
spatial. Additionally, we identify standard forms of node features, with the
most popular being the raw EEG signal and differential entropy. Our results
summarise the emerging trends in GNN-based approaches for EEG classification.
Finally, we discuss several promising research directions, such as exploring
the potential of transfer learning methods and appropriate modelling of
cross-frequency interactions.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02203" title="Abstract">arXiv:2310.02203</a> (cross-list from quant-ph) [<a href="/pdf/2310.02203" title="Download PDF">pdf</a>, <a href="/format/2310.02203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Quantum Power Flow for Risk Assessment in Power Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=S%C3%A6varsson%2C+B">Brynjar S&#xe6;varsson</a>, 
<a href="/search/quant-ph?searchtype=author&query=J%C3%B3hannsson%2C+H">Hj&#xf6;rtur J&#xf3;hannsson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chatzivasileiadis%2C+S">Spyros Chatzivasileiadis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, submitted to PSCC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper introduces, to the best of our knowledge, the first quantum
computing methodology for stochastic power flow. Stochastic power flow is
widely used in power system operation and planning to study the impact of
stochastic factors, such as uncertain generation, load, or contingencies, on
power systems. Most standard approaches use Monte-Carlo simulations. In this
paper, we focus on how the uncertainty of wind infeed affects the probability
of line overloadings in a power grid. Quantum Monte Carlo approaches and the
solution of linear systems have both been theoretically proven to be more
computationally efficient than classical computing approaches. For example,
Quantum Monte Carlo requires substantially fewer samples than Classical Monte
Carlo to achieve the same accuracy. This paper presents the first formulation
that exploits this quantum advantage to formulate a Quantum Stochastic Power
Flow method. The developed method is tested for two small power systems and
compared to classical simulations.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02215" title="Abstract">arXiv:2310.02215</a> (cross-list from physics.med-ph) [<a href="/pdf/2310.02215" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An experimental system for detection and localization of hemorrhage  using ultra-wideband microwaves with deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Hedayati%2C+E">Eisa Hedayati</a>, 
<a href="/search/physics?searchtype=author&query=Safari%2C+F">Fatemeh Safari</a>, 
<a href="/search/physics?searchtype=author&query=Verghese%2C+G">George Verghese</a>, 
<a href="/search/physics?searchtype=author&query=Ciancia%2C+V+R">Vito R. Ciancia</a>, 
<a href="/search/physics?searchtype=author&query=Sodickson%2C+D+K">Daniel K. Sodickson</a>, 
<a href="/search/physics?searchtype=author&query=Dehkharghani%2C+S">Seena Dehkharghani</a>, 
<a href="/search/physics?searchtype=author&query=Alon%2C+L">Leeor Alon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Stroke is a leading cause of mortality and disability. Emergent diagnosis and
intervention are critical, and predicated upon initial brain imaging; however,
existing clinical imaging modalities are generally costly, immobile, and demand
highly specialized operation and interpretation. Low-energy microwaves have
been explored as low-cost, small form factor, fast, and safe probes of tissue
dielectric properties, with both imaging and diagnostic potential.
Nevertheless, challenges inherent to microwave reconstruction have impeded
progress, hence microwave imaging (MWI) remains an elusive scientific aim.
Herein, we introduce a dedicated experimental framework comprising a robotic
navigation system to translate blood-mimicking phantoms within an anatomically
realistic human head model. An 8-element ultra-wideband (UWB) array of modified
antipodal Vivaldi antennas was developed and driven by a two-port vector
network analyzer spanning 0.6-9.0 GHz at an operating power of 1 mw. Complex
scattering parameters were measured, and dielectric signatures of hemorrhage
were learned using a dedicated deep neural network for prediction of hemorrhage
classes and localization. An overall sensitivity and specificity for detection
&gt;0.99 was observed, with Rayliegh mean localization error of 1.65 mm. The study
establishes the feasibility of a robust experimental model and deep learning
solution for UWB microwave stroke detection.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02233" title="Abstract">arXiv:2310.02233</a> (cross-list from stat.ML) [<a href="/pdf/2310.02233" title="Download PDF">pdf</a>, <a href="/format/2310.02233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Schr&#xf6;dinger Bridge Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+G">Guan-Horng Liu</a>, 
<a href="/search/stat?searchtype=author&query=Lipman%2C+Y">Yaron Lipman</a>, 
<a href="/search/stat?searchtype=author&query=Nickel%2C+M">Maximilian Nickel</a>, 
<a href="/search/stat?searchtype=author&query=Karrer%2C+B">Brian Karrer</a>, 
<a href="/search/stat?searchtype=author&query=Theodorou%2C+E+A">Evangelos A. Theodorou</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+R+T+Q">Ricky T. Q. Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Modern distribution matching algorithms for training diffusion or flow models
directly prescribe the time evolution of the marginal distributions between two
boundary distributions. In this work, we consider a generalized distribution
matching setup, where these marginals are only implicitly described as a
solution to some task-specific objective function. The problem setup, known as
the Generalized Schr\"odinger Bridge (GSB), appears prevalently in many
scientific areas both within and without machine learning. We propose
Generalized Schr\"odinger Bridge Matching (GSBM), a new matching algorithm
inspired by recent advances, generalizing them beyond kinetic energy
minimization and to account for task-specific state costs. We show that such a
generalization can be cast as solving conditional stochastic optimal control,
for which efficient variational approximations can be used, and further
debiased with the aid of path integral theory. Compared to prior methods for
solving GSB problems, our GSBM algorithm always preserves a feasible transport
map between the boundary distributions throughout training, thereby enabling
stable convergence and significantly improved scalability. We empirically
validate our claims on an extensive suite of experimental setups, including
crowd navigation, opinion depolarization, LiDAR manifolds, and image domain
transfer. Our work brings new algorithmic opportunities for training diffusion
models enhanced with task-specific optimality structures.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02243" title="Abstract">arXiv:2310.02243</a> (cross-list from quant-ph) [<a href="/pdf/2310.02243" title="Download PDF">pdf</a>, <a href="/format/2310.02243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning quantum Hamiltonians at any temperature in polynomial time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bakshi%2C+A">Ainesh Bakshi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+A">Allen Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Moitra%2C+A">Ankur Moitra</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tang%2C+E">Ewin Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of learning a local quantum Hamiltonian $H$ given copies
of its Gibbs state $\rho = e^{-\beta H}/\textrm{tr}(e^{-\beta H})$ at a known
inverse temperature $\beta&gt;0$. Anshu, Arunachalam, Kuwahara, and Soleimanifar
(<a href="/abs/2004.07266">arXiv:2004.07266</a>) gave an algorithm to learn a Hamiltonian on $n$ qubits to
precision $\epsilon$ with only polynomially many copies of the Gibbs state, but
which takes exponential time. Obtaining a computationally efficient algorithm
has been a major open problem [Alhambra'22 (<a href="/abs/2204.08349">arXiv:2204.08349</a>)], [Anshu,
Arunachalam'22 (<a href="/abs/2204.08349">arXiv:2204.08349</a>)], with prior work only resolving this in the
limited cases of high temperature [Haah, Kothari, Tang'21 (<a href="/abs/2108.04842">arXiv:2108.04842</a>)]
or commuting terms [Anshu, Arunachalam, Kuwahara, Soleimanifar'21]. We fully
resolve this problem, giving a polynomial time algorithm for learning $H$ to
precision $\epsilon$ from polynomially many copies of the Gibbs state at any
constant $\beta &gt; 0$.
<br />Our main technical contribution is a new flat polynomial approximation to the
exponential function, and a translation between multi-variate scalar
polynomials and nested commutators. This enables us to formulate Hamiltonian
learning as a polynomial system. We then show that solving a low-degree
sum-of-squares relaxation of this polynomial system suffices to accurately
learn the Hamiltonian.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02254" title="Abstract">arXiv:2310.02254</a> (cross-list from quant-ph) [<a href="/pdf/2310.02254" title="Download PDF">pdf</a>, <a href="/format/2310.02254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning unitaries with quantum statistical queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Angrisani%2C+A">Armando Angrisani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose several algorithms for learning unitary operators from quantum
statistical queries (QSQs) with respect to their Choi-Jamiolkowski state.
Quantum statistical queries capture the capabilities of a learner with limited
quantum resources, which receives as input only noisy estimates of expected
values of measurements. Our methods hinge on a novel technique for estimating
the Fourier mass of a unitary on a subset of Pauli strings with a single
quantum statistical query, generalizing a previous result for uniform quantum
examples. Exploiting this insight, we show that the quantum Goldreich-Levin
algorithm can be implemented with quantum statistical queries, whereas the
prior version of the algorithm involves oracle access to the unitary and its
inverse. Moreover, we prove that $\mathcal{O}(\log n)$-juntas and quantum
Boolean functions with constant total influence are efficiently learnable in
our model, and constant-depth circuits are learnable sample-efficiently with
quantum statistical queries. On the other hand, all previous algorithms for
these tasks require direct access to the Choi-Jamiolkowski state or oracle
access to the unitary. In addition, our upper bounds imply that the actions of
those classes of unitaries on locally scrambled ensembles can be efficiently
learned. We also demonstrate that, despite these positive results, quantum
statistical queries lead to an exponentially larger sample complexity for
certain tasks, compared to separable measurements to the Choi-Jamiolkowski
state. In particular, we show an exponential lower bound for learning a class
of phase-oracle unitaries and a double exponential lower bound for testing the
unitarity of channels, adapting to our setting previous arguments for quantum
states. Finally, we propose a new definition of average-case surrogate models,
showing a potential application of our results to hybrid quantum machine
learning.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02259" title="Abstract">arXiv:2310.02259</a> (cross-list from math.OC) [<a href="/pdf/2310.02259" title="Download PDF">pdf</a>, <a href="/ps/2310.02259" title="Download PostScript">ps</a>, <a href="/format/2310.02259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards An Analytical Framework for Potential Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guo%2C+X">Xin Guo</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yufei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Science and Game Theory (cs.GT); Probability (math.PR)

</div>
<p class="mathjax">Potential game is an emerging notion and framework for studying multi-agent
games, especially with heterogeneous agents. Up to date, potential games have
been extensively studied mostly from the algorithmic aspect in approximating
and computing the Nash equilibrium without verifying if the game is a potential
game, due to the lack of analytical structure.
<br />In this paper, we aim to build an analytical framework for dynamic potential
games. We prove that a game is a potential game if and only if each agent's
value function can be decomposed as a potential function and a residual term
that solely dependent on other agents' actions. This decomposition enables us
to identify and analyze a new and important class of potential games called the
distributed game. Moreover, by an appropriate notion of functional derivatives,
we prove that a game is a potential game if the value function has a symmetric
Jacobian. Consequently, for a general class of continuous-time stochastic
games, their potential functions can be further characterized from both the
probabilistic and the PDE approaches. The consistency of these two
characterisations are shown in a class of linear-quadratic games.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02261" title="Abstract">arXiv:2310.02261</a> (cross-list from math.OC) [<a href="/pdf/2310.02261" title="Download PDF">pdf</a>, <a href="/format/2310.02261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Online Non-stochastic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mhaisen%2C+N">Naram Mhaisen</a>, 
<a href="/search/math?searchtype=author&query=Iosifidis%2C+G">George Iosifidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We tackle the problem of Non-stochastic Control with the aim of obtaining
algorithms that adapt to the controlled environment. Namely, we tailor the FTRL
framework to dynamical systems where the existence of a state, or equivalently
a memory, couples the effect of the online decisions. By designing novel
regularization techniques that take the system's memory into consideration, we
obtain controllers with new sub-linear data adaptive policy regret bounds.
Furthermore, we append these regularizers with untrusted predictions of future
costs, which enables the design of the first Optimistic FTRL-based controller
whose regret bound is adaptive to the accuracy of the predictions, shrinking
when they are accurate while staying sub-linear even when they all fail.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed,  4 Oct 23</h3>
<dl>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1906.11062" title="Abstract">arXiv:1906.11062</a> (replaced) [<a href="/pdf/1906.11062" title="Download PDF">pdf</a>, <a href="/format/1906.11062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey of Information Encoding Techniques for DNA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Heinis%2C+T">Thomas Heinis</a>, 
<a href="/search/q-bio?searchtype=author&query=Sokolovskii%2C+R">Roman Sokolovskii</a>, 
<a href="/search/q-bio?searchtype=author&query=Alnasir%2C+J+J">Jamie J. Alnasir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Databases (cs.DB); Data Structures and Algorithms (cs.DS); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.10743" title="Abstract">arXiv:2005.10743</a> (replaced) [<a href="/pdf/2005.10743" title="Download PDF">pdf</a>, <a href="/format/2005.10743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Clustering with Planted Structures: Statistical Optimality and  Computational Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+Y">Yuetian Luo</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+A+R">Anru R. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Done a few clarifications and added low-degree polynomial based evidence for HPDS recovery conjecture 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.05825" title="Abstract">arXiv:2008.05825</a> (replaced) [<a href="/pdf/2008.05825" title="Download PDF">pdf</a>, <a href="/format/2008.05825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unifying supervised learning and VAEs -- coverage, systematics and  goodness-of-fit in normalizing-flow based neural network models for  astro-particle reconstructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gl%C3%BCsenkamp%2C+T">Thorsten Gl&#xfc;senkamp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; High Energy Astrophysical Phenomena (astro-ph.HE); Instrumentation and Methods for Astrophysics (astro-ph.IM); High Energy Physics - Experiment (hep-ex); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.12248" title="Abstract">arXiv:2008.12248</a> (replaced) [<a href="/pdf/2008.12248" title="Download PDF">pdf</a>, <a href="/ps/2008.12248" title="Download PostScript">ps</a>, <a href="/format/2008.12248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Reinforcement Learning for Combinatorial Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yunhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Whinston%2C+A">Andrew Whinston</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.08019" title="Abstract">arXiv:2010.08019</a> (replaced) [<a href="/pdf/2010.08019" title="Download PDF">pdf</a>, <a href="/format/2010.08019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error estimates of residual minimization using neural networks for  linear PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shin%2C+Y">Yeonjong Shin</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhongqiang Zhang</a>, 
<a href="/search/math?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.12478" title="Abstract">arXiv:2011.12478</a> (replaced) [<a href="/pdf/2011.12478" title="Download PDF">pdf</a>, <a href="/format/2011.12478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimax Estimation of Distances on a Surface and Minimax Manifold  Learning in the Isometric-to-Convex Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Arias-Castro%2C+E">Ery Arias-Castro</a>, 
<a href="/search/stat?searchtype=author&query=Chau%2C+P+A">Phong Alain Chau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.00146" title="Abstract">arXiv:2102.00146</a> (replaced) [<a href="/pdf/2102.00146" title="Download PDF">pdf</a>, <a href="/ps/2102.00146" title="Download PostScript">ps</a>, <a href="/format/2102.00146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving a Class of Infinite-Dimensional Tensor Eigenvalue Problems by  Translational Invariant Tensor Ring Approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Van+Beeumen%2C+R">Roel Van Beeumen</a>, 
<a href="/search/math?searchtype=author&query=Peri%C5%A1a%2C+L">Lana Peri&#x161;a</a>, 
<a href="/search/math?searchtype=author&query=Kressner%2C+D">Daniel Kressner</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+C">Chao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.00558" title="Abstract">arXiv:2103.00558</a> (replaced) [<a href="/pdf/2103.00558" title="Download PDF">pdf</a>, <a href="/ps/2103.00558" title="Download PostScript">ps</a>, <a href="/format/2103.00558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Simple Uniform Sampling Effective for Center-Based Clustering with  Outliers: When and Why?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiawei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hu Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1905.10143">arXiv:1905.10143</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG); Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.00253" title="Abstract">arXiv:2104.00253</a> (replaced) [<a href="/pdf/2104.00253" title="Download PDF">pdf</a>, <a href="/format/2104.00253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+Y">Yunhao Yang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Bajaj%2C+C">Chandrajit Bajaj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.02626" title="Abstract">arXiv:2106.02626</a> (replaced) [<a href="/pdf/2106.02626" title="Download PDF">pdf</a>, <a href="/format/2106.02626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics of specialization in neural modules under resource constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=B%C3%A9na%2C+G">Gabriel B&#xe9;na</a>, 
<a href="/search/q-bio?searchtype=author&query=Goodman%2C+D+F+M">Dan F. M. Goodman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.11299" title="Abstract">arXiv:2108.11299</a> (replaced) [<a href="/pdf/2108.11299" title="Download PDF">pdf</a>, <a href="/format/2108.11299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certifiers Make Neural Networks Vulnerable to Availability Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorenz%2C+T">Tobias Lorenz</a>, 
<a href="/search/cs?searchtype=author&query=Kwiatkowska%2C+M">Marta Kwiatkowska</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at 16th ACM Workshop on Artificial Intelligence and Security (AISec '23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.03459" title="Abstract">arXiv:2109.03459</a> (replaced) [<a href="/pdf/2109.03459" title="Download PDF">pdf</a>, <a href="/format/2109.03459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Correction Strategy for Ranking Distillation in Top-N Recommender  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Youngjune Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kee-Eung Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.07627" title="Abstract">arXiv:2201.07627</a> (replaced) [<a href="/pdf/2201.07627" title="Download PDF">pdf</a>, <a href="/format/2201.07627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Tracking-Based Distributed Constraint-Coupled Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jingwang Li</a>, 
<a href="/search/math?searchtype=author&query=Su%2C+H">Housheng Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in IEEE Transactions on Control of Network Systems, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.07762" title="Abstract">arXiv:2201.07762</a> (replaced) [<a href="/pdf/2201.07762" title="Download PDF">pdf</a>, <a href="/format/2201.07762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepAlloc: CNN-Based Approach to Efficient Spectrum Allocation in Shared  Spectrum Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghaderibaneh%2C+M">Mohammad Ghaderibaneh</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+C">Caitao Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H">Himanshu Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.12143" title="Abstract">arXiv:2201.12143</a> (replaced) [<a href="/pdf/2201.12143" title="Download PDF">pdf</a>, <a href="/format/2201.12143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Invariant Explanations: Towards Stable and Unidirectional  Explanations through Local Invariant Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhurandhar%2C+A">Amit Dhurandhar</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K">Karthikeyan Ramamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kartik Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Arya%2C+V">Vijay Arya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.01446" title="Abstract">arXiv:2203.01446</a> (replaced) [<a href="/pdf/2203.01446" title="Download PDF">pdf</a>, <a href="/format/2203.01446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoLoMa: Robust Loco-Manipulation for Quadruped Robots with Arms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrolho%2C+H">Henrique Ferrolho</a>, 
<a href="/search/cs?searchtype=author&query=Ivan%2C+V">Vladimir Ivan</a>, 
<a href="/search/cs?searchtype=author&query=Merkt%2C+W">Wolfgang Merkt</a>, 
<a href="/search/cs?searchtype=author&query=Havoutis%2C+I">Ioannis Havoutis</a>, 
<a href="/search/cs?searchtype=author&query=Vijayakumar%2C+S">Sethu Vijayakumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, accepted to Autonomous Robots. For associated videos, see <a href="https://shorturl.at/oFJU0">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.03958" title="Abstract">arXiv:2203.03958</a> (replaced) [<a href="/pdf/2203.03958" title="Download PDF">pdf</a>, <a href="/format/2203.03958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Betweenness Approximation for Edge Computing with Hypergraph Neural  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yaguang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wenxin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Dengcheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiwen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.12580" title="Abstract">arXiv:2205.12580</a> (replaced) [<a href="/pdf/2205.12580" title="Download PDF">pdf</a>, <a href="/format/2205.12580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On-Demand Redundancy Grouping: Selectable Soft-Error Tolerance for a  Multicore Cluster
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rogenmoser%2C+M">Michael Rogenmoser</a>, 
<a href="/search/cs?searchtype=author&query=Wistoff%2C+N">Nils Wistoff</a>, 
<a href="/search/cs?searchtype=author&query=Vogel%2C+P">Pirmin Vogel</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCrkaynak%2C+F">Frank G&#xfc;rkaynak</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISVLSI (2022) 398-401
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04661" title="Abstract">arXiv:2206.04661</a> (replaced) [<a href="/pdf/2206.04661" title="Download PDF">pdf</a>, <a href="/format/2206.04661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distillation Decision Tree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lu%2C+X">Xuetao Lu</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+J+J">J. Jack Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.06318" title="Abstract">arXiv:2206.06318</a> (replaced) [<a href="/pdf/2206.06318" title="Download PDF">pdf</a>, <a href="/format/2206.06318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limited-Trust in Diffusion of Competing Alternatives over Social  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leon%2C+V">Vincent Leon</a>, 
<a href="/search/cs?searchtype=author&query=Etesami%2C+S+R">S. Rasoul Etesami</a>, 
<a href="/search/cs?searchtype=author&query=Nagi%2C+R">Rakesh Nagi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08479" title="Abstract">arXiv:2206.08479</a> (replaced) [<a href="/pdf/2206.08479" title="Download PDF">pdf</a>, <a href="/format/2206.08479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modifying the Asynchronous Jacobi Method for Data Corruption Resilience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vogl%2C+C+J">Christopher J. Vogl</a>, 
<a href="/search/cs?searchtype=author&query=Atkins%2C+Z">Zach Atkins</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+A">Alyson Fox</a>, 
<a href="/search/cs?searchtype=author&query=Miedlar%2C+A">Agnieszka Miedlar</a>, 
<a href="/search/cs?searchtype=author&query=Ponce%2C+C">Colin Ponce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to SIAM SISC September 29, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.01145" title="Abstract">arXiv:2207.01145</a> (replaced) [<a href="/pdf/2207.01145" title="Download PDF">pdf</a>, <a href="/format/2207.01145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory Population in Continual Learning via Outlier Elimination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hurtado%2C+J">Julio Hurtado</a>, 
<a href="/search/cs?searchtype=author&query=Raymond-Saez%2C+A">Alain Raymond-Saez</a>, 
<a href="/search/cs?searchtype=author&query=Araujo%2C+V">Vladimir Araujo</a>, 
<a href="/search/cs?searchtype=author&query=Lomonaco%2C+V">Vincenzo Lomonaco</a>, 
<a href="/search/cs?searchtype=author&query=Soto%2C+A">Alvaro Soto</a>, 
<a href="/search/cs?searchtype=author&query=Bacciu%2C+D">Davide Bacciu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07726" title="Abstract">arXiv:2207.07726</a> (replaced) [<a href="/pdf/2207.07726" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcribing Medieval Manuscripts for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%C3%A9ville%2C+E">Estelle Gu&#xe9;ville</a>, 
<a href="/search/cs?searchtype=author&query=Wrisley%2C+D+J">David Joseph Wrisley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09534" title="Abstract">arXiv:2207.09534</a> (replaced) [<a href="/pdf/2207.09534" title="Download PDF">pdf</a>, <a href="/format/2207.09534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VisQuiz: Exploring Feedback Mechanisms to Improve Graphical Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Birchfield%2C+R">Ryan Birchfield</a>, 
<a href="/search/cs?searchtype=author&query=Caten%2C+M">Maddison Caten</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+E">Errica Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+M">Madyson Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Larson%2C+T">Truman Larson</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H+P">Hoan Phan Pham</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yiren Ding</a>, 
<a href="/search/cs?searchtype=author&query=Rakotondravony%2C+N">No&#xeb;lle Rakotondravony</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+L">Lane Harrison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, short paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of IEEE Visualization conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09608" title="Abstract">arXiv:2207.09608</a> (replaced) [<a href="/pdf/2207.09608" title="Download PDF">pdf</a>, <a href="/format/2207.09608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probablement, Wahrscheinlich, Likely ? A Cross-Language Study of How  People Verbalize Probabilities in Icon Array Visualizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakotondravony%2C+N">No&#xeb;lle Rakotondravony</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yiren Ding</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+L">Lane Harrison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 10 figures, conference paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Visualization and Computer Graphics 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10827" title="Abstract">arXiv:2207.10827</a> (replaced) [<a href="/pdf/2207.10827" title="Download PDF">pdf</a>, <a href="/format/2207.10827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn and Control while Switching: with Guaranteed Stability and  Sublinear Regret
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chekan%2C+J+A">Jafar Abbaszadeh Chekan</a>, 
<a href="/search/eess?searchtype=author&query=Langbort%2C+C">C&#xe9;dric Langbort</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07497" title="Abstract">arXiv:2208.07497</a> (replaced) [<a href="/pdf/2208.07497" title="Download PDF">pdf</a>, <a href="/format/2208.07497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bucketized Active Sampling for Learning ACOPF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klamkin%2C+M">Michael Klamkin</a>, 
<a href="/search/cs?searchtype=author&query=Tanneau%2C+M">Mathieu Tanneau</a>, 
<a href="/search/cs?searchtype=author&query=Mak%2C+T+W+K">Terrence W.K. Mak</a>, 
<a href="/search/cs?searchtype=author&query=Van+Hentenryck%2C+P">Pascal Van Hentenryck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07708" title="Abstract">arXiv:2208.07708</a> (replaced) [<a href="/pdf/2208.07708" title="Download PDF">pdf</a>, <a href="/ps/2208.07708" title="Download PostScript">ps</a>, <a href="/format/2208.07708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction Methods for Galois LCD codes over Finite Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+G+K">Gyanendra K. Verma</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Astha Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R+K">R. K. Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal of applied mathematics and computing (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.08723" title="Abstract">arXiv:2208.08723</a> (replaced) [<a href="/pdf/2208.08723" title="Download PDF">pdf</a>, <a href="/format/2208.08723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangled Contrastive Learning for Social Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiahao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingfan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengcai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.11838" title="Abstract">arXiv:2208.11838</a> (replaced) [<a href="/pdf/2208.11838" title="Download PDF">pdf</a>, <a href="/format/2208.11838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Task Automata for Reinforcement Learning using Hidden Markov  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abate%2C+A">Alessandro Abate</a> (1), 
<a href="/search/cs?searchtype=author&query=Almulla%2C+Y">Yousif Almulla</a> (2), 
<a href="/search/cs?searchtype=author&query=Fox%2C+J">James Fox</a> (1), 
<a href="/search/cs?searchtype=author&query=Hyland%2C+D">David Hyland</a> (1), 
<a href="/search/cs?searchtype=author&query=Wooldridge%2C+M">Michael Wooldridge</a> (1) ((1) University of Oxford, (2) Microsoft Azure Quantum)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures, Accepted to the 26th European Conference on Artificial Intelligence (ECAI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.14356" title="Abstract">arXiv:2208.14356</a> (replaced) [<a href="/pdf/2208.14356" title="Download PDF">pdf</a>, <a href="/ps/2208.14356" title="Download PostScript">ps</a>, <a href="/format/2208.14356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The syntactic side of autonomous categories enriched over generalised  metric spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dahlqvist%2C+F">Fredrik Dahlqvist</a>, 
<a href="/search/cs?searchtype=author&query=Neves%2C+R">Renato Neves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Journal version of "An Internal Language for Categories Enriched over Generalised Metric Spaces" [<a href="/abs/2105.08473">arXiv:2105.08473</a>] (<a href="https://doi.org/10.4230/LIPIcs.CSL.2022.16">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04894" title="Abstract">arXiv:2209.04894</a> (replaced) [<a href="/pdf/2209.04894" title="Download PDF">pdf</a>, <a href="/ps/2209.04894" title="Download PostScript">ps</a>, <a href="/format/2209.04894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearly all $k$-SAT functions are unate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Balogh%2C+J">J&#xf3;zsef Balogh</a>, 
<a href="/search/math?searchtype=author&query=Dong%2C+D">Dingding Dong</a>, 
<a href="/search/math?searchtype=author&query=Lidick%C3%BD%2C+B">Bernard Lidick&#xfd;</a>, 
<a href="/search/math?searchtype=author&query=Mani%2C+N">Nitya Mani</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+Y">Yufei Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages. v2 merges <a href="/abs/2107.09233">arXiv:2107.09233</a> (SODA22) and <a href="/abs/2209.04894">arXiv:2209.04894v1</a> (STOC23) along with expository improvements. This combined version is intended for journal submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07669" title="Abstract">arXiv:2209.07669</a> (replaced) [<a href="/pdf/2209.07669" title="Download PDF">pdf</a>, <a href="/format/2209.07669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Constrained Reinforcement Learning for Decentralized Real-Time  Voltage Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Feng%2C+J">Jie Feng</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+Y">Yuanyuan Shi</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+G">Guannan Qu</a>, 
<a href="/search/eess?searchtype=author&query=Low%2C+S+H">Steven H. Low</a>, 
<a href="/search/eess?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>, 
<a href="/search/eess?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by TCNS. arXiv admin note: text overlap with <a href="/abs/2109.14854">arXiv:2109.14854</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09404" title="Abstract">arXiv:2209.09404</a> (replaced) [<a href="/pdf/2209.09404" title="Download PDF">pdf</a>, <a href="/format/2209.09404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Approach to Solving Large Bilevel and Stochastic  Programs: Application to Cycling Network Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chan%2C+T+C+Y">Timothy C. Y. Chan</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+B">Bo Lin</a>, 
<a href="/search/math?searchtype=author&query=Saxe%2C+S">Shoshanna Saxe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11209" title="Abstract">arXiv:2209.11209</a> (replaced) [<a href="/pdf/2209.11209" title="Download PDF">pdf</a>, <a href="/format/2209.11209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Approximation Algorithms by Generalizing the Primal-Dual Method  Beyond Uncrossable Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bansal%2C+I">Ishan Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Cheriyan%2C+J">Joseph Cheriyan</a>, 
<a href="/search/cs?searchtype=author&query=Grout%2C+L">Logan Grout</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahimpur%2C+S">Sharat Ibrahimpur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated v2, improved exposition and organization, added another application of main theorem, other results and proofs are the same
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00193" title="Abstract">arXiv:2210.00193</a> (replaced) [<a href="/pdf/2210.00193" title="Download PDF">pdf</a>, <a href="/format/2210.00193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Riley%2C+P">Parker Riley</a>, 
<a href="/search/cs?searchtype=author&query=Dozat%2C+T">Timothy Dozat</a>, 
<a href="/search/cs?searchtype=author&query=Botha%2C+J+A">Jan A. Botha</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+X">Xavier Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Garrette%2C+D">Dan Garrette</a>, 
<a href="/search/cs?searchtype=author&query=Riesa%2C+J">Jason Riesa</a>, 
<a href="/search/cs?searchtype=author&query=Firat%2C+O">Orhan Firat</a>, 
<a href="/search/cs?searchtype=author&query=Constant%2C+N">Noah Constant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in TACL Vol. 11 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00383" title="Abstract">arXiv:2210.00383</a> (replaced) [<a href="/pdf/2210.00383" title="Download PDF">pdf</a>, <a href="/ps/2210.00383" title="Download PostScript">ps</a>, <a href="/format/2210.00383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On minimally tough chordal graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dallard%2C+C">Cl&#xe9;ment Dallard</a>, 
<a href="/search/math?searchtype=author&query=Fern%C3%A1ndez%2C+B">Blas Fern&#xe1;ndez</a>, 
<a href="/search/math?searchtype=author&query=Katona%2C+G+Y">Gyula Y. Katona</a>, 
<a href="/search/math?searchtype=author&query=Milani%C4%8D%2C+M">Martin Milani&#x10d;</a>, 
<a href="/search/math?searchtype=author&query=Varga%2C+K">Kitti Varga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00654" title="Abstract">arXiv:2210.00654</a> (replaced) [<a href="/pdf/2210.00654" title="Download PDF">pdf</a>, <a href="/format/2210.00654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relational Models for the Lambek Calculus with Intersection and  Constants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+S+L">Stepan L. Kuznetsov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article is an extended version of the conference paper presented at RAMiCS 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00832" title="Abstract">arXiv:2210.00832</a> (replaced) [<a href="/pdf/2210.00832" title="Download PDF">pdf</a>, <a href="/format/2210.00832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Square-root regret bounds for continuous-time episodic Markov decision  processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xuefeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X+Y">Xun Yu Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02166" title="Abstract">arXiv:2210.02166</a> (replaced) [<a href="/pdf/2210.02166" title="Download PDF">pdf</a>, <a href="/format/2210.02166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Bayesian Inference for Moving Horizon Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cao%2C+W">Wenhan Cao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lan%2C+Z">Zhiqian Lan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+W">Wei Pan</a>, 
<a href="/search/eess?searchtype=author&query=Alessandri%2C+A">Angelo Alessandri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02808" title="Abstract">arXiv:2210.02808</a> (replaced) [<a href="/pdf/2210.02808" title="Download PDF">pdf</a>, <a href="/format/2210.02808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Self-supervised Pre-training on Low-compute Networks without  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+F">Fuwen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Saleh%2C+F">Fatemeh Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+B">Brais Martinez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023 Camera Ready. Code is publicly available at <a href="https://github.com/saic-fi/SSLight">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.06282" title="Abstract">arXiv:2210.06282</a> (replaced) [<a href="/pdf/2210.06282" title="Download PDF">pdf</a>, <a href="/format/2210.06282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DialoGen: Generalized Long-Range Context Representation for Dialogue  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+S">Suvodip Dey</a>, 
<a href="/search/cs?searchtype=author&query=Desarkar%2C+M+S">Maunendra Sankar Desarkar</a>, 
<a href="/search/cs?searchtype=author&query=Ekbal%2C+A">Asif Ekbal</a>, 
<a href="/search/cs?searchtype=author&query=Srijith%2C+P+K">P.K. Srijith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at PACLIC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07467" title="Abstract">arXiv:2210.07467</a> (replaced) [<a href="/pdf/2210.07467" title="Download PDF">pdf</a>, <a href="/format/2210.07467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query Rewriting for Effective Misinformation Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazemi%2C+A">Ashkan Kazemi</a>, 
<a href="/search/cs?searchtype=author&query=Abzaliev%2C+A">Artem Abzaliev</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S+A">Scott A. Hale</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Rosas%2C+V">Ver&#xf3;nica P&#xe9;rez-Rosas</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AACL 2023 (long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.14267" title="Abstract">arXiv:2210.14267</a> (replaced) [<a href="/pdf/2210.14267" title="Download PDF">pdf</a>, <a href="/format/2210.14267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Deep Generative 3D-aware Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Weihao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jing-Hao Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM Computing Surveys. Project page: <a href="https://weihaox.github.io/3D-aware-Gen">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11298" title="Abstract">arXiv:2211.11298</a> (replaced) [<a href="/pdf/2211.11298" title="Download PDF">pdf</a>, <a href="/format/2211.11298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Physical Latent Spaces for High-Resolution Flow Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paliard%2C+C">Chloe Paliard</a>, 
<a href="/search/cs?searchtype=author&query=Thuerey%2C+N">Nils Thuerey</a>, 
<a href="/search/cs?searchtype=author&query=Um%2C+K">Kiwon Um</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13976" title="Abstract">arXiv:2211.13976</a> (replaced) [<a href="/pdf/2211.13976" title="Download PDF">pdf</a>, <a href="/format/2211.13976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expanding Small-Scale Datasets with Guided Imagination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Daquan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiashi Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Source code: <a href="https://github.com/Vanint/DatasetExpansion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.12921" title="Abstract">arXiv:2212.12921</a> (replaced) [<a href="/pdf/2212.12921" title="Download PDF">pdf</a>, <a href="/format/2212.12921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning k-Level Sparse Neural Networks Using a New Generalized Weighted  Group Sparse Envelope Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Refael%2C+Y">Yehonathan Refael</a>, 
<a href="/search/cs?searchtype=author&query=Arbel%2C+I">Iftach Arbel</a>, 
<a href="/search/cs?searchtype=author&query=Huleihel%2C+W">Wasim Huleihel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13406" title="Abstract">arXiv:2212.13406</a> (replaced) [<a href="/pdf/2212.13406" title="Download PDF">pdf</a>, <a href="/format/2212.13406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Cuts in Hypergraphs from Random Walks on Simplicial Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Louis%2C+A">Anand Louis</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+R">Rameesh Paul</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+A">Arka Ray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02950" title="Abstract">arXiv:2301.02950</a> (replaced) [<a href="/pdf/2301.02950" title="Download PDF">pdf</a>, <a href="/format/2301.02950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry of Set Functions in Game Theory: Combinatorial and  Computational Aspects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mermoud%2C+D+L">Dylan Laplace Mermoud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The author's PhD thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05785" title="Abstract">arXiv:2301.05785</a> (replaced) [<a href="/pdf/2301.05785" title="Download PDF">pdf</a>, <a href="/format/2301.05785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Activation Function Optimization through Surrogate Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bingham%2C+G">Garrett Bingham</a>, 
<a href="/search/cs?searchtype=author&query=Miikkulainen%2C+R">Risto Miikkulainen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. 28 pages, 16 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12227" title="Abstract">arXiv:2301.12227</a> (replaced) [<a href="/pdf/2301.12227" title="Download PDF">pdf</a>, <a href="/format/2301.12227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Operator Learning Lessens the Curse of Dimensionality for PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Ke Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunmei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haizhao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00674" title="Abstract">arXiv:2302.00674</a> (replaced) [<a href="/pdf/2302.00674" title="Download PDF">pdf</a>, <a href="/format/2302.00674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Albalak%2C+A">Alon Albalak</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, 25 pages, 8 figures, code available at <a href="https://github.com/alon-albalak/FLAD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05825" title="Abstract">arXiv:2302.05825</a> (replaced) [<a href="/pdf/2302.05825" title="Download PDF">pdf</a>, <a href="/format/2302.05825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Koopman-based generalization bound: New aspect for full-rank weights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+Y">Yuka Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Sonoda%2C+S">Sho Sonoda</a>, 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+I">Isao Ishikawa</a>, 
<a href="/search/cs?searchtype=author&query=Nitanda%2C+A">Atsushi Nitanda</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Taiji Suzuki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Functional Analysis (math.FA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05988" title="Abstract">arXiv:2302.05988</a> (replaced) [<a href="/pdf/2302.05988" title="Download PDF">pdf</a>, <a href="/format/2302.05988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When data driven reduced order modeling meets full waveform inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Borcea%2C+L">Liliana Borcea</a>, 
<a href="/search/math?searchtype=author&query=Garnier%2C+J">Josselin Garnier</a>, 
<a href="/search/math?searchtype=author&query=Mamonov%2C+A+V">Alexander V. Mamonov</a>, 
<a href="/search/math?searchtype=author&query=Zimmerling%2C+J">J&#xf6;rn Zimmerling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Geophysics (physics.geo-ph)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10130" title="Abstract">arXiv:2302.10130</a> (replaced) [<a href="/pdf/2302.10130" title="Download PDF">pdf</a>, <a href="/format/2302.10130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infinite-Dimensional Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pidstrigach%2C+J">Jakiw Pidstrigach</a>, 
<a href="/search/stat?searchtype=author&query=Marzouk%2C+Y">Youssef Marzouk</a>, 
<a href="/search/stat?searchtype=author&query=Reich%2C+S">Sebastian Reich</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+S">Sven Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00492" title="Abstract">arXiv:2303.00492</a> (replaced) [<a href="/pdf/2303.00492" title="Download PDF">pdf</a>, <a href="/format/2303.00492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized  Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Qiying Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+L">Lingyang Chu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, published in the Proceedings of the 39th IEEE International Conference on Data Engineering (ICDE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03031" title="Abstract">arXiv:2303.03031</a> (replaced) [<a href="/pdf/2303.03031" title="Download PDF">pdf</a>, <a href="/format/2303.03031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Computational Landscape of Autonomous Mobile Robots: The Visibility  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Archak Das</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Satakshi Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Avisek Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+P">Pritam Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Sau%2C+B">Buddhadeb Sau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2203.06546">arXiv:2203.06546</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05628" title="Abstract">arXiv:2303.05628</a> (replaced) [<a href="/pdf/2303.05628" title="Download PDF">pdf</a>, <a href="/format/2303.05628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Unlikelihood of D-Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feigenbaum%2C+I">Itai Feigenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+S">Shelby Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=Niebles%2C+J+C">Juan Carlos Niebles</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Weiran Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Arpit%2C+D">Devansh Arpit</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08250" title="Abstract">arXiv:2303.08250</a> (replaced) [<a href="/pdf/2303.08250" title="Download PDF">pdf</a>, <a href="/format/2303.08250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transforming Transformers for Resilient Lifelong Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Savadikar%2C+C">Chinmay Savadikar</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+M">Michelle Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianfu Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09590" title="Abstract">arXiv:2303.09590</a> (replaced) [<a href="/pdf/2303.09590" title="Download PDF">pdf</a>, <a href="/format/2303.09590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Analytics of Multivariate Networks with Representation Learning  and Composite Variable Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hsiao-Ying Lu</a>, 
<a href="/search/cs?searchtype=author&query=Fujiwara%2C+T">Takanori Fujiwara</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Ming-Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yang-chih Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ynnerman%2C+A">Anders Ynnerman</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kwan-Liu Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The previous version of this manuscript was accepted for publication by a journal. We decided to withdraw the version because the journal made an unacceptable requirement for us: change of the authors' affiliations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13093" title="Abstract">arXiv:2303.13093</a> (replaced) [<a href="/pdf/2303.13093" title="Download PDF">pdf</a>, <a href="/format/2303.13093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Probabilistic Stability of Stochastic Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziyin%2C+L">Liu Ziyin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Botao Li</a>, 
<a href="/search/cs?searchtype=author&query=Galanti%2C+T">Tomer Galanti</a>, 
<a href="/search/cs?searchtype=author&query=Ueda%2C+M">Masahito Ueda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint with revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13372" title="Abstract">arXiv:2303.13372</a> (replaced) [<a href="/pdf/2303.13372" title="Download PDF">pdf</a>, <a href="/format/2303.13372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified  Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Shoumik Saha</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kaya%2C+Y">Yigitcan Kaya</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Dumitras%2C+T">Tudor Dumitras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04736" title="Abstract">arXiv:2304.04736</a> (replaced) [<a href="/pdf/2304.04736" title="Download PDF">pdf</a>, <a href="/format/2304.04736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Possibilities of AI-Generated Text Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Sicheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06293" title="Abstract">arXiv:2304.06293</a> (replaced) [<a href="/pdf/2304.06293" title="Download PDF">pdf</a>, <a href="/format/2304.06293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A class of monotonicity-preserving variable-step discretizations for  Volterra integral equations and time fractional ordinary differential  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Feng%2C+Y">Yuanyuan Feng</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06656" title="Abstract">arXiv:2304.06656</a> (replaced) [<a href="/pdf/2304.06656" title="Download PDF">pdf</a>, <a href="/format/2304.06656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Approximations for Relative Survivable Network Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dinitz%2C+M">Michael Dinitz</a>, 
<a href="/search/cs?searchtype=author&query=Koranteng%2C+A">Ama Koranteng</a>, 
<a href="/search/cs?searchtype=author&query=Kortsarz%2C+G">Guy Kortsarz</a>, 
<a href="/search/cs?searchtype=author&query=Nutov%2C+Z">Zeev Nutov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08612" title="Abstract">arXiv:2304.08612</a> (replaced) [<a href="/pdf/2304.08612" title="Download PDF">pdf</a>, <a href="/format/2304.08612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Discrete and Backpropagation: Straight-Through and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chengyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08685" title="Abstract">arXiv:2304.08685</a> (replaced) [<a href="/pdf/2304.08685" title="Download PDF">pdf</a>, <a href="/format/2304.08685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-and-Hold Safety with Control Barrier Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bahati%2C+G">Gilbert Bahati</a>, 
<a href="/search/eess?searchtype=author&query=Ong%2C+P">Pio Ong</a>, 
<a href="/search/eess?searchtype=author&query=Ames%2C+A+D">Aaron D. Ames</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE American Control Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09418" title="Abstract">arXiv:2304.09418</a> (replaced) [<a href="/pdf/2304.09418" title="Download PDF">pdf</a>, <a href="/format/2304.09418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden convexity in the heat, linear transport, and Euler&#x27;s rigid body  equations: A computational approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kouskiya%2C+U">Uditnarayan Kouskiya</a>, 
<a href="/search/math?searchtype=author&query=Acharya%2C+A">Amit Acharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11328" title="Abstract">arXiv:2304.11328</a> (replaced) [<a href="/pdf/2304.11328" title="Download PDF">pdf</a>, <a href="/ps/2304.11328" title="Download PostScript">ps</a>, <a href="/format/2304.11328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Accelerating Diffusion-Based Sampling Process via Improved  Integration Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guoqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kenta%2C+N">Niwa Kenta</a>, 
<a href="/search/cs?searchtype=author&query=Kleijn%2C+W+B">W. Bastiaan Kleijn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12388" title="Abstract">arXiv:2304.12388</a> (replaced) [<a href="/pdf/2304.12388" title="Download PDF">pdf</a>, <a href="/ps/2304.12388" title="Download PostScript">ps</a>, <a href="/format/2304.12388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifying the First Nonzero Term: Physical ZKPs for ABC End View, Goishi  Hiroi, and Toichika
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruangwises%2C+S">Suthee Ruangwises</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preliminary version of this paper has appeared at FAW 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12541" title="Abstract">arXiv:2304.12541</a> (replaced) [<a href="/pdf/2304.12541" title="Download PDF">pdf</a>, <a href="/format/2304.12541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Bayesian inference using physics-informed invertible neural  networks for inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guan%2C+X">Xiaofei Guan</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xintong Wang</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+H">Hao Wu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+Z">Zihao Yang</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+P">Peng Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14069" title="Abstract">arXiv:2304.14069</a> (replaced) [<a href="/pdf/2304.14069" title="Download PDF">pdf</a>, <a href="/ps/2304.14069" title="Download PostScript">ps</a>, <a href="/format/2304.14069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting unate and balanced monotone Boolean functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Biswas%2C+A">Aniruddha Biswas</a>, 
<a href="/search/math?searchtype=author&query=Sarkar%2C+P">Palash Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14540" title="Abstract">arXiv:2304.14540</a> (replaced) [<a href="/pdf/2304.14540" title="Download PDF">pdf</a>, <a href="/format/2304.14540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Greybox Penetration Testing for Cloud Access Control using  IAM Modeling and Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Khurshid%2C+S">Sarfraz Khurshid</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mohit Tiwari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02154" title="Abstract">arXiv:2305.02154</a> (replaced) [<a href="/pdf/2305.02154" title="Download PDF">pdf</a>, <a href="/format/2305.02154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral bound for random Schreier graphs of the general linear group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Caillat-Grenier%2C+G">Geoffroy Caillat-Grenier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04195" title="Abstract">arXiv:2305.04195</a> (replaced) [<a href="/pdf/2305.04195" title="Download PDF">pdf</a>, <a href="/format/2305.04195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Modal Retrieval for Motion and Text via DopTriple Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Sheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xin Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by ACM MM Asia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04797" title="Abstract">arXiv:2305.04797</a> (replaced) [<a href="/pdf/2305.04797" title="Download PDF">pdf</a>, <a href="/format/2305.04797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli  SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyowon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Fern%C3%A1ndez%2C+A+F">Angel F. Garc&#xed;a-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yu Ge</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yuxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Svensson%2C+L">Lennart Svensson</a>, 
<a href="/search/cs?searchtype=author&query=Wymeersch%2C+H">Henk Wymeersch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06917" title="Abstract">arXiv:2305.06917</a> (replaced) [<a href="/pdf/2305.06917" title="Download PDF">pdf</a>, <a href="/format/2305.06917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Machine Learning Approach to Improving Timing Consistency between  Global Route and Detailed Route
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chhabria%2C+V+A">Vidya A. Chhabria</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenjing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kahng%2C+A+B">Andrew B. Kahng</a>, 
<a href="/search/cs?searchtype=author&query=Sapatnekar%2C+S+S">Sachin S. Sapatnekar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09241" title="Abstract">arXiv:2305.09241</a> (replaced) [<a href="/pdf/2305.09241" title="Download PDF">pdf</a>, <a href="/format/2305.09241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlearnable Examples Give a False Sense of Security: Piercing through  Unexploitable Data with Learnable Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+Y">Yunfeng Diao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">He Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jianxin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+R">Richang Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10616" title="Abstract">arXiv:2305.10616</a> (replaced) [<a href="/pdf/2305.10616" title="Download PDF">pdf</a>, <a href="/format/2305.10616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation Metrics for DNNs Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghobrial%2C+A">Abanoub Ghobrial</a>, 
<a href="/search/cs?searchtype=author&query=Budgett%2C+S">Samuel Budgett</a>, 
<a href="/search/cs?searchtype=author&query=Balemans%2C+D">Dieter Balemans</a>, 
<a href="/search/cs?searchtype=author&query=Asgari%2C+H">Hamid Asgari</a>, 
<a href="/search/cs?searchtype=author&query=Reiter%2C+P">Phil Reiter</a>, 
<a href="/search/cs?searchtype=author&query=Eder%2C+K">Kerstin Eder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12091" title="Abstract">arXiv:2305.12091</a> (replaced) [<a href="/pdf/2305.12091" title="Download PDF">pdf</a>, <a href="/format/2305.12091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;What do others think?&quot;: Task-Oriented Conversational Modeling with  Subjective Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gella%2C+S">Spandana Gella</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seokhwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Di Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hazarika%2C+D">Devamanyu Hazarika</a>, 
<a href="/search/cs?searchtype=author&query=Papangelis%2C+A">Alexandros Papangelis</a>, 
<a href="/search/cs?searchtype=author&query=Hedayatnia%2C+B">Behnam Hedayatnia</a>, 
<a href="/search/cs?searchtype=author&query=Namazifar%2C+M">Mahdi Namazifar</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hakkani-Tur%2C+D">Dilek Hakkani-Tur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGDIAL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13057" title="Abstract">arXiv:2305.13057</a> (replaced) [<a href="/pdf/2305.13057" title="Download PDF">pdf</a>, <a href="/format/2305.13057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causality-Aided Trade-off Analysis for Machine Learning Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zhenlan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Pingchuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanhui Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13269" title="Abstract">arXiv:2305.13269</a> (replaced) [<a href="/pdf/2305.13269" title="Download PDF">pdf</a>, <a href="/format/2305.13269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Knowledge: Grounding Large Language Models via Dynamic  Knowledge Adapting over Heterogeneous Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruochen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chia%2C+Y+K">Yew Ken Chia</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bosheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13300" title="Abstract">arXiv:2305.13300</a> (replaced) [<a href="/pdf/2305.13300" title="Download PDF">pdf</a>, <a href="/format/2305.13300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large  Language Models in Knowledge Conflicts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+R">Renze Lou</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14122" title="Abstract">arXiv:2305.14122</a> (replaced) [<a href="/pdf/2305.14122" title="Download PDF">pdf</a>, <a href="/format/2305.14122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transferring Learning Trajectories of Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chijiwa%2C+D">Daiki Chijiwa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: updates include theoretical analysis and additional experiments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15759" title="Abstract">arXiv:2305.15759</a> (replaced) [<a href="/pdf/2305.15759" title="Download PDF">pdf</a>, <a href="/format/2305.15759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Latent Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lyu%2C+S">Saiyue Lyu</a>, 
<a href="/search/stat?searchtype=author&query=Vinaroz%2C+M">Margarita Vinaroz</a>, 
<a href="/search/stat?searchtype=author&query=Liu%2C+M+F">Michael F. Liu</a>, 
<a href="/search/stat?searchtype=author&query=Park%2C+M">Mijung Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16446" title="Abstract">arXiv:2305.16446</a> (replaced) [<a href="/pdf/2305.16446" title="Download PDF">pdf</a>, <a href="/format/2305.16446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Representation Jensen-Shannon Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoyos-Osorio%2C+J+K">Jhoan K. Hoyos-Osorio</a>, 
<a href="/search/cs?searchtype=author&query=Posso-Murillo%2C+S">Santiago Posso-Murillo</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez-Giraldo%2C+L+G">Luis G. Sanchez-Giraldo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17692" title="Abstract">arXiv:2305.17692</a> (replaced) [<a href="/pdf/2305.17692" title="Download PDF">pdf</a>, <a href="/format/2305.17692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication Over Entanglement-Breaking Channels With Unreliable  Entanglement Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pereg%2C+U">Uzi Pereg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17767" title="Abstract">arXiv:2305.17767</a> (replaced) [<a href="/pdf/2305.17767" title="Download PDF">pdf</a>, <a href="/format/2305.17767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Alpha Algorithm To Enable Real-Life Process Discovery  Applications -- Extended Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%BCsters%2C+A">Aaron K&#xfc;sters</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Aalst%2C+W+M+P">Wil M.P. van der Aalst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, 97 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18030" title="Abstract">arXiv:2305.18030</a> (replaced) [<a href="/pdf/2305.18030" title="Download PDF">pdf</a>, <a href="/format/2305.18030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Search-Space Generation Neural Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Luming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+T">Tianyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zharkov%2C+I">Ilya Zharkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Graph visualization for DARTS, SuperResNet are omitted for arXiv version due to exceeding page dimension limit. Please refer to the open-review version for taking the visualizations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18403" title="Abstract">arXiv:2305.18403</a> (replaced) [<a href="/pdf/2305.18403" title="Download PDF">pdf</a>, <a href="/format/2305.18403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+L">Linlin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xinyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18593" title="Abstract">arXiv:2305.18593</a> (replaced) [<a href="/pdf/2305.18593" title="Download PDF">pdf</a>, <a href="/format/2305.18593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Diffusion Modeling for Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Livernoche%2C+V">Victor Livernoche</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vineet Jain</a>, 
<a href="/search/cs?searchtype=author&query=Hezaveh%2C+Y">Yashar Hezaveh</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19337" title="Abstract">arXiv:2305.19337</a> (replaced) [<a href="/pdf/2305.19337" title="Download PDF">pdf</a>, <a href="/format/2305.19337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiGen: Hierarchical Graph Generative Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karami%2C+M">Mahdi Karami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00814" title="Abstract">arXiv:2306.00814</a> (replaced) [<a href="/pdf/2306.00814" title="Download PDF">pdf</a>, <a href="/format/2306.00814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vocos: Closing the gap between time-domain and Fourier-based neural  vocoders for high-quality audio synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siuzdak%2C+H">Hubert Siuzdak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01102" title="Abstract">arXiv:2306.01102</a> (replaced) [<a href="/pdf/2306.01102" title="Download PDF">pdf</a>, <a href="/format/2306.01102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMatic: Neural Architecture Search via Large Language Models and  Quality Diversity Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasir%2C+M+U">Muhammad U. Nasir</a>, 
<a href="/search/cs?searchtype=author&query=Earle%2C+S">Sam Earle</a>, 
<a href="/search/cs?searchtype=author&query=Togelius%2C+J">Julian Togelius</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+S">Steven James</a>, 
<a href="/search/cs?searchtype=author&query=Cleghorn%2C+C">Christopher Cleghorn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03324" title="Abstract">arXiv:2306.03324</a> (replaced) [<a href="/pdf/2306.03324" title="Download PDF">pdf</a>, <a href="/format/2306.03324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Large Language Models on Generating Software Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+D">Danning Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+B">Byungwoo Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Mijung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+L">Lin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+S">Judy S. Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04204" title="Abstract">arXiv:2306.04204</a> (replaced) [<a href="/pdf/2306.04204" title="Download PDF">pdf</a>, <a href="/ps/2306.04204" title="Download PostScript">ps</a>, <a href="/format/2306.04204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitoring Blackbox Implementations of Multiparty Session Protocols
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+den+Heuvel%2C+B">Bas van den Heuvel</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+J+A">Jorge A. P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Dobre%2C+R+A">Rares A. Dobre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Full version with appendices of our RV'23 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05272" title="Abstract">arXiv:2306.05272</a> (replaced) [<a href="/pdf/2306.05272" title="Download PDF">pdf</a>, <a href="/format/2306.05272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Clustering via the Principle of Rate Reduction in the Age of  Pretrained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+T">Tianzhe Chu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Shengbang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+T">Tianjiao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xili Dai</a>, 
<a href="/search/cs?searchtype=author&query=Haeffele%2C+B+D">Benjamin David Haeffele</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+R">Ren&#xe9; Vidal</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06192" title="Abstract">arXiv:2306.06192</a> (replaced) [<a href="/pdf/2306.06192" title="Download PDF">pdf</a>, <a href="/format/2306.06192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for  Robotic Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+B">Bhrij Patel</a>, 
<a href="/search/cs?searchtype=author&query=Weerakoon%2C+K">Kasun Weerakoon</a>, 
<a href="/search/cs?searchtype=author&query=Suttle%2C+W+A">Wesley A. Suttle</a>, 
<a href="/search/cs?searchtype=author&query=Koppel%2C+A">Alec Koppel</a>, 
<a href="/search/cs?searchtype=author&query=Sadler%2C+B+M">Brian M. Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06900" title="Abstract">arXiv:2306.06900</a> (replaced) [<a href="/pdf/2306.06900" title="Download PDF">pdf</a>, <a href="/format/2306.06900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Knee Joint Angle Prediction through Dynamic Contextual Focus  and Gated Linear Units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saoud%2C+L+S">Lyes Saad Saoud</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+H">Humaid Ibrahim</a>, 
<a href="/search/cs?searchtype=author&query=Aljarah%2C+A">Ahmad Aljarah</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+I">Irfan Hussain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under consideration at Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07888" title="Abstract">arXiv:2306.07888</a> (replaced) [<a href="/pdf/2306.07888" title="Download PDF">pdf</a>, <a href="/format/2306.07888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMEO: A Causal Transfer Learning Approach for Performance Optimization  of Configurable Computer Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+M+S">Md Shahriar Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Ziyuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+I">Iftakhar Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+B">Baishakhi Ray</a>, 
<a href="/search/cs?searchtype=author&query=Jamshidi%2C+P">Pooyan Jamshidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Software Engineering (cs.SE); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10577" title="Abstract">arXiv:2306.10577</a> (replaced) [<a href="/pdf/2306.10577" title="Download PDF">pdf</a>, <a href="/format/2306.10577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenDataVal: a Unified Benchmark for Data Valuation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K+F">Kevin Fu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+W">Weixin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+Y">Yongchan Kwon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, NeurIPS 2023 Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11238" title="Abstract">arXiv:2306.11238</a> (replaced) [<a href="/pdf/2306.11238" title="Download PDF">pdf</a>, <a href="/format/2306.11238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMP-Net: Consistency-Aware Multi-Prior Network for Accelerated MRI  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Liping Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaobo Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Weitian Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13575" title="Abstract">arXiv:2306.13575</a> (replaced) [<a href="/pdf/2306.13575" title="Download PDF">pdf</a>, <a href="/format/2306.13575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling MLPs: A Tale of Inductive Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bachmann%2C+G">Gregor Bachmann</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13649" title="Abstract">arXiv:2306.13649</a> (replaced) [<a href="/pdf/2306.13649" title="Download PDF">pdf</a>, <a href="/format/2306.13649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Knowledge Distillation for Auto-regressive Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+R">Rishabh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Vieillard%2C+N">Nino Vieillard</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yongchao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Stanczyk%2C+P">Piotr Stanczyk</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+S">Sabela Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Geist%2C+M">Matthieu Geist</a>, 
<a href="/search/cs?searchtype=author&query=Bachem%2C+O">Olivier Bachem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally. Added new results and experiment details
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00481" title="Abstract">arXiv:2307.00481</a> (replaced) [<a href="/pdf/2307.00481" title="Download PDF">pdf</a>, <a href="/format/2307.00481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing is not Believing: An Identity Hider for Human Vision Privacy  Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yushu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zixuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Z">Zhongyun Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00724" title="Abstract">arXiv:2307.00724</a> (replaced) [<a href="/pdf/2307.00724" title="Download PDF">pdf</a>, <a href="/format/2307.00724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and  Camera Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Weiyi Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Q">Qing-Long Han</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yuxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Intelligent Vehicles
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01062" title="Abstract">arXiv:2307.01062</a> (replaced) [<a href="/pdf/2307.01062" title="Download PDF">pdf</a>, <a href="/format/2307.01062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Driven Approach to Geometric Modeling of Systems with  Low-Bandwidth Actuator Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Siming Deng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+B">Bibekananda Datta</a>, 
<a href="/search/cs?searchtype=author&query=Pantula%2C+A">Aishwarya Pantula</a>, 
<a href="/search/cs?searchtype=author&query=Gracias%2C+D+H">David H. Gracias</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+D">Thao D. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Bittner%2C+B+A">Brian A. Bittner</a>, 
<a href="/search/cs?searchtype=author&query=Cowan%2C+N+J">Noah J. Cowan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01686" title="Abstract">arXiv:2307.01686</a> (replaced) [<a href="/pdf/2307.01686" title="Download PDF">pdf</a>, <a href="/ps/2307.01686" title="Download PostScript">ps</a>, <a href="/format/2307.01686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transaction Fee Mechanism Design with Active Block Producers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bahrani%2C+M">Maryam Bahrani</a>, 
<a href="/search/cs?searchtype=author&query=Garimidi%2C+P">Pranav Garimidi</a>, 
<a href="/search/cs?searchtype=author&query=Roughgarden%2C+T">Tim Roughgarden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02738" title="Abstract">arXiv:2307.02738</a> (replaced) [<a href="/pdf/2307.02738" title="Download PDF">pdf</a>, <a href="/format/2307.02738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecallM: An Adaptable Memory Mechanism with Temporal Understanding for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kynoch%2C+B">Brandon Kynoch</a>, 
<a href="/search/cs?searchtype=author&query=Latapie%2C+H">Hugo Latapie</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Sluis%2C+D">Dwane van der Sluis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, 1 table, Our code is publicly available online at: <a href="https://github.com/cisco-open/DeepVision/tree/main/recallm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Symbolic Computation (cs.SC)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02827" title="Abstract">arXiv:2307.02827</a> (replaced) [<a href="/pdf/2307.02827" title="Download PDF">pdf</a>, <a href="/format/2307.02827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cell-Free XL-MIMO Meets Multi-Agent Reinforcement Learning:  Architectures, Challenges, and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhilong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Guizani%2C+M">Mohsen Guizani</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+B">Bo Ai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures, accepted by IEEE Wireless Communications Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03266" title="Abstract">arXiv:2307.03266</a> (replaced) [<a href="/pdf/2307.03266" title="Download PDF">pdf</a>, <a href="/format/2307.03266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Analysis of a Segmentation Foundation Model in Prostate  Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kim%2C+H">Heejong Kim</a>, 
<a href="/search/eess?searchtype=author&query=Butoi%2C+V+I">Victor Ion Butoi</a>, 
<a href="/search/eess?searchtype=author&query=Dalca%2C+A+V">Adrian V. Dalca</a>, 
<a href="/search/eess?searchtype=author&query=Margolis%2C+D+J+A">Daniel J.A. Margolis</a>, 
<a href="/search/eess?searchtype=author&query=Sabuncu%2C+M+R">Mert R. Sabuncu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to MICCAI MedAGI workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03690" title="Abstract">arXiv:2307.03690</a> (replaced) [<a href="/pdf/2307.03690" title="Download PDF">pdf</a>, <a href="/format/2307.03690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Suppressing unknown disturbances to dynamical systems using machine  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Restrepo%2C+J+G">Juan G. Restrepo</a>, 
<a href="/search/eess?searchtype=author&query=Byers%2C+C+P">Clayton P. Byers</a>, 
<a href="/search/eess?searchtype=author&query=Skardal%2C+P+S">Per Sebastian Skardal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 15 figures (including supplemental material)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05466" title="Abstract">arXiv:2307.05466</a> (replaced) [<a href="/pdf/2307.05466" title="Download PDF">pdf</a>, <a href="/format/2307.05466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Tolling in Arc-based Traffic Assignment Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chiu%2C+C">Chih-Yuan Chiu</a>, 
<a href="/search/eess?searchtype=author&query=Maheshwari%2C+C">Chinmay Maheshwari</a>, 
<a href="/search/eess?searchtype=author&query=Su%2C+P">Pan-Yang Su</a>, 
<a href="/search/eess?searchtype=author&query=Sastry%2C+S">Shankar Sastry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, 2 tables. arXiv admin note: text overlap with <a href="/abs/2304.04705">arXiv:2304.04705</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06695" title="Abstract">arXiv:2307.06695</a> (replaced) [<a href="/pdf/2307.06695" title="Download PDF">pdf</a>, <a href="/format/2307.06695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Traitor Tracing in Black-and-White-Box DNN Watermarking with  Tardos-based Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodriguez-Lois%2C+E">Elena Rodriguez-Lois</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Gonzalez%2C+F">Fernando Perez-Gonzalez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \c{opyright} 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06777" title="Abstract">arXiv:2307.06777</a> (replaced) [<a href="/pdf/2307.06777" title="Download PDF">pdf</a>, <a href="/format/2307.06777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciding Conjugacy of a Rational Relation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aiswarya%2C+C">C. Aiswarya</a>, 
<a href="/search/cs?searchtype=author&query=Manuel%2C+A">Amaldev Manuel</a>, 
<a href="/search/cs?searchtype=author&query=Sunny%2C+S">Saina Sunny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06945" title="Abstract">arXiv:2307.06945</a> (replaced) [<a href="/pdf/2307.06945" title="Download PDF">pdf</a>, <a href="/format/2307.06945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-context Autoencoder for Context Compression in a Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+T">Tao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si-Qing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2 (19 pages) with the code, data and model released
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08822" title="Abstract">arXiv:2307.08822</a> (replaced) [<a href="/pdf/2307.08822" title="Download PDF">pdf</a>, <a href="/format/2307.08822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting  Multiple Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Loli%2C+R+C">Rafael Cerna Loli</a>, 
<a href="/search/eess?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09520" title="Abstract">arXiv:2307.09520</a> (replaced) [<a href="/pdf/2307.09520" title="Download PDF">pdf</a>, <a href="/format/2307.09520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Bayesian Augmentation for Single-Source Domain  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sheng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Gokhale%2C+T">Tejas Gokhale</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yezhou Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10490" title="Abstract">arXiv:2307.10490</a> (replaced) [<a href="/pdf/2307.10490" title="Download PDF">pdf</a>, <a href="/format/2307.10490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Abusing Images and Sounds for Indirect Instruction Injection in  Multi-Modal LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bagdasaryan%2C+E">Eugene Bagdasaryan</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+T">Tsung-Yin Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Nassi%2C+B">Ben Nassi</a>, 
<a href="/search/cs?searchtype=author&query=Shmatikov%2C+V">Vitaly Shmatikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12360" title="Abstract">arXiv:2307.12360</a> (replaced) [<a href="/pdf/2307.12360" title="Download PDF">pdf</a>, <a href="/format/2307.12360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unravelling the Mechanics of Knitted Fabrics Through Hierarchical  Geometric Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Ding%2C+X">Xiaoxiao Ding</a>, 
<a href="/search/cond-mat?searchtype=author&query=Sanchez%2C+V">Vanessa Sanchez</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bertoldi%2C+K">Katia Bertoldi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Rycroft%2C+C+H">Chris H. Rycroft</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12375" title="Abstract">arXiv:2307.12375</a> (replaced) [<a href="/pdf/2307.12375" title="Download PDF">pdf</a>, <a href="/format/2307.12375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning Learns Label Relationships but Is Not Conventional  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kossen%2C+J">Jannik Kossen</a>, 
<a href="/search/cs?searchtype=author&query=Gal%2C+Y">Yarin Gal</a>, 
<a href="/search/cs?searchtype=author&query=Rainforth%2C+T">Tom Rainforth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12856" title="Abstract">arXiv:2307.12856</a> (replaced) [<a href="/pdf/2307.12856" title="Download PDF">pdf</a>, <a href="/format/2307.12856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Real-World WebAgent with Planning, Long Context Understanding, and  Program Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A">Austin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Safdari%2C+M">Mustafa Safdari</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Eck%2C+D">Douglas Eck</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13903" title="Abstract">arXiv:2307.13903</a> (replaced) [<a href="/pdf/2307.13903" title="Download PDF">pdf</a>, <a href="/ps/2307.13903" title="Download PostScript">ps</a>, <a href="/format/2307.13903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-Robust Lipschitz Contextual Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuo%2C+S">Shiliang Zuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15514" title="Abstract">arXiv:2307.15514</a> (replaced) [<a href="/pdf/2307.15514" title="Download PDF">pdf</a>, <a href="/format/2307.15514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Fully Convolutional Geometric Features for Object 6D Pose  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corsetti%2C+J">Jaime Corsetti</a>, 
<a href="/search/cs?searchtype=author&query=Boscaini%2C+D">Davide Boscaini</a>, 
<a href="/search/cs?searchtype=author&query=Poiesi%2C+F">Fabio Poiesi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready version, 18 pages and 13 figures. Published at the 8th International Workshop on Recovering 6D Object Pose
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16412" title="Abstract">arXiv:2307.16412</a> (replaced) [<a href="/pdf/2307.16412" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Ming Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+C">Chee-Ming Ting</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+F+F">Fung Fung Ting</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+R+C+-">Rapha&#xeb;l C.-W. Phan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In MICCAI 2023 LNCS vol. 14223 600-610 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP); Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16708" title="Abstract">arXiv:2307.16708</a> (replaced) [<a href="/pdf/2307.16708" title="Download PDF">pdf</a>, <a href="/format/2307.16708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Meets Adaptive Filtering: A Stein&#x27;s Unbiased Risk  Estimator Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Esmaeilbeig%2C+Z">Zahra Esmaeilbeig</a>, 
<a href="/search/eess?searchtype=author&query=Soltanalian%2C+M">Mojtaba Soltanalian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2011.07458">arXiv:2011.07458</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16789" title="Abstract">arXiv:2307.16789</a> (replaced) [<a href="/pdf/2307.16789" title="Download PDF">pdf</a>, <a href="/format/2307.16789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world  APIs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shihao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yining Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kunlun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yaxi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+B">Bill Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sihan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lauren Hong</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+R">Runchu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dahai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04603" title="Abstract">arXiv:2308.04603</a> (replaced) [<a href="/pdf/2308.04603" title="Download PDF">pdf</a>, <a href="/format/2308.04603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Brief Yet In-Depth Survey of Deep Learning-Based Image Watermarking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Arjon Das</a>, 
<a href="/search/cs?searchtype=author&query=Alrasheedi%2C+F">Fahad Alrasheedi</a>, 
<a href="/search/cs?searchtype=author&query=Tanvir%2C+A">Abdullah Tanvir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07505" title="Abstract">arXiv:2308.07505</a> (replaced) [<a href="/pdf/2308.07505" title="Download PDF">pdf</a>, <a href="/format/2308.07505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Race Detection Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Le Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xianzhong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Emani%2C+M">Murali Emani</a>, 
<a href="/search/cs?searchtype=author&query=Vanderbruggen%2C+T">Tristan Vanderbruggen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+P">Pei-hung Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Chuanhua Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11905" title="Abstract">arXiv:2308.11905</a> (replaced) [<a href="/pdf/2308.11905" title="Download PDF">pdf</a>, <a href="/format/2308.11905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Using Admissible Bounds for Learning Forward Search Heuristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=N%C3%BA%C3%B1ez-Molina%2C+C">Carlos N&#xfa;&#xf1;ez-Molina</a>, 
<a href="/search/cs?searchtype=author&query=Asai%2C+M">Masataro Asai</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Olivares%2C+J">Juan Fern&#xe1;ndez-Olivares</a>, 
<a href="/search/cs?searchtype=author&query=Mesejo%2C+P">Pablo Mesejo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11958" title="Abstract">arXiv:2308.11958</a> (replaced) [<a href="/pdf/2308.11958" title="Download PDF">pdf</a>, <a href="/format/2308.11958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maintaining Plasticity in Continual Learning via Regenerative  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Saurabh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Marklund%2C+H">Henrik Marklund</a>, 
<a href="/search/cs?searchtype=author&query=Van+Roy%2C+B">Benjamin Van Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12476" title="Abstract">arXiv:2308.12476</a> (replaced) [<a href="/pdf/2308.12476" title="Download PDF">pdf</a>, <a href="/format/2308.12476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of Change Blindness in Virtual Reality and  Augmented Reality Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">DongHoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongyun Han</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+I">Isaac Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted as a conference paper on ISMAR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12899" title="Abstract">arXiv:2308.12899</a> (replaced) [<a href="/pdf/2308.12899" title="Download PDF">pdf</a>, <a href="/format/2308.12899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Data Management and Comprehensive Performance Evaluation for  Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiawei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengkai Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures, VLDB under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14847" title="Abstract">arXiv:2308.14847</a> (replaced) [<a href="/pdf/2308.14847" title="Download PDF">pdf</a>, <a href="/format/2308.14847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NSF: Neural Surface Fields for Human Modeling from Monocular Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yuxuan Xue</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+B+L">Bharat Lal Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Marin%2C+R">Riccardo Marin</a>, 
<a href="/search/cs?searchtype=author&query=Sarafianos%2C+N">Nikolaos Sarafianos</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanlu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pons-Moll%2C+G">Gerard Pons-Moll</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+T">Tony Tung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpted to ICCV 2023; Homepage at: <a href="https://yuxuan-xue.com/nsf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16137" title="Abstract">arXiv:2308.16137</a> (replaced) [<a href="/pdf/2308.16137" title="Download PDF">pdf</a>, <a href="/format/2308.16137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LM-Infinite: Simple On-the-Fly Length Generalization for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chi Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wenhan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sinong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16708" title="Abstract">arXiv:2308.16708</a> (replaced) [<a href="/pdf/2308.16708" title="Download PDF">pdf</a>, <a href="/format/2308.16708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concentrating on the Impact: Consequence-based Explanations in  Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lubos%2C+S">Sebastian Lubos</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T+N+T">Thi Ngoc Trang Tran</a>, 
<a href="/search/cs?searchtype=author&query=Erdeniz%2C+S+P">Seda Polat Erdeniz</a>, 
<a href="/search/cs?searchtype=author&query=Mansi%2C+M+E">Merfat El Mansi</a>, 
<a href="/search/cs?searchtype=author&query=Felfernig%2C+A">Alexander Felfernig</a>, 
<a href="/search/cs?searchtype=author&query=Wundara%2C+M">Manfred Wundara</a>, 
<a href="/search/cs?searchtype=author&query=Leitner%2C+G">Gerhard Leitner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of the paper to be presented at IntRS'23: Joint Workshop on Interfaces and Human Decision Making for Recommender Systems, September 18, 2023, Singapore. paper will be published in the workshop proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00381" title="Abstract">arXiv:2309.00381</a> (replaced) [<a href="/pdf/2309.00381" title="Download PDF">pdf</a>, <a href="/ps/2309.00381" title="Download PostScript">ps</a>, <a href="/format/2309.00381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is RISC-V ready for HPC prime-time: Evaluating the 64-core Sophon SG2042  RISC-V CPU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brown%2C+N">Nick Brown</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+M">Maurice Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joseph Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Paul Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Author accepted version of paper in ACM Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (SC-W 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00410" title="Abstract">arXiv:2309.00410</a> (replaced) [<a href="/pdf/2309.00410" title="Download PDF">pdf</a>, <a href="/format/2309.00410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Scene Text Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitani%2C+H">Hayato Mitani</a>, 
<a href="/search/cs?searchtype=author&query=Kimura%2C+A">Akisato Kimura</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, Accepted at the 34th British Machine Vision Conference, code:<a href="https://github.com/mitanihayato/Selective-Scene-Text-Removal">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01750" title="Abstract">arXiv:2309.01750</a> (replaced) [<a href="/pdf/2309.01750" title="Download PDF">pdf</a>, <a href="/ps/2309.01750" title="Download PostScript">ps</a>, <a href="/format/2309.01750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On CNF formulas irredundant with respect to unit clause propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Savick%C3%BD%2C+P">Petr Savick&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, this version reformulates the result of the previous version in a more general context using ucp-equivalence and ucp-irredundancy instead of irredundant PC formulas
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02130" title="Abstract">arXiv:2309.02130</a> (replaced) [<a href="/pdf/2309.02130" title="Download PDF">pdf</a>, <a href="/ps/2309.02130" title="Download PostScript">ps</a>, <a href="/format/2309.02130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetric Momentum: A Rethinking of Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gongyue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuwen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Donghan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Toptan%2C+C+M">Carrie M. Toptan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Honghai Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03335" title="Abstract">arXiv:2309.03335</a> (replaced) [<a href="/pdf/2309.03335" title="Download PDF">pdf</a>, <a href="/format/2309.03335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayakumar%2C+N">Nivetha Jayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+T">Tonmoy Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miaomiao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ShapeMI MICCAI 2023: Workshop on Shape in Medical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04644" title="Abstract">arXiv:2309.04644</a> (replaced) [<a href="/pdf/2309.04644" title="Download PDF">pdf</a>, <a href="/format/2309.04644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Neural Collapse: The Effects of Batch  Normalization and Weight Decay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Leyan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xinyuan Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04747" title="Abstract">arXiv:2309.04747</a> (replaced) [<a href="/pdf/2309.04747" title="Download PDF">pdf</a>, <a href="/format/2309.04747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When to Learn What: Model-Adaptive Data Augmentation Curriculum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+C">Chengkai Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our paper is accpeted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05379" title="Abstract">arXiv:2309.05379</a> (replaced) [<a href="/pdf/2309.05379" title="Download PDF">pdf</a>, <a href="/ps/2309.05379" title="Download PostScript">ps</a>, <a href="/format/2309.05379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Truthful Constrained Heterogeneous Facility Location with Max-Variant  Cost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lotfi%2C+M">Mohammad Lotfi</a>, 
<a href="/search/cs?searchtype=author&query=Voudouris%2C+A+A">Alexandros A. Voudouris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05653" title="Abstract">arXiv:2309.05653</a> (replaced) [<a href="/pdf/2309.05653" title="Download PDF">pdf</a>, <a href="/format/2309.05653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAmmoTH: Building Math Generalist Models through Hybrid Instruction  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress; Xiang Yue and Wenhu Chen contributed equally to this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06545" title="Abstract">arXiv:2309.06545</a> (replaced) [<a href="/pdf/2309.06545" title="Download PDF">pdf</a>, <a href="/format/2309.06545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Homomorphic Operations on a Real-World Processing-In-Memory  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H">Harshita Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Kabra%2C+M">Mayank Kabra</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Luna%2C+J">Juan G&#xf3;mez-Luna</a>, 
<a href="/search/cs?searchtype=author&query=Kanellopoulos%2C+K">Konstantinos Kanellopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work will be presented at IISWC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06659" title="Abstract">arXiv:2309.06659</a> (replaced) [<a href="/pdf/2309.06659" title="Download PDF">pdf</a>, <a href="/format/2309.06659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond English: Centering Multilingualism in Data Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakotondravony%2C+N">No&#xeb;lle Rakotondravony</a>, 
<a href="/search/cs?searchtype=author&query=Dhawka%2C+P">Priya Dhawka</a>, 
<a href="/search/cs?searchtype=author&query=Bancilhon%2C+M">Melanie Bancilhon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, Visualization for Social Good @VIS23
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Workshop on Visualization for Social Good 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07806" title="Abstract">arXiv:2309.07806</a> (replaced) [<a href="/pdf/2309.07806" title="Download PDF">pdf</a>, <a href="/format/2309.07806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feasability of Learning Weighted Automata on a Semiring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daviaud%2C+L">Laure Daviaud</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+M">Marianne Johnson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08835" title="Abstract">arXiv:2309.08835</a> (replaced) [<a href="/pdf/2309.08835" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent machines work in unstructured environments by differential  neural computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shengbo Wang</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+S">Shuo Gao</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+C">Chenyu Tang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Cong Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shurui Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+H">Hubin Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+G">Guohua Hu</a>, 
<a href="/search/eess?searchtype=author&query=Nathan%2C+A">Arokia Nathan</a>, 
<a href="/search/eess?searchtype=author&query=Dahiya%2C+R">Ravinder Dahiya</a>, 
<a href="/search/eess?searchtype=author&query=Occhipinti%2C+L">Luigi Occhipinti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09075" title="Abstract">arXiv:2309.09075</a> (replaced) [<a href="/e-print/2309.09075" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Music Generation based on Generative Adversarial Networks with  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Ziyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruoxue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaoxuan Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The results exist serious factual error
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11500" title="Abstract">arXiv:2309.11500</a> (replaced) [<a href="/pdf/2309.11500" title="Download PDF">pdf</a>, <a href="/format/2309.11500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Large-scale Dataset for Audio-Language Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Luoyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuenan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mengyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12252" title="Abstract">arXiv:2309.12252</a> (replaced) [<a href="/pdf/2309.12252" title="Download PDF">pdf</a>, <a href="/format/2309.12252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallelizing non-linear sequential models over the sequence length
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+Y+H">Yi Heng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Selfridge%2C+J">Joshua Selfridge</a>, 
<a href="/search/cs?searchtype=author&query=Kasim%2C+M+F">Muhammad Firmansyah Kasim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12321" title="Abstract">arXiv:2309.12321</a> (replaced) [<a href="/pdf/2309.12321" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Case for AI Safety via Law
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnston%2C+J+W">Jeffrey W. Johnston</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12437" title="Abstract">arXiv:2309.12437</a> (replaced) [<a href="/pdf/2309.12437" title="Download PDF">pdf</a>, <a href="/format/2309.12437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementation of digital MemComputing using standard electronic  components
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan-Hang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Di+Ventra%2C+M">Massimiliano Di Ventra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Neural and Evolutionary Computing (cs.NE); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12449" title="Abstract">arXiv:2309.12449</a> (replaced) [<a href="/pdf/2309.12449" title="Download PDF">pdf</a>, <a href="/format/2309.12449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Prediction of Delays in Software Projects using Delay Patterns  and Bayesian Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kula%2C+E">Elvan Kula</a>, 
<a href="/search/cs?searchtype=author&query=Greuter%2C+E">Eric Greuter</a>, 
<a href="/search/cs?searchtype=author&query=van+Deursen%2C+A">Arie van Deursen</a>, 
<a href="/search/cs?searchtype=author&query=Gousios%2C+G">Georgios Gousios</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12955" title="Abstract">arXiv:2309.12955</a> (replaced) [<a href="/pdf/2309.12955" title="Download PDF">pdf</a>, <a href="/format/2309.12955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Data Fabrication in Collaborative Vehicular Perception: Attacks and  Countermeasures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shuowei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Ruiyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiachen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xumiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q+A">Qi Alfred Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z+M">Z. Morley Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 24 figures, accepted by Usenix Security 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13133" title="Abstract">arXiv:2309.13133</a> (replaced) [<a href="/pdf/2309.13133" title="Download PDF">pdf</a>, <a href="/ps/2309.13133" title="Download PostScript">ps</a>, <a href="/format/2309.13133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-One Laws for Random Feasibility Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Altschuler%2C+D+J">Dylan J. Altschuler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor edits
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13494" title="Abstract">arXiv:2309.13494</a> (replaced) [<a href="/pdf/2309.13494" title="Download PDF">pdf</a>, <a href="/format/2309.13494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Constrained Multi-Robot Exploration with Intermittent  Rendezvous
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+A+R">Alysson Ribeiro da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Chaimowicz%2C+L">Luiz Chaimowicz</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vijay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+T+C">Thales Costa Silva</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+A">Ani Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 12 figures, 1 table, video: <a href="https://youtu.be/EuVbCoyjuIY">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13526" title="Abstract">arXiv:2309.13526</a> (replaced) [<a href="/pdf/2309.13526" title="Download PDF">pdf</a>, <a href="/format/2309.13526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaMap: High-Scalable Real-Time Cooperative Perception at the Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yongjie Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kyungtae Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE/ACM SEC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14006" title="Abstract">arXiv:2309.14006</a> (replaced) [<a href="/pdf/2309.14006" title="Download PDF">pdf</a>, <a href="/format/2309.14006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple evolutionary pressures shape identical consonant avoidance in  the world&#x27;s languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cathcart%2C+C+A">Chundra A. Cathcart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pp
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14610" title="Abstract">arXiv:2309.14610</a> (replaced) [<a href="/pdf/2309.14610" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of  Urban Areas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+K">Kai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mostafavi%2C+A">Ali Mostafavi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14622" title="Abstract">arXiv:2309.14622</a> (replaced) [<a href="/pdf/2309.14622" title="Download PDF">pdf</a>, <a href="/format/2309.14622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide and Conquer in Video Anomaly Detection: A Comprehensive Review  and New Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jian Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+G">Genlin Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14642" title="Abstract">arXiv:2309.14642</a> (replaced) [<a href="/pdf/2309.14642" title="Download PDF">pdf</a>, <a href="/format/2309.14642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Editing Motion Graphics Video via Motion Vectorization and  Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sharon Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaju Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ritchie%2C+D">Daniel Ritchie</a>, 
<a href="/search/cs?searchtype=author&query=Agrawala%2C+M">Maneesh Agrawala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures, SIGGRAPH Asia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14662" title="Abstract">arXiv:2309.14662</a> (replaced) [<a href="/pdf/2309.14662" title="Download PDF">pdf</a>, <a href="/format/2309.14662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based classification of user queries for medical consultancy  with respect to expert specialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyutkin%2C+D">Dmitry Lyutkin</a>, 
<a href="/search/cs?searchtype=author&query=Soloviev%2C+A">Andrey Soloviev</a>, 
<a href="/search/cs?searchtype=author&query=Zhukov%2C+D">Dmitry Zhukov</a>, 
<a href="/search/cs?searchtype=author&query=Pozdnyakov%2C+D">Denis Pozdnyakov</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+M+S+I">Muhammad Shahid Iqbal Malik</a>, 
<a href="/search/cs?searchtype=author&query=Ignatov%2C+D+I">Dmitry I. Ignatov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14846" title="Abstract">arXiv:2309.14846</a> (replaced) [<a href="/pdf/2309.14846" title="Download PDF">pdf</a>, <a href="/format/2309.14846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supersonic: Learning to Generate Source Code Optimizations in C/C++
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zimin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+S">Sen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15183" title="Abstract">arXiv:2309.15183</a> (replaced) [<a href="/pdf/2309.15183" title="Download PDF">pdf</a>, <a href="/format/2309.15183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Shortest Route Is Not Always the Fastest: Probability-Modeled  Stereoscopic Eye Movement Completion Time in VR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duinkharjav%2C+B">Budmonde Duinkharjav</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Benjamin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Patney%2C+A">Anjul Patney</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+R">Rachel Brown</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qi Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15289" title="Abstract">arXiv:2309.15289</a> (replaced) [<a href="/pdf/2309.15289" title="Download PDF">pdf</a>, <a href="/format/2309.15289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEPT: Towards Efficient Scene Representation Learning for Motion  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhiqian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+E">Shengbo Eben Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Keqiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15499" title="Abstract">arXiv:2309.15499</a> (replaced) [<a href="/pdf/2309.15499" title="Download PDF">pdf</a>, <a href="/format/2309.15499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Personalized Federated Learning with Shared and Personalized  Uncertainty Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hengyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Longbing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tiancheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15517" title="Abstract">arXiv:2309.15517</a> (replaced) [<a href="/pdf/2309.15517" title="Download PDF">pdf</a>, <a href="/format/2309.15517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Residual Scheduling: A New Reinforcement Learning Approach to Solving  Job Shop Scheduling Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho%2C+K">Kuo-Hao Ho</a>, 
<a href="/search/cs?searchtype=author&query=Jheng%2C+R">Ruei-Yu Jheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Ji-Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+F">Fan Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Chi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuan-Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+I">I-Chen Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16253" title="Abstract">arXiv:2309.16253</a> (replaced) [<a href="/pdf/2309.16253" title="Download PDF">pdf</a>, <a href="/format/2309.16253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where do hard problems really exist?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Marino%2C+R">Raffaele Marino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Statistical Mechanics (cond-mat.stat-mech); Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16499" title="Abstract">arXiv:2309.16499</a> (replaced) [<a href="/pdf/2309.16499" title="Download PDF">pdf</a>, <a href="/format/2309.16499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-City Matters: A Multimodal Remote Sensing Benchmark Dataset for  Cross-City Semantic Segmentation using High-Resolution Domain Adaptation  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Danfeng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Werner%2C+M">Martin Werner</a>, 
<a href="/search/cs?searchtype=author&query=Chanussot%2C+J">Jocelyn Chanussot</a>, 
<a href="/search/cs?searchtype=author&query=Zipf%2C+A">Alexander Zipf</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16631" title="Abstract">arXiv:2309.16631</a> (replaced) [<a href="/e-print/2309.16631" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Offline Reinforcement Learning -- Certify the Confidence Interval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiarui Yao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon Shaolei Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> the theoretical and experimental were only partial and incomplete
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16737" title="Abstract">arXiv:2309.16737</a> (replaced) [<a href="/pdf/2309.16737" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Teams Propel Fresh Ideas in Science and Technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yiling Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingfei Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16742" title="Abstract">arXiv:2309.16742</a> (replaced) [<a href="/pdf/2309.16742" title="Download PDF">pdf</a>, <a href="/format/2309.16742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised Learning Models for Early Detection of Albuminuria Risk in  Type-2 Diabetes Mellitus Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muharram%2C+A+P">Arief Purnama Muharram</a>, 
<a href="/search/cs?searchtype=author&query=Tahapary%2C+D+L">Dicky Levenus Tahapary</a>, 
<a href="/search/cs?searchtype=author&query=Lestari%2C+Y+D">Yeni Dwi Lestari</a>, 
<a href="/search/cs?searchtype=author&query=Sarayar%2C+R">Randy Sarayar</a>, 
<a href="/search/cs?searchtype=author&query=Dirjayanto%2C+V+J">Valerie Josephine Dirjayanto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the 10th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16772" title="Abstract">arXiv:2309.16772</a> (replaced) [<a href="/pdf/2309.16772" title="Download PDF">pdf</a>, <a href="/format/2309.16772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XVO: Generalized Visual Odometry via Cross-Modal Self-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+L">Lei Lai</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+Z">Zhongkai Shangguan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jimuyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ohn-Bar%2C+E">Eshed Ohn-Bar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023, Paris <a href="https://genxvo.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16783" title="Abstract">arXiv:2309.16783</a> (replaced) [<a href="/pdf/2309.16783" title="Download PDF">pdf</a>, <a href="/format/2309.16783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Photonic Accelerators for Image Segmentation in Autonomous Driving and  Defect Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nair%2C+L">Lakshmi Nair</a>, 
<a href="/search/cs?searchtype=author&query=Widemann%2C+D">David Widemann</a>, 
<a href="/search/cs?searchtype=author&query=Turcott%2C+B">Brad Turcott</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+N">Nick Moore</a>, 
<a href="/search/cs?searchtype=author&query=Wleklinski%2C+A">Alexandra Wleklinski</a>, 
<a href="/search/cs?searchtype=author&query=Bunandar%2C+D">Darius Bunandar</a>, 
<a href="/search/cs?searchtype=author&query=Papavasileiou%2C+I">Ioannis Papavasileiou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shihu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Logan%2C+E">Eric Logan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16889" title="Abstract">arXiv:2309.16889</a> (replaced) [<a href="/pdf/2309.16889" title="Download PDF">pdf</a>, <a href="/format/2309.16889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Superpixel Transformers for Efficient Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+A+Z">Alex Zihao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jieru Mei</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Siyuan Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yukun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang-Chieh Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kretzschmar%2C+H">Henrik Kretzschmar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, 4 tables. Presented at IROS 2023. Equal contribution by A. Zhu and J. Mei
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16916" title="Abstract">arXiv:2309.16916</a> (replaced) [<a href="/pdf/2309.16916" title="Download PDF">pdf</a>, <a href="/format/2309.16916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ONNXExplainer: an ONNX Based Generic Framework to Explain Neural  Networks Using Shapley Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Runxin He</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+N">Nicholas Kersting</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Can Liu</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Shubham Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Chetia%2C+C">Chiranjeet Chetia</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16967" title="Abstract">arXiv:2309.16967</a> (replaced) [<a href="/pdf/2309.16967" title="Download PDF">pdf</a>, <a href="/format/2309.16967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+B">Bowen Jing</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">You Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16971" title="Abstract">arXiv:2309.16971</a> (replaced) [<a href="/pdf/2309.16971" title="Download PDF">pdf</a>, <a href="/format/2309.16971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Resolution Active Learning of Fourier Neural Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shibo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+W">Wei Xing</a>, 
<a href="/search/cs?searchtype=author&query=Kirby%2C+M">Mike Kirby</a>, 
<a href="/search/cs?searchtype=author&query=Narayan%2C+A">Akil Narayan</a>, 
<a href="/search/cs?searchtype=author&query=Zhe%2C+S">Shandian Zhe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17104" title="Abstract">arXiv:2309.17104</a> (replaced) [<a href="/e-print/2309.17104" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prototype-guided Cross-modal Completion and Alignment for Incomplete  Text-based Person Re-identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+T">Tiantian Gong</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+G">Guodong Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yongkang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liyan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Sorry, some collaborators do not agree to publish it on Arxiv, so please withdraw this paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM International Conference on Multimedia 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17415" title="Abstract">arXiv:2309.17415</a> (replaced) [<a href="/pdf/2309.17415" title="Download PDF">pdf</a>, <a href="/format/2309.17415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intuitive or Dependent? Investigating LLMs&#x27; Robustness to Conflicting  Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+J">Jiahao Ying</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+K">Kai Xiong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yidong He</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Long Cui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yongbin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00111" title="Abstract">arXiv:2310.00111</a> (replaced) [<a href="/pdf/2310.00111" title="Download PDF">pdf</a>, <a href="/format/2310.00111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient compression of $\mathcal{DH}^2$-matrices for  high-frequency problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=B%C3%B6rm%2C+S">Steffen B&#xf6;rm</a>, 
<a href="/search/math?searchtype=author&query=Henningsen%2C+J">Janne Henningsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00233" title="Abstract">arXiv:2310.00233</a> (replaced) [<a href="/pdf/2310.00233" title="Download PDF">pdf</a>, <a href="/format/2310.00233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CausalImages: An R Package for Causal Inference with Earth Observation,  Bio-medical, and Social Science Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jerzak%2C+C+T">Connor T. Jerzak</a>, 
<a href="/search/cs?searchtype=author&query=Daoud%2C+A">Adel Daoud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For accompanying software, see <a href="https://github.com/AIandGlobalDevelopmentLab/causalimages-software">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00238" title="Abstract">arXiv:2310.00238</a> (replaced) [<a href="/pdf/2310.00238" title="Download PDF">pdf</a>, <a href="/format/2310.00238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feasibility-Guaranteed Safety Critical Control with Applications to  Heterogeneous Platoons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+S">Shuo Liu</a>, 
<a href="/search/math?searchtype=author&query=Xiao%2C+W">Wei Xiao</a>, 
<a href="/search/math?searchtype=author&query=Belta%2C+C+A">Calin A. Belta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures. arXiv admin note: text overlap with <a href="/abs/2304.00372">arXiv:2304.00372</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00243" title="Abstract">arXiv:2310.00243</a> (replaced) [<a href="/pdf/2310.00243" title="Download PDF">pdf</a>, <a href="/format/2310.00243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age-Optimal Multi-Flow Status Updating with Errors: A Sample-Path  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Kompella%2C+S">Sastry Kompella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Journal of Communications and Networks (JCN) Special Issue. arXiv admin note: substantial text overlap with <a href="/abs/1801.02394">arXiv:1801.02394</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT); Performance (cs.PF); Social and Information Networks (cs.SI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00339" title="Abstract">arXiv:2310.00339</a> (replaced) [<a href="/pdf/2310.00339" title="Download PDF">pdf</a>, <a href="/format/2310.00339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedLPA: Personalized One-shot Federated Learning with Layer-Wise  Posterior Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liangxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Feiyang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yunheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xia Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Linshan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00429" title="Abstract">arXiv:2310.00429</a> (replaced) [<a href="/pdf/2310.00429" title="Download PDF">pdf</a>, <a href="/format/2310.00429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of Iterative Retraining of Generative Models on their  own Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertrand%2C+Q">Quentin Bertrand</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+A+J">Avishek Joey Bose</a>, 
<a href="/search/cs?searchtype=author&query=Duplessis%2C+A">Alexandre Duplessis</a>, 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+M">Marco Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00527" title="Abstract">arXiv:2310.00527</a> (replaced) [<a href="/pdf/2310.00527" title="Download PDF">pdf</a>, <a href="/format/2310.00527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-supervised Learning of Contextualized Local Visual Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+T+S">Thalles Santos Silva</a>, 
<a href="/search/cs?searchtype=author&query=Pedrini%2C+H">Helio Pedrini</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+A+R">Ad&#xed;n Ram&#xed;rez Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print. 4th Visual Inductive Priors for Data-Efficient Deep Learning Workshop ICCV 2023. Code at $\href{<a href="https://github.com/sthalles/CLoVE">this https URL</a>}{\text{this link}}$
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 4th Visual Inductive Priors for Data-Efficient Deep Learning
  Workshop ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00535" title="Abstract">arXiv:2310.00535</a> (replaced) [<a href="/pdf/2310.00535" title="Download PDF">pdf</a>, <a href="/format/2310.00535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and  Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S">Simon Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00602" title="Abstract">arXiv:2310.00602</a> (replaced) [<a href="/pdf/2310.00602" title="Download PDF">pdf</a>, <a href="/ps/2310.00602" title="Download PostScript">ps</a>, <a href="/format/2310.00602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wavelet Scattering Transform for Improving Generalization in  Low-Resourced Spoken Language Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dey%2C+S">Spandan Dey</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+P">Premjeet Singh</a>, 
<a href="/search/eess?searchtype=author&query=Saha%2C+G">Goutam Saha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and presented in INTERSPEECH 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00604" title="Abstract">arXiv:2310.00604</a> (replaced) [<a href="/pdf/2310.00604" title="Download PDF">pdf</a>, <a href="/format/2310.00604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Structured Multimarginal Schr&#xf6;dinger Bridge for Probabilistic  Learning of Hardware Resource Usage by Control Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bondar%2C+G+A">Georgiy A. Bondar</a>, 
<a href="/search/eess?searchtype=author&query=Gifford%2C+R">Robert Gifford</a>, 
<a href="/search/eess?searchtype=author&query=Phan%2C+L+T+X">Linh Thi Xuan Phan</a>, 
<a href="/search/eess?searchtype=author&query=Halder%2C+A">Abhishek Halder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures. Submitted to American Control Conference (ACC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00710" title="Abstract">arXiv:2310.00710</a> (replaced) [<a href="/pdf/2310.00710" title="Download PDF">pdf</a>, <a href="/format/2310.00710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How well does LLM generate security tests?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wenjia Song</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zhengjie Ji</a>, 
<a href="/search/cs?searchtype=author&query=Danfeng">Danfeng</a> (Daphne)Yao, 
<a href="/search/cs?searchtype=author&query=Meng%2C+N">Na Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00835" title="Abstract">arXiv:2310.00835</a> (replaced) [<a href="/pdf/2310.00835" title="Download PDF">pdf</a>, <a href="/format/2310.00835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRAM: Benchmarking Temporal Reasoning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00987" title="Abstract">arXiv:2310.00987</a> (replaced) [<a href="/pdf/2310.00987" title="Download PDF">pdf</a>, <a href="/format/2310.00987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T+S">Tin Sum Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>, 
<a href="/search/cs?searchtype=author&query=Dokmani%C4%87%2C+I">Ivan Dokmani&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/cs?searchtype=author&query=Belius%2C+D">David Belius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01057" title="Abstract">arXiv:2310.01057</a> (replaced) [<a href="/pdf/2310.01057" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancements in Optimization: Adaptive Differential Evolution with  Diversification Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01181" title="Abstract">arXiv:2310.01181</a> (replaced) [<a href="/pdf/2310.01181" title="Download PDF">pdf</a>, <a href="/format/2310.01181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Isomorphic Networks for Assessing Reliability of the  Medium-Voltage Grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Nooten%2C+C+C">Charlotte Cambier van Nooten</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Poll%2C+T">Tom van de Poll</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%BCllhase%2C+S">Sonja F&#xfc;llhase</a>, 
<a href="/search/cs?searchtype=author&query=Heres%2C+J">Jacco Heres</a>, 
<a href="/search/cs?searchtype=author&query=Heskes%2C+T">Tom Heskes</a>, 
<a href="/search/cs?searchtype=author&query=Shapovalova%2C+Y">Yuliya Shapovalova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01206" title="Abstract">arXiv:2310.01206</a> (replaced) [<a href="/pdf/2310.01206" title="Download PDF">pdf</a>, <a href="/format/2310.01206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> appjsonify: An Academic Paper PDF-to-JSON Conversion Toolkit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+A">Atsuki Yamaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Morishita%2C+T">Terufumi Morishita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. PyPI: <a href="https://pypi.org/project/appjsonify/">this https URL</a> GitHub: <a href="https://pypi.org/project/appjsonify/.">this https URL</a> Fixed Figure 1 containing paper PDF examples
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01215" title="Abstract">arXiv:2310.01215</a> (replaced) [<a href="/pdf/2310.01215" title="Download PDF">pdf</a>, <a href="/format/2310.01215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence proof for first-order position-based dynamics: An efficient  scheme for inequality constrained ODEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Plunder%2C+S">Steffen Plunder</a>, 
<a href="/search/math?searchtype=author&query=Merino-Aceituno%2C+S">Sara Merino-Aceituno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01259" title="Abstract">arXiv:2310.01259</a> (replaced) [<a href="/pdf/2310.01259" title="Download PDF">pdf</a>, <a href="/format/2310.01259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster and Accurate Neural Networks with Semantic Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayyed%2C+S">Sazzad Sayyed</a>, 
<a href="/search/cs?searchtype=author&query=Ashdown%2C+J">Jonathan Ashdown</a>, 
<a href="/search/cs?searchtype=author&query=Restuccia%2C+F">Francesco Restuccia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, conference format
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01272" title="Abstract">arXiv:2310.01272</a> (replaced) [<a href="/pdf/2310.01272" title="Download PDF">pdf</a>, <a href="/format/2310.01272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified View on Neural Message Passing with Opinion Dynamics for  Social Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lv%2C+O">Outongyi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bingxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weishu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lirong Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01307" title="Abstract">arXiv:2310.01307</a> (replaced) [<a href="/pdf/2310.01307" title="Download PDF">pdf</a>, <a href="/format/2310.01307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Generalization of Training-based ChatGPT Detection Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shenglai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yingqian Cui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Amy Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01405" title="Abstract">arXiv:2310.01405</a> (replaced) [<a href="/pdf/2310.01405" title="Download PDF">pdf</a>, <a href="/format/2310.01405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Engineering: A Top-Down Approach to AI Transparency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Andy Zou</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Long Phan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sarah Chen</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+J">James Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Phillip Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Richard Ren</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+A">Alexander Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xuwang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mazeika%2C+M">Mantas Mazeika</a>, 
<a href="/search/cs?searchtype=author&query=Dombrowski%2C+A">Ann-Kathrin Dombrowski</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Shashwat Goel</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nathaniel Li</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+M+J">Michael J. Byun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mallen%2C+A">Alex Mallen</a>, 
<a href="/search/cs?searchtype=author&query=Basart%2C+S">Steven Basart</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/andyzoujm/representation-engineering">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item404">Cross-lists</a></li>
<li><a href="#item456">Replacements</a></li>
</ul>
<small>[ total of 658 entries:  <b>1-658</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
