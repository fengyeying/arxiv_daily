<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed 18 Oct 23  to  Thu 19 Oct 23, announced Fri, 20 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item335">Cross-lists</a></li>
<li><a href="#item383">Replacements</a></li>
</ul>
<small>[ total of 638 entries:  <b>1-638</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri, 20 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12155" title="Abstract">arXiv:2310.12155</a> [<a href="/pdf/2310.12155" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing exploration and exploitation phases in whale optimization  algorithm: an insightful and empirical analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A+M">Aram M. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Rashid%2C+T+A">Tarik A. Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+B+A">Bryar A. Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Majidpour%2C+J">Jaffer Majidpour</a>, 
<a href="/search/cs?searchtype=author&query=Noori%2C+K+A">Kaniaw A. Noori</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+C+M">Chnoor Maheadeen Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M+H">Mohmad Hussein Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Qader%2C+S+M">Shko M. Qader</a>, 
<a href="/search/cs?searchtype=author&query=Tayfor%2C+N">Noor Tayfor</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+N+B">Naufel B Mohammed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Agents of any metaheuristic algorithms are moving in two modes, namely
exploration and exploitation. Obtaining robust results in any algorithm is
strongly dependent on how to balance between these two modes. Whale
optimization algorithm as a robust and well recognized metaheuristic algorithm
in the literature, has proposed a novel scheme to achieve this balance. It has
also shown superior results on a wide range of applications. Moreover, in the
previous chapter, an equitable and fair performance evaluation of the algorithm
was provided. However, to this point, only comparison of the final results is
considered, which does not explain how these results are obtained. Therefore,
this chapter attempts to empirically analyze the WOA algorithm in terms of the
local and global search capabilities i.e. the ratio of exploration and
exploitation phases. To achieve this objective, the dimension-wise diversity
measurement is employed, which, at various stages of the optimization process,
statistically evaluates the population's convergence and diversity.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12162" title="Abstract">arXiv:2310.12162</a> [<a href="/pdf/2310.12162" title="Download PDF">pdf</a>, <a href="/format/2310.12162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Potentiality and Awareness: A Position Paper from the Perspective of  Human-AI Teaming in Cybersecurity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarker%2C+I+H">Iqbal H. Sarker</a>, 
<a href="/search/cs?searchtype=author&query=Janicke%2C+H">Helge Janicke</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+N">Nazeeruddin Mohammad</a>, 
<a href="/search/cs?searchtype=author&query=Watters%2C+P">Paul Watters</a>, 
<a href="/search/cs?searchtype=author&query=Nepal%2C+S">Surya Nepal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, Springer
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This position paper explores the broad landscape of AI potentiality in the
context of cybersecurity, with a particular emphasis on its possible risk
factors with awareness, which can be managed by incorporating human experts in
the loop, i.e., "Human-AI" teaming. As artificial intelligence (AI)
technologies advance, they will provide unparalleled opportunities for attack
identification, incident response, and recovery. However, the successful
deployment of AI into cybersecurity measures necessitates an in-depth
understanding of its capabilities, challenges, and ethical and legal
implications to handle associated risk factors in real-world application areas.
Towards this, we emphasize the importance of a balanced approach that
incorporates AI's computational power with human expertise. AI systems may
proactively discover vulnerabilities and detect anomalies through pattern
recognition, and predictive modeling, significantly enhancing speed and
accuracy. Human experts can explain AI-generated decisions to stakeholders,
regulators, and end-users in critical situations, ensuring responsibility and
accountability, which helps establish trust in AI-driven security solutions.
Therefore, in this position paper, we argue that human-AI teaming is worthwhile
in cybersecurity, in which human expertise such as intuition, critical
thinking, or contextual understanding is combined with AI's computational power
to improve overall cyber defenses.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12168" title="Abstract">arXiv:2310.12168</a> [<a href="/pdf/2310.12168" title="Download PDF">pdf</a>, <a href="/format/2310.12168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RK-core: An Established Methodology for Exploring the Hierarchical  Structure within Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yutian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jiaqi Nie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zuohui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Q">Qi Xuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, the field of machine learning has undergone a transition from
model-centric to data-centric. The advancements in diverse learning tasks have
been propelled by the accumulation of more extensive datasets, subsequently
facilitating the training of larger models on these datasets. However, these
datasets remain relatively under-explored. To this end, we introduce a
pioneering approach known as RK-core, to empower gaining a deeper understanding
of the intricate hierarchical structure within datasets. Across several
benchmark datasets, we find that samples with low coreness values appear less
representative of their respective categories, and conversely, those with high
coreness values exhibit greater representativeness. Correspondingly, samples
with high coreness values make a more substantial contribution to the
performance in comparison to those with low coreness values. Building upon
this, we further employ RK-core to analyze the hierarchical structure of
samples with different coreset selection methods. Remarkably, we find that a
high-quality coreset should exhibit hierarchical diversity instead of solely
opting for representative samples. The code is available at
https://github.com/yaolu-zjut/Kcore.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12169" title="Abstract">arXiv:2310.12169</a> [<a href="/pdf/2310.12169" title="Download PDF">pdf</a>, <a href="/format/2310.12169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph  Embeddings Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Said%2C+A">Anwar Said</a>, 
<a href="/search/cs?searchtype=author&query=Shabbir%2C+M">Mudassir Shabbir</a>, 
<a href="/search/cs?searchtype=author&query=Derr%2C+T">Tyler Derr</a>, 
<a href="/search/cs?searchtype=author&query=Abbas%2C+W">Waseem Abbas</a>, 
<a href="/search/cs?searchtype=author&query=Koutsoukos%2C+X">Xenofon Koutsoukos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22nd IEEE International Conference on Machine Learning and Applications 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have shown remarkable merit in performing
various learning-based tasks in complex networks. The superior performance of
GNNs often correlates with the availability and quality of node-level features
in the input networks. However, for many network applications, such node-level
information may be missing or unreliable, thereby limiting the applicability
and efficacy of GNNs. To address this limitation, we present a novel approach
denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which
aims to enhance and design node features, particularly in scenarios where
information is lacking. Our method leverages the topological structure of the
local subgraph to create topology-aware node features. The subgraph features
are generated using an efficient spectral graph embedding technique, and they
serve as node features that capture the local topological organization of the
network. The explicit node features, if present, are then enhanced with the
subgraph embeddings in order to improve the overall performance. ESGEA is
compatible with any GNN-based architecture and is effective even in the absence
of node features. We evaluate the proposed method in a social network graph
classification task where node attributes are unavailable, as well as in a node
classification task where node features are corrupted or even absent. The
evaluation results on seven datasets and eight baseline models indicate up to a
10% improvement in AUC and a 7% improvement in accuracy for graph and node
classification tasks, respectively.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12172" title="Abstract">arXiv:2310.12172</a> [<a href="/pdf/2310.12172" title="Download PDF">pdf</a>, <a href="/format/2310.12172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of ImageArg-2023: The First Shared Task in Multimodal Argument  Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhexiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Elarby%2C+M">Mohamed Elarby</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Litman%2C+D">Diane Litman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In The 10th Argument Mining Workshop, held in conjunction with The Conference on Empirical Methods in Natural Language Processing (EMNLP), December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents an overview of the ImageArg shared task, the first
multimodal Argument Mining shared task co-located with the 10th Workshop on
Argument Mining at EMNLP 2023. The shared task comprises two classification
subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image
Persuasiveness Classification. The former determines the stance of a tweet
containing an image and a piece of text toward a controversial topic (e.g., gun
control and abortion). The latter determines whether the image makes the tweet
text more persuasive. The shared task received 31 submissions for Subtask-A and
21 submissions for Subtask-B from 9 different teams across 6 countries. The top
submission in Subtask-A achieved an F1-score of 0.8647 while the best
submission in Subtask-B achieved an F1-score of 0.5561.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12180" title="Abstract">arXiv:2310.12180</a> [<a href="/pdf/2310.12180" title="Download PDF">pdf</a>, <a href="/format/2310.12180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear-Time Verification of Data-Aware Processes Modulo Theories via  Covers and Automata (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gianola%2C+A">Alessandro Gianola</a>, 
<a href="/search/cs?searchtype=author&query=Montali%2C+M">Marco Montali</a>, 
<a href="/search/cs?searchtype=author&query=Winkler%2C+S">Sarah Winkler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The need to model and analyse dynamic systems operating over complex data is
ubiquitous in AI and neighboring areas, in particular business process
management. Analysing such data-aware systems is a notoriously difficult
problem, as they are intrinsically infinite-state. Existing approaches work for
specific datatypes, and/or limit themselves to the verification of safety
properties. In this paper, we lift both such limitations, studying for the
first time linear-time verification for so-called data-aware processes modulo
theories (DMTs), from the foundational and practical point of view. The DMT
model is very general, as it supports processes operating over variables that
can store arbitrary types of data, ranging over infinite domains and equipped
with domain-specific predicates. Specifically, we provide four contributions.
First, we devise a semi-decision procedure for linear-time verification of
DMTs, which works for a very large class of datatypes obeying to mild
model-theoretic assumptions. The procedure relies on a unique combination of
automata-theoretic and cover computation techniques to respectively deal with
linear-time properties and datatypes. Second, we identify an abstract, semantic
property that guarantees the existence of a faithful finite-state abstraction
of the original system, and show that our method becomes a decision procedure
in this case. Third, we identify concrete, checkable classes of systems that
satisfy this property, generalising several results in the literature. Finally,
we present an implementation and a first experimental evaluation.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12181" title="Abstract">arXiv:2310.12181</a> [<a href="/pdf/2310.12181" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise influence evaluation in complex networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bingyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qingyun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daqing Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Evaluating node influence is fundamental for identifying key nodes in complex
networks. Existing methods typically rely on generic indicators to rank node
influence across diverse networks, thereby ignoring the individualized features
of each network itself. Actually, node influence stems not only from general
features but the multi-scale individualized information encompassing specific
network structure and task. Here we design an active learning architecture to
predict node influence quantitively and precisely, which samples representative
nodes based on graph entropy correlation matrix integrating multi-scale
individualized information. This brings two intuitive advantages: (1)
discovering potential high-influence but weak-connected nodes that are usually
ignored in existing methods, (2) improving the influence maximization strategy
by deducing influence interference. Significantly, our architecture
demonstrates exceptional transfer learning capabilities across multiple types
of networks, which can identify those key nodes with large disputation across
different existing methods. Additionally, our approach, combined with a simple
greedy algorithm, exhibits dominant performance in solving the influence
maximization problem. This architecture holds great potential for applications
in graph mining and prediction tasks.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12182" title="Abstract">arXiv:2310.12182</a> [<a href="/pdf/2310.12182" title="Download PDF">pdf</a>, <a href="/format/2310.12182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block-Wise Mixed-Precision Quantization: Enabling High Efficiency for  Practical ReRAM-based DNN Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xueying Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hanson%2C+E">Edward Hanson</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nansu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qilin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoxuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huanrui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+F">Feng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pande%2C+P+P">Partha Pratim Pande</a>, 
<a href="/search/cs?searchtype=author&query=Doppa%2C+J+R">Janardhan Rao Doppa</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+K">Krishnendu Chakrabarty</a>, Hai (Helen)Li
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Resistive random access memory (ReRAM)-based processing-in-memory (PIM)
architectures have demonstrated great potential to accelerate Deep Neural
Network (DNN) training/inference. However, the computational accuracy of analog
PIM is compromised due to the non-idealities, such as the conductance variation
of ReRAM cells. The impact of these non-idealities worsens as the number of
concurrently activated wordlines and bitlines increases. To guarantee
computational accuracy, only a limited number of wordlines and bitlines of the
crossbar array can be turned on concurrently, significantly reducing the
achievable parallelism of the architecture.
<br />While the constraints on parallelism limit the efficiency of the
accelerators, they also provide a new opportunity for fine-grained
mixed-precision quantization. To enable efficient DNN inference on practical
ReRAM-based accelerators, we propose an algorithm-architecture co-design
framework called \underline{B}lock-\underline{W}ise mixed-precision
\underline{Q}uantization (BWQ). At the algorithm level, BWQ-A introduces a
mixed-precision quantization scheme at the block level, which achieves a high
weight and activation compression ratio with negligible accuracy degradation.
We also present the hardware architecture design BWQ-H, which leverages the
low-bit-width models achieved by BWQ-A to perform high-efficiency DNN inference
on ReRAM devices. BWQ-H also adopts a novel precision-aware weight mapping
method to increase the ReRAM crossbar's throughput. Our evaluation demonstrates
the effectiveness of BWQ, which achieves a 6.08x speedup and a 17.47x energy
saving on average compared to existing ReRAM-based architectures.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12184" title="Abstract">arXiv:2310.12184</a> [<a href="/pdf/2310.12184" title="Download PDF">pdf</a>, <a href="/format/2310.12184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architectural Implications of GNN Aggregation Programming Abstractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yingjie Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianlei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Ao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+T">Tong Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chunming Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, to be published in IEEE Computer Architecture Letters (CAL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Performance (cs.PF)

</div>
<p class="mathjax">Graph neural networks (GNNs) have gained significant popularity due to the
powerful capability to extract useful representations from graph data. As the
need for efficient GNN computation intensifies, a variety of programming
abstractions designed for optimizing GNN Aggregation have emerged to facilitate
acceleration. However, there is no comprehensive evaluation and analysis upon
existing abstractions, thus no clear consensus on which approach is better. In
this letter, we classify existing programming abstractions for GNN Aggregation
by the dimension of data organization and propagation method. By constructing
these abstractions on a state-of-the-art GNN library, we perform a thorough and
detailed characterization study to compare their performance and efficiency,
and provide several insights on future GNN acceleration based on our analysis.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12186" title="Abstract">arXiv:2310.12186</a> [<a href="/pdf/2310.12186" title="Download PDF">pdf</a>, <a href="/format/2310.12186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stranger Danger! Cross-Community Interactions with Fringe Users Increase  the Growth of Fringe Communities on Reddit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Russo%2C+G">Giuseppe Russo</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+M+H">Manoel Horta Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 Pages, 7 Figures, 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fringe communities promoting conspiracy theories and extremist ideologies
have thrived on mainstream platforms, raising questions about the mechanisms
driving their growth. Here, we hypothesize and study a possible mechanism: new
members may be recruited through fringe-interactions: the exchange of comments
between members and non-members of fringe communities. We apply text-based
causal inference techniques to study the impact of fringe-interactions on the
growth of three prominent fringe communities on Reddit: r/Incel,
r/GenderCritical, and r/The_Donald. Our results indicate that
fringe-interactions attract new members to fringe communities. Users who
receive these interactions are up to 4.2 percentage points (pp) more likely to
join fringe communities than similar, matched users who do not.
<br />This effect is influenced by 1) the characteristics of communities where the
interaction happens (e.g., left vs. right-leaning communities) and 2) the
language used in the interactions. Interactions using toxic language have a 5pp
higher chance of attracting newcomers to fringe communities than non-toxic
interactions. We find no effect when repeating this analysis by replacing
fringe (r/Incel, r/GenderCritical, and r/The_Donald) with non-fringe
communities (r/climatechange, r/NBA, r/leagueoflegends), suggesting this growth
mechanism is specific to fringe communities. Overall, our findings suggest that
curtailing fringe-interactions may reduce the growth of fringe communities on
mainstream platforms.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12187" title="Abstract">arXiv:2310.12187</a> [<a href="/pdf/2310.12187" title="Download PDF">pdf</a>, <a href="/format/2310.12187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Session Types With Multiple Senders Single Receiver (report version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zekun Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiong Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Message passing is a fundamental element in software development, ranging
from concurrent and mobile computing to distributed services, but it suffers
from communication errors such as deadlocks. Session types are a typing
discipline for enforcing safe structured interactions between multiple
participants. However, each typed interaction is restricted to having one fixed
sender and one fixed receiver. In this paper, we extend session types with
existential branching types, to handle a common interaction pattern with
multiple senders and a single receiver in a synchronized setting, i.e. a
receiver is available to receive messages from multiple senders, and which
sender actually participates in the interaction cannot be determined till
execution. We build the type system with existential branching types, which
retain the important properties induced by standard session types: type safety,
progress (i.e. deadlock-freedom), and fidelity. We further provide a novel
communication type system to guarantee progress of dynamically interleaved
multiparty sessions, by abandoning the strong restrictions of existing type
systems. Finally, we encode Rust multi-thread primitives in the extended
session types to show the expressivity, which can be considered an attempt to
check the deadlock-freedom of Rust multi-thread programs.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12189" title="Abstract">arXiv:2310.12189</a> [<a href="/pdf/2310.12189" title="Download PDF">pdf</a>, <a href="/format/2310.12189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mesh Represented Recycle Learning for 3D Hand Pose and Mesh Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Bosang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jonghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hyotae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lanying Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+J">Jeongwon Ha</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+D">Dowoo Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jungpyo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Im%2C+W">Wonhyeok Im</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">KyungMin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jungho Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In general, hand pose estimation aims to improve the robustness of model
performance in the real-world scenes. However, it is difficult to enhance the
robustness since existing datasets are obtained in restricted environments to
annotate 3D information. Although neural networks quantitatively achieve a high
estimation accuracy, unsatisfied results can be observed in visual quality.
This discrepancy between quantitative results and their visual qualities
remains an open issue in the hand pose representation. To this end, we propose
a mesh represented recycle learning strategy for 3D hand pose and mesh
estimation which reinforces synthesized hand mesh representation in a training
phase. To be specific, a hand pose and mesh estimation model first predicts
parametric 3D hand annotations (i.e., 3D keypoint positions and vertices for
hand mesh) with real-world hand images in the training phase. Second, synthetic
hand images are generated with self-estimated hand mesh representations. After
that, the synthetic hand images are fed into the same model again. Thus, the
proposed learning strategy simultaneously improves quantitative results and
visual qualities by reinforcing synthetic mesh representation. To encourage
consistency between original model output and its recycled one, we propose
self-correlation loss which maximizes the accuracy and reliability of our
learning strategy. Consequently, the model effectively conducts self-refinement
on hand pose estimation by learning mesh representation from its own output. To
demonstrate the effectiveness of our learning strategy, we provide extensive
experiments on FreiHAND dataset. Notably, our learning strategy improves the
performance on hand pose and mesh estimation without any extra computational
burden during the inference.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12190" title="Abstract">arXiv:2310.12190</a> [<a href="/pdf/2310.12190" title="Download PDF">pdf</a>, <a href="/format/2310.12190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+J">Jinbo Xing</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menghan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+T">Tien-Tsin Wong</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary demo code: <a href="https://github.com/AILab-CVC/VideoCrafter">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Enhancing a still image with motion offers more engaged visual experience.
Traditional image animation techniques mainly focus on animating natural scenes
with random dynamics, such as clouds and fluid, and thus limits their
applicability to generic visual contents. To overcome this limitation, we
explore the synthesis of dynamic content for open-domain images, converting
them into animated videos. The key idea is to utilize the motion prior of
text-to-video diffusion models by incorporating the image into the generative
process as guidance. Given an image, we first project it into a text-aligned
rich image embedding space using a learnable image encoding network, which
facilitates the video model to digest the image content compatibly. However,
some visual details still struggle to be preserved in the resulting videos. To
supplement more precise image information, we further feed the full image to
the diffusion model by concatenating it with the initial noises. Experimental
results reveal that our proposed method produces visually convincing animated
videos, exhibiting both natural motions and high fidelity to the input image.
Comparative evaluation demonstrates the notable superiority of our approach
over existing competitors. The source code will be released upon publication.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12214" title="Abstract">arXiv:2310.12214</a> [<a href="/pdf/2310.12214" title="Download PDF">pdf</a>, <a href="/format/2310.12214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PrivInfer: Privacy-Preserving Inference for Black-box Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+M">Meng Tong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kejiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Large language models (LLMs), such as ChatGPT, have simplified text
generation tasks, yet their inherent privacy risks are increasingly garnering
attention. While differential privacy techniques have been successfully applied
to text classification tasks, the resultant semantic bias makes them unsuitable
for text generation. Homomorphic encryption inference methods have also been
introduced, however, the significant computational and communication costs
limit their viability. Furthermore, closed-source, black-box models such as
GPT-4 withhold their architecture, thwarting certain privacy-enhancing
strategies such as splitting inference into local and remote and then adding
noise when communicating. To overcome these challenges, we introduce PrivInfer,
the first privacy-preserving inference framework for black-box LLMs in text
generation. Inspired by human writing, PrivInfer employs differential privacy
methods to generate perturbed prompts for remote LLMs inference and extracts
the meaningful response from the remote perturbed results. We also introduce
RANTEXT, a differential privacy scheme specifically for LLMs that leverages
random adjacency in text perturbations. Experimental results indicate that
PrivInfer is comparable to GPT-4 in terms of text generation quality while
protecting privacy, and RANTEXT provides enhanced privacy protection against
three types of differential privacy attacks, including our newly introduced GPT
inference attack, compared to baseline methods.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12234" title="Abstract">arXiv:2310.12234</a> [<a href="/pdf/2310.12234" title="Download PDF">pdf</a>, <a href="/format/2310.12234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Eager Satisfiability Modulo Theories Solver for Algebraic Datatypes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Amar Shah</a>, 
<a href="/search/cs?searchtype=author&query=Mora%2C+F">Federico Mora</a>, 
<a href="/search/cs?searchtype=author&query=Seshia%2C+S+A">Sanjit A. Seshia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">Algebraic data types (ADTs) are a construct classically found in functional
programming languages that capture data structures like enumerated types,
lists, and trees. In recent years, interest in ADTs has increased. For example,
popular programming languages, like Python, have added support for ADTs.
Automated reasoning about ADTs can be done using satisfiability modulo theories
(SMT) solving, an extension of the Boolean satisfiability problem with
constraints over first-order structures. Unfortunately, SMT solvers that
support ADTs do not scale as state-of-the-art approaches all use variations of
the same \emph{lazy} approach. In this paper, we present an SMT solver that
takes a fundamentally different approach, an \emph{eager} approach.
Specifically, our solver reduces ADT queries to a simpler logical theory,
uninterpreted functions (UF), and then uses an existing solver on the reduced
query. We prove the soundness and completeness of our approach and demonstrate
that it outperforms the state-of-theart on existing benchmarks, as well as a
new, more challenging benchmark set from the planning domain.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12236" title="Abstract">arXiv:2310.12236</a> [<a href="/pdf/2310.12236" title="Download PDF">pdf</a>, <a href="/format/2310.12236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Neural Machine Translation with Task-level Mixture of Experts  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tourni%2C+I+C">Isidora Chara Tourni</a>, 
<a href="/search/cs?searchtype=author&query=Naskar%2C+S">Subhajit Naskar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Direct neural machine translation (direct NMT) is a type of NMT system that
translates text between two non-English languages. Direct NMT systems often
face limitations due to the scarcity of parallel data between non-English
language pairs. Several approaches have been proposed to address this
limitation, such as multilingual NMT and pivot NMT (translation between two
languages via English). Task-level Mixture of expert models (Task-level MoE),
an inference-efficient variation of Transformer-based models, has shown
promising NMT performance for a large number of language pairs. In Task-level
MoE, different language groups can use different routing strategies to optimize
cross-lingual learning and inference speed. In this work, we examine Task-level
MoE's applicability in direct NMT and propose a series of high-performing
training and evaluation configurations, through which Task-level MoE-based
direct NMT systems outperform bilingual and pivot-based models for a large
number of low and high-resource direct pairs, and translation directions. Our
Task-level MoE with 16 experts outperforms bilingual NMT, Pivot NMT models for
7 language pairs, while pivot-based models still performed better in 9 pairs
and directions.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12238" title="Abstract">arXiv:2310.12238</a> [<a href="/pdf/2310.12238" title="Download PDF">pdf</a>, <a href="/format/2310.12238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot In-Context Imitation Learning via Implicit Graph Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vosylius%2C+V">Vitalis Vosylius</a>, 
<a href="/search/cs?searchtype=author&query=Johns%2C+E">Edward Johns</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at CoRL 2023. Videos are available on our project webpage at <a href="https://www.robot-learning.uk/implicit-graph-alignment">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Consider the following problem: given a few demonstrations of a task across a
few different objects, how can a robot learn to perform that same task on new,
previously unseen objects? This is challenging because the large variety of
objects within a class makes it difficult to infer the task-relevant
relationship between the new objects and the objects in the demonstrations. We
address this by formulating imitation learning as a conditional alignment
problem between graph representations of objects. Consequently, we show that
this conditioning allows for in-context learning, where a robot can perform a
task on a set of new objects immediately after the demonstrations, without any
prior knowledge about the object class or any further training. In our
experiments, we explore and validate our design choices, and we show that our
method is highly effective for few-shot learning of several real-world,
everyday tasks, whilst outperforming baselines. Videos are available on our
project webpage at https://www.robot-learning.uk/implicit-graph-alignment.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12239" title="Abstract">arXiv:2310.12239</a> [<a href="/pdf/2310.12239" title="Download PDF">pdf</a>, <a href="/ps/2310.12239" title="Download PostScript">ps</a>, <a href="/format/2310.12239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Space Usage of Approximate Distance Oracles with Sub-2 Stretch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kopelowitz%2C+T">Tsvi Kopelowitz</a>, 
<a href="/search/cs?searchtype=author&query=Korin%2C+A">Ariel Korin</a>, 
<a href="/search/cs?searchtype=author&query=Roditty%2C+L">Liam Roditty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">For an undirected unweighted graph $G=(V,E)$ with $n$ vertices and $m$ edges,
let $d(u,v)$ denote the distance from $u\in V$ to $v\in V$ in $G$. An
$(\alpha,\beta)$-stretch approximate distance oracle (ADO) for $G$ is a data
structure that given $u,v\in V$ returns in constant (or near constant) time a
value $\hat d (u,v)$ such that $d(u,v) \le \hat d (u,v) \le \alpha\cdot d(u,v)
+ \beta$, for some reals $\alpha &gt;1, \beta$. If $\beta = 0$, we say that the
ADO has stretch $\alpha$.
<br />Thorup and Zwick~\cite{thorup2005approximate} showed that one cannot beat
stretch 3 with subquadratic space (in terms of $n$) for general graphs.
P\v{a}tra\c{s}cu and Roditty~\cite{patrascu2010distance} showed that one can
obtain stretch 2 using $O(m^{1/3}n^{4/3})$ space, and so if $m$ is subquadratic
in $n$ then the space usage is also subquadratic. Moreover, P\v{a}tra\c{s}cu
and Roditty~\cite{patrascu2010distance} showed that one cannot beat stretch 2
with subquadratic space even for graphs where $m=\tilde{O}(n)$, based on the
set-intersection hypothesis.
<br />In this paper we explore the conditions for which an ADO can be stored using
subquadratic space while supporting a sub-2 stretch. In particular, we show
that if the maximum degree in $G$ is $\Delta_G \leq O(n^{1/2-\varepsilon})$ for
some $0&lt;\varepsilon \leq 1/2$, then there exists an ADO for $G$ that uses
$\tilde{O}(n^{2-\frac {2\varepsilon}{3}})$ space and has a sub-2 stretch.
<br />Moreover, we prove a conditional lower bound, based on the set intersection
hypothesis, which states that for any positive integer $k \leq \log n$,
obtaining a sub-$\frac{k+2}{k}$ stretch for graphs with maximum degree
$\Theta(n^{1/k})$ requires quadratic space. Thus, for graphs with maximum
degree $\Theta(n^{1/2})$, obtaining a sub-2 stretch requires quadratic space.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12241" title="Abstract">arXiv:2310.12241</a> [<a href="/pdf/2310.12241" title="Download PDF">pdf</a>, <a href="/format/2310.12241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Indoor Health: An In-depth Field Study on the Indoor Air  Quality Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karmakar%2C+P">Prasenjit Karmakar</a>, 
<a href="/search/cs?searchtype=author&query=Pradhan%2C+S">Swadhin Pradhan</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Sandip Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Indoor air pollution, a significant driver of respiratory and cardiovascular
diseases, claims 3.2 million lives yearly, according to the World Health
Organization, highlighting the pressing need to address this global crisis. In
contrast to unconstrained outdoor environments, room structures, floor plans,
ventilation systems, and occupant activities all impact the accumulation and
spread of pollutants. Yet, comprehensive in-the-wild empirical studies
exploring these unique indoor air pollution patterns and scope are lacking. To
address this, we conducted a three-month-long field study involving over 28
indoor spaces to delve into the complexities of indoor air pollution. Our study
was conducted using our custom-built DALTON air quality sensor and monitoring
system, an innovative IoT air quality monitoring solution that considers cost,
sensor type, accuracy, network connectivity, power, and usability. Our study
also revealed that conventional measures, such as the Indoor Air Quality Index
(IAQI), don't fully capture complex indoor air quality dynamics. Hence, we
proposed the Healthy Home Index (HHI), a new metric considering the context and
household activities, offering a more comprehensive understanding of indoor air
quality. Our findings suggest that HHI provides a more accurate air quality
assessment, underscoring the potential for wide-scale deployment of our indoor
air quality monitoring platform.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12243" title="Abstract">arXiv:2310.12243</a> [<a href="/pdf/2310.12243" title="Download PDF">pdf</a>, <a href="/format/2310.12243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary  Objects in Realistic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hull%2C+M">Matthew Hull</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+J">Zijie J. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep Learning models, such as those used in an autonomous vehicle are
vulnerable to adversarial attacks where an attacker could place an adversarial
object in the environment, leading to mis-classification. Generating these
adversarial objects in the digital space has been extensively studied, however
successfully transferring these attacks from the digital realm to the physical
realm has proven challenging when controlling for real-world environmental
factors. In response to these limitations, we introduce REVAMP, an easy-to-use
Python library that is the first-of-its-kind tool for creating attack scenarios
with arbitrary objects and simulating realistic environmental factors,
lighting, reflection, and refraction. REVAMP enables researchers and
practitioners to swiftly explore various scenarios within the digital realm by
offering a wide range of configurable options for designing experiments and
using differentiable rendering to reproduce physically plausible adversarial
objects. We will demonstrate and invite the audience to try REVAMP to produce
an adversarial texture on a chosen object while having control over various
scene parameters. The audience will choose a scene, an object to attack, the
desired attack class, and the number of camera positions to use. Then, in real
time, we show how this altered texture causes the chosen object to be
mis-classified, showcasing the potential of REVAMP in real-world scenarios.
REVAMP is open-source and available at https://github.com/poloclub/revamp.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12244" title="Abstract">arXiv:2310.12244</a> [<a href="/pdf/2310.12244" title="Download PDF">pdf</a>, <a href="/format/2310.12244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Approach to Domain Incremental Learning with Memory: Theory  and Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haizhou Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Domain incremental learning aims to adapt to a sequence of domains with
access to only a small subset of data (i.e., memory) from previous domains.
Various methods have been proposed for this problem, but it is still unclear
how they are related and when practitioners should choose one method over
another. In response, we propose a unified framework, dubbed Unified Domain
Incremental Learning (UDIL), for domain incremental learning with memory. Our
UDIL **unifies** various existing methods, and our theoretical analysis shows
that UDIL always achieves a tighter generalization error bound compared to
these methods. The key insight is that different existing methods correspond to
our bound with different **fixed** coefficients; based on insights from this
unification, our UDIL allows **adaptive** coefficients during training, thereby
always achieving the tightest bound. Empirical results show that our UDIL
outperforms the state-of-the-art domain incremental learning methods on both
synthetic and real-world datasets. Code will be available at
https://github.com/Wang-ML-Lab/unified-continual-learning.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12248" title="Abstract">arXiv:2310.12248</a> [<a href="/pdf/2310.12248" title="Download PDF">pdf</a>, <a href="/format/2310.12248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perez%2C+M">Mateo Perez</a>, 
<a href="/search/cs?searchtype=author&query=Somenzi%2C+F">Fabio Somenzi</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Ashutosh Trivedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL
-- have seen recent use as a way to express non-Markovian objectives in
reinforcement learning. We introduce a model-based probably approximately
correct (PAC) learning algorithm for omega-regular objectives in Markov
decision processes. Unlike prior approaches, our algorithm learns from sampled
trajectories of the system and does not require prior knowledge of the system's
topology.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12249" title="Abstract">arXiv:2310.12249</a> [<a href="/pdf/2310.12249" title="Download PDF">pdf</a>, <a href="/format/2310.12249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Link Transmission Model with Variable Speed Limits and Turn-Level  Queue Transmission at Signalized Intersections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wei%2C+L">Lei Wei</a>, 
<a href="/search/eess?searchtype=author&query=Waller%2C+S+T">S. Travis Waller</a>, 
<a href="/search/eess?searchtype=author&query=Mei%2C+Y">Yu Mei</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yunpeng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Meng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The link transmission model (LTM) is an efficient and widely used macro-level
approach for simulating traffic flow. However, the state-of-the-art LTMs
usually focused on segment-level modelling, in which the traffic operation
differences among multiple turning directions are neglected. Such models are
incapable of differentiating the turn-level queue transmission, resulting in
underrepresented queue spillbacks and misidentification of bottlenecks.
Moreover, a constant free-flow speed is usually assumed to formulate LTMs,
restricting their applications to model dynamic traffic management strategies
involving variable speed limits (VSL) and connected and automated vehicles.
This study proposed an extended LTM with VSL and turn-level queue transmission
to capture the traffic flow propagation at signalized intersections. First,
each road segment (RS) with multiple turning directions is divided into many
free-flow and queueing parts characterized by the triangular fundamental
diagrams. Then, the vehicle propagation within the link is described by the
turn-level link inflow, queue inflow, and outflow, which depends on the
free-flow speed changes. A node model involving an iterative procedure is
further defined to capture the potential queue spillback, which estimates the
actual flow propagation between the adjacent RSs. Simulated and field data were
used to verify the proposed model performance. Results reveal that the proposed
LTM predict traffic operations of complex intersections with multiple turning
movements, VSL and signal control schemes, and enables an accurate yet
computationally tractable representation of flow propagation.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12254" title="Abstract">arXiv:2310.12254</a> [<a href="/pdf/2310.12254" title="Download PDF">pdf</a>, <a href="/format/2310.12254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Charge Manipulation Attacks Against Smart Electric Vehicle Charging  Stations and Deep Learning-based Detection Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahangir%2C+H">Hamidreza Jahangir</a>, 
<a href="/search/cs?searchtype=author&query=Lakshminarayana%2C+S">Subhash Lakshminarayana</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The widespread deployment of "smart" electric vehicle charging stations
(EVCSs) will be a key step toward achieving green transportation. The
connectivity features of smart EVCSs can be utilized to schedule EV charging
operations while respecting user preferences, thus avoiding synchronous
charging from a large number of customers and relieving grid congestion.
However, the communication and connectivity requirements involved in smart
charging raise cybersecurity concerns. In this work, we investigate charge
manipulation attacks (CMAs) against EV charging, in which an attacker
manipulates the information exchanged during smart charging operations. The
objective of CMAs is to shift the EV aggregator's demand across different times
of the day. The proposed CMAs can bypass existing protection mechanisms in EV
communication protocols. We quantify the impact of CMAs on the EV aggregator's
economic profit by modeling their participation in the day-ahead (DA) and
real-time (RT) electricity markets. Finally, we propose an unsupervised deep
learning-based mechanism to detect CMAs by monitoring the parameters involved
in EV charging. We extensively analyze the attack impact and the efficiency of
the proposed detection on real-world EV charging datasets. The results
highlight the vulnerabilities of smart charging operations and the need for a
monitoring mechanism to detect malicious CMAs.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12262" title="Abstract">arXiv:2310.12262</a> [<a href="/pdf/2310.12262" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving SCGAN&#x27;s Similarity Constraint and Learning a Better  Disentangled Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yazdanpanah%2C+I">Iman Yazdanpanah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">SCGAN adds a similarity constraint between generated images and conditions as
a regularization term on generative adversarial networks. Similarity constraint
works as a tutor to instruct the generator network to comprehend the difference
of representations based on conditions. We understand how SCGAN works on a
deeper level. This understanding makes us realize that the similarity
constraint functions like the contrastive loss function. We believe that a
model with high understanding and intelligence measures the similarity between
images based on their structure and high level features, just like humans do.
Two major changes we applied to SCGAN in order to make a modified model are
using SSIM to measure similarity between images and applying contrastive loss
principles to the similarity constraint. The modified model performs better
using FID and FactorVAE metrics. The modified model also has better
generalisability compared to other models. Keywords Generative Adversarial
Nets, Unsupervised Learning, Disentangled Representation Learning, Contrastive
Disentanglement, SSIM
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12263" title="Abstract">arXiv:2310.12263</a> [<a href="/pdf/2310.12263" title="Download PDF">pdf</a>, <a href="/format/2310.12263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plan-Guided Reinforcement Learning for Whole-Body Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Barreiros%2C+J">Jose Barreiros</a>, 
<a href="/search/cs?searchtype=author&query=Onol%2C+A+O">Aykut Ozgun Onol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Synthesizing complex whole-body manipulation behaviors has fundamental
challenges due to the rapidly growing combinatorics inherent to contact
interaction planning. While model-based methods have shown promising results in
solving long-horizon manipulation tasks, they often work under strict
assumptions, such as known model parameters, oracular observation of the
environment state, and simplified dynamics, resulting in plans that cannot
easily transfer to hardware. Learning-based approaches, such as imitation
learning (IL) and reinforcement learning (RL), have been shown to be robust
when operating over in-distribution states; however, they need heavy human
supervision. Specifically, model-free RL requires a tedious reward-shaping
process. IL methods, on the other hand, rely on human demonstrations that
involve advanced teleoperation methods. In this work, we propose a plan-guided
reinforcement learning (PGRL) framework to combine the advantages of
model-based planning and reinforcement learning. Our method requires minimal
human supervision because it relies on plans generated by model-based planners
to guide the exploration in RL. In exchange, RL derives a more robust policy
thanks to domain randomization. We test this approach on a whole-body
manipulation task on Punyo, an upper-body humanoid robot with compliant,
air-filled arm coverings, to pivot and lift a large box. Our preliminary
results indicate that the proposed methodology is promising to address
challenges that remain difficult for either model- or learning-based strategies
alone.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12269" title="Abstract">arXiv:2310.12269</a> [<a href="/pdf/2310.12269" title="Download PDF">pdf</a>, <a href="/ps/2310.12269" title="Download PostScript">ps</a>, <a href="/format/2310.12269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly-Popular and Super-Popular Matchings with Ties and Their  Connection to Stable Matchings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cs%C3%A1ji%2C+G">Gergely Cs&#xe1;ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In this paper, we study a slightly different definition of popularity in
bipartite graphs $G=(U,W,E)$ with two-sided preferences, when ties are present
in the preference lists. This is motivated by the observation that if an agent
$u$ is indifferent between his original partner $w$ in matching $M$ and his new
partner $w'\ne w$ in matching $N$, then he may probably still prefer to stay
with his original partner, as change requires effort, so he votes for $M$ in
this case, instead of being indifferent.
<br />We show that this alternative definition of popularity, which we call
weak-popularity allows us to guarantee the existence of such a matching and
also to find a weakly-popular matching in polynomial-time that has size at
least $\frac{3}{4}$ the size of the maximum weakly popular matching. We also
show that this matching is at least $\frac{4}{5}$ times the size of the maximum
(weakly) stable matching, so may provide a more desirable solution than the
current best (and tight under certain assumptions) $\frac{2}{3}$-approximation
for such a stable matching. We also show that unfortunately, finding a maximum
size weakly popular matching is NP-hard, even with one-sided ties and that
assuming some complexity theoretic assumptions, the $\frac{3}{4}$-approximation
bound is tight.
<br />Then, we study a more general model than weak-popularity, where for each
edge, we can specify independently for both endpoints the size of improvement
the endpoint needs to vote in favor of a new matching $N$. We show that even in
this more general model, a so-called $\gamma$-popular matching always exists
and that the same positive results still hold.
<br />Finally, we define an other, stronger variant of popularity, called
super-popularity, where even a weak improvement is enough to vote in favor of a
new matching. We show that for this case, even the existence problem is
NP-hard.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12270" title="Abstract">arXiv:2310.12270</a> [<a href="/pdf/2310.12270" title="Download PDF">pdf</a>, <a href="/ps/2310.12270" title="Download PostScript">ps</a>, <a href="/format/2310.12270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Simpler Sorting Networks and Monotone Circuits for Majority
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dobrokhotova-Maikova%2C+N">Natalia Dobrokhotova-Maikova</a>, 
<a href="/search/cs?searchtype=author&query=Kozachinskiy%2C+A">Alexander Kozachinskiy</a>, 
<a href="/search/cs?searchtype=author&query=Podolskii%2C+V">Vladimir Podolskii</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this paper, we study the problem of computing the majority function by
low-depth monotone circuits and a related problem of constructing low-depth
sorting networks. We consider both the classical setting with elementary
operations of arity $2$ and the generalized setting with operations of arity
$k$, where $k$ is a parameter. For both problems and both settings, there are
various constructions known, the minimal known depth being logarithmic.
However, there is currently no known construction that simultaneously achieves
sub-log-squared depth, effective constructability, simplicity, and has a
potential to be used in practice. In this paper we make progress towards
resolution of this problem.
<br />For computing majority by standard monotone circuits (gates of arity 2) we
provide an explicit monotone circuit of depth $O(\log_2^{5/3} n)$. The
construction is a combination of several known and not too complicated ideas.
<br />For arbitrary arity of gates $k$ we provide a new sorting network
architecture inspired by representation of inputs as a high-dimensional cube.
As a result we provide a simple construction that improves previous upper bound
of $4 \log_k^2 n$ to $2 \log_k^2 n$. We prove the similar bound for the depth
of the circuit computing majority of $n$ bits consisting of gates computing
majority of $k$ bits. Note, that for both problems there is an explicit
construction of depth $O(\log_k n)$ known, but the construction is complicated
and the constant hidden in $O$-notation is huge.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12274" title="Abstract">arXiv:2310.12274</a> [<a href="/pdf/2310.12274" title="Download PDF">pdf</a>, <a href="/format/2310.12274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Image is Worth Multiple Words: Learning Object Level Concepts using  Multi-Concept Prompt Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Chen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tanno%2C+R">Ryutaro Tanno</a>, 
<a href="/search/cs?searchtype=author&query=Saseendran%2C+A">Amrutha Saseendran</a>, 
<a href="/search/cs?searchtype=author&query=Diethe%2C+T">Tom Diethe</a>, 
<a href="/search/cs?searchtype=author&query=Teare%2C+P">Philip Teare</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://github.com/lxasqjc/MCPL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Textural Inversion, a prompt learning method, learns a singular embedding for
a new "word" to represent image style and appearance, allowing it to be
integrated into natural language sentences to generate novel synthesised
images. However, identifying and integrating multiple object-level concepts
within one scene poses significant challenges even when embeddings for
individual concepts are attainable. This is further confirmed by our empirical
tests. To address this challenge, we introduce a framework for Multi-Concept
Prompt Learning (MCPL), where multiple new "words" are simultaneously learned
from a single sentence-image pair. To enhance the accuracy of word-concept
correlation, we propose three regularisation techniques: Attention Masking
(AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss
(PromptCL) to separate the embeddings of different concepts; and Bind adjective
(Bind adj.) to associate new "words" with known words. We evaluate via image
generation, editing, and attention visualisation with diverse images. Extensive
quantitative comparisons demonstrate that our method can learn more
semantically disentangled concepts with enhanced word-concept correlation.
Additionally, we introduce a novel dataset and evaluation protocol tailored for
this new task of learning object-level concepts.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12279" title="Abstract">arXiv:2310.12279</a> [<a href="/pdf/2310.12279" title="Download PDF">pdf</a>, <a href="/ps/2310.12279" title="Download PostScript">ps</a>, <a href="/format/2310.12279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adjoint-based inversion of frictional parameters in earthquake modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Stiernstr%C3%B6m%2C+V">Vidar Stiernstr&#xf6;m</a>, 
<a href="/search/math?searchtype=author&query=Almquist%2C+M">Martin Almquist</a>, 
<a href="/search/math?searchtype=author&query=Dunham%2C+E+M">Eric M. Dunham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present an adjoint-based optimization method to invert for frictional
parameters used in earthquake modeling. The forward problem is linear
elasticity with nonlinear rate-and-state frictional faults. The misfit
functional quantifies the difference between simulated and measured particle
displacements or velocities at receiver locations. The misfit may include
windowing or filtering operators. We derive the corresponding adjoint problem,
which is linear elasticity with linear rate-and-state friction. The gradient of
the misfit is efficiently computed by convolving forward and adjoint variables
on the fault. The method thus extends the framework of full-waveform inversion
to include frictional faults with rate-and-state friction. In addition, we
present a space-time dual-consistent discretization of a dynamic rupture
problem with a rough fault in antiplane shear, using high-order accurate
summation-by-parts finite differences in combination with explicit Runge-Kutta
time integration. The dual consistency of the discretization ensures that the
discrete adjoint-based gradient is the exact gradient of the discrete cost
functional as well as a consistent approximation of the continuous gradient.
Our theoretical results are corroborated by inversions with synthetic data.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12281" title="Abstract">arXiv:2310.12281</a> [<a href="/pdf/2310.12281" title="Download PDF">pdf</a>, <a href="/format/2310.12281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Performance of Automated Grade Prediction in MOOC using  Graph Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farokhi%2C+S">Soheila Farokhi</a>, 
<a href="/search/cs?searchtype=author&query=Yaramala%2C+A">Aswani Yaramala</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiangtao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+F+A">Muhammad F. A. Khan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojun Qi</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+H">Hamid Karimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, Massive Open Online Courses (MOOCs) have gained significant
traction as a rapidly growing phenomenon in online learning. Unlike traditional
classrooms, MOOCs offer a unique opportunity to cater to a diverse audience
from different backgrounds and geographical locations. Renowned universities
and MOOC-specific providers, such as Coursera, offer MOOC courses on various
subjects. Automated assessment tasks like grade and early dropout predictions
are necessary due to the high enrollment and limited direct interaction between
teachers and learners. However, current automated assessment approaches
overlook the structural links between different entities involved in the
downstream tasks, such as the students and courses. Our hypothesis suggests
that these structural relationships, manifested through an interaction graph,
contain valuable information that can enhance the performance of the task at
hand. To validate this, we construct a unique knowledge graph for a large MOOC
dataset, which will be publicly available to the research community.
Furthermore, we utilize graph embedding techniques to extract latent structural
information encoded in the interactions between entities in the dataset. These
techniques do not require ground truth labels and can be utilized for various
tasks. Finally, by combining entity-specific features, behavioral features, and
extracted structural features, we enhance the performance of predictive machine
learning models in student assignment grade prediction. Our experiments
demonstrate that structural features can significantly improve the predictive
performance of downstream assessment tasks. The code and data are available in
\url{https://github.com/DSAatUSU/MOOPer_grade_prediction}
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12282" title="Abstract">arXiv:2310.12282</a> [<a href="/pdf/2310.12282" title="Download PDF">pdf</a>, <a href="/ps/2310.12282" title="Download PostScript">ps</a>, <a href="/format/2310.12282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean-field games among teams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+J">Jayakumar Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akshat Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+A">Aditya Mahajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we present a model of a game among teams. Each team consists
of a homogeneous population of agents. Agents within a team are cooperative
while the teams compete with other teams. The dynamics and the costs are
coupled through the empirical distribution (or the mean field) of the state of
agents in each team. This mean-field is assumed to be observed by all agents.
Agents have asymmetric information (also called a non-classical information
structure). We propose a mean-field based refinement of the Team-Nash
equilibrium of the game, which we call mean-field Markov perfect equilibrium
(MF-MPE). We identify a dynamic programming decomposition to characterize
MF-MPE. We then consider the case where each team has a large number of players
and present a mean-field approximation which approximates the game among
large-population teams as a game among infinite-population teams. We show that
MF-MPE of the game among teams of infinite population is easier to compute and
is an $\varepsilon$-approximate MF-MPE of the game among teams of finite
population.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12286" title="Abstract">arXiv:2310.12286</a> [<a href="/pdf/2310.12286" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System identification and closed-loop control of laser hot-wire directed  energy deposition using the parameter-signature-property modeling scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dehaghani%2C+M+R">M. Rahmani Dehaghani</a>, 
<a href="/search/eess?searchtype=author&query=Sahraeidolatkhaneh%2C+A">Atieh Sahraeidolatkhaneh</a>, 
<a href="/search/eess?searchtype=author&query=Nilsen%2C+M">Morgan Nilsen</a>, 
<a href="/search/eess?searchtype=author&query=Sikstr%C3%B6m%2C+F">Fredrik Sikstr&#xf6;m</a>, 
<a href="/search/eess?searchtype=author&query=Sajadi%2C+P">Pouyan Sajadi</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+Y">Yifan Tang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+G+G">G. Gary Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 14 figures, 4 tables,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Hot-wire directed energy deposition using a laser beam (DED-LB/w) is a method
of metal additive manufacturing (AM) that has benefits of high material
utilization and deposition rate, but parts manufactured by DED-LB/w suffer from
a substantial heat input and undesired surface finish. Hence, monitoring and
controlling the process parameters and signatures during the deposition is
crucial to ensure the quality of final part properties and geometries. This
paper explores the dynamic modeling of the DED-LB/w process and introduces a
parameter-signature-property modeling and control approach to enhance the
quality of modeling and control of part properties that cannot be measured in
situ. The study investigates different process parameters that influence the
melt pool width (signature) and bead width (property) in single and multi-layer
beads. The proposed modeling approach utilizes a parameter-signature model as
F_1 and a signature-property model as F_2. Linear and nonlinear modeling
approaches are compared to describe a dynamic relationship between process
parameters and a process signature, the melt pool width (F_1). A fully
connected artificial neural network is employed to model and predict the final
part property, i.e., bead width, based on melt pool signatures (F_2). Finally,
the effectiveness and usefulness of the proposed parameter-signature-property
modeling is tested and verified by integrating the parameter-signature (F_1)
and signature-property (F_2) models in the closed-loop control of the width of
the part. Compared with the control loop with only F_1, the proposed method
shows clear advantages and bears potential to be applied to control other part
properties that cannot be directly measured or monitored in situ.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12287" title="Abstract">arXiv:2310.12287</a> [<a href="/pdf/2310.12287" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A field study on Polish customers&#x27; attitude towards a service robot in a  cafe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiraga%2C+M">Maria Kiraga</a>, 
<a href="/search/cs?searchtype=author&query=Samsel%2C+Z">Zofia Samsel</a>, 
<a href="/search/cs?searchtype=author&query=Indurkhya%2C+B">Bipin Indurkhya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">More and more stores in Poland are adopting robots as customer assistants or
promotional tools. However, customer attitudes to such novelty remain
unexplored. This study focused on the role of social robots in self-service
cafes. This domain has not been explored in Poland before, and there is not
much research in other countries as well. We conducted a field study in two
cafes with a teleoperated robot Nao, which sat next to the counter serving as
an assistant to a human barista. We observed customer behavior, conducted
semi-structured interviews and questionnaires with the customers. The results
show that Polish customers are neutral and insecure about robots. However, they
do not exhibit a total dislike of these technologies. We considered three
stages of the interaction and identified features of each stage that need to be
designed carefully to yield user satisfaction.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12289" title="Abstract">arXiv:2310.12289</a> [<a href="/pdf/2310.12289" title="Download PDF">pdf</a>, <a href="/format/2310.12289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Incremental Learning of Imbalanced Data for Just-In-Time Software  Defect Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yunhua Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This work stems from three observations on prior Just-In-Time Software Defect
Prediction (JIT-SDP) models. First, prior studies treat the JIT-SDP problem
solely as a classification problem. Second, prior JIT-SDP studies do not
consider that class balancing processing may change the underlying
characteristics of software changeset data. Third, only a single source of
concept drift, the class imbalance evolution is addressed in prior JIT-SDP
incremental learning models.
<br />We propose an incremental learning framework called CPI-JIT for JIT-SDP.
First, in addition to a classification modeling component, the framework
includes a time-series forecast modeling component in order to learn temporal
interdependent relationship in the changesets. Second, the framework features a
purposefully designed over-sampling balancing technique based on SMOTE and
Principal Curves called SMOTE-PC. SMOTE-PC preserves the underlying
distribution of software changeset data.
<br />In this framework, we propose an incremental deep neural network model called
DeepICP. Via an evaluation using \numprojs software projects, we show that: 1)
SMOTE-PC improves the model's predictive performance; 2) to some software
projects it can be beneficial for defect prediction to harness temporal
interdependent relationship of software changesets; and 3) principal curves
summarize the underlying distribution of changeset data and reveals a new
source of concept drift that the DeepICP model is proposed to adapt to.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12290" title="Abstract">arXiv:2310.12290</a> [<a href="/pdf/2310.12290" title="Download PDF">pdf</a>, <a href="/format/2310.12290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fact-based Agent modeling for Multi-Agent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+B">Baofu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Caiming Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In multi-agent systems, agents need to interact and collaborate with other
agents in environments. Agent modeling is crucial to facilitate agent
interactions and make adaptive cooperation strategies. However, it is
challenging for agents to model the beliefs, behaviors, and intentions of other
agents in non-stationary environment where all agent policies are learned
simultaneously. In addition, the existing methods realize agent modeling
through behavior cloning which assume that the local information of other
agents can be accessed during execution or training. However, this assumption
is infeasible in unknown scenarios characterized by unknown agents, such as
competition teams, unreliable communication and federated learning due to
privacy concerns. To eliminate this assumption and achieve agent modeling in
unknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which
fact-based belief inference (FBI) network models other agents in partially
observable environment only based on its local information. The reward and
observation obtained by agents after taking actions are called facts, and FAM
uses facts as reconstruction target to learn the policy representation of other
agents through a variational autoencoder. We evaluate FAM on various Multiagent
Particle Environment (MPE) and compare the results with several
state-of-the-art MARL algorithms. Experimental results show that compared with
baseline methods, FAM can effectively improve the efficiency of agent policy
learning by making adaptive cooperation strategies in multi-agent reinforcement
learning tasks, while achieving higher returns in complex
competitive-cooperative mixed scenarios.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12294" title="Abstract">arXiv:2310.12294</a> [<a href="/pdf/2310.12294" title="Download PDF">pdf</a>, <a href="/format/2310.12294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Set Multivariate Time-Series Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+T">Thomas Lai</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T+K+K">Thi Kieu Khanh Ho</a>, 
<a href="/search/cs?searchtype=author&query=Armanfard%2C+N">Narges Armanfard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 tables, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Numerous methods for time series anomaly detection (TSAD) methods have
emerged in recent years. Most existing methods are unsupervised and assume the
availability of normal training samples only, while few supervised methods have
shown superior performance by incorporating labeled anomalous samples in the
training phase. However, certain anomaly types are inherently challenging for
unsupervised methods to differentiate from normal data, while supervised
methods are constrained to detecting anomalies resembling those present during
training, failing to generalize to unseen anomaly classes. This paper is the
first attempt in providing a novel approach for the open-set TSAD problem, in
which a small number of labeled anomalies from a limited class of anomalies are
visible in the training phase, with the objective of detecting both seen and
unseen anomaly classes in the test phase. The proposed method, called
Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three
primary modules: a Feature Extractor to extract meaningful time-series
features; a Multi-head Network consisting of Generative-, Deviation-, and
Contrastive heads for capturing both seen and unseen anomaly classes; and an
Anomaly Scoring module leveraging the insights of the three heads to detect
anomalies. Extensive experiments on three real-world datasets consistently show
that our approach surpasses existing methods under various experimental
settings, thus establishing a new state-of-the-art performance in the TSAD
field.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12296" title="Abstract">arXiv:2310.12296</a> [<a href="/pdf/2310.12296" title="Download PDF">pdf</a>, <a href="/format/2310.12296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Video Transformers for Segmentation: A Survey of  Application and Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karim%2C+R">Rezaul Karim</a>, 
<a href="/search/cs?searchtype=author&query=Wildes%2C+R+P">Richard P. Wildes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video segmentation encompasses a wide range of categories of problem
formulation, e.g., object, scene, actor-action and multimodal video
segmentation, for delineating task-specific scene components with pixel-level
masks. Recently, approaches in this research area shifted from concentrating on
ConvNet-based to transformer-based models. In addition, various
interpretability approaches have appeared for transformer models and video
temporal dynamics, motivated by the growing interest in basic scientific
understanding, model diagnostics and societal implications of real-world
deployment. Previous surveys mainly focused on ConvNet models on a subset of
video segmentation tasks or transformers for classification tasks. Moreover,
component-wise discussion of transformer-based video segmentation models has
not yet received due focus. In addition, previous reviews of interpretability
methods focused on transformers for classification, while analysis of video
temporal dynamics modelling capabilities of video models received less
attention. In this survey, we address the above with a thorough discussion of
various categories of video segmentation, a component-wise discussion of the
state-of-the-art transformer-based models, and a review of related
interpretability methods. We first present an introduction to the different
video segmentation task categories, their objectives, specific challenges and
benchmark datasets. Next, we provide a component-wise review of recent
transformer-based models and document the state of the art on different video
segmentation tasks. Subsequently, we discuss post-hoc and ante-hoc
interpretability methods for transformer models and interpretability methods
for understanding the role of the temporal dimension in video models. Finally,
we conclude our discussion with future research directions.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12298" title="Abstract">arXiv:2310.12298</a> [<a href="/pdf/2310.12298" title="Download PDF">pdf</a>, <a href="/format/2310.12298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jorge: Approximate Preconditioning for GPU-efficient Second-order  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Siddharth Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sating%2C+Z">Zachary Sating</a>, 
<a href="/search/cs?searchtype=author&query=Bhatele%2C+A">Abhinav Bhatele</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Despite their better convergence properties compared to first-order
optimizers, second-order optimizers for deep learning have been less popular
due to their significant computational costs. The primary efficiency bottleneck
in such optimizers is matrix inverse calculations in the preconditioning step,
which are expensive to compute on GPUs. In this paper, we introduce Jorge, a
second-order optimizer that promises the best of both worlds -- rapid
convergence benefits of second-order methods, and high computational efficiency
typical of first-order methods. We address the primary computational bottleneck
of computing matrix inverses by completely eliminating them using an
approximation of the preconditioner computation. This makes Jorge extremely
efficient on GPUs in terms of wall-clock time. Further, we describe an approach
to determine Jorge's hyperparameters directly from a well-tuned SGD baseline,
thereby significantly minimizing tuning efforts. Our empirical evaluations
demonstrate the distinct advantages of using Jorge, outperforming
state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple
deep learning models, both in terms of sample efficiency and wall-clock time.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12300" title="Abstract">arXiv:2310.12300</a> [<a href="/pdf/2310.12300" title="Download PDF">pdf</a>, <a href="/format/2310.12300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Sheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingya Li</a>, 
<a href="/search/cs?searchtype=author&query=Bitterman%2C+D">Danielle Bitterman</a>, 
<a href="/search/cs?searchtype=author&query=Savova%2C+G">Guergana Savova</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning (ICL) is a new learning paradigm that has gained
popularity along with the development of large language models. In this work,
we adapt a recently proposed hardness metric, pointwise $\mathcal{V}$-usable
information (PVI), to an in-context version (in-context PVI). Compared to the
original PVI, in-context PVI is more efficient in that it requires only a few
exemplars and does not require fine-tuning. We conducted a comprehensive
empirical analysis to evaluate the reliability of in-context PVI. Our findings
indicate that in-context PVI estimates exhibit similar characteristics to the
original PVI. Specific to the in-context setting, we show that in-context PVI
estimates remain consistent across different exemplar selections and numbers of
shots. The variance of in-context PVI estimates across different exemplar
selections is insignificant, which suggests that in-context PVI are stable.
Furthermore, we demonstrate how in-context PVI can be employed to identify
challenging instances. Our work highlights the potential of in-context PVI and
provides new insights into the capabilities of ICL.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12303" title="Abstract">arXiv:2310.12303</a> [<a href="/pdf/2310.12303" title="Download PDF">pdf</a>, <a href="/format/2310.12303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Document-Level Language Models for Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrick%2C+F">Frithjof Petrick</a>, 
<a href="/search/cs?searchtype=author&query=Herold%2C+C">Christian Herold</a>, 
<a href="/search/cs?searchtype=author&query=Petrushkov%2C+P">Pavel Petrushkov</a>, 
<a href="/search/cs?searchtype=author&query=Khadivi%2C+S">Shahram Khadivi</a>, 
<a href="/search/cs?searchtype=author&query=Ney%2C+H">Hermann Ney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at WMT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12305" title="Abstract">arXiv:2310.12305</a> [<a href="/pdf/2310.12305" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Random, Fair, and Verifiable Games on Blockchain. Raffle smart  contract designs on Sui Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Justa Liang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ray Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pierce Hung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Damien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+A">Ashley Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Chalkias%2C+K">Konstantinos Chalkias</a>, 
<a href="/search/cs?searchtype=author&query=Pleros%2C+S">Stefanos Pleros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Randomness plays a pivotal role in modern online gaming, but disputes have
arisen over the accuracy of stated winning chances, resulting in legal issues
and financial setbacks for gaming companies. Fortunately, blockchain-based
games offer a solution to the transparency and fairness issue regarding
randomness. Furthermore, emerging blockchain technology like Sui Network
enhances the efficiency of smart contracts by eliminating traditional web3
barriers, such as inefficiencies and expensive transaction fees. This unlocks
the potential for extensive decentralized gaming applications.
<br />This paper aims to provide insights into designing a fair, verifiable, and
efficient smart contract game on blockchain by the example of building raffles
on the Sui Network. We explore efficient methods for implementing randomness on
smart contracts, including DRAND committee-based decentralized random beacons
and single private-key-based verifiable random functions (VRF). Then, progress
from basic to comprehensive smart contract design. We addressed limitations in
developing blockchain games in general, such as data input and storage space
constraints.
<br />We propose corresponding solutions, encompassing the utilization of Object
Tables, Delegate Object Creation, and Zero-Knowledge Proofs (ZKP) to optimize
storage and input efficiency. After testing our designs, we found that the
transaction fees for DRAND beacons and private-key-based VRFs are similar.
Moreover, Object Tables incur higher overall transaction fees, while the ZKP
setup fee is cheap but becomes very expensive during the verification process.
Moreover, we identified suitable designs for different application scenarios by
comparing the pros and cons of different smart contract implementations. Our
findings provide valuable guidance for future researchers and developers in
building random, fair, and verifiable games with smart contracts.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12306" title="Abstract">arXiv:2310.12306</a> [<a href="/pdf/2310.12306" title="Download PDF">pdf</a>, <a href="/format/2310.12306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding and Characterizing the Arbitrage Bot Scam In the  Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kai Li</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+S">Shixuan Guan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Darren Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM SIGMETRICS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">This paper presents the first comprehensive analysis of an emerging
cryptocurrency scam named "arbitrage bot" disseminated on online social
networks. The scam revolves around Decentralized Exchanges (DEX) arbitrage and
aims to lure victims into executing a so-called "bot contract" to steal funds
from them.
<br />To collect the scam at a large scale, we developed a fully automated scam
detection system named CryptoScamHunter, which continuously collects YouTube
videos and automatically detects scams. Meanwhile, CryptoScamHunter can
download the source code of the bot contract from the provided links and
extract the associated scam cryptocurrency address. Through deploying
CryptoScamHunter from Jun. 2022 to Jun. 2023, we have detected 10,442 arbitrage
bot scam videos published from thousands of YouTube accounts. Our analysis
reveals that different strategies have been utilized in spreading the scam,
including crafting popular accounts, registering spam accounts, and using
obfuscation tricks to hide the real scam address in the bot contracts.
Moreover, from the scam videos we have collected over 800 malicious bot
contracts with source code and extracted 354 scam addresses. By further
expanding the scam addresses with a similar contract matching technique, we
have obtained a total of 1,697 scam addresses. Through tracing the transactions
of all scam addresses on the Ethereum mainnet and Binance Smart Chain, we
reveal that over 25,000 victims have fallen prey to this scam, resulting in a
financial loss of up to 15 million USD.
<br />Overall, our work sheds light on the dissemination tactics and censorship
evasion strategies adopted in the arbitrage bot scam, as well as on the scale
and impact of such a scam on online social networks and blockchain platforms,
emphasizing the urgent need for effective detection and prevention mechanisms
against such fraudulent activity.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12309" title="Abstract">arXiv:2310.12309</a> [<a href="/pdf/2310.12309" title="Download PDF">pdf</a>, <a href="/format/2310.12309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unifying Framework for Learning Argumentation Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mileva%2C+Z">Zlatina Mileva</a>, 
<a href="/search/cs?searchtype=author&query=Bikakis%2C+A">Antonis Bikakis</a>, 
<a href="/search/cs?searchtype=author&query=D%27Asaro%2C+F+A">Fabio Aurelio D&#x27;Asaro</a>, 
<a href="/search/cs?searchtype=author&query=Law%2C+M">Mark Law</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+A">Alessandra Russo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Argumentation is a very active research field of Artificial Intelligence
concerned with the representation and evaluation of arguments used in dialogues
between humans and/or artificial agents. Acceptability semantics of formal
argumentation systems define the criteria for the acceptance or rejection of
arguments. Several software systems, known as argumentation solvers, have been
developed to compute the accepted/rejected arguments using such criteria. These
include systems that learn to identify the accepted arguments using
non-interpretable methods. In this paper we present a novel framework, which
uses an Inductive Logic Programming approach to learn the acceptability
semantics for several abstract and structured argumentation frameworks in an
interpretable way. Through an empirical evaluation we show that our framework
outperforms existing argumentation solvers, thus opening up new future research
directions in the area of formal argumentation and human-machine dialogues.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12318" title="Abstract">arXiv:2310.12318</a> [<a href="/pdf/2310.12318" title="Download PDF">pdf</a>, <a href="/format/2310.12318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sentiment Problem: A Critical Survey towards Deconstructing  Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkit%2C+P+N">Pranav Narayanan Venkit</a>, 
<a href="/search/cs?searchtype=author&query=Srinath%2C+M">Mukund Srinath</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+S">Sanjana Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Venkatraman%2C+S">Saranya Venkatraman</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vipul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Passonneau%2C+R+J">Rebecca J. Passonneau</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+S">Shomir Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted and will appear at the EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We conduct an inquiry into the sociotechnical aspects of sentiment analysis
(SA) by critically examining 189 peer-reviewed papers on their applications,
models, and datasets. Our investigation stems from the recognition that SA has
become an integral component of diverse sociotechnical systems, exerting
influence on both social and technical users. By delving into sociological and
technological literature on sentiment, we unveil distinct conceptualizations of
this term in domains such as finance, government, and medicine. Our study
exposes a lack of explicit definitions and frameworks for characterizing
sentiment, resulting in potential challenges and biases. To tackle this issue,
we propose an ethics sheet encompassing critical inquiries to guide
practitioners in ensuring equitable utilization of SA. Our findings underscore
the significance of adopting an interdisciplinary approach to defining
sentiment in SA and offer a pragmatic solution for its implementation.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12319" title="Abstract">arXiv:2310.12319</a> [<a href="/pdf/2310.12319" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arithmetic Operators over Finite Field GF($2^m$) in BCH and Reed-Solomon  Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nabipour%2C+S">Saeideh Nabipour</a>, 
<a href="/search/cs?searchtype=author&query=Gholizade%2C+M">Masoume Gholizade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 11 Figures, 5 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Galois field arithmetic circuits are utilized in various areas such as error
correction codes, communications, signal processing, and security engineering.
In this chapter, we will explain the significance of error detection and
correction technique, while also examining the fundamental principles and wide
range of techniques available. Moreover, explaining the mathematical details of
BCH and Reed-Solomon codes necessitates a comprehensive utilization of
GF($2^m$) arithmetic. Therefore, the primary contribution of this chapter
entails an investigation of the arithmetic operations over finite field that
are indispensable for the implementation of BCH and Reed-Solomon codes. These
operations involve addition, subtraction, multiplication, squaring, square
roots, multiplicative inverses, division, and exponentiation.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12320" title="Abstract">arXiv:2310.12320</a> [<a href="/pdf/2310.12320" title="Download PDF">pdf</a>, <a href="/format/2310.12320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Distributed Smoothing and Mapping via On-Manifold Consensus  ADMM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McGann%2C+D">Daniel McGann</a>, 
<a href="/search/cs?searchtype=author&query=Lassak%2C+K">Kyle Lassak</a>, 
<a href="/search/cs?searchtype=author&query=Kaess%2C+M">Michael Kaess</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper we present a fully distributed, asynchronous, and general
purpose optimization algorithm for Consensus Simultaneous Localization and
Mapping (CSLAM). Multi-robot teams require that agents have timely and accurate
solutions to their state as well as the states of the other robots in the team.
To optimize this solution we develop a CSLAM back-end based on Consensus ADMM
called MESA (Manifold, Edge-based, Separable ADMM). MESA is fully distributed
to tolerate failures of individual robots, asynchronous to tolerate practical
network conditions, and general purpose to handle any CSLAM problem
formulation. We demonstrate that MESA exhibits superior convergence rates and
accuracy compare to existing state-of-the art CSLAM back-end optimizers.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12321" title="Abstract">arXiv:2310.12321</a> [<a href="/pdf/2310.12321" title="Download PDF">pdf</a>, <a href="/format/2310.12321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of GPT-3 Family Large Language Models Including ChatGPT and  GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalyan%2C+K+S">Katikapalli Subramanyam Kalyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint under review, 58 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) are a special class of pretrained language
models obtained by scaling model size, pretraining corpus and computation.
LLMs, because of their large size and pretraining on large volumes of text
data, exhibit special abilities which allow them to achieve remarkable
performances without any task-specific training in many of the natural language
processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the
popularity of LLMs is increasing exponentially after the introduction of models
like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models,
including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With
the ever-rising popularity of GLLMs, especially in the research community,
there is a strong need for a comprehensive survey which summarizes the recent
research progress in multiple dimensions and can guide the research community
with insightful future research directions. We start the survey paper with
foundation concepts like transformers, transfer learning, self-supervised
learning, pretrained language models and large language models. We then present
a brief overview of GLLMs and discuss the performances of GLLMs in various
downstream tasks, specific domains and multiple languages. We also discuss the
data labelling and data augmentation abilities of GLLMs, the robustness of
GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with
multiple insightful future research directions. To summarize, this
comprehensive survey paper will serve as a good resource for both academic and
industry people to stay updated with the latest research related to GPT-3
family large language models.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12324" title="Abstract">arXiv:2310.12324</a> [<a href="/pdf/2310.12324" title="Download PDF">pdf</a>, <a href="/format/2310.12324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opportunities for Adaptive Experiments to Enable Continuous Improvement  that Trades-off Instructor and Researcher Incentives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Musabirov%2C+I">Ilya Musabirov</a>, 
<a href="/search/cs?searchtype=author&query=Zavaleta-Bernuy%2C+A">Angela Zavaleta-Bernuy</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liut%2C+M">Michael Liut</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J+J">Joseph Jay Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Randomized experimental comparisons of alternative pedagogical strategies
could provide useful empirical evidence in instructors' decision-making.
However, traditional experiments do not have a clear and simple pathway to
using data rapidly to try to increase the chances that students in an
experiment get the best conditions. Drawing inspiration from the use of machine
learning and experimentation in product development at leading technology
companies, we explore how adaptive experimentation might help in continuous
course improvement. In adaptive experiments, as different arms/conditions are
deployed to students, data is analyzed and used to change the experience for
future students. This can be done using machine learning algorithms to identify
which actions are more promising for improving student experience or outcomes.
This algorithm can then dynamically deploy the most effective conditions to
future students, resulting in better support for students' needs. We illustrate
the approach with a case study providing a side-by-side comparison of
traditional and adaptive experimentation of self-explanation prompts in online
homework problems in a CS1 course. This provides a first step in exploring the
future of how this methodology can be useful in bridging research and practice
in doing continuous improvement.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12328" title="Abstract">arXiv:2310.12328</a> [<a href="/pdf/2310.12328" title="Download PDF">pdf</a>, <a href="/ps/2310.12328" title="Download PostScript">ps</a>, <a href="/format/2310.12328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soccer on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkhoosh%2C+M+H">Mehdi Houshmand Sarkhoosh</a>, 
<a href="/search/cs?searchtype=author&query=Dorcheh%2C+S+M+M">Sayed Mohammad Majidi Dorcheh</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+S">Sushant Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Midoglu%2C+C">Cise Midoglu</a>, 
<a href="/search/cs?searchtype=author&query=Sabet%2C+S+S">Saeed Shafiee Sabet</a>, 
<a href="/search/cs?searchtype=author&query=Halvorsen%2C+P">P&#xe5;l Halvorsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In the era of digitalization, social media has become an integral part of our
lives, serving as a significant hub for individuals and businesses to share
information, communicate, and engage. This is also the case for professional
sports, where leagues, clubs and players are using social media to reach out to
their fans. In this respect, a huge amount of time is spent curating multimedia
content for various social media platforms and their target users. With the
emergence of Artificial Intelligence (AI), AI-based tools for automating
content generation and enhancing user experiences on social media have become
widely popular. However, to effectively utilize such tools, it is imperative to
comprehend the demographics and preferences of users on different platforms,
understand how content providers post information in these channels, and how
different types of multimedia are consumed by audiences. This report presents
an analysis of social media platforms, in terms of demographics, supported
multimedia modalities, and distinct features and specifications for different
modalities, followed by a comparative case study of select European soccer
leagues and teams, in terms of their social media practices. Through this
analysis, we demonstrate that social media, while being very important for and
widely used by supporters from all ages, also requires a fine-tuned effort on
the part of soccer professionals, in order to elevate fan experiences and
foster engagement.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12334" title="Abstract">arXiv:2310.12334</a> [<a href="/pdf/2310.12334" title="Download PDF">pdf</a>, <a href="/format/2310.12334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Representation Learning for Histopathologic Images with  Cluster Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weiyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chongyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=DiPalma%2C+J">Joseph DiPalma</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>, 
<a href="/search/cs?searchtype=author&query=Hassanpour%2C+S">Saeed Hassanpour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2023, pp. 21404-21414
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in whole-slide image (WSI) scanners and computational
capabilities have significantly propelled the application of artificial
intelligence in histopathology slide analysis. While these strides are
promising, current supervised learning approaches for WSI analysis come with
the challenge of exhaustively labeling high-resolution slides - a process that
is both labor-intensive and time-consuming. In contrast, self-supervised
learning (SSL) pretraining strategies are emerging as a viable alternative,
given that they don't rely on explicit data annotations. These SSL strategies
are quickly bridging the performance disparity with their supervised
counterparts. In this context, we introduce an SSL framework. This framework
aims for transferable representation learning and semantically meaningful
clustering by synergizing invariance loss and clustering loss in WSI analysis.
Notably, our approach outperforms common SSL methods in downstream
classification and clustering tasks, as evidenced by tests on the Camelyon16
and a pancreatic cancer dataset. The code and additional details are accessible
at: https://github.com/wwyi1828/CluSiam.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12337" title="Abstract">arXiv:2310.12337</a> [<a href="/pdf/2310.12337" title="Download PDF">pdf</a>, <a href="/ps/2310.12337" title="Download PostScript">ps</a>, <a href="/format/2310.12337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compiler Testing With Relaxed Memory Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geeson%2C+L">Luke Geeson</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+L">Lee Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, Submitted to IEEE/ACM International Symposium on Code Generation and Optimization,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Hardware Architecture (cs.AR); Software Engineering (cs.SE)

</div>
<p class="mathjax">It is critical that compilers are correct! Finding bugs is one aspect of
testing the correctness of compilers in wide use today. A compiler is correct
if every compiled program behaves as allowed by the semantics of its source
code - else there is a bug. Memory consistency models define the semantics of
concurrent programs. We focus on how to detect concurrency bugs introduced by
compilers, as identified using memory models. We seek a testing technique that
automatically covers concurrency bugs up to fixed bounds on program sizes and
that scales to find bugs in compiled programs with many lines of code.
Otherwise, a testing technique can miss bugs. Unfortunately, the
state-of-the-art techniques are yet to satisfy all of these properties. We
present the T\'el\'echat compiler testing tool for concurrent programs.
T\'el\'echat finds a concurrency bug when the behaviour of a compiled program,
as allowed by its architecture memory model, is not a behaviour of the source
program under its source model. We make three claims: T\'el\'echat improves the
state-of-the-art at finding bugs in code generation for multi-threaded
execution, it is the first public description of a compiler testing tool for
concurrency that is deployed in industry, and it is the first tool that takes a
significant step towards the desired properties. We provide experimental
evidence suggesting T\'el\'echat finds bugs missed by other state-of-the-art
techniques, case studies indicating that T\'el\'echat satisfies the properties,
and reports of our experience deploying T\'el\'echat in industry.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12342" title="Abstract">arXiv:2310.12342</a> [<a href="/pdf/2310.12342" title="Download PDF">pdf</a>, <a href="/format/2310.12342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliminating Reasoning via Inferring with Planning: A New Framework to  Guide LLMs&#x27; Non-linear Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+Y">Yongqi Tong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Simeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Chain-of-Thought(CoT) prompting and its variants explore equipping large
language models (LLMs) with high-level reasoning abilities by emulating
human-like linear cognition and logic. However, the human mind is complicated
and mixed with both linear and nonlinear thinking. In this work, we propose
\textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel
prompting that combines the principles of elimination and inference in order to
guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize
Natural Language Inference (NLI) to deduce each possible solution's entailment
relation with context, commonsense, or facts, therefore yielding a broader
perspective by thinking back for inferring. This forward planning and backward
eliminating process allows IEP to better simulate the complex human thinking
processes compared to other CoT-based methods, which only reflect linear
cognitive processes. We conducted a series of empirical studies and have
corroborated that IEP consistently outperforms CoT across various tasks.
Additionally, we observe that integrating IEP and CoT further improves the
LLMs' performance on certain tasks, highlighting the necessity of equipping
LLMs with mixed logic processes. Moreover, to better evaluate comprehensive
features inherent in human logic, we introduce \textbf{M}ental-\textbf{A}bility
\textbf{R}easoning \textbf{B}enchmark (MARB). The benchmark comprises six novel
subtasks with a total of 9,115 questions, among which 1,685 are developed with
hand-crafted rationale references. We believe both \textsc{IEP} and
\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and
verbal reasoning abilities and drive further advancements. \textsc{MARB} will
be available at ~\texttt{anonymity link} soon.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12343" title="Abstract">arXiv:2310.12343</a> [<a href="/pdf/2310.12343" title="Download PDF">pdf</a>, <a href="/format/2310.12343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Environment Adaptation with Few Shots for OFDM Receiver and mmWave  Beamforming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+O">Ouya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shenglong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G+Y">Geoffrey Ye Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Few-shot learning (FSL) enables adaptation to new tasks with only limited
training data. In wireless communications, channel environments can vary
drastically; therefore, FSL techniques can quickly adjust transceiver
accordingly. In this paper, we develop two FSL frameworks that fit in wireless
transceiver design. Both frameworks are base on optimization programs that can
be solved by well-known algorithms like the inexact alternating direction
method of multipliers (iADMM) and the inexact alternating direction method
(iADM). As examples, we demonstrate how the proposed two FSL frameworks are
used for the OFDM receiver and beamforming (BF) for the millimeter wave
(mmWave) system. The numerical experiments confirm their desirable performance
in both applications compared to other popular approaches, such as transfer
learning (TL) and model-agnostic meta-learning.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12344" title="Abstract">arXiv:2310.12344</a> [<a href="/pdf/2310.12344" title="Download PDF">pdf</a>, <a href="/format/2310.12344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LACMA: Language-Aligning Contrastive Learning with Meta-Actions for  Embodied Instruction Following
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng-Fu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xiyang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y+F">Yu-Chiang Frank Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">End-to-end Transformers have demonstrated an impressive success rate for
Embodied Instruction Following when the environment has been seen in training.
However, they tend to struggle when deployed in an unseen environment. This
lack of generalizability is due to the agent's insensitivity to subtle changes
in natural language instructions. To mitigate this issue, we propose explicitly
aligning the agent's hidden states with the instructions via contrastive
learning. Nevertheless, the semantic gap between high-level language
instructions and the agent's low-level action space remains an obstacle.
Therefore, we further introduce a novel concept of meta-actions to bridge the
gap. Meta-actions are ubiquitous action patterns that can be parsed from the
original action sequence. These patterns represent higher-level semantics that
are intuitively aligned closer to the instructions. When meta-actions are
applied as additional training signals, the agent generalizes better to unseen
environments. Compared to a strong multi-modal Transformer baseline, we achieve
a significant 4.5% absolute gain in success rate in unseen environments of
ALFRED Embodied Instruction Following. Additional analysis shows that the
contrastive objective and meta-actions are complementary in achieving the best
results, and the resulting agent better aligns its states with corresponding
instructions, making it more suitable for real-world embodied agents. The code
is available at: https://github.com/joeyy5588/LACMA.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12345" title="Abstract">arXiv:2310.12345</a> [<a href="/pdf/2310.12345" title="Download PDF">pdf</a>, <a href="/format/2310.12345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClusT3: Information Invariant Test-Time Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hakim%2C+G+A+V">Gustavo A. Vargas Hakim</a>, 
<a href="/search/cs?searchtype=author&query=Osowiechi%2C+D">David Osowiechi</a>, 
<a href="/search/cs?searchtype=author&query=Noori%2C+M">Mehrdad Noori</a>, 
<a href="/search/cs?searchtype=author&query=Cheraghalikhani%2C+M">Milad Cheraghalikhani</a>, 
<a href="/search/cs?searchtype=author&query=Ayed%2C+I+B">Ismail Ben Ayed</a>, 
<a href="/search/cs?searchtype=author&query=Desrosiers%2C+C">Christian Desrosiers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep Learning models have shown remarkable performance in a broad range of
vision tasks. However, they are often vulnerable against domain shifts at
test-time. Test-time training (TTT) methods have been developed in an attempt
to mitigate these vulnerabilities, where a secondary task is solved at training
time simultaneously with the main task, to be later used as an self-supervised
proxy task at test-time. In this work, we propose a novel unsupervised TTT
technique based on the maximization of Mutual Information between multi-scale
feature maps and a discrete latent representation, which can be integrated to
the standard training as an auxiliary clustering task. Experimental results
demonstrate competitive classification performance on different popular
test-time adaptation benchmarks.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12347" title="Abstract">arXiv:2310.12347</a> [<a href="/pdf/2310.12347" title="Download PDF">pdf</a>, <a href="/format/2310.12347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VISGRADER: Automatic Grading of D3 Visualizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hull%2C+M">Matthew Hull</a>, 
<a href="/search/cs?searchtype=author&query=Pednekar%2C+V">Vivian Pednekar</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+H">Hannah Murray</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nimisha Roy</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+E">Emmanuel Tung</a>, 
<a href="/search/cs?searchtype=author&query=Routray%2C+S">Susanta Routray</a>, 
<a href="/search/cs?searchtype=author&query=Guerin%2C+C">Connor Guerin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Justin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z+J">Zijie J. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seongmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Roozbahani%2C+M">Mahdi Roozbahani</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Manually grading D3 data visualizations is a challenging endeavor, and is
especially difficult for large classes with hundreds of students. Grading an
interactive visualization requires a combination of interactive, quantitative,
and qualitative evaluation that are conventionally done manually and are
difficult to scale up as the visualization complexity, data size, and number of
students increase. We present VISGRADER, a first-of-its kind automatic grading
method for D3 visualizations that scalably and precisely evaluates the data
bindings, visual encodings, interactions, and design specifications used in a
visualization. Our method enhances students learning experience, enabling them
to submit their code frequently and receive rapid feedback to better inform
iteration and improvement to their code and visualization design. We have
successfully deployed our method and auto-graded D3 submissions from more than
4000 students in a visualization course at Georgia Tech, and received positive
feedback for expanding its adoption.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12349" title="Abstract">arXiv:2310.12349</a> [<a href="/pdf/2310.12349" title="Download PDF">pdf</a>, <a href="/format/2310.12349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing 3D Virtual Safety Risk Terrain for UAS Operations in Complex  Urban Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhenyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+J">John-Paul Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Mardanov%2C+J">Javid Mardanov</a>, 
<a href="/search/cs?searchtype=author&query=Marais%2C+K">Karen Marais</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Unmanned Aerial Systems (UAS), an integral part of the Advanced Air Mobility
(AAM) vision, are capable of performing a wide spectrum of tasks in urban
environments. The societal integration of UAS is a pivotal challenge, as these
systems must operate harmoniously within the constraints imposed by regulations
and societal concerns. In complex urban environments, UAS safety has been a
perennial obstacle to their large-scale deployment. To mitigate UAS safety risk
and facilitate risk-aware UAS operations planning, we propose a novel concept
called \textit{3D virtual risk terrain}. This concept converts public risk
constraints in an urban environment into 3D exclusion zones that UAS operations
should avoid to adequately reduce risk to Entities of Value (EoV). To implement
the 3D virtual risk terrain, we develop a conditional probability framework
that comprehensively integrates most existing basic models for UAS ground risk.
To demonstrate the concept, we build risk terrains on a Chicago downtown model
and observe their characteristics under different conditions. We believe that
the 3D virtual risk terrain has the potential to become a new routine tool for
risk-aware UAS operations planning, urban airspace management, and policy
development. The same idea can also be extended to other forms of societal
impacts, such as noise, privacy, and perceived risk.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12350" title="Abstract">arXiv:2310.12350</a> [<a href="/pdf/2310.12350" title="Download PDF">pdf</a>, <a href="/format/2310.12350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equipping Federated Graph Neural Networks with Structure-aware Group  Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+N">Nan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiuling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+H">Wendy Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+V">Violet Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yue Ning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have been widely used for various types of graph
data processing and analytical tasks in different domains. Training GNNs over
centralized graph data can be infeasible due to privacy concerns and regulatory
restrictions. Thus, federated learning (FL) becomes a trending solution to
address this challenge in a distributed learning paradigm. However, as GNNs may
inherit historical bias from training data and lead to discriminatory
predictions, the bias of local models can be easily propagated to the global
model in distributed settings. This poses a new challenge in mitigating bias in
federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair
Federated Graph Neural Network, that enhances group fairness of federated GNNs.
As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN
aims to mitigate both types of bias under federated settings. First, we provide
theoretical insights on the connection between data bias in a training graph
and statistical fairness metrics of the trained GNN models. Based on the
theoretical analysis, we design $\text{F}^2$GNN which contains two key
components: a fairness-aware local model update scheme that enhances group
fairness of the local models on the client side, and a fairness-weighted global
model update scheme that takes both data bias and fairness metrics of local
models into consideration in the aggregation process. We evaluate
$\text{F}^2$GNN empirically versus a number of baseline methods, and
demonstrate that $\text{F}^2$GNN outperforms these baselines in terms of both
fairness and model accuracy.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12352" title="Abstract">arXiv:2310.12352</a> [<a href="/pdf/2310.12352" title="Download PDF">pdf</a>, <a href="/format/2310.12352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> knn-seq: Efficient, Extensible kNN-MT Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deguchi%2C+H">Hiroyuki Deguchi</a>, 
<a href="/search/cs?searchtype=author&query=Hirano%2C+H">Hayate Hirano</a>, 
<a href="/search/cs?searchtype=author&query=Hoshino%2C+T">Tomoki Hoshino</a>, 
<a href="/search/cs?searchtype=author&query=Nishida%2C+Y">Yuto Nishida</a>, 
<a href="/search/cs?searchtype=author&query=Vasselli%2C+J">Justin Vasselli</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Taro Watanabe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">k-nearest-neighbor machine translation (kNN-MT) boosts the translation
quality of a pre-trained neural machine translation (NMT) model by utilizing
translation examples during decoding. Translation examples are stored in a
vector database, called a datastore, which contains one entry for each target
token from the parallel data it is made from. Due to its size, it is
computationally expensive both to construct and to retrieve examples from the
datastore. In this paper, we present an efficient and extensible kNN-MT
framework, knn-seq, for researchers and developers that is carefully designed
to run efficiently, even with a billion-scale large datastore. knn-seq is
developed as a plug-in on fairseq and easy to switch models and kNN indexes.
Experimental results show that our implemented kNN-MT achieves a comparable
gain to the original kNN-MT, and the billion-scale datastore construction took
2.21 hours in the WMT'19 German-to-English translation task. We publish our
knn-seq as an MIT-licensed open-source project and the code is available on
https://github.com/naist-nlp/knn-seq . The demo video is available on
https://youtu.be/zTDzEOq80m0 .
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12353" title="Abstract">arXiv:2310.12353</a> [<a href="/pdf/2310.12353" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Networkwide Traffic State Forecasting Using Exogenous Information: A  Multi-Dimensional Graph Attention-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+S">Syed Islam</a>, 
<a href="/search/cs?searchtype=author&query=Filipovska%2C+M">Monika Filipovska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Transportation Research Board Annual Meeting 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Traffic state forecasting is crucial for traffic management and control
strategies, as well as user- and system-level decision making in the
transportation network. While traffic forecasting has been approached with a
variety of techniques over the last couple of decades, most approaches simply
rely on endogenous traffic variables for state prediction, despite the evidence
that exogenous factors can significantly impact traffic conditions. This paper
proposes a multi-dimensional spatio-temporal graph attention-based traffic
prediction approach (M-STGAT), which predicts traffic based on past
observations of speed, along with lane closure events, temperature, and
visibility across the transportation network. The approach is based on a graph
attention network architecture, which also learns based on the structure of the
transportation network on which these variables are observed. Numerical
experiments are performed using traffic speed and lane closure data from the
California Department of Transportation (Caltrans) Performance Measurement
System (PeMS). The corresponding weather data were downloaded from the National
Oceanic and Atmospheric Administration (NOOA) Automated Surface Observing
Systems (ASOS). For comparison, the numerical experiments implement three
alternative models which do not allow for the multi-dimensional input. The
M-STGAT is shown to outperform the three alternative models, when performing
tests using our primary data set for prediction with a 30-, 45-, and 60-minute
prediction horizon, in terms of three error measures: Mean Absolute Error
(MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE).
However, the model's transferability can vary for different transfer data sets
and this aspect may require further investigation.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12357" title="Abstract">arXiv:2310.12357</a> [<a href="/pdf/2310.12357" title="Download PDF">pdf</a>, <a href="/format/2310.12357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Code Analysis: Do LLMs Really Do Their Job?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chongzhou Fang</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+N">Ning Miao</a>, 
<a href="/search/cs?searchtype=author&query=Srivastav%2C+S">Shaurya Srivastav</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Ruijie Fang</a>, 
<a href="/search/cs?searchtype=author&query=Asmita%2C+A">Asmita Asmita</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+R">Ryan Tsang</a>, 
<a href="/search/cs?searchtype=author&query=Nazari%2C+N">Najmeh Nazari</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Homayoun%2C+H">Houman Homayoun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated significant potential in the
realm of natural language understanding and programming code processing tasks.
Their capacity to comprehend and generate human-like code has spurred research
into harnessing LLMs for code analysis purposes. However, the existing body of
literature falls short in delivering a systematic evaluation and assessment of
LLMs' effectiveness in code analysis, particularly in the context of obfuscated
code.
<br />This paper seeks to bridge this gap by offering a comprehensive evaluation of
LLMs' capabilities in performing code analysis tasks. Additionally, it presents
real-world case studies that employ LLMs for the analysis of malicious code.
Our findings indicate that LLMs can indeed serve as valuable tools for
automating code analysis, albeit with certain limitations. Through meticulous
exploration, this research contributes to a deeper understanding of the
potential and constraints associated with utilizing LLMs in code analysis,
paving the way for enhanced applications in this critical domain.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12359" title="Abstract">arXiv:2310.12359</a> [<a href="/pdf/2310.12359" title="Download PDF">pdf</a>, <a href="/format/2310.12359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable  Speed Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Quinones-Grueiro%2C+M">Marcos Quinones-Grueiro</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanbing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Barbour%2C+W">William Barbour</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+G">Gautam Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Work%2C+D">Daniel Work</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Variable speed limit (VSL) control is a promising traffic management strategy
for enhancing safety and mobility. This work introduces MARVEL, a multi-agent
reinforcement learning (MARL) framework for implementing large-scale VSL
control on freeway corridors using only commonly available data. The agents
learn through a reward structure that incorporates adaptability to traffic
conditions, safety, and mobility; enabling coordination among the agents. The
proposed framework scales to cover corridors with many gantries thanks to a
parameter sharing among all VSL agents. The agents are trained in a
microsimulation environment based on a short freeway stretch with 8 gantries
spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near
Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no
control scenario and enhances traffic mobility by 14.6% compared to a
state-of-the-practice algorithm that has been deployed on I-24. An
explainability analysis is undertaken to explore the learned policy under
different traffic conditions and the results provide insights into the
decision-making process of agents. Finally, we test the policy learned from the
simulation-based experiments on real input data from I-24 to illustrate the
potential deployment capability of the learned policy.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12360" title="Abstract">arXiv:2310.12360</a> [<a href="/pdf/2310.12360" title="Download PDF">pdf</a>, <a href="/format/2310.12360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRI: Graph-based Relative Isomorphism of Word Embedding Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+M+A">Muhammad Asif Ali</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jianbin Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automated construction of bilingual dictionaries using monolingual embedding
spaces is a core challenge in machine translation. The end performance of these
dictionaries relies upon the geometric similarity of individual spaces, i.e.,
their degree of isomorphism. Existing attempts aimed at controlling the
relative isomorphism of different spaces fail to incorporate the impact of
semantically related words in the training objective. To address this, we
propose GRI that combines the distributional training objectives with attentive
graph convolutions to unanimously consider the impact of semantically similar
words required to define/compute the relative isomorphism of multiple spaces.
Experimental evaluation shows that GRI outperforms the existing research by
improving the average P@1 by a relative score of up to 63.6%. We release the
codes for GRI at https://github.com/asif6827/GRI.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12361" title="Abstract">arXiv:2310.12361</a> [<a href="/pdf/2310.12361" title="Download PDF">pdf</a>, <a href="/format/2310.12361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieve-Cluster-Summarize: An Alternative to End-to-End Training for  Query-specific Article Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lennox%2C+C">Connor Lennox</a>, 
<a href="/search/cs?searchtype=author&query=Kashyapi%2C+S">Sumanta Kashyapi</a>, 
<a href="/search/cs?searchtype=author&query=Dietz%2C+L">Laura Dietz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Query-specific article generation is the task of, given a search query,
generate a single article that gives an overview of the topic. We envision such
articles as an alternative to presenting a ranking of search results. While
generative Large Language Models (LLMs) like chatGPT also address this task,
they are known to hallucinate new information, their models are secret, hard to
analyze and control. Some generative LLMs provide supporting references, yet
these are often unrelated to the generated content. As an alternative, we
propose to study article generation systems that integrate document retrieval,
query-specific clustering, and summarization. By design, such models can
provide actual citations as provenance for their generated text. In particular,
we contribute an evaluation framework that allows to separately trains and
evaluate each of these three components before combining them into one system.
We experimentally demonstrate that a system comprised of the best-performing
individual components also obtains the best F-1 overall system quality.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12362" title="Abstract">arXiv:2310.12362</a> [<a href="/pdf/2310.12362" title="Download PDF">pdf</a>, <a href="/format/2310.12362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruisi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+S+S">Shehzeen Samarah Hussain</a>, 
<a href="/search/cs?searchtype=author&query=Neekhara%2C+P">Paarth Neekhara</a>, 
<a href="/search/cs?searchtype=author&query=Koushanfar%2C+F">Farinaz Koushanfar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We present REMARK-LLM, a novel efficient, and robust watermarking framework
designed for texts generated by large language models (LLMs). Synthesizing
human-like content using LLMs necessitates vast computational resources and
extensive datasets, encapsulating critical intellectual property (IP). However,
the generated content is prone to malicious exploitation, including spamming
and plagiarism. To address the challenges, REMARK-LLM proposes three new
components: (i) a learning-based message encoding module to infuse binary
signatures into LLM-generated texts; (ii) a reparameterization module to
transform the dense distributions from the message encoding to the sparse
distribution of the watermarked textual tokens; (iii) a decoding module
dedicated for signature extraction; Furthermore, we introduce an optimized beam
search algorithm to guarantee the coherence and consistency of the generated
content. REMARK-LLM is rigorously trained to encourage the preservation of
semantic integrity in watermarked content, while ensuring effective watermark
retrieval. Extensive evaluations on multiple unseen datasets highlight
REMARK-LLM proficiency and transferability in inserting 2 times more signature
bits into the same texts when compared to prior art, all while maintaining
semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against
a spectrum of watermark detection and removal attacks.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12364" title="Abstract">arXiv:2310.12364</a> [<a href="/pdf/2310.12364" title="Download PDF">pdf</a>, <a href="/format/2310.12364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster randomized partial trace estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+T">Tyler Chen</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+R">Robert Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+K">Kevin Li</a>, 
<a href="/search/math?searchtype=author&query=Nzeuton%2C+S">Skai Nzeuton</a>, 
<a href="/search/math?searchtype=author&query=Pan%2C+Y">Yilu Pan</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yixin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Strongly Correlated Electrons (cond-mat.str-el); Quantum Physics (quant-ph)

</div>
<p class="mathjax">We develop randomized matrix-free algorithms for estimating partial traces.
Our algorithm improves on the typicality-based approach used in [T. Chen and
Y-C. Cheng, Numerical computation of the equilibrium-reduced density matrix for
strongly coupled open quantum systems, J. Chem. Phys. 157, 064106 (2022)] by
deflating important subspaces (e.g. corresponding to the low-energy
eigenstates) explicitly. This results in a significant variance reduction for
matrices with quickly decaying singular values. We then apply our algorithm to
study the thermodynamics of several Heisenberg spin systems, particularly the
entanglement spectrum and ergotropy.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12369" title="Abstract">arXiv:2310.12369</a> [<a href="/pdf/2310.12369" title="Download PDF">pdf</a>, <a href="/format/2310.12369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Identifying Points of Semantic Shift Across Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+H+W">Hyung Wook Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+M">Mat Kelly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In 17th International Conference on Metadata and Semantics Research, October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The semantics used for particular terms in an academic field organically
evolve over time. Tracking this evolution through inspection of published
literature has either been from the perspective of Linguistic scholars or has
concentrated the focus of term evolution within a single domain of study. In
this paper, we performed a case study to identify semantic evolution across
different domains and identify examples of inter-domain semantic shifts. We
initially used keywords as the basis of our search and executed an iterative
process of following citations to find the initial mention of the concepts in
the field. We found that a select set of keywords like ``semaphore'',
``polymorphism'', and ``ontology'' were mentioned within Computer Science
literature and tracked the seminal study that borrowed those terms from
original fields by citations. We marked these events as semantic evolution
points. Through this manual investigation method, we can identify term
evolution across different academic fields. This study reports our initial
findings that will seed future automated and computational methods of
incorporating concepts from additional academic fields.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12370" title="Abstract">arXiv:2310.12370</a> [<a href="/pdf/2310.12370" title="Download PDF">pdf</a>, <a href="/format/2310.12370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-Regret Learning in Bilateral Trade via Global Budget Balance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernasconi%2C+M">Martino Bernasconi</a>, 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+M">Matteo Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Celli%2C+A">Andrea Celli</a>, 
<a href="/search/cs?searchtype=author&query=Fusco%2C+F">Federico Fusco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bilateral trade revolves around the challenge of facilitating transactions
between two strategic agents -- a seller and a buyer -- both of whom have a
private valuations for the item. We study the online version of the problem, in
which at each time step a new seller and buyer arrive. The learner's task is to
set a price for each agent, without any knowledge about their valuations. The
sequence of sellers and buyers is chosen by an oblivious adversary. In this
setting, known negative results rule out the possibility of designing
algorithms with sublinear regret when the learner has to guarantee budget
balance for each iteration. In this paper, we introduce the notion of global
budget balance, which requires the agent to be budget balance only over the
entire time horizon. By requiring global budget balance, we provide the first
no-regret algorithms for bilateral trade with adversarial inputs under various
feedback models. First, we show that in the full-feedback model the learner can
guarantee $\tilde{O}(\sqrt{T})$ regret against the best fixed prices in
hindsight, which is order-wise optimal. Then, in the case of partial feedback
models, we provide an algorithm guaranteeing a $\tilde{O}(T^{3/4})$ regret
upper bound with one-bit feedback, which we complement with a nearly-matching
lower bound. Finally, we investigate how these results vary when measuring
regret using an alternative benchmark.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12375" title="Abstract">arXiv:2310.12375</a> [<a href="/pdf/2310.12375" title="Download PDF">pdf</a>, <a href="/format/2310.12375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearly Optimal Bounds for Sample-Based Testing and Learning of  $k$-Monotone Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Black%2C+H">Hadley Black</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study monotonicity testing of functions $f \colon \{0,1\}^d \to \{0,1\}$
using sample-based algorithms, which are only allowed to observe the value of
$f$ on points drawn independently from the uniform distribution. A classic
result by Bshouty-Tamon (J. ACM 1996) proved that monotone functions can be
learned with $\exp(O(\min\{\frac{1}{\varepsilon}\sqrt{d},d\}))$ samples and it
is not hard to show that this bound extends to testing. Prior to our work the
only lower bound for this problem was $\Omega(\sqrt{\exp(d)/\varepsilon})$ in
the small $\varepsilon$ parameter regime, when $\varepsilon = O(d^{-3/2})$, due
to Goldreich-Goldwasser-Lehman-Ron-Samorodnitsky (Combinatorica 2000). Thus,
the sample complexity of monotonicity testing was wide open for $\varepsilon
\gg d^{-3/2}$. We resolve this question, obtaining a tight lower bound of
$\exp(\Omega(\min\{\frac{1}{\varepsilon}\sqrt{d},d\}))$ for all $\varepsilon$
at most a sufficiently small constant. In fact, we prove a much more general
result, showing that the sample complexity of $k$-monotonicity testing and
learning for functions $f \colon \{0,1\}^d \to [r]$ is
$\exp(\Theta(\min\{\frac{rk}{\varepsilon}\sqrt{d},d\}))$. For testing with
one-sided error we show that the sample complexity is $\exp(\Theta(d))$.
<br />Beyond the hypercube, we prove nearly tight bounds (up to polylog factors of
$d,k,r,1/\varepsilon$ in the exponent) of
$\exp(\widetilde{\Theta}(\min\{\frac{rk}{\varepsilon}\sqrt{d},d\}))$ on the
sample complexity of testing and learning measurable $k$-monotone functions $f
\colon \mathbb{R}^d \to [r]$ under product distributions. Our upper bound
improves upon the previous bound of
$\exp(\widetilde{O}(\min\{\frac{k}{\varepsilon^2}\sqrt{d},d\}))$ by
Harms-Yoshida (ICALP 2022) for Boolean functions ($r=2$).
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12379" title="Abstract">arXiv:2310.12379</a> [<a href="/pdf/2310.12379" title="Download PDF">pdf</a>, <a href="/format/2310.12379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Hard Analogy Questions with Relation Embedding Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nitesh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Schockaert%2C+S">Steven Schockaert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modelling how concepts are related is a central topic in Lexical Semantics. A
common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to
model the relation between two concepts as a set of paths. However, KGs are
limited to a fixed set of relation types, and they are incomplete and often
noisy. Another strategy is to distill relation embeddings from a fine-tuned
language model. However, this is less suitable for words that are only
indirectly related and it does not readily allow us to incorporate structured
domain knowledge. In this paper, we aim to combine the best of both worlds. We
model relations as paths but associate their edges with relation embeddings.
The paths are obtained by first identifying suitable intermediate words and
then selecting those words for which informative relation embeddings can be
obtained. We empirically show that our proposed representations are useful for
solving hard analogy questions.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12381" title="Abstract">arXiv:2310.12381</a> [<a href="/pdf/2310.12381" title="Download PDF">pdf</a>, <a href="/format/2310.12381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VDKMS: Vehicular Decentralized Key Management System for Cellular  Vehicular-to-Everything Networks, A Blockchain-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deek%2C+F+P">Fadi P. Deek</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guiling Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, accepted by IEEE Globecom 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The rapid development of intelligent transportation systems and connected
vehicles has highlighted the need for secure and efficient key management
systems (KMS). In this paper, we introduce VDKMS (Vehicular Decentralized Key
Management System), a novel Decentralized Key Management System designed
specifically as an infrastructure for Cellular Vehicular-to-Everything (V2X)
networks, utilizing a blockchain-based approach. The proposed VDKMS addresses
the challenges of secure communication, privacy preservation, and efficient key
management in V2X scenarios. It integrates blockchain technology,
Self-Sovereign Identity (SSI) principles, and Decentralized Identifiers (DIDs)
to enable secure and trustworthy V2X applications among vehicles,
infrastructures, and networks. We first provide a comprehensive overview of the
system architecture, components, protocols, and workflows, covering aspects
such as provisioning, registration, verification, and authorization. We then
present a detailed performance evaluation, discussing the security properties
and compatibility of the proposed solution, as well as a security analysis.
Finally, we present potential applications in the vehicular ecosystem that can
leverage the advantages of our approach.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12386" title="Abstract">arXiv:2310.12386</a> [<a href="/pdf/2310.12386" title="Download PDF">pdf</a>, <a href="/format/2310.12386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Learning and Planning in Cognitive Hierarchies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hengst%2C+B">Bernhard Hengst</a>, 
<a href="/search/cs?searchtype=author&query=Pagnucco%2C+M">Maurice Pagnucco</a>, 
<a href="/search/cs?searchtype=author&query=Rajaratnam%2C+D">David Rajaratnam</a>, 
<a href="/search/cs?searchtype=author&query=Sammut%2C+C">Claude Sammut</a>, 
<a href="/search/cs?searchtype=author&query=Thielscher%2C+M">Michael Thielscher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Complex robot behaviour typically requires the integration of multiple
robotic and Artificial Intelligence (AI) techniques and components. Integrating
such disparate components into a coherent system, while also ensuring global
properties and behaviours, is a significant challenge for cognitive robotics.
Using a formal framework to model the interactions between components can be an
important step in dealing with this challenge. In this paper we extend an
existing formal framework [Clark et al., 2016] to model complex integrated
reasoning behaviours of robotic systems; from symbolic planning through to
online learning of policies and transition systems. Furthermore the new
framework allows for a more flexible modelling of the interactions between
different reasoning components.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12387" title="Abstract">arXiv:2310.12387</a> [<a href="/pdf/2310.12387" title="Download PDF">pdf</a>, <a href="/format/2310.12387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Solve Climate Sensor Placement Problems with a Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+V">Victoria Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bryce Chen</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+J">Jochen Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The optimal placement of sensors for environmental monitoring and disaster
management is a challenging problem due to its NP-hard nature. Traditional
methods for sensor placement involve exact, approximation, or heuristic
approaches, with the latter being the most widely used. However, heuristic
methods are limited by expert intuition and experience. Deep learning (DL) has
emerged as a promising approach for generating heuristic algorithms
automatically. In this paper, we introduce a novel sensor placement approach
focused on learning improvement heuristics using deep reinforcement learning
(RL) methods. Our approach leverages an RL formulation for learning improvement
heuristics, driven by an actor-critic algorithm for training the policy
network. We compare our method with several state-of-the-art approaches by
conducting comprehensive experiments, demonstrating the effectiveness and
superiority of our proposed approach in producing high-quality solutions. Our
work presents a promising direction for applying advanced DL and RL techniques
to challenging climate sensor placement problems.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12389" title="Abstract">arXiv:2310.12389</a> [<a href="/pdf/2310.12389" title="Download PDF">pdf</a>, <a href="/format/2310.12389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Computing for MIMO Beam Selection Problem: Model and Optical  Experimental Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chengkang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+S">Shuai Hou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chunfeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jingwei Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiaqi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chongyu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+K">Kai Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Globecom 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Massive multiple-input multiple-output (MIMO) has gained widespread
popularity in recent years due to its ability to increase data rates, improve
signal quality, and provide better coverage in challenging environments. In
this paper, we investigate the MIMO beam selection (MBS) problem, which is
proven to be NP-hard and computationally intractable. To deal with this
problem, quantum computing that can provide faster and more efficient solutions
to large-scale combinatorial optimization is considered. MBS is formulated in a
quadratic unbounded binary optimization form and solved with Coherent Ising
Machine (CIM) physical machine. We compare the performance of our solution with
two classic heuristics, simulated annealing and Tabu search. The results
demonstrate an average performance improvement by a factor of 261.23 and 20.6,
respectively, which shows that CIM-based solution performs significantly better
in terms of selecting the optimal subset of beams. This work shows great
promise for practical 5G operation and promotes the application of quantum
computing in solving computationally hard problems in communication.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12393" title="Abstract">arXiv:2310.12393</a> [<a href="/pdf/2310.12393" title="Download PDF">pdf</a>, <a href="/format/2310.12393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Techniques for Video Instance Segmentation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenhao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chang-Tsun Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yongjian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+C+P">Chee Peng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Creighton%2C+D">Douglas Creighton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video instance segmentation, also known as multi-object tracking and
segmentation, is an emerging computer vision research area introduced in 2019,
aiming at detecting, segmenting, and tracking instances in videos
simultaneously. By tackling the video instance segmentation tasks through
effective analysis and utilization of visual information in videos, a range of
computer vision-enabled applications (e.g., human action recognition, medical
image processing, autonomous vehicle navigation, surveillance, etc) can be
implemented. As deep-learning techniques take a dominant role in various
computer vision areas, a plethora of deep-learning-based video instance
segmentation schemes have been proposed. This survey offers a multifaceted view
of deep-learning schemes for video instance segmentation, covering various
architectural paradigms, along with comparisons of functional performance,
model complexity, and computational overheads. In addition to the common
architectural designs, auxiliary techniques for improving the performance of
deep-learning models for video instance segmentation are compiled and
discussed. Finally, we discuss a range of major challenges and directions for
further investigations to help advance this promising research field.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12394" title="Abstract">arXiv:2310.12394</a> [<a href="/pdf/2310.12394" title="Download PDF">pdf</a>, <a href="/ps/2310.12394" title="Download PostScript">ps</a>, <a href="/format/2310.12394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An $O(\log n)$-Competitive Posted-Price Algorithm for Online Matching on  the Line
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arndt%2C+S">Stephen Arndt</a>, 
<a href="/search/cs?searchtype=author&query=Ascher%2C+J">Josh Ascher</a>, 
<a href="/search/cs?searchtype=author&query=Pruhs%2C+K">Kirk Pruhs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Motivated by demand-responsive parking pricing systems, we consider
posted-price algorithms for the online metric matching problem. We give an
$O(\log n)$-competitive posted-price randomized algorithm in the case that the
metric space is a line. In particular, in this setting we show how to implement
the ubiquitous guess-and-double technique using prices.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12395" title="Abstract">arXiv:2310.12395</a> [<a href="/pdf/2310.12395" title="Download PDF">pdf</a>, <a href="/format/2310.12395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-Form Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scarvelis%2C+C">Christopher Scarvelis</a>, 
<a href="/search/cs?searchtype=author&query=de+Oc%C3%A1riz+Borde%2C+H+S">Haitz S&#xe1;ez de Oc&#xe1;riz Borde</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+J">Justin Solomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Score-based generative models (SGMs) sample from a target distribution by
iteratively transforming noise using the score function of the perturbed
target. For any finite training set, this score function can be evaluated in
closed form, but the resulting SGM memorizes its training data and does not
generate novel samples. In practice, one approximates the score by training a
neural network via score-matching. The error in this approximation promotes
generalization, but neural SGMs are costly to train and sample, and the
effective regularization this error provides is not well-understood
theoretically. In this work, we instead explicitly smooth the closed-form score
to obtain an SGM that generates novel samples without training. We analyze our
model and propose an efficient nearest-neighbor-based estimator of its score
function. Using this estimator, our method achieves sampling times competitive
with neural SGMs while running on consumer-grade CPUs.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12397" title="Abstract">arXiv:2310.12397</a> [<a href="/pdf/2310.12397" title="Download PDF">pdf</a>, <a href="/format/2310.12397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4 Doesn&#x27;t Know It&#x27;s Wrong: An Analysis of Iterative Prompting for  Reasoning Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stechly%2C+K">Kaya Stechly</a>, 
<a href="/search/cs?searchtype=author&query=Marquez%2C+M">Matthew Marquez</a>, 
<a href="/search/cs?searchtype=author&query=Kambhampati%2C+S">Subbarao Kambhampati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">There has been considerable divergence of opinion on the reasoning abilities
of Large Language Models (LLMs). While the initial optimism that reasoning
might emerge automatically with scale has been tempered thanks to a slew of
counterexamples, a wide spread belief in their iterative self-critique
capabilities persists. In this paper, we set out to systematically investigate
the effectiveness of iterative prompting of LLMs in the context of Graph
Coloring, a canonical NP-complete reasoning problem that is related to
propositional satisfiability as well as practical problems like scheduling and
allocation. We present a principled empirical study of the performance of GPT4
in solving graph coloring instances or verifying the correctness of candidate
colorings. In iterative modes, we experiment with the model critiquing its own
answers and an external correct reasoner verifying proposed solutions. In both
cases, we analyze whether the content of the criticisms actually affects bottom
line performance. The study seems to indicate that (i) LLMs are bad at solving
graph coloring instances (ii) they are no better at verifying a solution--and
thus are not effective in iterative modes with LLMs critiquing LLM-generated
solutions (iii) the correctness and content of the criticisms--whether by LLMs
or external solvers--seems largely irrelevant to the performance of iterative
prompting. We show that the observed increase in effectiveness is largely due
to the correct solution being fortuitously present in the top-k completions of
the prompt (and being recognized as such by an external verifier). Our results
thus call into question claims about the self-critiquing capabilities of state
of the art LLMs.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12401" title="Abstract">arXiv:2310.12401</a> [<a href="/pdf/2310.12401" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Hierarchical Anonymization Framework over Encrypted  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jing Jia</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+K">Kenta Saito</a>, 
<a href="/search/cs?searchtype=author&query=Nishi%2C+H">Hiroaki Nishi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 12 figures, submitted to IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS and under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Smart cities, which can monitor the real world and provide smart services in
a variety of fields, have improved people's living standards as urbanization
has accelerated. However, there are security and privacy concerns because smart
city applications collect large amounts of privacy-sensitive information from
people and their social circles. Anonymization, which generalizes data and
reduces data uniqueness is an important step in preserving the privacy of
sensitive information. However, anonymization methods frequently require large
datasets and rely on untrusted third parties to collect and manage data,
particularly in a cloud environment. In this case, private data leakage remains
a critical issue, discouraging users from sharing their data and impeding the
advancement of smart city services. This problem can be solved if the
computational entity can perform the anonymization process without obtaining
the original plain text. This study proposed a hierarchical k-anonymization
framework using homomorphic encryption and secret sharing composed of two types
of domains. Different computing methods are selected flexibly, and two domains
are connected hierarchically to obtain higher-level anonymization results in an
efficient manner. The experimental results show that connecting two domains can
accelerate the anonymization process, indicating that the proposed secure
hierarchical architecture is practical and efficient.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12403" title="Abstract">arXiv:2310.12403</a> [<a href="/pdf/2310.12403" title="Download PDF">pdf</a>, <a href="/format/2310.12403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Minibatching in Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balin%2C+M+F">Muhammed Fatih Balin</a>, 
<a href="/search/cs?searchtype=author&query=LaSalle%2C+D">Dominique LaSalle</a>, 
<a href="/search/cs?searchtype=author&query=%C3%87ataly%C3%BCrek%2C+%C3%9C+V">&#xdc;mit V. &#xc7;ataly&#xfc;rek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Significant computational resources are required to train Graph Neural
Networks (GNNs) at a large scale, and the process is highly data-intensive. One
of the most effective ways to reduce resource requirements is minibatch
training coupled with graph sampling. GNNs have the unique property that items
in a minibatch have overlapping data. However, the commonly implemented
Independent Minibatching approach assigns each Processing Element (PE) its own
minibatch to process, leading to duplicated computations and input data access
across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which
is the main bottleneck limiting scaling. To reduce the effects of NEP in the
multi-PE setting, we propose a new approach called Cooperative Minibatching.
Our approach capitalizes on the fact that the size of the sampled subgraph is a
concave function of the batch size, leading to significant reductions in the
amount of work per seed vertex as batch sizes increase. Hence, it is favorable
for processors equipped with a fast interconnect to work on a large minibatch
together as a single larger processor, instead of working on separate smaller
minibatches, even though global batch size is identical. We also show how to
take advantage of the same phenomenon in serial execution by generating
dependent consecutive minibatches. Our experimental evaluations show up to 4x
bandwidth savings for fetching vertex embeddings, by simply increasing this
dependency without harming model convergence. Combining our proposed
approaches, we achieve up to 64% speedup over Independent Minibatching on
single-node multi-GPU systems.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12404" title="Abstract">arXiv:2310.12404</a> [<a href="/pdf/2310.12404" title="Download PDF">pdf</a>, <a href="/format/2310.12404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Maezawa%2C+A">Akira Maezawa</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gus Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yamamoto%2C+K">Kazuhiko Yamamoto</a>, 
<a href="/search/cs?searchtype=author&query=Dixon%2C+S">Simon Dixon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source code and demo video are available at \url{<a href="https://sites.google.com/view/loop-copilot">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Creating music is iterative, requiring varied methods at each stage. However,
existing AI music systems fall short in orchestrating multiple subsystems for
diverse needs. To address this gap, we introduce Loop Copilot, a novel system
that enables users to generate and iteratively refine music through an
interactive, multi-round dialogue interface. The system uses a large language
model to interpret user intentions and select appropriate AI models for task
execution. Each backend model is specialized for a specific task, and their
outputs are aggregated to meet the user's requirements. To ensure musical
coherence, essential attributes are maintained in a centralized table. We
evaluate the effectiveness of the proposed system through semi-structured
interviews and questionnaires, highlighting its utility not only in
facilitating music creation but also its potential for broader applications.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12406" title="Abstract">arXiv:2310.12406</a> [<a href="/pdf/2310.12406" title="Download PDF">pdf</a>, <a href="/format/2310.12406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FinEntity: Entity-level Sentiment Classification for Financial Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yixuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A+H">Allen H Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tam%2C+A">Andy Tam</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J+Z">Justin Z Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP'23 Main Conference Short Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the financial domain, conducting entity-level sentiment analysis is
crucial for accurately assessing the sentiment directed toward a specific
financial entity. To our knowledge, no publicly available dataset currently
exists for this purpose. In this work, we introduce an entity-level sentiment
classification dataset, called \textbf{FinEntity}, that annotates financial
entity spans and their sentiment (positive, neutral, and negative) in financial
news. We document the dataset construction process in the paper. Additionally,
we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on
entity-level sentiment classification. In a case study, we demonstrate the
practical utility of using FinEntity in monitoring cryptocurrency markets. The
data and code of FinEntity is available at
\url{https://github.com/yixuantt/FinEntity}
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12407" title="Abstract">arXiv:2310.12407</a> [<a href="/pdf/2310.12407" title="Download PDF">pdf</a>, <a href="/format/2310.12407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification-Aided Robust Multiple Target Tracking Using Neural  Enhanced Message Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xianglong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zengfu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Quan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Tao Yun</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+H">Hua Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">We address the challenge of tracking an unknown number of targets in strong
clutter environments using measurements from a radar sensor. Leveraging the
range-Doppler spectra information, we identify the measurement classes, which
serve as additional information to enhance clutter rejection and data
association, thus bolstering the robustness of target tracking. We first
introduce a novel neural enhanced message passing approach, where the beliefs
obtained by the unified message passing are fed into the neural network as
additional information. The output beliefs are then utilized to refine the
original beliefs. Then, we propose a classification-aided robust multiple
target tracking algorithm, employing the neural enhanced message passing
technique. This algorithm is comprised of three modules: a message-passing
module, a neural network module, and a Dempster-Shafer module. The
message-passing module is used to represent the statistical model by the factor
graph and infers target kinematic states, visibility states, and data
associations based on the spatial measurement information. The neural network
module is employed to extract features from range-Doppler spectra and derive
beliefs on whether a measurement is target-generated or clutter-generated. The
Dempster-Shafer module is used to fuse the beliefs obtained from both the
factor graph and the neural network. As a result, our proposed algorithm adopts
a model-and-data-driven framework, effectively enhancing clutter suppression
and data association, leading to significant improvements in multiple target
tracking performance. We validate the effectiveness of our approach using both
simulated and real data scenarios, demonstrating its capability to handle
challenging tracking scenarios in practical radar applications.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12408" title="Abstract">arXiv:2310.12408</a> [<a href="/pdf/2310.12408" title="Download PDF">pdf</a>, <a href="/format/2310.12408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Guarantees for Neural Networks via Gradient Feature Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhenmei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Junyi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingyu Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, 71 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural networks have achieved remarkable empirical performance, while the
current theoretical analysis is not adequate for understanding their success,
e.g., the Neural Tangent Kernel approach fails to capture their key feature
learning ability, while recent analyses on feature learning are typically
problem-specific. This work proposes a unified analysis framework for two-layer
networks trained by gradient descent. The framework is centered around the
principle of feature learning from gradients, and its effectiveness is
demonstrated by applications in several prototypical problems, such as mixtures
of Gaussians and parity functions. The framework also sheds light on
interesting network learning phenomena such as feature learning beyond kernels
and the lottery ticket hypothesis.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12409" title="Abstract">arXiv:2310.12409</a> [<a href="/pdf/2310.12409" title="Download PDF">pdf</a>, <a href="/format/2310.12409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-Aware Impedance Control for Human-Robot Collaborative Task with  Online Object Parameter Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinseong Park</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+Y">Yong-Sik Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sanghyun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, for associated video, see <a href="https://youtu.be/bGH6GAFlRgA?si=wXj_SRzEE8BYoV2a">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Physical human-robot interactions (pHRIs) can improve robot autonomy and
reduce physical demands on humans. In this paper, we consider a collaborative
task with a considerably long object and no prior knowledge of the object's
parameters. An integrated control framework with an online object parameter
estimator and a Cartesian object-aware impedance controller is proposed to
realize complicated scenarios. During the transportation task, the object
parameters are estimated online while a robot and human lift an object. The
perturbation motion is incorporated into the null space of the desired
trajectory to enhance the estimator accuracy. An object-aware impedance
controller is designed using the real-time estimation results to effectively
transmit the intended human motion to the robot through the object.
Experimental demonstrations of collaborative tasks, including object
transportation and assembly tasks, are implemented to show the effectiveness of
our proposed method.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12411" title="Abstract">arXiv:2310.12411</a> [<a href="/pdf/2310.12411" title="Download PDF">pdf</a>, <a href="/format/2310.12411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Multi-IMU Calibration Using Visual-Inertial Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartzer%2C+J">Jacob Hartzer</a>, 
<a href="/search/cs?searchtype=author&query=Saripalli%2C+S">Srikanth Saripalli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This work presents a centralized multi-IMU filter framework with online
intrinsic and extrinsic calibration for unsynchronized inertial measurement
units that is robust against changes in calibration parameters. The novel
EKF-based method estimates the positional and rotational offsets of the system
of sensors as well as their intrinsic biases without the use of rigid body
geometric constraints. Additionally, the filter is flexible in the total number
of sensors used while leveraging the commonly used MSCKF framework for camera
measurements. The filter framework has been validated using Monte Carlo
simulation as well as experimentally. In both simulations and experiments,
using multiple IMU measurement streams within the proposed filter framework
outperforms the use of a single IMU in a filter prediction step while also
producing consistent and accurate estimates of initial calibration errors.
Compared to current state-of-the-art optimizers, the filter produces similar
intrinsic and extrinsic calibration parameters for each sensor. Finally, an
open source repository has been provided at
https://github.com/unmannedlab/ekf-cal containing both the online estimator and
the simulation used for testing and evaluation.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12415" title="Abstract">arXiv:2310.12415</a> [<a href="/pdf/2310.12415" title="Download PDF">pdf</a>, <a href="/format/2310.12415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SURE: A Visualized Failure Indexing Approach using Program Memory  Spectrum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yi Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaoyuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Songqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Quanming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruizhi Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Due to the limitation "The abstract field cannot be longer than 1,920 characters", the abstract here is shorter than that in the PDF file
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Failure indexing is a longstanding crux in software testing and debugging,
the goal of which is to automatically divide failures (e.g., failed test cases)
into distinct groups according to the culprit root causes, as such multiple
faults in a faulty program can be handled independently and simultaneously.
This community has long been plagued by two challenges: 1) The effectiveness of
division is still far from promising. Existing techniques only employ a limited
source of run-time data (e.g., code coverage) to be failure proximity, which
typically delivers unsatisfactory results. 2) The outcome can be hardly
comprehensible. A developer who receives the failure indexing result does not
know why all failures should be divided the way they are. This leads to
difficulties for developers to be convinced by the result, which in turn
affects the adoption of the results. To tackle these challenges, in this paper,
we propose SURE, a viSUalized failuRe indExing approach using the program
memory spectrum. We first collect the run-time memory information at preset
breakpoints during the execution of failed test cases, and transform it into
human-friendly images (called program memory spectrum, PMS). Then, any pair of
PMS images that serve as proxies for two failures is fed to a trained Siamese
convolutional neural network, to predict the likelihood of them being triggered
by the same fault. Results demonstrate the effectiveness of SURE: It achieves
101.20% and 41.38% improvements in faults number estimation, as well as 105.20%
and 35.53% improvements in clustering, compared with the state-of-the-art
technique in this field, in simulated and real-world environments,
respectively. Moreover, we carry out a human study to quantitatively evaluate
the comprehensibility of PMS, revealing that this novel type of representation
can help developers better comprehend failure indexing results.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12417" title="Abstract">arXiv:2310.12417</a> [<a href="/pdf/2310.12417" title="Download PDF">pdf</a>, <a href="/format/2310.12417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metadata for Scientific Experiment Reporting: A Case Study in  Metal-Organic Frameworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xintong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Langlois%2C+K">Kyle Langlois</a>, 
<a href="/search/cs?searchtype=author&query=Furst%2C+J">Jacob Furst</a>, 
<a href="/search/cs?searchtype=author&query=McClellan%2C+S">Scott McClellan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaohua Hu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+Y">Yuan An</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Gualdr%C3%B3n%2C+D+A">Diego A. G&#xf3;mez-Gualdr&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Uribe-Romo%2C+F+J">Fernando J. Uribe-Romo</a>, 
<a href="/search/cs?searchtype=author&query=Greenberg%2C+J">Jane Greenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 17th International Conference on Metadata and Semantics Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">Research methods and procedures are core aspects of the research process.
Metadata focused on these components is critical to supporting the FAIR
principles, particularly reproducibility. The research reported on in this
paper presents a methodological framework for metadata documentation supporting
the reproducibility of research producing Metal Organic Frameworks (MOFs). The
MOF case study involved natural language processing to extract key synthesis
experiment information from a corpus of research literature. Following, a
classification activity was performed by domain experts to identify
entity-relation pairs. Results include: 1) a research framework for metadata
design, 2) a metadata schema that includes nine entities and two relationships
for reporting MOF synthesis experiments, and 3) a growing database of MOF
synthesis reports structured by our metadata scheme. The metadata schema is
intended to support discovery and reproducibility of metal-organic framework
research and the FAIR principles. The paper provides background information,
identifies the research goals and objectives, research design, results, a
discussion, and the conclusion.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12418" title="Abstract">arXiv:2310.12418</a> [<a href="/pdf/2310.12418" title="Download PDF">pdf</a>, <a href="/format/2310.12418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Shifted and The Overlooked: A Task-oriented Investigation of  User-GPT Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Siru Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuohang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+M">Ming Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yizhu Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Iter%2C+D">Dan Iter</a>, 
<a href="/search/cs?searchtype=author&query=Pryzant%2C+R">Reid Pryzant</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenguang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent progress in Large Language Models (LLMs) has produced models that
exhibit remarkable performance across a variety of NLP tasks. However, it
remains unclear whether the existing focus of NLP research accurately captures
the genuine requirements of human users. This paper provides a comprehensive
analysis of the divergence between current NLP research and the needs of
real-world NLP applications via a large-scale collection of user-GPT
conversations. We analyze a large-scale collection of real user queries to GPT.
We compare these queries against existing NLP benchmark tasks and identify a
significant gap between the tasks that users frequently request from LLMs and
the tasks that are commonly studied in academic research. For example, we find
that tasks such as ``design'' and ``planning'' are prevalent in user
interactions but are largely neglected or different from traditional NLP
benchmarks. We investigate these overlooked tasks, dissect the practical
challenges they pose, and provide insights toward a roadmap to make LLMs better
aligned with user needs.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12419" title="Abstract">arXiv:2310.12419</a> [<a href="/pdf/2310.12419" title="Download PDF">pdf</a>, <a href="/format/2310.12419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Unbiased Multiple-Target Fuzzing with Path Diversity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rong%2C+H">Huanyao Rong</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+W">Wei You</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaofeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+T">Tianhao Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this paper, we propose a novel directed fuzzing solution named AFLRun,
which features target path-diversity metric and unbiased energy assignment.
Firstly, we develop a new coverage metric by maintaining extra virgin map for
each covered target to track the coverage status of seeds that hit the target.
This approach enables the storage of waypoints into the corpus that hit a
target through interesting path, thus enriching the path diversity for each
target. Additionally, we propose a corpus-level energy assignment strategy that
guarantees fairness for each target. AFLRun starts with uniform target weight
and propagates this weight to seeds to get a desired seed weight distribution.
By assigning energy to each seed in the corpus according to such desired
distribution, a precise and unbiased energy assignment can be achieved.
<br />We built a prototype system and assessed its performance using a standard
benchmark and several extensively fuzzed real-world applications. The
evaluation results demonstrate that AFLRun outperforms state-of-the-art fuzzers
in terms of vulnerability detection, both in quantity and speed. Moreover,
AFLRun uncovers 29 previously unidentified vulnerabilities, including 8 CVEs,
across four distinct programs.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12421" title="Abstract">arXiv:2310.12421</a> [<a href="/pdf/2310.12421" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting and Mitigating Algorithmic Bias in Binary Classification using  Causal Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hui%2C+W">Wendy Hui</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+W+K">Wai Kwong Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures, 6 tables, R-script in appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This paper proposes the use of causal modeling to detect and mitigate
algorithmic bias. We provide a brief description of causal modeling and a
general overview of our approach. We then use the Adult dataset, which is
available for download from the UC Irvine Machine Learning Repository, to
develop (1) a prediction model, which is treated as a black box, and (2) a
causal model for bias mitigation. In this paper, we focus on gender bias and
the problem of binary classification. We show that gender bias in the
prediction model is statistically significant at the 0.05 level. We demonstrate
the effectiveness of the causal model in mitigating gender bias by
cross-validation. Furthermore, we show that the overall classification accuracy
is improved slightly. Our novel approach is intuitive, easy-to-use, and can be
implemented using existing statistical software tools such as "lavaan" in R.
Hence, it enhances explainability and promotes trust.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12422" title="Abstract">arXiv:2310.12422</a> [<a href="/pdf/2310.12422" title="Download PDF">pdf</a>, <a href="/format/2310.12422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low rank approximation method for perturbed linear systems with  applications to elliptic type stochastic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhu%2C+Y">Yujun Zhu</a>, 
<a href="/search/math?searchtype=author&query=Ming%2C+J">Ju Ming</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+J">Jie Zhu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Zhongming Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper, we propose a low rank approximation method for efficiently
solving stochastic partial differential equations. Specifically, our method
utilizes a novel low rank approximation of the stiffness matrices, which can
significantly reduce the computational load and storage requirements associated
with matrix inversion without losing accuracy. To demonstrate the versatility
and applicability of our method, we apply it to address two crucial uncertainty
quantification problems: stochastic elliptic equations and optimal control
problems governed by stochastic elliptic PDE constraints. Based on varying
dimension reduction ratios, our algorithm exhibits the capability to yield a
high precision numerical solution for stochastic partial differential
equations, or provides a rough representation of the exact solutions as a
pre-processing phase. Meanwhile, our algorithm for solving stochastic optimal
control problems allows a diverse range of gradient-based unconstrained
optimization methods, rendering it particularly appealing for computationally
intensive large-scale problems. Numerical experiments are conducted and the
results provide strong validation of the feasibility and effectiveness of our
algorithm.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12425" title="Abstract">arXiv:2310.12425</a> [<a href="/pdf/2310.12425" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Repair of Declarative Software Specifications in the Era of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+R">Md Rashedul Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+I">Iftekhar Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Bagheri%2C+H">Hamid Bagheri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 Pages with reference, 4 Tables, 2 Figures, 2 Listings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The growing adoption of declarative software specification languages, coupled
with their inherent difficulty in debugging, has underscored the need for
effective and automated repair techniques applicable to such languages.
Researchers have recently explored various methods to automatically repair
declarative software specifications, such as template-based repair,
feedback-driven iterative repair, and bounded exhaustive approaches. The latest
developments in large language models provide new opportunities for the
automatic repair of declarative specifications. In this study, we assess the
effectiveness of utilizing OpenAI's ChatGPT to repair software specifications
written in the Alloy declarative language. Unlike imperative languages,
specifications in Alloy are not executed but rather translated into logical
formulas and evaluated using backend constraint solvers to identify
specification instances and counterexamples to assertions. Our evaluation
focuses on ChatGPT's ability to improve the correctness and completeness of
Alloy declarative specifications through automatic repairs. We analyze the
results produced by ChatGPT and compare them with those of leading automatic
Alloy repair methods. Our study revealed that while ChatGPT falls short in
comparison to existing techniques, it was able to successfully repair bugs that
no other technique could address. Our analysis also identified errors in
ChatGPT's generated repairs, including improper operator usage, type errors,
higher-order logic misuse, and relational arity mismatches. Additionally, we
observed instances of hallucinations in ChatGPT-generated repairs and
inconsistency in its results. Our study provides valuable insights for software
practitioners, researchers, and tool builders considering ChatGPT for
declarative specification repairs.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12426" title="Abstract">arXiv:2310.12426</a> [<a href="/pdf/2310.12426" title="Download PDF">pdf</a>, <a href="/format/2310.12426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nathani%2C+D">Deepak Nathani</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">David Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Main Conference, Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language Models (LMs) have shown impressive performance in various natural
language tasks. However, when it comes to natural language reasoning, LMs still
face challenges such as hallucination, generating incorrect intermediate
reasoning steps, and making mathematical errors. Recent research has focused on
enhancing LMs through self-improvement using feedback. Nevertheless, existing
approaches relying on a single generic feedback source fail to address the
diverse error types found in LM-generated reasoning chains. In this work, we
propose Multi-Aspect Feedback, an iterative refinement framework that
integrates multiple feedback modules, including frozen LMs and external tools,
each focusing on a specific error category. Our experimental results
demonstrate the efficacy of our approach to addressing several errors in the
LM-generated reasoning chain and thus improving the overall performance of an
LM in several reasoning tasks. We see a relative improvement of up to 20% in
Mathematical Reasoning and up to 18% in Logical Entailment.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12429" title="Abstract">arXiv:2310.12429</a> [<a href="/pdf/2310.12429" title="Download PDF">pdf</a>, <a href="/format/2310.12429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surface Assisted High-Speed Train  Communications: Coverage Performance Analysis and Placement Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changzhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ruisi He</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+B">Bo Ai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Meilin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhangfeng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gongpu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zhangdui Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 figures, accepted by IEEE Transactions on Vehicular Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable intelligent surface (RIS) emerges as an efficient and
promising technology for the next wireless generation networks and has
attracted a lot of attention owing to the capability of extending wireless
coverage by reflecting signals toward targeted receivers. In this paper, we
consider a RIS-assisted high-speed train (HST) communication system to enhance
wireless coverage and improve coverage probability. First, coverage performance
of the downlink single-input-single-output system is investigated, and the
closed-form expression of coverage probability is derived. Moreover, travel
distance maximization problem is formulated to facilitate RIS discrete phase
design and RIS placement optimization, which is subject to coverage probability
constraint. Simulation results validate that better coverage performance and
higher travel distance can be achieved with deployment of RIS. The impacts of
some key system parameters including transmission power, signal-to-noise ratio
threshold, number of RIS elements, number of RIS quantization bits, horizontal
distance between base station and RIS, and speed of HST on system performance
are investigated. In addition, it is found that RIS can well improve coverage
probability with limited power consumption for HST communications.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12430" title="Abstract">arXiv:2310.12430</a> [<a href="/pdf/2310.12430" title="Download PDF">pdf</a>, <a href="/format/2310.12430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocXChain: A Powerful Open-Source Toolchain for Document Parsing and  Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+C">Cong Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In this report, we introduce DocXChain, a powerful open-source toolchain for
document parsing, which is designed and developed to automatically convert the
rich information embodied in unstructured documents, such as text, tables and
charts, into structured representations that are readable and manipulable by
machines. Specifically, basic capabilities, including text detection, text
recognition, table structure recognition and layout analysis, are provided.
Upon these basic capabilities, we also build a set of fully functional
pipelines for document parsing, i.e., general text reading, table parsing, and
document structurization, to drive various applications related to documents in
real-world scenarios. Moreover, DocXChain is concise, modularized and flexible,
such that it can be readily integrated with existing tools, libraries or models
(such as LangChain and ChatGPT), to construct more powerful systems that can
accomplish more complicated and challenging tasks. The code of DocXChain is
publicly available
at:~\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Applications/DocXChain}
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12431" title="Abstract">arXiv:2310.12431</a> [<a href="/pdf/2310.12431" title="Download PDF">pdf</a>, <a href="/format/2310.12431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Anything Meets Universal Adversarial Perturbation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongshen Han</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">As Segment Anything Model (SAM) becomes a popular foundation model in
computer vision, its adversarial robustness has become a concern that cannot be
ignored. This works investigates whether it is possible to attack SAM with
image-agnostic Universal Adversarial Perturbation (UAP). In other words, we
seek a single perturbation that can fool the SAM to predict invalid masks for
most (if not all) images. We demonstrate convetional image-centric attack
framework is effective for image-independent attacks but fails for universal
adversarial attack. To this end, we propose a novel perturbation-centric
framework that results in a UAP generation method based on self-supervised
contrastive learning (CL), where the UAP is set to the anchor sample and the
positive sample is augmented from the UAP. The representations of negative
samples are obtained from the image encoder in advance and saved in a memory
bank. The effectiveness of our proposed CL-based UAP generation method is
validated by both quantitative and qualitative results. On top of the ablation
study to understand various components in our proposed method, we shed light on
the roles of positive and negative samples in making the generated UAP
effective for attacking SAM.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12432" title="Abstract">arXiv:2310.12432</a> [<a href="/pdf/2310.12432" title="Download PDF">pdf</a>, <a href="/format/2310.12432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAT: Closed-loop Adversarial Training for Safe End-to-End Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhenghao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Quanyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bolei Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7th Conference on Robot Learning (CoRL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Driving safety is a top priority for autonomous vehicles. Orthogonal to prior
work handling accident-prone traffic events by algorithm designs at the policy
level, we investigate a Closed-loop Adversarial Training (CAT) framework for
safe end-to-end driving in this paper through the lens of environment
augmentation. CAT aims to continuously improve the safety of driving agents by
training the agent on safety-critical scenarios that are dynamically generated
over time. A novel resampling technique is developed to turn log-replay
real-world driving scenarios into safety-critical ones via probabilistic
factorization, where the adversarial traffic generation is modeled as the
multiplication of standard motion prediction sub-problems. Consequently, CAT
can launch more efficient physical attacks compared to existing safety-critical
scenario generation methods and yields a significantly less computational cost
in the iterative learning pipeline. We incorporate CAT into the MetaDrive
simulator and validate our approach on hundreds of driving scenarios imported
from real-world driving datasets. Experimental results demonstrate that CAT can
effectively generate adversarial scenarios countering the agent being trained.
After training, the agent can achieve superior driving safety in both
log-replay and safety-critical traffic scenarios on the held-out test set. Code
and data are available at https://metadriverse.github.io/cat.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12433" title="Abstract">arXiv:2310.12433</a> [<a href="/pdf/2310.12433" title="Download PDF">pdf</a>, <a href="/format/2310.12433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial Crowdsourcing Task Allocation Scheme for Massive Data with  Spatial Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hongwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiuzhen Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minghui Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Spatial crowdsourcing (SC) engages large worker pools for location-based
tasks, attracting growing research interest. However, prior SC task allocation
approaches exhibit limitations in computational efficiency, balanced matching,
and participation incentives. To address these challenges, we propose a
graph-based allocation framework optimized for massive heterogeneous spatial
data. The framework first clusters similar tasks and workers separately to
reduce allocation scale. Next, it constructs novel non-crossing graph
structures to model balanced adjacencies between unevenly distributed tasks and
workers. Based on the graphs, a bidirectional worker-task matching scheme is
designed to produce allocations optimized for mutual interests. Extensive
experiments on real-world datasets analyze the performance under various
parameter settings.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12439" title="Abstract">arXiv:2310.12439</a> [<a href="/pdf/2310.12439" title="Download PDF">pdf</a>, <a href="/format/2310.12439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Hongwei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian Lou</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhan Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code will be released on: <a href="https://github.com/grasses/PoisonPrompt">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prompts have significantly improved the performance of pretrained Large
Language Models (LLMs) on various downstream tasks recently, making them
increasingly indispensable for a diverse range of LLM application scenarios.
However, the backdoor vulnerability, a serious security threat that can
maliciously alter the victim model's normal predictions, has not been
sufficiently explored for prompt-based LLMs. In this paper, we present
POISONPROMPT, a novel backdoor attack capable of successfully compromising both
hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and
robustness of POISONPROMPT through extensive experiments on three popular
prompt methods, using six datasets and three widely used LLMs. Our findings
highlight the potential security threats posed by backdoor attacks on
prompt-based LLMs and emphasize the need for further research in this area.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12440" title="Abstract">arXiv:2310.12440</a> [<a href="/pdf/2310.12440" title="Download PDF">pdf</a>, <a href="/ps/2310.12440" title="Download PostScript">ps</a>, <a href="/format/2310.12440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Evaluation of Evolutionary Algorithms for Analog Integrated  Circuit Design Optimisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rashid%2C+R">Ria Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Raghunath%2C+G">Gopavaram Raghunath</a>, 
<a href="/search/cs?searchtype=author&query=Badugu%2C+V">Vasant Badugu</a>, 
<a href="/search/cs?searchtype=author&query=Nambath%2C+N">Nandakumar Nambath</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Microelectronics Journal, 2023, 105983, ISSN 0026-2692
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">An automated sizing approach for analog circuits using evolutionary
algorithms is presented in this paper. A targeted search of the search space
has been implemented using a particle generation function and a repair-bounds
function that has resulted in faster convergence to the optimal solution. The
algorithms are tuned and modified to converge to a better optimal solution with
less standard deviation for multiple runs compared to standard versions.
Modified versions of the artificial bee colony optimisation algorithm, genetic
algorithm, grey wolf optimisation algorithm, and particle swarm optimisation
algorithm are tested and compared for the optimal sizing of two operational
amplifier topologies. An extensive performance evaluation of all the modified
algorithms showed that the modifications have resulted in consistent
performance with improved convergence for all the algorithms. The
implementation of parallel computation in the algorithms has reduced run time.
Among the considered algorithms, the modified artificial bee colony
optimisation algorithm gave the most optimal solution with consistent results
across multiple runs.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12441" title="Abstract">arXiv:2310.12441</a> [<a href="/pdf/2310.12441" title="Download PDF">pdf</a>, <a href="/ps/2310.12441" title="Download PostScript">ps</a>, <a href="/format/2310.12441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Plaintext Functional Bootstrapping in FHE with Small Bootstrapping  Keys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dengfa Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongbo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,under review of some journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Differential Geometry (math.DG)

</div>
<p class="mathjax">Functional bootstrapping is a core technique in Fully Homomorphic Encryption
(FHE). For large plaintext, to evaluate a general function homomorphically over
a ciphertext, in the FHEW/TFHE approach, since the function in look-up table
form is encoded in the coefficients of a test polynomial, the degree of the
polynomial must be high enough to hold the entire table. This increases the
bootstrapping time complexity and memory cost, as the size of bootstrapping
keys and keyswitching keys need to be large accordingly. In this paper, we
propose to encode the look-up table of any function in a polynomial vector,
whose coefficients can hold more data. The corresponding representation of the
additive group Zq used in the RGSW-based bootstrapping is the group of monic
monomial permutation matrices, which integrates the permutation matrix
representation used by Alperin-Sheriff and Peikert in 2014, and the monic
monomial representation used in the FHEW/TFHE scheme. We make comprehensive
investigation of the new representation, and propose a new bootstrapping
algorithm based on it. The new algorithm has the prominent benefit of small
bootstrapping key size and small key-switching key size, which leads to
polynomial factor improvement in key size, in addition to constant factor
improvement in run-time cost.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12442" title="Abstract">arXiv:2310.12442</a> [<a href="/pdf/2310.12442" title="Download PDF">pdf</a>, <a href="/format/2310.12442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Long-Range Transformers: You Need to Attend More, but Not  Necessarily at Every Layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ram%2C+D">Dhananjay Ram</a>, 
<a href="/search/cs?searchtype=author&query=Hawkins%2C+C">Cole Hawkins</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+S">Sheng Zha</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023 Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Pretrained transformer models have demonstrated remarkable performance across
various natural language processing tasks. These models leverage the attention
mechanism to capture long- and short-range dependencies in the sequence.
However, the (full) attention mechanism incurs high computational cost -
quadratic in the sequence length, which is not affordable in tasks with long
sequences, e.g., inputs with 8k tokens. Although sparse attention can be used
to improve computational efficiency, as suggested in existing work, it has
limited modeling capacity and often fails to capture complicated dependencies
in long sequences. To tackle this challenge, we propose MASFormer, an
easy-to-implement transformer variant with Mixed Attention Spans. Specifically,
MASFormer is equipped with full attention to capture long-range dependencies,
but only at a small number of layers. For the remaining layers, MASformer only
employs sparse attention to capture short-range dependencies. Our experiments
on natural language modeling and generation tasks show that a decoder-only
MASFormer model of 1.3B parameters can achieve competitive performance to
vanilla transformers with full attention while significantly reducing
computational cost (up to 75%). Additionally, we investigate the effectiveness
of continual training with long sequence data and how sequence length impacts
downstream generation performance, which may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12443" title="Abstract">arXiv:2310.12443</a> [<a href="/pdf/2310.12443" title="Download PDF">pdf</a>, <a href="/format/2310.12443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy  Searcher
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinpeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qikai Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, under peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The advent of Large Language Models (LLMs) has shown the potential to improve
relevance and provide direct answers in web searches. However, challenges arise
in validating the reliability of generated results and the credibility of
contributing sources, due to the limitations of traditional information
retrieval algorithms and the LLM hallucination problem. Aiming to create a
"PageRank" for the LLM era, we strive to transform LLM into a relevant,
responsible, and trustworthy searcher. We propose a novel generative retrieval
framework leveraging the knowledge of LLMs to foster a direct link between
queries and online sources. This framework consists of three core modules:
Generator, Validator, and Optimizer, each focusing on generating trustworthy
online sources, verifying source reliability, and refining unreliable sources,
respectively. Extensive experiments and evaluations highlight our method's
superior relevance, responsibility, and trustfulness against various SOTA
methods.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12444" title="Abstract">arXiv:2310.12444</a> [<a href="/pdf/2310.12444" title="Download PDF">pdf</a>, <a href="/format/2310.12444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Sparse Retrieval for Few-shot Entity Linking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Entity linking aims to link ambiguous mentions to their corresponding
entities in a knowledge base. One of the key challenges comes from insufficient
labeled data for specific domains. Although dense retrievers have achieved
excellent performance on several benchmarks, their performance decreases
significantly when only a limited amount of in-domain labeled data is
available. In such few-shot setting, we revisit the sparse retrieval method,
and propose an ELECTRA-based keyword extractor to denoise the mention context
and construct a better query expression. For training the extractor, we propose
a distant supervision method to automatically generate training data based on
overlapping tokens between mention contexts and entity descriptions.
Experimental results on the ZESHEL dataset demonstrate that the proposed method
outperforms state-of-the-art models by a significant margin across all test
domains, showing the effectiveness of keyword-enhanced sparse retrieval.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12446" title="Abstract">arXiv:2310.12446</a> [<a href="/pdf/2310.12446" title="Download PDF">pdf</a>, <a href="/format/2310.12446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Electromagnetic Information Theory Improve Wireless Systems? A  Channel Estimation Example
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jieao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiaofeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhongzhichao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+L">Linglong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+T+J">Tie Jun Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Electromagnetic information theory (EIT) is an emerging interdisciplinary subject, aiming at providing a unified analytical framework for wireless systems as well as guiding practical system design. This paper answers the question: "How can we improve wireless communication systems via EIT"?
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Electromagnetic information theory (EIT) is an emerging interdisciplinary
subject that integrates classical Maxwell electromagnetics and Shannon
information theory. The goal of EIT is to uncover the information transmission
mechanisms from an electromagnetic (EM) perspective in wireless systems.
Existing works on EIT are mainly focused on the analysis of degrees-of-freedom
(DoF), system capacity, and characteristics of the electromagnetic channel.
However, these works do not clarify how EIT can improve wireless communication
systems. To answer this question, in this paper, we provide a novel
demonstration of the application of EIT. By integrating EM knowledge into the
classical MMSE channel estimator, we observe for the first time that EIT is
capable of improving the channel estimation performace. Specifically, the EM
knowledge is first encoded into a spatio-temporal correlation function (STCF),
which we term as the EM kernel. This EM kernel plays the role of side
information to the channel estimator. Since the EM kernel takes the form of
Gaussian processes (GP), we propose the EIT-based Gaussian process regression
(EIT-GPR) to derive the channel estimations. In addition, since the EM kernel
allows parameter tuning, we propose EM kernel learning to fit the EM kernel to
channel observations. Simulation results show that the application of EIT to
the channel estimator enables it to outperform traditional isotropic MMSE
algorithm, thus proving the practical values of EIT.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12450" title="Abstract">arXiv:2310.12450</a> [<a href="/pdf/2310.12450" title="Download PDF">pdf</a>, <a href="/format/2310.12450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Read-and-Select Framework for Zero-shot Entity Linking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yulin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Baotian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Zero-shot entity linking (EL) aims at aligning entity mentions to unseen
entities to challenge the generalization ability. Previous methods largely
focus on the candidate retrieval stage and ignore the essential candidate
ranking stage, which disambiguates among entities and makes the final linking
prediction. In this paper, we propose a read-and-select (ReS) framework by
modeling the main components of entity disambiguation, i.e., mention-entity
matching and cross-entity comparison. First, for each candidate, the reading
module leverages mention context to output mention-aware entity
representations, enabling mention-entity matching. Then, in the selecting
module, we frame the choice of candidates as a sequence labeling problem, and
all candidate representations are fused together to enable cross-entity
comparison. Our method achieves the state-of-the-art performance on the
established zero-shot EL dataset ZESHEL with a 2.55\% micro-average accuracy
gain, with no need for laborious multi-phase pre-training used in most of the
previous work, showing the effectiveness of both mention-entity and
cross-entity interaction.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12451" title="Abstract">arXiv:2310.12451</a> [<a href="/pdf/2310.12451" title="Download PDF">pdf</a>, <a href="/format/2310.12451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MTS-LOF: Medical Time-Series Representation Learning via  Occlusion-Invariant Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huayu Li</a>, 
<a href="/search/cs?searchtype=author&query=Carreon-Rascon%2C+A+S">Ana S. Carreon-Rascon</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiwen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+G">Geng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Medical time series data are indispensable in healthcare, providing critical
insights for disease diagnosis, treatment planning, and patient management. The
exponential growth in data complexity, driven by advanced sensor technologies,
has presented challenges related to data labeling. Self-supervised learning
(SSL) has emerged as a transformative approach to address these challenges,
eliminating the need for extensive human annotation. In this study, we
introduce a novel framework for Medical Time Series Representation Learning,
known as MTS-LOF. MTS-LOF leverages the strengths of contrastive learning and
Masked Autoencoder (MAE) methods, offering a unique approach to representation
learning for medical time series data. By combining these techniques, MTS-LOF
enhances the potential of healthcare applications by providing more
sophisticated, context-rich representations. Additionally, MTS-LOF employs a
multi-masking strategy to facilitate occlusion-invariant feature learning. This
approach allows the model to create multiple views of the data by masking
portions of it. By minimizing the discrepancy between the representations of
these masked patches and the fully visible patches, MTS-LOF learns to capture
rich contextual information within medical time series datasets. The results of
experiments conducted on diverse medical time series datasets demonstrate the
superiority of MTS-LOF over other methods. These findings hold promise for
significantly enhancing healthcare applications by improving representation
learning. Furthermore, our work delves into the integration of joint-embedding
SSL and MAE techniques, shedding light on the intricate interplay between
temporal and structural dependencies in healthcare data. This understanding is
crucial, as it allows us to grasp the complexities of healthcare data analysis.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12452" title="Abstract">arXiv:2310.12452</a> [<a href="/pdf/2310.12452" title="Download PDF">pdf</a>, <a href="/format/2310.12452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not Just Learning from Others but Relying on Yourself: A New Perspective  on Few-Shot Segmentation in Remote Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+H">Hanbo Bi</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yingchao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiyuan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yongqiang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+W">Wenhui Diao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xian Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to IEEE TGRS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Few-shot segmentation (FSS) is proposed to segment unknown class targets with
just a few annotated samples. Most current FSS methods follow the paradigm of
mining the semantics from the support images to guide the query image
segmentation. However, such a pattern of `learning from others' struggles to
handle the extreme intra-class variation, preventing FSS from being directly
generalized to remote sensing scenes. To bridge the gap of intra-class
variance, we develop a Dual-Mining network named DMNet for cross-image mining
and self-mining, meaning that it no longer focuses solely on support images but
pays more attention to the query image itself. Specifically, we propose a
Class-public Region Mining (CPRM) module to effectively suppress irrelevant
feature pollution by capturing the common semantics between the support-query
image pair. The Class-specific Region Mining (CSRM) module is then proposed to
continuously mine the class-specific semantics of the query image itself in a
`filtering' and `purifying' manner. In addition, to prevent the co-existence of
multiple classes in remote sensing scenes from exacerbating the collapse of FSS
generalization, we also propose a new Known-class Meta Suppressor (KMS) module
to suppress the activation of known-class objects in the sample. Extensive
experiments on the iSAID and LoveDA remote sensing datasets have demonstrated
that our method sets the state-of-the-art with a minimum number of model
parameters. Significantly, our model with the backbone of Resnet-50 achieves
the mIoU of 49.58% and 51.34% on iSAID under 1-shot and 5-shot settings,
outperforming the state-of-the-art method by 1.8% and 1.12%, respectively. The
code is publicly available at https://github.com/HanboBizl/DMNet.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12454" title="Abstract">arXiv:2310.12454</a> [<a href="/pdf/2310.12454" title="Download PDF">pdf</a>, <a href="/format/2310.12454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Construction of Effective Metrics for Understanding the  Mechanisms of Pretrained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">You Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jinhui Yin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuming Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pretrained language models are expected to effectively map input text to a
set of vectors while preserving the inherent relationships within the text.
Consequently, designing a white-box model to compute metrics that reflect the
presence of specific internal relations in these vectors has become a common
approach for post-hoc interpretability analysis of pretrained language models.
However, achieving interpretability in white-box models and ensuring the rigor
of metric computation becomes challenging when the source model lacks inherent
interpretability. Therefore, in this paper, we discuss striking a balance in
this trade-off and propose a novel line to constructing metrics for
understanding the mechanisms of pretrained language models. We have
specifically designed a family of metrics along this line of investigation, and
the model used to compute these metrics is referred to as the tree topological
probe. We conducted measurements on BERT-large by using these metrics. Based on
the experimental results, we propose a speculation regarding the working
mechanism of BERT-like pretrained language models, as well as a strategy for
enhancing fine-tuning performance by leveraging the topological probe to
improve specific submodules.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12455" title="Abstract">arXiv:2310.12455</a> [<a href="/pdf/2310.12455" title="Download PDF">pdf</a>, <a href="/format/2310.12455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto Search Indexer for End-to-End Document Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianchi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Minghui Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haizhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weiwei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Feng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Generative retrieval, which is a new advanced paradigm for document
retrieval, has recently attracted research interests, since it encodes all
documents into the model and directly generates the retrieved documents.
However, its power is still underutilized since it heavily relies on the
"preprocessed" document identifiers (docids), thus limiting its retrieval
performance and ability to retrieve new documents. In this paper, we propose a
novel fully end-to-end retrieval paradigm. It can not only end-to-end learn the
best docids for existing and new documents automatically via a semantic
indexing module, but also perform end-to-end document retrieval via an
encoder-decoder-based generative model, namely Auto Search Indexer (ASI).
Besides, we design a reparameterization mechanism to combine the above two
modules into a joint optimization framework. Extensive experimental results
demonstrate the superiority of our model over advanced baselines on both public
and industrial datasets and also verify the ability to deal with new documents.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12457" title="Abstract">arXiv:2310.12457</a> [<a href="/pdf/2310.12457" title="Download PDF">pdf</a>, <a href="/format/2310.12457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuseGNN: Interpretable and Convergent Graph Neural Network Layers at  Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haitian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Renjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhenkun Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Minjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wipf%2C+D">David Wipf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Among the many variants of graph neural network (GNN) architectures capable
of modeling data with cross-instance relations, an important subclass involves
layers designed such that the forward pass iteratively reduces a
graph-regularized energy function of interest. In this way, node embeddings
produced at the output layer dually serve as both predictive features for
solving downstream tasks (e.g., node classification) and energy function
minimizers that inherit desirable inductive biases and interpretability.
However, scaling GNN architectures constructed in this way remains challenging,
in part because the convergence of the forward pass may involve models with
considerable depth. To tackle this limitation, we propose a sampling-based
energy function and scalable GNN layers that iteratively reduce it, guided by
convergence guarantees in certain settings. We also instantiate a full GNN
architecture based on these designs, and the model achieves competitive
accuracy and scalability when applied to the largest publicly-available node
classification benchmark exceeding 1TB in size.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12459" title="Abstract">arXiv:2310.12459</a> [<a href="/pdf/2310.12459" title="Download PDF">pdf</a>, <a href="/format/2310.12459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affective Conversational Agents: Understanding Expectations and Personal  Influences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+J">Javier Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Suh%2C+J">Jina Suh</a>, 
<a href="/search/cs?searchtype=author&query=Amores%2C+J">Judith Amores</a>, 
<a href="/search/cs?searchtype=author&query=Rowan%2C+K">Kael Rowan</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+G">Gonzalo Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Czerwinski%2C+M">Mary Czerwinski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rise of AI conversational agents has broadened opportunities to enhance
human capabilities across various domains. As these agents become more
prevalent, it is crucial to investigate the impact of different affective
abilities on their performance and user experience. In this study, we surveyed
745 respondents to understand the expectations and preferences regarding
affective skills in various applications. Specifically, we assessed preferences
concerning AI agents that can perceive, respond to, and simulate emotions
across 32 distinct scenarios. Our results indicate a preference for scenarios
that involve human interaction, emotional support, and creative tasks, with
influences from factors such as emotional reappraisal and personality traits.
Overall, the desired affective skills in AI agents depend largely on the
application's context and nature, emphasizing the need for adaptability and
context-awareness in the design of affective AI conversational agents.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12461" title="Abstract">arXiv:2310.12461</a> [<a href="/pdf/2310.12461" title="Download PDF">pdf</a>, <a href="/format/2310.12461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Group Convolution: An Improved Group Convolution Based on  Approximability Estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Youngkyu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jongho Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chang-Ock Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The performance of neural networks has been significantly improved by
increasing the number of channels in convolutional layers. However, this
increase in performance comes with a higher computational cost, resulting in
numerous studies focused on reducing it. One promising approach to address this
issue is group convolution, which effectively reduces the computational cost by
grouping channels. However, to the best of our knowledge, there has been no
theoretical analysis on how well the group convolution approximates the
standard convolution. In this paper, we mathematically analyze the
approximation of the group convolution to the standard convolution with respect
to the number of groups. Furthermore, we propose a novel variant of the group
convolution called balanced group convolution, which shows a higher
approximation with a small additional computational cost. We provide
experimental results that validate our theoretical findings and demonstrate the
superior performance of the balanced group convolution over other variants of
group convolution.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12462" title="Abstract">arXiv:2310.12462</a> [<a href="/pdf/2310.12462" title="Download PDF">pdf</a>, <a href="/format/2310.12462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unmasking Transformers: A Theoretical Approach to Data Recovery via  Attention Weights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yichuan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shenghao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chiwun Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the realm of deep learning, transformers have emerged as a dominant
architecture, particularly in natural language processing tasks. However, with
their widespread adoption, concerns regarding the security and privacy of the
data processed by these models have arisen. In this paper, we address a pivotal
question: Can the data fed into transformers be recovered using their attention
weights and outputs? We introduce a theoretical framework to tackle this
problem. Specifically, we present an algorithm that aims to recover the input
data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top
\in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by
minimizing the loss function $L(X)$. This loss function captures the
discrepancy between the expected output and the actual output of the
transformer. Our findings have significant implications for the Localized
Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's
design from a security and privacy perspective. This work underscores the
importance of understanding and safeguarding the internal workings of
transformers to ensure the confidentiality of processed data.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12464" title="Abstract">arXiv:2310.12464</a> [<a href="/pdf/2310.12464" title="Download PDF">pdf</a>, <a href="/format/2310.12464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lidar Panoptic Segmentation and Tracking without Bells and Whistles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwalla%2C+A">Abhinav Agarwalla</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuhua Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ziglar%2C+J">Jason Ziglar</a>, 
<a href="/search/cs?searchtype=author&query=Ferroni%2C+F">Francesco Ferroni</a>, 
<a href="/search/cs?searchtype=author&query=Leal-Taix%C3%A9%2C+L">Laura Leal-Taix&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Hays%2C+J">James Hays</a>, 
<a href="/search/cs?searchtype=author&query=O%C5%A1ep%2C+A">Aljo&#x161;a O&#x161;ep</a>, 
<a href="/search/cs?searchtype=author&query=Ramanan%2C+D">Deva Ramanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IROS 2023. Code at <a href="https://github.com/abhinavagarwalla/most-lps">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">State-of-the-art lidar panoptic segmentation (LPS) methods follow bottom-up
segmentation-centric fashion wherein they build upon semantic segmentation
networks by utilizing clustering to obtain object instances. In this paper, we
re-think this approach and propose a surprisingly simple yet effective
detection-centric network for both LPS and tracking. Our network is modular by
design and optimized for all aspects of both the panoptic segmentation and
tracking task. One of the core components of our network is the object instance
detection branch, which we train using point-level (modal) annotations, as
available in segmentation-centric datasets. In the absence of amodal (cuboid)
annotations, we regress modal centroids and object extent using
trajectory-level supervision that provides information about object size, which
cannot be inferred from single scans due to occlusions and the sparse nature of
the lidar data. We obtain fine-grained instance segments by learning to
associate lidar points with detected centroids. We evaluate our method on
several 3D/4D LPS benchmarks and observe that our model establishes a new
state-of-the-art among open-sourced models, outperforming recent query-based
models.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12465" title="Abstract">arXiv:2310.12465</a> [<a href="/pdf/2310.12465" title="Download PDF">pdf</a>, <a href="/format/2310.12465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WeedCLR: Weed Contrastive Learning through Visual Representations with  Class-Optimized Loss in Long-Tailed Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saleh%2C+A">Alzayat Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Olsen%2C+A">Alex Olsen</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+J">Jake Wood</a>, 
<a href="/search/cs?searchtype=author&query=Philippa%2C+B">Bronson Philippa</a>, 
<a href="/search/cs?searchtype=author&query=Azghadi%2C+M+R">Mostafa Rahimi Azghadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 10 figures, 8 tables. Submitted to the Computers and Electronics in Agriculture journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image classification is a crucial task in modern weed management and crop
intervention technologies. However, the limited size, diversity, and balance of
existing weed datasets hinder the development of deep learning models for
generalizable weed identification. In addition, the expensive labelling
requirements of mainstream fully-supervised weed classifiers make them cost-
and time-prohibitive to deploy widely, for new weed species, and in
site-specific weed management. This paper proposes a novel method for Weed
Contrastive Learning through visual Representations (WeedCLR), that uses
class-optimized loss with Von Neumann Entropy of deep representation for weed
classification in long-tailed datasets. WeedCLR leverages self-supervised
learning to learn rich and robust visual features without any labels and
applies a class-optimized loss function to address the class imbalance problem
in long-tailed datasets. WeedCLR is evaluated on two public weed datasets:
CottonWeedID15, containing 15 weed species, and DeepWeeds, containing 8 weed
species. WeedCLR achieves an average accuracy improvement of 4.3\% on
CottonWeedID15 and 5.6\% on DeepWeeds over previous methods. It also
demonstrates better generalization ability and robustness to different
environmental conditions than existing methods without the need for expensive
and time-consuming human annotations. These significant improvements make
WeedCLR an effective tool for weed classification in long-tailed datasets and
allows for more rapid and widespread deployment of site-specific weed
management and crop intervention technologies.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12467" title="Abstract">arXiv:2310.12467</a> [<a href="/pdf/2310.12467" title="Download PDF">pdf</a>, <a href="/format/2310.12467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Learning for Inference in Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ishii%2C+E">Etsuko Ishii</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wilie%2C+B">Bryan Wilie</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ziwei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Lovenia%2C+H">Holy Lovenia</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+W">Willy Chung</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+P">Pascale Fung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Inference, especially those derived from inductive processes, is a crucial
component in our conversation to complement the information implicitly or
explicitly conveyed by a speaker. While recent large language models show
remarkable advances in inference tasks, their performance in inductive
reasoning, where not all information is present in the context, is far behind
deductive reasoning. In this paper, we analyze the behavior of the models based
on the task difficulty defined by the semantic information gap -- which
distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).
Our analysis reveals that the disparity in information between dialogue
contexts and desired inferences poses a significant challenge to the inductive
inference process. To mitigate this information gap, we investigate a
contrastive learning approach by feeding negative samples. Our experiments
suggest negative samples help models understand what is wrong and improve their
inference generations.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12470" title="Abstract">arXiv:2310.12470</a> [<a href="/pdf/2310.12470" title="Download PDF">pdf</a>, <a href="/format/2310.12470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecolorCloud: A Point Cloud Tool for Recoloring, Segmentation, and  Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez%2C+E+S">Esteban Segarra Martinez</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+R+P">Ryan P. McMahan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages, 9 figures, 1 table, To be submitted to the ACM MMSys 2024 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Point clouds are a 3D space representation of an environment that was
recorded with a high precision laser scanner. These scanners can suffer from
environmental interference such as surface shading, texturing, and reflections.
Because of this, point clouds may be contaminated with fake or incorrect
colors. Current open source or proprietary tools offer limited or no access to
correcting these visual errors automatically.
<br />RecolorCloud is a tool developed to resolve these color conflicts by
utilizing automated color recoloring. We offer the ability to deleting or
recoloring outlier points automatically with users only needing to specify
bounding box regions to effect colors. Results show a vast improvement of the
photo-realistic quality of large point clouds. Additionally, users can quickly
recolor a point cloud with set semantic segmentation colors.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12474" title="Abstract">arXiv:2310.12474</a> [<a href="/pdf/2310.12474" title="Download PDF">pdf</a>, <a href="/format/2310.12474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing High-Resolution 3D Generation through Pixel-wise Gradient  Clipping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zijie Pan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiachen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiatian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">High-resolution 3D object generation remains a challenging task primarily due
to the limited availability of comprehensive annotated training data. Recent
advancements have aimed to overcome this constraint by harnessing image
generative models, pretrained on extensive curated web datasets, using
knowledge transfer techniques like Score Distillation Sampling (SDS).
Efficiently addressing the requirements of high-resolution rendering often
necessitates the adoption of latent representation-based models, such as the
Latent Diffusion Model (LDM). In this framework, a significant challenge
arises: To compute gradients for individual image pixels, it is necessary to
backpropagate gradients from the designated latent space through the frozen
components of the image model, such as the VAE encoder used within LDM.
However, this gradient propagation pathway has never been optimized, remaining
uncontrolled during training. We find that the unregulated gradients adversely
affect the 3D model's capacity in acquiring texture-related information from
the image generative model, leading to poor quality appearance synthesis. To
address this overarching challenge, we propose an innovative operation termed
Pixel-wise Gradient Clipping (PGC) designed for seamless integration into
existing 3D generative models, thereby enhancing their synthesis quality.
Specifically, we control the magnitude of stochastic gradients by clipping the
pixel-wise gradients efficiently, while preserving crucial texture-related
gradient directions. Despite this simplicity and minimal extra cost, extensive
experiments demonstrate the efficacy of our PGC in enhancing the performance of
existing 3D generative models for high-resolution object rendering.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12480" title="Abstract">arXiv:2310.12480</a> [<a href="/pdf/2310.12480" title="Download PDF">pdf</a>, <a href="/format/2310.12480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRAPE-S: Near Real-Time Coalition Formation for Multiple Service  Collectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diehl%2C+G">Grace Diehl</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+J+A">Julie A. Adams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robotic collectives for military and disaster response applications require
coalition formation algorithms to partition robots into appropriate task teams.
Collectives' missions will often incorporate tasks that require multiple
high-level robot behaviors or services, which coalition formation must
accommodate. The highly dynamic and unstructured application domains also
necessitate that coalition formation algorithms produce near optimal solutions
(i.e., &gt;95% utility) in near real-time (i.e., &lt;5 minutes) with very large
collectives (i.e., hundreds of robots). No previous coalition formation
algorithm satisfies these requirements. An initial evaluation found that
traditional auction-based algorithms' runtimes are too long, even though the
centralized simulator incorporated ideal conditions unlikely to occur in
real-world deployments (i.e., synchronization across robots and perfect,
instantaneous communication). The hedonic game-based GRAPE algorithm can
produce solutions in near real-time, but cannot be applied to multiple service
collectives. This manuscript integrates GRAPE and a services model, producing
GRAPE-S and Pair-GRAPE-S. These algorithms and two auction baselines were
evaluated using a centralized simulator with up to 1000 robots, and via the
largest distributed coalition formation simulated evaluation to date, with up
to 500 robots. The evaluations demonstrate that auctions transfer poorly to
distributed collectives, resulting in excessive runtimes and low utility
solutions. GRAPE-S satisfies the target domains' coalition formation
requirements, producing near optimal solutions in near real-time, and
Pair-GRAPE-S more than satisfies the domain requirements, producing optimal
solutions in near real-time. GRAPE-S and Pair-GRAPE-S are the first algorithms
demonstrated to support near real-time coalition formation for very large,
distributed collectives with multiple services.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12481" title="Abstract">arXiv:2310.12481</a> [<a href="/pdf/2310.12481" title="Download PDF">pdf</a>, <a href="/format/2310.12481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jingyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+R">Ruyi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jen-tse Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we identify a cultural dominance issue within large language
models (LLMs) due to the predominant use of English data in model training
(e.g. ChatGPT). LLMs often provide inappropriate English-culture-related
answers that are not relevant to the expected culture when users ask in
non-English languages. To systematically evaluate the cultural dominance issue,
we build a benchmark that consists of both concrete (e.g. holidays and songs)
and abstract (e.g. values and opinions) cultural objects. Empirical results
show that the representative GPT models suffer from the culture dominance
problem, where GPT-4 is the most affected while text-davinci-003 suffers the
least from this problem. Our study emphasizes the need for critical examination
of cultural dominance and ethical consideration in their development and
deployment. We show two straightforward methods in model development (i.e.
pretraining on more diverse data) and deployment (e.g. culture-aware prompting)
can significantly mitigate the cultural dominance issue in LLMs.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12486" title="Abstract">arXiv:2310.12486</a> [<a href="/pdf/2310.12486" title="Download PDF">pdf</a>, <a href="/format/2310.12486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trapped acoustic waves and raindrops: high-order accurate integral  equation method for localized excitation of a periodic staircase
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Agocs%2C+F+J">Fruzsina J. Agocs</a>, 
<a href="/search/math?searchtype=author&query=Barnett%2C+A+H">Alex H. Barnett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 11 figures. Submitted to JCP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We present a high-order boundary integral equation (BIE) method for the
frequency-domain acoustic scattering of a point source by a singly-periodic,
infinite, corrugated boundary. We apply it to the accurate numerical study of
acoustic radiation in the neighborhood of a sound-hard two-dimensional
staircase modeled after the El Castillo pyramid. Such staircases support
trapped waves which travel along the surface and decay exponentially away from
it. We use the array scanning method (Floquet--Bloch transform) to recover the
scattered field as an integral over the family of quasiperiodic solutions
parameterized by their on-surface wavenumber. Each such BIE solution requires
the quasiperiodic Green's function, which we evaluate using an efficient
integral representation of lattice sum coefficients. We avoid the singularities
and branch cuts present in the array scanning integral by complex contour
deformation. For each frequency, this enables a solution accurate to around 10
digits in a couple of seconds. We propose a residue method to extract the
limiting powers carried by trapped modes far from the source. Finally, by
computing the trapped mode dispersion relation, we use a simple ray model to
explain an observed acoustic "raindrop" effect (chirp-like time-domain
response).
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12487" title="Abstract">arXiv:2310.12487</a> [<a href="/pdf/2310.12487" title="Download PDF">pdf</a>, <a href="/format/2310.12487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Operator Learning by Orthogonal Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zipeng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhongkai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bokai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhijie Deng</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural operators, as an efficient surrogate model for learning the solutions
of PDEs, have received extensive attention in the field of scientific machine
learning. Among them, attention-based neural operators have become one of the
mainstreams in related research. However, existing approaches overfit the
limited training data due to the considerable number of parameters in the
attention mechanism. To address this, we develop an orthogonal attention based
on the eigendecomposition of the kernel integral operator and the neural
approximation of eigenfunctions. The orthogonalization naturally poses a proper
regularization effect on the resulting neural operator, which aids in resisting
overfitting and boosting generalization. Experiments on six standard neural
operator benchmark datasets comprising both regular and irregular geometries
show that our method can outperform competing baselines with decent margins.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12489" title="Abstract">arXiv:2310.12489</a> [<a href="/pdf/2310.12489" title="Download PDF">pdf</a>, <a href="/format/2310.12489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI  Responses in Health Consultations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ojo%2C+O+E">Olumide E. Ojo</a>, 
<a href="/search/cs?searchtype=author&query=Adebanji%2C+O+O">Olaronke O. Adebanji</a>, 
<a href="/search/cs?searchtype=author&query=Gelbukh%2C+A">Alexander Gelbukh</a>, 
<a href="/search/cs?searchtype=author&query=Calvo%2C+H">Hiram Calvo</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+A">Anna Feldman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Zero-shot classification has enabled the classification of text into classes
that were not seen during training. In this paper, we investigate the
effectiveness of pre-trained language models to accurately classify responses
from Doctors and AI in health consultations through zero-shot learning. Our
study aims to determine whether these models can effectively detect if a text
originates from human or AI models without specific corpus training. For our
experiments, we collected responses from doctors to patient inquiries about
their health and posed the same question/response to AI models. Our findings
revealed that while pre-trained language models demonstrate a strong
understanding of language generally, they may require specific corpus training
or other techniques to achieve accurate classification of doctor- and
AI-generated text in healthcare consultations. As a baseline approach, this
study shows the limitations of relying solely on zero-shot classification in
medical classification tasks. This research lays the groundwork for further
research into the field of medical text classification, informing the
development of more effective approaches to accurately classify doctor- and
AI-generated text in health consultations.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12490" title="Abstract">arXiv:2310.12490</a> [<a href="/pdf/2310.12490" title="Download PDF">pdf</a>, <a href="/format/2310.12490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co$^2$PT: Mitigating Bias in Pre-trained Language Models through  Counterfactual Contrastive Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangjue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Ziwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhuoer Wang</a>, 
<a href="/search/cs?searchtype=author&query=Teleki%2C+M">Maria Teleki</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained Language Models are widely used in many important real-world
applications. However, recent studies show that these models can encode social
biases from large pre-training corpora and even amplify biases in downstream
applications. To address this challenge, we propose Co$^2$PT, an efficient and
effective debias-while-prompt tuning method for mitigating biases via
counterfactual contrastive prompt tuning on downstream tasks. Our experiments
conducted on three extrinsic bias benchmarks demonstrate the effectiveness of
Co$^2$PT on bias mitigation during the prompt tuning process and its
adaptability to existing upstream debiased language models. These findings
indicate the strength of Co$^2$PT and provide promising avenues for further
enhancement in bias mitigation on downstream tasks.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12491" title="Abstract">arXiv:2310.12491</a> [<a href="/pdf/2310.12491" title="Download PDF">pdf</a>, <a href="/format/2310.12491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hiding Access-pattern is Not Enough! Veil: A Storage and Communication  Efficient Volume-Hiding Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shanshan Han</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+V">Vishal Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Goodrich%2C+M">Michael Goodrich</a>, 
<a href="/search/cs?searchtype=author&query=Mehrotra%2C+S">Sharad Mehrotra</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shantanu Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">This paper addresses volume leakage (i.e., leakage of the number of records
in the answer set) when processing keyword queries in encrypted key-value (KV)
datasets. Volume leakage, coupled with prior knowledge about data distribution
and/or previously executed queries, can reveal both ciphertexts and current
user queries. We develop a solution to prevent volume leakage, entitled Veil,
that partitions the dataset by randomly mapping keys to a set of equi-sized
buckets. Veil provides a tunable mechanism for data owners to explore a
trade-off between storage and communication overheads. To make buckets
indistinguishable to the adversary, Veil uses a novel padding strategy that
allow buckets to overlap, reducing the need to add fake records. Both
theoretical and experimental results show Veil to significantly outperform
existing state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12494" title="Abstract">arXiv:2310.12494</a> [<a href="/pdf/2310.12494" title="Download PDF">pdf</a>, <a href="/format/2310.12494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDGym: Low-Code Reinforcement Learning Environments using System  Dynamics Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klu%2C+E">Emmanuel Klu</a>, 
<a href="/search/cs?searchtype=author&query=Sethi%2C+S">Sameer Sethi</a>, 
<a href="/search/cs?searchtype=author&query=Passey%2C+D">DJ Passey</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+D">Donald Martin Jr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Understanding the long-term impact of algorithmic interventions on society is
vital to achieving responsible AI. Traditional evaluation strategies often fall
short due to the complex, adaptive and dynamic nature of society. While
reinforcement learning (RL) can be a powerful approach for optimizing decisions
in dynamic settings, the difficulty of realistic environment design remains a
barrier to building robust agents that perform well in practical settings. To
address this issue we tap into the field of system dynamics (SD) as a
complementary method that incorporates collaborative simulation model
specification practices. We introduce SDGym, a low-code library built on the
OpenAI Gym framework which enables the generation of custom RL environments
based on SD simulation models. Through a feasibility study we validate that
well specified, rich RL environments can be generated from preexisting SD
models and a few lines of configuration code. We demonstrate the capabilities
of the SDGym environment using an SD model of the electric vehicle adoption
problem. We compare two SD simulators, PySD and BPTK-Py for parity, and train a
D4PG agent using the Acme framework to showcase learning and environment
interaction. Our preliminary findings underscore the dual potential of SD to
improve RL environment design and for RL to improve dynamic policy discovery
within SD models. By open-sourcing SDGym, the intent is to galvanize further
research and promote adoption across the SD and RL communities, thereby
catalyzing collaboration in this emerging interdisciplinary space.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12498" title="Abstract">arXiv:2310.12498</a> [<a href="/pdf/2310.12498" title="Download PDF">pdf</a>, <a href="/format/2310.12498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi Manhattan Wasserstein Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+E+U">Evan Unit Lim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The Quasi Manhattan Wasserstein Distance (QMWD) is a metric designed to
quantify the dissimilarity between two matrices by combining elements of the
Wasserstein Distance with specific transformations. It offers improved time and
space complexity compared to the Manhattan Wasserstein Distance (MWD) while
maintaining accuracy. QMWD is particularly advantageous for large datasets or
situations with limited computational resources. This article provides a
detailed explanation of QMWD, its computation, complexity analysis, and
comparisons with WD and MWD.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12505" title="Abstract">arXiv:2310.12505</a> [<a href="/pdf/2310.12505" title="Download PDF">pdf</a>, <a href="/format/2310.12505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attack Prompt Generation for Red Teaming and Defending Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+B">Boyi Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiangnan He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) are susceptible to red teaming attacks, which
can induce LLMs to generate harmful content. Previous research constructs
attack prompts via manual or automatic methods, which have their own
limitations on construction cost and quality. To address these issues, we
propose an integrated approach that combines manual and automatic methods to
economically generate high-quality attack prompts. Specifically, considering
the impressive capabilities of newly emerged LLMs, we propose an attack
framework to instruct LLMs to mimic human-generated prompts through in-context
learning. Furthermore, we propose a defense framework that fine-tunes victim
LLMs through iterative interactions with the attack framework to enhance their
safety against red teaming attacks. Extensive experiments on different LLMs
validate the effectiveness of our proposed attack and defense frameworks.
Additionally, we release a series of attack prompts datasets named SAP with
varying sizes, facilitating the safety evaluation and enhancement of more LLMs.
Our code and dataset is available on https://github.com/Aatrox103/SAP .
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12508" title="Abstract">arXiv:2310.12508</a> [<a href="/pdf/2310.12508" title="Download PDF">pdf</a>, <a href="/format/2310.12508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency  in Both Image Classification and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chongyu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Dennis Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With evolving data regulations, machine unlearning (MU) has become an
important tool for fostering trust and safety in today's AI models. However,
existing MU methods focusing on data and/or weight perspectives often grapple
with limitations in unlearning accuracy, stability, and cross-domain
applicability. To address these challenges, we introduce the concept of 'weight
saliency' in MU, drawing parallels with input saliency in model explanation.
This innovation directs MU's attention toward specific model weights rather
than the entire model, improving effectiveness and efficiency. The resultant
method that we call saliency unlearning (SalUn) narrows the performance gap
with 'exact' unlearning (model retraining from scratch after removing the
forgetting dataset). To the best of our knowledge, SalUn is the first
principled MU approach adaptable enough to effectively erase the influence of
forgetting data, classes, or concepts in both image classification and
generation. For example, SalUn yields a stability advantage in high-variance
random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on
the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from
generating harmful images, SalUn achieves nearly 100% unlearning accuracy,
outperforming current state-of-the-art baselines like Erased Stable Diffusion
and Forget-Me-Not.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12509" title="Abstract">arXiv:2310.12509</a> [<a href="/pdf/2310.12509" title="Download PDF">pdf</a>, <a href="/format/2310.12509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning for Leaf Disease Classification: Data, Techniques and  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jianping Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+S+N">Son N. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Sawyer%2C+S">Samantha Sawyer</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Saurabh Garg</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artificial Intelligence Review 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The growing demand for sustainable development brings a series of information
technologies to help agriculture production. Especially, the emergence of
machine learning applications, a branch of artificial intelligence, has shown
multiple breakthroughs which can enhance and revolutionize plant pathology
approaches. In recent years, machine learning has been adopted for leaf disease
classification in both academic research and industrial applications.
Therefore, it is enormously beneficial for researchers, engineers, managers,
and entrepreneurs to have a comprehensive view about the recent development of
machine learning technologies and applications for leaf disease detection. This
study will provide a survey in different aspects of the topic including data,
techniques, and applications. The paper will start with publicly available
datasets. After that, we summarize common machine learning techniques,
including traditional (shallow) learning, deep learning, and augmented
learning. Finally, we discuss related applications. This paper would provide
useful resources for future study and application of machine learning for smart
agriculture in general and leaf disease classification in particular.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12511" title="Abstract">arXiv:2310.12511</a> [<a href="/pdf/2310.12511" title="Download PDF">pdf</a>, <a href="/ps/2310.12511" title="Download PostScript">ps</a>, <a href="/format/2310.12511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The weight enumerator polynomials of the lifted codes of the projective  Solomon-Stiffler codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Minjia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Helleseth%2C+T">Tor Helleseth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript was first submitted on September 9, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Determining the weight distribution of a code is an old and fundamental topic
in coding theory that has been thoroughly studied. In 1977, Helleseth,
Kl{\o}ve, and Mykkeltveit presented a weight enumerator polynomial of the
lifted code over $\mathbb{F}_{q^\ell}$ of a $q$-ary linear code with
significant combinatorial properties, which can determine the support weight
distribution of this linear code. The Solomon-Stiffler codes are a family of
famous Griesmer codes, which were proposed by Solomon and Stiffler in 1965. In
this paper, we determine the weight enumerator polynomials of the lifted codes
of the projective Solomon-Stiffler codes using some combinatorial properties of
subspaces. As a result, we determine the support weight distributions of the
projective Solomon-Stiffler codes. In particular, we determine the weight
hierarchies of the projective Solomon-Stiffler codes.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12515" title="Abstract">arXiv:2310.12515</a> [<a href="/pdf/2310.12515" title="Download PDF">pdf</a>, <a href="/format/2310.12515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WeaveNet for Approximating Two-sided Matching Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sone%2C+S">Shusaku Sone</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaxin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+A">Atsushi Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Chiba%2C+N">Naoya Chiba</a>, 
<a href="/search/cs?searchtype=author&query=Ushiku%2C+Y">Yoshitaka Ushiku</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Matching, a task to optimally assign limited resources under constraints, is
a fundamental technology for society. The task potentially has various
objectives, conditions, and constraints; however, the efficient neural network
architecture for matching is underexplored. This paper proposes a novel graph
neural network (GNN), \textit{WeaveNet}, designed for bipartite graphs. Since a
bipartite graph is generally dense, general GNN architectures lose node-wise
information by over-smoothing when deeply stacked. Such a phenomenon is
undesirable for solving matching problems. WeaveNet avoids it by preserving
edge-wise information while passing messages densely to reach a better
solution. To evaluate the model, we approximated one of the \textit{strongly
NP-hard} problems, \textit{fair stable matching}. Despite its inherent
difficulties and the network's general purpose design, our model reached a
comparative performance with state-of-the-art algorithms specially designed for
stable matching for small numbers of agents.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12516" title="Abstract">arXiv:2310.12516</a> [<a href="/pdf/2310.12516" title="Download PDF">pdf</a>, <a href="/format/2310.12516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Hallucination Assessment for Aligned Large Language Models via  Transferable Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaodong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although remarkable progress has been achieved in preventing large language
model (LLM) hallucinations using instruction tuning and retrieval augmentation,
it remains challenging to measure the reliability of LLMs using human-crafted
evaluation data which is not available for many tasks and domains and could
suffer from data leakage. Inspired by adversarial machine learning, this paper
aims to develop a method of automatically generating evaluation data by
appropriately modifying existing data on which LLMs behave faithfully.
Specifically, this paper presents AutoDebug, an LLM-based framework to use
prompting chaining to generate transferable adversarial attacks in the form of
question-answering examples. We seek to understand the extent to which these
examples trigger the hallucination behaviors of LLMs.
<br />We implement AutoDebug using ChatGPT and evaluate the resulting two variants
of a popular open-domain question-answering dataset, Natural Questions (NQ), on
a collection of open-source and proprietary LLMs under various prompting
settings. Our generated evaluation data is human-readable and, as we show,
humans can answer these modified questions well. Nevertheless, we observe
pronounced accuracy drops across multiple LLMs including GPT-4. Our
experimental results show that LLMs are likely to hallucinate in two categories
of question-answering scenarios where (1) there are conflicts between knowledge
given in the prompt and their parametric knowledge, or (2) the knowledge
expressed in the prompt is complex. Finally, we find that the adversarial
examples generated by our method are transferable across all considered LLMs.
The examples generated by a small model can be used to debug a much larger
model, making our approach cost-effective.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12520" title="Abstract">arXiv:2310.12520</a> [<a href="/pdf/2310.12520" title="Download PDF">pdf</a>, <a href="/format/2310.12520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lost in Translation: When GPT-4V(ision) Can&#x27;t See Eye to Eye with Text.  A Vision-Language-Consistency Analysis of VLLMs and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Senyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zijun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+N">Ning Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent advancements in multimodal techniques open exciting possibilities for
models excelling in diverse tasks involving text, audio, and image processing.
Models like GPT-4V, blending computer vision and language modeling, excel in
complex text and image tasks. Numerous prior research endeavors have diligently
examined the performance of these Vision Large Language Models (VLLMs) across
tasks like object detection, image captioning and others. However, these
analyses often focus on evaluating the performance of each modality in
isolation, lacking insights into their cross-modal interactions. Specifically,
questions concerning whether these vision-language models execute vision and
language tasks consistently or independently have remained unanswered. In this
study, we draw inspiration from recent investigations into multilingualism and
conduct a comprehensive analysis of model's cross-modal interactions. We
introduce a systematic framework that quantifies the capability disparities
between different modalities in the multi-modal setting and provide a set of
datasets designed for these evaluations. Our findings reveal that models like
GPT-4V tend to perform consistently modalities when the tasks are relatively
simple. However, the trustworthiness of results derived from the vision
modality diminishes as the tasks become more challenging. Expanding on our
findings, we introduce "Vision Description Prompting," a method that
effectively improves performance in challenging vision-related tasks.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12522" title="Abstract">arXiv:2310.12522</a> [<a href="/pdf/2310.12522" title="Download PDF">pdf</a>, <a href="/format/2310.12522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Named Entity Recognition for Monitoring Plant Health Threats in Tweets:  a ChouBERT Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shufan Jiang</a> (CRESTIC, ISEP), 
<a href="/search/cs?searchtype=author&query=Angarita%2C+R">Rafael Angarita</a> (ISEP), 
<a href="/search/cs?searchtype=author&query=Cormier%2C+S">St&#xe9;phane Cormier</a> (CRESTIC), 
<a href="/search/cs?searchtype=author&query=Rousseaux%2C+F">Francis Rousseaux</a> (CRESTIC)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2022 6th International Conference on Universal Village (UV), Oct 2022, Boston, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">An important application scenario of precision agriculture is detecting and
measuring crop health threats using sensors and data analysis techniques.
However, the textual data are still under-explored among the existing solutions
due to the lack of labelled data and fine-grained semantic resources. Recent
research suggests that the increasing connectivity of farmers and the emergence
of online farming communities make social media like Twitter a participatory
platform for detecting unfamiliar plant health events if we can extract
essential information from unstructured textual data. ChouBERT is a French
pre-trained language model that can identify Tweets concerning observations of
plant health issues with generalizability on unseen natural hazards. This paper
tackles the lack of labelled data by further studying ChouBERT's know-how on
token-level annotation tasks over small labeled sets.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12523" title="Abstract">arXiv:2310.12523</a> [<a href="/pdf/2310.12523" title="Download PDF">pdf</a>, <a href="/format/2310.12523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Preserving Large Language Models: ChatGPT Case Study Based  Vision and Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ullah%2C+I">Imdad Ullah</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+N">Najm Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Gill%2C+S+S">Sukhpal Singh Gill</a>, 
<a href="/search/cs?searchtype=author&query=Suleiman%2C+B">Basem Suleiman</a>, 
<a href="/search/cs?searchtype=author&query=Ahanger%2C+T+A">Tariq Ahamed Ahanger</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+Z">Zawar Shah</a>, 
<a href="/search/cs?searchtype=author&query=Qadir%2C+J">Junaid Qadir</a>, 
<a href="/search/cs?searchtype=author&query=Kanhere%2C+S+S">Salil S. Kanhere</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The generative Artificial Intelligence (AI) tools based on Large Language
Models (LLMs) use billions of parameters to extensively analyse large datasets
and extract critical private information such as, context, specific details,
identifying information etc. This have raised serious threats to user privacy
and reluctance to use such tools. This article proposes the conceptual model
called PrivChatGPT, a privacy-preserving model for LLMs that consists of two
main components i.e., preserving user privacy during the data
curation/pre-processing together with preserving private context and the
private training process for large-scale data. To demonstrate its
applicability, we show how a private mechanism could be integrated into the
existing model for training LLMs to protect user privacy; specifically, we
employed differential privacy and private training using Reinforcement Learning
(RL). We measure the privacy loss and evaluate the measure of uncertainty or
randomness once differential privacy is applied. It further recursively
evaluates the level of privacy guarantees and the measure of uncertainty of
public database and resources, during each update when new information is added
for training purposes. To critically evaluate the use of differential privacy
for private LLMs, we hypothetically compared other mechanisms e..g, Blockchain,
private information retrieval, randomisation, for various performance measures
such as the model performance and accuracy, computational complexity, privacy
vs. utility etc. We conclude that differential privacy, randomisation, and
obfuscation can impact utility and performance of trained models, conversely,
the use of ToR, Blockchain, and PIR may introduce additional computational
complexity and high training latency. We believe that the proposed model could
be used as a benchmark for proposing privacy preserving LLMs for generative AI
tools.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12524" title="Abstract">arXiv:2310.12524</a> [<a href="/pdf/2310.12524" title="Download PDF">pdf</a>, <a href="/format/2310.12524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Browsing Models for Linear and Grid Layouts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raj%2C+A">Amifa Raj</a>, 
<a href="/search/cs?searchtype=author&query=Ekstrand%2C+M">Michael Ekstrand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Many information access systems operationalize their results in terms of
rankings, which are then displayed to users in various ranking layouts such as
linear lists or grids. User interaction with a retrieved item is highly
dependent on the item's position in the layout, and users do not provide
similar attention to every position in ranking (under any layout model). User
attention is an important component in the evaluation process of ranking, due
to its use in effectiveness metrics that estimate utility as well as fairness
metrics that evaluate ranking based on social and ethical concerns. These
metrics take user browsing behavior into account in their measurement
strategies to estimate the attention the user is likely to provide to each item
in ranking. Research on understanding user browsing behavior has proposed
several user browsing models, and further observed that user browsing behavior
differs with different ranking layouts. However, the underlying concepts of
these browsing models are often similar, including varying components and
parameter settings. We seek to leverage that similarity to represent multiple
browsing models in a generalized, configurable framework which can be further
extended to more complex ranking scenarios. In this paper, we describe a
probabilistic user browsing model for linear rankings, show how they can be
configured to yield models commonly used in current evaluation practice, and
generalize this model to also account for browsing behaviors in grid-based
layouts. This model provides configurable framework for estimating the
attention that results from user browsing activity for a range of IR evaluation
and measurement applications in multiple formats, and also identifies
parameters that need to be estimated through user studies to provide realistic
evaluation beyond ranked lists.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12526" title="Abstract">arXiv:2310.12526</a> [<a href="/pdf/2310.12526" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Bayesian Optimization Using Satisficing Thompson Sampling for  Time-Sensitive Black-Box Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaobin Song</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Benben Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Bayesian optimization (BO) is widely used for black-box optimization
problems, and have been shown to perform well in various real-world tasks.
However, most of the existing BO methods aim to learn the optimal solution,
which may become infeasible when the parameter space is extremely large or the
problem is time-sensitive. In these contexts, switching to a satisficing
solution that requires less information can result in better performance. In
this work, we focus on time-sensitive black-box optimization problems and
propose satisficing Thompson sampling-based parallel Bayesian optimization
(STS-PBO) approaches, including synchronous and asynchronous versions. We shift
the target from an optimal solution to a satisficing solution that is easier to
learn. The rate-distortion theory is introduced to construct a loss function
that balances the amount of information that needs to be learned with
sub-optimality, and the Blahut-Arimoto algorithm is adopted to compute the
target solution that reaches the minimum information rate under the distortion
limit at each step. Both discounted and undiscounted Bayesian cumulative regret
bounds are theoretically derived for the proposed STS-PBO approaches. The
effectiveness of the proposed methods is demonstrated on a fast-charging design
problem of Lithium-ion batteries. The results are accordant with theoretical
analyses, and show that our STS-PBO methods outperform both sequential
counterparts and parallel BO with traditional Thompson sampling in both
synchronous and asynchronous settings.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12527" title="Abstract">arXiv:2310.12527</a> [<a href="/pdf/2310.12527" title="Download PDF">pdf</a>, <a href="/format/2310.12527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing the Consistency of Performance Scores Reported for Binary  Classification Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fazekas%2C+A">Attila Fazekas</a>, 
<a href="/search/cs?searchtype=author&query=Kov%C3%A1cs%2C+G">Gy&#xf6;rgy Kov&#xe1;cs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Binary classification is a fundamental task in machine learning, with
applications spanning various scientific domains. Whether scientists are
conducting fundamental research or refining practical applications, they
typically assess and rank classification techniques based on performance
metrics such as accuracy, sensitivity, and specificity. However, reported
performance scores may not always serve as a reliable basis for research
ranking. This can be attributed to undisclosed or unconventional practices
related to cross-validation, typographical errors, and other factors. In a
given experimental setup, with a specific number of positive and negative test
items, most performance scores can assume specific, interrelated values. In
this paper, we introduce numerical techniques to assess the consistency of
reported performance scores and the assumed experimental setup. Importantly,
the proposed approach does not rely on statistical inference but uses numerical
methods to identify inconsistencies with certainty. Through three different
applications related to medicine, we demonstrate how the proposed techniques
can effectively detect inconsistencies, thereby safeguarding the integrity of
research fields. To benefit the scientific community, we have made the
consistency tests available in an open-source Python package.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12531" title="Abstract">arXiv:2310.12531</a> [<a href="/pdf/2310.12531" title="Download PDF">pdf</a>, <a href="/format/2310.12531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICU: Conquering Language Barriers in Vision-and-Language Modeling by  Dividing the Tasks into Image Captioning and Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Guojun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP23 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Most multilingual vision-and-language (V&amp;L) research aims to accomplish
multilingual and multimodal capabilities within one model. However, the
scarcity of multilingual captions for images has hindered the development. To
overcome this obstacle, we propose ICU, Image Caption Understanding, which
divides a V&amp;L task into two stages: a V&amp;L model performs image captioning in
English, and a multilingual language model (mLM), in turn, takes the caption as
the alt text and performs crosslingual language understanding. The burden of
multilingual processing is lifted off V&amp;L model and placed on mLM. Since the
multilingual text data is relatively of higher abundance and quality, ICU can
facilitate the conquering of language barriers for V&amp;L models. In experiments
on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can
achieve new state-of-the-art results for five languages, and comparable results
for the rest.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12534" title="Abstract">arXiv:2310.12534</a> [<a href="/pdf/2310.12534" title="Download PDF">pdf</a>, <a href="/format/2310.12534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cormas: The Software for Participatory Modelling and its Application for  Managing Natural Resources in Senegal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaitsev%2C+O">Oleksandr Zaitsev</a> (SENS, Cirad, UM), 
<a href="/search/cs?searchtype=author&query=Vendel%2C+F">Fran&#xe7;ois Vendel</a> (Cirad, SENS, ISRA, CNRF), 
<a href="/search/cs?searchtype=author&query=Delay%2C+E">Etienne Delay</a> (UPR GREEN, Cirad, SENS, ESP, UMMISCO)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2nd workshop on Resource AWareness of Systems and Society (RAW
  2023), Aug 2023, Limassol, Cyprus
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Cormas is an agent-based simulation platform developed in the late 90s by the
Green research at CIRAD unit to support the management of natural resources and
understand the interactions between natural and social dynamics. This platform
is well-suited for a participatory simulation approach that empowers local
stakeholders by including them in all modelling and knowledge-sharing steps. In
this short paper, we present the Cormas platform and discuss its unique
features and their importance for the participatory simulation approach. We
then present the early results of our ongoing study on managing pastoral
resources in the Sahel region, identify the problems faced by local
stakeholders, and discuss the potential use of Cormas at the next stage of our
study to collectively model and understand the effective ways of managing the
shared agro-sylvo-pastoral resources.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12536" title="Abstract">arXiv:2310.12536</a> [<a href="/pdf/2310.12536" title="Download PDF">pdf</a>, <a href="/format/2310.12536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Onboard Low-Power Localization with Semantic Sensor Fusion on a  Nano-UAV using Floor Plans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zimmerman%2C+N">Nicky Zimmerman</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+H">Hanna M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Magno%2C+M">Michele Magno</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for ICRA 2024, 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Nano-sized unmanned aerial vehicles (UAVs) are well-fit for indoor
applications and for close proximity to humans. To enable autonomy, the
nano-UAV must be able to self-localize in its operating environment. This is a
particularly-challenging task due to the limited sensing and compute resources
on board. This work presents an online and onboard approach for localization in
floor plans annotated with semantic information. Unlike sensor-based maps,
floor plans are readily-available, and do not increase the cost and time of
deployment. To overcome the difficulty of localizing in sparse maps, the
proposed approach fuses geometric information from miniaturized time-of-flight
sensors and semantic cues. The semantic information is extracted from images by
deploying a state-of-the-art object detection model on a high-performance
multi-core microcontroller onboard the drone, consuming only 2.5mJ per frame
and executing in 38ms. In our evaluation, we globally localize in a real-world
office environment, achieving 90% success rate. We also release an open-source
implementation of our work.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12537" title="Abstract">arXiv:2310.12537</a> [<a href="/pdf/2310.12537" title="Download PDF">pdf</a>, <a href="/format/2310.12537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Product Attribute Value Extraction using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brinkmann%2C+A">Alexander Brinkmann</a>, 
<a href="/search/cs?searchtype=author&query=Shraga%2C+R">Roee Shraga</a>, 
<a href="/search/cs?searchtype=author&query=Bizer%2C+C">Christian Bizer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">E-commerce applications such as faceted product search or product comparison
are based on structured product descriptions like attribute/value pairs. The
vendors on e-commerce platforms do not provide structured product descriptions
but describe offers using titles or descriptions. To process such offers, it is
necessary to extract attribute/value pairs from textual product attributes.
State-of-the-art attribute/value extraction techniques rely on pre-trained
language models (PLMs), such as BERT. Two major drawbacks of these models for
attribute/value extraction are that (i) the models require significant amounts
of task-specific training data and (ii) the fine-tuned models face challenges
in generalizing to attribute values not included in the training data. This
paper explores the potential of large language models (LLMs) as a training
data-efficient and robust alternative to PLM-based attribute/value extraction
methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as
open-source LLMs based on Llama2. We evaluate the models in a zero-shot
scenario and in a scenario where task-specific training data is available. In
the zero-shot scenario, we compare various prompt designs for representing
information about the target attributes of the extraction. In the scenario with
training data, we investigate (i) the provision of example attribute values,
(ii) the selection of in-context demonstrations, and (iii) the fine-tuning of
GPT-3.5. Our experiments show that GPT-4 achieves an average F1-score of 85% on
the two evaluation datasets while the best PLM-based techniques perform on
average 5% worse using the same amount of training data. GPT-4 achieves a 10%
higher F1-score than the best open-source LLM. The fine-tuned GPT-3.5 model
reaches a similar performance as GPT-4 while being significantly more
cost-efficient.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12538" title="Abstract">arXiv:2310.12538</a> [<a href="/pdf/2310.12538" title="Download PDF">pdf</a>, <a href="/format/2310.12538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Expensive Optimization Problems in Dynamic Environments with  Meta-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jinliang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Liang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Dynamic environments pose great challenges for expensive optimization
problems, as the objective functions of these problems change over time and
thus require remarkable computational resources to track the optimal solutions.
Although data-driven evolutionary optimization and Bayesian optimization (BO)
approaches have shown promise in solving expensive optimization problems in
static environments, the attempts to develop such approaches in dynamic
environments remain rarely unexplored. In this paper, we propose a simple yet
effective meta-learning-based optimization framework for solving expensive
dynamic optimization problems. This framework is flexible, allowing any
off-the-shelf continuously differentiable surrogate model to be used in a
plug-in manner, either in data-driven evolutionary optimization or BO
approaches. In particular, the framework consists of two unique components: 1)
the meta-learning component, in which a gradient-based meta-learning approach
is adopted to learn experience (effective model parameters) across different
dynamics along the optimization process. 2) the adaptation component, where the
learned experience (model parameters) is used as the initial parameters for
fast adaptation in the dynamic environment based on few shot samples. By doing
so, the optimization process is able to quickly initiate the search in a new
environment within a strictly restricted computational budget. Experiments
demonstrate the effectiveness of the proposed algorithm framework compared to
several state-of-the-art algorithms on common benchmark test problems under
different dynamic characteristics.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12540" title="Abstract">arXiv:2310.12540</a> [<a href="/pdf/2310.12540" title="Download PDF">pdf</a>, <a href="/ps/2310.12540" title="Download PostScript">ps</a>, <a href="/format/2310.12540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Pure Type Systems in the lambda-Pi-calculus modulo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cousineau%2C+D">Denis Cousineau</a> (LIX), 
<a href="/search/cs?searchtype=author&query=Dowek%2C+G">Gilles Dowek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The lambda-Pi-calculus allows to express proofs of minimal predicate logic.
It can be extended, in a very simple way, by adding computation rules. This
leads to the lambda-Pi-calculus modulo. We show in this paper that this simple
extension is surprisingly expressive and, in particular, that all functional
Pure Type Systems, such as the system F, or the Calculus of Constructions, can
be embedded in it. And, moreover, that this embedding is conservative under
termination hypothesis.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12541" title="Abstract">arXiv:2310.12541</a> [<a href="/pdf/2310.12541" title="Download PDF">pdf</a>, <a href="/format/2310.12541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model for Multi-objective Evolutionary Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenkun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xialiang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingxuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Multiobjective evolutionary algorithms (MOEAs) are major methods for solving
multiobjective optimization problems (MOPs). Many MOEAs have been proposed in
the past decades, of which the operators need carefully handcrafted design with
domain knowledge. Recently, some attempts have been made to replace the
manually designed operators in MOEAs with learning-based operators (e.g.,
neural network models). However, much effort is still required for designing
and training such models, and the learned operators might not generalize well
to solve new problems. To tackle the above challenges, this work investigates a
novel approach that leverages the powerful large language model (LLM) to design
MOEA operators. With proper prompt engineering, we successfully let a general
LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D)
in a zero-shot manner. In addition, by learning from the LLM behavior, we
further design an explicit white-box operator with randomness and propose a new
version of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on
different test benchmarks show that our proposed method can achieve competitive
performance with widely used MOEAs. It is also promising to see the operator
only learned from a few instances can have robust generalization performance on
unseen problems with quite different patterns and settings. The results reveal
the potential benefits of using pre-trained LLMs in the design of MOEAs.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12545" title="Abstract">arXiv:2310.12545</a> [<a href="/pdf/2310.12545" title="Download PDF">pdf</a>, <a href="/format/2310.12545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Picard algorithm for general semilinear parabolic PDEs with  gradient-dependent nonlinearities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neufeld%2C+A">Ariel Neufeld</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+S">Sizhou Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Probability (math.PR)

</div>
<p class="mathjax">In this paper we introduce a multilevel Picard approximation algorithm for
general semilinear parabolic PDEs with gradient-dependent nonlinearities whose
coefficient functions do not need to be constant. We also provide a full
convergence and complexity analysis of our algorithm. To obtain our main
results, we consider a particular stochastic fixed-point equation (SFPE)
motivated by the Feynman-Kac representation and the Bismut-Elworthy-Li formula.
We show that the PDE under consideration has a unique viscosity solution which
coincides with the first component of the unique solution of the stochastic
fixed-point equation. Moreover, if the PDE admits a strong solution, then the
gradient of the unique solution of the PDE coincides with the second component
of the unique solution of the stochastic fixed-point equation.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12547" title="Abstract">arXiv:2310.12547</a> [<a href="/pdf/2310.12547" title="Download PDF">pdf</a>, <a href="/format/2310.12547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PGA: Personalizing Grasping Agents with Single Human-Robot Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+G">Gi-Cheon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaein Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Seoyun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+M">Minjoon Jung</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Byoung-Tak Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that
ground and grasp objects based on natural language instructions. While robots
capable of recognizing personal objects like "my wallet" can interact more
naturally with non-expert users, current LCRG systems primarily limit robots to
understanding only generic expressions. To this end, we introduce a task
scenario GraspMine with a novel dataset that aims to locate and grasp personal
objects given personal indicators via learning from a single human-robot
interaction. To address GraspMine, we propose Personalized Grasping Agent
(PGA), that learns personal objects by propagating user-given information
through a Reminiscence-a collection of raw images from the user's environment.
Specifically, PGA acquires personal object information by a user presenting a
personal object with its associated indicator, followed by PGA inspecting the
object by rotating it. Based on the acquired information, PGA pseudo-labels
objects in the Reminiscence by our proposed label propagation algorithm.
Harnessing the information acquired from the interactions and the
pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding
model to grasp personal objects. Experiments on GraspMine show that PGA
significantly outperforms baseline methods both in offline and online settings,
signifying its effectiveness and personalization applicability on real-world
scenarios. Finally, qualitative analysis shows the effectiveness of PGA through
a detailed investigation of results in each phase.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12551" title="Abstract">arXiv:2310.12551</a> [<a href="/pdf/2310.12551" title="Download PDF">pdf</a>, <a href="/format/2310.12551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative PnP and its application in 3D-2D vascular image registration  for robot navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jingwei Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Keke Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+T">Tuoyu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">This paper reports on a new real-time robot-centered 3D-2D vascular image
alignment algorithm, which is robust to outliers and can align nonrigid shapes.
Few works have managed to achieve both real-time and accurate performance for
vascular intervention robots. This work bridges high-accuracy 3D-2D
registration techniques and computational efficiency requirements in
intervention robot applications. We categorize centerline-based vascular 3D-2D
image registration problems as an iterative Perspective-n-Point (PnP) problem
and propose to use the Levenberg-Marquardt solver on the Lie manifold. Then,
the recently developed Reproducing Kernel Hilbert Space (RKHS) algorithm is
introduced to overcome the ``big-to-small'' problem in typical robotic
scenarios. Finally, an iterative reweighted least squares is applied to solve
RKHS-based formulation efficiently. Experiments indicate that the proposed
algorithm processes registration over 50 Hz (rigid) and 20 Hz (nonrigid) and
obtains competing registration accuracy similar to other works. Results
indicate that our Iterative PnP is suitable for future vascular intervention
robot applications.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12553" title="Abstract">arXiv:2310.12553</a> [<a href="/pdf/2310.12553" title="Download PDF">pdf</a>, <a href="/format/2310.12553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explanation-Based Training with Differentiable Insertion/Deletion  Metric-Aware Regularizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoshikawa%2C+u">uya Yoshikawa</a>, 
<a href="/search/cs?searchtype=author&query=Iwata%2C+T">Tomoharu Iwata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">The quality of explanations for the predictions of complex machine learning
predictors is often measured using insertion and deletion metrics, which assess
the faithfulness of the explanations, i.e., how correctly the explanations
reflect the predictor's behavior. To improve the faithfulness, we propose
insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which
optimizes differentiable predictors to improve both insertion and deletion
scores of the explanations while keeping their predictive accuracy. Since the
original insertion and deletion metrics are indifferentiable with respect to
the explanations and directly unavailable for gradient-based optimization, we
extend the metrics to be differentiable and use them to formalize insertion and
deletion metric-based regularizers. The experimental results on image and
tabular datasets show that the deep neural networks-based predictors fine-tuned
using ID-ExpO enable popular post-hoc explainers to produce more faithful and
easy-to-interpret explanations while keeping high predictive accuracy.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12554" title="Abstract">arXiv:2310.12554</a> [<a href="/pdf/2310.12554" title="Download PDF">pdf</a>, <a href="/format/2310.12554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GMEM: Generalized Memory Management for Peripheral Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Weixi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cox%2C+A+L">Alan L. Cox</a>, 
<a href="/search/cs?searchtype=author&query=Rixner%2C+S">Scott Rixner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Finished before Weixi left Rice and submitted to ASPLOS'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Operating Systems (cs.OS)</span>

</div>
<p class="mathjax">This paper presents GMEM, generalized memory management, for peripheral
devices. GMEM provides OS support for centralized memory management of both CPU
and devices. GMEM provides a high-level interface that decouples MMU-specific
functions. Device drivers can thus attach themselves to a process's address
space and let the OS take charge of their memory management. This eliminates
the need for device drivers to "reinvent the wheel" and allows them to benefit
from general memory optimizations integrated by GMEM. Furthermore, GMEM
internally coordinates all attached devices within each virtual address space.
This drastically improves user-level programmability, since programmers can use
a single address space within their program, even when operating across the CPU
and multiple devices. A case study on device drivers demonstrates these
benefits. A GMEM-based IOMMU driver eliminates around seven hundred lines of
code and obtains 54% higher network receive throughput utilizing 32% less CPU
compared to the state-of-the-art. In addition, the GMEM-based driver of a
simulated GPU takes less than 70 lines of code, excluding its MMU functions.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12556" title="Abstract">arXiv:2310.12556</a> [<a href="/pdf/2310.12556" title="Download PDF">pdf</a>, <a href="/ps/2310.12556" title="Download PostScript">ps</a>, <a href="/format/2310.12556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Algorithm for Counting Cycles in QC and APM LDPC Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholami%2C+M">Mohammad Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+Z">Zahra Gholami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Number Theory (math.NT)

</div>
<p class="mathjax">In this paper, a new method is given for counting cycles in the Tanner graph
of a (Type-I) quasi-cyclic (QC) low-density parity-check (LDPC) code which the
complexity mainly is dependent on the base matrix, independent from the
CPM-size of the constructed code. Interestingly, for large CPM-sizes, in
comparison of the existing methods, this algorithm is the first approach which
efficiently counts the cycles in the Tanner graphs of QC-LDPC codes. In fact,
the algorithm recursively counts the cycles in the parity-check matrix
column-by-column by finding all non-isomorph tailless backtrackless closed
(TBC) walks in the base graph and enumerating theoretically their corresponding
cycles in the same equivalent class. Moreover, this approach can be modified in
few steps to find the cycle distributions of a class of LDPC codes based on
Affine permutation matrices (APM-LDPC codes). Interestingly, unlike the
existing methods which count the cycles up to $2g-2$, where $g$ is the girth,
the proposed algorithm can be used to enumerate the cycles of arbitrary length
in the Tanner graph. Moreover, the proposed cycle searching algorithm improves
upon various previously known methods, in terms of computational complexity and
memory requirements.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12557" title="Abstract">arXiv:2310.12557</a> [<a href="/pdf/2310.12557" title="Download PDF">pdf</a>, <a href="/format/2310.12557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial  Reasoning in Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuaiyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spatial reasoning in text plays a crucial role in various real-world
applications. Existing approaches for spatial reasoning typically infer spatial
relations from pure text, which overlook the gap between natural language and
symbolic structures. Graph neural networks (GNNs) have showcased exceptional
proficiency in inducing and aggregating symbolic structures. However, classical
GNNs face challenges in handling multi-hop spatial reasoning due to the
over-smoothing issue, \textit{i.e.}, the performance decreases substantially as
the number of graph layers increases. To cope with these challenges, we propose
a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural
\textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node
memory scheme and aggregate the information over the depth dimension instead of
the breadth dimension of the graph, which empowers the ability to collect long
dependencies without stacking multiple layers. Experimental results on two
challenging multi-hop spatial reasoning datasets show that DepWiGNN outperforms
existing spatial reasoning methods. The comparisons with the other three GNNs
further demonstrate its superiority in capturing long dependency in the graph.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12558" title="Abstract">arXiv:2310.12558</a> [<a href="/pdf/2310.12558" title="Download PDF">pdf</a>, <a href="/format/2310.12558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Help Humans Verify Truthfulness -- Except When  They Are Convincingly Wrong
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+C">Chenglei Si</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+N">Navita Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S+T">Sherry Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large Language Models (LLMs) are increasingly used for accessing information
on the web. Their truthfulness and factuality are thus of great interest. To
help users make the right decisions about the information they're getting, LLMs
should not only provide but also help users fact-check information. In this
paper, we conduct experiments with 80 crowdworkers in total to compare language
models with search engines (information retrieval systems) at facilitating
fact-checking by human users. We prompt LLMs to validate a given claim and
provide corresponding explanations. Users reading LLM explanations are
significantly more efficient than using search engines with similar accuracy.
However, they tend to over-rely the LLMs when the explanation is wrong. To
reduce over-reliance on LLMs, we ask LLMs to provide contrastive information -
explain both why the claim is true and false, and then we present both sides of
the explanation to users. This contrastive explanation mitigates users'
over-reliance on LLMs, but cannot significantly outperform search engines.
However, showing both search engine results and LLM explanations offers no
complementary benefits as compared to search engines alone. Taken together,
natural language explanations by LLMs may not be a reliable replacement for
reading the retrieved passages yet, especially in high-stakes settings where
over-relying on wrong AI explanations could lead to critical consequences.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12560" title="Abstract">arXiv:2310.12560</a> [<a href="/pdf/2310.12560" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Model Debias with Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Huimin Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianxiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jin Hao</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by NIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent discoveries have revealed that deep neural networks might behave in a
biased manner in many real-world scenarios. For instance, deep networks trained
on a large-scale face recognition dataset CelebA tend to predict blonde hair
for females and black hair for males. Such biases not only jeopardize the
robustness of models but also perpetuate and amplify social biases, which is
especially concerning for automated decision-making processes in healthcare,
recruitment, etc., as they could exacerbate unfair economic and social
inequalities among different groups. Existing debiasing methods suffer from
high costs in bias labeling or model re-training, while also exhibiting a
deficiency in terms of elucidating the origins of biases within the model. To
this respect, we propose a fast model debiasing framework (FMD) which offers an
efficient approach to identify, evaluate and remove biases inherent in trained
models. The FMD identifies biased attributes through an explicit counterfactual
concept and quantifies the influence of data samples with influence functions.
Moreover, we design a machine unlearning-based strategy to efficiently and
effectively remove the bias in a trained model with a small counterfactual
dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets
along with experiments with large language models demonstrate that our method
achieves superior or competing accuracies compared with state-of-the-art
methods while attaining significantly fewer biases and requiring much less
debiasing cost. Notably, our method requires only a small external dataset and
updating a minimal amount of model parameters, without the requirement of
access to training data that may be too large or unavailable in practice.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12562" title="Abstract">arXiv:2310.12562</a> [<a href="/pdf/2310.12562" title="Download PDF">pdf</a>, <a href="/format/2310.12562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Click on Mask: A Labor-efficient Annotation Framework with Level Set for  Infrared Small Target Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinfu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yifei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runshi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 5 figures, references added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Infrared Small Target Detection is a challenging task to separate small
targets from infrared clutter background. Recently, deep learning paradigms
have achieved promising results. However, these data-driven methods need plenty
of manual annotation. Due to the small size of infrared targets, manual
annotation consumes more resources and restricts the development of this field.
This letter proposed a labor-efficient and cursory annotation framework with
level set, which obtains a high-quality pseudo mask with only one cursory
click. A variational level set formulation with an expectation difference
energy functional is designed, in which the zero level contour is intrinsically
maintained during the level set evolution. It solves the issue that zero level
contour disappearing due to small target size and excessive regularization.
Experiments on the NUAA-SIRST and IRSTD-1k datasets reveal that our approach
achieves superior performance. Code is available at
https://github.com/Li-Haoqing/COM.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12565" title="Abstract">arXiv:2310.12565</a> [<a href="/pdf/2310.12565" title="Download PDF">pdf</a>, <a href="/format/2310.12565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-World Lifelong Graph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+M">Marcel Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Galke%2C+L">Lukas Galke</a>, 
<a href="/search/cs?searchtype=author&query=Scherp%2C+A">Ansgar Scherp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the problem of lifelong graph learning in an open-world scenario,
where a model needs to deal with new tasks and potentially unknown classes. We
utilize Out-of-Distribution (OOD) detection methods to recognize new classes
and adapt existing non-graph OOD detection methods to graph data. Crucially, we
suggest performing new class detection by combining OOD detection methods with
information aggregated from the graph neighborhood. Most OOD detection methods
avoid determining a crisp threshold for deciding whether a vertex is OOD. To
tackle this problem, we propose a Weakly-supervised Relevance Feedback
(Open-WRF) method, which decreases the sensitivity to thresholds in OOD
detection. We evaluate our approach on six benchmark datasets. Our results show
that the proposed neighborhood aggregation method for OOD scores outperforms
existing methods independent of the underlying graph neural network.
Furthermore, we demonstrate that our Open-WRF method is more robust to
threshold selection and analyze the influence of graph neighborhood on OOD
detection. The aggregation and threshold methods are compatible with arbitrary
graph neural networks and OOD detection methods, making our approach versatile
and applicable to many real-world applications.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12567" title="Abstract">arXiv:2310.12567</a> [<a href="/pdf/2310.12567" title="Download PDF">pdf</a>, <a href="/format/2310.12567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weidong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruiyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yiran Geng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yifan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Juntao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12568" title="Abstract">arXiv:2310.12568</a> [<a href="/pdf/2310.12568" title="Download PDF">pdf</a>, <a href="/format/2310.12568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Julearn: an easy-to-use library for leakage-free evaluation and  inspection of ML models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamdan%2C+S">Sami Hamdan</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+S">Shammi More</a>, 
<a href="/search/cs?searchtype=author&query=Sasse%2C+L">Leonard Sasse</a>, 
<a href="/search/cs?searchtype=author&query=Komeyer%2C+V">Vera Komeyer</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+K+R">Kaustubh R. Patil</a>, 
<a href="/search/cs?searchtype=author&query=Raimondo%2C+F">Federico Raimondo</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">The fast-paced development of machine learning (ML) methods coupled with its
increasing adoption in research poses challenges for researchers without
extensive training in ML. In neuroscience, for example, ML can help understand
brain-behavior relationships, diagnose diseases, and develop biomarkers using
various data sources like magnetic resonance imaging and
electroencephalography. The primary objective of ML is to build models that can
make accurate predictions on unseen data. Researchers aim to prove the
existence of such generalizable models by evaluating performance using
techniques such as cross-validation (CV), which uses systematic subsampling to
estimate the generalization performance. Choosing a CV scheme and evaluating an
ML pipeline can be challenging and, if used improperly, can lead to
overestimated results and incorrect interpretations.
<br />We created julearn, an open-source Python library, that allow researchers to
design and evaluate complex ML pipelines without encountering in common
pitfalls. In this manuscript, we present the rationale behind julearn's design,
its core features, and showcase three examples of previously-published research
projects that can be easily implemented using this novel library. Julearn aims
to simplify the entry into the ML world by providing an easy-to-use environment
with built in guards against some of the most common ML pitfalls. With its
design, unique features and simple interface, it poses as a useful Python-based
library for research projects.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12571" title="Abstract">arXiv:2310.12571</a> [<a href="/pdf/2310.12571" title="Download PDF">pdf</a>, <a href="/format/2310.12571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum computing through the lens of control: A tutorial introduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Berberich%2C+J">Julian Berberich</a>, 
<a href="/search/eess?searchtype=author&query=Fink%2C+D">Daniel Fink</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Quantum computing is a fascinating interdisciplinary research field that
promises to revolutionize computing by efficiently solving previously
intractable problems. Recent years have seen tremendous progress on both the
experimental realization of quantum computing devices as well as the
development and implementation of quantum algorithms. Yet, realizing
computational advantages of quantum computers in practice remains a widely open
problem due to numerous fundamental challenges. Interestingly, many of these
challenges are connected to performance, robustness, scalability, optimization,
or feedback, all of which are central concepts in control theory. This paper
provides a tutorial introduction to quantum computing from the perspective of
control theory. We introduce the mathematical framework of quantum algorithms
ranging from basic elements including quantum bits and quantum gates to more
advanced concepts such as variational quantum algorithms and quantum errors.
The tutorial only requires basic knowledge of linear algebra and, in
particular, no prior exposure to quantum physics. Our main goal is to equip
readers with the mathematical basics required to understand and possibly solve
(control-related) problems in quantum computing. In particular, beyond the
tutorial introduction, we provide a list of research challenges in the field of
quantum computing and discuss their connections to control.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12572" title="Abstract">arXiv:2310.12572</a> [<a href="/pdf/2310.12572" title="Download PDF">pdf</a>, <a href="/format/2310.12572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Notes on Small Private Key Attacks on Common Prime RSA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mengce Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">We point out critical deficiencies in lattice-based cryptanalysis of common
prime RSA presented in ``Remarks on the cryptanalysis of common prime RSA for
IoT constrained low power devices'' [Information Sciences, 538 (2020) 54--68].
To rectify these flaws, we carefully scrutinize the relevant parameters
involved in the analysis during solving a specific trivariate integer
polynomial equation. Additionally, we offer a synthesized attack illustration
of small private key attacks on common prime RSA.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12575" title="Abstract">arXiv:2310.12575</a> [<a href="/pdf/2310.12575" title="Download PDF">pdf</a>, <a href="/format/2310.12575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual estimation of political-party positioning: From label  aggregation to long-input Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikolaev%2C+D">Dmitry Nikolaev</a>, 
<a href="/search/cs?searchtype=author&query=Ceron%2C+T">Tanise Ceron</a>, 
<a href="/search/cs?searchtype=author&query=Pad%C3%B3%2C+S">Sebastian Pad&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Scaling analysis is a technique in computational political science that
assigns a political actor (e.g. politician or party) a score on a predefined
scale based on a (typically long) body of text (e.g. a parliamentary speech or
an election manifesto). For example, political scientists have often used the
left--right scale to systematically analyse political landscapes of different
countries. NLP methods for automatic scaling analysis can find broad
application provided they (i) are able to deal with long texts and (ii) work
robustly across domains and languages. In this work, we implement and compare
two approaches to automatic scaling analysis of political-party manifestos:
label aggregation, a pipeline strategy relying on annotations of individual
statements from the manifestos, and long-input-Transformer-based models, which
compute scaling values directly from raw text. We carry out the analysis of the
Comparative Manifestos Project dataset across 41 countries and 27 languages and
find that the task can be efficiently solved by state-of-the-art models, with
label aggregation producing the best results.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12580" title="Abstract">arXiv:2310.12580</a> [<a href="/pdf/2310.12580" title="Download PDF">pdf</a>, <a href="/format/2310.12580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretraining Language Models with Text-Attributed Heterogeneous Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+T">Tao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Le Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yifei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Leilei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bowen Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In many real-world scenarios (e.g., academic networks, social platforms),
different types of entities are not only associated with texts but also
connected by various relationships, which can be abstracted as Text-Attributed
Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models
(LMs) primarily focus on separately learning the textual information of each
entity and overlook the crucial aspect of capturing topological connections
among entities in TAHGs. In this paper, we present a new pretraining framework
for LMs that explicitly considers the topological and heterogeneous information
in TAHGs. Firstly, we define a context graph as neighborhoods of a target node
within specific orders and propose a topology-aware pretraining task to predict
nodes involved in the context graph by jointly optimizing an LM and an
auxiliary heterogeneous graph neural network. Secondly, based on the
observation that some nodes are text-rich while others have little text, we
devise a text augmentation strategy to enrich textless nodes with their
neighbors' texts for handling the imbalance issue. We conduct link prediction
and node classification tasks on three datasets from various domains.
Experimental results demonstrate the superiority of our approach over existing
methods and the rationality of each design. Our code is available at
https://github.com/Hope-Rita/THLM.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12582" title="Abstract">arXiv:2310.12582</a> [<a href="/pdf/2310.12582" title="Download PDF">pdf</a>, <a href="/ps/2310.12582" title="Download PostScript">ps</a>, <a href="/format/2310.12582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Risk Minimization over Artificial Neural Networks Overcomes  the Curse of Dimensionality in the Numerical Approximation of Linear  Kolmogorov Partial Differential Equations with Unbounded Initial Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xiao%2C+J">Jichang Xiao</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xiaoqun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Deep learning algorithms have been successfully applied to numerically solve
linear Kolmogorov partial differential equations (PDEs). A recent research
shows that the empirical risk minimization~(ERM) over deep artificial neural
networks overcomes the curse of dimensionality in the numerical approximation
of linear Kolmogorov PDEs with bounded initial functions. However, the initial
functions may be unbounded in many applications such as the Black Scholes PDEs
in pricing call options. In this paper, we extend this result to the cases
involving unbounded initial functions. We prove that for $d$-dimensional linear
Kolmogorov PDEs with unbounded initial functions, under suitable assumptions,
the number of training data and the size of the artificial neural network
required to achieve an accuracy $\varepsilon$ for the ERM grow polynomially in
both $d$ and $\varepsilon^{-1}$. Moreover, we verify that the required
assumptions hold for Black-Scholes PDEs and heat equations which are two
important cases of linear Kolmogorov PDEs.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12583" title="Abstract">arXiv:2310.12583</a> [<a href="/pdf/2310.12583" title="Download PDF">pdf</a>, <a href="/format/2310.12583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diverse Diffusion: Enhancing Image Diversity in Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zameshina%2C+M">Mariia Zameshina</a> (LIGM), 
<a href="/search/cs?searchtype=author&query=Teytaud%2C+O">Olivier Teytaud</a> (TAU), 
<a href="/search/cs?searchtype=author&query=Najman%2C+L">Laurent Najman</a> (LIGM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Latent diffusion models excel at producing high-quality images from text.
Yet, concerns appear about the lack of diversity in the generated imagery. To
tackle this, we introduce Diverse Diffusion, a method for boosting image
diversity beyond gender and ethnicity, spanning into richer realms, including
color diversity.Diverse Diffusion is a general unsupervised technique that can
be applied to existing text-to-image models. Our approach focuses on finding
vectors in the Stable Diffusion latent space that are distant from each other.
We generate multiple vectors in the latent space until we find a set of vectors
that meets the desired distance requirements and the required batch size.To
evaluate the effectiveness of our diversity methods, we conduct experiments
examining various characteristics, including color diversity, LPIPS metric, and
ethnicity/gender representation in images featuring humans.The results of our
experiments emphasize the significance of diversity in generating realistic and
varied images, offering valuable insights for improving text-to-image models.
Through the enhancement of image diversity, our approach contributes to the
creation of more inclusive and representative AI-generated art.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12585" title="Abstract">arXiv:2310.12585</a> [<a href="/pdf/2310.12585" title="Download PDF">pdf</a>, <a href="/format/2310.12585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Aware Representation Learning for Time-Sensitive Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Son%2C+J">Jungbin Son</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 EMNLP Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Time is one of the crucial factors in real-world question answering (QA)
problems. However, language models have difficulty understanding the
relationships between time specifiers, such as 'after' and 'before', and
numbers, since existing QA datasets do not include sufficient time expressions.
To address this issue, we propose a Time-Context aware Question Answering
(TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE)
task, and build a time-context dependent data generation framework for model
training. Moreover, we present a metric to evaluate the time awareness of the
QA model using TCSE. The TCSE task consists of a question and four sentence
candidates classified as correct or incorrect based on time and context. The
model is trained to extract the answer span from the sentence that is both
correct in time and context. The model trained with TCQA outperforms baseline
models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code
are available at https://github.com/sonjbin/TCQA
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12590" title="Abstract">arXiv:2310.12590</a> [<a href="/pdf/2310.12590" title="Download PDF">pdf</a>, <a href="/format/2310.12590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PrivacyGAN: robust generative image privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zameshina%2C+M">Mariia Zameshina</a> (LIGM), 
<a href="/search/cs?searchtype=author&query=Careil%2C+M">Marlene Careil</a> (MM, IDS), 
<a href="/search/cs?searchtype=author&query=Teytaud%2C+O">Olivier Teytaud</a> (LRI, TANC), 
<a href="/search/cs?searchtype=author&query=Najman%2C+L">Laurent Najman</a> (LIGM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Classical techniques for protecting facial image privacy typically fall into
two categories: data-poisoning methods, exemplified by Fawkes, which introduce
subtle perturbations to images, or anonymization methods that generate images
resembling the original only in several characteristics, such as gender,
ethnicity, or facial expression.In this study, we introduce a novel approach,
PrivacyGAN, that uses the power of image generation techniques, such as VQGAN
and StyleGAN, to safeguard privacy while maintaining image usability,
particularly for social media applications. Drawing inspiration from Fawkes,
our method entails shifting the original image within the embedding space
towards a decoy image.We evaluate our approach using privacy metrics on
traditional and novel facial image datasets. Additionally, we propose new
criteria for evaluating the robustness of privacy-protection methods against
unknown image recognition techniques, and we demonstrate that our approach is
effective even in unknown embedding transfer scenarios. We also provide a human
evaluation that further proves that the modified image preserves its utility as
it remains recognisable as an image of the same person by friends and family.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12594" title="Abstract">arXiv:2310.12594</a> [<a href="/pdf/2310.12594" title="Download PDF">pdf</a>, <a href="/format/2310.12594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infection Curve Flattening via Targeted Interventions and Self-Isolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doostmohammadian%2C+M">Mohammadreza Doostmohammadian</a>, 
<a href="/search/cs?searchtype=author&query=Zarrabi%2C+H">Houman Zarrabi</a>, 
<a href="/search/cs?searchtype=author&query=Doustmohammadian%2C+A">Azam Doustmohammadian</a>, 
<a href="/search/cs?searchtype=author&query=Rabiee%2C+H+R">Hamid R. Rabiee</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SNAM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Systems and Control (eess.SY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Understanding the impact of network clustering and small-world properties on
epidemic spread can be crucial in developing effective strategies for managing
and controlling infectious diseases. Particularly in this work, we study the
impact of these network features on targeted intervention (e.g., self-isolation
and quarantine). The targeted individuals for self-isolation are based on
centrality measures and node influence metrics. Compared to our previous works
on scale-free networks, small-world networks are considered in this paper.
Small-world networks resemble real-world social and human networks. In this
type of network, most nodes are not directly connected but can be reached
through a few intermediaries (known as the small-worldness property). Real
social networks, such as friendship networks, also exhibit this small-worldness
property, where most people are connected through a relatively small number of
intermediaries. We particularly study the epidemic curve flattening by
centrality-based interventions/isolation over small-world networks. Our results
show that high clustering while having low small-worldness (higher shortest
path characteristics) implies flatter infection curves. In reality, a flatter
infection curve implies that the number of new cases of a disease is spread out
over a longer period of time, rather than a sharp and sudden increase in cases
(a peak in epidemic). In turn, this reduces the strain on healthcare resources
and helps to relieve the healthcare services.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12595" title="Abstract">arXiv:2310.12595</a> [<a href="/pdf/2310.12595" title="Download PDF">pdf</a>, <a href="/format/2310.12595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Similarity-Based Hierarchical Bayesian Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wharrie%2C+S">Sophie Wharrie</a>, 
<a href="/search/cs?searchtype=author&query=Kaski%2C+S">Samuel Kaski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The key challenge underlying machine learning is generalisation to new data.
This work studies generalisation for datasets consisting of related tasks that
may differ in causal mechanisms. For example, observational medical data for
complex diseases suffers from heterogeneity in causal mechanisms of disease
across patients, creating challenges for machine learning algorithms that need
to generalise to new patients outside of the training dataset. Common
approaches for learning supervised models with heterogeneous datasets include
learning a global model for the entire dataset, learning local models for each
tasks' data, or utilising hierarchical, meta-learning and multi-task learning
approaches to learn how to generalise from data pooled across multiple tasks.
In this paper we propose causal similarity-based hierarchical Bayesian models
to improve generalisation to new tasks by learning how to pool data from
training tasks with similar causal mechanisms. We apply this general modelling
principle to Bayesian neural networks and compare a variety of methods for
estimating causal task similarity (for both known and unknown causal models).
We demonstrate the benefits of our approach and applicability to real world
problems through a range of experiments on simulated and real data.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12598" title="Abstract">arXiv:2310.12598</a> [<a href="/pdf/2310.12598" title="Download PDF">pdf</a>, <a href="/format/2310.12598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is More? An Empirical Study on Configuration Issues in Python PyPI  Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruida Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruoke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Cuiyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by ICSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Python is widely used in the open-source community, largely owing to the
extensive support from diverse third-party libraries within the PyPI ecosystem.
Nevertheless, the utilization of third-party libraries can potentially lead to
conflicts in dependencies, prompting researchers to develop dependency conflict
detectors. Moreover, endeavors have been made to automatically infer
dependencies. These approaches focus on version-level checks and inference,
based on the assumption that configurations of libraries in the PyPI ecosystem
are correct. However, our study reveals that this assumption is not universally
valid, and relying solely on version-level checks proves inadequate in ensuring
compatible run-time environments. In this paper, we conduct an empirical study
to comprehensively study the configuration issues in the PyPI ecosystem.
Specifically, we propose PyCon, a source-level detector, for detecting
potential configuration issues. PyCon employs three distinct checks, targeting
the setup, packing, and usage stages of libraries, respectively. To evaluate
the effectiveness of the current automatic dependency inference approaches, we
build a benchmark called VLibs, comprising library releases that pass all three
checks of PyCon. We identify 15 kinds of configuration issues and find that
183,864 library releases suffer from potential configuration issues.
Remarkably, 68% of these issues can only be detected via the source-level
check. Our experiment results show that the most advanced automatic dependency
inference approach, PyEGo, can successfully infer dependencies for only 65% of
library releases. The primary failures stem from dependency conflicts and the
absence of required libraries in the generated configurations. Based on the
empirical results, we derive six findings and draw two implications for
open-source developers and future research in automatic dependency inference.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12600" title="Abstract">arXiv:2310.12600</a> [<a href="/pdf/2310.12600" title="Download PDF">pdf</a>, <a href="/format/2310.12600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FUSC: Fetal Ultrasound Semantic Clustering of Second Trimester Scans  Using Deep Self-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alasmawi%2C+H">Hussain Alasmawi</a>, 
<a href="/search/cs?searchtype=author&query=Bricker%2C+L">Leanne Bricker</a>, 
<a href="/search/cs?searchtype=author&query=Yaqub%2C+M">Mohammad Yaqub</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Ultrasound is the primary imaging modality in clinical practice during
pregnancy. More than 140M fetuses are born yearly, resulting in numerous scans.
The availability of a large volume of fetal ultrasound scans presents the
opportunity to train robust machine learning models. However, the abundance of
scans also has its challenges, as manual labeling of each image is needed for
supervised methods. Labeling is typically labor-intensive and requires
expertise to annotate the images accurately. This study presents an
unsupervised approach for automatically clustering ultrasound images into a
large range of fetal views, reducing or eliminating the need for manual
labeling. Our Fetal Ultrasound Semantic Clustering (FUSC) method is developed
using a large dataset of 88,063 images and further evaluated on an additional
unseen dataset of 8,187 images achieving over 92% clustering purity. The result
of our investigation hold the potential to significantly impact the field of
fetal ultrasound imaging and pave the way for more advanced automated labeling
solutions. Finally, we make the code and the experimental setup publicly
available to help advance the field.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12605" title="Abstract">arXiv:2310.12605</a> [<a href="/pdf/2310.12605" title="Download PDF">pdf</a>, <a href="/ps/2310.12605" title="Download PostScript">ps</a>, <a href="/format/2310.12605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate Coarse Residual for Two-Level Asynchronous Domain Decomposition  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gbikpi-Benissan%2C+G">Guillaume Gbikpi-Benissan</a>, 
<a href="/search/math?searchtype=author&query=Magoul%C3%A8s%2C+F">Fr&#xe9;d&#xe9;ric Magoul&#xe8;s</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Recently, asynchronous coarse-space correction has been achieved within both
the overlapping Schwarz and the primal Schur frameworks. Both additive and
multiplicative corrections have been discussed. In this paper, we address some
implementation drawbacks of the proposed additive correction scheme. In the
existing approach, each coarse solution is applied only once, leaving most of
the iterations of the solver without coarse-space information while building
the right-hand side of the coarse problem. Moreover, one-sided routines of the
Message Passing Interface (MPI) standard were considered, which introduced the
need for a sleep statement in the iterations loop of the coarse solver. This
implies a tuning of the sleep period, which is a non-discrete quantity. In this
paper, we improve the accuracy of the coarse right-hand side, which allowed for
more frequent corrections. In addition, we highlight a two-sided implementation
which better suits the asynchronous coarse-space correction scheme. Numerical
experiments show a significant performance gain with such increased
incorporation of the coarse space.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12609" title="Abstract">arXiv:2310.12609</a> [<a href="/pdf/2310.12609" title="Download PDF">pdf</a>, <a href="/format/2310.12609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Heat-inspired Diffusion with Insulators for Collision Free  Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Junwoo Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+H">Hyunwoo Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Soochul Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Joohwan Seo</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+N">Nikhil Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jongeun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Horowitz%2C+R">Roberto Horowitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models have risen as a powerful tool in robotics due to their
flexibility and multi-modality. While some of these methods effectively address
complex problems, they often depend heavily on inference-time obstacle
detection and require additional equipment. Addressing these challenges, we
present a method that, during inference time, simultaneously generates only
reachable goals and plans motions that avoid obstacles, all from a single
visual input. Central to our approach is the novel use of a collision-avoiding
diffusion kernel for training. Through evaluations against behavior-cloning and
classical diffusion models, our framework has proven its robustness. It is
particularly effective in multi-modal environments, navigating toward goals and
avoiding unreachable ones blocked by obstacles, while ensuring collision
avoidance.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12611" title="Abstract">arXiv:2310.12611</a> [<a href="/pdf/2310.12611" title="Download PDF">pdf</a>, <a href="/format/2310.12611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and Adapting Transformer-Components Responsible for Gender  Bias in an English Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chintam%2C+A">Abhijith Chintam</a>, 
<a href="/search/cs?searchtype=author&query=Beloch%2C+R">Rahel Beloch</a>, 
<a href="/search/cs?searchtype=author&query=Zuidema%2C+W">Willem Zuidema</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+M">Michael Hanna</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Wal%2C+O">Oskar van der Wal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at BlackboxNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models (LMs) exhibit and amplify many types of undesirable biases
learned from the training data, including gender bias. However, we lack tools
for effectively and efficiently changing this behavior without hurting general
language modeling performance. In this paper, we study three methods for
identifying causal relations between LM components and particular output:
causal mediation analysis, automated circuit discovery and our novel, efficient
method called DiffMask+ based on differential masking. We apply the methods to
GPT-2 small and the problem of gender bias, and use the discovered sets of
components to perform parameter-efficient fine-tuning for bias mitigation. Our
results show significant overlap in the identified components (despite huge
differences in the computational requirements of the methods) as well as
success in mitigating gender bias, with less damage to general language
modeling compared to full model fine-tuning. However, our work also underscores
the difficulty of defining and measuring bias, and the sensitivity of causal
discovery procedures to dataset choice. We hope our work can contribute to more
attention for dataset development, and lead to more effective mitigation
strategies for other types of bias.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12612" title="Abstract">arXiv:2310.12612</a> [<a href="/pdf/2310.12612" title="Download PDF">pdf</a>, <a href="/format/2310.12612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How a student becomes a teacher: learning and forgetting through  Spectral methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giambagli%2C+L">Lorenzo Giambagli</a>, 
<a href="/search/cs?searchtype=author&query=Buffoni%2C+L">Lorenzo Buffoni</a>, 
<a href="/search/cs?searchtype=author&query=Chicchi%2C+L">Lorenzo Chicchi</a>, 
<a href="/search/cs?searchtype=author&query=Fanelli%2C+D">Duccio Fanelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages + references + supplemental material. Poster presentation at NeurIPS 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
<p class="mathjax">In theoretical ML, the teacher-student paradigm is often employed as an
effective metaphor for real-life tuition. The above scheme proves particularly
relevant when the student network is overparameterized as compared to the
teacher network. Under these operating conditions, it is tempting to speculate
that the student ability to handle the given task could be eventually stored in
a sub-portion of the whole network. This latter should be to some extent
reminiscent of the frozen teacher structure, according to suitable metrics,
while being approximately invariant across different architectures of the
student candidate network. Unfortunately, state-of-the-art conventional
learning techniques could not help in identifying the existence of such an
invariant subnetwork, due to the inherent degree of non-convexity that
characterizes the examined problem. In this work, we take a leap forward by
proposing a radically different optimization scheme which builds on a spectral
representation of the linear transfer of information between layers. The
gradient is hence calculated with respect to both eigenvalues and eigenvectors
with negligible increase in terms of computational and complexity load, as
compared to standard training algorithms. Working in this framework, we could
isolate a stable student substructure, that mirrors the true complexity of the
teacher in terms of computing neurons, path distribution and topological
attributes. When pruning unimportant nodes of the trained student, as follows a
ranking that reflects the optimized eigenvalues, no degradation in the recorded
performance is seen above a threshold that corresponds to the effective teacher
size. The observed behavior can be pictured as a genuine second-order phase
transition that bears universality traits.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12613" title="Abstract">arXiv:2310.12613</a> [<a href="/pdf/2310.12613" title="Download PDF">pdf</a>, <a href="/ps/2310.12613" title="Download PostScript">ps</a>, <a href="/format/2310.12613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Normalization of Linear Temporal Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esparza%2C+J">Javier Esparza</a>, 
<a href="/search/cs?searchtype=author&query=Rubio%2C+R">Rub&#xe9;n Rubio</a>, 
<a href="/search/cs?searchtype=author&query=Sickert%2C+S">Salomon Sickert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to J. ACM. arXiv admin note: text overlap with <a href="/abs/2304.08872">arXiv:2304.08872</a>, <a href="/abs/2005.00472">arXiv:2005.00472</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In the mid 80s, Lichtenstein, Pnueli, and Zuck proved a classical theorem
stating that every formula of Past LTL (the extension of LTL with past
operators) is equivalent to a formula of the form $\bigwedge_{i=1}^n
\mathbf{G}\mathbf{F}\, \varphi_i \vee \mathbf{F}\mathbf{G}\, \psi_i $, where
$\varphi_i$ and $\psi_i$ contain only past operators. Some years later, Chang,
Manna, and Pnueli built on this result to derive a similar normal form for LTL.
Both normalization procedures have a non-elementary worst-case blow-up, and
follow an involved path from formulas to counter-free automata to star-free
regular expressions and back to formulas. We improve on both points. We present
direct and purely syntactic normalization procedures for LTL, yielding a normal
form very similar to the one by Chang, Manna, and Pnueli, that exhibit only a
single exponential blow-up. As an application, we derive a simple algorithm to
translate LTL into deterministic Rabin automata. The algorithm normalizes the
formula, translates it into a special very weak alternating automaton, and
applies a simple determinization procedure, valid only for these special
automata.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12615" title="Abstract">arXiv:2310.12615</a> [<a href="/pdf/2310.12615" title="Download PDF">pdf</a>, <a href="/format/2310.12615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Asynchronous Graph Characterization of Byzantine Firing Rebels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galeana%2C+H+R">Hugo Rincon Galeana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Discrete Mathematics (cs.DM); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Byzantine agreement tasks have been studied extensively through many diverse
frameworks ranging from epistemic modal logic to combinatorial, topological and
even game theoretical approaches. Among byzantine agreement tasks, firing
rebels with relay is of particular interest, since it is a basic primitive
necessary for solving more complex tasks such as Consistent Broadcast. The
epistemic logic approach has yielded both necessary conditions and sufficient
knowledge conditions for solving firing rebels with relay. However, these
conditions are stated in terms of knowledge and in principle do not explore the
conditions on the communication structure which is often assumed to be
complete. That is, any process is assumed to be capable of communicating with
any other process at any time.
<br />In this paper, we characterize byzantine firing rebels solvability with and
without relay in terms of the communication structure of the system. We define
a relation between asynchronous message schedules and directed graph sequences,
which we call network abstractions. This allows us to capture the message relay
capabilities of the system into a combinatorial object. Although there are some
similarities between network abstractions and causal cones, there is a
fundamental difference. Namely, causal cones allow only to look at events in
the past, while network abstractions allow us to reason about future
possibilities. Thus enabling us to reason about liveness properties, which can
not be expressed by looking only at past events.
<br />Furthermore, we formalize our characterization by using a temporal epistemic
logic for byzantine systems. Such formulation constitutes the necessary and
sufficient a-priori knowledge regarding network connectivity for solving firing
rebels with relay.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12616" title="Abstract">arXiv:2310.12616</a> [<a href="/pdf/2310.12616" title="Download PDF">pdf</a>, <a href="/format/2310.12616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-attention Spatio-temporal Context Transformer for Semantic  Segmentation of Historical Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Sidi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yizi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+K">Konrad Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Hurni%2C+L">Lorenz Hurni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Historical maps provide useful spatio-temporal information on the Earth's
surface before modern earth observation techniques came into being. To extract
information from maps, neural networks, which gain wide popularity in recent
years, have replaced hand-crafted map processing methods and tedious manual
labor. However, aleatoric uncertainty, known as data-dependent uncertainty,
inherent in the drawing/scanning/fading defects of the original map sheets and
inadequate contexts when cropping maps into small tiles considering the memory
limits of the training process, challenges the model to make correct
predictions. As aleatoric uncertainty cannot be reduced even with more training
data collected, we argue that complementary spatio-temporal contexts can be
helpful. To achieve this, we propose a U-Net-based network that fuses
spatio-temporal features with cross-attention transformers (U-SpaTem),
aggregating information at a larger spatial range as well as through a temporal
sequence of images. Our model achieves a better performance than other
state-or-art models that use either temporal or spatial contexts. Compared with
pure vision transformers, our model is more lightweight and effective. To the
best of our knowledge, leveraging both spatial and temporal contexts have been
rarely explored before in the segmentation task. Even though our application is
on segmenting historical maps, we believe that the method can be transferred
into other fields with similar problems like temporal sequences of satellite
images. Our code is freely accessible at
https://github.com/chenyizi086/wu.2023.sigspatial.git.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12620" title="Abstract">arXiv:2310.12620</a> [<a href="/pdf/2310.12620" title="Download PDF">pdf</a>, <a href="/format/2310.12620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predict the Future from the Past? On the Temporal Data Distribution  Shift in Financial Sentiment Classifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chenxi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Temporal data distribution shift is prevalent in the financial text. How can
a financial sentiment analysis system be trained in a volatile market
environment that can accurately infer sentiment and be robust to temporal data
distribution shifts? In this paper, we conduct an empirical study on the
financial sentiment analysis system under temporal data distribution shifts
using a real-world financial social media dataset that spans three years. We
find that the fine-tuned models suffer from general performance degradation in
the presence of temporal distribution shifts. Furthermore, motivated by the
unique temporal nature of the financial text, we propose a novel method that
combines out-of-distribution detection with time series modeling for temporal
financial sentiment analysis. Experimental results show that the proposed
method enhances the model's capability to adapt to evolving temporal shifts in
a volatile financial market.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12622" title="Abstract">arXiv:2310.12622</a> [<a href="/pdf/2310.12622" title="Download PDF">pdf</a>, <a href="/format/2310.12622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video  Requesting and Edge Caching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Linchang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yipeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Miao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">As users conveniently stream their favored online videos, video request
records will be automatically seized by video content providers, which may leak
users' privacy. Unfortunately, most existing privacy-enhancing approaches are
not applicable for protecting users' privacy in requests, which cannot be
easily altered or distorted by users and must be visible for content providers
to stream correct videos. To preserve request privacy in online video services,
it is possible to request additional videos irrelevant to users' interests so
that content providers cannot precisely infer users' interest information.
However, a naive redundant requesting approach will significantly degrade the
performance of edge caches and increase bandwidth overhead accordingly. In this
paper, we are among the first to propose a Cache-Friendly Redundant Video
Requesting (cRVR) algorithm for User Devices (UDs) and its corresponding
caching algorithm for the Edge Cache (EC), which can effectively mitigate the
problem of request privacy leakage with minimal impact on the EC's performance.
To solve the problem, we develop a Stackelberg game to analyze the dedicated
interaction between UDs and EC and obtain their optimal strategies to maximize
their respective utility. For UDs, the utility function is a combination of
both video playback utility and privacy protection utility. We theoretically
prove the existence and uniqueness of the equilibrium of the Stackelberg game.
In the end, extensive experiments are conducted with real traces to demonstrate
that cRVR can effectively protect video request privacy by reducing up to
57.96\% of privacy disclosure compared to baseline algorithms. Meanwhile, the
caching performance of ECs is only slightly affected.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12629" title="Abstract">arXiv:2310.12629</a> [<a href="/pdf/2310.12629" title="Download PDF">pdf</a>, <a href="/format/2310.12629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Improved Metarounding Algorithm via Frank-Wolfe
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitsuboshi%2C+R">Ryotaro Mitsuboshi</a>, 
<a href="/search/cs?searchtype=author&query=Hatano%2C+K">Kohei Hatano</a>, 
<a href="/search/cs?searchtype=author&query=Takimoto%2C+E">Eiji Takimoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Metarounding is an approach to convert an approximation algorithm for linear
optimization over some combinatorial classes to an online linear optimization
algorithm for the same class. We propose a new metarounding algorithm under a
natural assumption that a relax-based approximation algorithm exists for the
combinatorial class. Our algorithm is much more efficient in both theoretical
and practical aspects.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12630" title="Abstract">arXiv:2310.12630</a> [<a href="/pdf/2310.12630" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heart Disease Detection using Vision-Based Transformer Models from ECG  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kilimci%2C+Z+H">Zeynep Hilal Kilimci</a>, 
<a href="/search/cs?searchtype=author&query=Yalcin%2C+M">Mustafa Yalcin</a>, 
<a href="/search/cs?searchtype=author&query=Kucukmanisa%2C+A">Ayhan Kucukmanisa</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A+K">Amit Kumar Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Heart disease, also known as cardiovascular disease, is a prevalent and
critical medical condition characterized by the impairment of the heart and
blood vessels, leading to various complications such as coronary artery
disease, heart failure, and myocardial infarction. The timely and accurate
detection of heart disease is of paramount importance in clinical practice.
Early identification of individuals at risk enables proactive interventions,
preventive measures, and personalized treatment strategies to mitigate the
progression of the disease and reduce adverse outcomes. In recent years, the
field of heart disease detection has witnessed notable advancements due to the
integration of sophisticated technologies and computational approaches. These
include machine learning algorithms, data mining techniques, and predictive
modeling frameworks that leverage vast amounts of clinical and physiological
data to improve diagnostic accuracy and risk stratification. In this work, we
propose to detect heart disease from ECG images using cutting-edge
technologies, namely vision transformer models. These models are Google-Vit,
Microsoft-Beit, and Swin-Tiny. To the best of our knowledge, this is the
initial endeavor concentrating on the detection of heart diseases through
image-based ECG data by employing cuttingedge technologies namely, transformer
models. To demonstrate the contribution of the proposed framework, the
performance of vision transformer models are compared with state-of-the-art
studies. Experiment results show that the proposed framework exhibits
remarkable classification results.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12632" title="Abstract">arXiv:2310.12632</a> [<a href="/pdf/2310.12632" title="Download PDF">pdf</a>, <a href="/format/2310.12632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Deep Learning-based Online Quality Prediction System for  Welding Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahn%2C+Y">Yannik Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Maack%2C+R">Robert Maack</a>, 
<a href="/search/cs?searchtype=author&query=Buchholz%2C+G">Guido Buchholz</a>, 
<a href="/search/cs?searchtype=author&query=Purrio%2C+M">Marion Purrio</a>, 
<a href="/search/cs?searchtype=author&query=Angerhausen%2C+M">Matthias Angerhausen</a>, 
<a href="/search/cs?searchtype=author&query=Tercan%2C+H">Hasan Tercan</a>, 
<a href="/search/cs?searchtype=author&query=Meisen%2C+T">Tobias Meisen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for CIRP CMS '23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The digitization of manufacturing processes enables promising applications
for machine learning-assisted quality assurance. A widely used manufacturing
process that can strongly benefit from data-driven solutions is \ac{GMAW}. The
welding process is characterized by complex cause-effect relationships between
material properties, process conditions and weld quality. In non-laboratory
environments with frequently changing process parameters, accurate
determination of weld quality by destructive testing is economically
unfeasible. Deep learning offers the potential to identify the relationships in
available process data and predict the weld quality from process observations.
In this paper, we present a concept for a deep learning based predictive
quality system in \ac{GMAW}. At its core, the concept involves a pipeline
consisting of four major phases: collection and management of multi-sensor data
(e.g. current and voltage), real-time processing and feature engineering of the
time series data by means of autoencoders, training and deployment of suitable
recurrent deep learning models for quality predictions, and model evolutions
under changing process conditions using continual learning. The concept
provides the foundation for future research activities in which we will realize
an online predictive quality system for running production.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12638" title="Abstract">arXiv:2310.12638</a> [<a href="/pdf/2310.12638" title="Download PDF">pdf</a>, <a href="/format/2310.12638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph  Question-Answering Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akl%2C+H+A">Hanna Abi Akl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, 2 tables, accepted for the Scholarly-QALD challenge at the International Semantic Web Conference (ISWC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The Scholarly Question Answering over Linked Data (Scholarly QALD) at The
International Semantic Web Conference (ISWC) 2023 challenge presents two
sub-tasks to tackle question answering (QA) over knowledge graphs (KGs). We
answer the KGQA over DBLP (DBLP-QUAD) task by proposing a neuro-symbolic (NS)
framework based on PSYCHIC, an extractive QA model capable of identifying the
query and entities related to a KG question. Our system achieved a F1 score of
00.18% on question answering and came in third place for entity linking (EL)
with a score of 71.00%.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12640" title="Abstract">arXiv:2310.12640</a> [<a href="/pdf/2310.12640" title="Download PDF">pdf</a>, <a href="/format/2310.12640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Autoregressive Sentence Ordering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bin%2C+Y">Yi Bin</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenhao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+B">Bin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yujuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing sentence ordering approaches generally employ encoder-decoder
frameworks with the pointer net to recover the coherence by recurrently
predicting each sentence step-by-step. Such an autoregressive manner only
leverages unilateral dependencies during decoding and cannot fully explore the
semantic dependency between sentences for ordering. To overcome these
limitations, in this paper, we propose a novel Non-Autoregressive Ordering
Network, dubbed \textit{NAON}, which explores bilateral dependencies between
sentences and predicts the sentence for each position in parallel. We claim
that the non-autoregressive manner is not just applicable but also particularly
suitable to the sentence ordering task because of two peculiar characteristics
of the task: 1) each generation target is in deterministic length, and 2) the
sentences and positions should match exclusively. Furthermore, to address the
repetition issue of the naive non-autoregressive Transformer, we introduce an
exclusive loss to constrain the exclusiveness between positions and sentences.
To verify the effectiveness of the proposed model, we conduct extensive
experiments on several common-used datasets and the experimental results show
that our method outperforms all the autoregressive approaches and yields
competitive performance compared with the state-of-the-arts. The codes are
available at:
\url{https://github.com/steven640pixel/nonautoregressive-sentence-ordering}.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12648" title="Abstract">arXiv:2310.12648</a> [<a href="/pdf/2310.12648" title="Download PDF">pdf</a>, <a href="/format/2310.12648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Real-World Streaming Speech Translation for Code-Switched Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alastruey%2C+B">Belen Alastruey</a>, 
<a href="/search/cs?searchtype=author&query=Sperber%2C+M">Matthias Sperber</a>, 
<a href="/search/cs?searchtype=author&query=Gollan%2C+C">Christian Gollan</a>, 
<a href="/search/cs?searchtype=author&query=Telaar%2C+D">Dominic Telaar</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+T">Tim Ng</a>, 
<a href="/search/cs?searchtype=author&query=Agargwal%2C+A">Aashish Agargwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Code-switching (CS), i.e. mixing different languages in a single sentence, is
a common phenomenon in communication and can be challenging in many Natural
Language Processing (NLP) settings. Previous studies on CS speech have shown
promising results for end-to-end speech translation (ST), but have been limited
to offline scenarios and to translation to one of the languages present in the
source (\textit{monolingual transcription}).
<br />In this paper, we focus on two essential yet unexplored areas for real-world
CS speech translation: streaming settings, and translation to a third language
(i.e., a language not included in the source). To this end, we extend the
Fisher and Miami test and validation datasets to include new targets in Spanish
and German. Using this data, we train a model for both offline and streaming ST
and we establish baseline results for the two settings mentioned earlier.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12650" title="Abstract">arXiv:2310.12650</a> [<a href="/pdf/2310.12650" title="Download PDF">pdf</a>, <a href="/format/2310.12650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hibikino-Musashi@Home 2023 Team Description Paper
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shiba%2C+T">Tomoya Shiba</a>, 
<a href="/search/cs?searchtype=author&query=Mizutani%2C+A">Akinobu Mizutani</a>, 
<a href="/search/cs?searchtype=author&query=Yano%2C+Y">Yuga Yano</a>, 
<a href="/search/cs?searchtype=author&query=Ono%2C+T">Tomohiro Ono</a>, 
<a href="/search/cs?searchtype=author&query=Tokuno%2C+S">Shoshi Tokuno</a>, 
<a href="/search/cs?searchtype=author&query=Kanaoka%2C+D">Daiju Kanaoka</a>, 
<a href="/search/cs?searchtype=author&query=Fukuda%2C+Y">Yukiya Fukuda</a>, 
<a href="/search/cs?searchtype=author&query=Amano%2C+H">Hayato Amano</a>, 
<a href="/search/cs?searchtype=author&query=Koresawa%2C+M">Mayu Koresawa</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+Y">Yoshifumi Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Takemoto%2C+R">Ryogo Takemoto</a>, 
<a href="/search/cs?searchtype=author&query=Tamai%2C+K">Katsunori Tamai</a>, 
<a href="/search/cs?searchtype=author&query=Nakahara%2C+K">Kazuo Nakahara</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+H">Hiroyuki Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Fujimatsu%2C+S">Satsuki Fujimatsu</a>, 
<a href="/search/cs?searchtype=author&query=Mizoguchi%2C+Y">Yusuke Mizoguchi</a>, 
<a href="/search/cs?searchtype=author&query=Anraku%2C+M">Moeno Anraku</a>, 
<a href="/search/cs?searchtype=author&query=Suzuka%2C+M">Mayo Suzuka</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Maeda%2C+K">Kohei Maeda</a>, 
<a href="/search/cs?searchtype=author&query=Matsuzaki%2C+F">Fumiya Matsuzaki</a>, 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+I">Ikuya Matsumoto</a>, 
<a href="/search/cs?searchtype=author&query=Murai%2C+K">Kazuya Murai</a>, 
<a href="/search/cs?searchtype=author&query=Isomoto%2C+K">Kosei Isomoto</a>, 
<a href="/search/cs?searchtype=author&query=Minje%2C+K">Kim Minje</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+Y">Yuichiro Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Morie%2C+T">Takashi Morie</a>, 
<a href="/search/cs?searchtype=author&query=Tamukoh%2C+H">Hakaru Tamukoh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper describes an overview of the techniques of Hibikino-Musashi@Home,
which intends to participate in the domestic standard platform league. The team
has developed a dataset generator for the training of a robot vision system and
an open-source development environment running on a human support robot
simulator. The robot system comprises self-developed libraries including those
for motion synthesis and open-source software works on the robot operating
system. The team aims to realize a home service robot that assists humans in a
home, and continuously attend the competition to evaluate the developed system.
The brain-inspired artificial intelligence system is also proposed for service
robots which are expected to work in a real home environment.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12657" title="Abstract">arXiv:2310.12657</a> [<a href="/pdf/2310.12657" title="Download PDF">pdf</a>, <a href="/format/2310.12657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4-Cycle Free Spatially Coupled LDPC Codes with an Explicit Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roostaie%2C+Z">Zeinab Roostaie</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+M">Mohammad Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Parvaresh%2C+F">Farzad Parvaresh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Spatially coupled low-density parity-check (SC-LDPC) codes are a class of
capacity approaching LDPC codes with low message recovery latency when a
sliding window decoding is used. In this paper, we first present a new method
for the construction of a class of SC-LDPC codes by the incidence matrices of a
given non-negative integer matrix $E$, and then the relationship of 4-cycles
between matrix $E$ and the corresponding SC-LDPC code are investigated.
Finally, by defining a new class of integer finite sequences, called {\it good
sequences}, for the first time, we give an explicit method for the construction
of a class of 4-cycle free SC-LDPC codes that can achieve (in most cases) the
minimum coupling width.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12658" title="Abstract">arXiv:2310.12658</a> [<a href="/pdf/2310.12658" title="Download PDF">pdf</a>, <a href="/format/2310.12658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> phyloDB: A framework for large-scale phylogenetic analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Louren%C3%A7o%2C+B">Bruno Louren&#xe7;o</a>, 
<a href="/search/cs?searchtype=author&query=Vaz%2C+C">C&#xe1;tia Vaz</a>, 
<a href="/search/cs?searchtype=author&query=Coimbra%2C+M+E">Miguel E. Coimbra</a>, 
<a href="/search/cs?searchtype=author&query=Francisco%2C+A+P">Alexandre P. Francisco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2012.13363">arXiv:2012.13363</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">phyloDB is a modular and extensible framework for large-scale phylogenetic
analyses, which are essential for understanding epidemics evolution. It relies
on the Neo4j graph database for data storage and processing, providing a schema
and an API for representing and querying phylogenetic data. Custom algorithms
are also supported, allowing to perform heavy computations directly over the
data, and to store results in the database. Multiple computation results are
stored as multilayer networks, promoting and facilitating comparative analyses,
as well as avoiding unnecessary ab initio computations. The experimental
evaluation results showcase that phyloDB is efficient and scalable with respect
to both API operations and algorithms execution.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12659" title="Abstract">arXiv:2310.12659</a> [<a href="/pdf/2310.12659" title="Download PDF">pdf</a>, <a href="/format/2310.12659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparison Of Direct Solvers In FROSch Applied To Chemo-Mechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Heinlein%2C+A">Alexander Heinlein</a>, 
<a href="/search/math?searchtype=author&query=Kiefer%2C+B">Bjoern Kiefer</a>, 
<a href="/search/math?searchtype=author&query=Pr%C3%BCger%2C+S">Stefan Pr&#xfc;ger</a>, 
<a href="/search/math?searchtype=author&query=Rheinbach%2C+O">Oliver Rheinbach</a>, 
<a href="/search/math?searchtype=author&query=R%C3%B6ver%2C+F">Friederike R&#xf6;ver</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Sparse direct linear solvers are at the computational core of domain
decomposition preconditioners and therefore have a strong impact on their
performance. In this paper, we consider the Fast and Robust Overlapping Schwarz
(FROSch) solver framework of the Trilinos software library, which contains a
parallel implementations of the GDSW domain decomposition preconditioner. We
compare three different sparse direct solvers used to solve the subdomain
problems in FROSch. The preconditioner is applied to different model problems;
linear elasticity and more complex fully-coupled deformation diffusion-boundary
value problems from chemo-mechanics. We employ FROSch in fully algebraic mode,
and therefore, we do not expect numerical scalability. Strong scalability is
studied from 64 to 4096 cores, where good scaling results are obtained up to
1728 cores. The increasing size of the coarse problem increases the solution
time for all sparse direct solvers.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12660" title="Abstract">arXiv:2310.12660</a> [<a href="/pdf/2310.12660" title="Download PDF">pdf</a>, <a href="/format/2310.12660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Descent Fails to Learn High-frequency Functions and Modular  Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takhanov%2C+R">Rustem Takhanov</a>, 
<a href="/search/cs?searchtype=author&query=Tezekbayev%2C+M">Maxat Tezekbayev</a>, 
<a href="/search/cs?searchtype=author&query=Pak%2C+A">Artur Pak</a>, 
<a href="/search/cs?searchtype=author&query=Bolatov%2C+A">Arman Bolatov</a>, 
<a href="/search/cs?searchtype=author&query=Assylbekov%2C+Z">Zhenisbek Assylbekov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Classes of target functions containing a large number of approximately
orthogonal elements are known to be hard to learn by the Statistical Query
algorithms. Recently this classical fact re-emerged in a theory of
gradient-based optimization of neural networks. In the novel framework, the
hardness of a class is usually quantified by the variance of the gradient with
respect to a random choice of a target function.
<br />A set of functions of the form $x\to ax \bmod p$, where $a$ is taken from
${\mathbb Z}_p$, has attracted some attention from deep learning theorists and
cryptographers recently. This class can be understood as a subset of
$p$-periodic functions on ${\mathbb Z}$ and is tightly connected with a class
of high-frequency periodic functions on the real line.
<br />We present a mathematical analysis of limitations and challenges associated
with using gradient-based learning techniques to train a high-frequency
periodic function or modular multiplication from examples. We highlight that
the variance of the gradient is negligibly small in both cases when either a
frequency or the prime base $p$ is large. This in turn prevents such a learning
algorithm from being successful.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12663" title="Abstract">arXiv:2310.12663</a> [<a href="/pdf/2310.12663" title="Download PDF">pdf</a>, <a href="/format/2310.12663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge from Uncertainty in Evidential Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davies%2C+C">Cai Davies</a>, 
<a href="/search/cs?searchtype=author&query=Vilamala%2C+M+R">Marc Roig Vilamala</a>, 
<a href="/search/cs?searchtype=author&query=Preece%2C+A+D">Alun D. Preece</a>, 
<a href="/search/cs?searchtype=author&query=Cerutti%2C+F">Federico Cerutti</a>, 
<a href="/search/cs?searchtype=author&query=Kaplan%2C+L+M">Lance M. Kaplan</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Supriyo Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work reveals an evidential signal that emerges from the uncertainty
value in Evidential Deep Learning (EDL). EDL is one example of a class of
uncertainty-aware deep learning approaches designed to provide confidence (or
epistemic uncertainty) about the current test sample. In particular for
computer vision and bidirectional encoder large language models, the
`evidential signal' arising from the Dirichlet strength in EDL can, in some
cases, discriminate between classes, which is particularly strong when using
large language models. We hypothesise that the KL regularisation term causes
EDL to couple aleatoric and epistemic uncertainty. In this paper, we
empirically investigate the correlations between misclassification and
evaluated uncertainty, and show that EDL's `evidential signal' is due to
misclassification bias. We critically evaluate EDL with other Dirichlet-based
approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior
Networks, and show theoretically and empirically the differences between these
loss functions. We conclude that EDL's coupling of uncertainty arises from
these differences due to the use (or lack) of out-of-distribution samples
during training.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12664" title="Abstract">arXiv:2310.12664</a> [<a href="/pdf/2310.12664" title="Download PDF">pdf</a>, <a href="/format/2310.12664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT a Financial Expert? Evaluating Language Models on Financial  Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 (short paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The emergence of Large Language Models (LLMs), such as ChatGPT, has
revolutionized general natural language preprocessing (NLP) tasks. However,
their expertise in the financial domain lacks a comprehensive evaluation. To
assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval,
a framework for Financial Language Model Evaluation, comprising nine datasets
designed to evaluate the performance of language models. This study compares
the performance of encoder-only language models and the decoder-only language
models. Our findings reveal that while some decoder-only LLMs demonstrate
notable performance across most financial tasks via zero-shot prompting, they
generally lag behind the fine-tuned expert models, especially when dealing with
proprietary datasets. We hope this study provides foundation evaluations for
continuing efforts to build more advanced LLMs in the financial domain.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12665" title="Abstract">arXiv:2310.12665</a> [<a href="/pdf/2310.12665" title="Download PDF">pdf</a>, <a href="/format/2310.12665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xinlei He</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 33rd USENIX Security Symposium, August 2024, Philadelphia, PA, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">While advanced machine learning (ML) models are deployed in numerous
real-world applications, previous works demonstrate these models have security
and privacy vulnerabilities. Various empirical research has been done in this
field. However, most of the experiments are performed on target ML models
trained by the security researchers themselves. Due to the high computational
resource requirement for training advanced models with complex architectures,
researchers generally choose to train a few target models using relatively
simple architectures on typical experiment datasets. We argue that to
understand ML models' vulnerabilities comprehensively, experiments should be
performed on a large set of models trained with various purposes (not just the
purpose of evaluating ML attacks and defenses). To this end, we propose using
publicly available models with weights from the Internet (public models) for
evaluating attacks and defenses on ML models. We establish a database, namely
SecurityNet, containing 910 annotated image classification models. We then
analyze the effectiveness of several representative attacks/defenses, including
model stealing attacks, membership inference attacks, and backdoor detection on
these public models. Our evaluation empirically shows the performance of these
attacks/defenses can vary significantly on public models compared to
self-trained models. We share SecurityNet with the research community. and
advocate researchers to perform experiments on public models to better
demonstrate their proposed methods' effectiveness in the future.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12666" title="Abstract">arXiv:2310.12666</a> [<a href="/pdf/2310.12666" title="Download PDF">pdf</a>, <a href="/format/2310.12666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Who Benefits from a Multi-Cloud Market? A Trading Networks Based  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wasserkrug%2C+S">Segev Wasserkrug</a>, 
<a href="/search/cs?searchtype=author&query=Osogami%2C+T">Takayuki Osogami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">In enterprise cloud computing, there is a big and increasing investment to
move to multi-cloud computing, which allows enterprises to seamlessly utilize
IT resources from multiple cloud providers, so as to take advantage of
different cloud providers' capabilities and costs. This investment raises
several key questions: Will multi-cloud always be more beneficial to the cloud
users? How will this impact the cloud providers? Is it possible to create a
multi-cloud market that is beneficial to all participants?
<br />In this work, we begin addressing these questions by using the game theoretic
model of trading networks and formally compare between the single and
multi-cloud markets. This comparson a) provides a sufficient condition under
which the multi-cloud network can be considered more efficient than the single
cloud one in the sense that a centralized coordinator having full information
can impose an outcome that is strongly Pareto-dominant for all players and b)
shows a surprising result that without centralized coordination, settings are
possible in which even the cloud buyers' utilities may decrease when moving
from a single cloud to a multi-cloud network. As these two results emphasize
the need for centralized coordination to ensure a Pareto-dominant outcome and
as the aforementioned Pareto-dominant result requires truthful revelation of
participant's private information, we provide an automated mechanism design
(AMD) approach, which, in the Bayesian setting, finds mechanisms which result
in expectation in such Pareto-dominant outcomes, and in which truthful
revelation of the parties' private information is the dominant strategy. We
also provide empirical analysis to show the validity of our AMD approach.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12670" title="Abstract">arXiv:2310.12670</a> [<a href="/pdf/2310.12670" title="Download PDF">pdf</a>, <a href="/format/2310.12670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reliable and Efficient In-Memory Fault Tolerance of Large Language Model  Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shaohuai Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhenheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xinglin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A+C">Amelie Chi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaowen Chu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fault Tolerance, Checkpoint Optimization, Large Language Model, 3D parallelism
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Extensive system scales (i.e. thousands of GPU/TPUs) and prolonged training
periods (i.e. months of pretraining) significantly escalate the probability of
failures when training large language models (LLMs). Thus, efficient and
reliable fault-tolerance methods are in urgent need. Checkpointing is the
primary fault-tolerance method to periodically save parameter snapshots from
GPU memory to disks via CPU memory. In this paper, we identify the frequency of
existing checkpoint-based fault-tolerance being significantly limited by the
storage I/O overheads, which results in hefty re-training costs on restarting
from the nearest checkpoint. In response to this gap, we introduce an in-memory
fault-tolerance framework for large-scale LLM pretraining. The framework boosts
the efficiency and reliability of fault tolerance from three aspects: (1)
Reduced Data Transfer and I/O: By asynchronously caching parameters, i.e.,
sharded model parameters, optimizer states, and RNG states, to CPU volatile
memory, Our framework significantly reduces communication costs and bypasses
checkpoint I/O. (2) Enhanced System Reliability: Our framework enhances
parameter protection with a two-layer hierarchy: snapshot management processes
(SMPs) safeguard against software failures, together with Erasure Coding (EC)
protecting against node failures. This double-layered protection greatly
improves the survival probability of the parameters compared to existing
checkpointing methods. (3) Improved Snapshotting Frequency: Our framework
achieves more frequent snapshotting compared with asynchronous checkpointing
optimizations under the same saving time budget, which improves the fault
tolerance efficiency. Empirical results demonstrate that Our framework
minimizes the overhead of fault tolerance of LLM pretraining by effectively
leveraging redundant CPU resources.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12671" title="Abstract">arXiv:2310.12671</a> [<a href="/pdf/2310.12671" title="Download PDF">pdf</a>, <a href="/format/2310.12671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural networks for insurance pricing with frequency and severity data:  a benchmark study from data preprocessing to technical tariff
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holvoet%2C+F">Freek Holvoet</a>, 
<a href="/search/cs?searchtype=author&query=Antonio%2C+K">Katrien Antonio</a>, 
<a href="/search/cs?searchtype=author&query=Henckaerts%2C+R">Roel Henckaerts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Risk Management (q-fin.RM)

</div>
<p class="mathjax">Insurers usually turn to generalized linear models for modelling claim
frequency and severity data. Due to their success in other fields, machine
learning techniques are gaining popularity within the actuarial toolbox. Our
paper contributes to the literature on frequency-severity insurance pricing
with machine learning via deep learning structures. We present a benchmark
study on four insurance data sets with frequency and severity targets in the
presence of multiple types of input features. We compare in detail the
performance of: a generalized linear model on binned input data, a
gradient-boosted tree model, a feed-forward neural network (FFNN), and the
combined actuarial neural network (CANN). Our CANNs combine a baseline
prediction established with a GLM and GBM, respectively, with a neural network
correction. We explain the data preprocessing steps with specific focus on the
multiple types of input features typically present in tabular insurance data
sets, such as postal codes, numeric and categorical covariates. Autoencoders
are used to embed the categorical variables into the neural network and we
explore their potential advantages in a frequency-severity setting. Finally, we
construct global surrogate models for the neural nets' frequency and severity
models. These surrogates enable the translation of the essential insights
captured by the FFNNs or CANNs to GLMs. As such, a technical tariff table
results that can easily be deployed in practice.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12677" title="Abstract">arXiv:2310.12677</a> [<a href="/pdf/2310.12677" title="Download PDF">pdf</a>, <a href="/format/2310.12677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Learning for Breast Cancer Prediction on Mammograms in  Realistic Settings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pathak%2C+S">Shreyasi Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%B6tterer%2C+J">J&#xf6;rg Schl&#xf6;tterer</a>, 
<a href="/search/cs?searchtype=author&query=Geerdink%2C+J">Jeroen Geerdink</a>, 
<a href="/search/cs?searchtype=author&query=Vijlbrief%2C+O+D">Onno Dirk Vijlbrief</a>, 
<a href="/search/cs?searchtype=author&query=van+Keulen%2C+M">Maurice van Keulen</a>, 
<a href="/search/cs?searchtype=author&query=Seifert%2C+C">Christin Seifert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Automatic methods for early detection of breast cancer on mammography can
significantly decrease mortality. Broad uptake of those methods in hospitals is
currently hindered because the methods have too many constraints. They assume
annotations available for single images or even regions-of-interest (ROIs), and
a fixed number of images per patient. Both assumptions do not hold in a general
hospital setting. Relaxing those assumptions results in a weakly supervised
learning setting, where labels are available per case, but not for individual
images or ROIs. Not all images taken for a patient contain malignant regions
and the malignant ROIs cover only a tiny part of an image, whereas most image
regions represent benign tissue. In this work, we investigate a two-level
multi-instance learning (MIL) approach for case-level breast cancer prediction
on two public datasets (1.6k and 5k cases) and an in-house dataset of 21k
cases. Observing that breast cancer is usually only present in one side, while
images of both breasts are taken as a precaution, we propose a domain-specific
MIL pooling variant. We show that two-level MIL can be applied in realistic
clinical settings where only case labels, and a variable number of images per
patient are available. Data in realistic settings scales with continuous
patient intake, while manual annotation efforts do not. Hence, research should
focus in particular on unsupervised ROI extraction, in order to improve breast
cancer prediction for all patients.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12678" title="Abstract">arXiv:2310.12678</a> [<a href="/pdf/2310.12678" title="Download PDF">pdf</a>, <a href="/format/2310.12678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TapMo: Shape-aware Motion Generation of Skeleton-free Characters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shaoli Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhigang Tu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xiaohang Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Gang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Previous motion generation methods are limited to the pre-rigged 3D human
model, hindering their applications in the animation of various non-rigged
characters. In this work, we present TapMo, a Text-driven Animation Pipeline
for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The
pivotal innovation in TapMo is its use of shape deformation-aware features as a
condition to guide the diffusion model, thereby enabling the generation of
mesh-specific motions for various characters. Specifically, TapMo comprises two
main components - Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh
Handle Predictor predicts the skinning weights and clusters mesh vertices into
adaptive handles for deformation control, which eliminates the need for
traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion
with mesh-specific adaptations. This module employs text-guided motions and
mesh features extracted during the first stage, preserving the geometric
integrity of the animations by accounting for the character's shape and
deformation. Trained in a weakly-supervised manner, TapMo can accommodate a
multitude of non-human meshes, both with and without associated text motions.
We demonstrate the effectiveness and generalizability of TapMo through rigorous
qualitative and quantitative experiments. Our results reveal that TapMo
consistently outperforms existing auto-animation methods, delivering
superior-quality animations for both seen or unseen heterogeneous 3D
characters.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12680" title="Abstract">arXiv:2310.12680</a> [<a href="/pdf/2310.12680" title="Download PDF">pdf</a>, <a href="/format/2310.12680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Optimization and Generalization of Multi-head Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deora%2C+P">Puneesh Deora</a>, 
<a href="/search/cs?searchtype=author&query=Ghaderi%2C+R">Rouzbeh Ghaderi</a>, 
<a href="/search/cs?searchtype=author&query=Taheri%2C+H">Hossein Taheri</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 page; presented in the Workshop on High-dimensional Learning Dynamics, ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">The training and generalization dynamics of the Transformer's core mechanism,
namely the Attention mechanism, remain under-explored. Besides, existing
analyses primarily focus on single-head attention. Inspired by the demonstrated
benefits of overparameterization when training fully-connected networks, we
investigate the potential optimization and generalization advantages of using
multiple attention heads. Towards this goal, we derive convergence and
generalization guarantees for gradient-descent training of a single-layer
multi-head self-attention model, under a suitable realizability condition on
the data. We then establish primitive conditions on the initialization that
ensure realizability holds. Finally, we demonstrate that these conditions are
satisfied for a simple tokenized-mixture model. We expect the analysis can be
extended to various data-model and architecture variations.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12684" title="Abstract">arXiv:2310.12684</a> [<a href="/pdf/2310.12684" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> knowCC: Knowledge, awareness of computer &amp; cyber ethics between  CS/non-CS university students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kshetri%2C+N">Naresh Kshetri</a>, 
<a href="/search/cs?searchtype=author&query=Vasudha">Vasudha</a>, 
<a href="/search/cs?searchtype=author&query=Hoxha%2C+D">Denisa Hoxha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Technology has advanced dramatically in the previous several years. There are
also cyber assaults. Cyberattacks pose a possible danger to information
security and the general public. Since data practice and internet consumption
rates continue to upswing, cyber awareness has become progressively important.
Furthermore, as businesses pace their digital transformation with mobile
devices, cloud services, communal media, and Internet of Things services,
cybersecurity has appeared as a critical issue in corporate risk management.
This research focuses on the relations between cybersecurity awareness, cyber
knowledge, computer ethics, cyber ethics, and cyber behavior, as well as
protective tools, across university students in general. The findings express
that while internet users are alert of cyber threats, they only take the most
elementary and easy-to-implement precautions. Several knowledge and awareness
have been proposed to knob the issue of cyber security. It also grants the
principles of cybersecurity in terms of its structure, workforces, and evidence
pertaining to the shield of personal information in the cyber world. The first
step is for people to educate themselves about the negative aspects of the
internet and to learn more about cyber threats so that they can notice when an
attack is taking place. To validate the efficiency of the suggested analysis
between CS and non-CS university students, case study along with several
comparisons are provided.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12688" title="Abstract">arXiv:2310.12688</a> [<a href="/pdf/2310.12688" title="Download PDF">pdf</a>, <a href="/format/2310.12688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression of Recurrent Neural Networks using Matrix Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maison%2C+L">Lucas Maison</a>, 
<a href="/search/cs?searchtype=author&query=Bourboux%2C+H+d+M+d">H&#xe9;lion du Mas des Bourboux</a>, 
<a href="/search/cs?searchtype=author&query=Courtat%2C+T">Thomas Courtat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Compressing neural networks is a key step when deploying models for real-time
or embedded applications. Factorizing the model's matrices using low-rank
approximations is a promising method for achieving compression. While it is
possible to set the rank before training, this approach is neither flexible nor
optimal. In this work, we propose a post-training rank-selection method called
Rank-Tuning that selects a different rank for each matrix. Used in combination
with training adaptations, our method achieves high compression rates with no
or little performance degradation. Our numerical experiments on signal
processing tasks show that we can compress recurrent neural networks up to 14x
with at most 1.4% relative performance reduction.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12690" title="Abstract">arXiv:2310.12690</a> [<a href="/pdf/2310.12690" title="Download PDF">pdf</a>, <a href="/format/2310.12690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neurosymbolic Grounding for Compositional World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sehgal%2C+A">Atharva Sehgal</a>, 
<a href="/search/cs?searchtype=author&query=Grayeli%2C+A">Arya Grayeli</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J+J">Jennifer J. Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+S">Swarat Chaudhuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce Cosmos, a framework for object-centric world modeling that is
designed for compositional generalization (CG), i.e., high performance on
unseen input scenes obtained through the composition of known visual "atoms."
The central insight behind Cosmos is the use of a novel form of neurosymbolic
grounding. Specifically, the framework introduces two new tools: (i)
neurosymbolic scene encodings, which represent each entity in a scene using a
real vector computed using a neural encoder, as well as a vector of composable
symbols describing attributes of the entity, and (ii) a neurosymbolic attention
mechanism that binds these entities to learned rules of interaction. Cosmos is
end-to-end differentiable; also, unlike traditional neurosymbolic methods that
require representations to be manually mapped to symbols, it computes an
entity's symbolic attributes using vision-language foundation models. Through
an evaluation that considers two different forms of CG on an established
blocks-pushing domain, we show that the framework establishes a new
state-of-the-art for CG in world modeling.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12691" title="Abstract">arXiv:2310.12691</a> [<a href="/pdf/2310.12691" title="Download PDF">pdf</a>, <a href="/ps/2310.12691" title="Download PostScript">ps</a>, <a href="/format/2310.12691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete-to-Continuum Rates of Convergence for $p$-Laplacian  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Weihs%2C+A">Adrien Weihs</a>, 
<a href="/search/math?searchtype=author&query=Fadili%2C+J">Jalal Fadili</a>, 
<a href="/search/math?searchtype=author&query=Thorpe%2C+M">Matthew Thorpe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Higher-order regularization problem formulations are popular frameworks used
in machine learning, inverse problems and image/signal processing. In this
paper, we consider the computational problem of finding the minimizer of the
Sobolev $\mathrm{W}^{1,p}$ semi-norm with a data-fidelity term. We propose a
discretization procedure and prove convergence rates between our numerical
solution and the target function. Our approach consists of discretizing an
appropriate gradient flow problem in space and time. The space discretization
is a nonlocal approximation of the p-Laplacian operator and our rates directly
depend on the localization parameter $\epsilon_n$ and the time mesh-size
$\tau_n$. We precisely characterize the asymptotic behaviour of $\epsilon_n$
and $\tau_n$ in order to ensure convergence to the considered minimizer.
Finally, we apply our results to the setting of random graph models.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12692" title="Abstract">arXiv:2310.12692</a> [<a href="/pdf/2310.12692" title="Download PDF">pdf</a>, <a href="/format/2310.12692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Learning via Consistent Assignment of Views over Random  Partitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+T">Thalles Silva</a>, 
<a href="/search/cs?searchtype=author&query=Rivera%2C+A+R">Ad&#xed;n Ram&#xed;rez Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023. Code available at <a href="https://github.com/sthalles/carp">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present Consistent Assignment of Views over Random Partitions (CARP), a
self-supervised clustering method for representation learning of visual
features. CARP learns prototypes in an end-to-end online fashion using gradient
descent without additional non-differentiable modules to solve the cluster
assignment problem. CARP optimizes a new pretext task based on random
partitions of prototypes that regularizes the model and enforces consistency
between views' assignments. Additionally, our method improves training
stability and prevents collapsed solutions in joint-embedding training. Through
an extensive evaluation, we demonstrate that CARP's representations are
suitable for learning downstream tasks. We evaluate CARP's representations
capabilities in 17 datasets across many standard protocols, including linear
evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy
detection. We compare CARP performance to 11 existing self-supervised methods.
We extensively ablate our method and demonstrate that our proposed random
partition pretext task improves the quality of the learned representations by
devising multiple random classification tasks. In transfer learning tasks, CARP
achieves the best performance on average against many SSL methods trained for a
longer time.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12693" title="Abstract">arXiv:2310.12693</a> [<a href="/pdf/2310.12693" title="Download PDF">pdf</a>, <a href="/ps/2310.12693" title="Download PostScript">ps</a>, <a href="/format/2310.12693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RANDGENER: Distributed Randomness Beacon from Verifiable Delay Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+A">Arup Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Rooparaghunath%2C+R+H">Ruthu Hulikal Rooparaghunath</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Debayan Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Buoyed by the excitement around secure decentralized applications, the last
few decades have seen numerous constructions of distributed randomness beacons
(DRB) along with use cases; however, a secure DRB (in many variations) remains
an open problem. We further note that it is natural to want some kind of reward
for participants who spend time and energy evaluating the randomness beacon
value -- this is already common in distributed protocols.
<br />In this work, we present RandGener, a novel $n$-party commit-reveal-recover
(or collaborative) DRB protocol with a novel reward and penalty mechanism along
with a set of realistic guarantees. We design our protocol using trapdoor
watermarkable verifiable delay functions in the RSA group setting (without
requiring a trusted dealer or distributed key generation).
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12694" title="Abstract">arXiv:2310.12694</a> [<a href="/pdf/2310.12694" title="Download PDF">pdf</a>, <a href="/ps/2310.12694" title="Download PostScript">ps</a>, <a href="/format/2310.12694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Querying Incomplete Data : Complexity and Tractability via Datalog and  First-Order Rewritings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gheerbrant%2C+A">Am&#xe9;lie Gheerbrant</a>, 
<a href="/search/cs?searchtype=author&query=Libkin%2C+L">Leonid Libkin</a>, 
<a href="/search/cs?searchtype=author&query=Rogova%2C+A">Alexandra Rogova</a>, 
<a href="/search/cs?searchtype=author&query=Sirangelo%2C+C">Cristina Sirangelo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under consideration in Theory and Practice of Logic Programming (TPLP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">To answer database queries over incomplete data the gold standard is finding
certain answers: those that are true regardless of how incomplete data is
interpreted. Such answers can be found efficiently for conjunctive queries and
their unions, even in the presence of constraints. With negation added, the
problem becomes intractable however. We concentrate on the complexity of
certain answers under constraints, and on effficiently answering queries
outside the usual classes of (unions) of conjunctive queries by means of
rewriting as Datalog and first-order queries. We first notice that there are
three different ways in which query answering can be cast as a decision
problem. We complete the existing picture and provide precise complexity bounds
on all versions of the decision problem, for certain and best answers. We then
study a well-behaved class of queries that extends unions of conjunctive
queries with a mild form of negation. We show that for them, certain answers
can be expressed in Datalog with negation, even in the presence of functional
dependencies, thus making them tractable in data complexity. We show that in
general Datalog cannot be replaced by first-order logic, but without
constraints such a rewriting can be done in first-order. The paper is under
consideration in Theory and Practice of Logic Programming (TPLP).
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12696" title="Abstract">arXiv:2310.12696</a> [<a href="/pdf/2310.12696" title="Download PDF">pdf</a>, <a href="/format/2310.12696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protection from Evil and Good: The Differential Effects of Page  Protection on Wikipedia Article Quality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruprechter%2C+T">Thorsten Ruprechter</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+M+H">Manoel Horta Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>, 
<a href="/search/cs?searchtype=author&query=Helic%2C+D">Denis Helic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Wikipedia, the Web's largest encyclopedia, frequently faces content disputes
or malicious users seeking to subvert its integrity. Administrators can
mitigate such disruptions by enforcing "page protection" that selectively
limits contributions to specific articles to help prevent the degradation of
content. However, this practice contradicts one of Wikipedia's fundamental
principles$-$that it is open to all contributors$-$and may hinder further
improvement of the encyclopedia. In this paper, we examine the effect of page
protection on article quality to better understand whether and when page
protections are warranted. Using decade-long data on page protections from the
English Wikipedia, we conduct a quasi-experimental study analyzing pages that
received "requests for page protection"$-$written appeals submitted by
Wikipedia editors to administrators to impose page protections. We match pages
that indeed received page protection with similar pages that did not and
quantify the causal effect of the interventions on a well-established measure
of article quality. Our findings indicate that the effect of page protection on
article quality depends on the characteristics of the page prior to the
intervention: high-quality articles are affected positively as opposed to
low-quality articles that are impacted negatively. Subsequent analysis suggests
that high-quality articles degrade when left unprotected, whereas low-quality
articles improve. Overall, with our study, we outline page protections on
Wikipedia and inform best practices on whether and when to protect an article.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12701" title="Abstract">arXiv:2310.12701</a> [<a href="/pdf/2310.12701" title="Download PDF">pdf</a>, <a href="/ps/2310.12701" title="Download PostScript">ps</a>, <a href="/format/2310.12701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parity Games on Temporal Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Austin%2C+P">Pete Austin</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+S">Sougata Bose</a>, 
<a href="/search/cs?searchtype=author&query=Totzke%2C+P">Patrick Totzke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Temporal graphs are a popular modelling mechanism for dynamic complex systems
that extend ordinary graphs with discrete time. Simply put, time progresses one
unit per step and the availability of edges can change with time.
<br />We consider the complexity of solving $\omega$-regular games played on
temporal graphs where the edge availability is ultimately periodic and fixed a
priori.
<br />We show that solving parity games on temporal graphs is decidable in PSPACE,
only assuming the edge predicate itself is in PSPACE. A matching lower bound
already holds for what we call punctual reachability games on static graphs,
where one player wants to reach the target at a given, binary encoded, point in
time. We further study syntactic restrictions that imply more efficient
procedures. In particular, if the edge predicate is in $P$ and is monotonically
increasing for one player and decreasing for the other, then the complexity of
solving games is only polynomially increased compared to static graphs.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12702" title="Abstract">arXiv:2310.12702</a> [<a href="/pdf/2310.12702" title="Download PDF">pdf</a>, <a href="/format/2310.12702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Function Hook Latency in Cloud-Native Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kahlhofer%2C+M">Mario Kahlhofer</a>, 
<a href="/search/cs?searchtype=author&query=Kern%2C+P">Patrick Kern</a>, 
<a href="/search/cs?searchtype=author&query=Henning%2C+S">S&#xf6;ren Henning</a>, 
<a href="/search/cs?searchtype=author&query=Rass%2C+S">Stefan Rass</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in the 14th Symposium on Software Performance (SSP 2023), source code available at <a href="https://github.com/dynatrace-research/function-hook-latency-benchmarking">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
<p class="mathjax">Researchers and engineers are increasingly adopting cloud-native technologies
for application development and performance evaluation. While this has improved
the reproducibility of benchmarks in the cloud, the complexity of cloud-native
environments makes it difficult to run benchmarks reliably. Cloud-native
applications are often instrumented or altered at runtime, by dynamically
patching or hooking them, which introduces a significant performance overhead.
Our work discusses the benchmarking-related pitfalls of the dominant
cloud-native technology, Kubernetes, and how they affect performance
measurements of dynamically patched or hooked applications. We present
recommendations to mitigate these risks and demonstrate how an improper
experimental setup can negatively impact latency measurements.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12705" title="Abstract">arXiv:2310.12705</a> [<a href="/pdf/2310.12705" title="Download PDF">pdf</a>, <a href="/format/2310.12705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Low-confidence Pseudo-labels for Source-free Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhihong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zilei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Source-free object detection (SFOD) aims to adapt a source-trained detector
to an unlabeled target domain without access to the labeled source data.
Current SFOD methods utilize a threshold-based pseudo-label approach in the
adaptation phase, which is typically limited to high-confidence pseudo-labels
and results in a loss of information. To address this issue, we propose a new
approach to take full advantage of pseudo-labels by introducing high and low
confidence thresholds. Specifically, the pseudo-labels with confidence scores
above the high threshold are used conventionally, while those between the low
and high thresholds are exploited using the Low-confidence Pseudo-labels
Utilization (LPU) module. The LPU module consists of Proposal Soft Training
(PST) and Local Spatial Contrastive Learning (LSCL). PST generates soft labels
of proposals for soft training, which can mitigate the label mismatch problem.
LSCL exploits the local spatial relationship of proposals to improve the
model's ability to differentiate between spatially adjacent proposals, thereby
optimizing representational features further. Combining the two components
overcomes the challenges faced by traditional methods in utilizing
low-confidence pseudo-labels. Extensive experiments on five cross-domain object
detection benchmarks demonstrate that our proposed method outperforms the
previous SFOD methods, achieving state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12706" title="Abstract">arXiv:2310.12706</a> [<a href="/pdf/2310.12706" title="Download PDF">pdf</a>, <a href="/format/2310.12706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trenchcoat: Human-Computable Hashing Algorithms for Password Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rooparaghunath%2C+R+H">Ruthu Hulikal Rooparaghunath</a>, 
<a href="/search/cs?searchtype=author&query=Harikrishnan%2C+T+S">T.S. Harikrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Debayan Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The average user has between 90-130 online accounts, and around $3 \times
10^{11}$ passwords are in use this year. Most people are terrible at
remembering "random" passwords, so they reuse or create similar passwords using
a combination of predictable words, numbers, and symbols. Previous
password-generation or management protocols have imposed so large a cognitive
load that users have abandoned them in favor of insecure yet simpler methods
(e.g., writing them down or reusing minor variants).
<br />We describe a range of candidate human-computable "hash" functions suitable
for use as password generators - as long as the human (with minimal education
assumptions) keeps a single, easily-memorizable "master" secret - and rate them
by various metrics, including effective security.
<br />These functions hash master-secrets with user accounts to produce sub-secrets
that can be used as passwords; $F_R($s$, w) \longrightarrow y$, takes a website
$w$, produces a password $y$, parameterized by master secret $s$, which may or
may not be a string.
<br />We exploit the unique configuration $R$ of each user's associative and
implicit memory (detailed in section 2) to ensure that sources of randomness
unique to each user are present in each master-secret $F_R$. An adversary
cannot compute or verify $F_R$ efficiently since $R$ is unique to each
individual; in that sense, our hash function is similar to a physically
unclonable function. For the algorithms we propose, the user need only complete
primitive operations such as addition, spatial navigation or searching.
Critically, most of our methods are also accessible to neurodiverse, or
cognitively or physically differently-abled persons.
<br />We present results from a survey (n=134 individuals) investigating real-world
usage of these methods and how people currently come up with their passwords,
we also survey 400 websites to collate current password advice.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12707" title="Abstract">arXiv:2310.12707</a> [<a href="/pdf/2310.12707" title="Download PDF">pdf</a>, <a href="/format/2310.12707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recoverable Privacy-Preserving Image Classification through Noise-like  Adversarial Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiantao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jinyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">With the increasing prevalence of cloud computing platforms, ensuring data
privacy during the cloud-based image related services such as classification
has become crucial. In this study, we propose a novel privacypreserving image
classification scheme that enables the direct application of classifiers
trained in the plaintext domain to classify encrypted images, without the need
of retraining a dedicated classifier. Moreover, encrypted images can be
decrypted back into their original form with high fidelity (recoverable) using
a secret key. Specifically, our proposed scheme involves utilizing a feature
extractor and an encoder to mask the plaintext image through a newly designed
Noise-like Adversarial Example (NAE). Such an NAE not only introduces a
noise-like visual appearance to the encrypted image but also compels the target
classifier to predict the ciphertext as the same label as the original
plaintext image. At the decoding phase, we adopt a Symmetric Residual Learning
(SRL) framework for restoring the plaintext image with minimal degradation.
Extensive experiments demonstrate that 1) the classification accuracy of the
classifier trained in the plaintext domain remains the same in both the
ciphertext and plaintext domains; 2) the encrypted images can be recovered into
their original form with an average PSNR of up to 51+ dB for the SVHN dataset
and 48+ dB for the VGGFace2 dataset; 3) our system exhibits satisfactory
generalization capability on the encryption, decryption and classification
tasks across datasets that are different from the training one; and 4) a
high-level of security is achieved against three potential threat models. The
code is available at https://github.com/csjunjun/RIC.git.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12708" title="Abstract">arXiv:2310.12708</a> [<a href="/pdf/2310.12708" title="Download PDF">pdf</a>, <a href="/format/2310.12708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Robust Adversarial Examples against Online Social Networks  (OSNs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiantao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haiwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jinyu Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Online Social Networks (OSNs) have blossomed into prevailing transmission
channels for images in the modern era. Adversarial examples (AEs) deliberately
designed to mislead deep neural networks (DNNs) are found to be fragile against
the inevitable lossy operations conducted by OSNs. As a result, the AEs would
lose their attack capabilities after being transmitted over OSNs. In this work,
we aim to design a new framework for generating robust AEs that can survive the
OSN transmission; namely, the AEs before and after the OSN transmission both
possess strong attack capabilities. To this end, we first propose a
differentiable network termed SImulated OSN (SIO) to simulate the various
operations conducted by an OSN. Specifically, the SIO network consists of two
modules: 1) a differentiable JPEG layer for approximating the ubiquitous JPEG
compression and 2) an encoder-decoder subnetwork for mimicking the remaining
operations. Based upon the SIO network, we then formulate an optimization
framework to generate robust AEs by enforcing model outputs with and without
passing through the SIO to be both misled. Extensive experiments conducted over
Facebook, WeChat and QQ demonstrate that our attack methods produce more robust
AEs than existing approaches, especially under small distortion constraints;
the performance gain in terms of Attack Success Rate (ASR) could be more than
60%. Furthermore, we build a public dataset containing more than 10,000 pairs
of AEs processed by Facebook, WeChat or QQ, facilitating future research in the
robust AEs generation. The dataset and code are available at
https://github.com/csjunjun/RobustOSNAttack.git.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12709" title="Abstract">arXiv:2310.12709</a> [<a href="/pdf/2310.12709" title="Download PDF">pdf</a>, <a href="/format/2310.12709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacity Limitation and Optimization Strategy for Flexible  Point-to-Multi-Point Optical Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Ji Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haide Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangchuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changyuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaohui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been submitted to the IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Point-to-multi-point (PtMP) optical networks become the main solutions for
network-edge applications such as passive optical networks and radio access
networks. Entropy-loading digital subcarrier multiplexing (DSCM) is the core
technology to achieve low latency and approach high capacity for flexible PtMP
optical networks. However, the high peak-to-average power ratio of the
entropy-loading DSCM signal limits the power budget and restricts the capacity,
which can be reduced effectively by clipping operation. In this paper, we
derive the theoretical capacity limitation of the flexible PtMP optical
networks based on the entropy-loading DSCM signal. Meanwhile, an optimal
clipping ratio for the clipping operation is acquired to approach the highest
capacity limitation. Based on an accurate clipping-noise model under the
optimal clipping ratio, we establish a three-dimensional look-up table for
bit-error ratio, spectral efficiency, and link loss. Based on the
three-dimensional look-up table, an optimization strategy is proposed to
acquire optimal spectral efficiencies for achieving a higher capacity of the
flexible PtMP optical networks.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12712" title="Abstract">arXiv:2310.12712</a> [<a href="/pdf/2310.12712" title="Download PDF">pdf</a>, <a href="/ps/2310.12712" title="Download PostScript">ps</a>, <a href="/format/2310.12712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curvature Aligned Simplex Gradient: Principled Sample Set Construction  For Numerical Differentiation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lengyel%2C+D">Daniel Lengyel</a>, 
<a href="/search/math?searchtype=author&query=Parpas%2C+P">Panos Parpas</a>, 
<a href="/search/math?searchtype=author&query=Kantas%2C+N">Nikolas Kantas</a>, 
<a href="/search/math?searchtype=author&query=Jennings%2C+N+R">Nicholas R. Jennings</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 Pages, 5 Figures, Submitted to IMA Numerical Analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">The simplex gradient, a popular numerical differentiation method due to its
flexibility, lacks a principled method by which to construct the sample set,
specifically the location of function evaluations. Such evaluations, especially
from real-world systems, are often noisy and expensive to obtain, making it
essential that each evaluation is carefully chosen to reduce cost and increase
accuracy. This paper introduces the curvature aligned simplex gradient (CASG),
which provably selects the optimal sample set under a mean squared error
objective. As CASG requires function-dependent information often not available
in practice, we additionally introduce a framework which exploits a history of
function evaluations often present in practical applications. Our numerical
results, focusing on applications in sensitivity analysis and derivative free
optimization, show that our methodology significantly outperforms or matches
the performance of the benchmark gradient estimator given by forward
differences (FD) which is given exact function-dependent information that is
not available in practice. Furthermore, our methodology is comparable to the
performance of central differences (CD) that requires twice the number of
function evaluations.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12713" title="Abstract">arXiv:2310.12713</a> [<a href="/pdf/2310.12713" title="Download PDF">pdf</a>, <a href="/format/2310.12713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn from the Past: A Proxy based Adversarial Defense Framework to  Boost Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaohua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiaxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+X">Xianghao Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Risheng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In light of the vulnerability of deep learning models to adversarial samples
and the ensuing security issues, a range of methods, including Adversarial
Training (AT) as a prominent representative, aimed at enhancing model
robustness against various adversarial attacks, have seen rapid development.
However, existing methods essentially assist the current state of target model
to defend against parameter-oriented adversarial attacks with explicit or
implicit computation burdens, which also suffers from unstable convergence
behavior due to inconsistency of optimization trajectories. Diverging from
previous work, this paper reconsiders the update rule of target model and
corresponding deficiency to defend based on its current state. By introducing
the historical state of the target model as a proxy, which is endowed with much
prior information for defense, we formulate a two-stage update rule, resulting
in a general adversarial defense framework, which we refer to as `LAST' ({\bf
L}earn from the P{\bf ast}). Besides, we devise a Self Distillation (SD) based
defense objective to constrain the update process of the proxy model without
the introduction of larger teacher models. Experimentally, we demonstrate
consistent and significant performance enhancements by refining a series of
single-step and multi-step AT methods (e.g., up to $\bf 9.2\%$ and $\bf 20.5\%$
improvement of Robust Accuracy (RA) on CIFAR10 and CIFAR100 datasets,
respectively) across various datasets, backbones and attack modalities, and
validate its ability to enhance training stability and ameliorate catastrophic
overfitting issues meanwhile.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12715" title="Abstract">arXiv:2310.12715</a> [<a href="/pdf/2310.12715" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Biomimetic Morphing Dorsal Fin Affects the Swimming Performance of a  Free-swimming Tuna Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hongbing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhonglu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinhu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">It is well known that tuna fish in the ocean can dynamically morph their
median fins to achieve optimal hydrodynamic performance, e.g. linear
acceleration and maneuverability. In this study, based on the previous studies
about the median fin's hydrodynamic effects focusing on tethered conditions, we
continue to explore the hydrodynamic function of tuna morphing dorsal fin in
free swimming conditions for better approaching real-life situations.Here, we
developed a tuna-inspired robotic fish platform that can swim independently in
three dimensions, equipped with a biomimetic morphing dorsal fin magnetically
attached to the robotic fish. Based on the free-swimming robotic fish platform,
we investigated how the erected dorsal fin affects the speed, cost of transport
(COT), and robotic fish's yaw angle at different frequencies and amplitudes.
The erected dorsal fin plays a positive role in improving the yaw stability of
robotic fish. However, it shows little influence on the speed and COT in our
test. This remains to be further investigated in the future.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12723" title="Abstract">arXiv:2310.12723</a> [<a href="/pdf/2310.12723" title="Download PDF">pdf</a>, <a href="/format/2310.12723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Short-Lived Signatures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+A">Arup Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Rooparaghunath%2C+R+H">Ruthu Hulikal Rooparaghunath</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Debayan Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">A Time-lock puzzle (TLP) sends information into the future: a predetermined
number of sequential computations must occur (i.e., a predetermined amount of
time must pass) to retrieve the information, regardless of parallelization.
Buoyed by the excitement around secure decentralized applications and
cryptocurrencies, the last decade has witnessed numerous constructions of TLP
variants and related applications (e.g., cost-efficient blockchain designs,
randomness beacons, e-voting, etc.).
<br />In this poster, we first extend the notion of TLP by formally defining the
"time-lock public key encryption" (TLPKE) scheme. Next, we introduce and
construct a "tight short-lived signatures" scheme using our TLPKE. Furthermore,
to test the validity of our proposed schemes, we do a proof-of-concept
implementation and run detailed simulations.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12724" title="Abstract">arXiv:2310.12724</a> [<a href="/pdf/2310.12724" title="Download PDF">pdf</a>, <a href="/format/2310.12724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-aware Long Video Localization and Relation Discrimination for Deep  Video Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanxing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yuting Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bin Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM MM 2023 Grand Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The surge in video and social media content underscores the need for a deeper
understanding of multimedia data. Most of the existing mature video
understanding techniques perform well with short formats and content that
requires only shallow understanding, but do not perform well with long format
videos that require deep understanding and reasoning. Deep Video Understanding
(DVU) Challenge aims to push the boundaries of multimodal extraction, fusion,
and analytics to address the problem of holistically analyzing long videos and
extract useful knowledge to solve different types of queries. This paper
introduces a query-aware method for long video localization and relation
discrimination, leveraging an imagelanguage pretrained model. This model
adeptly selects frames pertinent to queries, obviating the need for a complete
movie-level knowledge graph. Our approach achieved first and fourth positions
for two groups of movie-level queries. Sufficient experiments and final
rankings demonstrate its effectiveness and robustness.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12727" title="Abstract">arXiv:2310.12727</a> [<a href="/pdf/2310.12727" title="Download PDF">pdf</a>, <a href="/format/2310.12727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representing and Computing Uncertainty in Phonological Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=List%2C+J">Johann-Mattis List</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+N+W">Nathan W. Hill</a>, 
<a href="/search/cs?searchtype=author&query=Forkel%2C+R">Robert Forkel</a>, 
<a href="/search/cs?searchtype=author&query=Blum%2C+F">Frederic Blum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in: Proceedings of the 4th Workshop on Computational Approaches to Historical Language Change
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the inherently fuzzy nature of reconstructions in historical
linguistics, most scholars do not represent their uncertainty when proposing
proto-forms. With the increasing success of recently proposed approaches to
automating certain aspects of the traditional comparative method, the formal
representation of proto-forms has also improved. This formalization makes it
possible to address both the representation and the computation of uncertainty.
Building on recent advances in supervised phonological reconstruction, during
which an algorithm learns how to reconstruct words in a given proto-language
relying on previously annotated data, and inspired by improved methods for
automated word prediction from cognate sets, we present a new framework that
allows for the representation of uncertainty in linguistic reconstruction and
also includes a workflow for the computation of fuzzy reconstructions from
linguistic data.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12729" title="Abstract">arXiv:2310.12729</a> [<a href="/pdf/2310.12729" title="Download PDF">pdf</a>, <a href="/format/2310.12729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancements in Radar Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frosi%2C+M">Matteo Frosi</a>, 
<a href="/search/cs?searchtype=author&query=Usuelli%2C+M">Mirko Usuelli</a>, 
<a href="/search/cs?searchtype=author&query=Matteucci%2C+M">Matteo Matteucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Radar odometry estimation has emerged as a critical technique in the field of
autonomous navigation, providing robust and reliable motion estimation under
various environmental conditions. Despite its potential, the complex nature of
radar signals and the inherent challenges associated with processing these
signals have limited the widespread adoption of this technology. This paper
aims to address these challenges by proposing novel improvements to an existing
method for radar odometry estimation, designed to enhance accuracy and
reliability in diverse scenarios. Our pipeline consists of filtering, motion
compensation, oriented surface points computation, smoothing, one-to-many radar
scan registration, and pose refinement. The developed method enforces local
understanding of the scene, by adding additional information through smoothing
techniques, and alignment of consecutive scans, as a refinement posterior to
the one-to-many registration. We present an in-depth investigation of the
contribution of each improvement to the localization accuracy, and we benchmark
our system on the sequences of the main datasets for radar understanding, i.e.,
the Oxford Radar RobotCar, MulRan, and Boreas datasets. The proposed pipeline
is able to achieve superior results, on all scenarios considered and under
harsh environmental constraints.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12732" title="Abstract">arXiv:2310.12732</a> [<a href="/pdf/2310.12732" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming the encoding limit NH0(S) using Set Shaping Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koch%2C+A">Aida Koch</a>, 
<a href="/search/cs?searchtype=author&query=Petit%2C+A">Alix Petit</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+C">Christian Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Vdberg%2C+A">Adrain Vdberg</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+L">Logan Lewis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Given the importance of the claim, we want to start by exposing the following
consideration: this claim comes out more than a year after the article
"Practical applications of Set Shaping Theory in Huffman coding" which reports
the program that carried out an experiment of data compression in which the
coding limit NH0(S) of a single sequence was questioned. We waited so long
because, before making a claim of this type, we wanted to be sure of the
consistency of the result. All this time the program has always been public;
anyone could download it, modify it and independently obtain the reported
results. In this period there have been many information theory experts who
have tested the program and agreed to help us, we thank these people for the
time dedicated to us and their precious advice. Given a sequence S of random
variables i.i.d. with symbols belonging to an alphabet A; the parameter NH0(S)
(the zero-order empirical entropy multiplied by the length of the sequence) is
considered the average coding limit of the symbols of the sequence S through a
uniquely decipherable and instantaneous code. Our experiment that calls into
question this limit is the following: a sequence S is generated in a random and
uniform way, the value NH0(S) is calculated, the sequence S is transformed into
a new sequence f(S), longer but with the symbols belonging to the same
alphabet, finally we code f(S) using Huffman coding. By generating a
statistically significant number of sequences we obtain that the average value
of the length of the encoded sequence f(S) is less than the average value of
NH0(S). In this way, a result is obtained which is incompatible with the
meaning given to NH0(S).
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12736" title="Abstract">arXiv:2310.12736</a> [<a href="/pdf/2310.12736" title="Download PDF">pdf</a>, <a href="/format/2310.12736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExtSwap: Leveraging Extended Latent Mapper for Generating High Quality  Face Swapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=PN%2C+A+R">Aravinda Reddy PN</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+K+S">K.Sreenivasa Rao</a>, 
<a href="/search/cs?searchtype=author&query=Ramachandra%2C+R">Raghavendra Ramachandra</a>, 
<a href="/search/cs?searchtype=author&query=mitra%2C+P">Pabitra mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a novel face swapping method using the progressively growing
structure of a pre-trained StyleGAN. Previous methods use different encoder
decoder structures, embedding integration networks to produce high-quality
results, but their quality suffers from entangled representation. We
disentangle semantics by deriving identity and attribute features separately.
By learning to map the concatenated features into the extended latent space, we
leverage the state-of-the-art quality and its rich semantic extended latent
space. Extensive experiments suggest that the proposed method successfully
disentangles identity and attribute features and outperforms many
state-of-the-art face swapping methods, both qualitatively and quantitatively.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12737" title="Abstract">arXiv:2310.12737</a> [<a href="/pdf/2310.12737" title="Download PDF">pdf</a>, <a href="/format/2310.12737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new complex variable solution on noncircular shallow tunnelling with  reasonable far-field displacement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lin%2C+L">Luo-bin Lin</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+F">Fu-quan Chen</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+S">Shang-shun Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 10 figures. arXiv admin note: text overlap with <a href="/abs/2308.03994">arXiv:2308.03994</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Complex Variables (math.CV)

</div>
<p class="mathjax">A new mechanical model on noncircular shallow tunnelling considering initial
stress field is proposed in this paper by constraining far-field ground surface
to eliminate displacement singularity at infinity, and the originally
unbalanced tunnel excavation problem in existing solutions is turned to an
equilibrium one of mixed boundaries. By applying analytic continuation, the
mixed boundaries are transformed to a homogenerous Riemann-Hilbert problem,
which is subsequently solved via an efficient and accurate iterative method
with boundary conditions of static equilibrium, displacement single-valuedness,
and traction along tunnel periphery. The Lanczos filtering technique is used in
the final stress and displacement solution to reduce the Gibbs phenomena caused
by the constrained far-field ground surface for more accurte results. Several
numerical cases are conducted to intensively verify the proposed solution by
examining boundary conditions and comparing with existing solutions, and all
the results are in good agreements. Then more numerical cases are conducted to
investigate the stress and deformation distribution along ground surface and
tunnel periphery, and several engineering advices are given. Further
discussions on the defects of the proposed solution are also conducted for
objectivity.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12739" title="Abstract">arXiv:2310.12739</a> [<a href="/pdf/2310.12739" title="Download PDF">pdf</a>, <a href="/format/2310.12739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strongly stable dual-pairing summation by parts finite difference  schemes for the vector invariant nonlinear shallow water equations -- I:  numerical scheme and validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hew%2C+J+K+J">Justin Kin Jun Hew</a>, 
<a href="/search/math?searchtype=author&query=Duru%2C+K">Kenneth Duru</a>, 
<a href="/search/math?searchtype=author&query=Roberts%2C+S">Stephen Roberts</a>, 
<a href="/search/math?searchtype=author&query=Zoppou%2C+C">Christopher Zoppou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 10 figures, comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Atmospheric and Oceanic Physics (physics.ao-ph); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">We present an energy/entropy stable and high order accurate finite difference
method for solving the linear/nonlinear shallow water equations (SWE) in vector
invariant form using the newly developed dual-pairing (DP) and
dispersion-relation preserving (DRP) summation by parts (SBP) finite difference
operators. We derive new well-posed boundary conditions for the SWE in one
space dimension, formulated in terms of fluxes and applicable to linear and
nonlinear problems. For nonlinear problems, entropy stability ensures the
boundedness of numerical solutions, however, it does not guarantee convergence.
Adequate amount of numerical dissipation is necessary to control high frequency
errors which could ruin numerical simulations. Using the dual-pairing SBP
framework, we derive high order accurate and nonlinear hyper-viscosity operator
which dissipates entropy and enstrophy. The hyper-viscosity operator
effectively tames oscillations from shocks and discontinuities, and eliminates
poisonous high frequency grid-scale errors. The numerical method is most
suitable for the simulations of sub-critical flows typical observed in
atmospheric and geostrophic flow problems. We prove a priori error estimates
for the semi-discrete approximations of both linear and nonlinear SWE. We
verify convergence, accuracy and well-balanced property via the method of
manufactured solutions (MMS) and canonical test problems such as the dam break,
lake at rest, and a two-dimensional rotating and merging vortex problem.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12740" title="Abstract">arXiv:2310.12740</a> [<a href="/pdf/2310.12740" title="Download PDF">pdf</a>, <a href="/ps/2310.12740" title="Download PostScript">ps</a>, <a href="/format/2310.12740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the power of iid information for linear approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sonnleitner%2C+M">Mathias Sonnleitner</a>, 
<a href="/search/math?searchtype=author&query=Ullrich%2C+M">Mario Ullrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 61 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Complexity (cs.CC); Information Theory (cs.IT)

</div>
<p class="mathjax">This survey is concerned with the power of random information for
approximation in the (deterministic) worst-case setting, with special emphasis
on information that is obtained independently and identically distributed (iid)
from a given distribution on a class of admissible information. We present a
general result based on a weighted least squares method and derive consequences
for special cases. Improvements are available if the information is "Gaussian"
or if we consider iid function values for Sobolev spaces. We include open
questions to guide future research on the power of random information in the
context of information-based complexity.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12741" title="Abstract">arXiv:2310.12741</a> [<a href="/pdf/2310.12741" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Botization of Science? Large-scale study of the presence and impact  of Twitter bots in science dissemination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arroyo-Machado%2C+W">Wenceslao Arroyo-Machado</a>, 
<a href="/search/cs?searchtype=author&query=Herrera-Viedma%2C+E">Enrique Herrera-Viedma</a>, 
<a href="/search/cs?searchtype=author&query=Torres-Salinas%2C+D">Daniel Torres-Salinas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Twitter bots are a controversial element of the platform, and their negative
impact is well known. In the field of scientific communication, they have been
perceived in a more positive light, and the accounts that serve as feeds
alerting about scientific publications are quite common. However, despite being
aware of the presence of bots in the dissemination of science, no large-scale
estimations have been made nor has it been evaluated if they can truly
interfere with altmetrics. Analyzing a dataset of 3,744,231 papers published
between 2017 and 2021 and their associated 51,230,936 Twitter mentions, our
goal was to determine the volume of publications mentioned by bots and whether
they skew altmetrics indicators. Using the BotometerLite API, we categorized
Twitter accounts based on their likelihood of being bots. The results showed
that 11,073 accounts (0.23% of total users) exhibited automated behavior,
contributing to 4.72% of all mentions. A significant bias was observed in the
activity of bots. Their presence was particularly pronounced in disciplines
such as Mathematics, Physics, and Space Sciences, with some specialties even
exceeding 70% of the tweets. However, these are extreme cases, and the impact
of this activity on altmetrics varies by speciality, with minimal influence in
Arts &amp; Humanities and Social Sciences. This research emphasizes the importance
of distinguishing between specialties and disciplines when using Twitter as an
altmetric.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12746" title="Abstract">arXiv:2310.12746</a> [<a href="/pdf/2310.12746" title="Download PDF">pdf</a>, <a href="/format/2310.12746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TabuLa: Harnessing Language Models for Tabular Data Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zilong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Birke%2C+R">Robert Birke</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lydia Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Given the ubiquitous use of tabular data in industries and the growing
concerns in data privacy and security, tabular data synthesis emerges as a
critical research area. The recent state-of-the-art methods show that large
language models (LLMs) can be adopted to generate realistic tabular data. As
LLMs pre-process tabular data as full text, they have the advantage of avoiding
the curse of dimensionality associated with one-hot encoding high-dimensional
data. However, their long training time and limited re-usability on new tasks
prevent them from replacing exiting tabular generative models. In this paper,
we propose Tabula, a tabular data synthesizer based on the language model
structure. Through Tabula, we demonstrate the inherent limitation of employing
pre-trained language models designed for natural language processing (NLP) in
the context of tabular data synthesis. Our investigation delves into the
development of a dedicated foundational model tailored specifically for tabular
data synthesis. Additionally, we propose a token sequence compression strategy
to significantly reduce training time while preserving the quality of synthetic
data. Extensive experiments on six datasets demonstrate that using a language
model structure without loading the well-trained model weights yields a better
starting model for tabular data synthesis. Moreover, the Tabula model,
previously trained on other tabular data, serves as an excellent foundation
model for new tabular data synthesis tasks. Additionally, the token sequence
compression method substantially reduces the model's training time. Results
show that Tabula averagely reduces 46.2% training time per epoch comparing to
current LLMs-based state-of-the-art algorithm and consistently achieves even
higher synthetic data utility.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12751" title="Abstract">arXiv:2310.12751</a> [<a href="/pdf/2310.12751" title="Download PDF">pdf</a>, <a href="/format/2310.12751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Character-level Chinese Backpack Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hewitt%2C+J">John Hewitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BlackboxNLP 2023 Camera-Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The Backpack is a Transformer alternative shown to improve interpretability
in English language modeling by decomposing predictions into a weighted sum of
token sense components. However, Backpacks' reliance on token-defined meaning
raises questions as to their potential for languages other than English, a
language for which subword tokenization provides a reasonable approximation for
lexical items. In this work, we train, evaluate, interpret, and control
Backpack language models in character-tokenized Chinese, in which words are
often composed of many characters. We find that our (134M parameter) Chinese
Backpack language model performs comparably to a (104M parameter) Transformer,
and learns rich character-level meanings that log-additively compose to form
word meanings. In SimLex-style lexical semantic evaluations, simple averages of
Backpack character senses outperform input embeddings from a Transformer. We
find that complex multi-character meanings are often formed by using the same
per-character sense weights consistently across context. Exploring
interpretability-through control, we show that we can localize a source of
gender bias in our Backpacks to specific character senses and intervene to
reduce the bias.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12752" title="Abstract">arXiv:2310.12752</a> [<a href="/pdf/2310.12752" title="Download PDF">pdf</a>, <a href="/format/2310.12752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic  Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spectral clustering and its extensions usually consist of two steps: (1)
constructing a graph and computing the relaxed solution; (2) discretizing
relaxed solutions. Although the former has been extensively investigated, the
discretization techniques are mainly heuristic methods, e.g., k-means, spectral
rotation. Unfortunately, the goal of the existing methods is not to find a
discrete solution that minimizes the original objective. In other words, the
primary drawback is the neglect of the original objective when computing the
discrete solution. Inspired by the first-order optimization algorithms, we
propose to develop a first-order term to bridge the original problem and
discretization algorithm, which is the first non-heuristic to the best of our
knowledge. Since the non-heuristic method is aware of the original graph cut
problem, the final discrete solution is more reliable and achieves the
preferable loss value. We also theoretically show that the continuous optimum
is beneficial to discretization algorithms though simply finding its closest
discrete solution is an existing heuristic algorithm which is also unreliable.
Sufficient experiments significantly show the superiority of our method.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12753" title="Abstract">arXiv:2310.12753</a> [<a href="/pdf/2310.12753" title="Download PDF">pdf</a>, <a href="/format/2310.12753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patch-CLIP: A Patch-Text Pre-Trained Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xunzhu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ezzini%2C+S">Saad Ezzini</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Haoye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jacques Klein</a>, 
<a href="/search/cs?searchtype=author&query=Bissyande%2C+T+F">Tegawende F. Bissyande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In recent years, patch representation learning has emerged as a necessary
research direction for exploiting the capabilities of machine learning in
software generation. These representations have driven significant performance
enhancements across a variety of tasks involving code changes. While the
progress is undeniable, a common limitation among existing models is their
specialization: they predominantly excel in either predictive tasks, such as
security patch classification, or in generative tasks such as patch description
generation. This dichotomy is further exacerbated by a prevalent dependency on
potentially noisy data sources. Specifically, many models utilize patches
integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain
parsing inaccuracies, thus acting as a suboptimal source of supervision. In
response to these challenges, we introduce PATCH-CLIP, a novel pre-training
framework for patches and natural language text. PATCH-CLIP deploys a
triple-loss training strategy for 1) patch-description contrastive learning,
which enables to separate patches and descriptions in the embedding space, 2)
patch-description matching, which ensures that each patch is associated to its
description in the embedding space, and 3) patch-description generation, which
ensures that the patch embedding is effective for generation. These losses are
implemented for joint learning to achieve good performance in both predictive
and generative tasks involving patches. Empirical evaluations focusing on patch
description generation, demonstrate that PATCH-CLIP sets new state of the art
performance, consistently outperforming the state-of-the-art in metrics like
BLEU, ROUGE-L, METEOR, and Recall.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12755" title="Abstract">arXiv:2310.12755</a> [<a href="/pdf/2310.12755" title="Download PDF">pdf</a>, <a href="/format/2310.12755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimalist and High-Performance Semantic Segmentation with Plain Vision  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yuanduo Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Huihui Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the wake of Masked Image Modeling (MIM), a diverse range of plain,
non-hierarchical Vision Transformer (ViT) models have been pre-trained with
extensive datasets, offering new paradigms and significant potential for
semantic segmentation. Current state-of-the-art systems incorporate numerous
inductive biases and employ cumbersome decoders. Building upon the original
motivations of plain ViTs, which are simplicity and generality, we explore
high-performance `minimalist' systems to this end. Our primary purpose is to
provide simple and efficient baselines for practical semantic segmentation with
plain ViTs. Specifically, we first explore the feasibility and methodology for
achieving high-performance semantic segmentation using the last feature map. As
a result, we introduce the PlainSeg, a model comprising only three 3$\times$3
convolutions in addition to the transformer layers (either encoder or decoder).
In this process, we offer insights into two underlying principles: (i)
high-resolution features are crucial to high performance in spite of employing
simple up-sampling techniques and (ii) the slim transformer decoder requires a
much larger learning rate than the wide transformer decoder. On this basis, we
further present the PlainSeg-Hier, which allows for the utilization of
hierarchical features. Extensive experiments on four popular benchmarks
demonstrate the high performance and efficiency of our methods. They can also
serve as powerful tools for assessing the transfer ability of base models in
semantic segmentation. Code is available at
\url{https://github.com/ydhongHIT/PlainSeg}.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12763" title="Abstract">arXiv:2310.12763</a> [<a href="/pdf/2310.12763" title="Download PDF">pdf</a>, <a href="/ps/2310.12763" title="Download PostScript">ps</a>, <a href="/format/2310.12763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Liveness Properties in Geometric Logic for Domain-Theoretic Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Riba%2C+C">Colin Riba</a>, 
<a href="/search/cs?searchtype=author&query=Stern%2C+S">Solal Stern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We devise a version of Linear Temporal Logic (LTL) on a denotational domain
of streams. We investigate this logic in terms of domain theory, (point-free)
topology and geometric logic. This yields the first steps toward an extension
of the "Domain Theory in Logical Form" paradigm to temporal liveness
properties. We show that the negation-free formulae of LTL induce sober
subspaces of streams, but that this is in general not the case in presence of
negation. We propose a direct, inductive, translation of negation-free LTL to
geometric logic. This translation reflects the approximations used to compute
the usual fixpoint representations of LTL modalities. As a motivating example,
we handle a natural input-output specification for the usual filter function on
streams.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12765" title="Abstract">arXiv:2310.12765</a> [<a href="/pdf/2310.12765" title="Download PDF">pdf</a>, <a href="/format/2310.12765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Based Models For Speech Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wanli Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zehai Tu</a>, 
<a href="/search/cs?searchtype=author&query=Ragni%2C+A">Anton Ragni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recently there has been a lot of interest in non-autoregressive (non-AR)
models for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike
AR models, these models do not have autoregressive dependencies among outputs
which makes inference efficient. This paper expands the range of available
non-AR models with another member called energy-based models (EBMs). The paper
describes how noise contrastive estimation, which relies on the comparison
between positive and negative samples, can be used to train EBMs. It proposes a
number of strategies for generating effective negative samples, including using
high-performing AR models. It also describes how sampling from EBMs can be
performed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin
MCMC enables to draw connections between EBMs and currently popular diffusion
models. Experiments on LJSpeech dataset show that the proposed approach offers
improvements over Tacotron 2.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12766" title="Abstract">arXiv:2310.12766</a> [<a href="/pdf/2310.12766" title="Download PDF">pdf</a>, <a href="/format/2310.12766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based Entity Legal Form Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arimond%2C+A">Alexander Arimond</a>, 
<a href="/search/cs?searchtype=author&query=Molteni%2C+M">Mauro Molteni</a>, 
<a href="/search/cs?searchtype=author&query=Jany%2C+D">Dominik Jany</a>, 
<a href="/search/cs?searchtype=author&query=Manolova%2C+Z">Zornitsa Manolova</a>, 
<a href="/search/cs?searchtype=author&query=Borth%2C+D">Damian Borth</a>, 
<a href="/search/cs?searchtype=author&query=Hoepner%2C+A+G+F">Andreas G.F. Hoepner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose the application of Transformer-based language models for
classifying entity legal forms from raw legal entity names. Specifically, we
employ various BERT variants and compare their performance against multiple
traditional baselines. Our evaluation encompasses a substantial subset of
freely available Legal Entity Identifier (LEI) data, comprising over 1.1
million legal entities from 30 different legal jurisdictions. The ground truth
labels for classification per jurisdiction are taken from the Entity Legal Form
(ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT
variants outperform traditional text classification approaches in terms of F1
score, while also performing comparably well in the Macro F1 Score. Moreover,
the validity of our proposal is supported by the outcome of third-party expert
reviews conducted in ten selected jurisdictions. This study highlights the
significant potential of Transformer-based models in advancing data
standardization and data integration. The presented approaches can greatly
benefit financial institutions, corporations, governments and other
organizations in assessing business relationships, understanding risk exposure,
and promoting effective governance.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12767" title="Abstract">arXiv:2310.12767</a> [<a href="/pdf/2310.12767" title="Download PDF">pdf</a>, <a href="/ps/2310.12767" title="Download PostScript">ps</a>, <a href="/format/2310.12767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Two-Player Games under Progress Assumptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmuck%2C+A">Anne-Kathrin Schmuck</a>, 
<a href="/search/cs?searchtype=author&query=Thejaswini%2C+K+S">K. S. Thejaswini</a>, 
<a href="/search/cs?searchtype=author&query=Sa%C4%9Flam%2C+I">Irmak Sa&#x11f;lam</a>, 
<a href="/search/cs?searchtype=author&query=Nayak%2C+S+P">Satya Prakash Nayak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> VMCAI 2024. arXiv admin note: text overlap with <a href="/abs/1904.12446">arXiv:1904.12446</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">This paper considers the problem of solving infinite two-player games over
finite graphs under various classes of progress assumptions motivated by
applications in cyber-physical system (CPS) design.
<br />Formally, we consider a game graph G, a temporal specification $\Phi$ and a
temporal assumption $\psi$, where both are given as linear temporal logic (LTL)
formulas over the vertex set of G. We call the tuple $(G,\Phi,\psi)$ an
'augmented game' and interpret it in the classical way, i.e., winning the
augmented game $(G,\Phi,\psi)$ is equivalent to winning the (standard) game
$(G,\psi \implies \Phi)$. Given a reachability or parity game $(G,\Phi)$ and
some progress assumption $\psi$, this paper establishes whether solving the
augmented game $(G,\Phi,\psi)$ lies in the same complexity class as solving
$(G,\Phi)$. While the answer to this question is negative for arbitrary
combinations of $\Phi$ and $\psi$, a positive answer results in more efficient
algorithms, in particular for large game graphs.
<br />We therefore restrict our attention to particular classes of CPS-motivated
progress assumptions and establish the worst-case time complexity of the
resulting augmented games. Thereby, we pave the way towards a better
understanding of assumption classes that can enable the development of
efficient solution algorithms in augmented two-player games.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12769" title="Abstract">arXiv:2310.12769</a> [<a href="/pdf/2310.12769" title="Download PDF">pdf</a>, <a href="/format/2310.12769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixing Histopathology Prototypes into Robust Slide-Level Representations  for Cancer Subtyping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Butke%2C+J">Joshua Butke</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+N">Noriaki Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>, 
<a href="/search/cs?searchtype=author&query=Miyoshi%2C+H">Hiroaki Miyoshi</a>, 
<a href="/search/cs?searchtype=author&query=Ohshima%2C+K">Koichi Ohshima</a>, 
<a href="/search/cs?searchtype=author&query=Sakuma%2C+J">Jun Sakuma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The final authenticated publication is available online at <a href="https://doi.org/10.1007/978-3-031-45676-3_12">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine Learning in Medical Imaging. MLMI 2023. Lecture Notes in
  Computer Science, vol 14349, pp. 114-123. Cham: Springer Nature Switzerland
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Whole-slide image analysis via the means of computational pathology often
relies on processing tessellated gigapixel images with only slide-level labels
available. Applying multiple instance learning-based methods or transformer
models is computationally expensive as, for each image, all instances have to
be processed simultaneously. The MLP-Mixer is an under-explored alternative
model to common vision transformers, especially for large-scale datasets. Due
to the lack of a self-attention mechanism, they have linear computational
complexity to the number of input patches but achieve comparable performance on
natural image datasets. We propose a combination of feature embedding and
clustering to preprocess the full whole-slide image into a reduced prototype
representation which can then serve as input to a suitable MLP-Mixer
architecture. Our experiments on two public benchmarks and one inhouse
malignant lymphoma dataset show comparable performance to current
state-of-the-art methods, while achieving lower training costs in terms of
computational time and memory load. Code is publicly available at
https://github.com/butkej/ProtoMixer.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12771" title="Abstract">arXiv:2310.12771</a> [<a href="/pdf/2310.12771" title="Download PDF">pdf</a>, <a href="/format/2310.12771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Average Gradient : A Simple Empirical Investigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Notsawo%2C+P+J+T">Pascal Junior Tikeng Notsawo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 52 figures. arXiv admin note: substantial text overlap with <a href="/abs/1309.2388">arXiv:1309.2388</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Despite the recent growth of theoretical studies and empirical successes of
neural networks, gradient backpropagation is still the most widely used
algorithm for training such networks. On the one hand, we have deterministic or
full gradient (FG) approaches that have a cost proportional to the amount of
training data used but have a linear convergence rate, and on the other hand,
stochastic gradient (SG) methods that have a cost independent of the size of
the dataset, but have a less optimal convergence rate than the determinist
approaches. To combine the cost of the stochastic approach with the convergence
rate of the deterministic approach, a stochastic average gradient (SAG) has
been proposed. SAG is a method for optimizing the sum of a finite number of
smooth convex functions. Like SG methods, the SAG method's iteration cost is
independent of the number of terms in the sum. In this work, we propose to
compare SAG to some standard optimizers used in machine learning. SAG converges
faster than other optimizers on simple toy problems and performs better than
many other optimizers on simple machine learning problems. We also propose a
combination of SAG with the momentum algorithm and Adam. These combinations
allow empirically higher speed and obtain better performance than the other
methods, especially when the landscape of the function to optimize presents
obstacles or is ill-conditioned.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12773" title="Abstract">arXiv:2310.12773</a> [<a href="/pdf/2310.12773" title="Download PDF">pdf</a>, <a href="/format/2310.12773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe RLHF: Safe Reinforcement Learning from Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Josef Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruiyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinbo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mickel Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the development of large language models (LLMs), striking a balance
between the performance and safety of AI systems has never been more critical.
However, the inherent tension between the objectives of helpfulness and
harmlessness presents a significant challenge during LLM training. To address
this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe
RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly
decouples human preferences regarding helpfulness and harmlessness, effectively
avoiding the crowdworkers' confusion about the tension and allowing us to train
separate reward and cost models. We formalize the safety concern of LLMs as an
optimization task of maximizing the reward function while satisfying specified
cost constraints. Leveraging the Lagrangian method to solve this constrained
problem, Safe RLHF dynamically adjusts the balance between the two objectives
during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we
demonstrate a superior ability to mitigate harmful responses while enhancing
model performance compared to existing value-aligned algorithms.
Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with
collected human preferences, significantly improving its helpfulness and
harmlessness according to human evaluations.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12774" title="Abstract">arXiv:2310.12774</a> [<a href="/pdf/2310.12774" title="Download PDF">pdf</a>, <a href="/format/2310.12774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survival of the Most Influential Prompts: Efficient Black-Box Prompt  Search via Clustering and Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xingchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023. 10 pages, 5 figures, 4 tables (14 pages, 5 figures, 8 tables including references and appendices)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompt-based learning has been an effective paradigm for large pretrained
language models (LLM), enabling few-shot or even zero-shot learning. Black-box
prompt search has received growing interest recently for its distinctive
properties of gradient-free optimization, proven particularly useful and
powerful for model-as-a-service usage. However, the discrete nature and the
complexity of combinatorial optimization hinder the efficiency of modern
black-box approaches. Despite extensive research on search algorithms, the
crucial aspect of search space design and optimization has been largely
overlooked. In this paper, we first conduct a sensitivity analysis by prompting
LLM, revealing that only a small number of tokens exert a disproportionate
amount of influence on LLM predictions. Leveraging this insight, we propose the
Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple
black-box search method that first clusters and prunes the search space to
focus exclusively on influential prompt tokens. By employing even simple search
methods within the pruned search space, ClaPS achieves state-of-the-art
performance across various tasks and LLMs, surpassing the performance of
complex approaches while significantly reducing search costs. Our findings
underscore the critical role of search space design and optimization in
enhancing both the usefulness and the efficiency of black-box prompt-based
learning.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12775" title="Abstract">arXiv:2310.12775</a> [<a href="/pdf/2310.12775" title="Download PDF">pdf</a>, <a href="/ps/2310.12775" title="Download PostScript">ps</a>, <a href="/format/2310.12775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ethical Aspects of Faking Emotions in Chatbots and Social Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Indurkhya%2C+B">Bipin Indurkhya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of Ro-MAN 2023, Busan, South Korea, Aug. 28-31, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Telling lies and faking emotions is quite common in human-human interactions:
though there are risks, in many situations such behaviours provide social
benefits. In recent years, there have been many social robots and chatbots that
fake emotions or behave deceptively with their users. In this paper, I present
a few examples of such robots and chatbots, and analyze their ethical aspects.
Three scenarios are presented where some kind of lying or deceptive behaviour
might be justified. Then five approaches to deceptive behaviours - no
deception, blatant deception, tactful deception, nudging, and self deception -
are discussed and their implications are analyzed. I conclude by arguing that
we need to develop localized and culture-specific solutions to incorporating
deception in social robots and chatbots.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12778" title="Abstract">arXiv:2310.12778</a> [<a href="/pdf/2310.12778" title="Download PDF">pdf</a>, <a href="/format/2310.12778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-Aware Automatic Verbalizer for Few-Shot Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thaminkaew%2C+T">Thanakorn Thaminkaew</a>, 
<a href="/search/cs?searchtype=author&query=Lertvittayakumjorn%2C+P">Piyawat Lertvittayakumjorn</a>, 
<a href="/search/cs?searchtype=author&query=Vateekul%2C+P">Peerapon Vateekul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompt-based learning has shown its effectiveness in few-shot text
classification. One important factor in its success is a verbalizer, which
translates output from a language model into a predicted class. Notably, the
simplest and widely acknowledged verbalizer employs manual labels to represent
the classes. However, manual selection does not guarantee the optimality of the
selected words when conditioned on the chosen language model. Therefore, we
propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the
manual labels to achieve better few-shot classification results. Specifically,
we use the manual labels along with the conjunction "and" to induce the model
to generate more effective words for the verbalizer. The experimental results
on five datasets across five languages demonstrate that LAAV significantly
outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV
suggests more relevant words compared to similar approaches, especially in
mid-to-low resource languages.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12785" title="Abstract">arXiv:2310.12785</a> [<a href="/pdf/2310.12785" title="Download PDF">pdf</a>, <a href="/format/2310.12785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off  Pareto Frontier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hua Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">While the accuracy-fairness trade-off has been frequently observed in the
literature of fair machine learning, rigorous theoretical analyses have been
scarce. To demystify this long-standing challenge, this work seeks to develop a
theoretical framework by characterizing the shape of the accuracy-fairness
trade-off Pareto frontier (FairFrontier), determined by a set of all optimal
Pareto classifiers that no other classifiers can dominate. Specifically, we
first demonstrate the existence of the trade-off in real-world scenarios and
then propose four potential categories to characterize the important properties
of the accuracy-fairness Pareto frontier. For each category, we identify the
necessary conditions that lead to corresponding trade-offs. Experimental
results on synthetic data suggest insightful findings of the proposed
framework: (1) When sensitive attributes can be fully interpreted by
non-sensitive attributes, FairFrontier is mostly continuous. (2) Accuracy can
suffer a \textit{sharp} decline when over-pursuing fairness. (3) Eliminate the
trade-off via a two-step streamlined approach. The proposed research enables an
in-depth understanding of the accuracy-fairness trade-off, pushing current fair
machine-learning research to a new frontier.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12786" title="Abstract">arXiv:2310.12786</a> [<a href="/pdf/2310.12786" title="Download PDF">pdf</a>, <a href="/format/2310.12786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SYNPA: SMT Performance Analysis and Allocation of Threads to Cores in  ARM Processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navarro%2C+M">Marta Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Feliu%2C+J">Josu&#xe9; Feliu</a>, 
<a href="/search/cs?searchtype=author&query=Petit%2C+S">Salvador Petit</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez%2C+M+E">Mar&#xed;a E. G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Sahuquillo%2C+J">Julio Sahuquillo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF)

</div>
<p class="mathjax">Simultaneous multithreading processors improve throughput over
single-threaded processors thanks to sharing internal core resources among
instructions from distinct threads. However, resource sharing introduces
inter-thread interference within the core, which has a negative impact on
individual application performance and can significantly increase the
turnaround time of multi-program workloads. The severity of the interference
effects depends on the competing co-runners sharing the core. Thus, it can be
mitigated by applying a thread-to-core allocation policy that smartly selects
applications to be run in the same core to minimize their interference.
<br />This paper presents SYNPA, a simple approach that dynamically allocates
threads to cores in an SMT processor based on their run-time dynamic behavior.
The approach uses a regression model to select synergistic pairs to mitigate
intra-core interference. The main novelty of SYNPA is that it uses just three
variables collected from the performance counters available in current ARM
processors at the dispatch stage. Experimental results show that SYNPA
outperforms the default Linux scheduler by around 36%, on average, in terms of
turnaround time in 8-application workloads combining frontend bound and backend
bound benchmarks.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12787" title="Abstract">arXiv:2310.12787</a> [<a href="/pdf/2310.12787" title="Download PDF">pdf</a>, <a href="/format/2310.12787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DT/MARS-CycleGAN: Improved Object Detection for MARS Phenotyping Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">David Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhengkun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changying Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Robotic crop phenotyping has emerged as a key technology to assess crops'
morphological and physiological traits at scale. These phenotypical
measurements are essential for developing new crop varieties with the aim of
increasing productivity and dealing with environmental challenges such as
climate change. However, developing and deploying crop phenotyping robots face
many challenges such as complex and variable crop shapes that complicate
robotic object detection, dynamic and unstructured environments that baffle
robotic control, and real-time computing and managing big data that challenge
robotic hardware/software. This work specifically tackles the first challenge
by proposing a novel Digital-Twin(DT)MARS-CycleGAN model for image augmentation
to improve our Modular Agricultural Robotic System (MARS)'s crop object
detection from complex and variable backgrounds. Our core idea is that in
addition to the cycle consistency losses in the CycleGAN model, we designed and
enforced a new DT-MARS loss in the deep learning model to penalize the
inconsistency between real crop images captured by MARS and synthesized images
sensed by DT MARS. Therefore, the generated synthesized crop images closely
mimic real images in terms of realism, and they are employed to fine-tune
object detectors such as YOLOv8. Extensive experiments demonstrated that our
new DT/MARS-CycleGAN framework significantly boosts our MARS' crop object/row
detector's performance, contributing to the field of robotic crop phenotyping.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12790" title="Abstract">arXiv:2310.12790</a> [<a href="/pdf/2310.12790" title="Download PDF">pdf</a>, <a href="/format/2310.12790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiawen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Choubo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Open-set supervised anomaly detection (OSAD) - a recently emerging anomaly
detection area - aims at utilizing a few samples of anomaly classes seen during
training to detect unseen anomalies (i.e., samples from open-set anomaly
classes), while effectively identifying the seen anomalies. Benefiting from the
prior knowledge illustrated by the seen anomalies, current OSAD methods can
often largely reduce false positive errors. However, these methods treat the
anomaly examples as from a homogeneous distribution, rendering them less
effective in generalizing to unseen anomalies that can be drawn from any
distribution. In this paper, we propose to learn heterogeneous anomaly
distributions using the limited anomaly examples to address this issue. To this
end, we introduce a novel approach, namely Anomaly Heterogeneity Learning
(AHL), that simulates a diverse set of heterogeneous (seen and unseen) anomaly
distributions and then utilizes them to learn a unified heterogeneous
abnormality model. Further, AHL is a generic framework that existing OSAD
models can plug and play for enhancing their abnormality modeling. Extensive
experiments on nine real-world anomaly detection datasets show that AHL can 1)
substantially enhance different state-of-the-art (SOTA) OSAD models in
detecting both seen and unseen anomalies, achieving new SOTA performance on a
large set of datasets, and 2) effectively generalize to unseen anomalies in new
target domains.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12792" title="Abstract">arXiv:2310.12792</a> [<a href="/pdf/2310.12792" title="Download PDF">pdf</a>, <a href="/format/2310.12792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Optimal Locality Sensitive Orderings in Euclidean Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhimeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Har-Peled%2C+S">Sariel Har-Peled</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">$ \newcommand{\Re}{\mathbb{R}} \newcommand{\reals}{\mathbb{R}}
\newcommand{\SetX}{\mathsf{X}} \newcommand{\rad}{r}
\newcommand{\Eps}{\Mh{\mathcal{E}}} \newcommand{\p}{\Mh{p}}
\newcommand{\q}{\Mh{q}} \newcommand{\Mh}[1]{#1} \newcommand{\query}{q}
\newcommand{\eps}{\varepsilon} \newcommand{\VorX}[1]{\mathcal{V} \pth{#1}}
\newcommand{\Polygon}{\mathsf{P}} \newcommand{\IntRange}[1]{[ #1 ]}
\newcommand{\Space}{\overline{\mathsf{m}}}
\newcommand{\pth}[2][\!]{#1\left({#2}\right)}
\newcommand{\polylog}{\mathrm{polylog}} \newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z} \newcommand{\pt}{p} \newcommand{\distY}[2]{\left\|
{#1} - {#2} \right\|} \newcommand{\ptq}{q} \newcommand{\pts}{s}$
<br />For a parameter $\eps \in (0,1)$, we present a new construction of
$\eps$-locality-sensitive orderings (&lt;em&gt;LSO&lt;/em&gt;s) in $\Re^d$ of size $M =
O(\Eps^{d-1} \log \Eps)$, where $\Eps = 1/\eps$. This improves over previous
work by a factor of $\Eps$, and is optimal up to a factor of $\log \Eps$. Such
a set of LSOs has the property that for any two points, $\p, \q \in [0,1]^d$,
there exist an order in the set such that all the points between $\p$ and $\q$
in the order are $\eps$-close to either $\p$ or $\q$.
<br />The existence of such LSOs is a fundamental property of low dimensional
Euclidean space, conceptually similar to the existence of well-separated pairs
decomposition, so the question of how to compute (near) optimal construction of
LSOs is quite natural.
<br />As a consequence we get a flotilla of improved dynamic geometric algorithms,
such as maintaining bichromatic closest pair, and spanners, among others. In
particular, for geometric dynamic spanners the new result matches (up to the
aforementioned $\log \Eps$ factor) the lower bound, Thus offering a
near-optimal simple dynamic data-structure for maintaining spanners under
insertions and deletions.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12793" title="Abstract">arXiv:2310.12793</a> [<a href="/pdf/2310.12793" title="Download PDF">pdf</a>, <a href="/format/2310.12793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OODRobustBench: benchmarking and analyzing adversarial robustness under  distribution shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sitawarin%2C+C">Chawin Sitawarin</a>, 
<a href="/search/cs?searchtype=author&query=Spratling%2C+M">Michael Spratling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Existing works have made great progress in improving adversarial robustness,
but typically test their method only on data from the same distribution as the
training data, i.e. in-distribution (ID) testing. As a result, it is unclear
how such robustness generalizes under input distribution shifts, i.e.
out-of-distribution (OOD) testing. This is a concerning omission as such
distribution shifts are unavoidable when methods are deployed in the wild. To
address this issue we propose a benchmark named OODRobustBench to
comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts
(i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts
(i.e., unforeseen adversarial threat models). OODRobustBench is used to assess
706 robust models using 60.7K adversarial evaluations. This large-scale
analysis shows that: 1) adversarial robustness suffers from a severe OOD
generalization issue; 2) ID robustness correlates strongly with OOD robustness,
in a positive linear way, under many distribution shifts. The latter enables
the prediction of OOD robustness from ID robustness. Based on this, we are able
to predict the upper limit of OOD robustness for existing robust training
schemes. The results suggest that achieving OOD robustness requires designing
novel methods beyond the conventional ones. Last, we discover that extra data,
data augmentation, advanced model architectures and particular regularization
approaches can improve OOD robustness. Noticeably, the discovered training
schemes, compared to the baseline, exhibit dramatically higher robustness under
threat shift while keeping high ID robustness, demonstrating new promising
solutions for robustness against both multi-attack and unforeseen attacks.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12794" title="Abstract">arXiv:2310.12794</a> [<a href="/pdf/2310.12794" title="Download PDF">pdf</a>, <a href="/format/2310.12794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Structural Concepts Universal in Transformer Language Models?  Towards Interpretable Cross-Lingual Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jingting Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Menghan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have exhibited considerable cross-lingual
generalization abilities, whereby they implicitly transfer knowledge across
languages. However, the transfer is not equally successful for all languages,
especially for low-resource ones, which poses an ongoing challenge. It is
unclear whether we have reached the limits of implicit cross-lingual
generalization and if explicit knowledge transfer is viable. In this paper, we
investigate the potential for explicitly aligning conceptual correspondence
between languages to enhance cross-lingual generalization. Using the syntactic
aspect of language as a testbed, our analyses of 43 languages reveal a high
degree of alignability among the spaces of structural concepts within each
language for both encoder-only and decoder-only LLMs. We then propose a
meta-learning-based method to learn to align conceptual spaces of different
languages, which facilitates zero-shot and few-shot generalization in concept
classification and also offers insights into the cross-lingual in-context
learning phenomenon. Experiments on syntactic analysis tasks show that our
approach achieves competitive results with state-of-the-art methods and narrows
the performance gap between languages, particularly benefiting those with
limited resources.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12795" title="Abstract">arXiv:2310.12795</a> [<a href="/pdf/2310.12795" title="Download PDF">pdf</a>, <a href="/format/2310.12795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-triggered Consensus Control of Multi-agent Systems from Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+J">Jian Sun</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jie Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper considers self-triggered consensus control of unknown linear
multi-agent systems (MASs). Self-triggering mechanisms (STMs) are widely used
in MASs, thanks to their advantages in avoiding continuous monitoring and
saving computing and communication resources. However, existing results require
the knowledge of system matrices, which are difficult to obtain in real-world
settings. To address this challenge, we present a data-driven approach to
designing STMs for unknown MASs building upon the model-based solutions. Our
approach leverages a system lifting method, which allows us to derive a
data-driven representation for the MAS. Subsequently, a data-driven
self-triggered consensus control (STC) scheme is designed, which combines a
data-driven STM with a state feedback control law. We establish a data-based
stability criterion for asymptotic consensus of the closed-loop MAS in terms of
linear matrix inequalities, whose solution provides a matrix for the STM as
well as a stabilizing controller gain. In the presence of external
disturbances, a model-based STC scheme is put forth for
$\mathcal{H}_{\infty}$-consensus of MASs, serving as a baseline for the
data-driven STC. Numerical tests are conducted to validate the correctness of
the data- and model-based STC approaches. Our data-driven approach demonstrates
a superior trade-off between control performance and communication efficiency
from finite, noisy data relative to the system identification-based one.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12798" title="Abstract">arXiv:2310.12798</a> [<a href="/pdf/2310.12798" title="Download PDF">pdf</a>, <a href="/format/2310.12798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and  Uni-Modal Adapter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sihang Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yanchen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+H">Hao Fei</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP main conference. 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Language Models (LMs) have demonstrated impressive molecule understanding
ability on various 1D text-related tasks. However, they inherently lack 2D
graph perception - a critical ability of human professionals in comprehending
molecules' topological structures. To bridge this gap, we propose MolCA:
Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal
Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and
graph-based molecular contents via the cross-modal projector. Specifically, the
cross-modal projector is implemented as a Q-Former to connect a graph encoder's
representation space and an LM's text space. Further, MolCA employs a uni-modal
adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks.
Unlike previous studies that couple an LM with a graph encoder via cross-modal
contrastive learning, MolCA retains the LM's ability of open-ended text
generation and augments it with 2D graph information. To showcase its
effectiveness, we extensively benchmark MolCA on tasks of molecule captioning,
IUPAC name prediction, and molecule-text retrieval, on which MolCA
significantly outperforms the baselines. Our codes and checkpoints can be found
at https://github.com/acharkq/MolCA.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12800" title="Abstract">arXiv:2310.12800</a> [<a href="/pdf/2310.12800" title="Download PDF">pdf</a>, <a href="/format/2310.12800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Graph Neural Networks for Indian Legal Judgment Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khatri%2C+M">Mann Khatri</a>, 
<a href="/search/cs?searchtype=author&query=Yusuf%2C+M">Mirza Yusuf</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+Y">Yaman Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R+R">Rajiv Ratn Shah</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The burdensome impact of a skewed judges-to-cases ratio on the judicial
system manifests in an overwhelming backlog of pending cases alongside an
ongoing influx of new ones. To tackle this issue and expedite the judicial
process, the proposition of an automated system capable of suggesting case
outcomes based on factual evidence and precedent from past cases gains
significance. This research paper centres on developing a graph neural
network-based model to address the Legal Judgment Prediction (LJP) problem,
recognizing the intrinsic graph structure of judicial cases and making it a
binary node classification problem. We explored various embeddings as model
features, while nodes such as time nodes and judicial acts were added and
pruned to evaluate the model's performance. The study is done while considering
the ethical dimension of fairness in these predictions, considering gender and
name biases. A link prediction task is also conducted to assess the model's
proficiency in anticipating connections between two specified nodes. By
harnessing the capabilities of graph neural networks and incorporating fairness
analyses, this research aims to contribute insights towards streamlining the
adjudication process, enhancing judicial efficiency, and fostering a more
equitable legal landscape, ultimately alleviating the strain imposed by
mounting case backlogs. Our best-performing model with XLNet pre-trained
embeddings as its features gives the macro F1 score of 75% for the LJP task.
For link prediction, the same set of features is the best performing giving ROC
of more than 80%
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12803" title="Abstract">arXiv:2310.12803</a> [<a href="/pdf/2310.12803" title="Download PDF">pdf</a>, <a href="/format/2310.12803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal-structure Driven Augmentations for Text OOD Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feder%2C+A">Amir Feder</a>, 
<a href="/search/cs?searchtype=author&query=Wald%2C+Y">Yoav Wald</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Claudia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Saria%2C+S">Suchi Saria</a>, 
<a href="/search/cs?searchtype=author&query=Blei%2C+D">David Blei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Forthcoming in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The reliance of text classifiers on spurious correlations can lead to poor
generalization at deployment, raising concerns about their use in
safety-critical domains such as healthcare. In this work, we propose to use
counterfactual data augmentation, guided by knowledge of the causal structure
of the data, to simulate interventions on spurious features and to learn more
robust text classifiers. We show that this strategy is appropriate in
prediction problems where the label is spuriously correlated with an attribute.
Under the assumptions of such problems, we discuss the favorable sample
complexity of counterfactual data augmentation, compared to importance
re-weighting. Pragmatically, we match examples using auxiliary data, based on
diff-in-diff methodology, and use a large language model (LLM) to represent a
conditional probability of text. Through extensive experimentation on learning
caregiver-invariant predictors of clinical diagnoses from medical narratives
and on semi-synthetic data, we demonstrate that our method for simulating
interventions improves out-of-distribution (OOD) accuracy compared to baseline
invariant learning algorithms.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12805" title="Abstract">arXiv:2310.12805</a> [<a href="/pdf/2310.12805" title="Download PDF">pdf</a>, <a href="/format/2310.12805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection and Evaluation of bias-inducing Features in Machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Openja%2C+M">Moses Openja</a>, 
<a href="/search/cs?searchtype=author&query=Laberge%2C+G">Gabriel Laberge</a>, 
<a href="/search/cs?searchtype=author&query=Khomh%2C+F">Foutse Khomh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 65 pages, manuscript accepted at EMSE journal, manuscript number, EMSE-D-22-00330R3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The cause-to-effect analysis can help us decompose all the likely causes of a
problem, such as an undesirable business situation or unintended harm to the
individual(s). This implies that we can identify how the problems are
inherited, rank the causes to help prioritize fixes, simplify a complex problem
and visualize them. In the context of machine learning (ML), one can use
cause-to-effect analysis to understand the reason for the biased behavior of
the system. For example, we can examine the root causes of biases by checking
each feature for a potential cause of bias in the model. To approach this, one
can apply small changes to a given feature or a pair of features in the data,
following some guidelines and observing how it impacts the decision made by the
model (i.e., model prediction). Therefore, we can use cause-to-effect analysis
to identify the potential bias-inducing features, even when these features are
originally are unknown. This is important since most current methods require a
pre-identification of sensitive features for bias assessment and can actually
miss other relevant bias-inducing features, which is why systematic
identification of such features is necessary. Moreover, it often occurs that to
achieve an equitable outcome, one has to take into account sensitive features
in the model decision. Therefore, it should be up to the domain experts to
decide based on their knowledge of the context of a decision whether bias
induced by specific features is acceptable or not. In this study, we propose an
approach for systematically identifying all bias-inducing features of a model
to help support the decision-making of domain experts. We evaluated our
technique using four well-known datasets to showcase how our contribution can
help spearhead the standard procedure when developing, testing, maintaining,
and deploying fair/equitable machine learning systems.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12808" title="Abstract">arXiv:2310.12808</a> [<a href="/pdf/2310.12808" title="Download PDF">pdf</a>, <a href="/format/2310.12808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Merging by Uncertainty-Based Gradient Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daheim%2C+N">Nico Daheim</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6llenhoff%2C+T">Thomas M&#xf6;llenhoff</a>, 
<a href="/search/cs?searchtype=author&query=Ponti%2C+E+M">Edoardo Maria Ponti</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+E">Mohammad Emtiyaz Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Models trained on different datasets can be merged by a weighted-averaging of
their parameters, but why does it work and when can it fail? Here, we connect
the inaccuracy of weighted-averaging to mismatches in the gradients and propose
a new uncertainty-based scheme to improve the performance by reducing the
mismatch. The connection also reveals implicit assumptions in other schemes
such as averaging, task arithmetic, and Fisher-weighted averaging. Our new
method gives consistent improvements for large language models and vision
transformers, both in terms of performance and robustness to hyperparameters.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12809" title="Abstract">arXiv:2310.12809</a> [<a href="/pdf/2310.12809" title="Download PDF">pdf</a>, <a href="/format/2310.12809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Forecasting at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sprangers%2C+O">Olivier Sprangers</a>, 
<a href="/search/cs?searchtype=author&query=Wadman%2C+W">Wander Wadman</a>, 
<a href="/search/cs?searchtype=author&query=Schelter%2C+S">Sebastian Schelter</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Existing hierarchical forecasting techniques scale poorly when the number of
time series increases. We propose to learn a coherent forecast for millions of
time series with a single bottom-level forecast model by using a sparse loss
function that directly optimizes the hierarchical product and/or temporal
structure. The benefit of our sparse hierarchical loss function is that it
provides practitioners a method of producing bottom-level forecasts that are
coherent to any chosen cross-sectional or temporal hierarchy. In addition,
removing the need for a post-processing step as required in traditional
hierarchical forecasting techniques reduces the computational cost of the
prediction phase in the forecasting pipeline. On the public M5 dataset, our
sparse hierarchical loss function performs up to 10% (RMSE) better compared to
the baseline loss function. We implement our sparse hierarchical loss function
within an existing forecasting model at bol, a large European e-commerce
platform, resulting in an improved forecasting performance of 2% at the product
level. Finally, we found an increase in forecasting performance of about 5-10%
when evaluating the forecasting performance across the cross-sectional
hierarchies that we defined. These results demonstrate the usefulness of our
sparse hierarchical loss applied to a production forecasting system at a major
e-commerce platform.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12815" title="Abstract">arXiv:2310.12815</a> [<a href="/pdf/2310.12815" title="Download PDF">pdf</a>, <a href="/format/2310.12815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Injection Attacks and Defenses in LLM-Integrated Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yupei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yuqi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+R">Runpeng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinyuan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) are increasingly deployed as the backend for a
variety of real-world applications called LLM-Integrated Applications. Multiple
recent works showed that LLM-Integrated Applications are vulnerable to prompt
injection attacks, in which an attacker injects malicious instruction/data into
the input of those applications such that they produce results as the attacker
desires. However, existing works are limited to case studies. As a result, the
literature lacks a systematic understanding of prompt injection attacks and
their defenses. We aim to bridge the gap in this work. In particular, we
propose a general framework to formalize prompt injection attacks. Existing
attacks, which are discussed in research papers and blog posts, are special
cases in our framework. Our framework enables us to design a new attack by
combining existing attacks. Moreover, we also propose a framework to
systematize defenses against prompt injection attacks. Using our frameworks, we
conduct a systematic evaluation on prompt injection attacks and their defenses
with 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in
this field. Our code is available at
https://github.com/liu00222/Open-Prompt-Injection.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12816" title="Abstract">arXiv:2310.12816</a> [<a href="/pdf/2310.12816" title="Download PDF">pdf</a>, <a href="/format/2310.12816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Robot Local Motion Planning Using Dynamic Optimization Fabrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakker%2C+S">Saray Bakker</a>, 
<a href="/search/cs?searchtype=author&query=Knoedler%2C+L">Luzia Knoedler</a>, 
<a href="/search/cs?searchtype=author&query=Spahn%2C+M">Max Spahn</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hmer%2C+W">Wendelin B&#xf6;hmer</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages + 1 page references, 2 tables, 4 figures, preprint version to accepted paper to IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems, Boston, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In this paper, we address the problem of real-time motion planning for
multiple robotic manipulators that operate in close proximity. We build upon
the concept of dynamic fabrics and extend them to multi-robot systems, referred
to as Multi-Robot Dynamic Fabrics (MRDF). This geometric method enables a very
high planning frequency for high-dimensional systems at the expense of being
reactive and prone to deadlocks. To detect and resolve deadlocks, we propose
Rollout Fabrics where MRDF are forward simulated in a decentralized manner. We
validate the methods in simulated close-proximity pick-and-place scenarios with
multiple manipulators, showing high success rates and real-time performance.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12817" title="Abstract">arXiv:2310.12817</a> [<a href="/pdf/2310.12817" title="Download PDF">pdf</a>, <a href="/format/2310.12817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 2D-3D Interlaced Transformer for Point Cloud Segmentation with  Scene-Level Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng-Kun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min-Hung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yung-Yu Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yen-Yu Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV 2023 (main + supp). Website: <a href="https://jimmy15923.github.io/mit_web/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a Multimodal Interlaced Transformer (MIT) that jointly considers
2D and 3D data for weakly supervised point cloud segmentation. Research studies
have shown that 2D and 3D features are complementary for point cloud
segmentation. However, existing methods require extra 2D annotations to achieve
2D-3D information fusion. Considering the high annotation cost of point clouds,
effective 2D and 3D feature fusion based on weakly supervised learning is in
great demand. To this end, we propose a transformer model with two encoders and
one decoder for weakly supervised point cloud segmentation using only
scene-level class tags. Specifically, the two encoders compute the
self-attended features for 3D point clouds and 2D multi-view images,
respectively. The decoder implements interlaced 2D-3D cross-attention and
carries out implicit 2D and 3D feature fusion. We alternately switch the roles
of queries and key-value pairs in the decoder layers. It turns out that the 2D
and 3D features are iteratively enriched by each other. Experiments show that
it performs favorably against existing weakly supervised point cloud
segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The
project page will be available at https://jimmy15923.github.io/mit_web/.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12818" title="Abstract">arXiv:2310.12818</a> [<a href="/pdf/2310.12818" title="Download PDF">pdf</a>, <a href="/format/2310.12818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared  Pre-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weize Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Parameter-shared pre-trained language models (PLMs) have emerged as a
successful approach in resource-constrained environments, enabling substantial
reductions in model storage and memory costs without significant performance
compromise. However, it is important to note that parameter sharing does not
alleviate computational burdens associated with inference, thus impeding its
practicality in situations characterized by limited stringent latency
requirements or computational resources. Building upon neural ordinary
differential equations (ODEs), we introduce a straightforward technique to
enhance the inference efficiency of parameter-shared PLMs. Additionally, we
propose a simple pre-training technique that leads to fully or partially shared
models capable of achieving even greater inference acceleration. The
experimental results demonstrate the effectiveness of our methods on both
autoregressive and autoencoding PLMs, providing novel insights into more
efficient utilization of parameter-shared models in resource-constrained
settings.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12819" title="Abstract">arXiv:2310.12819</a> [<a href="/pdf/2310.12819" title="Download PDF">pdf</a>, <a href="/format/2310.12819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Search for Efficient Planning with Completeness Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kujanp%C3%A4%C3%A4%2C+K">Kalle Kujanp&#xe4;&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Pajarinen%2C+J">Joni Pajarinen</a>, 
<a href="/search/cs?searchtype=author&query=Ilin%2C+A">Alexander Ilin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Solving complex planning problems has been a long-standing challenge in
computer science. Learning-based subgoal search methods have shown promise in
tackling these problems, but they often suffer from a lack of completeness
guarantees, meaning that they may fail to find a solution even if one exists.
In this paper, we propose an efficient approach to augment a subgoal search
method to achieve completeness in discrete action spaces. Specifically, we
augment the high-level search with low-level actions to execute a multi-level
(hybrid) search, which we call complete subgoal search. This solution achieves
the best of both worlds: the practical efficiency of high-level search and the
completeness of low-level search. We apply the proposed search method to a
recently proposed subgoal search algorithm and evaluate the algorithm trained
on offline data on complex planning problems. We demonstrate that our complete
subgoal search not only guarantees completeness but can even improve
performance in terms of search expansions for instances that the high-level
could solve without low-level augmentations. Our approach makes it possible to
apply subgoal-level planning for systems where completeness is a critical
requirement.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12820" title="Abstract">arXiv:2310.12820</a> [<a href="/pdf/2310.12820" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Centralized Voltage Controller for Offshore Wind Plants: NY State Grid  Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Lin Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Leonardi%2C+B">Bruno Leonardi</a>, 
<a href="/search/eess?searchtype=author&query=Haddadi%2C+A">Aboutaleb Haddadi</a>, 
<a href="/search/eess?searchtype=author&query=Dutta%2C+S">Sudipta Dutta</a>, 
<a href="/search/eess?searchtype=author&query=Del+Rosso%2C+A">Alberto Del Rosso</a>, 
<a href="/search/eess?searchtype=author&query=Paduani%2C+V">Victor Paduani</a>, 
<a href="/search/eess?searchtype=author&query=Hooshyar%2C+H">Hossein Hooshyar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 13 figures, conference paper, IEEE PES Innovative Smart Grid Technologies - Latin America, San Juan, Puerto Rico, Nov. 6-9, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper proposes a centralized multi-plant reactive power and voltage
controller to support voltage control in the interconnected onshore power
system. This controller utilizes a hierarchical control structure consisting of
a master controller and multiple slave controllers. To validate the proposed
method, a realistic planning case of the New York State grid is created for the
year 2035, in which nearly 9,500 MW AC or DC connected offshore wind resources
are modeled. The performance of the proposed controller is analyzed in the
large-scale model under three realistic disturbance scenarios: generator loss,
load ramps, and load steps. Results demonstrate how the controller can
adequately perform under disturbances to share reactive support proportionally
among plants based on their ratings and improve grid voltage stability margins.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12821" title="Abstract">arXiv:2310.12821</a> [<a href="/pdf/2310.12821" title="Download PDF">pdf</a>, <a href="/format/2310.12821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding  with Large Language Model Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tengxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shengdong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiqiang Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Current gesture recognition systems primarily focus on identifying gestures
within a predefined set, leaving a gap in connecting these gestures to
interactive GUI elements or system functions (e.g., linking a 'thumb-up'
gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture
understanding and grounding framework leveraging large language models (LLMs).
Gesture descriptions are formulated based on hand landmark coordinates from
gesture videos and fed into our dual-agent dialogue system. A gesture agent
deciphers these descriptions and queries about the interaction context (e.g.,
interface, history, gaze data), which a context agent organizes and provides.
Following iterative exchanges, the gesture agent discerns user intent,
grounding it to an interactive function. We validated the gesture description
module using public first-view and third-view gesture datasets and tested the
whole system in two real-world settings: video streaming and smart home IoT
control. The highest zero-shot Top-5 grounding accuracies are 80.11% for video
streaming and 90.78% for smart home tasks, showing potential of the new gesture
understanding paradigm.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12823" title="Abstract">arXiv:2310.12823</a> [<a href="/pdf/2310.12823" title="Download PDF">pdf</a>, <a href="/format/2310.12823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AgentTuning: Enabling Generalized Agent Abilities for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Aohan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingdao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Rui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Open large language models (LLMs) with great performance in various tasks
have significantly advanced the development of LLMs. However, they are far
inferior to commercial models such as ChatGPT and GPT-4 when acting as agents
to tackle complex tasks in the real world. These agent tasks employ LLMs as the
central controller responsible for planning, memorization, and tool
utilization, necessitating both fine-grained prompting methods and robust LLMs
to achieve satisfactory performance. Though many prompting methods have been
proposed to complete particular agent tasks, there is lack of research focusing
on improving the agent capabilities of LLMs themselves without compromising
their general abilities. In this work, we present AgentTuning, a simple and
general method to enhance the agent abilities of LLMs while maintaining their
general LLM capabilities. We construct AgentInstruct, a lightweight
instruction-tuning dataset containing high-quality interaction trajectories. We
employ a hybrid instruction-tuning strategy by combining AgentInstruct with
open-source instructions from general domains. AgentTuning is used to
instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show
that AgentTuning enables LLMs' agent capabilities without compromising general
abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent
tasks, demonstrating generalized agent capabilities. We open source the
AgentInstruct and AgentLM-7B, 13B, and 70B models at
https://github.com/THUDM/AgentTuning , serving open and powerful alternatives
to commercial LLMs for agent tasks.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12827" title="Abstract">arXiv:2310.12827</a> [<a href="/pdf/2310.12827" title="Download PDF">pdf</a>, <a href="/format/2310.12827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privately Answering Queries on Skewed Data via Per Record Differential  Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seeman%2C+J">Jeremy Seeman</a>, 
<a href="/search/cs?searchtype=author&query=Sexton%2C+W">William Sexton</a>, 
<a href="/search/cs?searchtype=author&query=Pujol%2C+D">David Pujol</a>, 
<a href="/search/cs?searchtype=author&query=Machanavajjhala%2C+A">Ashwin Machanavajjhala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">We consider the problem of the private release of statistics (like aggregate
payrolls) where it is critical to preserve the contribution made by a small
number of outlying large entities. We propose a privacy formalism, per-record
zero concentrated differential privacy (PzCDP), where the privacy loss
associated with each record is a public function of that record's value. Unlike
other formalisms which provide different privacy losses to different records,
PzCDP's privacy loss depends explicitly on the confidential data. We define our
formalism, derive its properties, and propose mechanisms which satisfy PzCDP
that are uniquely suited to publishing skewed or heavy-tailed statistics, where
a small number of records contribute substantially to query answers. This
targeted relaxation helps overcome the difficulties of applying standard DP to
these data products.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12828" title="Abstract">arXiv:2310.12828</a> [<a href="/pdf/2310.12828" title="Download PDF">pdf</a>, <a href="/format/2310.12828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Informed Trees (FIT*): Adaptive Batch-Size Approach for  Informed Sampling-Based Planner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liding Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+Z">Zhenshan Bing</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kejia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lingyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Krumbholz%2C+P">Peter Krumbholz</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhilin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Haddadin%2C+S">Sami Haddadin</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In modern approaches to path planning and robot motion planning, anytime
almost-surely asymptotically optimal planners dominate the benchmark of
sample-based planners. A notable example is Batch Informed Trees (BIT*), where
planners iteratively determine paths to groups of vertices within the
exploration area. However, maintaining a consistent batch size is crucial for
initial pathfinding and optimal performance, relying on effective task
allocation. This paper introduces Flexible Informed Tree (FIT*), a novel
planner integrating an adaptive batch-size method to enhance task scheduling in
various environments. FIT* employs a flexible approach in adjusting batch sizes
dynamically based on the inherent complexity of the planning domain and the
current n-dimensional hyperellipsoid of the system. By constantly optimizing
batch sizes, FIT* achieves improved computational efficiency and scalability
while maintaining solution quality. This adaptive batch-size method
significantly enhances the planner's ability to handle diverse and evolving
problem domains. FIT* outperforms existing single-query, sampling-based
planners on the tested problems in R^2 to R^8, and was demonstrated in
real-world environments with KI-Fabrik/DARKO-Project Europe.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12831" title="Abstract">arXiv:2310.12831</a> [<a href="/pdf/2310.12831" title="Download PDF">pdf</a>, <a href="/format/2310.12831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Metric Imitation Learning for Stable Motion Primitives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Dattari%2C+R">Rodrigo P&#xe9;rez-Dattari</a>, 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Imitation Learning (IL) is a powerful technique for intuitive robotic
programming. However, ensuring the reliability of learned behaviors remains a
challenge. In the context of reaching motions, a robot should consistently
reach its goal, regardless of its initial conditions. To meet this requirement,
IL methods often employ specialized function approximators that guarantee this
property by construction. Although effective, these approaches come with a set
of limitations: 1) they are unable to fully exploit the capabilities of modern
Deep Neural Network (DNN) architectures, 2) some are restricted in the family
of motions they can model, resulting in suboptimal IL capabilities, and 3) they
require explicit extensions to account for the geometry of motions that
consider orientations. To address these challenges, we introduce a novel
stability loss function, drawing inspiration from the triplet loss used in the
deep metric learning literature. This loss does not constrain the DNN's
architecture and enables learning policies that yield accurate results.
Furthermore, it is easily adaptable to the geometry of the robot's state space.
We provide a proof of the stability properties induced by this loss and
empirically validate our method in various settings. These settings include
Euclidean and non-Euclidean state spaces, as well as first-order and
second-order motions, both in simulation and with real robots. More details
about the experimental results can be found at: https://youtu.be/ZWKLGntCI6w.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12835" title="Abstract">arXiv:2310.12835</a> [<a href="/pdf/2310.12835" title="Download PDF">pdf</a>, <a href="/format/2310.12835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Dynamic Range mmWave Massive MU-MIMO with Householder Reflections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palhares%2C+V">Victoria Palhares</a>, 
<a href="/search/cs?searchtype=author&query=Marti%2C+G">Gian Marti</a>, 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1eda%2C+O">Oscar Casta&#xf1;eda</a>, 
<a href="/search/cs?searchtype=author&query=Studer%2C+C">Christoph Studer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented at Asilomar Conference on Signals, Systems, and Computers 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">All-digital massive multiuser (MU) multiple-input multiple-output (MIMO) at
millimeter-wave (mmWave) frequencies is a promising technology for
next-generation wireless systems. Low-resolution analog-to-digital converters
(ADCs) can be utilized to reduce the power consumption of all-digital
basestation (BS) designs. However, simultaneously transmitting user equipments
(UEs) with vastly different BS-side receive powers either drown weak UEs in
quantization noise or saturate the ADCs. To address this issue, we propose high
dynamic range (HDR) MIMO, a new paradigm that enables simultaneous reception of
strong and weak UEs with low-resolution ADCs. HDR MIMO combines an adaptive
analog spatial transform with digital equalization: The spatial transform
focuses strong UEs on a subset of ADCs in order to mitigate quantization and
saturation artifacts; digital equalization is then used for data detection. We
demonstrate the efficacy of HDR MIMO in a massive MU-MIMO mmWave scenario that
uses Householder reflections as spatial transform.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12836" title="Abstract">arXiv:2310.12836</a> [<a href="/pdf/2310.12836" title="Download PDF">pdf</a>, <a href="/format/2310.12836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Augmented Language Model Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baek%2C+J">Jinheon Baek</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Soyeong Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Minki Kang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J+C">Jong C. Park</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent Language Models (LMs) have shown impressive capabilities in generating
texts with the knowledge internalized in parameters. Yet, LMs often generate
the factually incorrect responses to the given queries, since their knowledge
may be inaccurate, incomplete, and outdated. To address this problem, previous
works propose to augment LMs with the knowledge retrieved from an external
knowledge source. However, such approaches often show suboptimal text
generation performance due to two reasons: 1) the model may fail to retrieve
the knowledge relevant to the given query, or 2) the model may not faithfully
reflect the retrieved knowledge in the generated text. To overcome these, we
propose to verify the output and the knowledge of the knowledge-augmented LMs
with a separate verifier, which is a small LM that is trained to detect those
two types of errors through instruction-finetuning. Then, when the verifier
recognizes an error, we can rectify it by either retrieving new knowledge or
generating new text. Further, we use an ensemble of the outputs from different
instructions with a single verifier to enhance the reliability of the
verification processes. We validate the effectiveness of the proposed
verification steps on multiple question answering benchmarks, whose results
show that the proposed verifier effectively identifies retrieval and generation
errors, allowing LMs to provide more factually correct outputs. Our code is
available at https://github.com/JinheonBaek/KALMV.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12846" title="Abstract">arXiv:2310.12846</a> [<a href="/pdf/2310.12846" title="Download PDF">pdf</a>, <a href="/format/2310.12846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Information Neural Networks for Solving High-index  Differential-algebraic Equation Systems Based on Radau Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+J">Jiasheng Chen</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+J">Juan Tang</a>, 
<a href="/search/math?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/math?searchtype=author&query=Lai%2C+S">Shuai Lai</a>, 
<a href="/search/math?searchtype=author&query=Liang%2C+K">Kun Liang</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+J">Jianguang Lu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+W">Wenqiang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As is well known, differential algebraic equations (DAEs), which are able to
describe dynamic changes and underlying constraints, have been widely applied
in engineering fields such as fluid dynamics, multi-body dynamics, mechanical
systems and control theory. In practical physical modeling within these
domains, the systems often generate high-index DAEs. Classical implicit
numerical methods typically result in varying order reduction of numerical
accuracy when solving high-index systems.~Recently, the physics-informed neural
network (PINN) has gained attention for solving DAE systems. However, it faces
challenges like the inability to directly solve high-index systems, lower
predictive accuracy, and weaker generalization capabilities. In this paper, we
propose a PINN computational framework, combined Radau IIA numerical method
with a neural network structure via the attention mechanisms, to directly solve
high-index DAEs. Furthermore, we employ a domain decomposition strategy to
enhance solution accuracy. We conduct numerical experiments with two classical
high-index systems as illustrative examples, investigating how different orders
of the Radau IIA method affect the accuracy of neural network solutions. The
experimental results demonstrate that the PINN based on a 5th-order Radau IIA
method achieves the highest level of system accuracy. Specifically, the
absolute errors for all differential variables remains as low as $10^{-6}$, and
the absolute errors for algebraic variables is maintained at $10^{-5}$,
surpassing the results found in existing literature. Therefore, our method
exhibits excellent computational accuracy and strong generalization
capabilities, providing a feasible approach for the high-precision solution of
larger-scale DAEs with higher indices or challenging high-dimensional partial
differential algebraic equation systems.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12848" title="Abstract">arXiv:2310.12848</a> [<a href="/pdf/2310.12848" title="Download PDF">pdf</a>, <a href="/format/2310.12848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Degradation Representation Learning for All-In-One Image  Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+M">Mingde Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruikang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yuanshen Guan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhiwei Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing methods have demonstrated effective performance on a single
degradation type. In practical applications, however, the degradation is often
unknown, and the mismatch between the model and the degradation will result in
a severe performance drop. In this paper, we propose an all-in-one image
restoration network that tackles multiple degradations. Due to the
heterogeneous nature of different types of degradations, it is difficult to
process multiple degradations in a single network. To this end, we propose to
learn a neural degradation representation (NDR) that captures the underlying
characteristics of various degradations. The learned NDR decomposes different
types of degradations adaptively, similar to a neural dictionary that
represents basic degradation components. Subsequently, we develop a degradation
query module and a degradation injection module to effectively recognize and
utilize the specific degradation based on NDR, enabling the all-in-one
restoration ability for multiple degradations. Moreover, we propose a
bidirectional optimization strategy to effectively drive NDR to learn the
degradation representation by optimizing the degradation and restoration
processes alternately. Comprehensive experiments on representative types of
degradations (including noise, haze, rain, and downsampling) demonstrate the
effectiveness and generalization capability of our method.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12851" title="Abstract">arXiv:2310.12851</a> [<a href="/pdf/2310.12851" title="Download PDF">pdf</a>, <a href="/format/2310.12851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EmoDiarize: Speaker Diarization and Emotion Identification from Speech  Signals using Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamza%2C+H">Hanan Hamza</a>, 
<a href="/search/cs?searchtype=author&query=Gafoor%2C+F">Fiza Gafoor</a>, 
<a href="/search/cs?searchtype=author&query=Sithara%2C+F">Fathima Sithara</a>, 
<a href="/search/cs?searchtype=author&query=Anil%2C+G">Gayathri Anil</a>, 
<a href="/search/cs?searchtype=author&query=Anoop%2C+V+S">V. S. Anoop</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In the era of advanced artificial intelligence and human-computer
interaction, identifying emotions in spoken language is paramount. This
research explores the integration of deep learning techniques in speech emotion
recognition, offering a comprehensive solution to the challenges associated
with speaker diarization and emotion identification. It introduces a framework
that combines a pre-existing speaker diarization pipeline and an emotion
identification model built on a Convolutional Neural Network (CNN) to achieve
higher precision. The proposed model was trained on data from five speech
emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out
of which the latter is a speech emotion dataset created specifically for this
research. The features extracted from each sample include Mel Frequency
Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS),
and various data augmentation algorithms like pitch, noise, stretch, and shift.
This feature extraction approach aims to enhance prediction accuracy while
reducing computational complexity. The proposed model yields an unweighted
accuracy of 63%, demonstrating remarkable efficiency in accurately identifying
emotional states within speech signals.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12858" title="Abstract">arXiv:2310.12858</a> [<a href="/pdf/2310.12858" title="Download PDF">pdf</a>, <a href="/format/2310.12858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio Editing with Non-Rigid Text Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paissan%2C+F">Francesco Paissan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhepei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
<a href="/search/cs?searchtype=author&query=Smaragdis%2C+P">Paris Smaragdis</a>, 
<a href="/search/cs?searchtype=author&query=Subakan%2C+C">Cem Subakan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we explore audio-editing with non-rigid text edits. We show
that the proposed editing pipeline is able to create audio edits that remain
faithful to the input audio. We explore text prompts that perform addition,
style transfer, and in-painting. We quantitatively and qualitatively show that
the edits are able to obtain results which outperform Audio-LDM, a recently
released text-prompted audio generation model. Qualitative inspection of the
results points out that the edits given by our approach remain more faithful to
the input audio in terms of keeping the original onsets and offsets of the
audio events.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12860" title="Abstract">arXiv:2310.12860</a> [<a href="/pdf/2310.12860" title="Download PDF">pdf</a>, <a href="/format/2310.12860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing LLMs for hate speech detection: strengths and vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Sarthak Roy</a>, 
<a href="/search/cs?searchtype=author&query=Harshavardhan%2C+A">Ashish Harshavardhan</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Punyajoy Saha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures, 7 tables, accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Recently efforts have been made by social media platforms as well as
researchers to detect hateful or toxic language using large language models.
However, none of these works aim to use explanation, additional context and
victim community information in the detection process. We utilise different
prompt variation, input information and evaluate large language models in zero
shot setting (without adding any in-context examples). We select three large
language models (GPT-3.5, text-davinci and Flan-T5) and three datasets -
HateXplain, implicit hate and ToxicSpans. We find that on average including the
target information in the pipeline improves the model performance substantially
(~20-30%) over the baseline across the datasets. There is also a considerable
effect of adding the rationales/explanations into the pipeline (~10-20%) over
the baseline across the datasets. In addition, we further provide a typology of
the error cases where these large language models fail to (i) classify and (ii)
explain the reason for the decisions they take. Such vulnerable points
automatically constitute 'jailbreak' prompts for these models and industry
scale safeguard techniques need to be developed to make the models robust
against such prompts.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12862" title="Abstract">arXiv:2310.12862</a> [<a href="/pdf/2310.12862" title="Download PDF">pdf</a>, <a href="/format/2310.12862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuning Generative Models as an Inference Method for Robotic Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krupnik%2C+O">Orr Krupnik</a>, 
<a href="/search/cs?searchtype=author&query=Shafer%2C+E">Elisei Shafer</a>, 
<a href="/search/cs?searchtype=author&query=Jurgenson%2C+T">Tom Jurgenson</a>, 
<a href="/search/cs?searchtype=author&query=Tamar%2C+A">Aviv Tamar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7th Conference on Robot Learning, 2023. Project website at <a href="https://www.orrkrup.com/mace">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Adaptable models could greatly benefit robotic agents operating in the real
world, allowing them to deal with novel and varying conditions. While
approaches such as Bayesian inference are well-studied frameworks for adapting
models to evidence, we build on recent advances in deep generative models which
have greatly affected many areas of robotics. Harnessing modern GPU
acceleration, we investigate how to quickly adapt the sample generation of
neural network models to observations in robotic tasks. We propose a simple and
general method that is applicable to various deep generative models and robotic
environments. The key idea is to quickly fine-tune the model by fitting it to
generated samples matching the observed evidence, using the cross-entropy
method. We show that our method can be applied to both autoregressive models
and variational autoencoders, and demonstrate its usability in object shape
inference from grasping, inverse kinematics calculation, and point cloud
completion.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12864" title="Abstract">arXiv:2310.12864</a> [<a href="/pdf/2310.12864" title="Download PDF">pdf</a>, <a href="/format/2310.12864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Locality and Symmetry of Positional Encodings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lihu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Varoquaux%2C+G">Ga&#xeb;l Varoquaux</a>, 
<a href="/search/cs?searchtype=author&query=Suchanek%2C+F+M">Fabian M. Suchanek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long Paper in Findings of EMNLP23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Positional Encodings (PEs) are used to inject word-order information into
transformer-based language models. While they can significantly enhance the
quality of sentence representations, their specific contribution to language
models is not fully understood, especially given recent findings that various
positional encodings are insensitive to word order. In this work, we conduct a
systematic study of positional encodings in \textbf{Bidirectional Masked
Language Models} (BERT-style) , which complements existing work in three
aspects: (1) We uncover the core function of PEs by identifying two common
properties, Locality and Symmetry; (2) We show that the two properties are
closely correlated with the performances of downstream tasks; (3) We quantify
the weakness of current PEs by introducing two new probing tasks, on which
current PEs perform poorly. We believe that these results are the basis for
developing better PEs for transformer-based language models. The code is
available at \faGithub~ \url{https://github.com/tigerchen52/locality\_symmetry}
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12868" title="Abstract">arXiv:2310.12868</a> [<a href="/pdf/2310.12868" title="Download PDF">pdf</a>, <a href="/format/2310.12868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lanhong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+D">Debesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Keles%2C+E">Elif Keles</a>, 
<a href="/search/cs?searchtype=author&query=Medetalibeyoglu%2C+A">Alpay Medetalibeyoglu</a>, 
<a href="/search/cs?searchtype=author&query=Bagci%2C+U">Ulas Bagci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large-scale, big-variant, and high-quality data are crucial for developing
robust and successful deep-learning models for medical applications since they
potentially enable better generalization performance and avoid overfitting.
However, the scarcity of high-quality labeled data always presents significant
challenges. This paper proposes a novel approach to address this challenge by
developing controllable diffusion models for medical image synthesis, called
EMIT-Diff. We leverage recent diffusion probabilistic models to generate
realistic and diverse synthetic medical image data that preserve the essential
characteristics of the original medical images by incorporating edge
information of objects to guide the synthesis process. In our approach, we
ensure that the synthesized samples adhere to medically relevant constraints
and preserve the underlying structure of imaging data. Due to the random
sampling process by the diffusion model, we can generate an arbitrary number of
synthetic images with diverse appearances. To validate the effectiveness of our
proposed method, we conduct an extensive set of medical image segmentation
experiments on multiple datasets, including Ultrasound breast (+13.87%), CT
spleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements
over the baseline segmentation methods. For the first time, to our best
knowledge, the promising results demonstrate the effectiveness of our EMIT-Diff
for medical image segmentation tasks and show the feasibility of introducing a
first-ever text-guided diffusion model for general medical image segmentation
tasks. With carefully designed ablation experiments, we investigate the
influence of various data augmentation ratios, hyper-parameter settings, patch
size for generating random merging mask settings, and combined influence with
different network architectures.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12869" title="Abstract">arXiv:2310.12869</a> [<a href="/pdf/2310.12869" title="Download PDF">pdf</a>, <a href="/format/2310.12869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification of Bandgaps in Acoustic Metamaterials with  Stochastic Geometric Defects and Material Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mahabadi%2C+R+K">Rayehe Karimi Mahabadi</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+C">Cynthia Rudin</a>, 
<a href="/search/cs?searchtype=author&query=Guilleminot%2C+J">Johann Guilleminot</a>, 
<a href="/search/cs?searchtype=author&query=Brinson%2C+L+C">L. Catherine Brinson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Applied Physics (physics.app-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">This paper studies the utility of techniques within uncertainty
quantification, namely spectral projection and polynomial chaos expansion, in
reducing sampling needs for characterizing acoustic metamaterial dispersion
band responses given stochastic material properties and geometric defects. A
novel method of encoding geometric defects in an interpretable, resolution
independent is showcased in the formation of input space probability
distributions. Orders of magnitude sampling reductions down to $\sim10^0$ and
$\sim10^1$ are achieved in the 1D and 7D input space scenarios respectively
while maintaining accurate output space probability distributions through
combining Monte Carlo, quadrature rule, and sparse grid sampling with surrogate
model fitting.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12870" title="Abstract">arXiv:2310.12870</a> [<a href="/pdf/2310.12870" title="Download PDF">pdf</a>, <a href="/ps/2310.12870" title="Download PostScript">ps</a>, <a href="/format/2310.12870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure preservation in high-order hybrid discretisations of  advection-diffusion equations: linear and nonlinear approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lemaire%2C+S">Simon Lemaire</a>, 
<a href="/search/math?searchtype=author&query=Moatti%2C+J">Julien Moatti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We are interested in the high-order approximation of anisotropic
advection-diffusion problems on general polytopal partitions. We study two
hybrid schemes, both built upon the Hybrid High-Order technology. The first one
hinges on exponential fitting and is linear, whereas the second is nonlinear.
The existence of solutions is established for both schemes. Both schemes are
also shown to enjoy a discrete entropy structure, ensuring that the discrete
long-time behaviour of solutions mimics the PDE one. The nonlinear scheme is
designed so as to enforce the positivity of discrete solutions. On the
contrary, we display numerical evidence indicating that the linear scheme
violates positivity, whatever the order. Finally, we verify numerically that
the nonlinear scheme has optimal order of convergence, expected long-time
behaviour, and that raising the polynomial degree results, also in the
nonlinear case, in an efficiency gain.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12874" title="Abstract">arXiv:2310.12874</a> [<a href="/pdf/2310.12874" title="Download PDF">pdf</a>, <a href="/format/2310.12874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StoryAnalogy: Deriving Story-level Analogies from Large Language Models  to Unlock Analogical Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiayang%2C+C">Cheng Jiayang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lin Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+H">Tsz Ho Chan</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tianqing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+C">Chunkit Chan</a>, 
<a href="/search/cs?searchtype=author&query=Ru%2C+D">Dongyu Ru</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qipeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Analogy-making between narratives is one of the most critical abilities in
natural language understanding. In this paper, we evaluate the ability to
identify and generate analogy by building a first-of-its-kind large-scale
story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from
diverse domains with human annotations on two similarities from the extended
Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting
the first evaluation of story-level analogy identification and generation.
Interestingly, we find that the analogy identification tasks are extremely
challenging not only for the sentence embedding models but also for the recent
large language models (LLMs) such as ChatGPT and LLaMa, where ChatGPT only
achieved around 30% accuracy in multiple-choice questions (&gt; 85% accuracy for
humans). Finally, we find that data in StoryAnalogy can improve LLMs analogy
generation quality, where a fine-tuned FlanT5-xxl model yields comparable
performance to zero-shot ChatGPT.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12880" title="Abstract">arXiv:2310.12880</a> [<a href="/pdf/2310.12880" title="Download PDF">pdf</a>, <a href="/format/2310.12880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Kinaci%2C+O+K">Omer Kemal Kinaci</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+T+Q">Trung Q. Duong</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on WS01 IEEE ICC 2023 Workshop on The Evolution of Digital Twin Paradigm in Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The idea of next-generation ports has become more apparent in the last ten
years in response to the challenge posed by the rising demand for efficiency
and the ever-increasing volume of goods. In this new era of intelligent
infrastructure and facilities, it is evident that cyber-security has recently
received the most significant attention from the seaport and maritime
authorities, and it is a primary concern on the agenda of most ports.
Traditional security solutions can be applied to safeguard IoT and
Cyber-Physical Systems (CPS) from harmful entities. Nevertheless, security
researchers can only watch, examine, and learn about the behaviors of attackers
if these solutions operate more transparently. Herein, honeypots are potential
solutions since they offer valuable information about the attackers. It can be
virtual or physical. Virtual honeypots must be more realistic to entice
attackers, necessitating better high-fidelity. To this end, Digital Twin (DT)
technology can be employed to increase the complexity and simulation fidelity
of the honeypots. Seaports can be attacked from both their existing devices and
external devices at the same time. Existing mechanisms are insufficient to
detect external attacks; therefore, the current systems cannot handle attacks
at the desired level. DT and honeypot technologies can be used together to
tackle them. Consequently, we suggest a DT-assisted honeypot, called TwinPot,
for external attacks in smart seaports. Moreover, we propose an intelligent
attack detection mechanism to handle different attack types using DT for
internal attacks. Finally, we build an extensive smart seaport dataset for
internal and external attacks using the MANSIM tool and two existing datasets
to test the performance of our system. We show that under simultaneous internal
and external attacks on the system, our solution successfully detects internal
and external attacks.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12884" title="Abstract">arXiv:2310.12884</a> [<a href="/pdf/2310.12884" title="Download PDF">pdf</a>, <a href="/format/2310.12884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connected Components and Disjunctive Existential Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alfonso%2C+E+M">Enrique Matos Alfonso</a>, 
<a href="/search/cs?searchtype=author&query=Stamou%2C+G">Giorgos Stamou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Databases (cs.DB)

</div>
<p class="mathjax">In this paper, we explore conjunctive query rewriting, focusing on queries
containing universally quantified negation within the framework of disjunctive
existential rules. We address the undecidability of the existence of a finite
and complete UCQ-rewriting and the identification of finite unification sets
(fus) of rules. We introduce new rule classes, connected linear rules and
connected domain restricted rules, that exhibit the fus property for
existential rules. Additionally, we propose disconnected disjunction for
disjunctive existential rules to achieve the fus property when we extend the
introduced rule fragments to disjunctive existential rules. We present
ECOMPLETO, a system for efficient query rewriting with disjunctive existential
rules, capable of handling UCQs with universally quantified negation. Our
experiments demonstrate ECOMPLETO's consistent ability to produce finite
UCQ-rewritings and describe the performance on different ontologies and
queries.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12887" title="Abstract">arXiv:2310.12887</a> [<a href="/pdf/2310.12887" title="Download PDF">pdf</a>, <a href="/format/2310.12887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial and Temporal Attention-based emotion estimation on HRI-AVC  dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+K">Karthik Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Saurav Singh</a>, 
<a href="/search/cs?searchtype=author&query=Namba%2C+J">Justin Namba</a>, 
<a href="/search/cs?searchtype=author&query=Heard%2C+J">Jamison Heard</a>, 
<a href="/search/cs?searchtype=author&query=Kanan%2C+C">Christopher Kanan</a>, 
<a href="/search/cs?searchtype=author&query=Sahin%2C+F">Ferat Sahin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Many attempts have been made at estimating discrete emotions (calmness,
anxiety, boredom, surprise, anger) and continuous emotional measures commonly
used in psychology, namely `valence' (The pleasantness of the emotion being
displayed) and `arousal' (The intensity of the emotion being displayed).
Existing methods to estimate arousal and valence rely on learning from data
sets, where an expert annotator labels every image frame. Access to an expert
annotator is not always possible, and the annotation can also be tedious. Hence
it is more practical to obtain self-reported arousal and valence values
directly from the human in a real-time Human-Robot collaborative setting. Hence
this paper provides an emotion data set (HRI-AVC) obtained while conducting a
human-robot interaction (HRI) task. The self-reported pair of labels in this
data set is associated with a set of image frames. This paper also proposes a
spatial and temporal attention-based network to estimate arousal and valence
from this set of image frames. The results show that an attention-based network
can estimate valence and arousal on the HRI-AVC data set even when Arousal and
Valence values are unavailable per frame.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12888" title="Abstract">arXiv:2310.12888</a> [<a href="/pdf/2310.12888" title="Download PDF">pdf</a>, <a href="/ps/2310.12888" title="Download PostScript">ps</a>, <a href="/format/2310.12888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized GM-MDS: Polynomial Codes are Higher Order MDS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brakensiek%2C+J">Joshua Brakensiek</a>, 
<a href="/search/cs?searchtype=author&query=Dhar%2C+M">Manik Dhar</a>, 
<a href="/search/cs?searchtype=author&query=Gopi%2C+S">Sivakanth Gopi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Algebraic Geometry (math.AG); Combinatorics (math.CO)

</div>
<p class="mathjax">The GM-MDS theorem, conjectured by Dau-Song-Dong-Yuen and proved by Lovett
and Yildiz-Hassibi, shows that the generator matrices of Reed-Solomon codes can
attain every possible configuration of zeros for an MDS code. The recently
emerging theory of higher order MDS codes has connected the GM-MDS theorem to
other important properties of Reed-Solomon codes, including showing that
Reed-Solomon codes can achieve list decoding capacity, even over fields of size
linear in the message length.
<br />A few works have extended the GM-MDS theorem to other families of codes,
including Gabidulin and skew polynomial codes. In this paper, we generalize all
these previous results by showing that the GM-MDS theorem applies to any
\emph{polynomial code}, i.e., a code where the columns of the generator matrix
are obtained by evaluating linearly independent polynomials at different
points. We also show that the GM-MDS theorem applies to dual codes of such
polynomial codes, which is non-trivial since the dual of a polynomial code may
not be a polynomial code. More generally, we show that GM-MDS theorem also
holds for algebraic codes (and their duals) where columns of the generator
matrix are chosen to be points on some irreducible variety which is not
contained in a hyperplane through the origin. Our generalization has
applications to constructing capacity-achieving list-decodable codes as shown
in a follow-up work by Brakensiek-Dhar-Gopi-Zhang, where it is proved that
randomly punctured algebraic-geometric (AG) codes achieve list-decoding
capacity over constant-sized fields.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12889" title="Abstract">arXiv:2310.12889</a> [<a href="/pdf/2310.12889" title="Download PDF">pdf</a>, <a href="/format/2310.12889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Enumerative Perspective on Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akmal%2C+S">Shyan Akmal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Connectivity (or equivalently, unweighted maximum flow) is an important
measure in graph theory and combinatorial optimization. Given a graph $G$ with
vertices $s$ and $t$, the connectivity $\lambda(s,t)$ from $s$ to $t$ is
defined to be the maximum number of edge-disjoint paths from $s$ to $t$ in $G$.
<br />Much research has gone into designing fast algorithms for computing
connectivities in graphs. Previous work showed that it is possible to compute
connectivities for all pairs of vertices in directed graphs with $m$ edges in
$\tilde{O}(m^\omega)$ time [Chueng, Lau, and Leung, FOCS 2011], where $\omega
\in [2,2.3716)$ is the exponent of matrix multiplication. For the related
problem of computing "small connectivities," it was recently shown that for any
positive integer $k$, we can compute $\min(k,\lambda(s,t))$ for all pairs of
vertices $(s,t)$ in a directed graph with $n$ nodes in $\tilde{O}((kn)^\omega)$
time [Akmal and Jin, ICALP 2023].
<br />In this paper, we present an alternate exposition of these
$\tilde{O}(m^\omega)$ and $\tilde{O}((kn)^\omega)$ time algorithms, with
simpler proofs of correctness. Earlier proofs were somewhat indirect,
introducing an elegant but ad hoc "flow vector framework" for showing
correctness of these algorithms. In contrast, we observe that these algorithms
for computing exact and small connectivity values can be interpreted as testing
whether certain generating functions enumerating families of edge-disjoint
paths are nonzero. This new perspective yields more transparent proofs, and
ties the approach for these problems more closely to the literature surrounding
algebraic graph algorithms.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12892" title="Abstract">arXiv:2310.12892</a> [<a href="/pdf/2310.12892" title="Download PDF">pdf</a>, <a href="/format/2310.12892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Study of Performance Disparities in Multilingual  Task-Oriented Dialogue Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songbo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Han Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Moy Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Gritta%2C+M">Milan Gritta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guchun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Iacobacci%2C+I">Ignacio Iacobacci</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+A">Anna Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Vuli%C4%87%2C+I">Ivan Vuli&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Achieving robust language technologies that can perform well across the
world's many languages is a central goal of multilingual NLP. In this work, we
take stock of and empirically analyse task performance disparities that exist
between multilingual task-oriented dialogue (ToD) systems. We first define new
quantitative measures of absolute and relative equivalence in system
performance, capturing disparities across languages and within individual
languages. Through a series of controlled experiments, we demonstrate that
performance disparities depend on a number of factors: the nature of the ToD
task at hand, the underlying pretrained language model, the target language,
and the amount of ToD annotated data. We empirically prove the existence of the
adaptation and intrinsic biases in current ToD systems: e.g., ToD systems
trained for Arabic or Turkish using annotated ToD data fully parallel to
English ToD data still exhibit diminished ToD task performance. Beyond
providing a series of insights into the performance disparities of ToD systems
in different languages, our analyses offer practical tips on how to approach
ToD data collection and system development for new languages.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12898" title="Abstract">arXiv:2310.12898</a> [<a href="/pdf/2310.12898" title="Download PDF">pdf</a>, <a href="/ps/2310.12898" title="Download PostScript">ps</a>, <a href="/format/2310.12898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AG codes achieve list decoding capacity over contant-sized fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brakensiek%2C+J">Joshua Brakensiek</a>, 
<a href="/search/cs?searchtype=author&query=Dhar%2C+M">Manik Dhar</a>, 
<a href="/search/cs?searchtype=author&query=Gopi%2C+S">Sivakanth Gopi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+i">ihan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Algebraic Geometry (math.AG)

</div>
<p class="mathjax">The recently-emerging field of higher order MDS codes has sought to unify a
number of concepts in coding theory. Such areas captured by higher order MDS
codes include maximally recoverable (MR) tensor codes, codes with optimal
list-decoding guarantees, and codes with constrained generator matrices (as in
the GM-MDS theorem).
<br />By proving these equivalences, Brakensiek-Gopi-Makam showed the existence of
optimally list-decodable Reed-Solomon codes over exponential sized fields.
Building on this, recent breakthroughs by Guo-Zhang and Alrabiah-Guruswami-Li
have shown that randomly punctured Reed-Solomon codes achieve list-decoding
capacity (which is a relaxation of optimal list-decodability) over linear size
fields. We extend these works by developing a formal theory of relaxed higher
order MDS codes. In particular, we show that there are two inequivalent
relaxations which we call lower and upper relaxations. The lower relaxation is
equivalent to relaxed optimal list-decodable codes and the upper relaxation is
equivalent to relaxed MR tensor codes with a single parity check per column.
<br />We then generalize the techniques of GZ and AGL to show that both these
relaxations can be constructed over constant size fields by randomly puncturing
suitable algebraic-geometric codes. For this, we crucially use the generalized
GM-MDS theorem for polynomial codes recently proved by Brakensiek-Dhar-Gopi. We
obtain the following corollaries from our main result. First, randomly
punctured AG codes of rate $R$ achieve list-decoding capacity with list size
$O(1/\epsilon)$ and field size $\exp(O(1/\epsilon^2))$. Prior to this work, AG
codes were not even known to achieve list-decoding capacity. Second, by
randomly puncturing AG codes, we can construct relaxed MR tensor codes with a
single parity check per column over constant-sized fields, whereas
(non-relaxed) MR tensor codes require exponential field size.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12899" title="Abstract">arXiv:2310.12899</a> [<a href="/pdf/2310.12899" title="Download PDF">pdf</a>, <a href="/ps/2310.12899" title="Download PostScript">ps</a>, <a href="/format/2310.12899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age-Appropriate Robot Design: In-The-Wild Child-Robot Interaction  Studies of Perseverance Styles and Robot&#x27;s Unexpected Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wr%C3%B3bel%2C+A">Alicja Wr&#xf3;bel</a>, 
<a href="/search/cs?searchtype=author&query=%C5%B9r%C3%B3bek%2C+K">Karolina &#x179;r&#xf3;bek</a>, 
<a href="/search/cs?searchtype=author&query=Schaper%2C+M">Marie-Monique Schaper</a>, 
<a href="/search/cs?searchtype=author&query=Zguda%2C+P">Paulina Zguda</a>, 
<a href="/search/cs?searchtype=author&query=Indurkhya%2C+B">Bipin Indurkhya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of Ro-MAN 2023, Busan, South Korea, Aug 28-31, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">As child-robot interactions become more and more common in daily life
environment, it is important to examine how robot's errors influence children's
behavior. We explored how a robot's unexpected behaviors affect child-robot
interactions during two workshops on active reading: one in a modern art museum
and one in a school. We observed the behavior and attitudes of 42 children from
three age groups: 6-7 years, 8-10 years, and 10-12 years. Through our
observations, we identified six different types of surprising robot behaviors:
personality, movement malfunctions, inconsistent behavior, mispronunciation,
delays, and freezing. Using a qualitative analysis, we examined how children
responded to each type of behavior, and we observed similarities and
differences between the age groups. Based on our findings, we propose
guidelines for designing age-appropriate learning interactions with social
robots.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12900" title="Abstract">arXiv:2310.12900</a> [<a href="/pdf/2310.12900" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized human mobility prediction for HuMob challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+M">Masahiro Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Furuta%2C+S">Shomu Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Fukazawa%2C+Y">Yusuke Fukazawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We explain the methodology used to create the data submitted to HuMob
Challenge, a data analysis competition for human mobility prediction. We
adopted a personalized model to predict the individual's movement trajectory
from their data, instead of predicting from the overall movement, based on the
hypothesis that human movement is unique to each person. We devised the
features such as the date and time, activity time, days of the week, time of
day, and frequency of visits to POI (Point of Interest). As additional
features, we incorporated the movement of other individuals with similar
behavior patterns through the employment of clustering. The machine learning
model we adopted was the Support Vector Regression (SVR). We performed accuracy
through offline assessment and carried out feature selection and parameter
tuning. Although overall dataset provided consists of 100,000 users trajectory,
our method use only 20,000 target users data, and do not need to use other
80,000 data. Despite the personalized model's traditional feature engineering
approach, this model yields reasonably good accuracy with lower computational
cost.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12902" title="Abstract">arXiv:2310.12902</a> [<a href="/pdf/2310.12902" title="Download PDF">pdf</a>, <a href="/format/2310.12902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Narratives: A Comparison of Human Crowdsourced Storytelling  and AI Storytelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Begus%2C+N">Nina Begus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The paper proposes a framework that combines behavioral and computational
experiments employing fictional prompts as a novel tool for investigating
cultural artifacts and social biases in storytelling both by humans and
generative AI. The study analyzes 250 stories authored by crowdworkers in June
2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging
methods from narratology and inferential statistics. Both crowdworkers and
large language models responded to identical prompts about creating and falling
in love with an artificial human. The proposed experimental paradigm allows a
direct comparison between human and LLM-generated storytelling. Responses to
the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth
in the collective imaginary of both humans and large language models. All
solicited narratives present a scientific or technological pursuit. The
analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more
more progressive in terms of gender roles and sexuality than those written by
humans. While AI narratives can occasionally provide innovative plot twists,
they offer less imaginative scenarios and rhetoric than human-authored texts.
The proposed framework argues that fiction can be used as a window into human
and AI-based collective imaginary and social dimensions.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12904" title="Abstract">arXiv:2310.12904</a> [<a href="/pdf/2310.12904" title="Download PDF">pdf</a>, <a href="/format/2310.12904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Object Localization in the Era of Self-Supervised ViTs: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sim%C3%A9oni%2C+O">Oriane Sim&#xe9;oni</a>, 
<a href="/search/cs?searchtype=author&query=Zablocki%2C+%C3%89">&#xc9;loi Zablocki</a>, 
<a href="/search/cs?searchtype=author&query=Gidaris%2C+S">Spyros Gidaris</a>, 
<a href="/search/cs?searchtype=author&query=Puy%2C+G">Gilles Puy</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+P">Patrick P&#xe9;rez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recent enthusiasm for open-world vision systems show the high interest of
the community to perform perception tasks outside of the closed-vocabulary
benchmark setups which have been so popular until now. Being able to discover
objects in images/videos without knowing in advance what objects populate the
dataset is an exciting prospect. But how to find objects without knowing
anything about them? Recent works show that it is possible to perform
class-agnostic unsupervised object localization by exploiting self-supervised
pre-trained features. We propose here a survey of unsupervised object
localization methods that discover objects in images without requiring any
manual annotation in the era of self-supervised ViTs. We gather links of
discussed methods in the repository
https://github.com/valeoai/Awesome-Unsupervised-Object-Localization.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12909" title="Abstract">arXiv:2310.12909</a> [<a href="/pdf/2310.12909" title="Download PDF">pdf</a>, <a href="/format/2310.12909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Adaptation: Learning to Recover from Unforeseen  Malfunctions in Multi-Robot Teams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Findik%2C+Y">Yasin Findik</a>, 
<a href="/search/cs?searchtype=author&query=Robinette%2C+P">Paul Robinette</a>, 
<a href="/search/cs?searchtype=author&query=Jerath%2C+K">Kshitij Jerath</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadzadeh%2C+S+R">S. Reza Ahmadzadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at Multi-Agent Dynamic Games (MADGames) workshop at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Cooperative multi-agent reinforcement learning (MARL) approaches tackle the
challenge of finding effective multi-agent cooperation strategies for
accomplishing individual or shared objectives in multi-agent teams. In
real-world scenarios, however, agents may encounter unforeseen failures due to
constraints like battery depletion or mechanical issues. Existing
state-of-the-art methods in MARL often recover slowly -- if at all -- from such
malfunctions once agents have already converged on a cooperation strategy. To
address this gap, we present the Collaborative Adaptation (CA) framework. CA
introduces a mechanism that guides collaboration and accelerates adaptation
from unforeseen failures by leveraging inter-agent relationships. Our findings
demonstrate that CA enables agents to act on the knowledge of inter-agent
relations, recovering from unforeseen agent failures and selecting appropriate
cooperative strategies.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12910" title="Abstract">arXiv:2310.12910</a> [<a href="/pdf/2310.12910" title="Download PDF">pdf</a>, <a href="/format/2310.12910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influence of Team Interactions on Multi-Robot Cooperation: A Relational  Network Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Findik%2C+Y">Yasin Findik</a>, 
<a href="/search/cs?searchtype=author&query=Osooli%2C+H">Hamid Osooli</a>, 
<a href="/search/cs?searchtype=author&query=Robinette%2C+P">Paul Robinette</a>, 
<a href="/search/cs?searchtype=author&query=Jerath%2C+K">Kshitij Jerath</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadzadeh%2C+S+R">S. Reza Ahmadzadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Multi-Robot and Multi-Agent Systems (IEEE MRS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Relational networks within a team play a critical role in the performance of
many real-world multi-robot systems. To successfully accomplish tasks that
require cooperation and coordination, different agents (e.g., robots)
necessitate different priorities based on their positioning within the team.
Yet, many of the existing multi-robot cooperation algorithms regard agents as
interchangeable and lack a mechanism to guide the type of cooperation strategy
the agents should exhibit. To account for the team structure in cooperative
tasks, we propose a novel algorithm that uses a relational network comprising
inter-agent relationships to prioritize certain agents over others. Through
appropriate design of the team's relational network, we can guide the
cooperation strategy, resulting in the emergence of new behaviors that
accomplish the specified task. We conducted six experiments in a multi-robot
setting with a cooperative task. Our results demonstrate that the proposed
method can effectively influence the type of solution that the algorithm
converges to by specifying the relationships between the agents, making it a
promising approach for tasks that require cooperation among agents with a
specified team structure.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12911" title="Abstract">arXiv:2310.12911</a> [<a href="/pdf/2310.12911" title="Download PDF">pdf</a>, <a href="/format/2310.12911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight approximability of MAX 2-SAT and relatives, under UGC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brakensiek%2C+J">Joshua Brakensiek</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+N">Neng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zwick%2C+U">Uri Zwick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 1 figure; to appear in SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Austrin showed that the approximation ratio $\beta\approx 0.94016567$
obtained by the MAX 2-SAT approximation algorithm of Lewin, Livnat and Zwick
(LLZ) is optimal modulo the Unique Games Conjecture (UGC) and modulo a
Simplicity Conjecture that states that the worst performance of the algorithm
is obtained on so called simple configurations. We prove Austrin's conjecture,
thereby showing the optimality of the LLZ approximation algorithm, relying only
on the Unique Games Conjecture. Our proof uses a combination of analytic and
computational tools.
<br />We also present new approximation algorithms for two restrictions of the MAX
2-SAT problem. For MAX HORN-$\{1,2\}$-SAT, i.e., MAX CSP$(\{x\lor y,\bar{x}\lor
y,x,\bar{x}\})$, in which clauses are not allowed to contain two negated
literals, we obtain an approximation ratio of $0.94615981$. For MAX
CSP$(\{x\lor y,x,\bar{x}\})$, i.e., when 2-clauses are not allowed to contain
negated literals, we obtain an approximation ratio of $0.95397990$. By adapting
Austrin's and our arguments for the MAX 2-SAT problem we show that these two
approximation ratios are also tight, modulo only the UGC conjecture. This
completes a full characterization of the approximability of the MAX 2-SAT
problem and its restrictions.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12912" title="Abstract">arXiv:2310.12912</a> [<a href="/pdf/2310.12912" title="Download PDF">pdf</a>, <a href="/format/2310.12912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Relational Networks in Multi-Agent Learning: A Value-Based  Factorization View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Findik%2C+Y">Yasin Findik</a>, 
<a href="/search/cs?searchtype=author&query=Robinette%2C+P">Paul Robinette</a>, 
<a href="/search/cs?searchtype=author&query=Jerath%2C+K">Kshitij Jerath</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadzadeh%2C+S+R">S. Reza Ahmadzadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to International Conference on Decision and Control (IEEE CDC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Effective coordination and cooperation among agents are crucial for
accomplishing individual or shared objectives in multi-agent systems. In many
real-world multi-agent systems, agents possess varying abilities and
constraints, making it necessary to prioritize agents based on their specific
properties to ensure successful coordination and cooperation within the team.
However, most existing cooperative multi-agent algorithms do not take into
account these individual differences, and lack an effective mechanism to guide
coordination strategies. We propose a novel multi-agent learning approach that
incorporates relationship awareness into value-based factorization methods.
Given a relational network, our approach utilizes inter-agents relationships to
discover new team behaviors by prioritizing certain agents over other,
accounting for differences between them in cooperative tasks. We evaluated the
effectiveness of our proposed approach by conducting fifteen experiments in two
different environments. The results demonstrate that our proposed algorithm can
influence and shape team behavior, guide cooperation strategies, and expedite
agent learning. Therefore, our approach shows promise for use in multi-agent
systems, especially when agents have diverse properties.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12913" title="Abstract">arXiv:2310.12913</a> [<a href="/pdf/2310.12913" title="Download PDF">pdf</a>, <a href="/ps/2310.12913" title="Download PostScript">ps</a>, <a href="/format/2310.12913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic 3SUM-Hardness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fischer%2C+N">Nick Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Kaliciak%2C+P">Piotr Kaliciak</a>, 
<a href="/search/cs?searchtype=author&query=Polak%2C+A">Adam Polak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">As one of the three main pillars of fine-grained complexity theory, the 3SUM
problem explains the hardness of many diverse polynomial-time problems via
fine-grained reductions. Many of these reductions are either directly based on
or heavily inspired by P\u{a}tra\c{s}cu's framework involving additive hashing
and are thus randomized. Some selected reductions were derandomized in previous
work [Chan, He; SOSA'20], but the current techniques are limited and a major
fraction of the reductions remains randomized.
<br />In this work we gather a toolkit aimed to derandomize reductions based on
additive hashing. Using this toolkit, we manage to derandomize almost all known
3SUM-hardness reductions. As technical highlights we derandomize the hardness
reductions to (offline) Set Disjointness, (offline) Set Intersection and
Triangle Listing -- these questions were explicitly left open in previous work
[Kopelowitz, Pettie, Porat; SODA'16]. The few exceptions to our work fall into
a special category of recent reductions based on structure-versus-randomness
dichotomies.
<br />We expect that our toolkit can be readily applied to derandomize future
reductions as well. As a conceptual innovation, our work thereby promotes the
theory of deterministic 3SUM-hardness.
<br />As our second contribution, we prove that there is a deterministic universe
reduction for 3SUM. Specifically, using additive hashing it is a standard trick
to assume that the numbers in 3SUM have size at most $n^3$. We prove that this
assumption is similarly valid for deterministic algorithms.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12914" title="Abstract">arXiv:2310.12914</a> [<a href="/pdf/2310.12914" title="Download PDF">pdf</a>, <a href="/format/2310.12914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network-Aware AutoML Framework for Software-Defined Sensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Horsanali%2C+E">Emre Horsanali</a>, 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Secinti%2C+G">Gokhan Secinti</a>, 
<a href="/search/cs?searchtype=author&query=Karameseoglu%2C+A">Aytac Karameseoglu</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2021 17th International Conference on Distributed Computing in
  Sensor Systems (DCOSS), Pafos, Cyprus, 2021, pp. 451-457
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">As the current detection solutions of distributed denial of service attacks
(DDoS) need additional infrastructures to handle high aggregate data rates,
they are not suitable for sensor networks or the Internet of Things. Besides,
the security architecture of software-defined sensor networks needs to pay
attention to the vulnerabilities of both software-defined networks and sensor
networks. In this paper, we propose a network-aware automated machine learning
(AutoML) framework which detects DDoS attacks in software-defined sensor
networks. Our framework selects an ideal machine learning algorithm to detect
DDoS attacks in network-constrained environments, using metrics such as
variable traffic load, heterogeneous traffic rate, and detection time while
preventing over-fitting. Our contributions are two-fold: (i) we first
investigate the trade-off between the efficiency of ML algorithms and
network/traffic state in the scope of DDoS detection. (ii) we design and
implement a software architecture containing open-source network tools, with
the deployment of multiple ML algorithms. Lastly, we show that under the denial
of service attacks, our framework ensures the traffic packets are still
delivered within the network with additional delays.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12920" title="Abstract">arXiv:2310.12920</a> [<a href="/pdf/2310.12920" title="Download PDF">pdf</a>, <a href="/format/2310.12920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Marginalization Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sulin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ramadge%2C+P+J">Peter J. Ramadge</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+R+P">Ryan P. Adams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce marginalization models (MaMs), a new family of generative models
for high-dimensional discrete data. They offer scalable and flexible generative
modeling with tractable likelihoods by explicitly modeling all induced marginal
distributions. Marginalization models enable fast evaluation of arbitrary
marginal probabilities with a single forward pass of the neural network, which
overcomes a major limitation of methods with exact marginal inference, such as
autoregressive models (ARMs). We propose scalable methods for learning the
marginals, grounded in the concept of "marginalization self-consistency".
Unlike previous methods, MaMs support scalable training of any-order generative
models for high-dimensional problems under the setting of energy-based
training, where the goal is to match the learned distribution to a given
desired probability (specified by an unnormalized (log) probability function
such as energy function or reward function). We demonstrate the effectiveness
of the proposed model on a variety of discrete data distributions, including
binary images, language, physical systems, and molecules, for maximum
likelihood and energy-based training settings. MaMs achieve orders of magnitude
speedup in evaluating the marginal probabilities on both settings. For
energy-based training tasks, MaMs enable any-order generative modeling of
high-dimensional problems beyond the capability of previous methods. Code is at
https://github.com/PrincetonLIPS/MaM.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12921" title="Abstract">arXiv:2310.12921</a> [<a href="/pdf/2310.12921" title="Download PDF">pdf</a>, <a href="/format/2310.12921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Models are Zero-Shot Reward Models for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rocamonde%2C+J">Juan Rocamonde</a>, 
<a href="/search/cs?searchtype=author&query=Montesinos%2C+V">Victoriano Montesinos</a>, 
<a href="/search/cs?searchtype=author&query=Nava%2C+E">Elvis Nava</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>, 
<a href="/search/cs?searchtype=author&query=Lindner%2C+D">David Lindner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning (RL) requires either manually specifying a reward
function, which is often infeasible, or learning a reward model from a large
amount of human feedback, which is often very expensive. We study a more
sample-efficient alternative: using pretrained vision-language models (VLMs) as
zero-shot reward models (RMs) to specify tasks via natural language. We propose
a natural and general approach to using VLMs as reward models, which we call
VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn
complex tasks without a manually specified reward function, such as kneeling,
doing the splits, and sitting in a lotus position. For each of these tasks, we
only provide a single sentence text prompt describing the desired task with
minimal prompt engineering. We provide videos of the trained agents at:
https://sites.google.com/view/vlm-rm. We can improve performance by providing a
second ``baseline'' prompt and projecting out parts of the CLIP embedding space
irrelevant to distinguish between goal and baseline. Further, we find a strong
scaling effect for VLM-RMs: larger VLMs trained with more compute and data are
better reward models. The failure modes of VLM-RMs we encountered are all
related to known capability limitations of current VLMs, such as limited
spatial reasoning ability or visually unrealistic environments that are far
off-distribution for the VLM. We find that VLM-RMs are remarkably robust as
long as the VLM is large enough. This suggests that future VLMs will become
more and more useful reward models for a wide range of RL applications.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12924" title="Abstract">arXiv:2310.12924</a> [<a href="/pdf/2310.12924" title="Download PDF">pdf</a>, <a href="/format/2310.12924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous  Core Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Bal%2C+B">Bahadir Bal</a>, 
<a href="/search/cs?searchtype=author&query=Karameseoglu%2C+A">Aytac Karameseoglu</a>, 
<a href="/search/cs?searchtype=author&query=Duong%2C+T+Q">Trung Q. Duong</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Communications Standards Magazine, vol. 6, no. 3, pp.
  38-44, September 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Existing distributed denial of service attack (DDoS) solutions cannot handle
highly aggregated data rates; thus, they are unsuitable for Internet service
provider (ISP) core networks. This article proposes a digital twin-enabled
intelligent DDoS detection mechanism using an online learning method for
autonomous systems. Our contributions are three-fold: we first design a DDoS
detection architecture based on the digital twin for ISP core networks. We
implemented a Yet Another Next Generation (YANG) model and an automated feature
selection (AutoFS) module to handle core network data. We used an online
learning approach to update the model instantly and efficiently, improve the
learning model quickly, and ensure accurate predictions. Finally, we reveal
that our proposed solution successfully detects DDoS attacks and updates the
feature selection method and learning model with a true classification rate of
ninety-seven percent. Our proposed solution can estimate the attack within
approximately fifteen minutes after the DDoS attack starts.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12928" title="Abstract">arXiv:2310.12928</a> [<a href="/pdf/2310.12928" title="Download PDF">pdf</a>, <a href="/ps/2310.12928" title="Download PostScript">ps</a>, <a href="/format/2310.12928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolving social dilemmas with minimal reward transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Willis%2C+R">Richard Willis</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Leibo%2C+J+Z">Joel Z Leibo</a>, 
<a href="/search/cs?searchtype=author&query=Luck%2C+M">Michael Luck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 13 tables, submitted to the Journal of Autonomous Agents and Multi-Agent Systems: Special Issue on Citizen-Centric AI Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Multi-agent cooperation is an important topic, and is particularly
challenging in mixed-motive situations where it does not pay to be nice to
others. Consequently, self-interested agents often avoid collective behaviour,
resulting in suboptimal outcomes for the group. In response, in this paper we
introduce a metric to quantify the disparity between what is rational for
individual agents and what is rational for the group, which we call the general
self-interest level. This metric represents the maximum proportion of
individual rewards that all agents can retain while ensuring that achieving
social welfare optimum becomes a dominant strategy. By aligning the individual
and group incentives, rational agents acting to maximise their own reward will
simultaneously maximise the collective reward. As agents transfer their rewards
to motivate others to consider their welfare, we diverge from traditional
concepts of altruism or prosocial behaviours. The general self-interest level
is a property of a game that is useful for assessing the propensity of players
to cooperate and understanding how features of a game impact this. We
illustrate the effectiveness of our method on several novel games
representations of social dilemmas with arbitrary numbers of players.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12929" title="Abstract">arXiv:2310.12929</a> [<a href="/pdf/2310.12929" title="Download PDF">pdf</a>, <a href="/format/2310.12929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Modeling of Human Teams to Infer False Beliefs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soares%2C+P">Paulo Soares</a>, 
<a href="/search/cs?searchtype=author&query=Pyarelal%2C+A">Adarsh Pyarelal</a>, 
<a href="/search/cs?searchtype=author&query=Barnard%2C+K">Kobus Barnard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, presented in the 2021 AAAI Fall Symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We develop a probabilistic graphical model (PGM) for artificially intelligent
(AI) agents to infer human beliefs during a simulated urban search and rescue
(USAR) scenario executed in a Minecraft environment with a team of three
players. The PGM approach makes observable states and actions explicit, as well
as beliefs and intentions grounded by evidence about what players see and do
over time. This approach also supports inferring the effect of interventions,
which are vital if AI agents are to assist human teams. The experiment
incorporates manipulations of players' knowledge, and the virtual
Minecraft-based testbed provides access to several streams of information,
including the objects in the players' field of view. The participants are
equipped with a set of marker blocks that can be placed near room entrances to
signal the presence or absence of victims in the rooms to their teammates. In
each team, one of the members is given a different legend for the markers than
the other two, which may mislead them about the state of the rooms; that is,
they will hold a false belief. We extend previous works in this field by
introducing ToMCAT, an AI agent that can reason about individual and shared
mental states. We find that the players' behaviors are affected by what they
see in their in-game field of view, their beliefs about the meaning of the
markers, and their beliefs about which meaning the team decided to adopt. In
addition, we show that ToMCAT's beliefs are consistent with the players'
actions and that it can infer false beliefs with accuracy significantly better
than chance and comparable to inferences made by human observers.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12931" title="Abstract">arXiv:2310.12931</a> [<a href="/pdf/2310.12931" title="Download PDF">pdf</a>, <a href="/format/2310.12931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eureka: Human-Level Reward Design via Coding Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y+J">Yecheng Jason Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+W">William Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">De-An Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bastani%2C+O">Osbert Bastani</a>, 
<a href="/search/cs?searchtype=author&query=Jayaraman%2C+D">Dinesh Jayaraman</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Linxi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website and open-source code: <a href="https://eureka-research.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have excelled as high-level semantic planners
for sequential decision-making tasks. However, harnessing them to learn complex
low-level manipulation tasks, such as dexterous pen spinning, remains an open
problem. We bridge this fundamental gap and present Eureka, a human-level
reward design algorithm powered by LLMs. Eureka exploits the remarkable
zero-shot generation, code-writing, and in-context improvement capabilities of
state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over
reward code. The resulting rewards can then be used to acquire complex skills
via reinforcement learning. Without any task-specific prompting or pre-defined
reward templates, Eureka generates reward functions that outperform expert
human-engineered rewards. In a diverse suite of 29 open-source RL environments
that include 10 distinct robot morphologies, Eureka outperforms human experts
on 83% of the tasks, leading to an average normalized improvement of 52%. The
generality of Eureka also enables a new gradient-free in-context learning
approach to reinforcement learning from human feedback (RLHF), readily
incorporating human inputs to improve the quality and the safety of the
generated rewards without model updating. Finally, using Eureka rewards in a
curriculum learning setting, we demonstrate for the first time, a simulated
Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a
pen in circles at rapid speed.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12934" title="Abstract">arXiv:2310.12934</a> [<a href="/pdf/2310.12934" title="Download PDF">pdf</a>, <a href="/format/2310.12934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Flow Networks as Entropy-Regularized RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiapkin%2C+D">Daniil Tiapkin</a>, 
<a href="/search/cs?searchtype=author&query=Morozov%2C+N">Nikita Morozov</a>, 
<a href="/search/cs?searchtype=author&query=Naumov%2C+A">Alexey Naumov</a>, 
<a href="/search/cs?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The recently proposed generative flow networks (GFlowNets) are a method of
training a policy to sample compositional discrete objects with probabilities
proportional to a given reward via a sequence of actions. GFlowNets exploit the
sequential nature of the problem, drawing parallels with reinforcement learning
(RL). Our work extends the connection between RL and GFlowNets to a general
case. We demonstrate how the task of learning a generative flow network can be
efficiently redefined as an entropy-regularized RL problem with a specific
reward and regularizer structure. Furthermore, we illustrate the practical
efficiency of this reformulation by applying standard soft RL algorithms to
GFlowNet training across several probabilistic modeling tasks. Contrary to
previously reported results, we show that entropic RL approaches can be
competitive against established GFlowNet training methods. This perspective
opens a direct path for integrating reinforcement learning principles into the
realm of generative flow networks.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12935" title="Abstract">arXiv:2310.12935</a> [<a href="/pdf/2310.12935" title="Download PDF">pdf</a>, <a href="/ps/2310.12935" title="Download PostScript">ps</a>, <a href="/format/2310.12935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A representation of odd Sugihara chains via weakening relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Craig%2C+A">Andrew Craig</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C">Claudette Robinson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We present a relational representation of odd Sugihara chains. The elements
of the algebra are represented as weakening relations over a particular poset
which consists of two densely embedded copies of the rationals. Our
construction mimics that of Maddux (2010) where a relational representation of
the even Sugihara chains is given. An order automorphism between the two copies
of the rationals is the key to ensuring that the identity element of the monoid
is fixed by the involution.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12936" title="Abstract">arXiv:2310.12936</a> [<a href="/pdf/2310.12936" title="Download PDF">pdf</a>, <a href="/format/2310.12936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Predictive Factor Analysis of Social Biases and Task-Performance in  Pretrained Masked Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>, 
<a href="/search/cs?searchtype=author&query=Bollegala%2C+D">Danushka Bollegala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Various types of social biases have been reported with pretrained Masked
Language Models (MLMs) in prior work. However, multiple underlying factors are
associated with an MLM such as its model size, size of the training data,
training objectives, the domain from which pretraining data is sampled,
tokenization, and languages present in the pretrained corpora, to name a few.
It remains unclear as to which of those factors influence social biases that
are learned by MLMs. To study the relationship between model factors and the
social biases learned by an MLM, as well as the downstream task performance of
the model, we conduct a comprehensive study over 39 pretrained MLMs covering
different model sizes, training objectives, tokenization methods, training data
domains and languages. Our results shed light on important factors often
neglected in prior literature, such as tokenization or model objectives.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12937" title="Abstract">arXiv:2310.12937</a> [<a href="/pdf/2310.12937" title="Download PDF">pdf</a>, <a href="/format/2310.12937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Delay Minimization based on Joint Optimization of DNN  Partitioning and Resource Allocation for Cooperative Edge Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xinrui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanzan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+D">Dingzhu Wen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Guanjin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunqing Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 9 figures, 1 table, 1 algorithm, to be published in IEEE 98th Vehicular Technology Conference (VTC2023-Fall)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Cooperative inference in Mobile Edge Computing (MEC), achieved by deploying
partitioned Deep Neural Network (DNN) models between resource-constrained user
equipments (UEs) and edge servers (ESs), has emerged as a promising paradigm.
Firstly, we consider scenarios of continuous Artificial Intelligence (AI) task
arrivals, like the object detection for video streams, and utilize a serial
queuing model for the accurate evaluation of End-to-End (E2E) delay in
cooperative edge inference. Secondly, to enhance the long-term performance of
inference systems, we formulate a multi-slot stochastic E2E delay optimization
problem that jointly considers model partitioning and multi-dimensional
resource allocation. Finally, to solve this problem, we introduce a
Lyapunov-guided Multi-Dimensional Optimization algorithm (LyMDO) that decouples
the original problem into per-slot deterministic problems, where Deep
Reinforcement Learning (DRL) and convex optimization are used for joint
optimization of partitioning decisions and complementary resource allocation.
Simulation results show that our approach effectively improves E2E delay while
balancing long-term resource constraints.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12941" title="Abstract">arXiv:2310.12941</a> [<a href="/pdf/2310.12941" title="Download PDF">pdf</a>, <a href="/format/2310.12941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Foundation Model Transparency Index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bommasani%2C+R">Rishi Bommasani</a>, 
<a href="/search/cs?searchtype=author&query=Klyman%2C+K">Kevin Klyman</a>, 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+S">Sayash Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Maslej%2C+N">Nestor Maslej</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+B">Betty Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Daniel Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Project page: <a href="https://crfm.stanford.edu/fmti">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Foundation models have rapidly permeated society, catalyzing a wave of
generative AI applications spanning enterprise and consumer-facing contexts.
While the societal impact of foundation models is growing, transparency is on
the decline, mirroring the opacity that has plagued past digital technologies
(e.g. social media). Reversing this trend is essential: transparency is a vital
precondition for public accountability, scientific innovation, and effective
governance. To assess the transparency of the foundation model ecosystem and
help improve transparency over time, we introduce the Foundation Model
Transparency Index. The Foundation Model Transparency Index specifies 100
fine-grained indicators that comprehensively codify transparency for foundation
models, spanning the upstream resources used to build a foundation model (e.g
data, labor, compute), details about the model itself (e.g. size, capabilities,
risks), and the downstream use (e.g. distribution channels, usage policies,
affected geographies). We score 10 major foundation model developers (e.g.
OpenAI, Google, Meta) against the 100 indicators to assess their transparency.
To facilitate and standardize assessment, we score developers in relation to
their practices for their flagship foundation model (e.g. GPT-4 for OpenAI,
PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings about
the foundation model ecosystem: for example, no developer currently discloses
significant information about the downstream impact of its flagship model, such
as the number of users, affected market sectors, or how users can seek redress
for harm. Overall, the Foundation Model Transparency Index establishes the
level of transparency today to drive progress on foundation model governance
via industry standards and regulatory intervention.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12942" title="Abstract">arXiv:2310.12942</a> [<a href="/pdf/2310.12942" title="Download PDF">pdf</a>, <a href="/format/2310.12942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Representational Capacity of Recurrent Neural Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nowak%2C+F">Franz Nowak</a>, 
<a href="/search/cs?searchtype=author&query=Svete%2C+A">Anej Svete</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Li Du</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work investigates the computational expressivity of language models
(LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992)
famously showed that RNNs with rational weights and hidden states and unbounded
computation time are Turing complete. However, LMs define weightings over
strings in addition to just (unweighted) language membership and the analysis
of the computational power of RNN LMs (RLMs) should reflect this. We extend the
Turing completeness result to the probabilistic case, showing how a rationally
weighted RLM with unbounded computation time can simulate any probabilistic
Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a
symbol at every time step, we treat the above result as an upper bound on the
expressivity of RLMs. We also provide a lower bound by showing that under the
restriction to real-time computation, such models can simulate deterministic
real-time rational PTMs.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12945" title="Abstract">arXiv:2310.12945</a> [<a href="/pdf/2310.12945" title="Download PDF">pdf</a>, <a href="/format/2310.12945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D-GPT: Procedural 3D Modeling with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chunyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Junlin Han</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weijian Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinlong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zishan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gould%2C+S">Stephen Gould</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://chuny1.github.io/3DGPT/3dgpt.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the pursuit of efficient automated content creation, procedural
generation, leveraging modifiable parameters and rule-based systems, emerges as
a promising approach. Nonetheless, it could be a demanding endeavor, given its
intricate nature necessitating a deep understanding of rules, algorithms, and
parameters. To reduce workload, we introduce 3D-GPT, a framework utilizing
large language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT
positions LLMs as proficient problem solvers, dissecting the procedural 3D
modeling tasks into accessible segments and appointing the apt agent for each
task. 3D-GPT integrates three core agents: the task dispatch agent, the
conceptualization agent, and the modeling agent. They collaboratively achieve
two objectives. First, it enhances concise initial scene descriptions, evolving
them into detailed forms while dynamically adapting the text based on
subsequent instructions. Second, it integrates procedural generation,
extracting parameter values from enriched text to effortlessly interface with
3D software for asset creation. Our empirical investigations confirm that
3D-GPT not only interprets and executes instructions, delivering reliable
results but also collaborates effectively with human designers. Furthermore, it
seamlessly integrates with Blender, unlocking expanded manipulation
possibilities. Our work highlights the potential of LLMs in 3D modeling,
offering a basic framework for future advancements in scene generation and
animation.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12952" title="Abstract">arXiv:2310.12952</a> [<a href="/pdf/2310.12952" title="Download PDF">pdf</a>, <a href="/format/2310.12952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity  Metrics For Science And Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasarkar%2C+A">Amey Pasarkar</a>, 
<a href="/search/cs?searchtype=author&query=Dieng%2C+A+B">Adji Bousso Dieng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code for evaluating diversity using the Vendi scores can be found at <a href="https://github.com/vertaix/Vendi-Score.">this https URL</a> Code for using the scores within Vendi Sampling can be found at <a href="https://github.com/vertaix/Vendi-Sampling">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">Measuring diversity accurately is important for many scientific fields,
including machine learning (ML), ecology, and chemistry. The Vendi Score was
introduced as a generic similarity-based diversity metric that extends the Hill
number of order q=1 by leveraging ideas from quantum statistical mechanics.
Contrary to many diversity metrics in ecology, the Vendi Score accounts for
similarity and does not require knowledge of the prevalence of the categories
in the collection to be evaluated for diversity. However, the Vendi Score
treats each item in a given collection with a level of sensitivity proportional
to the item's prevalence. This is undesirable in settings where there is a
significant imbalance in item prevalence. In this paper, we extend the other
Hill numbers using similarity to provide flexibility in allocating sensitivity
to rare or common items. This leads to a family of diversity metrics -- Vendi
scores with different levels of sensitivity -- that can be used in a variety of
applications. We study the properties of the scores in a synthetic controlled
setting where the ground truth diversity is known. We then test their utility
in improving molecular simulations via Vendi Sampling. Finally, we use the
Vendi scores to better understand the behavior of image generative models in
terms of memorization, duplication, diversity, and sample quality.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12953" title="Abstract">arXiv:2310.12953</a> [<a href="/pdf/2310.12953" title="Download PDF">pdf</a>, <a href="/format/2310.12953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Generation and Exploration of Design Space with Large  Language Models for Human-AI Co-Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suh%2C+S">Sangho Suh</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Meng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+B">Bryan Min</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Thanks to their generative capabilities, large language models (LLMs) have
become an invaluable tool for creative processes. These models have the
capacity to produce hundreds and thousands of visual and textual outputs,
offering abundant inspiration for creative endeavors. But are we harnessing
their full potential? We argue that current interaction paradigms fall short,
guiding users towards rapid convergence on a limited set of ideas, rather than
empowering them to explore the vast latent design space in generative models.
To address this limitation, we propose a framework that facilitates the
structured generation of design space in which users can seamlessly explore,
evaluate, and synthesize a multitude of responses. We demonstrate the
feasibility and usefulness of this framework through the design and development
of an interactive system, Luminate, and a user study with 8 professional
writers. Our work advances how we interact with LLMs for creative tasks,
introducing a way to harness the creative potential of LLMs.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12955" title="Abstract">arXiv:2310.12955</a> [<a href="/pdf/2310.12955" title="Download PDF">pdf</a>, <a href="/format/2310.12955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Offline Reinforcement Learning under Diverse Data  Corruption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiawei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lei Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline reinforcement learning (RL) presents a promising approach for
learning reinforced policies from offline datasets without the need for costly
or unsafe interactions with the environment. However, datasets collected by
humans in real-world environments are often noisy and may even be maliciously
corrupted, which can significantly degrade the performance of offline RL. In
this work, we first investigate the performance of current offline RL
algorithms under comprehensive data corruption, including states, actions,
rewards, and dynamics. Our extensive experiments reveal that implicit
Q-learning (IQL) demonstrates remarkable resilience to data corruption among
various offline RL algorithms. Furthermore, we conduct both empirical and
theoretical analyses to understand IQL's robust performance, identifying its
supervised policy learning scheme as the key factor. Despite its relative
robustness, IQL still suffers from heavy-tail targets of Q functions under
dynamics corruption. To tackle this challenge, we draw inspiration from robust
statistics to employ the Huber loss to handle the heavy-tailedness and utilize
quantile estimators to balance penalization for corrupted data and learning
stability. By incorporating these simple yet effective modifications into IQL,
we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive
experiments demonstrate that RIQL exhibits highly robust performance when
subjected to diverse data corruption scenarios.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12956" title="Abstract">arXiv:2310.12956</a> [<a href="/pdf/2310.12956" title="Download PDF">pdf</a>, <a href="/format/2310.12956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced  Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+D+T">David T. Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Schrodi%2C+S">Simon Schrodi</a>, 
<a href="/search/cs?searchtype=author&query=Behrmann%2C+N">Nadine Behrmann</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+V">Volker Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Brox%2C+T">Thomas Brox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this work, we study rapid, step-wise improvements of the loss in
transformers when being confronted with multi-step decision tasks. We found
that transformers struggle to learn the intermediate tasks, whereas CNNs have
no such issue on the tasks we studied. When transformers learn the intermediate
task, they do this rapidly and unexpectedly after both training and validation
loss saturated for hundreds of epochs. We call these rapid improvements
Eureka-moments, since the transformer appears to suddenly learn a previously
incomprehensible task. Similar leaps in performance have become known as
Grokking. In contrast to Grokking, for Eureka-moments, both the validation and
the training loss saturate before rapidly improving. We trace the problem back
to the Softmax function in the self-attention block of transformers and show
ways to alleviate the problem. These fixes improve training speed. The improved
models reach 95% of the baseline model in just 20% of training steps while
having a much higher likelihood to learn the intermediate task, lead to higher
final accuracy and are more robust to hyper-parameters.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12958" title="Abstract">arXiv:2310.12958</a> [<a href="/pdf/2310.12958" title="Download PDF">pdf</a>, <a href="/format/2310.12958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Non-Cooperative Games with Principled Player Selection for  Scalable Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chahine%2C+M">Makram Chahine</a>, 
<a href="/search/cs?searchtype=author&query=Firoozi%2C+R">Roya Firoozi</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+W">Wei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Schwager%2C+M">Mac Schwager</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be published in IROS 2023 conference proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Game-theoretic motion planners are a powerful tool for the control of
interactive multi-agent robot systems. Indeed, contrary to predict-then-plan
paradigms, game-theoretic planners do not ignore the interactive nature of the
problem, and simultaneously predict the behaviour of other agents while
considering change in one's policy. This, however, comes at the expense of
computational complexity, especially as the number of agents considered grows.
In fact, planning with more than a handful of agents can quickly become
intractable, disqualifying game-theoretic planners as possible candidates for
large scale planning. In this paper, we propose a planning algorithm enabling
the use of game-theoretic planners in robot systems with a large number of
agents. Our planner is based on the reality of locality of information and thus
deploys local games with a selected subset of agents in a receding horizon
fashion to plan collision avoiding trajectories. We propose five different
principled schemes for selecting game participants and compare their collision
avoidance performance. We observe that the use of Control Barrier Functions for
priority ranking is a potent solution to the player selection problem for
motion planning.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12960" title="Abstract">arXiv:2310.12960</a> [<a href="/pdf/2310.12960" title="Download PDF">pdf</a>, <a href="/format/2310.12960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xueliang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have driven substantial progress in artificial
intelligence in recent years, exhibiting impressive capabilities across a wide
range of tasks, including mathematical problem-solving. Inspired by the success
of subgoal-based methods, we propose a novel framework called
\textbf{SE}quential sub\textbf{G}oal \textbf{O}ptimization (SEGO) to enhance
LLMs' ability to solve mathematical problems. By establishing a connection
between the subgoal breakdown process and the probability of solving problems,
SEGO aims to identify better subgoals with theoretical guarantees. Addressing
the challenge of identifying suitable subgoals in a large solution space, our
framework generates problem-specific subgoals and adjusts them according to
carefully designed criteria. Incorporating these optimized subgoals into the
policy model training leads to significant improvements in problem-solving
performance. We validate SEGO's efficacy through experiments on two benchmarks,
GSM8K and MATH, where our approach outperforms existing methods, highlighting
the potential of SEGO in AI-driven mathematical problem-solving.
<br />Data and code associated with this paper will be available at
https://github.com/zhaoxlpku/SEGO
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12962" title="Abstract">arXiv:2310.12962</a> [<a href="/pdf/2310.12962" title="Download PDF">pdf</a>, <a href="/format/2310.12962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Emulator for Fine-Tuning Large Language Models using Small Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Rafailov%2C+R">Rafael Rafailov</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Archit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C+D">Christopher D. Manning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Widely used language models (LMs) are typically built by scaling up a
two-stage training pipeline: a pre-training stage that uses a very large,
diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that
uses targeted examples or other specifications of desired behaviors. While it
has been hypothesized that knowledge and skills come from pre-training, and
fine-tuning mostly filters this knowledge and skillset, this intuition has not
been extensively tested. To aid in doing so, we introduce a novel technique for
decoupling the knowledge and skills gained in these two stages, enabling a
direct answer to the question, "What would happen if we combined the knowledge
learned by a large model during pre-training with the knowledge learned by a
small model during fine-tuning (or vice versa)?" Using an RL-based framework
derived from recent developments in learning from human preferences, we
introduce emulated fine-tuning (EFT), a principled and practical method for
sampling from a distribution that approximates (or 'emulates') the result of
pre-training and fine-tuning at different scales. Our experiments with EFT show
that scaling up fine-tuning tends to improve helpfulness, while scaling up
pre-training tends to improve factuality. Beyond decoupling scale, we show that
EFT enables test-time adjustment of competing behavioral traits like
helpfulness and harmlessness without additional training. Finally, a special
case of emulated fine-tuning, which we call LM up-scaling, avoids
resource-intensive fine-tuning of large pre-trained models by ensembling them
with small fine-tuned models, essentially emulating the result of fine-tuning
the large pre-trained model. Up-scaling consistently improves helpfulness and
factuality of instruction-following models in the Llama, Llama-2, and Falcon
families, without additional hyperparameters or training.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12963" title="Abstract">arXiv:2310.12963</a> [<a href="/pdf/2310.12963" title="Download PDF">pdf</a>, <a href="/format/2310.12963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoMix: Automatically Mixing Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+P">Pranjal Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+A">Ankit Anand</a>, 
<a href="/search/cs?searchtype=author&query=Potharaju%2C+S+P">Srividya Pranavi Potharaju</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aditya Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rajagopal%2C+D">Dheeraj Rajagopal</a>, 
<a href="/search/cs?searchtype=author&query=Kappaganthu%2C+K">Karthik Kappaganthu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+S">Shyam Upadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>, 
<a href="/search/cs?searchtype=author&query=Faruqui%2C+M">Manaal Faruqui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally. Work started and partly done during Aman's internship at Google
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are now available in various sizes and
configurations from cloud API providers. While this diversity offers a broad
spectrum of choices, effectively leveraging the options to optimize
computational cost and performance remains challenging. In this work, we
present AutoMix, an approach that strategically routes queries to larger LMs,
based on the approximate correctness of outputs from a smaller LM. Central to
AutoMix is a few-shot self-verification mechanism, which estimates the
reliability of its own outputs without requiring training. Given that
verifications can be noisy, we employ a meta verifier in AutoMix to refine the
accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five
context-grounded reasoning datasets demonstrate that AutoMix surpasses
established baselines, improving the incremental benefit per cost by up to 89%.
Our code and data are available at https://github.com/automix-llm/automix.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12967" title="Abstract">arXiv:2310.12967</a> [<a href="/pdf/2310.12967" title="Download PDF">pdf</a>, <a href="/format/2310.12967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Your Model Think Like an Engineer? Explainable AI for Bearing Fault  Detection with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decker%2C+T">Thomas Decker</a>, 
<a href="/search/cs?searchtype=author&query=Lebacher%2C+M">Michael Lebacher</a>, 
<a href="/search/cs?searchtype=author&query=Tresp%2C+V">Volker Tresp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep Learning has already been successfully applied to analyze industrial
sensor data in a variety of relevant use cases. However, the opaque nature of
many well-performing methods poses a major obstacle for real-world deployment.
Explainable AI (XAI) and especially feature attribution techniques promise to
enable insights about how such models form their decision. But the plain
application of such methods often fails to provide truly informative and
problem-specific insights to domain experts. In this work, we focus on the
specific task of detecting faults in rolling element bearings from vibration
signals. We propose a novel and domain-specific feature attribution framework
that allows us to evaluate how well the underlying logic of a model corresponds
with expert reasoning. Utilizing the framework we are able to validate the
trustworthiness and to successfully anticipate the generalization ability of
different well-performing deep learning models. Our methodology demonstrates
how signal processing tools can effectively be used to enhance Explainable AI
techniques and acts as a template for similar problems.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12969" title="Abstract">arXiv:2310.12969</a> [<a href="/pdf/2310.12969" title="Download PDF">pdf</a>, <a href="/format/2310.12969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying the Myths and Legends of Nonconvex Convergence of SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Aritra Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Bergou%2C+E+H">El Houcine Bergou</a>, 
<a href="/search/cs?searchtype=author&query=Boucherouite%2C+S">Soumia Boucherouite</a>, 
<a href="/search/cs?searchtype=author&query=Werge%2C+N">Nicklas Werge</a>, 
<a href="/search/cs?searchtype=author&query=Kandemir%2C+M">Melih Kandemir</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
<p class="mathjax">Stochastic gradient descent (SGD) and its variants are the main workhorses
for solving large-scale optimization problems with nonconvex objective
functions. Although the convergence of SGDs in the (strongly) convex case is
well-understood, their convergence for nonconvex functions stands on weak
mathematical foundations. Most existing studies on the nonconvex convergence of
SGD show the complexity results based on either the minimum of the expected
gradient norm or the functional sub-optimality gap (for functions with extra
structural property) by searching the entire range of iterates. Hence the last
iterations of SGDs do not necessarily maintain the same complexity guarantee.
This paper shows that an $\epsilon$-stationary point exists in the final
iterates of SGDs, given a large enough total iteration budget, $T$, not just
anywhere in the entire range of iterates -- a much stronger result than the
existing one. Additionally, our analyses allow us to measure the density of the
$\epsilon$-stationary points in the final iterates of SGD, and we recover the
classical $O(\frac{1}{\sqrt{T}})$ asymptotic rate under various existing
assumptions on the objective function and the bounds on the stochastic
gradient. As a result of our analyses, we addressed certain myths and legends
related to the nonconvex convergence of SGD and posed some thought-provoking
questions that could set new directions for research.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12970" title="Abstract">arXiv:2310.12970</a> [<a href="/pdf/2310.12970" title="Download PDF">pdf</a>, <a href="/format/2310.12970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Motion Prediction via Heterogeneous Polyline Transformer with  Relative Pose Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhejun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liniger%2C+A">Alexander Liniger</a>, 
<a href="/search/cs?searchtype=author&query=Sakaridis%2C+C">Christos Sakaridis</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fisher Yu</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The real-world deployment of an autonomous driving system requires its
components to run on-board and in real-time, including the motion prediction
module that predicts the future trajectories of surrounding traffic
participants. Existing agent-centric methods have demonstrated outstanding
performance on public benchmarks. However, they suffer from high computational
overhead and poor scalability as the number of agents to be predicted
increases. To address this problem, we introduce the K-nearest neighbor
attention with relative pose encoding (KNARPE), a novel attention mechanism
allowing the pairwise-relative representation to be used by Transformers. Then,
based on KNARPE we present the Heterogeneous Polyline Transformer with Relative
pose encoding (HPTR), a hierarchical framework enabling asynchronous token
update during the online inference. By sharing contexts among agents and
reusing the unchanged contexts, our approach is as efficient as scene-centric
methods, while performing on par with state-of-the-art agent-centric methods.
Experiments on Waymo and Argoverse-2 datasets show that HPTR achieves superior
performance among end-to-end methods that do not apply expensive
post-processing or model ensembling. The code is available at
https://github.com/zhejz/HPTR.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12971" title="Abstract">arXiv:2310.12971</a> [<a href="/pdf/2310.12971" title="Download PDF">pdf</a>, <a href="/format/2310.12971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLAIR: Evaluating Image Captions with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+D">David Chan</a>, 
<a href="/search/cs?searchtype=author&query=Petryk%2C+S">Suzanne Petryk</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Canny%2C+J">John Canny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The evaluation of machine-generated image captions poses an interesting yet
persistent challenge. Effective evaluation measures must consider numerous
dimensions of similarity, including semantic relevance, visual structure,
object interactions, caption diversity, and specificity. Existing
highly-engineered measures attempt to capture specific aspects, but fall short
in providing a holistic score that aligns closely with human judgments. Here,
we propose CLAIR, a novel method that leverages the zero-shot language modeling
capabilities of large language models (LLMs) to evaluate candidate captions. In
our evaluations, CLAIR demonstrates a stronger correlation with human judgments
of caption quality compared to existing measures. Notably, on Flickr8K-Expert,
CLAIR achieves relative correlation improvements over SPICE of 39.6% and over
image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides
noisily interpretable results by allowing the language model to identify the
underlying reasoning behind its assigned score. Code is available at
https://davidmchan.github.io/clair/
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12972" title="Abstract">arXiv:2310.12972</a> [<a href="/pdf/2310.12972" title="Download PDF">pdf</a>, <a href="/format/2310.12972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CCIL: Continuity-based Data Augmentation for Corrective Imitation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+L">Liyiming Ke</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunchu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Abhay Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+S">Siddhartha Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present a new technique to enhance the robustness of imitation learning
methods by generating corrective data to account for compounding errors and
disturbances. While existing methods rely on interactive expert labeling,
additional offline datasets, or domain-specific invariances, our approach
requires minimal additional assumptions beyond access to expert data. The key
insight is to leverage local continuity in the environment dynamics to generate
corrective labels. Our method first constructs a dynamics model from the expert
demonstration, encouraging local Lipschitz continuity in the learned model. In
locally continuous regions, this model allows us to generate corrective labels
within the neighborhood of the demonstrations but beyond the actual set of
states and actions in the dataset. Training on this augmented data enhances the
agent's ability to recover from perturbations and deal with compounding errors.
We demonstrate the effectiveness of our generated labels through experiments in
a variety of robotics domains in simulation that have distinct forms of
continuity and discontinuity, including classic control problems, drone flying,
navigation with high-dimensional sensor observations, legged locomotion, and
tabletop manipulation.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12973" title="Abstract">arXiv:2310.12973</a> [<a href="/pdf/2310.12973" title="Download PDF">pdf</a>, <a href="/format/2310.12973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frozen Transformers in Language Models Are Effective Visual Encoder  Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Z">Ziqi Pang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Ziyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Man%2C+Y">Yunze Man</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 13 figures. Code at <a href="https://github.com/ziqipang/LM4VisualEncoding">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper reveals that large language models (LLMs), despite being trained
solely on textual data, are surprisingly strong encoders for purely visual
tasks in the absence of language. Even more intriguingly, this can be achieved
by a simple yet previously overlooked strategy -- employing a frozen
transformer block from pre-trained LLMs as a constituent encoder layer to
directly process visual tokens. Our work pushes the boundaries of leveraging
LLMs for computer vision tasks, significantly departing from conventional
practices that typically necessitate a multi-modal vision-language setup with
associated language prompts, inputs, or outputs. We demonstrate that our
approach consistently enhances performance across a diverse range of tasks,
encompassing pure 2D and 3D visual recognition tasks (e.g., image and point
cloud classification), temporal modeling tasks (e.g., action recognition),
non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g.,
2D/3D visual question answering and image-text retrieval). Such improvements
are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and
OPT) and different LLM transformer blocks. We additionally propose the
information filtering hypothesis to explain the effectiveness of pre-trained
LLMs in visual encoding -- the pre-trained LLM transformer blocks discern
informative visual tokens and further amplify their effect. This hypothesis is
empirically supported by the observation that the feature activation, after
training with LLM transformer blocks, exhibits a stronger focus on relevant
regions. We hope that our work inspires new perspectives on utilizing LLMs and
deepening our understanding of their underlying mechanisms. Code is available
at https://github.com/ziqipang/LM4VisualEncoding.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12974" title="Abstract">arXiv:2310.12974</a> [<a href="/pdf/2310.12974" title="Download PDF">pdf</a>, <a href="/format/2310.12974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lunayach%2C+M">Mayank Lunayach</a>, 
<a href="/search/cs?searchtype=author&query=Zakharov%2C+S">Sergey Zakharov</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ambrus%2C+R">Rares Ambrus</a>, 
<a href="/search/cs?searchtype=author&query=Kira%2C+Z">Zsolt Kira</a>, 
<a href="/search/cs?searchtype=author&query=Irshad%2C+M+Z">Muhammad Zubair Irshad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://fsd6d.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this work, we address the challenging task of 3D object recognition
without the reliance on real-world 3D labeled data. Our goal is to predict the
3D shape, size, and 6D pose of objects within a single RGB-D image, operating
at the category level and eliminating the need for CAD models during inference.
While existing self-supervised methods have made strides in this field, they
often suffer from inefficiencies arising from non-end-to-end processing,
reliance on separate models for different object categories, and slow surface
extraction during the training of implicit reconstruction models; thus
hindering both the speed and real-world applicability of the 3D recognition
process. Our proposed method leverages a multi-stage training pipeline,
designed to efficiently transfer synthetic performance to the real-world
domain. This approach is achieved through a combination of 2D and 3D supervised
losses during the synthetic domain training, followed by the incorporation of
2D supervised and 3D self-supervised losses on real-world data in two
additional learning stages. By adopting this comprehensive strategy, our method
successfully overcomes the aforementioned limitations and outperforms existing
self-supervised 6D pose and size estimation baselines on the NOCS test-set with
a 16.4% absolute improvement in mAP for 6D pose estimation while running in
near real-time at 5 Hz.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12975" title="Abstract">arXiv:2310.12975</a> [<a href="/pdf/2310.12975" title="Download PDF">pdf</a>, <a href="/format/2310.12975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Inference for SDEs Driven by Fractional Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daems%2C+R">Rembert Daems</a>, 
<a href="/search/cs?searchtype=author&query=Opper%2C+M">Manfred Opper</a>, 
<a href="/search/cs?searchtype=author&query=Crevecoeur%2C+G">Guillaume Crevecoeur</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a novel variational framework for performing inference in (neural)
stochastic differential equations (SDEs) driven by Markov-approximate
fractional Brownian motion (fBM). SDEs offer a versatile tool for modeling
real-world continuous-time dynamic systems with inherent noise and randomness.
Combining SDEs with the powerful inference capabilities of variational methods,
enables the learning of representative function distributions through
stochastic gradient descent. However, conventional SDEs typically assume the
underlying noise to follow a Brownian motion (BM), which hinders their ability
to capture long-term dependencies. In contrast, fractional Brownian motion
(fBM) extends BM to encompass non-Markovian dynamics, but existing methods for
inferring fBM parameters are either computationally demanding or statistically
inefficient. In this paper, building upon the Markov approximation of fBM, we
derive the evidence lower bound essential for efficient variational inference
of posterior path measures, drawing from the well-established field of
stochastic analysis. Additionally, we provide a closed-form expression to
determine optimal approximation coefficients. Furthermore, we propose the use
of neural networks to learn the drift, diffusion and control terms within our
variational posterior, leading to the variational training of neural-SDEs. In
this framework, we also optimize the Hurst index, governing the nature of our
fractional noise. Beyond validation on synthetic data, we contribute a novel
architecture for variational latent video prediction,-an approach that, to the
best of our knowledge, enables the first variational neural-SDE application to
video perception.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12976" title="Abstract">arXiv:2310.12976</a> [<a href="/pdf/2310.12976" title="Download PDF">pdf</a>, <a href="/format/2310.12976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Hidden Waves of Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yinpeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xiyang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youzuo Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we introduce an intriguing phenomenon-the successful
reconstruction of images using a set of one-way wave equations with hidden and
learnable speeds. Each individual image corresponds to a solution with a unique
initial condition, which can be computed from the original image using a visual
encoder (e.g., a convolutional neural network). Furthermore, the solution for
each image exhibits two noteworthy mathematical properties: (a) it can be
decomposed into a collection of special solutions of the same one-way wave
equations that are first-order autoregressive, with shared coefficient matrices
for autoregression, and (b) the product of these coefficient matrices forms a
diagonal matrix with the speeds of the wave equations as its diagonal elements.
We term this phenomenon hidden waves, as it reveals that, although the speeds
of the set of wave equations and autoregressive coefficient matrices are
latent, they are both learnable and shared across images. This represents a
mathematical invariance across images, providing a new mathematical perspective
to understand images.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12977" title="Abstract">arXiv:2310.12977</a> [<a href="/pdf/2310.12977" title="Download PDF">pdf</a>, <a href="/format/2310.12977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Dynamics of Deep Network Linear Regions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Humayun%2C+A+I">Ahmed Imtiaz Humayun</a>, 
<a href="/search/cs?searchtype=author&query=Balestriero%2C+R">Randall Balestriero</a>, 
<a href="/search/cs?searchtype=author&query=Baraniuk%2C+R">Richard Baraniuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The study of Deep Network (DN) training dynamics has largely focused on the
evolution of the loss function, evaluated on or around train and test set data
points. In fact, many DN phenomenon were first introduced in literature with
that respect, e.g., double descent, grokking. In this study, we look at the
training dynamics of the input space partition or linear regions formed by
continuous piecewise affine DNs, e.g., networks with (leaky)ReLU
nonlinearities. First, we present a novel statistic that encompasses the local
complexity (LC) of the DN based on the concentration of linear regions inside
arbitrary dimensional neighborhoods around data points. We observe that during
training, the LC around data points undergoes a number of phases, starting with
a decreasing trend after initialization, followed by an ascent and ending with
a final descending trend. Using exact visualization methods, we come across the
perplexing observation that during the final LC descent phase of training,
linear regions migrate away from training and test samples towards the decision
boundary, making the DN input-output nearly linear everywhere else. We also
observe that the different LC phases are closely related to the memorization
and generalization performance of the DN, especially during grokking.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12978" title="Abstract">arXiv:2310.12978</a> [<a href="/pdf/2310.12978" title="Download PDF">pdf</a>, <a href="/format/2310.12978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HumanTOMATO: Text-aligned Whole-body Motion Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shunlin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling-Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Ailing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jing Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruimao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H">Heung-Yeung Shum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 15 figures, 16 tables. Project page: <a href="https://lhchen.top/HumanTOMATO">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This work targets a novel text-driven whole-body motion generation task,
which takes a given textual description as input and aims at generating
high-quality, diverse, and coherent facial expressions, hand gestures, and body
motions simultaneously. Previous works on text-driven motion generation tasks
mainly have two limitations: they ignore the key role of fine-grained hand and
face controlling in vivid whole-body motion generation, and lack a good
alignment between text and motion. To address such limitations, we propose a
Text-aligned whOle-body Motion generATiOn framework, named HumanTOMATO, which
is the first attempt to our knowledge towards applicable holistic motion
generation in this research area. To tackle this challenging task, our solution
includes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) and
a Hierarchical-GPT for fine-grained body and hand motion reconstruction and
generation with two structured codebooks; and (2) a pre-trained
text-motion-alignment model to help generated motion align with the input
textual description explicitly. Comprehensive experiments verify that our model
has significant advantages in both the quality of generated motions and their
alignment with text.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12982" title="Abstract">arXiv:2310.12982</a> [<a href="/pdf/2310.12982" title="Download PDF">pdf</a>, <a href="/format/2310.12982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Putting the Object Back into Video Object Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H+K">Ho Kei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+W">Seoung Wug Oh</a>, 
<a href="/search/cs?searchtype=author&query=Price%2C+B">Brian Price</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joon-Young Lee</a>, 
<a href="/search/cs?searchtype=author&query=Schwing%2C+A">Alexander Schwing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://hkchengrex.github.io/Cutie">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present Cutie, a video object segmentation (VOS) network with object-level
memory reading, which puts the object representation from memory back into the
video object segmentation result. Recent works on VOS employ bottom-up
pixel-level memory reading which struggles due to matching noise, especially in
the presence of distractors, resulting in lower performance in more challenging
data. In contrast, Cutie performs top-down object-level memory reading by
adapting a small set of object queries for restructuring and interacting with
the bottom-up pixel features iteratively with a query-based object transformer
(qt, hence Cutie). The object queries act as a high-level summary of the target
object, while high-resolution feature maps are retained for accurate
segmentation. Together with foreground-background masked attention, Cutie
cleanly separates the semantics of the foreground object from the background.
On the challenging MOSE dataset, Cutie improves by 8.7 J&amp;F over XMem with a
similar running time and improves by 4.2 J&amp;F over DeAOT while running three
times as fast. Code is available at: https://hkchengrex.github.io/Cutie
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri, 20 Oct 23</h3>
<dl>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04934" title="Abstract">arXiv:2305.04934</a> (cross-list from q-bio.BM) [<a href="/pdf/2305.04934" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Pretrained Autoregressive Transformer Graph Neural Network  applied to the Analysis and Discovery of Novel Proteins
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Soft Condensed Matter (cond-mat.soft); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We report a flexible language-model based deep learning strategy, applied
here to solve complex forward and inverse problems in protein modeling, based
on an attention neural network that integrates transformer and graph
convolutional architectures in a causal multi-headed graph mechanism, to
realize a generative pretrained model. The model is applied to predict
secondary structure content (per-residue level and overall content), protein
solubility, and sequencing tasks. Further trained on inverse tasks, the model
is rendered capable of designing proteins with these properties as target
features. The model is formulated as a general framework, completely
prompt-based, and can be adapted for a variety of downstream tasks. We find
that adding additional tasks yields emergent synergies that the model exploits
in improving overall performance, beyond what would be possible by training a
model on each dataset alone. Case studies are presented to validate the method,
yielding protein designs specifically focused on structural proteins, but also
exploring the applicability in the design of soluble, antimicrobial
biomaterials. While our model is trained to ultimately perform 8 distinct
tasks, with available datasets it can be extended to solve additional problems.
In a broader sense, this work illustrates a form of multiscale modeling that
relates a set of ultimate building blocks (here, byte-level utf8 characters
that define the nature of the physical system at hand) to complex output. This
materiomic scheme captures complex emergent relationships between universal
building block and resulting properties via a synergizing learning capacity to
express a set of potentialities embedded in the knowledge used in training, via
the interplay of universality and diversity.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17525" title="Abstract">arXiv:2306.17525</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2306.17525" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MeLM, a generative pretrained language modeling framework that solves  forward and inverse mechanics problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Mesoscale and Nanoscale Physics (cond-mat.mes-hall); Other Condensed Matter (cond-mat.other); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We report a flexible multi-modal mechanics language model, MeLM, applied to
solve various nonlinear forward and inverse problems, that can deal with a set
of instructions, numbers and microstructure data. The framework is applied to
various examples including bio-inspired hierarchical honeycomb design, carbon
nanotube mechanics, and protein unfolding. In spite of the flexible nature of
the model-which allows us to easily incorporate diverse materials, scales, and
mechanical features-it performs well across disparate forward and inverse
tasks. Based on an autoregressive attention-model, MeLM effectively represents
a large multi-particle system consisting of hundreds of millions of neurons,
where the interaction potentials are discovered through graph-forming
self-attention mechanisms that are then used to identify relationships from
emergent structures, while taking advantage of synergies discovered in the
training data. We show that the model can solve complex degenerate mechanics
design problems and determine novel material architectures across a range of
hierarchical levels, providing an avenue for materials discovery and analysis.
Looking beyond the demonstrations reported in this paper, we discuss other
opportunities in applied mechanics and general considerations about the use of
large language models in modeling, design, and analysis that can span a broad
spectrum of material properties from mechanical, thermal, optical, to
electronic.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10170" title="Abstract">arXiv:2309.10170</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2309.10170" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative modeling, design and analysis of spider silk protein  sequences for enhanced mechanical properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Lu%2C+W">Wei Lu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kaplan%2C+D+L">David L. Kaplan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">Spider silks are remarkable materials characterized by superb mechanical
properties such as strength, extensibility and lightweightedness. Yet, to date,
limited models are available to fully explore sequence-property relationships
for analysis and design. Here we propose a custom generative large-language
model to enable design of novel spider silk protein sequences to meet complex
combinations of target mechanical properties. The model, pretrained on a large
set of protein sequences, is fine-tuned on ~1,000 major ampullate spidroin
(MaSp) sequences for which associated fiber-level mechanical properties exist,
to yield an end-to-end forward and inverse generative strategy. Performance is
assessed through: (1), a novelty analysis and protein type classification for
generated spidroin sequences through BLAST searches, (2) property evaluation
and comparison with similar sequences, (3) comparison of molecular structures,
as well as, and (4) a detailed sequence motif analyses. We generate silk
sequences with property combinations that do not exist in nature, and develop a
deep understanding the mechanistic roles of sequence patterns in achieving
overarching key mechanical properties (elastic modulus, strength, toughness,
failure strain). The model provides an efficient approach to expand the silkome
dataset, facilitating further sequence-structure analyses of silks, and
establishes a foundation for synthetic silk design and optimization.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12156" title="Abstract">arXiv:2310.12156</a> (cross-list from nlin.AO) [<a href="/pdf/2310.12156" title="Download PDF">pdf</a>, <a href="/format/2310.12156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operator-Based Detecting, Learning, and Stabilizing Unstable Periodic  Orbits of Chaotic Attractors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Tavasoli%2C+A">Ali Tavasoli</a>, 
<a href="/search/nlin?searchtype=author&query=Shakeri%2C+H">Heman Shakeri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2304.07832">arXiv:2304.07832</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Adaptation and Self-Organizing Systems (nlin.AO)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS); Optimization and Control (math.OC); Spectral Theory (math.SP)

</div>
<p class="mathjax">This paper examines the use of operator-theoretic approaches to the analysis
of chaotic systems through the lens of their unstable periodic orbits (UPOs).
Our approach involves three data-driven steps for detecting, identifying, and
stabilizing UPOs. We demonstrate the use of kernel integral operators within
delay coordinates as an innovative method for UPO detection. For identifying
the dynamic behavior associated with each individual UPO, we utilize the
Koopman operator to present the dynamics as linear equations in the space of
Koopman eigenfunctions. This allows for characterizing the chaotic attractor by
investigating its principal dynamical modes across varying UPOs. We extend this
methodology into an interpretable machine learning framework aimed at
stabilizing strange attractors on their UPOs. To illustrate the efficacy of our
approach, we apply it to the Lorenz attractor as a case study.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12174" title="Abstract">arXiv:2310.12174</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.12174" title="Download PDF">pdf</a>, <a href="/format/2310.12174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Traffic Control Framework for Uncrewed Aircraft Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gupta%2C+A+V">Ananay Vikram Gupta</a>, 
<a href="/search/physics?searchtype=author&query=Kattekola%2C+A+P">Aaditya Prakash Kattekola</a>, 
<a href="/search/physics?searchtype=author&query=Gupta%2C+A+V">Ansh Vikram Gupta</a>, 
<a href="/search/physics?searchtype=author&query=Abhiram%2C+D+V">Dacharla Venkata Abhiram</a>, 
<a href="/search/physics?searchtype=author&query=Namuduri%2C+K">Kamesh Namuduri</a>, 
<a href="/search/physics?searchtype=author&query=Subramanian%2C+R">Ravichandran Subramanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computational Engineering, Finance, and Science (cs.CE); Systems and Control (eess.SY)

</div>
<p class="mathjax">The exponential growth of Advanced Air Mobility (AAM) services demands
assurances of safety in the airspace. This research a Traffic Control Framework
(TCF) for developing digital flight rules for Uncrewed Aircraft System (UAS)
flying in designated air corridors. The proposed TCF helps model, deploy, and
test UAS control, agents, regardless of their hardware configurations. This
paper investigates the importance of digital flight rules in preventing
collisions in the context of AAM. TCF is introduced as a platform for
developing strategies for managing traffic towards enhanced autonomy in the
airspace. It allows for assessment and evaluation of autonomous navigation,
route planning, obstacle avoidance, and adaptive decision making for UAS. It
also allows for the introduction and evaluation of advance technologies
Artificial Intelligence (AI) and Machine Learning (ML) in a simulation
environment before deploying them in the real world. TCF can be used as a tool
for comprehensive UAS traffic analysis, including KPI measurements. It offers
flexibility for further testing and deployment laying the foundation for
improved airspace safety - a vital aspect of UAS technological advancement.
Finally, this papers demonstrates the capabilities of the proposed TCF in
managing UAS traffic at intersections and its impact on overall traffic flow in
air corridors, noting the bottlenecks and the inverse relationship safety and
traffic volume.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12183" title="Abstract">arXiv:2310.12183</a> (cross-list from math.OC) [<a href="/pdf/2310.12183" title="Download PDF">pdf</a>, <a href="/format/2310.12183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel  Inventories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Harsha%2C+P">Pavithra Harsha</a>, 
<a href="/search/math?searchtype=author&query=Subramanian%2C+S">Shivaram Subramanian</a>, 
<a href="/search/math?searchtype=author&query=Koc%2C+A">Ali Koc</a>, 
<a href="/search/math?searchtype=author&query=Ramakrishna%2C+M">Mahesh Ramakrishna</a>, 
<a href="/search/math?searchtype=author&query=Quanz%2C+B">Brian Quanz</a>, 
<a href="/search/math?searchtype=author&query=Shah%2C+D">Dhruv Shah</a>, 
<a href="/search/math?searchtype=author&query=Narayanaswami%2C+C">Chandra Narayanaswami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a new class of data-driven and distribution-free
optimistic-robust bimodal inventory optimization (BIO) strategy to effectively
allocate inventory across a retail chain to meet time-varying, uncertain
omnichannel demand. While prior Robust optimization (RO) methods emphasize the
downside, i.e., worst-case adversarial demand, BIO also considers the upside to
remain resilient like RO while also reaping the rewards of improved
average-case performance by overcoming the presence of endogenous outliers.
This bimodal strategy is particularly valuable for balancing the tradeoff
between lost sales at the store and the costs of cross-channel e-commerce
fulfillment, which is at the core of our inventory optimization model. These
factors are asymmetric due to the heterogenous behavior of the channels, with a
bias towards the former in terms of lost-sales cost and a dependence on network
effects for the latter. We provide structural insights about the BIO solution
and how it can be tuned to achieve a preferred tradeoff between robustness and
the average-case. Our experiments show that significant benefits can be
achieved by rethinking traditional approaches to inventory management, which
are siloed by channel and location. Using a real-world dataset from a large
American omnichannel retail chain, a business value assessment during a peak
period indicates over a 15% profitability gain for BIO over RO and other
baselines while also preserving the (practical) worst case performance.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12209" title="Abstract">arXiv:2310.12209</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.12209" title="Download PDF">pdf</a>, <a href="/format/2310.12209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Shih%2C+D">David Shih</a>, 
<a href="/search/astro-ph?searchtype=author&query=Freytsis%2C+M">Marat Freytsis</a>, 
<a href="/search/astro-ph?searchtype=author&query=Taylor%2C+S+R">Stephen R. Taylor</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dror%2C+J+A">Jeff A. Dror</a>, 
<a href="/search/astro-ph?searchtype=author&query=Smyth%2C+N">Nolan Smyth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; High Energy Astrophysical Phenomena (astro-ph.HE); Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc); High Energy Physics - Phenomenology (hep-ph)

</div>
<p class="mathjax">Pulsar timing arrays (PTAs) perform Bayesian posterior inference with
expensive MCMC methods. Given a dataset of ~10-100 pulsars and O(10^3) timing
residuals each, producing a posterior distribution for the stochastic
gravitational wave background (SGWB) can take days to a week. The computational
bottleneck arises because the likelihood evaluation required for MCMC is
extremely costly when considering the dimensionality of the search space.
Fortunately, generating simulated data is fast, so modern simulation-based
inference techniques can be brought to bear on the problem. In this paper, we
demonstrate how conditional normalizing flows trained on simulated data can be
used for extremely fast and accurate estimation of the SGWB posteriors,
reducing the sampling time from weeks to a matter of seconds.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12256" title="Abstract">arXiv:2310.12256</a> (cross-list from quant-ph) [<a href="/pdf/2310.12256" title="Download PDF">pdf</a>, <a href="/format/2310.12256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some improvements to product formula circuits for Hamiltonian simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kornell%2C+A">Andre Kornell</a>, 
<a href="/search/quant-ph?searchtype=author&query=Selinger%2C+P">Peter Selinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">We provide three improvements to the standard implementation of the ground
state energy estimation algorithm via Trotter-Suzuki decomposition. These
consist of smaller circuit templates for each Hamiltonian term, parallelization
of commuting controlled rotations, and more efficient scheduling. These
improvements may be regarded separately, and we anticipate that they may be
combined with other improvements to the standard implementation.
<br />Note that we are not proposing a new algorithm for ground state energy
estimation, nor are we claiming that the Trotter-Suzuki product formula family
of algorithms is the optimal choice for this problem. Rather, we are
demonstrating the use of circuit optimization techniques to give a very
efficient implementation of this particular algorithm.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12299" title="Abstract">arXiv:2310.12299</a> (cross-list from math.DG) [<a href="/pdf/2310.12299" title="Download PDF">pdf</a>, <a href="/format/2310.12299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instantaneous Frequency Estimation in Unbalanced Systems Using Affine  Differential Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alshawabkeh%2C+A">Ali Alshawabkeh</a>, 
<a href="/search/math?searchtype=author&query=Tzounas%2C+G">Georgios Tzounas</a>, 
<a href="/search/math?searchtype=author&query=Milano%2C+F">Federico Milano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Differential Geometry (math.DG)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">The paper discusses the relationships between electrical quantities, namely
voltages and frequency, and affine differential geometry ones, namely affine
arc length and curvature. Moreover, it establishes a link between frequency and
time derivatives of voltage, through the utilization of affine differential
geometry invariants. Based on this link, a new instantaneous frequency
estimation formula is proposed, which is particularly suited for unbalanced
systems. An application of the proposed formula to single-phase systems is also
provided. Several numerical examples based on balanced, unbalanced, as well as
single-phase systems illustrate the findings of the paper.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12304" title="Abstract">arXiv:2310.12304</a> (cross-list from stat.ML) [<a href="/pdf/2310.12304" title="Download PDF">pdf</a>, <a href="/format/2310.12304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preference Optimization for Molecular Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Park%2C+R">Ryan Park</a>, 
<a href="/search/stat?searchtype=author&query=Theisen%2C+R">Ryan Theisen</a>, 
<a href="/search/stat?searchtype=author&query=Sahni%2C+N">Navriti Sahni</a>, 
<a href="/search/stat?searchtype=author&query=Patek%2C+M">Marcel Patek</a>, 
<a href="/search/stat?searchtype=author&query=Cicho%C5%84ska%2C+A">Anna Cicho&#x144;ska</a>, 
<a href="/search/stat?searchtype=author&query=Rahman%2C+R">Rayees Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Molecular language modeling is an effective approach to generating novel
chemical structures. However, these models do not \emph{a priori} encode
certain preferences a chemist may desire. We investigate the use of fine-tuning
using Direct Preference Optimization to better align generated molecules with
chemist preferences. Our findings suggest that this approach is simple,
efficient, and highly effective.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12346" title="Abstract">arXiv:2310.12346</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.12346" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking electricity losses and their perceived causes using nighttime  light and social media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kerber%2C+S+W">Samuel W Kerber</a>, 
<a href="/search/physics?searchtype=author&query=Duncan%2C+N+A">Nicholas A Duncan</a>, 
<a href="/search/physics?searchtype=author&query=LHer%2C+G+F">Guillaume F LHer</a>, 
<a href="/search/physics?searchtype=author&query=Bazilian%2C+M">Morgan Bazilian</a>, 
<a href="/search/physics?searchtype=author&query=Elvidge%2C+C">Chris Elvidge</a>, 
<a href="/search/physics?searchtype=author&query=Deinert%2C+M+R">Mark R Deinert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Urban environments are intricate systems where the breakdown of critical
infrastructure can impact both the economic and social well-being of
communities. Electricity systems hold particular significance, as they are
essential for other infrastructure, and disruptions can trigger widespread
consequences. Typically, assessing electricity availability requires
ground-level data, a challenge in conflict zones and regions with limited
access. This study shows how satellite imagery, social media, and information
extraction can monitor blackouts and their perceived causes. Night-time light
data (in March 2019 for Caracas, Venezuela) is used to indicate blackout
regions. Twitter data is used to determine sentiment and topic trends, while
statistical analysis and topic modeling delved into public perceptions
regarding blackout causes. The findings show an inverse relationship between
nighttime light intensity. Tweets mentioning the Venezuelan President displayed
heightened negativity and a greater prevalence of blame-related terms,
suggesting a perception of government accountability for the outages.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12351" title="Abstract">arXiv:2310.12351</a> (cross-list from quant-ph) [<a href="/pdf/2310.12351" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NuQKD: A Modular Quantum Key Distribution Simulation Framework for  Engineering Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Gkouliaras%2C+K">Konstantinos Gkouliaras</a>, 
<a href="/search/quant-ph?searchtype=author&query=Theos%2C+V">Vasileios Theos</a>, 
<a href="/search/quant-ph?searchtype=author&query=Evans%2C+P">Phil Evans</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chatzidakis%2C+S">Stylianos Chatzidakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">An experimental Quantum Key Distribution (QKD) implementation requires
advanced costly hardware, unavailable in most research environments, making
protocol testing and performance evaluation complicated. Historically, this has
been a major motivation for the development of QKD simulation frameworks, to
allow researchers to obtain insight before proceeding into practical
implementations. Several simulators have been introduced over the recent years.
However, only four are publicly available, only one of which models equipment
imperfections. Currently, no open-source simulator includes all following
capabilities: channel attenuation modelling, equipment imperfections and effect
on key rates, estimation of elapsed time during quantum channel processes, use
of truly random binary sequences for qubits and measurement bases, shared-bit
fraction customization. In this paper, we present NuQKD, an open-source
modular, intuitive simulator, featuring all the above capabilities. NuQKD
establishes communication between two computer terminals, accepts custom inputs
(iterations, raw key size, interception rate etc.) and evaluates the sifted key
length, Quantum Bit Error Rate (QBER), elapsed communication time and more).
NuQKD capabilities include optical fiber and free-space simulation, modeling of
equipment/channel imperfections, bitstrings from True Random Number Generator,
modular design and automated evaluation of performance metrics. We expect NuQKD
to enable convenient and accurate representation of actual experimental
conditions.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12371" title="Abstract">arXiv:2310.12371</a> (cross-list from eess.AS) [<a href="/pdf/2310.12371" title="Download PDF">pdf</a>, <a href="/format/2310.12371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling  Technique for Synthetic Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Park%2C+T+J">Tae Jin Park</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+H">He Huang</a>, 
<a href="/search/eess?searchtype=author&query=Hooper%2C+C">Coleman Hooper</a>, 
<a href="/search/eess?searchtype=author&query=Koluguri%2C+N">Nithin Koluguri</a>, 
<a href="/search/eess?searchtype=author&query=Dhawan%2C+K">Kunal Dhawan</a>, 
<a href="/search/eess?searchtype=author&query=Jukic%2C+A">Ante Jukic</a>, 
<a href="/search/eess?searchtype=author&query=Balam%2C+J">Jagadeesh Balam</a>, 
<a href="/search/eess?searchtype=author&query=Ginsburg%2C+B">Boris Ginsburg</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CHiME-7 Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">We introduce a sophisticated multi-speaker speech data simulator,
specifically engineered to generate multi-speaker speech recordings. A notable
feature of this simulator is its capacity to modulate the distribution of
silence and overlap via the adjustment of statistical parameters. This
capability offers a tailored training environment for developing neural models
suited for speaker diarization and voice activity detection. The acquisition of
substantial datasets for speaker diarization often presents a significant
challenge, particularly in multi-speaker scenarios. Furthermore, the precise
time stamp annotation of speech data is a critical factor for training both
speaker diarization and voice activity detection. Our proposed multi-speaker
simulator tackles these problems by generating large-scale audio mixtures that
maintain statistical properties closely aligned with the input parameters. We
demonstrate that the proposed multi-speaker simulator generates audio mixtures
with statistical properties that closely align with the input parameters
derived from real-world statistics. Additionally, we present the effectiveness
of speaker diarization and voice activity detection models, which have been
trained exclusively on the generated simulated datasets.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12378" title="Abstract">arXiv:2310.12378</a> (cross-list from eess.AS) [<a href="/pdf/2310.12378" title="Download PDF">pdf</a>, <a href="/format/2310.12378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The CHiME-7 Challenge: System Description and Performance of NeMo Team&#x27;s  DASR System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Park%2C+T+J">Tae Jin Park</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+H">He Huang</a>, 
<a href="/search/eess?searchtype=author&query=Jukic%2C+A">Ante Jukic</a>, 
<a href="/search/eess?searchtype=author&query=Dhawan%2C+K">Kunal Dhawan</a>, 
<a href="/search/eess?searchtype=author&query=Puvvada%2C+K+C">Krishna C. Puvvada</a>, 
<a href="/search/eess?searchtype=author&query=Koluguri%2C+N">Nithin Koluguri</a>, 
<a href="/search/eess?searchtype=author&query=Karpov%2C+N">Nikolay Karpov</a>, 
<a href="/search/eess?searchtype=author&query=Laptev%2C+A">Aleksandr Laptev</a>, 
<a href="/search/eess?searchtype=author&query=Balam%2C+J">Jagadeesh Balam</a>, 
<a href="/search/eess?searchtype=author&query=Ginsburg%2C+B">Boris Ginsburg</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CHiME-7 Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">We present the NVIDIA NeMo team's multi-channel speech recognition system for
the 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task,
focusing on the development of a multi-channel, multi-speaker speech
recognition system tailored to transcribe speech from distributed microphones
and microphone arrays. The system predominantly comprises of the following
integral modules: the Speaker Diarization Module, Multi-channel Audio Front-End
Processing Module, and the ASR Module. These components collectively establish
a cascading system, meticulously processing multi-channel and multi-speaker
audio input. Moreover, this paper highlights the comprehensive optimization
process that significantly enhanced our system's performance. Our team's
submission is largely based on NeMo toolkits and will be publicly available.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12402" title="Abstract">arXiv:2310.12402</a> (cross-list from stat.ME) [<a href="/pdf/2310.12402" title="Download PDF">pdf</a>, <a href="/format/2310.12402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data visualization and dimension reduction for metric-valued response  regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Soale%2C+A">Abdul-Nasah Soale</a>, 
<a href="/search/stat?searchtype=author&query=Dong%2C+Y">Yuexiao Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Human-Computer Interaction (cs.HC); Computation (stat.CO)

</div>
<p class="mathjax">As novel data collection becomes increasingly common, traditional dimension
reduction and data visualization techniques are becoming inadequate to analyze
these complex data. A surrogate-assisted sufficient dimension reduction (SDR)
method for regression with a general metric-valued response on Euclidean
predictors is proposed. The response objects are mapped to a real-valued
distance matrix using an appropriate metric and then projected onto a large
sample of random unit vectors to obtain scalar-valued surrogate responses. An
ensemble estimate of the subspaces for the regression of the surrogate
responses versus the predictor is used to estimate the original central space.
Under this framework, classical SDR methods such as ordinary least squares and
sliced inverse regression are extended. The surrogate-assisted method applies
to responses on compact metric spaces including but not limited to Euclidean,
distributional, and functional. An extensive simulation experiment demonstrates
the superior performance of the proposed surrogate-assisted method on synthetic
data compared to existing competing methods where applicable. The analysis of
the distributions and functional trajectories of county-level COVID-19
transmission rates in the U.S. as a function of demographic characteristics is
also provided. The theoretical justifications are included as well.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12405" title="Abstract">arXiv:2310.12405</a> (cross-list from eess.IV) [<a href="/pdf/2310.12405" title="Download PDF">pdf</a>, <a href="/format/2310.12405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoMAE: Low-level Vision Masked Autoencoders for Low-dose CT Denoising
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+D">Dayang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yongshun Xu</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+S">Shuo Han</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zhan Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Morovati%2C+B">Bahareh Morovati</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+H">Hengyong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Low-dose computed tomography (LDCT) offers reduced X-ray radiation exposure
but at the cost of compromised image quality, characterized by increased noise
and artifacts. Recently, transformer models emerged as a promising avenue to
enhance LDCT image quality. However, the success of such models relies on a
large amount of paired noisy and clean images, which are often scarce in
clinical settings. In the fields of computer vision and natural language
processing, masked autoencoders (MAE) have been recognized as an effective
label-free self-pretraining method for transformers, due to their exceptional
feature representation ability. However, the original pretraining and
fine-tuning design fails to work in low-level vision tasks like denoising. In
response to this challenge, we redesign the classical encoder-decoder learning
model and facilitate a simple yet effective low-level vision MAE, referred to
as LoMAE, tailored to address the LDCT denoising problem. Moreover, we
introduce an MAE-GradCAM method to shed light on the latent learning mechanisms
of the MAE/LoMAE. Additionally, we explore the LoMAE's robustness and
generability across a variety of noise levels. Experiments results show that
the proposed LoMAE can enhance the transformer's denoising performance and
greatly relieve the dependence on the ground truth clean data. It also
demonstrates remarkable robustness and generalizability over a spectrum of
noise levels.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12428" title="Abstract">arXiv:2310.12428</a> (cross-list from stat.ML) [<a href="/pdf/2310.12428" title="Download PDF">pdf</a>, <a href="/format/2310.12428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Enhanced Local Explainability of Random Forests: a  Proximity-Based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rosaler%2C+J">Joshua Rosaler</a>, 
<a href="/search/stat?searchtype=author&query=Desai%2C+D">Dhruv Desai</a>, 
<a href="/search/stat?searchtype=author&query=Sarmah%2C+B">Bhaskarjit Sarmah</a>, 
<a href="/search/stat?searchtype=author&query=Vamvourellis%2C+D">Dimitrios Vamvourellis</a>, 
<a href="/search/stat?searchtype=author&query=Onay%2C+D">Deran Onay</a>, 
<a href="/search/stat?searchtype=author&query=Mehta%2C+D">Dhagash Mehta</a>, 
<a href="/search/stat?searchtype=author&query=Pasquali%2C+S">Stefano Pasquali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistical Finance (q-fin.ST); Methodology (stat.ME)

</div>
<p class="mathjax">We initiate a novel approach to explain the out of sample performance of
random forest (RF) models by exploiting the fact that any RF can be formulated
as an adaptive weighted K nearest-neighbors model. Specifically, we use the
proximity between points in the feature space learned by the RF to re-write
random forest predictions exactly as a weighted average of the target labels of
training data points. This linearity facilitates a local notion of
explainability of RF predictions that generates attributions for any model
prediction across observations in the training set, and thereby complements
established methods like SHAP, which instead generates attributions for a model
prediction across dimensions of the feature space. We demonstrate this approach
in the context of a bond pricing model trained on US corporate bond trades, and
compare our approach to various existing approaches to model explainability.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12447" title="Abstract">arXiv:2310.12447</a> (cross-list from stat.ML) [<a href="/pdf/2310.12447" title="Download PDF">pdf</a>, <a href="/format/2310.12447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Reweighting of Distributions: an Optimal Transport Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chakraborty%2C+A">Abhisek Chakraborty</a>, 
<a href="/search/stat?searchtype=author&query=Bhattacharya%2C+A">Anirban Bhattacharya</a>, 
<a href="/search/stat?searchtype=author&query=Pati%2C+D">Debdeep Pati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2303.10085">arXiv:2303.10085</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We commonly encounter the problem of identifying an optimally weight adjusted
version of the empirical distribution of observed data, adhering to predefined
constraints on the weights. Such constraints often manifest as restrictions on
the moments, tail behaviour, shapes, number of modes, etc., of the resulting
weight adjusted empirical distribution. In this article, we substantially
enhance the flexibility of such methodology by introducing a nonparametrically
imbued distributional constraints on the weights, and developing a general
framework leveraging the maximum entropy principle and tools from optimal
transport. The key idea is to ensure that the maximum entropy weight adjusted
empirical distribution of the observed data is close to a pre-specified
probability distribution in terms of the optimal transport metric while
allowing for subtle departures. The versatility of the framework is
demonstrated in the context of three disparate applications where data
re-weighting is warranted to satisfy side constraints on the optimization
problem at the heart of the statistical task: namely, portfolio allocation,
semi-parametric inference for complex surveys, and ensuring algorithmic
fairness in machine learning algorithms.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12466" title="Abstract">arXiv:2310.12466</a> (cross-list from math.NT) [<a href="/pdf/2310.12466" title="Download PDF">pdf</a>, <a href="/ps/2310.12466" title="Download PostScript">ps</a>, <a href="/format/2310.12466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher Level Completeness for Permutation Polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rajagopal%2C+S">S. Rajagopal</a>, 
<a href="/search/math?searchtype=author&query=Vanchinathan%2C+P">P. Vanchinathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages. Comments Welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Cryptography and Security (cs.CR); Combinatorics (math.CO)

</div>
<p class="mathjax">Generalising the concept of a complete permutation polynomial over a finite
field, we define completness to level $k$ for $k\ge1$ in fields of odd
characteristic. We construct two families of polynomials that satisfy the
condition of high level completeness for all finite fields, and two more
families complete to the maximum level a possible for large collection of
finite fields.
<br />Under the binary operation of composition of functions one family of
polynomials is an abelian group isomorphic to the additive group, while the
other is isomorphic to the multiplicative group.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12477" title="Abstract">arXiv:2310.12477</a> (cross-list from eess.AS) [<a href="/pdf/2310.12477" title="Download PDF">pdf</a>, <a href="/format/2310.12477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Exploration of In-Context Learning for Speech Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hsu%2C+M">Ming-Hao Hsu</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Ever since the development of GPT-3 in the natural language processing (NLP)
field, in-context learning (ICL) has played an important role in utilizing
large language models (LLMs). By presenting the LM utterance-label
demonstrations at the input, the LM can accomplish few-shot learning without
relying on gradient descent or requiring explicit modification of its
parameters. This enables the LM to learn and adapt in a black-box manner.
Despite the success of ICL in NLP, little work is exploring the possibility of
ICL in speech processing. This study proposes the first exploration of ICL with
a speech LM without text supervision. We first show that the current speech LM
does not have the ICL capability. With the proposed warmup training, the speech
LM can, therefore, perform ICL on unseen tasks. In this work, we verify the
feasibility of ICL for speech LM on speech classification tasks.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12500" title="Abstract">arXiv:2310.12500</a> (cross-list from q-fin.PR) [<a href="/pdf/2310.12500" title="Download PDF">pdf</a>, <a href="/format/2310.12500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> American Option Pricing using Self-Attention GRU and Shapley Value  Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Shen%2C+Y">Yanhui Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Pricing of Securities (q-fin.PR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Options, serving as a crucial financial instrument, are used by investors to
manage and mitigate their investment risks within the securities market.
Precisely predicting the present price of an option enables investors to make
informed and efficient decisions. In this paper, we propose a machine learning
method for forecasting the prices of SPY (ETF) option based on gated recurrent
unit (GRU) and self-attention mechanism. We first partitioned the raw dataset
into 15 subsets according to moneyness and days to maturity criteria. For each
subset, we matched the corresponding U.S. government bond rates and Implied
Volatility Indices. This segmentation allows for a more insightful exploration
of the impacts of risk-free rates and underlying volatility on option pricing.
Next, we built four different machine learning models, including multilayer
perceptron (MLP), long short-term memory (LSTM), self-attention LSTM, and
self-attention GRU in comparison to the traditional binomial model. The
empirical result shows that self-attention GRU with historical data outperforms
other models due to its ability to capture complex temporal dependencies and
leverage the contextual information embedded in the historical data. Finally,
in order to unveil the "black box" of artificial intelligence, we employed the
SHapley Additive exPlanations (SHAP) method to interpret and analyze the
prediction results of the self-attention GRU model with historical data. This
provides insights into the significance and contributions of different input
features on the pricing of American-style options.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12528" title="Abstract">arXiv:2310.12528</a> (cross-list from astro-ph.IM) [<a href="/pdf/2310.12528" title="Download PDF">pdf</a>, <a href="/format/2310.12528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing Impactful Machine Learning Research for Astronomy: Best  Practices for Researchers and Reviewers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Huppenkothen%2C+D">D. Huppenkothen</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ntampaka%2C+M">M. Ntampaka</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ho%2C+M">M. Ho</a>, 
<a href="/search/astro-ph?searchtype=author&query=Fouesneau%2C+M">M. Fouesneau</a>, 
<a href="/search/astro-ph?searchtype=author&query=Nord%2C+B">B. Nord</a>, 
<a href="/search/astro-ph?searchtype=author&query=Peek%2C+J+E+G">J. E. G. Peek</a>, 
<a href="/search/astro-ph?searchtype=author&query=Walmsley%2C+M">M. Walmsley</a>, 
<a href="/search/astro-ph?searchtype=author&query=Wu%2C+J+F">J. F. Wu</a>, 
<a href="/search/astro-ph?searchtype=author&query=Avestruz%2C+C">C. Avestruz</a>, 
<a href="/search/astro-ph?searchtype=author&query=Buck%2C+T">T. Buck</a>, 
<a href="/search/astro-ph?searchtype=author&query=Brescia%2C+M">M. Brescia</a>, 
<a href="/search/astro-ph?searchtype=author&query=Finkbeiner%2C+D+P">D. P. Finkbeiner</a>, 
<a href="/search/astro-ph?searchtype=author&query=Goulding%2C+A+D">A. D. Goulding</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kacprzak%2C+T">T. Kacprzak</a>, 
<a href="/search/astro-ph?searchtype=author&query=Melchior%2C+P">P. Melchior</a>, 
<a href="/search/astro-ph?searchtype=author&query=Pasquato%2C+M">M. Pasquato</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ramachandra%2C+N">N. Ramachandra</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ting%2C+Y+-">Y.-S. Ting</a>, 
<a href="/search/astro-ph?searchtype=author&query=van+de+Ven%2C+G">G. van de Ven</a>, 
<a href="/search/astro-ph?searchtype=author&query=Villar%2C+S">S. Villar</a>, 
<a href="/search/astro-ph?searchtype=author&query=Villar%2C+V+A">V.A. Villar</a>, 
<a href="/search/astro-ph?searchtype=author&query=Zinger%2C+E">E. Zinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures; submitted to the Bulletin of the American Astronomical Society
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning has rapidly become a tool of choice for the astronomical
community. It is being applied across a wide range of wavelengths and problems,
from the classification of transients to neural network emulators of
cosmological simulations, and is shifting paradigms about how we generate and
report scientific results. At the same time, this class of method comes with
its own set of best practices, challenges, and drawbacks, which, at present,
are often reported on incompletely in the astrophysical literature. With this
paper, we aim to provide a primer to the astronomical community, including
authors, reviewers, and editors, on how to implement machine learning models
and report their results in a way that ensures the accuracy of the results,
reproducibility of the findings, and usefulness of the method.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12533" title="Abstract">arXiv:2310.12533</a> (cross-list from quant-ph) [<a href="/pdf/2310.12533" title="Download PDF">pdf</a>, <a href="/format/2310.12533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Private Function Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cao%2C+Z">Zhu Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> New Journal of Physics 25, 103027 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Private function evaluation is a task that aims to obtain the output of a
function while keeping the function secret. So far its quantum analogue has not
yet been articulated. In this study, we initiate the study of quantum private
function evaluation, the quantum analogue of classical private function
evaluation. We give a formal definition of quantum private function evaluation
and present two schemes together with their security proofs. We then give an
experimental demonstration of the scheme. Finally we apply quantum private
function evaluation to quantum copy protection to illustrate its usage.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12544" title="Abstract">arXiv:2310.12544</a> (cross-list from stat.ML) [<a href="/pdf/2310.12544" title="Download PDF">pdf</a>, <a href="/format/2310.12544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Likelihood Approximation for Integer Valued Time Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=O%27Loughlin%2C+L">Luke O&#x27;Loughlin</a>, 
<a href="/search/stat?searchtype=author&query=Maclean%2C+J">John Maclean</a>, 
<a href="/search/stat?searchtype=author&query=Black%2C+A">Andrew Black</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Stochastic processes defined on integer valued state spaces are popular
within the physical and biological sciences. These models are necessary for
capturing the dynamics of small systems where the individual nature of the
populations cannot be ignored and stochastic effects are important. The
inference of the parameters of such models, from time series data, is difficult
due to intractability of the likelihood; current methods, based on simulations
of the underlying model, can be so computationally expensive as to be
prohibitive. In this paper we construct a neural likelihood approximation for
integer valued time series data using causal convolutions, which allows us to
evaluate the likelihood of the whole time series in parallel. We demonstrate
our method by performing inference on a number of ecological and
epidemiological models, showing that we can accurately approximate the true
posterior while achieving significant computational speed ups in situations
where current methods struggle.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12563" title="Abstract">arXiv:2310.12563</a> (cross-list from stat.ML) [<a href="/pdf/2310.12563" title="Download PDF">pdf</a>, <a href="/format/2310.12563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate information maximization for bandit games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Barbier--Chebbah%2C+A">Alex Barbier--Chebbah</a> (IP, CNRS, UPCit&#xe9;), 
<a href="/search/stat?searchtype=author&query=Vestergaard%2C+C+L">Christian L. Vestergaard</a> (IP, CNRS, UPCit&#xe9;), 
<a href="/search/stat?searchtype=author&query=Masson%2C+J">Jean-Baptiste Masson</a> (IP, CNRS, UPCit&#xe9;), 
<a href="/search/stat?searchtype=author&query=Boursier%2C+E">Etienne Boursier</a> (CELESTE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Entropy maximization and free energy minimization are general physical
principles for modeling the dynamics of various physical systems. Notable
examples include modeling decision-making within the brain using the
free-energy principle, optimizing the accuracy-complexity trade-off when
accessing hidden variables with the information bottleneck principle (Tishby et
al., 2000), and navigation in random environments using information
maximization (Vergassola et al., 2007). Built on this principle, we propose a
new class of bandit algorithms that maximize an approximation to the
information of a key variable within the system. To this end, we develop an
approximated analytical physics-based representation of an entropy to forecast
the information gain of each action and greedily choose the one with the
largest information gain. This method yields strong performances in classical
bandit settings. Motivated by its empirical success, we prove its asymptotic
optimality for the two-armed bandit problem with Gaussian rewards. Owing to its
ability to encompass the system's properties in a global physical functional,
this approach can be efficiently adapted to more complex bandit settings,
calling for further investigation of information maximization approaches for
multi-armed bandit problems.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12570" title="Abstract">arXiv:2310.12570</a> (cross-list from eess.IV) [<a href="/pdf/2310.12570" title="Download PDF">pdf</a>, <a href="/format/2310.12570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DA-TransUNet: Integrating Spatial and Channel Dual Attention with  Transformer U-Net for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guanqun Sun</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+Y">Yizhi Pan</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+W">Weikun Kong</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Z">Zichang Xu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+J">Jianhua Ma</a>, 
<a href="/search/eess?searchtype=author&query=Racharak%2C+T">Teeradaj Racharak</a>, 
<a href="/search/eess?searchtype=author&query=Nguyen%2C+L">Le-Minh Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Great progress has been made in automatic medical image segmentation due to
powerful deep representation learning. The influence of transformer has led to
research into its variants, and large-scale replacement of traditional CNN
modules. However, such trend often overlooks the intrinsic feature extraction
capabilities of the transformer and potential refinements to both the model and
the transformer module through minor adjustments. This study proposes a novel
deep medical image segmentation framework, called DA-TransUNet, aiming to
introduce the Transformer and dual attention block into the encoder and decoder
of the traditional U-shaped architecture. Unlike prior transformer-based
solutions, our DA-TransUNet utilizes attention mechanism of transformer and
multifaceted feature extraction of DA-Block, which can efficiently combine
global, local, and multi-scale features to enhance medical image segmentation.
Meanwhile, experimental results show that a dual attention block is added
before the Transformer layer to facilitate feature extraction in the U-net
structure. Furthermore, incorporating dual attention blocks in skip connections
can enhance feature transfer to the decoder, thereby improving image
segmentation performance. Experimental results across various benchmark of
medical image segmentation reveal that DA-TransUNet significantly outperforms
the state-of-the-art methods. The codes and parameters of our model will be
publicly available at https://github.com/SUN-1024/DA-TransUnet.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12574" title="Abstract">arXiv:2310.12574</a> (cross-list from eess.IV) [<a href="/pdf/2310.12574" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A reproducible 3D convolutional neural network with dual attention  module (3D-DAM) for Alzheimer&#x27;s disease classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hoang%2C+G+M">Gia Minh Hoang</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+Y">Youngjoo Lee</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+J+G">Jae Gwan Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Alzheimer's disease is one of the most common types of neurodegenerative
disease, characterized by the accumulation of amyloid-beta plaque and tau
tangles. Recently, deep learning approaches have shown promise in Alzheimer's
disease diagnosis. In this study, we propose a reproducible model that utilizes
a 3D convolutional neural network with a dual attention module for Alzheimer's
disease classification. We trained the model in the ADNI database and verified
the generalizability of our method in two independent datasets (AIBL and
OASIS1). Our method achieved state-of-the-art classification performance, with
an accuracy of 91.94% for MCI progression classification and 96.30% for
Alzheimer's disease classification on the ADNI dataset. Furthermore, the model
demonstrated good generalizability, achieving an accuracy of 86.37% on the AIBL
dataset and 83.42% on the OASIS1 dataset. These results indicate that our
proposed approach has competitive performance and generalizability when
compared to recent studies in the field.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12581" title="Abstract">arXiv:2310.12581</a> (cross-list from q-bio.PE) [<a href="/pdf/2310.12581" title="Download PDF">pdf</a>, <a href="/ps/2310.12581" title="Download PostScript">ps</a>, <a href="/format/2310.12581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolutionary stability of cooperation by the leading eight norms in  indirect reciprocity under noisy and private assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Fujimoto%2C+Y">Yuma Fujimoto</a>, 
<a href="/search/q-bio?searchtype=author&query=Ohtsuki%2C+H">Hisashi Ohtsuki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages &amp; 5 figures (main), 7 pages &amp; 2 figures (supplement)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Indirect reciprocity is a mechanism that explains large-scale cooperation in
human societies. In indirect reciprocity, an individual chooses whether or not
to cooperate with another based on reputation information, and others evaluate
the action as good or bad. Under what evaluation rule (called ``social norm'')
cooperation evolves has long been of central interest in the literature. It has
been reported that if individuals can share their evaluations (i.e., public
reputation), social norms called ``leading eight'' can be evolutionarily
stable. On the other hand, when they cannot share their evaluations (i.e.,
private assessment), the evolutionary stability of cooperation is still in
question. To tackle this problem, we create a novel method to analyze the
reputation structure in the population under private assessment. Specifically,
we characterize each individual by two variables, ``goodness'' (what proportion
of the population considers the individual as good) and ``self-reputation''
(whether an individual thinks of him/herself as good or bad), and analyze the
stochastic process of how these two variables change over time. We discuss
evolutionary stability of each of the leading eight social norms by studying
the robustness against invasions of unconditional cooperators and defectors. We
identify key pivots in those social norms for establishing a high level of
cooperation or stable cooperation against mutants. Our finding gives an insight
into how human cooperation is established in a real-world society.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12631" title="Abstract">arXiv:2310.12631</a> (cross-list from cond-mat.stat-mech) [<a href="/pdf/2310.12631" title="Download PDF">pdf</a>, <a href="/format/2310.12631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Renormalization Group of Disordered Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Bachtis%2C+D">Dimitrios Bachtis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); High Energy Physics - Lattice (hep-lat)

</div>
<p class="mathjax">We propose inverse renormalization group transformations to construct
approximate configurations for lattice volumes that have not yet been accessed
by supercomputers or large-scale simulations in the study of spin glasses.
Specifically, starting from lattices of volume $V=8^{3}$ in the case of the
three-dimensional Edwards-Anderson model we employ machine learning algorithms
to construct rescaled lattices up to $V'=128^{3}$, which we utilize to extract
two critical exponents. We conclude by discussing how to incorporate numerical
exactness within inverse renormalization group approaches of disordered
systems, thus opening up the opportunity to explore a sustainable and
energy-efficient generation of exact configurations for increasing lattice
volumes without the use of dedicated supercomputers.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12646" title="Abstract">arXiv:2310.12646</a> (cross-list from eess.IV) [<a href="/pdf/2310.12646" title="Download PDF">pdf</a>, <a href="/format/2310.12646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRUSTED: The Paired 3D Transabdominal Ultrasound and CT Human Data for  Kidney Segmentation and Registration Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ndzimbong%2C+W">William Ndzimbong</a>, 
<a href="/search/eess?searchtype=author&query=Fourniol%2C+C">Cyril Fourniol</a>, 
<a href="/search/eess?searchtype=author&query=Themyr%2C+L">Loic Themyr</a>, 
<a href="/search/eess?searchtype=author&query=Thome%2C+N">Nicolas Thome</a>, 
<a href="/search/eess?searchtype=author&query=Keeza%2C+Y">Yvonne Keeza</a>, 
<a href="/search/eess?searchtype=author&query=Sauer%2C+B">Beniot Sauer</a>, 
<a href="/search/eess?searchtype=author&query=Piechaud%2C+P">Pierre-Thierry Piechaud</a>, 
<a href="/search/eess?searchtype=author&query=Mejean%2C+A">Arnaud Mejean</a>, 
<a href="/search/eess?searchtype=author&query=Marescaux%2C+J">Jacques Marescaux</a>, 
<a href="/search/eess?searchtype=author&query=George%2C+D">Daniel George</a>, 
<a href="/search/eess?searchtype=author&query=Mutter%2C+D">Didier Mutter</a>, 
<a href="/search/eess?searchtype=author&query=Hostettler%2C+A">Alexandre Hostettler</a>, 
<a href="/search/eess?searchtype=author&query=Collins%2C+T">Toby Collins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Alexandre Hostettler, and Toby Collins share last authorship
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Inter-modal image registration (IMIR) and image segmentation with abdominal
Ultrasound (US) data has many important clinical applications, including
image-guided surgery, automatic organ measurement and robotic navigation.
However, research is severely limited by the lack of public datasets. We
propose TRUSTED (the Tridimensional Renal Ultra Sound TomodEnsitometrie
Dataset), comprising paired transabdominal 3DUS and CT kidney images from 48
human patients (96 kidneys), including segmentation, and anatomical landmark
annotations by two experienced radiographers. Inter-rater segmentation
agreement was over 94 (Dice score), and gold-standard segmentations were
generated using the STAPLE algorithm. Seven anatomical landmarks were
annotated, important for IMIR systems development and evaluation. To validate
the dataset's utility, 5 competitive Deep Learning models for automatic kidney
segmentation were benchmarked, yielding average DICE scores from 83.2% to 89.1%
for CT, and 61.9% to 79.4% for US images. Three IMIR methods were benchmarked,
and Coherent Point Drift performed best with an average Target Registration
Error of 4.53mm. The TRUSTED dataset may be used freely researchers to develop
and validate new segmentation and IMIR methods.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12667" title="Abstract">arXiv:2310.12667</a> (cross-list from stat.ML) [<a href="/pdf/2310.12667" title="Download PDF">pdf</a>, <a href="/format/2310.12667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning  Energy-Based Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Karimi%2C+B">Belhal Karimi</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+J">Jianwen Xie</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+P">Ping Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1207.5938">arXiv:1207.5938</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose in this paper, STANLEY, a STochastic gradient ANisotropic LangEvin
dYnamics, for sampling high dimensional data. With the growing efficacy and
potential of Energy-Based modeling, also known as non-normalized probabilistic
modeling, for modeling a generative process of different natures of high
dimensional data observations, we present an end-to-end learning algorithm for
Energy-Based models (EBM) with the purpose of improving the quality of the
resulting sampled data points. While the unknown normalizing constant of EBMs
makes the training procedure intractable, resorting to Markov Chain Monte Carlo
(MCMC) is in general a viable option. Realizing what MCMC entails for the EBM
training, we propose in this paper, a novel high dimensional sampling method,
based on an anisotropic stepsize and a gradient-informed covariance matrix,
embedded into a discretized Langevin diffusion. We motivate the necessity for
an anisotropic update of the negative samples in the Markov Chain by the
nonlinearity of the backbone of the EBM, here a Convolutional Neural Network.
Our resulting method, namely STANLEY, is an optimization algorithm for training
Energy-Based models via our newly introduced MCMC method. We provide a
theoretical understanding of our sampling scheme by proving that the sampler
leads to a geometrically uniformly ergodic Markov Chain. Several image
generation experiments are provided in our paper to show the effectiveness of
our method.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12682" title="Abstract">arXiv:2310.12682</a> (cross-list from quant-ph) [<a href="/pdf/2310.12682" title="Download PDF">pdf</a>, <a href="/format/2310.12682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correcting phenomenological quantum noise via belief propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kuo%2C+K">Kao-Yueh Kuo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lai%2C+C">Ching-Yi Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Quantum stabilizer codes often face the challenge of syndrome errors due to
error-prone measurements. To address this issue, multiple rounds of syndrome
extraction are typically employed to obtain reliable error syndromes. In this
paper, we consider phenomenological decoding problems, where data qubit errors
may occur between two syndrome extractions, and each syndrome measurement can
be faulty. To handle these diverse error sources, we define a generalized check
matrix over mixed quaternary and binary alphabets to characterize their error
syndromes. This generalized check matrix leads to the creation of a Tanner
graph comprising quaternary and binary variable nodes, which facilitates the
development of belief propagation (BP) decoding algorithms to tackle
phenomenological errors. Importantly, our BP decoders are applicable to general
sparse quantum codes. Through simulations of quantum memory protected by
rotated toric codes, we demonstrates an error threshold of 3.3% in the
phenomenological noise model. Additionally, we propose a method to construct
effective redundant stabilizer checks for single-shot error correction.
Simulations show that BP decoding performs exceptionally well, even when the
syndrome error rate greatly exceeds the data error rate.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12733" title="Abstract">arXiv:2310.12733</a> (cross-list from eess.IV) [<a href="/pdf/2310.12733" title="Download PDF">pdf</a>, <a href="/format/2310.12733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Motion-Aware and Spatial-Temporal-Channel Contextual Coding  Network for Learned Video Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yiming Wang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Q">Qian Huang</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+B">Bin Tang</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+H">Huashan Sun</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12pages,12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, learned video compression has achieved exciting performance.
Following the traditional hybrid prediction coding framework, most learned
methods generally adopt the motion estimation motion compensation (MEMC) method
to remove inter-frame redundancy. However, inaccurate motion vector (MV)
usually lead to the distortion of reconstructed frame. In addition, most
approaches ignore the spatial and channel redundancy. To solve above problems,
we propose a motion-aware and spatial-temporal-channel contextual coding based
video compression network (MASTC-VC), which learns the latent representation
and uses variational autoencoders (VAEs) to capture the characteristics of
intra-frame pixels and inter-frame motion. Specifically, we design a multiscale
motion-aware module (MS-MAM) to estimate spatial-temporal-channel consistent
motion vector by utilizing the multiscale motion prediction information in a
coarse-to-fine way. On the top of it, we further propose a
spatial-temporal-channel contextual module (STCCM), which explores the
correlation of latent representation to reduce the bit consumption from
spatial, temporal and channel aspects respectively. Comprehensive experiments
show that our proposed MASTC-VC is surprior to previous state-of-the-art (SOTA)
methods on three public benchmark datasets. More specifically, our method
brings average 10.15\% BD-rate savings against H.265/HEVC (HM-16.20) in PSNR
metric and average 23.93\% BD-rate savings against H.266/VVC (VTM-13.2) in
MS-SSIM metric.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12743" title="Abstract">arXiv:2310.12743</a> (cross-list from stat.ML) [<a href="/pdf/2310.12743" title="Download PDF">pdf</a>, <a href="/format/2310.12743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Canonical normalizing flows for manifold learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Flouris%2C+K">Kyriakos Flouris</a>, 
<a href="/search/stat?searchtype=author&query=Konukoglu%2C+E">Ender Konukoglu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> https://neurips.cc/virtual/2023/poster/69924
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Differential Geometry (math.DG); Computation (stat.CO)

</div>
<p class="mathjax">Manifold learning flows are a class of generative modelling techniques that
assume a low-dimensional manifold description of the data. The embedding of
such manifold into the high-dimensional space of the data is achieved via
learnable invertible transformations. Therefore, once the manifold is properly
aligned via a reconstruction loss, the probability density is tractable on the
manifold and maximum likelihood can be used optimize the network parameters.
Naturally, the lower-dimensional representation of the data requires an
injective-mapping. Recent approaches were able to enforce that density aligns
with the modelled manifold, while efficiently calculating the density
volume-change term when embedding to the higher-dimensional space. However,
unless the injective-mapping is analytically predefined, the learned manifold
is not necessarily an efficient representation of the data. Namely, the latent
dimensions of such models frequently learn an entangled intrinsic basis with
degenerate information being stored in each dimension. Alternatively, if a
locally orthogonal and/or sparse basis is to be learned, here coined canonical
intrinsic basis, it can serve in learning a more compact latent space
representation. Towards this end, we propose a canonical manifold learning flow
method, where a novel optimization objective enforces the transformation matrix
to have few prominent and orthogonal basis functions. Canonical manifold flow
yields a more efficient use of the latent space, automatically generating fewer
prominent and distinct dimensions to represent data, and consequently a better
approximation of target distributions than other manifold flow methods in most
experiments we conducted, resulting in lower FID scores.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12768" title="Abstract">arXiv:2310.12768</a> (cross-list from eess.SP) [<a href="/pdf/2310.12768" title="Download PDF">pdf</a>, <a href="/format/2310.12768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemantIC: Semantic Interference Cancellation Towards 6G Wireless  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lin%2C+W">Wensheng Lin</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+Y">Yuna Yan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lixin Li</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+Z">Zhu Han</a>, 
<a href="/search/eess?searchtype=author&query=Matsumoto%2C+T">Tad Matsumoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">This letter proposes a novel anti-interference technique, semantic
interference cancellation (SemantIC), for enhancing information quality towards
the sixth-generation (6G) wireless networks. SemantIC only requires the
receiver to concatenate the channel decoder with a semantic auto-encoder. This
constructs a turbo loop which iteratively and alternately eliminates noise in
the signal domain and the semantic domain. From the viewpoint of network
information theory, the neural network of the semantic auto-encoder stores side
information by training, and provides side information in iterative decoding,
as an implementation of the Wyner-Ziv theorem. Simulation results verify the
performance improvement by SemantIC without extra channel resource cost.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12781" title="Abstract">arXiv:2310.12781</a> (cross-list from stat.ML) [<a href="/pdf/2310.12781" title="Download PDF">pdf</a>, <a href="/format/2310.12781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Density Estimations from Privacy-Protected Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xiong%2C+X">Xiaofei Xiong</a>, 
<a href="/search/stat?searchtype=author&query=Ju%2C+N+P">Nianqiao P. Ju</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+S">Sanguo Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Many modern statistical analysis and machine learning applications require
training models on sensitive user data. Differential privacy provides a formal
guarantee that individual-level information about users does not leak. In this
framework, randomized algorithms inject calibrated noise into the confidential
data, resulting in privacy-protected datasets or queries. However, restricting
access to only the privatized data during statistical analysis makes it
computationally challenging to perform valid inferences on parameters
underlying the confidential data. In this work, we propose simulation-based
inference methods from privacy-protected datasets. Specifically, we use neural
conditional density estimators as a flexible family of distributions to
approximate the posterior distribution of model parameters given the observed
private query results. We illustrate our methods on discrete time-series data
under an infectious disease model and on ordinary linear regression models.
Illustrating the privacy-utility trade-off, our experiments and analysis
demonstrate the necessity and feasibility of designing valid statistical
inference procedures to correct for biases introduced by the privacy-protection
mechanisms.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12802" title="Abstract">arXiv:2310.12802</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.12802" title="Download PDF">pdf</a>, <a href="/format/2310.12802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An effective theory of collective deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Arola-Fern%C3%A1ndez%2C+L">Llu&#xed;s Arola-Fern&#xe1;ndez</a>, 
<a href="/search/physics?searchtype=author&query=Lacasa%2C+L">Lucas Lacasa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">Unraveling the emergence of collective learning in systems of coupled
artificial neural networks is an endeavor with broader implications for
physics, machine learning, neuroscience and society. Here we introduce a
minimal model that condenses several recent decentralized algorithms by
considering a competition between two terms: the local learning dynamics in the
parameters of each neural network unit, and a diffusive coupling among units
that tends to homogenize the parameters of the ensemble. We derive the
coarse-grained behavior of our model via an effective theory for linear
networks that we show is analogous to a deformed Ginzburg-Landau model with
quenched disorder. This framework predicts (depth-dependent)
disorder-order-disorder phase transitions in the parameters' solutions that
reveal the onset of a collective learning phase, along with a depth-induced
delay of the critical point and a robust shape of the microscopic learning
path. We validate our theory in realistic ensembles of coupled nonlinear
networks trained in the MNIST dataset under privacy constraints. Interestingly,
experiments confirm that individual networks -- trained only with private data
-- can fully generalize to unseen data classes when the collective learning
phase emerges. Our work elucidates the physics of collective learning and
contributes to the mechanistic interpretability of deep learning in
decentralized settings.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12804" title="Abstract">arXiv:2310.12804</a> (cross-list from hep-ex) [<a href="/pdf/2310.12804" title="Download PDF">pdf</a>, <a href="/format/2310.12804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Vertex Fitting for Jet Flavour Tagging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Smith%2C+R+E+C">Rachel E. C. Smith</a>, 
<a href="/search/hep-ex?searchtype=author&query=Ochoa%2C+I">In&#xea;s Ochoa</a>, 
<a href="/search/hep-ex?searchtype=author&query=In%C3%A1cio%2C+R">R&#xfa;ben In&#xe1;cio</a>, 
<a href="/search/hep-ex?searchtype=author&query=Shoemaker%2C+J">Jonathan Shoemaker</a>, 
<a href="/search/hep-ex?searchtype=author&query=Kagan%2C+M">Michael Kagan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Machine Learning (cs.LG); High Energy Physics - Phenomenology (hep-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">We propose a differentiable vertex fitting algorithm that can be used for
secondary vertex fitting, and that can be seamlessly integrated into neural
networks for jet flavour tagging. Vertex fitting is formulated as an
optimization problem where gradients of the optimized solution vertex are
defined through implicit differentiation and can be passed to upstream or
downstream neural network components for network training. More broadly, this
is an application of differentiable programming to integrate physics knowledge
into neural network models in high energy physics. We demonstrate how
differentiable secondary vertex fitting can be integrated into larger
transformer-based models for flavour tagging and improve heavy flavour jet
classification.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12806" title="Abstract">arXiv:2310.12806</a> (cross-list from stat.ML) [<a href="/pdf/2310.12806" title="Download PDF">pdf</a>, <a href="/format/2310.12806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCSI -- An improved measure of cluster separability based on separation  and connectedness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gauss%2C+J">Jana Gauss</a>, 
<a href="/search/stat?searchtype=author&query=Scheipl%2C+F">Fabian Scheipl</a>, 
<a href="/search/stat?searchtype=author&query=Herrmann%2C+M">Moritz Herrmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Whether class labels in a given data set correspond to meaningful clusters is
crucial for the evaluation of clustering algorithms using real-world data sets.
This property can be quantified by separability measures. A review of the
existing literature shows that neither classification-based complexity measures
nor cluster validity indices (CVIs) adequately incorporate the central aspects
of separability for density-based clustering: between-class separation and
within-class connectedness. A newly developed measure (density cluster
separability index, DCSI) aims to quantify these two characteristics and can
also be used as a CVI. Extensive experiments on synthetic data indicate that
DCSI correlates strongly with the performance of DBSCAN measured via the
adjusted rand index (ARI) but lacks robustness when it comes to multi-class
data sets with overlapping classes that are ill-suited for density-based hard
clustering. Detailed evaluation on frequently used real-world data sets shows
that DCSI can correctly identify touching or overlapping classes that do not
form meaningful clusters.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12812" title="Abstract">arXiv:2310.12812</a> (cross-list from math.CO) [<a href="/pdf/2310.12812" title="Download PDF">pdf</a>, <a href="/format/2310.12812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systems of Discrete Differential Equations, Constructive Algebraicity of  the Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Notarantonio%2C+H">Hadrien Notarantonio</a>, 
<a href="/search/math?searchtype=author&query=Yurkevich%2C+S">Sergey Yurkevich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">In this article, we study systems of $n \geq 1$, not necessarily linear,
discrete differential equations (DDEs) of order $k \geq 1$ with one catalytic
variable. We provide a constructive and elementary proof of algebraicity of the
solutions of such equations. This part of the present article can be seen as a
generalization of the pioneering work by Bousquet-M\'elou and Jehanne~(2006)
who settled down the case $n=1$. Moreover, we obtain effective bounds for the
algebraicity degrees of the solutions and provide an algorithm for computing
annihilating polynomials of the algebraic series. Finally, we carry out a first
analysis in the direction of effectivity for solving systems of DDEs in view of
practical applications.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12822" title="Abstract">arXiv:2310.12822</a> (cross-list from stat.ML) [<a href="/pdf/2310.12822" title="Download PDF">pdf</a>, <a href="/format/2310.12822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating collective counterfactual explanations in score-based  classification via mathematical optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Carrizosa%2C+E">Emilio Carrizosa</a>, 
<a href="/search/stat?searchtype=author&query=Ram%C3%ADrez-Ayerbe%2C+J">Jasone Ram&#xed;rez-Ayerbe</a>, 
<a href="/search/stat?searchtype=author&query=Morales%2C+D+R">Dolores Romero Morales</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This research has been funded in part by research projects EC H2020 MSCA RISE NeEDS (Grant agreement ID: 822214), FQM-329, P18-FR-2369 and US-1381178 (Junta de Andaluc\'{\i}a, Spain), and PID2019-110886RB-I00 and PID2022-137818OB-I00 (Ministerio de Ciencia, Innovaci\'on y Universidades, Spain). This support is gratefully acknowledged
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to the increasing use of Machine Learning models in high stakes decision
making settings, it has become increasingly important to have tools to
understand how models arrive at decisions. Assuming a trained Supervised
Classification model, explanations can be obtained via counterfactual analysis:
a counterfactual explanation of an instance indicates how this instance should
be minimally modified so that the perturbed instance is classified in the
desired class by the Machine Learning classification model. Most of the
Counterfactual Analysis literature focuses on the single-instance
single-counterfactual setting, in which the analysis is done for one single
instance to provide one single explanation. Taking a stakeholder's perspective,
in this paper we introduce the so-called collective counterfactual
explanations. By means of novel Mathematical Optimization models, we provide a
counterfactual explanation for each instance in a group of interest, so that
the total cost of the perturbations is minimized under some linking
constraints. Making the process of constructing counterfactuals collective
instead of individual enables us to detect the features that are critical to
the entire dataset to have the individuals classified in the desired class. Our
methodology allows for some instances to be treated individually, performing
the collective counterfactual analysis for a fraction of records of the group
of interest. This way, outliers are identified and handled appropriately. Under
some assumptions on the classifier and the space in which counterfactuals are
sought, finding collective counterfactuals is reduced to solving a convex
quadratic linearly constrained mixed integer optimization problem, which, for
datasets of moderate size, can be solved to optimality using existing solvers.
The performance of our approach is illustrated on real-world datasets,
demonstrating its usefulness.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12842" title="Abstract">arXiv:2310.12842</a> (cross-list from stat.ML) [<a href="/pdf/2310.12842" title="Download PDF">pdf</a>, <a href="/format/2310.12842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-agnostic variable importance for predictive uncertainty: an  entropy-based approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wood%2C+D">Danny Wood</a>, 
<a href="/search/stat?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/stat?searchtype=author&query=Benatan%2C+M">Matt Benatan</a>, 
<a href="/search/stat?searchtype=author&query=Allmendinger%2C+R">Richard Allmendinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In order to trust the predictions of a machine learning algorithm, it is
necessary to understand the factors that contribute to those predictions. In
the case of probabilistic and uncertainty-aware models, it is necessary to
understand not only the reasons for the predictions themselves, but also the
model's level of confidence in those predictions. In this paper, we show how
existing methods in explainability can be extended to uncertainty-aware models
and how such extensions can be used to understand the sources of uncertainty in
a model's predictive distribution. In particular, by adapting permutation
feature importance, partial dependence plots, and individual conditional
expectation plots, we demonstrate that novel insights into model behaviour may
be obtained and that these methods can be used to measure the impact of
features on both the entropy of the predictive distribution and the
log-likelihood of the ground truth labels under that distribution. With
experiments using both synthetic and real-world data, we demonstrate the
utility of these approaches in understanding both the sources of uncertainty
and their impact on model performance.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12852" title="Abstract">arXiv:2310.12852</a> (cross-list from quant-ph) [<a href="/pdf/2310.12852" title="Download PDF">pdf</a>, <a href="/format/2310.12852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Annealing Solutions for the Closest String Problem with D-Wave  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Dissanayake%2C+C">Chandeepa Dissanayake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, submitted to IEEE Transactions on Computers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Optimization and Control (math.OC)

</div>
<p class="mathjax">The Closest String Problem is an NP-complete problem which appears more
commonly in bioinformatics and coding theory. Less surprisingly, classical
approaches have been pursued with two prominent algorithms being the genetic
algorithm and simulated annealing. Latest improvements to quantum computing
devices with a specialization in optimization tasks such as DWave systems,
suggest that an attempt to embed the problem in a model accepted by such
systems is worthwhile. In this work, two QUBO formulations have been proposed,
with one being a slight modification over the other. Subsequently, an
evaluation based on a few simple test cases had been carried out on both
formulations. In this regard, the D-Wave annealers have been used, while
providing guidelines for optimality on certain platform-specific concerns. For
evaluation purposes, a metric termed Occurrence Ratio (OR) has been defined.
With minimal hyperparameter tuning, the expected solutions were obtained for
every test case and the optimality was guaranteed. To address practical and
implementation issues, an inherent decomposition strategy based on the
possibility of having substrings has been elucidated to accommodate the
restricted qubit count. Conclusively, the need for further investigation on
tuning the hyperparameters is emphasized.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12866" title="Abstract">arXiv:2310.12866</a> (cross-list from eess.IV) [<a href="/pdf/2310.12866" title="Download PDF">pdf</a>, <a href="/format/2310.12866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Ovarian Cancer Treatment Response in Histopathology using  Hierarchical Vision Transformers and Multiple Instance Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Breen%2C+J">Jack Breen</a>, 
<a href="/search/eess?searchtype=author&query=Allen%2C+K">Katie Allen</a>, 
<a href="/search/eess?searchtype=author&query=Zucker%2C+K">Kieran Zucker</a>, 
<a href="/search/eess?searchtype=author&query=Hall%2C+G">Geoff Hall</a>, 
<a href="/search/eess?searchtype=author&query=Ravikumar%2C+N">Nishant Ravikumar</a>, 
<a href="/search/eess?searchtype=author&query=Orsi%2C+N+M">Nicolas M. Orsi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submission to ATEC23 challenge at MICCAI 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">For many patients, current ovarian cancer treatments offer limited clinical
benefit. For some therapies, it is not possible to predict patients' responses,
potentially exposing them to the adverse effects of treatment without any
therapeutic benefit. As part of the automated prediction of treatment
effectiveness in ovarian cancer using histopathological images (ATEC23)
challenge, we evaluated the effectiveness of deep learning to predict whether a
course of treatment including the antiangiogenic drug bevacizumab could
contribute to remission or prevent disease progression for at least 6 months in
a set of 282 histopathology whole slide images (WSIs) from 78 ovarian cancer
patients. Our approach used a pretrained Hierarchical Image Pyramid Transformer
(HIPT) to extract region-level features and an attention-based multiple
instance learning (ABMIL) model to aggregate features and classify whole
slides. The optimal HIPT-ABMIL model had an internal balanced accuracy of 60.2%
+- 2.9% and an AUC of 0.646 +- 0.033. Histopathology-specific model pretraining
was found to be beneficial to classification performance, though hierarchical
transformers were not, with a ResNet feature extractor achieving similar
performance. Due to the dataset being small and highly heterogeneous,
performance was variable across 5-fold cross-validation folds, and there were
some extreme differences between validation and test set performance within
folds. The model did not generalise well to tissue microarrays, with accuracy
worse than random chance. It is not yet clear whether ovarian cancer WSIs
contain information that can be used to accurately predict treatment response,
with further validation using larger, higher-quality datasets required.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12871" title="Abstract">arXiv:2310.12871</a> (cross-list from stat.AP) [<a href="/pdf/2310.12871" title="Download PDF">pdf</a>, <a href="/format/2310.12871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The origins of unpredictability in life trajectory prediction tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lundberg%2C+I">Ian Lundberg</a>, 
<a href="/search/stat?searchtype=author&query=Brown-Weinstock%2C+R">Rachel Brown-Weinstock</a>, 
<a href="/search/stat?searchtype=author&query=Clampet-Lundquist%2C+S">Susan Clampet-Lundquist</a>, 
<a href="/search/stat?searchtype=author&query=Pachman%2C+S">Sarah Pachman</a>, 
<a href="/search/stat?searchtype=author&query=Nelson%2C+T+J">Timothy J. Nelson</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+V">Vicki Yang</a>, 
<a href="/search/stat?searchtype=author&query=Edin%2C+K">Kathryn Edin</a>, 
<a href="/search/stat?searchtype=author&query=Salganik%2C+M+J">Matthew J. Salganik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Why are life trajectories difficult to predict? We investigated this question
through in-depth qualitative interviews with 40 families sampled from a
multi-decade longitudinal study. Our sampling and interviewing process were
informed by the earlier efforts of hundreds of researchers to predict life
outcomes for participants in this study. The qualitative evidence we uncovered
in these interviews combined with a well-known mathematical decomposition of
prediction error helps us identify some origins of unpredictability and create
a new conceptual framework. Our specific evidence and our more general
framework suggest that unpredictability should be expected in many life
trajectory prediction tasks, even in the presence of complex algorithms and
large datasets. Our work also provides a foundation for future empirical and
theoretical work on unpredictability in human lives.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12877" title="Abstract">arXiv:2310.12877</a> (cross-list from eess.IV) [<a href="/pdf/2310.12877" title="Download PDF">pdf</a>, <a href="/format/2310.12877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Assessment and Optimization of High Dynamic Range Image  Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cao%2C+P">Peibei Cao</a>, 
<a href="/search/eess?searchtype=author&query=Mantiuk%2C+R+K">Rafal K. Mantiuk</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+K">Kede Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">High dynamic range (HDR) imaging has gained increasing popularity for its
ability to faithfully reproduce the luminance levels in natural scenes.
Accordingly, HDR image quality assessment (IQA) is crucial but has been
superficially treated. The majority of existing IQA models are developed for
and calibrated against low dynamic range (LDR) images, which have been shown to
be poorly correlated with human perception of HDR image quality. In this work,
we propose a family of HDR IQA models by transferring the recent advances in
LDR IQA. The key step in our approach is to specify a simple inverse display
model that decomposes an HDR image to a set of LDR images with different
exposures, which will be assessed by existing LDR quality models. The local
quality scores of each exposure are then aggregated with the help of a simple
well-exposedness measure into a global quality score for each exposure, which
will be further weighted across exposures to obtain the overall quality score.
When assessing LDR images, the proposed HDR quality models reduce gracefully to
the original LDR ones with the same performance. Experiments on four
human-rated HDR image datasets demonstrate that our HDR quality models are
consistently better than existing IQA methods, including the HDR-VDP family.
Moreover, we demonstrate their strengths in perceptual optimization of HDR
novel view synthesis.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12893" title="Abstract">arXiv:2310.12893</a> (cross-list from quant-ph) [<a href="/pdf/2310.12893" title="Download PDF">pdf</a>, <a href="/format/2310.12893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind quantum machine learning with quantum bipartite correlator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+C">Changhao Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+B">Boning Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Amer%2C+O">Omar Amer</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shaydulin%2C+R">Ruslan Shaydulin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chakrabarti%2C+S">Shouvanik Chakrabarti</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+G">Guoqing Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+H">Haowei Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Schoch%2C+I">Isidor Schoch</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kumar%2C+N">Niraj Kumar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lim%2C+C">Charles Lim</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+J">Ju Li</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cappellaro%2C+P">Paola Cappellaro</a>, 
<a href="/search/quant-ph?searchtype=author&query=Pistoia%2C+M">Marco Pistoia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Distributed quantum computing is a promising computational paradigm for
performing computations that are beyond the reach of individual quantum
devices. Privacy in distributed quantum computing is critical for maintaining
confidentiality and protecting the data in the presence of untrusted computing
nodes. In this work, we introduce novel blind quantum machine learning
protocols based on the quantum bipartite correlator algorithm. Our protocols
have reduced communication overhead while preserving the privacy of data from
untrusted parties. We introduce robust algorithm-specific privacy-preserving
mechanisms with low computational overhead that do not require complex
cryptographic techniques. We then validate the effectiveness of the proposed
protocols through complexity and privacy analysis. Our findings pave the way
for advancements in distributed quantum computing, opening up new possibilities
for privacy-aware machine learning applications in the era of quantum
technologies.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12964" title="Abstract">arXiv:2310.12964</a> (cross-list from stat.ML) [<a href="/pdf/2310.12964" title="Download PDF">pdf</a>, <a href="/format/2310.12964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC Prediction Sets Under Label Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Si%2C+W">Wenwen Si</a>, 
<a href="/search/stat?searchtype=author&query=Park%2C+S">Sangdon Park</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+I">Insup Lee</a>, 
<a href="/search/stat?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>, 
<a href="/search/stat?searchtype=author&query=Bastani%2C+O">Osbert Bastani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Prediction sets capture uncertainty by predicting sets of labels rather than
individual labels, enabling downstream decisions to conservatively account for
all plausible outcomes. Conformal inference algorithms construct prediction
sets guaranteed to contain the true label with high probability. These
guarantees fail to hold in the face of distribution shift, which is precisely
when reliable uncertainty quantification can be most useful. We propose a novel
algorithm for constructing prediction sets with PAC guarantees in the label
shift setting. This method estimates the predicted probabilities of the classes
in a target domain, as well as the confusion matrix, then propagates
uncertainty in these estimates through a Gaussian elimination algorithm to
compute confidence intervals for importance weights. Finally, it uses these
intervals to construct prediction sets. We evaluate our approach on five
datasets: the CIFAR-10, ChestX-Ray and Entity-13 image datasets, the tabular
CDC Heart dataset, and the AGNews text dataset. Our algorithm satisfies the PAC
guarantee while producing smaller, more informative, prediction sets compared
to several baselines.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri, 20 Oct 23</h3>
<dl>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1604.03049" title="Abstract">arXiv:1604.03049</a> (replaced) [<a href="/pdf/1604.03049" title="Download PDF">pdf</a>, <a href="/ps/1604.03049" title="Download PostScript">ps</a>, <a href="/format/1604.03049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Channel Estimation for Millimeter-Wave Massive MIMO with Hybrid  Precoding over Frequency-Selective Fading Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+L">Linglong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaocheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, Zhen Gao, Chen Hu, Linglong Dai and Zhaocheng Wang, "Channel estimation for millimeter-wave massive MIMO with hybrid precoding over frequency-selective fading channels," IEEE Communications Letters, vol. 20, no. 6, pp. 1259-1262, June 2016
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1604.03695" title="Abstract">arXiv:1604.03695</a> (replaced) [<a href="/pdf/1604.03695" title="Download PDF">pdf</a>, <a href="/ps/1604.03695" title="Download PostScript">ps</a>, <a href="/format/1604.03695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatially Common Sparsity Based Adaptive Channel Estimation and Feedback  for FDD Massive MIMO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+L">Linglong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaocheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sheng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 13 figures. Zhen Gao, Linglong Dai, Zhaocheng Wang, and Sheng Chen, "Spatially common sparsity based adaptive channel estimation and feedback for FDD massive MIMO," IEEE Transactions on Signal Processing, vol. 63, no. 23, pp. 6169-6183, Dec. 2015
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Trans. Signal Process., vol. 63, no. 23, pp. 6169-6183, Dec.
  2015
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1811.11479" title="Abstract">arXiv:1811.11479</a> (replaced) [<a href="/pdf/1811.11479" title="Download PDF">pdf</a>, <a href="/format/1811.11479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient On-Device Machine Learning: Federated  Distillation and Augmentation under Non-IID Private Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+E">Eunjeong Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Seungeun Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyesung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jihong Park</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seong-Lyun Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> presented at the 32nd Conference on Neural Information Processing Systems (NIPS 2018), 2nd Workshop on Machine Learning on the Phone and other Consumer Devices (MLPCD 2), Montr\'eal, Canada
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1812.09561" title="Abstract">arXiv:1812.09561</a> (replaced) [<a href="/pdf/1812.09561" title="Download PDF">pdf</a>, <a href="/format/1812.09561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fair Division of Hereditary Set Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhentao Li</a>, 
<a href="/search/cs?searchtype=author&query=Vetta%2C+A">Adrian Vetta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 1 figure, full version of WINE 2018 submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2002.11064" title="Abstract">arXiv:2002.11064</a> (replaced) [<a href="/pdf/2002.11064" title="Download PDF">pdf</a>, <a href="/format/2002.11064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pricing ASICs for Cryptocurrency Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaish%2C+A">Aviv Yaish</a>, 
<a href="/search/cs?searchtype=author&query=Zohar%2C+A">Aviv Zohar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 8 figures, 3 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In 5th Conference on Advances in Financial Technologies (AFT 2023)
  (Vol. 282, pp. 2:1-2:25) Schloss Dagstuhl - Leibniz-Zentrum fur Informatik
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.09520" title="Abstract">arXiv:2005.09520</a> (replaced) [<a href="/pdf/2005.09520" title="Download PDF">pdf</a>, <a href="/format/2005.09520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choral: Object-Oriented Choreographic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giallorenzo%2C+S">Saverio Giallorenzo</a>, 
<a href="/search/cs?searchtype=author&query=Montesi%2C+F">Fabrizio Montesi</a>, 
<a href="/search/cs?searchtype=author&query=Peressotti%2C+M">Marco Peressotti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.13041" title="Abstract">arXiv:2011.13041</a> (replaced) [<a href="/pdf/2011.13041" title="Download PDF">pdf</a>, <a href="/format/2011.13041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transformations of Muller Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casares%2C+A">Antonio Casares</a>, 
<a href="/search/cs?searchtype=author&query=Colcombet%2C+T">Thomas Colcombet</a>, 
<a href="/search/cs?searchtype=author&query=Fijalkow%2C+N">Nathana&#xeb;l Fijalkow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper superseded by the extended version <a href="/abs/2305.04323">arXiv:2305.04323</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.02343" title="Abstract">arXiv:2103.02343</a> (replaced) [<a href="/e-print/2103.02343" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provability in BI&#x27;s Sequent Calculus is Decidable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gheorghiu%2C+A">Alexander Gheorghiu</a>, 
<a href="/search/cs?searchtype=author&query=Docherty%2C+S">Simon Docherty</a>, 
<a href="/search/cs?searchtype=author&query=Pym%2C+D">David Pym</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper contains an error so the result is invalid
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Symbolic Computation (cs.SC); Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.09407" title="Abstract">arXiv:2105.09407</a> (replaced) [<a href="/pdf/2105.09407" title="Download PDF">pdf</a>, <a href="/ps/2105.09407" title="Download PostScript">ps</a>, <a href="/format/2105.09407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trilevel and Multilevel Optimization using Monotone Operator Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shafiei%2C+A">Allahkaram Shafiei</a>, 
<a href="/search/math?searchtype=author&query=Kungurtsev%2C+V">Vyacheslav Kungurtsev</a>, 
<a href="/search/math?searchtype=author&query=Marecek%2C+J">Jakub Marecek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.14082" title="Abstract">arXiv:2105.14082</a> (replaced) [<a href="/pdf/2105.14082" title="Download PDF">pdf</a>, <a href="/format/2105.14082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bhasacitra: Visualising the dialect geography of South Asia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Aryaman Arora</a>, 
<a href="/search/cs?searchtype=author&query=Farris%2C+A">Adam Farris</a>, 
<a href="/search/cs?searchtype=author&query=R%2C+G">Gopalakrishnan R</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+S">Samopriya Basu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures. Published at LChange'21 workshop located at ACL 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.11729" title="Abstract">arXiv:2109.11729</a> (replaced) [<a href="/pdf/2109.11729" title="Download PDF">pdf</a>, <a href="/format/2109.11729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal error bounds in the absence of constraint qualifications with  applications to the $p$-cones and beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lindstrom%2C+S+B">Scott B. Lindstrom</a>, 
<a href="/search/math?searchtype=author&query=Louren%C3%A7o%2C+B+F">Bruno F. Louren&#xe7;o</a>, 
<a href="/search/math?searchtype=author&query=Pong%2C+T+K">Ting Kei Pong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, comments welcome. Some small fixes and three new figures were added in order to better explain the results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.08117" title="Abstract">arXiv:2111.08117</a> (replaced) [<a href="/pdf/2111.08117" title="Download PDF">pdf</a>, <a href="/ps/2111.08117" title="Download PostScript">ps</a>, <a href="/format/2111.08117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural networks with linear threshold activations: structure and  algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalife%2C+S">Sammy Khalife</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hongyu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+A">Amitabh Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06273" title="Abstract">arXiv:2202.06273</a> (replaced) [<a href="/pdf/2202.06273" title="Download PDF">pdf</a>, <a href="/format/2202.06273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Occupancy Mapping in Dynamic Environments Using Particles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+P">Peng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiangyang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Transactions on Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.08717" title="Abstract">arXiv:2203.08717</a> (replaced) [<a href="/pdf/2203.08717" title="Download PDF">pdf</a>, <a href="/format/2203.08717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relational Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Mingkai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Shan You</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changshui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaogang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of NeurIPS 2021 paper. arXiv admin note: substantial text overlap with <a href="/abs/2107.09282">arXiv:2107.09282</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.14276" title="Abstract">arXiv:2203.14276</a> (replaced) [<a href="/pdf/2203.14276" title="Download PDF">pdf</a>, <a href="/format/2203.14276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Example-based Hypernetworks for Out-of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Volk%2C+T">Tomer Volk</a>, 
<a href="/search/cs?searchtype=author&query=Ben-David%2C+E">Eyal Ben-David</a>, 
<a href="/search/cs?searchtype=author&query=Amosy%2C+O">Ohad Amosy</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Reichart%2C+R">Roi Reichart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally to this work. Our code and data are available at: <a href="https://github.com/TomerVolk/Hyper-PADA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.03251" title="Abstract">arXiv:2204.03251</a> (replaced) [<a href="/pdf/2204.03251" title="Download PDF">pdf</a>, <a href="/format/2204.03251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automatic Construction of Filipino WordNet: Word Sense Induction  and Synset Induction Using Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Velasco%2C+D+J">Dan John Velasco</a>, 
<a href="/search/cs?searchtype=author&query=Alba%2C+A">Axel Alba</a>, 
<a href="/search/cs?searchtype=author&query=Pelagio%2C+T+G">Trisha Gail Pelagio</a>, 
<a href="/search/cs?searchtype=author&query=Ramirez%2C+B+A">Bryce Anthony Ramirez</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+U">Unisse Chua</a>, 
<a href="/search/cs?searchtype=author&query=Samson%2C+B+P">Briane Paul Samson</a>, 
<a href="/search/cs?searchtype=author&query=Cruz%2C+J+C+B">Jan Christian Blaise Cruz</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Charibeth Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SEALP 2023. Formerly titled "Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.07439" title="Abstract">arXiv:2204.07439</a> (replaced) [<a href="/pdf/2204.07439" title="Download PDF">pdf</a>, <a href="/format/2204.07439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Changhun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyungjun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+E">Eunhyeok Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jae-Joon Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.02147" title="Abstract">arXiv:2206.02147</a> (replaced) [<a href="/pdf/2206.02147" title="Download PDF">pdf</a>, <a href="/format/2206.02147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for  Text-to-Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Ziyue Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Su%2C+Z">Zhe Su</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Q">Qian Yang</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+Y">Yi Ren</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jinglin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+Z">Zhenhui Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v3: fix the introduction for the concurrent similar work of Neural Lexicon Reader (<a href="/abs/2110.09698">arXiv:2110.09698</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.04767" title="Abstract">arXiv:2206.04767</a> (replaced) [<a href="/pdf/2206.04767" title="Download PDF">pdf</a>, <a href="/format/2206.04767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Do We Mean When We Say &quot;Insight&quot;? A Formal Synthesis of Existing  Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Battle%2C+L">Leilani Battle</a>, 
<a href="/search/cs?searchtype=author&query=Ottley%2C+A">Alvitta Ottley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE TVCG Oct 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05359" title="Abstract">arXiv:2206.05359</a> (replaced) [<a href="/pdf/2206.05359" title="Download PDF">pdf</a>, <a href="/format/2206.05359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blades: A Unified Benchmark Suite for Byzantine Attacks and Defenses in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shenghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Ngai%2C+E">Edith Ngai</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Fanghua Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+L">Li Ju</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Voigt%2C+T">Thiemo Voigt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07162" title="Abstract">arXiv:2206.07162</a> (replaced) [<a href="/pdf/2206.07162" title="Download PDF">pdf</a>, <a href="/format/2206.07162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Category-Agnostic 6D Pose Estimation with Conditional Neural Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yumeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+N">Ning Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ziesche%2C+H">Hanna Ziesche</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVPR2022 workshop: Women in Computer Vision (WiCV)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CVPR2022 workshop: Women in Computer Vision (WiCV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.07940" title="Abstract">arXiv:2206.07940</a> (replaced) [<a href="/pdf/2206.07940" title="Download PDF">pdf</a>, <a href="/format/2206.07940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Rigidity Hurts: Soft Consistency Regularization for Probabilistic  Hierarchical Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamarthi%2C+H">Harshavardhan Kamarthi</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingkai Kong</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+A">Alexander Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at KDD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08517" title="Abstract">arXiv:2206.08517</a> (replaced) [<a href="/pdf/2206.08517" title="Download PDF">pdf</a>, <a href="/format/2206.08517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECTLO: Effective Continuous-time Odometry Using Range Image for LiDAR  with Small FoV
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianke Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures. Accepted for publication in the Proceedings of the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.01227" title="Abstract">arXiv:2207.01227</a> (replaced) [<a href="/pdf/2207.01227" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cybersecurity: Past, Present and Future
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Shahid Alam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Author's copy of the book published under ISBN: 978-620-4-74421-6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.05072" title="Abstract">arXiv:2207.05072</a> (replaced) [<a href="/pdf/2207.05072" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An On-demand Photonic Ising Machine with Simplified Hamiltonian  Calculation by Phase-encoding and Intensity Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+J">Jiayi Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yuxuan Liao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiyao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Deyang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xue Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaowen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+K">Kaiyu Cui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yidong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06949" title="Abstract">arXiv:2207.06949</a> (replaced) [<a href="/pdf/2207.06949" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeking the Truth Beyond the Data. An Unsupervised Machine Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Saligkaras%2C+D">Dimitrios Saligkaras</a>, 
<a href="/search/stat?searchtype=author&query=Papageorgiou%2C+V+E">Vasileios E. Papageorgiou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in the proceedings of the 3rd International Scientific Forum on Computer and Energy Sciences (WFCES 2022)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AIP Conference Proceedings, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP); Computation (stat.CO); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07025" title="Abstract">arXiv:2207.07025</a> (replaced) [<a href="/pdf/2207.07025" title="Download PDF">pdf</a>, <a href="/format/2207.07025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to translate by learning to communicate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Downey%2C+C+M">C.M. Downey</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L+Z">Leo Z. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Steinert-Threlkeld%2C+S">Shane Steinert-Threlkeld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready for 3rd Multilingual Representation Learning Workshop (MRL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.07333" title="Abstract">arXiv:2207.07333</a> (replaced) [<a href="/pdf/2207.07333" title="Download PDF">pdf</a>, <a href="/format/2207.07333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rain regime segmentation of Sentinel-1 observation learning from NEXRAD  collocations with Convolution Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colin%2C+A">Aur&#xe9;lien Colin</a> (1,2), 
<a href="/search/cs?searchtype=author&query=Tandeo%2C+P">Pierre Tandeo</a> (1), 
<a href="/search/cs?searchtype=author&query=Peureux%2C+C">Charles Peureux</a> (2), 
<a href="/search/cs?searchtype=author&query=Husson%2C+R">Romain Husson</a> (2), 
<a href="/search/cs?searchtype=author&query=Long%C3%A9p%C3%A9%2C+N">Nicolas Long&#xe9;p&#xe9;</a> (3), 
<a href="/search/cs?searchtype=author&query=Fablet%2C+R">Ronan Fablet</a> (1) ((1) IMT Atlantique, Lab-STICC, UMR CNRS, France, (2) Collecte Localisation Satellites, Brest, France, (3) Phi-lab Explore Office, ESRIN, European Space Agency (ESA), Frascati, Italy)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.06348" title="Abstract">arXiv:2208.06348</a> (replaced) [<a href="/pdf/2208.06348" title="Download PDF">pdf</a>, <a href="/format/2208.06348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Brain Signals Reveal Inner Alignment with Human Languages?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Han%2C+W">William Han</a>, 
<a href="/search/q-bio?searchtype=author&query=Qiu%2C+J">Jielin Qiu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+J">Jiacheng Zhu</a>, 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+M">Mengdi Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Weber%2C+D">Douglas Weber</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhao%2C+D">Ding Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.12803" title="Abstract">arXiv:2208.12803</a> (replaced) [<a href="/pdf/2208.12803" title="Download PDF">pdf</a>, <a href="/format/2208.12803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avoidability beyond paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gurvich%2C+V">Vladimir Gurvich</a>, 
<a href="/search/math?searchtype=author&query=Krnc%2C+M">Matja&#x17e; Krnc</a>, 
<a href="/search/math?searchtype=author&query=Milani%C4%8D%2C+M">Martin Milani&#x10d;</a>, 
<a href="/search/math?searchtype=author&query=Vyalyi%2C+M">Mikhail Vyalyi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.04030" title="Abstract">arXiv:2209.04030</a> (replaced) [<a href="/pdf/2209.04030" title="Download PDF">pdf</a>, <a href="/format/2209.04030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling the Connections between Privacy and Certified Robustness in  Federated Learning Against Poisoning Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chulin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yunhui Long</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qinbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM CCS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.09035" title="Abstract">arXiv:2209.09035</a> (replaced) [<a href="/pdf/2209.09035" title="Download PDF">pdf</a>, <a href="/format/2209.09035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness in Face Presentation Attack Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meiling Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wufei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kuijper%2C+A">Arjan Kuijper</a>, 
<a href="/search/cs?searchtype=author&query=Struc%2C+V">Vitomir Struc</a>, 
<a href="/search/cs?searchtype=author&query=Damer%2C+N">Naser Damer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Pattern Recognition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05015" title="Abstract">arXiv:2210.05015</a> (replaced) [<a href="/pdf/2210.05015" title="Download PDF">pdf</a>, <a href="/format/2210.05015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimality Guarantees for Particle Belief Approximation of POMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+M+H">Michael H. Lim</a>, 
<a href="/search/cs?searchtype=author&query=Becker%2C+T+J">Tyler J. Becker</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>, 
<a href="/search/cs?searchtype=author&query=Tomlin%2C+C+J">Claire J. Tomlin</a>, 
<a href="/search/cs?searchtype=author&query=Sunberg%2C+Z+N">Zachary N. Sunberg</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Artificial Intelligence Research, 77, 1591-1636 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.05344" title="Abstract">arXiv:2210.05344</a> (replaced) [<a href="/pdf/2210.05344" title="Download PDF">pdf</a>, <a href="/ps/2210.05344" title="Download PostScript">ps</a>, <a href="/format/2210.05344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Proof-theoretic Validity to Base-extension Semantics for  Intuitionistic Propositional Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gheorghiu%2C+A+V">Alexander V. Gheorghiu</a>, 
<a href="/search/cs?searchtype=author&query=Pym%2C+D+J">David J. Pym</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11388" title="Abstract">arXiv:2210.11388</a> (replaced) [<a href="/pdf/2210.11388" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed Deep Diffusion MRI Reconstruction: Break Training Data  Bottleneck in Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Y">Yuncheng Gao</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+M">Mingyang Han</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ruan%2C+D">Dan Ruan</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+Y">Yu Shen</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yiping Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yirong Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chengyan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+B">Boyu Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+R">Ran Tao</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zhigang Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiazheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Liuhong Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Y">Yi Guo</a>, 
<a href="/search/eess?searchtype=author&query=Kang%2C+T">Taishan Kang</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+J">Jianzhong Lin</a>, 
<a href="/search/eess?searchtype=author&query=Gong%2C+T">Tao Gong</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/eess?searchtype=author&query=Fei%2C+G">Guoqiang Fei</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+M">Meijin Lin</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+D">Di Guo</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jianjun Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Meiyun Wang</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+X">Xiaobo Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13623" title="Abstract">arXiv:2210.13623</a> (replaced) [<a href="/pdf/2210.13623" title="Download PDF">pdf</a>, <a href="/format/2210.13623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning and Bandits for Speech and Language Processing:  Tutorial, Review and Outlook
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Baihan Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Expert Systems with Applications. Accompanying INTERSPEECH 2022 Tutorial on the same topic. Including latest advancements in large language models (LLMs)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.00617" title="Abstract">arXiv:2211.00617</a> (replaced) [<a href="/pdf/2211.00617" title="Download PDF">pdf</a>, <a href="/format/2211.00617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of policy gradient methods for finite-horizon stochastic  linear-quadratic control problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Giegrich%2C+M">Michael Giegrich</a>, 
<a href="/search/math?searchtype=author&query=Reisinger%2C+C">Christoph Reisinger</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yufei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add a model-free implementation of the algorithm at the end of Section 2.4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08238" title="Abstract">arXiv:2211.08238</a> (replaced) [<a href="/pdf/2211.08238" title="Download PDF">pdf</a>, <a href="/format/2211.08238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Contrastive Learning and Numerical Evidence for Improving  Confusing Legal Judgment Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Leilei Gan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baokui Li</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yating Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11659" title="Abstract">arXiv:2211.11659</a> (replaced) [<a href="/pdf/2211.11659" title="Download PDF">pdf</a>, <a href="/format/2211.11659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Abstractions for Packet Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohan%2C+A">Anshuman Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+N">Nate Foster</a>, 
<a href="/search/cs?searchtype=author&query=Kapp%C3%A9%2C+T">Tobias Kapp&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Kozen%2C+D">Dexter Kozen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14214" title="Abstract">arXiv:2211.14214</a> (replaced) [<a href="/pdf/2211.14214" title="Download PDF">pdf</a>, <a href="/format/2211.14214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity Framework for Forbidden Subgraphs II: When Hardness Is Not  Preserved under Edge Subdivision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martin%2C+B">Barnaby Martin</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S">Sukanya Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Paulusma%2C+D">Daniel Paulusma</a>, 
<a href="/search/cs?searchtype=author&query=Siggers%2C+M">Mark Siggers</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+S">Siani Smith</a>, 
<a href="/search/cs?searchtype=author&query=van+Leeuwen%2C+E+J">Erik Jan van Leeuwen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01365" title="Abstract">arXiv:2212.01365</a> (replaced) [<a href="/pdf/2212.01365" title="Download PDF">pdf</a>, <a href="/format/2212.01365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+H+J">Hong Jun Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Van+Roy%2C+B">Benjamin Van Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05258" title="Abstract">arXiv:2212.05258</a> (replaced) [<a href="/pdf/2212.05258" title="Download PDF">pdf</a>, <a href="/format/2212.05258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image augmentation with conformal mappings for a convolutional neural  network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rainio%2C+O">Oona Rainio</a>, 
<a href="/search/cs?searchtype=author&query=Nasser%2C+M+M+S">Mohamed M.S. Nasser</a>, 
<a href="/search/cs?searchtype=author&query=Vuorinen%2C+M">Matti Vuorinen</a>, 
<a href="/search/cs?searchtype=author&query=Kl%C3%A9n%2C+R">Riku Kl&#xe9;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07469" title="Abstract">arXiv:2212.07469</a> (replaced) [<a href="/pdf/2212.07469" title="Download PDF">pdf</a>, <a href="/format/2212.07469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning threshold neurons via the &quot;edge of stability&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+K">Kwangjun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Bubeck%2C+S">S&#xe9;bastien Bubeck</a>, 
<a href="/search/cs?searchtype=author&query=Chewi%2C+S">Sinho Chewi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+T">Yin Tat Lee</a>, 
<a href="/search/cs?searchtype=author&query=Suarez%2C+F">Felipe Suarez</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 13 figures, Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08599" title="Abstract">arXiv:2212.08599</a> (replaced) [<a href="/pdf/2212.08599" title="Download PDF">pdf</a>, <a href="/ps/2212.08599" title="Download PostScript">ps</a>, <a href="/format/2212.08599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Well-Covered Vector Spaces of Graphs using Modular  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Milani%C4%8D%2C+M">Martin Milani&#x10d;</a>, 
<a href="/search/math?searchtype=author&query=Piva%C4%8D%2C+N">Nevena Piva&#x10d;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08731" title="Abstract">arXiv:2212.08731</a> (replaced) [<a href="/pdf/2212.08731" title="Download PDF">pdf</a>, <a href="/format/2212.08731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-person 3D pose estimation from unlabelled data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodriguez-Criado%2C+D">Daniel Rodriguez-Criado</a>, 
<a href="/search/cs?searchtype=author&query=Bachiller%2C+P">Pilar Bachiller</a>, 
<a href="/search/cs?searchtype=author&query=Vogiatzis%2C+G">George Vogiatzis</a>, 
<a href="/search/cs?searchtype=author&query=Manso%2C+L+J">Luis J. Manso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09561" title="Abstract">arXiv:2212.09561</a> (replaced) [<a href="/pdf/2212.09561" title="Download PDF">pdf</a>, <a href="/format/2212.09561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Better Reasoners with Self-Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+Y">Yixuan Weng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minjun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bin Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shizhu He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accept in EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09724" title="Abstract">arXiv:2212.09724</a> (replaced) [<a href="/pdf/2212.09724" title="Download PDF">pdf</a>, <a href="/format/2212.09724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Retrieve-and-Read Framework for Knowledge Graph Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pahuja%2C+V">Vardaan Pahuja</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Latapie%2C+H">Hugo Latapie</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasa%2C+J">Jayanth Srinivasa</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CIKM'23; Published version DOI: <a href="https://doi.org/10.1145/3583780.3614769">this https URL</a>; 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09730" title="Abstract">arXiv:2212.09730</a> (replaced) [<a href="/pdf/2212.09730" title="Download PDF">pdf</a>, <a href="/format/2212.09730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speaking Style Conversion in the Waveform Domain Using Discrete  Self-Supervised Units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maimon%2C+G">Gallil Maimon</a>, 
<a href="/search/cs?searchtype=author&query=Adi%2C+Y">Yossi Adi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10784" title="Abstract">arXiv:2212.10784</a> (replaced) [<a href="/pdf/2212.10784" title="Download PDF">pdf</a>, <a href="/format/2212.10784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical  Relation Extraction?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiashu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M+D">Mingyu Derek Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages; ACL 2023; code in <a href="https://github.com/luka-group/NLI_as_Indirect_Supervision">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10931" title="Abstract">arXiv:2212.10931</a> (replaced) [<a href="/pdf/2212.10931" title="Download PDF">pdf</a>, <a href="/format/2212.10931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Elementary Proof of the FMP for Kleene Algebra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapp%C3%A9%2C+T">Tobias Kapp&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02125" title="Abstract">arXiv:2301.02125</a> (replaced) [<a href="/pdf/2301.02125" title="Download PDF">pdf</a>, <a href="/ps/2301.02125" title="Download PostScript">ps</a>, <a href="/format/2301.02125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defining Logical Systems via Algebraic Constraints on Proofs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gheorghiu%2C+A+V">Alexander V. Gheorghiu</a>, 
<a href="/search/cs?searchtype=author&query=Pym%2C+D+J">David J. Pym</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Logic and Computation 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09077" title="Abstract">arXiv:2301.09077</a> (replaced) [<a href="/pdf/2301.09077" title="Download PDF">pdf</a>, <a href="/format/2301.09077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleash the Potential of Image Branch for Cross-modal 3D Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Junhui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yixuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+G">Guoliang Xing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12321" title="Abstract">arXiv:2301.12321</a> (replaced) [<a href="/pdf/2301.12321" title="Download PDF">pdf</a>, <a href="/format/2301.12321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Relation Graph: A Unified Framework for Identifying Label Noise  and Outlier Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jang-Hyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sangdoo Yun</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H+O">Hyun Oh Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00029" title="Abstract">arXiv:2302.00029</a> (replaced) [<a href="/pdf/2302.00029" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determining Which Sine Wave Frequencies Correspond to Signal and Which  Correspond to Noise in Eye-Tracking Time-Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raju%2C+M+H">Mehedi H. Raju</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+L">Lee Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Bouman%2C+T+M">Troy M. Bouman</a>, 
<a href="/search/cs?searchtype=author&query=Komogortsev%2C+O+V">Oleg V. Komogortsev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pages-16, Figures-11, Tables-4. arXiv admin note: text overlap with <a href="/abs/2209.07657">arXiv:2209.07657</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00381" title="Abstract">arXiv:2302.00381</a> (replaced) [<a href="/pdf/2302.00381" title="Download PDF">pdf</a>, <a href="/format/2302.00381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BotPercent: Estimating Bot Populations in Twitter Communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Sclar%2C+M">Melanie Sclar</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Herun Wan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Minnan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01063" title="Abstract">arXiv:2302.01063</a> (replaced) [<a href="/pdf/2302.01063" title="Download PDF">pdf</a>, <a href="/format/2302.01063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Modelling and Verification of Social Explainable AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Sidoruk%2C+T">Teofil Sidoruk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01205" title="Abstract">arXiv:2302.01205</a> (replaced) [<a href="/pdf/2302.01205" title="Download PDF">pdf</a>, <a href="/format/2302.01205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A characteristic mapping method for incompressible hydrodynamics on a  rotating sphere
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Taylor%2C+S">Seth Taylor</a>, 
<a href="/search/math?searchtype=author&query=Nave%2C+J">Jean-Christophe Nave</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01328" title="Abstract">arXiv:2302.01328</a> (replaced) [<a href="/pdf/2302.01328" title="Download PDF">pdf</a>, <a href="/format/2302.01328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IC3: Image Captioning by Committee Consensus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+D+M">David M. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Myers%2C+A">Austin Myers</a>, 
<a href="/search/cs?searchtype=author&query=Vijayanarasimhan%2C+S">Sudheendra Vijayanarasimhan</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+D+A">David A. Ross</a>, 
<a href="/search/cs?searchtype=author&query=Canny%2C+J">John Canny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03098" title="Abstract">arXiv:2302.03098</a> (replaced) [<a href="/pdf/2302.03098" title="Download PDF">pdf</a>, <a href="/format/2302.03098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-shot Empirical Privacy Estimation for Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrew%2C+G">Galen Andrew</a>, 
<a href="/search/cs?searchtype=author&query=Kairouz%2C+P">Peter Kairouz</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S">Sewoong Oh</a>, 
<a href="/search/cs?searchtype=author&query=Oprea%2C+A">Alina Oprea</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
<a href="/search/cs?searchtype=author&query=Suriyakumar%2C+V">Vinith Suriyakumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06132" title="Abstract">arXiv:2302.06132</a> (replaced) [<a href="/pdf/2302.06132" title="Download PDF">pdf</a>, <a href="/format/2302.06132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NNKGC: Improving Knowledge Graph Completion with Node Neighborhoods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Boming Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> DL4KG Workshop, ISWC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06674" title="Abstract">arXiv:2302.06674</a> (replaced) [<a href="/pdf/2302.06674" title="Download PDF">pdf</a>, <a href="/format/2302.06674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded  Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+M">Minsik Oh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joosung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference. Code available at <a href="https://github.com/minsik-ai/PK-ICR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06980" title="Abstract">arXiv:2302.06980</a> (replaced) [<a href="/pdf/2302.06980" title="Download PDF">pdf</a>, <a href="/format/2302.06980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-Based Modeling of 5G Core Control Plane for 5G Network  Digital Twin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tao%2C+Z">Zhenyu Tao</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Y">Yongliang Guo</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+G">Guanghui He</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yongming Huang</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+X">Xiaohu You</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Cognitive Communications and Networking
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08724" title="Abstract">arXiv:2302.08724</a> (replaced) [<a href="/pdf/2302.08724" title="Download PDF">pdf</a>, <a href="/format/2302.08724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Piecewise Deterministic Markov Processes for Bayesian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Goan%2C+E">Ethan Goan</a>, 
<a href="/search/stat?searchtype=author&query=Perrin%2C+D">Dimitri Perrin</a>, 
<a href="/search/stat?searchtype=author&query=Mengersen%2C+K">Kerrie Mengersen</a>, 
<a href="/search/stat?searchtype=author&query=Fookes%2C+C">Clinton Fookes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Includes correction to software and corrigendum note
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Other Statistics (stat.OT)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08932" title="Abstract">arXiv:2302.08932</a> (replaced) [<a href="/pdf/2302.08932" title="Download PDF">pdf</a>, <a href="/format/2302.08932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An MPC-based Optimal Motion Control Framework for Pendulum-driven  Spherical Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+X">Xiaoqing Guan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Boyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">You Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to Control Engineering Practice
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11084" title="Abstract">arXiv:2302.11084</a> (replaced) [<a href="/pdf/2302.11084" title="Download PDF">pdf</a>, <a href="/format/2302.11084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Distribution Normalization for Contrastively Learned  Vision-language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yifei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Juntao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fengyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zabih%2C+R">Ramin Zabih</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+S">Ser-Nam Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023, project webpage: <a href="https://fengyuli-dev.github.io/dn-website/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13116" title="Abstract">arXiv:2302.13116</a> (replaced) [<a href="/pdf/2302.13116" title="Download PDF">pdf</a>, <a href="/ps/2302.13116" title="Download PostScript">ps</a>, <a href="/format/2302.13116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The $\mathsf{AC}^0$-Complexity Of Visibly Pushdown Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B6ller%2C+S">Stefan G&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Grosshans%2C+N">Nathan Grosshans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 81 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Computational Complexity (cs.CC); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13405" title="Abstract">arXiv:2302.13405</a> (replaced) [<a href="/pdf/2302.13405" title="Download PDF">pdf</a>, <a href="/format/2302.13405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic (Timed) Computation Tree Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arias%2C+J">Jaime Arias</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Penczek%2C+W">Wojciech Penczek</a>, 
<a href="/search/cs?searchtype=author&query=Petrucci%2C+L">Laure Petrucci</a>, 
<a href="/search/cs?searchtype=author&query=Sidoruk%2C+T">Teofil Sidoruk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06623" title="Abstract">arXiv:2303.06623</a> (replaced) [<a href="/pdf/2303.06623" title="Download PDF">pdf</a>, <a href="/format/2303.06623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MWE as WSD: Solving Multiword Expression Identification with Word Sense  Disambiguation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tanner%2C+J">Joshua Tanner</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+J">Jacob Hoffman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09200" title="Abstract">arXiv:2303.09200</a> (replaced) [<a href="/pdf/2303.09200" title="Download PDF">pdf</a>, <a href="/format/2303.09200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduction of rain-induced errors for wind speed estimation on SAR  observations using convolutional neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colin%2C+A">Aur&#xe9;lien Colin</a> (1, 2), 
<a href="/search/cs?searchtype=author&query=Tandeo%2C+P">Pierre Tandeo</a> (1, 3), 
<a href="/search/cs?searchtype=author&query=Peureux%2C+C">Charles Peureux</a> (2), 
<a href="/search/cs?searchtype=author&query=Husson%2C+R">Romain Husson</a> (2), 
<a href="/search/cs?searchtype=author&query=Fablet%2C+R">Ronan Fablet</a> (1, 3) ((1) IMT Atlantique, Lab-STICC, UMR CNRS 6285, F-29238, France, (2) Collecte Localisation Satellites, Brest, France, (3) Odyssey, Inria/IMT, France)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing (2023), Vol. 16, pp. 8586-8594
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12410" title="Abstract">arXiv:2303.12410</a> (replaced) [<a href="/pdf/2303.12410" title="Download PDF">pdf</a>, <a href="/format/2303.12410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EDGI: Equivariant Diffusion for Planning with Embodied Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brehmer%2C+J">Johann Brehmer</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+J">Joey Bose</a>, 
<a href="/search/cs?searchtype=author&query=de+Haan%2C+P">Pim de Haan</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+T">Taco Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. v2: matches camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13047" title="Abstract">arXiv:2303.13047</a> (replaced) [<a href="/pdf/2303.13047" title="Download PDF">pdf</a>, <a href="/format/2303.13047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Better Dynamic Graph Learning: New Architecture and Unified  Library
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Le Yu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Leilei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bowen Du</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+W">Weifeng Lv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023, Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13121" title="Abstract">arXiv:2303.13121</a> (replaced) [<a href="/pdf/2303.13121" title="Download PDF">pdf</a>, <a href="/format/2303.13121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DetOFA: Efficient Training of Once-for-All Networks for Object Detection  Using Path Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sakuma%2C+Y">Yuiko Sakuma</a>, 
<a href="/search/cs?searchtype=author&query=Ishii%2C+M">Masato Ishii</a>, 
<a href="/search/cs?searchtype=author&query=Narihira%2C+T">Takuya Narihira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14090" title="Abstract">arXiv:2303.14090</a> (replaced) [<a href="/pdf/2303.14090" title="Download PDF">pdf</a>, <a href="/ps/2303.14090" title="Download PostScript">ps</a>, <a href="/format/2303.14090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed neural networks in the recreation of hydrodynamic  simulations from dark matter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Dai%2C+Z">Zhenyu Dai</a>, 
<a href="/search/astro-ph?searchtype=author&query=Moews%2C+B">Ben Moews</a>, 
<a href="/search/astro-ph?searchtype=author&query=Vilalta%2C+R">Ricardo Vilalta</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dave%2C+R">Romeel Dave</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Astrophysics of Galaxies (astro-ph.GA); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15861" title="Abstract">arXiv:2303.15861</a> (replaced) [<a href="/pdf/2303.15861" title="Download PDF">pdf</a>, <a href="/ps/2303.15861" title="Download PostScript">ps</a>, <a href="/format/2303.15861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating exponential integrators to efficiently solve semilinear  advection-diffusion-reaction equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Caliari%2C+M">Marco Caliari</a>, 
<a href="/search/math?searchtype=author&query=Cassini%2C+F">Fabio Cassini</a>, 
<a href="/search/math?searchtype=author&query=Einkemmer%2C+L">Lukas Einkemmer</a>, 
<a href="/search/math?searchtype=author&query=Ostermann%2C+A">Alexander Ostermann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16282" title="Abstract">arXiv:2303.16282</a> (replaced) [<a href="/pdf/2303.16282" title="Download PDF">pdf</a>, <a href="/format/2303.16282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACFA: Secure Runtime Auditing &amp; Guaranteed Device Healing via Active  Control Flow Attestation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caulfield%2C+A">Adam Caulfield</a>, 
<a href="/search/cs?searchtype=author&query=Rattanavipanon%2C+N">Norrathep Rattanavipanon</a>, 
<a href="/search/cs?searchtype=author&query=De+Oliveira+Nunes%2C+I">Ivan De Oliveira Nunes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In 29th USENIX Security Symposium (USENIX Security 23), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.16355" title="Abstract">arXiv:2303.16355</a> (replaced) [<a href="/pdf/2303.16355" title="Download PDF">pdf</a>, <a href="/format/2303.16355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the impact of Byzantine attacks on coupled phase oscillators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Tyloo%2C+M">Melvyn Tyloo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Adaptation and Self-Organizing Systems (nlin.AO)</span>; Systems and Control (eess.SY); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17544" title="Abstract">arXiv:2303.17544</a> (replaced) [<a href="/pdf/2303.17544" title="Download PDF">pdf</a>, <a href="/format/2303.17544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TorKameleon: Improving Tor&#x27;s Censorship Resistance with K-anonymization  and Media-based Covert Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vilalonga%2C+A">Afonso Vilalonga</a>, 
<a href="/search/cs?searchtype=author&query=Resende%2C+J+S">Jo&#xe3;o S. Resende</a>, 
<a href="/search/cs?searchtype=author&query=Domingos%2C+H">Henrique Domingos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03965" title="Abstract">arXiv:2304.03965</a> (replaced) [<a href="/pdf/2304.03965" title="Download PDF">pdf</a>, <a href="/format/2304.03965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The n-vehicle exploration problem is NP-complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jinchuan Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoya Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04951" title="Abstract">arXiv:2304.04951</a> (replaced) [<a href="/pdf/2304.04951" title="Download PDF">pdf</a>, <a href="/format/2304.04951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing the Tracy-Widom distribution for arbitrary $&#x3b2;&gt;0$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Trogdon%2C+T">Thomas Trogdon</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yiting Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05306" title="Abstract">arXiv:2304.05306</a> (replaced) [<a href="/pdf/2304.05306" title="Download PDF">pdf</a>, <a href="/format/2304.05306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Linear Correctors: A Tight Output Min-Entropy Bound and  Selection Technique
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruji%C4%87%2C+M">Milo&#x161; Gruji&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Verbauwhede%2C+I">Ingrid Verbauwhede</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Final version after the review process. Accepted for publication in IEEE Transactions on Information Forensics and Security. Corrected typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06523" title="Abstract">arXiv:2304.06523</a> (replaced) [<a href="/pdf/2304.06523" title="Download PDF">pdf</a>, <a href="/format/2304.06523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The 2-Attractor Problem is NP-Complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+J">Janosch Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Whittington%2C+P">Philip Whittington</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06762" title="Abstract">arXiv:2304.06762</a> (replaced) [<a href="/pdf/2304.06762" title="Download PDF">pdf</a>, <a href="/format/2304.06762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shall We Pretrain Autoregressive Language Models with Retrieval? A  Comprehensive Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ping%2C+W">Wei Ping</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=McAfee%2C+L">Lawrence McAfee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shoeybi%2C+M">Mohammad Shoeybi</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Kuchaiev%2C+O">Oleksii Kuchaiev</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06923" title="Abstract">arXiv:2304.06923</a> (replaced) [<a href="/pdf/2304.06923" title="Download PDF">pdf</a>, <a href="/format/2304.06923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An NMPC-ECBF Framework for Dynamic Motion Planning and Execution in  vision-based Human-Robot Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dianhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Van%2C+M">Mien Van</a>, 
<a href="/search/cs?searchtype=author&query=Sopasakis%2C+P">Pantelis Sopasakis</a>, 
<a href="/search/cs?searchtype=author&query=McLoone%2C+S">Se&#xe1;n McLoone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Image and Video Processing (eess.IV); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07063" title="Abstract">arXiv:2304.07063</a> (replaced) [<a href="/pdf/2304.07063" title="Download PDF">pdf</a>, <a href="/format/2304.07063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Complex Queries on Knowledge Graphs with Neural Link  Predictors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Databases (cs.DB); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07133" title="Abstract">arXiv:2304.07133</a> (replaced) [<a href="/pdf/2304.07133" title="Download PDF">pdf</a>, <a href="/format/2304.07133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRe: A Programming Model for Verifiably Safe Local-First Software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haas%2C+J">Julian Haas</a> (1), 
<a href="/search/cs?searchtype=author&query=Mogk%2C+R">Ragnar Mogk</a> (1), 
<a href="/search/cs?searchtype=author&query=Yanakieva%2C+E">Elena Yanakieva</a> (2), 
<a href="/search/cs?searchtype=author&query=Bieniusa%2C+A">Annette Bieniusa</a> (2), 
<a href="/search/cs?searchtype=author&query=Mezini%2C+M">Mira Mezini</a> (1) ((1) Technische Universit&#xe4;t Darmstadt, (2) University of Kaiserslautern-Landau)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the extended version of the work accepted at ECOOP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08503" title="Abstract">arXiv:2304.08503</a> (replaced) [<a href="/pdf/2304.08503" title="Download PDF">pdf</a>, <a href="/format/2304.08503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Test Problem Generator for Sequential Transfer Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+X">Xiaoming Xue</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cuie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Liang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09310" title="Abstract">arXiv:2304.09310</a> (replaced) [<a href="/pdf/2304.09310" title="Download PDF">pdf</a>, <a href="/format/2304.09310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Adaptive $&#x3c4;$-Lasso: Robustness and Oracle Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mozafari-Majd%2C+E">Emadaldin Mozafari-Majd</a>, 
<a href="/search/stat?searchtype=author&query=Koivunen%2C+V">Visa Koivunen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10398" title="Abstract">arXiv:2304.10398</a> (replaced) [<a href="/pdf/2304.10398" title="Download PDF">pdf</a>, <a href="/format/2304.10398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-label Node Classification On Graph-Structured Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tianqi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+N+T">Ngan Thi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hanjalic%2C+A">Alan Hanjalic</a>, 
<a href="/search/cs?searchtype=author&query=Khosla%2C+M">Megha Khosla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10557" title="Abstract">arXiv:2304.10557</a> (replaced) [<a href="/pdf/2304.10557" title="Download PDF">pdf</a>, <a href="/format/2304.10557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Introduction to Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12410" title="Abstract">arXiv:2304.12410</a> (replaced) [<a href="/pdf/2304.12410" title="Download PDF">pdf</a>, <a href="/format/2304.12410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEFT-Ref: A Modular Reference Architecture and Typology for  Parameter-Efficient Finetuning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabry%2C+M">Mohammed Sabry</a>, 
<a href="/search/cs?searchtype=author&query=Belz%2C+A">Anya Belz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12526" title="Abstract">arXiv:2304.12526</a> (replaced) [<a href="/pdf/2304.12526" title="Download PDF">pdf</a>, <a href="/format/2304.12526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patch Diffusion: Faster and More Data-Efficient Training of Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhendong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Huangjie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01115" title="Abstract">arXiv:2305.01115</a> (replaced) [<a href="/pdf/2305.01115" title="Download PDF">pdf</a>, <a href="/format/2305.01115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning Unlocked for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhendong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yifan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yadong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02967" title="Abstract">arXiv:2305.02967</a> (replaced) [<a href="/pdf/2305.02967" title="Download PDF">pdf</a>, <a href="/ps/2305.02967" title="Download PostScript">ps</a>, <a href="/format/2305.02967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Urgency Annotations for Alternating Choices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keskin%2C+E">Eren Keskin</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+R">Roland Meyer</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Wall%2C+S">S&#xf6;ren van der Wall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03495" title="Abstract">arXiv:2305.03495</a> (replaced) [<a href="/pdf/2305.03495" title="Download PDF">pdf</a>, <a href="/format/2305.03495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Prompt Optimization with &quot;Gradient Descent&quot; and Beam Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pryzant%2C+R">Reid Pryzant</a>, 
<a href="/search/cs?searchtype=author&query=Iter%2C+D">Dan Iter</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jerry Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y+T">Yin Tat Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenguang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+M">Michael Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04226" title="Abstract">arXiv:2305.04226</a> (replaced) [<a href="/pdf/2305.04226" title="Download PDF">pdf</a>, <a href="/format/2305.04226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design, Implementation and Evaluation of an External Pose-Tracking  System for Underwater Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Winkel%2C+B">Birger Winkel</a>, 
<a href="/search/cs?searchtype=author&query=Nakath%2C+D">David Nakath</a>, 
<a href="/search/cs?searchtype=author&query=Woelk%2C+F">Felix Woelk</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6ser%2C+K">Kevin K&#xf6;ser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04616" title="Abstract">arXiv:2305.04616</a> (replaced) [<a href="/pdf/2305.04616" title="Download PDF">pdf</a>, <a href="/ps/2305.04616" title="Download PostScript">ps</a>, <a href="/format/2305.04616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Scheduling of Agents in ADTrees: Specialised Algorithm and  Declarative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arias%2C+J">Jaime Arias</a>, 
<a href="/search/cs?searchtype=author&query=Olarte%2C+C">Carlos Olarte</a>, 
<a href="/search/cs?searchtype=author&query=Petrucci%2C+L">Laure Petrucci</a>, 
<a href="/search/cs?searchtype=author&query=Ma%C5%9Bko%2C+%C5%81">&#x141;ukasz Ma&#x15b;ko</a>, 
<a href="/search/cs?searchtype=author&query=Penczek%2C+W">Wojciech Penczek</a>, 
<a href="/search/cs?searchtype=author&query=Sidoruk%2C+T">Teofil Sidoruk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2101.06838">arXiv:2101.06838</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05799" title="Abstract">arXiv:2305.05799</a> (replaced) [<a href="/pdf/2305.05799" title="Download PDF">pdf</a>, <a href="/format/2305.05799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing double with a multifunctional reservoir computer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Flynn%2C+A">Andrew Flynn</a>, 
<a href="/search/math?searchtype=author&query=Tsachouridis%2C+V+A">Vassilios A. Tsachouridis</a>, 
<a href="/search/math?searchtype=author&query=Amann%2C+A">Andreas Amann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Chaos. Please use Adobe Acrobat Reader to view this paper as intended. The appearance of some figures may vary in different .pdf readers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05896" title="Abstract">arXiv:2305.05896</a> (replaced) [<a href="/pdf/2305.05896" title="Download PDF">pdf</a>, <a href="/format/2305.05896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Black-Box Attack on Code Models via Representation Nearest Neighbor  Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shangqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaofei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Traon%2C+Y+L">Yves Le Traon</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06040" title="Abstract">arXiv:2305.06040</a> (replaced) [<a href="/pdf/2305.06040" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable orthogonal delay-division multiplexed OEO artificial neural  network trained for TI-ADC equalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zazzi%2C+A">Andrea Zazzi</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A+D">Arka Dipta Das</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%BCssen%2C+L">Lukas H&#xfc;ssen</a>, 
<a href="/search/cs?searchtype=author&query=Negra%2C+R">Renato Negra</a>, 
<a href="/search/cs?searchtype=author&query=Witzens%2C+J">Jeremy Witzens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08281" title="Abstract">arXiv:2305.08281</a> (replaced) [<a href="/pdf/2305.08281" title="Download PDF">pdf</a>, <a href="/format/2305.08281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FactKB: Generalizable Factuality Evaluation using Language Models  Enhanced with Factual Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuyang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09606" title="Abstract">arXiv:2305.09606</a> (replaced) [<a href="/pdf/2305.09606" title="Download PDF">pdf</a>, <a href="/format/2305.09606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reward Learning with Intractable Normalizing Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoegerman%2C+J">Joshua Hoegerman</a>, 
<a href="/search/cs?searchtype=author&query=Losey%2C+D+P">Dylan P. Losey</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters, vol. 8, no. 11, pp.
  7511-7518, Nov. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09955" title="Abstract">arXiv:2305.09955</a> (replaced) [<a href="/pdf/2305.09955" title="Download PDF">pdf</a>, <a href="/format/2305.09955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Card: Filling LLMs&#x27; Knowledge Gaps with Plug-in Specialized  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuyang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+V">Vidhisha Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10744" title="Abstract">arXiv:2305.10744</a> (replaced) [<a href="/pdf/2305.10744" title="Download PDF">pdf</a>, <a href="/format/2305.10744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Resource Allocation in Episodic Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Duksang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Overman%2C+W">William Overman</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dabeen Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10800" title="Abstract">arXiv:2305.10800</a> (replaced) [<a href="/pdf/2305.10800" title="Download PDF">pdf</a>, <a href="/ps/2305.10800" title="Download PostScript">ps</a>, <a href="/format/2305.10800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Cell-Free ISAC Networks: Joint BS Mode Selection and  Beamforming Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11171" title="Abstract">arXiv:2305.11171</a> (replaced) [<a href="/pdf/2305.11171" title="Download PDF">pdf</a>, <a href="/format/2305.11171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrueTeacher: Learning Factual Consistency Evaluation with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gekhman%2C+Z">Zorik Gekhman</a>, 
<a href="/search/cs?searchtype=author&query=Herzig%2C+J">Jonathan Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Aharoni%2C+R">Roee Aharoni</a>, 
<a href="/search/cs?searchtype=author&query=Elkind%2C+C">Chen Elkind</a>, 
<a href="/search/cs?searchtype=author&query=Szpektor%2C+I">Idan Szpektor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a long paper in EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11202" title="Abstract">arXiv:2305.11202</a> (replaced) [<a href="/pdf/2305.11202" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-based Frameworks for Power Engineering from Routine to Novel Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ran Li</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+C">Chuanqing Pu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Junyi Tao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Canbing Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+F">Feilong Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yue Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sijie Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Software Engineering (cs.SE); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11790" title="Abstract">arXiv:2305.11790</a> (replaced) [<a href="/pdf/2305.11790" title="Download PDF">pdf</a>, <a href="/format/2305.11790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting with Pseudo-Code Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+M">Mayank Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Prince Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+R">Riyaz Bhat</a>, 
<a href="/search/cs?searchtype=author&query=V%2C+R+M">Rudra Murthy V</a>, 
<a href="/search/cs?searchtype=author&query=Contractor%2C+D">Danish Contractor</a>, 
<a href="/search/cs?searchtype=author&query=Tamilselvam%2C+S">Srikanth Tamilselvam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in EMNLP 2023 main track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12295" title="Abstract">arXiv:2305.12295</a> (replaced) [<a href="/pdf/2305.12295" title="Download PDF">pdf</a>, <a href="/format/2305.12295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logic-LM: Empowering Large Language Models with Symbolic Solvers for  Faithful Logical Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Albalak%2C+A">Alon Albalak</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings, long paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12467" title="Abstract">arXiv:2305.12467</a> (replaced) [<a href="/pdf/2305.12467" title="Download PDF">pdf</a>, <a href="/format/2305.12467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Multi-phase Optimization Dynamics and Rich Nonlinear  Behaviors of ReLU Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chao Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 92 pages, NeurIPS 2023 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12634" title="Abstract">arXiv:2305.12634</a> (replaced) [<a href="/pdf/2305.12634" title="Download PDF">pdf</a>, <a href="/format/2305.12634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-efficient Active Learning for Structured Prediction with Partial  Annotation and Self-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhisong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Strubell%2C+E">Emma Strubell</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+E">Eduard Hovy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12818" title="Abstract">arXiv:2305.12818</a> (replaced) [<a href="/pdf/2305.12818" title="Download PDF">pdf</a>, <a href="/format/2305.12818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crosslingual Transfer Learning for Low-Resource Languages Based on  Multilingual Colexification Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yihong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haotian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Weissweiler%2C+L">Leonie Weissweiler</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+R">Renhao Pei</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14232" title="Abstract">arXiv:2305.14232</a> (replaced) [<a href="/pdf/2305.14232" title="Download PDF">pdf</a>, <a href="/format/2305.14232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pre-training Multi-task Contrastive Learning Models for Scientific  Literature Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhihong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye-Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages; Accepted to Findings of EMNLP 2023 (Project Page: <a href="https://scimult.github.io/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14281" title="Abstract">arXiv:2305.14281</a> (replaced) [<a href="/pdf/2305.14281" title="Download PDF">pdf</a>, <a href="/format/2305.14281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
<a href="/search/cs?searchtype=author&query=Nematzadeh%2C+A">Aida Nematzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Hendricks%2C+L+A">Lisa Anne Hendricks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14381" title="Abstract">arXiv:2305.14381</a> (replaced) [<a href="/pdf/2305.14381" title="Download PDF">pdf</a>, <a href="/format/2305.14381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connecting Multi-modal Contrastive Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zehan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xize Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haifeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiageng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Li Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+A">Aoxiong Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14610" title="Abstract">arXiv:2305.14610</a> (replaced) [<a href="/pdf/2305.14610" title="Download PDF">pdf</a>, <a href="/format/2305.14610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bryan Li</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14766" title="Abstract">arXiv:2305.14766</a> (replaced) [<a href="/pdf/2305.14766" title="Download PDF">pdf</a>, <a href="/format/2305.14766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Allies: Prompting Large Language Model with Beam Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+D">Daxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14994" title="Abstract">arXiv:2305.14994</a> (replaced) [<a href="/pdf/2305.14994" title="Download PDF">pdf</a>, <a href="/format/2305.14994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RefGPT: Dialogue Generation of GPT, by GPT, and for GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Dongjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruifeng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yuantao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zili Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shusen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15020" title="Abstract">arXiv:2305.15020</a> (replaced) [<a href="/pdf/2305.15020" title="Download PDF">pdf</a>, <a href="/format/2305.15020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Multilingual Language Model Compression through Vocabulary  Trimming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ushio%2C+A">Asahi Ushio</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15118" title="Abstract">arXiv:2305.15118</a> (replaced) [<a href="/pdf/2305.15118" title="Download PDF">pdf</a>, <a href="/format/2305.15118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness in Streaming Submodular Maximization over a Matroid Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halabi%2C+M+E">Marwa El Halabi</a>, 
<a href="/search/cs?searchtype=author&query=Fusco%2C+F">Federico Fusco</a>, 
<a href="/search/cs?searchtype=author&query=Norouzi-Fard%2C+A">Ashkan Norouzi-Fard</a>, 
<a href="/search/cs?searchtype=author&query=Tardos%2C+J">Jakab Tardos</a>, 
<a href="/search/cs?searchtype=author&query=Tarnawski%2C+J">Jakub Tarnawski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICML 23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15538" title="Abstract">arXiv:2305.15538</a> (replaced) [<a href="/pdf/2305.15538" title="Download PDF">pdf</a>, <a href="/format/2305.15538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Post-processing Private Synthetic Data for Improving Utility on Selected  Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sudalairaj%2C+S">Shivchander Sudalairaj</a>, 
<a href="/search/cs?searchtype=author&query=Henning%2C+J">John Henning</a>, 
<a href="/search/cs?searchtype=author&query=Greenewald%2C+K">Kristjan Greenewald</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Akash Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Databases (cs.DB); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16291" title="Abstract">arXiv:2305.16291</a> (replaced) [<a href="/pdf/2305.16291" title="Download PDF">pdf</a>, <a href="/format/2305.16291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voyager: An Open-Ended Embodied Agent with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuqi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yunfan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Mandlekar%2C+A">Ajay Mandlekar</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Linxi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website and open-source codebase: <a href="https://voyager.minedojo.org/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16546" title="Abstract">arXiv:2305.16546</a> (replaced) [<a href="/pdf/2305.16546" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preliminary studies: Comparing LSTM and BLSTM Deep Neural Networks for  Power Consumption Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+D+G">Davi Guimar&#xe3;es da Silva</a>, 
<a href="/search/cs?searchtype=author&query=de+Moura+Meneses%2C+A+A">Anderson Alvarenga de Moura Meneses</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, in English, 13 figures and 13 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16986" title="Abstract">arXiv:2305.16986</a> (replaced) [<a href="/pdf/2305.16986" title="Download PDF">pdf</a>, <a href="/format/2305.16986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Gengze Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yicong Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17103" title="Abstract">arXiv:2305.17103</a> (replaced) [<a href="/pdf/2305.17103" title="Download PDF">pdf</a>, <a href="/ps/2305.17103" title="Download PostScript">ps</a>, <a href="/format/2305.17103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On regular sets of affine type in finite Desarguesian planes and related  codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aguglia%2C+A">Angela Aguglia</a>, 
<a href="/search/math?searchtype=author&query=Csajb%C3%B3k%2C+B">Bence Csajb&#xf3;k</a>, 
<a href="/search/math?searchtype=author&query=Giuzzi%2C+L">Luca Giuzzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages/revised and improved version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17931" title="Abstract">arXiv:2305.17931</a> (replaced) [<a href="/pdf/2305.17931" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monocular 2D Camera-based Proximity Monitoring for Human-Machine  Collision Warning on Construction Sites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuexiong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiaowei Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18945" title="Abstract">arXiv:2305.18945</a> (replaced) [<a href="/pdf/2305.18945" title="Download PDF">pdf</a>, <a href="/format/2305.18945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> String Diagrams for $&#x3bb;$-calculi and Functional Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghica%2C+D">Dan Ghica</a>, 
<a href="/search/cs?searchtype=author&query=Zanasi%2C+F">Fabio Zanasi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Programming Languages (cs.PL); Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19435" title="Abstract">arXiv:2305.19435</a> (replaced) [<a href="/pdf/2305.19435" title="Download PDF">pdf</a>, <a href="/format/2305.19435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdANNS: A Framework for Adaptive Semantic Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rege%2C+A">Aniket Rege</a>, 
<a href="/search/cs?searchtype=author&query=Kusupati%2C+A">Aditya Kusupati</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S+R">Sharan Ranjit S</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+A">Alan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingqing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S">Sham Kakade</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Prateek Jain</a>, 
<a href="/search/cs?searchtype=author&query=Farhadi%2C+A">Ali Farhadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 15 figures. NeurIPS 2023 camera ready publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19713" title="Abstract">arXiv:2305.19713</a> (replaced) [<a href="/pdf/2305.19713" title="Download PDF">pdf</a>, <a href="/format/2305.19713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Red Teaming Language Model Detectors with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhouxing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Fan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangning Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Accepted by TACL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00477" title="Abstract">arXiv:2306.00477</a> (replaced) [<a href="/pdf/2306.00477" title="Download PDF">pdf</a>, <a href="/format/2306.00477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make Pre-trained Model Reversible: From Parameter to Memory Efficient  Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shaomu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS camera-ready version. Code at <a href="https://github.com/BaohaoLiao/mefts">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03642" title="Abstract">arXiv:2306.03642</a> (replaced) [<a href="/pdf/2306.03642" title="Download PDF">pdf</a>, <a href="/format/2306.03642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GarmentCode: Programming Parametric Sewing Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korosteleva%2C+M">Maria Korosteleva</a>, 
<a href="/search/cs?searchtype=author&query=Sorkine-Hornung%2C+O">Olga Sorkine-Hornung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at SIGGRAPH Asia 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Trans. Graph. 42, 6, Article 197 (December 2023), 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03659" title="Abstract">arXiv:2306.03659</a> (replaced) [<a href="/pdf/2306.03659" title="Download PDF">pdf</a>, <a href="/format/2306.03659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing  Semantics with MASCHInE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hubert%2C+N">Nicolas Hubert</a>, 
<a href="/search/cs?searchtype=author&query=Paulheim%2C+H">Heiko Paulheim</a>, 
<a href="/search/cs?searchtype=author&query=Monnin%2C+P">Pierre Monnin</a>, 
<a href="/search/cs?searchtype=author&query=Brun%2C+A">Armelle Brun</a>, 
<a href="/search/cs?searchtype=author&query=Monticolo%2C+D">Davy Monticolo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04542" title="Abstract">arXiv:2306.04542</a> (replaced) [<a href="/pdf/2306.04542" title="Download PDF">pdf</a>, <a href="/format/2306.04542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Design Fundamentals of Diffusion Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Ziyi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Koulieris%2C+G+A">George Alex Koulieris</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H+P+H">Hubert P. H. Shum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05644" title="Abstract">arXiv:2306.05644</a> (replaced) [<a href="/pdf/2306.05644" title="Download PDF">pdf</a>, <a href="/format/2306.05644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised  Span Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nagata%2C+M">Masaaki Nagata</a>, 
<a href="/search/cs?searchtype=author&query=Tsuruoka%2C+Y">Yoshimasa Tsuruoka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACL 2023 main conference long paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06344" title="Abstract">arXiv:2306.06344</a> (replaced) [<a href="/pdf/2306.06344" title="Download PDF">pdf</a>, <a href="/format/2306.06344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Guided Traffic Simulation via Scene-Level Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Ziyuan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Rempe%2C+D">Davis Rempe</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ivanovic%2C+B">Boris Ivanovic</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yulong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Danfei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+B">Baishakhi Ray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06599" title="Abstract">arXiv:2306.06599</a> (replaced) [<a href="/pdf/2306.06599" title="Download PDF">pdf</a>, <a href="/format/2306.06599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Imbalanced Regression: Fair Uncertainty Quantification via  Probabilistic Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06798" title="Abstract">arXiv:2306.06798</a> (replaced) [<a href="/pdf/2306.06798" title="Download PDF">pdf</a>, <a href="/format/2306.06798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kepler: Robust Learning for Faster Parametric Query Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doshi%2C+L">Lyric Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+V">Vincent Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+G">Gaurav Jain</a>, 
<a href="/search/cs?searchtype=author&query=Marcus%2C+R">Ryan Marcus</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haoyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Altinb%C3%BCken%2C+D">Deniz Altinb&#xfc;ken</a>, 
<a href="/search/cs?searchtype=author&query=Brevdo%2C+E">Eugene Brevdo</a>, 
<a href="/search/cs?searchtype=author&query=Fraser%2C+C">Campbell Fraser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIGMOD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07960" title="Abstract">arXiv:2306.07960</a> (replaced) [<a href="/pdf/2306.07960" title="Download PDF">pdf</a>, <a href="/format/2306.07960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric Neural-Collapse Representations with Supervised Contrastive  Loss: The Impact of ReLU and Batching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kini%2C+G+R">Ganesh Ramachandra Kini</a>, 
<a href="/search/cs?searchtype=author&query=Vakilian%2C+V">Vala Vakilian</a>, 
<a href="/search/cs?searchtype=author&query=Behnia%2C+T">Tina Behnia</a>, 
<a href="/search/cs?searchtype=author&query=Gill%2C+J">Jaidev Gill</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> change of title and additional experimental results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08141" title="Abstract">arXiv:2306.08141</a> (replaced) [<a href="/pdf/2306.08141" title="Download PDF">pdf</a>, <a href="/format/2306.08141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in  Artistic Creations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vodrahalli%2C+K">Kailas Vodrahalli</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08670" title="Abstract">arXiv:2306.08670</a> (replaced) [<a href="/pdf/2306.08670" title="Download PDF">pdf</a>, <a href="/format/2306.08670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Power of Populations in Decentralized Learning Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazarsfeld%2C+J">John Lazarsfeld</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09434" title="Abstract">arXiv:2306.09434</a> (replaced) [<a href="/pdf/2306.09434" title="Download PDF">pdf</a>, <a href="/format/2306.09434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECO-CHIP: Estimation of Carbon Footprint of Chiplet-based Architectures  for Sustainable VLSI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudarshan%2C+C+C">Chetan Choppali Sudarshan</a>, 
<a href="/search/cs?searchtype=author&query=Matkar%2C+N">Nikhil Matkar</a>, 
<a href="/search/cs?searchtype=author&query=Vrudhula%2C+S">Sarma Vrudhula</a>, 
<a href="/search/cs?searchtype=author&query=Sapatnekar%2C+S+S">Sachin S. Sapatnekar</a>, 
<a href="/search/cs?searchtype=author&query=Chhabria%2C+V+A">Vidya A. Chhabria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at HPCA23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09821" title="Abstract">arXiv:2306.09821</a> (replaced) [<a href="/pdf/2306.09821" title="Download PDF">pdf</a>, <a href="/format/2306.09821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of User Feedback: Leveraging Large Language  Model as User Simulator to Enhance Dialogue System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yue Feng</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Lipani%2C+A">Aldo Lipani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09983" title="Abstract">arXiv:2306.09983</a> (replaced) [<a href="/pdf/2306.09983" title="Download PDF">pdf</a>, <a href="/format/2306.09983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Superhuman Models with Consistency Checks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fluri%2C+L">Lukas Fluri</a>, 
<a href="/search/cs?searchtype=author&query=Paleka%2C+D">Daniel Paleka</a>, 
<a href="/search/cs?searchtype=author&query=Tram%C3%A8r%2C+F">Florian Tram&#xe8;r</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 18 figures. Code and data are available at <a href="https://github.com/ethz-spylab/superhuman-ai-consistency">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11586" title="Abstract">arXiv:2306.11586</a> (replaced) [<a href="/pdf/2306.11586" title="Download PDF">pdf</a>, <a href="/format/2306.11586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Powerful Graph Neural Networks for Directed Multigraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Egressy%2C+B">B&#xe9;ni Egressy</a>, 
<a href="/search/cs?searchtype=author&query=von+Niederh%C3%A4usern%2C+L">Luc von Niederh&#xe4;usern</a>, 
<a href="/search/cs?searchtype=author&query=Blanusa%2C+J">Jovan Blanusa</a>, 
<a href="/search/cs?searchtype=author&query=Altman%2C+E">Erik Altman</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>, 
<a href="/search/cs?searchtype=author&query=Atasu%2C+K">Kubilay Atasu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12129" title="Abstract">arXiv:2306.12129</a> (replaced) [<a href="/pdf/2306.12129" title="Download PDF">pdf</a>, <a href="/format/2306.12129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Based Compensation for Inconsistencies in Knitted Force  Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Aigner%2C+R">Roland Aigner</a>, 
<a href="/search/eess?searchtype=author&query=St%C3%B6ckl%2C+A">Andreas St&#xf6;ckl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13856" title="Abstract">arXiv:2306.13856</a> (replaced) [<a href="/pdf/2306.13856" title="Download PDF">pdf</a>, <a href="/format/2306.13856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-to-Rank Meets Language: Boosting Language-Driven Ordering  Alignment for Ordinal Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peipei Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+C">Chunshui Cao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14790" title="Abstract">arXiv:2306.14790</a> (replaced) [<a href="/pdf/2306.14790" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Assessment of Divergent Thinking in Chinese Language with  TransDis: A Transformer-Based Language Model Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianchen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhaoyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yubo Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15687" title="Abstract">arXiv:2306.15687</a> (replaced) [<a href="/pdf/2306.15687" title="Download PDF">pdf</a>, <a href="/format/2306.15687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Le%2C+M">Matthew Le</a>, 
<a href="/search/eess?searchtype=author&query=Vyas%2C+A">Apoorv Vyas</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+B">Bowen Shi</a>, 
<a href="/search/eess?searchtype=author&query=Karrer%2C+B">Brian Karrer</a>, 
<a href="/search/eess?searchtype=author&query=Sari%2C+L">Leda Sari</a>, 
<a href="/search/eess?searchtype=author&query=Moritz%2C+R">Rashel Moritz</a>, 
<a href="/search/eess?searchtype=author&query=Williamson%2C+M">Mary Williamson</a>, 
<a href="/search/eess?searchtype=author&query=Manohar%2C+V">Vimal Manohar</a>, 
<a href="/search/eess?searchtype=author&query=Adi%2C+Y">Yossi Adi</a>, 
<a href="/search/eess?searchtype=author&query=Mahadeokar%2C+J">Jay Mahadeokar</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+W">Wei-Ning Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02688" title="Abstract">arXiv:2307.02688</a> (replaced) [<a href="/pdf/2307.02688" title="Download PDF">pdf</a>, <a href="/ps/2307.02688" title="Download PostScript">ps</a>, <a href="/format/2307.02688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epistemic systems and Flagg and Friedman&#x27;s translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Inou%C3%A9%2C+T">Takao Inou&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03512" title="Abstract">arXiv:2307.03512</a> (replaced) [<a href="/pdf/2307.03512" title="Download PDF">pdf</a>, <a href="/format/2307.03512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning of Semantic Segmentation Methods for Identifying  Buried Archaeological Structures on LiDAR Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sech%2C+G">Gregory Sech</a>, 
<a href="/search/cs?searchtype=author&query=Soleni%2C+P">Paolo Soleni</a>, 
<a href="/search/cs?searchtype=author&query=der+Vaart%2C+W+B+V">Wouter B. Verschoof-van der Vaart</a>, 
<a href="/search/cs?searchtype=author&query=Kokalj%2C+%C5%BD">&#x17d;iga Kokalj</a>, 
<a href="/search/cs?searchtype=author&query=Traviglia%2C+A">Arianna Traviglia</a>, 
<a href="/search/cs?searchtype=author&query=Fiorucci%2C+M">Marco Fiorucci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE International Geoscience and Remote Sensing Symposium 2023 (IGARSS 2023) @IEEE copyright
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03810" title="Abstract">arXiv:2307.03810</a> (replaced) [<a href="/pdf/2307.03810" title="Download PDF">pdf</a>, <a href="/format/2307.03810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> URL: A Representation Learning Benchmark for Transferable Uncertainty  Estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirchhof%2C+M">Michael Kirchhof</a>, 
<a href="/search/cs?searchtype=author&query=Mucs%C3%A1nyi%2C+B">B&#xe1;lint Mucs&#xe1;nyi</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+S+J">Seong Joon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS D&amp;B 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04228" title="Abstract">arXiv:2307.04228</a> (replaced) [<a href="/pdf/2307.04228" title="Download PDF">pdf</a>, <a href="/format/2307.04228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian tomography using polynomial chaos expansion and deep generative  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Meles%2C+G+A">Giovanni Angelo Meles</a>, 
<a href="/search/physics?searchtype=author&query=Amaya%2C+M">Macarena Amaya</a>, 
<a href="/search/physics?searchtype=author&query=Levy%2C+S">Shiran Levy</a>, 
<a href="/search/physics?searchtype=author&query=Marelli%2C+S">Stefano Marelli</a>, 
<a href="/search/physics?searchtype=author&query=Linde%2C+N">Niklas Linde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04661" title="Abstract">arXiv:2307.04661</a> (replaced) [<a href="/pdf/2307.04661" title="Download PDF">pdf</a>, <a href="/ps/2307.04661" title="Download PostScript">ps</a>, <a href="/format/2307.04661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the power of graph neural networks and the role of the activation  function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalife%2C+S">Sammy Khalife</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+A">Amitabh Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05033" title="Abstract">arXiv:2307.05033</a> (replaced) [<a href="/pdf/2307.05033" title="Download PDF">pdf</a>, <a href="/format/2307.05033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Anytime Optical Flow Estimation with Event Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yaozu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xiaoting Yin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yining Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaiwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code will be available at <a href="https://github.com/Yaozhuwa/EVA-Flow">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05141" title="Abstract">arXiv:2307.05141</a> (replaced) [<a href="/pdf/2307.05141" title="Download PDF">pdf</a>, <a href="/format/2307.05141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Probabilistic Movement Primitives with a Bayesian Aggregator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Przystupa%2C+M">Michael Przystupa</a>, 
<a href="/search/cs?searchtype=author&query=Haghverd%2C+F">Faezeh Haghverd</a>, 
<a href="/search/cs?searchtype=author&query=Jagersand%2C+M">Martin Jagersand</a>, 
<a href="/search/cs?searchtype=author&query=Tosatto%2C+S">Samuele Tosatto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05345" title="Abstract">arXiv:2307.05345</a> (replaced) [<a href="/pdf/2307.05345" title="Download PDF">pdf</a>, <a href="/ps/2307.05345" title="Download PostScript">ps</a>, <a href="/format/2307.05345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A generalization of Floater--Hormann interpolants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Themistoclakis%2C+W">Woula Themistoclakis</a>, 
<a href="/search/math?searchtype=author&query=Van+Barel%2C+M">Marc Van Barel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06125" title="Abstract">arXiv:2307.06125</a> (replaced) [<a href="/pdf/2307.06125" title="Download PDF">pdf</a>, <a href="/format/2307.06125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Hierarchical Interactive Multi-Object Search for Mobile  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmalstieg%2C+F">Fabian Schmalstieg</a>, 
<a href="/search/cs?searchtype=author&query=Honerkamp%2C+D">Daniel Honerkamp</a>, 
<a href="/search/cs?searchtype=author&query=Welschehold%2C+T">Tim Welschehold</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, Accepted for publication in RA-L. Code and Models: <a href="http://himos.cs.uni-freiburg.de/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06528" title="Abstract">arXiv:2307.06528</a> (replaced) [<a href="/pdf/2307.06528" title="Download PDF">pdf</a>, <a href="/format/2307.06528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimised Least Squares Approach for Accurate Polygon and Ellipse  Fitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quan%2C+Y">Yiming Quan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shian Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07362" title="Abstract">arXiv:2307.07362</a> (replaced) [<a href="/pdf/2307.07362" title="Download PDF">pdf</a>, <a href="/format/2307.07362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A scoping review on multimodal deep learning in biomedical images and  texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhaoyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Mingquan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by the Journal of Biomedical Informatics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Biomedical Informatics, Volume 146, October 2023,
  104482
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09020" title="Abstract">arXiv:2307.09020</a> (replaced) [<a href="/pdf/2307.09020" title="Download PDF">pdf</a>, <a href="/format/2307.09020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Face-PAST: Facial Pose Awareness and Style Transfer Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khowaja%2C+S+A">Sunder Ali Khowaja</a>, 
<a href="/search/cs?searchtype=author&query=Nkenyereye%2C+L">Lewis Nkenyereye</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jiseok Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I+H">Ik Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Dev%2C+K">Kapal Dev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09688" title="Abstract">arXiv:2307.09688</a> (replaced) [<a href="/pdf/2307.09688" title="Download PDF">pdf</a>, <a href="/format/2307.09688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for  Recommendation and Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haoming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongzhi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+H">Haoyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hanqing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruirui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M+X">Monica Xiao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Goutam%2C+R">Rahul Goutam</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Subbian%2C+K">Karthik Subbian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yizhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Bing Yin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xianfeng Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023, Track on Datasets and Benchmarks; Dataset for KDD Cup 2023, <a href="https://kddcup23.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12444" title="Abstract">arXiv:2307.12444</a> (replaced) [<a href="/pdf/2307.12444" title="Download PDF">pdf</a>, <a href="/format/2307.12444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proximal Galerkin: A structure-preserving finite element method for  pointwise bound constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Keith%2C+B">Brendan Keith</a>, 
<a href="/search/math?searchtype=author&query=Surowiec%2C+T+M">Thomas M. Surowiec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12964" title="Abstract">arXiv:2307.12964</a> (replaced) [<a href="/pdf/2307.12964" title="Download PDF">pdf</a>, <a href="/format/2307.12964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibrahimi%2C+S">Sarah Ibrahimi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaohang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+A">Amanmeet Garg</a>, 
<a href="/search/cs?searchtype=author&query=Sanan%2C+A">Ashutosh Sanan</a>, 
<a href="/search/cs?searchtype=author&query=Omar%2C+M">Mohamed Omar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14984" title="Abstract">arXiv:2307.14984</a> (replaced) [<a href="/pdf/2307.14984" title="Download PDF">pdf</a>, <a href="/format/2307.14984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S3: Social-network Simulation System with Large Language Model-Empowered  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiaochong Lan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhihong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jinzhu Mao</a>, 
<a href="/search/cs?searchtype=author&query=Piao%2C+J">Jinghua Piao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huandong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02463" title="Abstract">arXiv:2308.02463</a> (replaced) [<a href="/pdf/2308.02463" title="Download PDF">pdf</a>, <a href="/format/2308.02463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalist Foundation Model for Radiology by Leveraging  Web-scale 2D&amp;3D Medical Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chaoyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoman Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06368" title="Abstract">arXiv:2308.06368</a> (replaced) [<a href="/pdf/2308.06368" title="Download PDF">pdf</a>, <a href="/ps/2308.06368" title="Download PostScript">ps</a>, <a href="/format/2308.06368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic-Level Bayesian Surprise and Serendipity for Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hasan%2C+T">Tonmoy Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Bunescu%2C+R">Razvan Bunescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Errata: Table 2 caption should be "Surprising item *detection* ..."
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06405" title="Abstract">arXiv:2308.06405</a> (replaced) [<a href="/pdf/2308.06405" title="Download PDF">pdf</a>, <a href="/format/2308.06405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> White-box Membership Inference Attacks against Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+X">Xuhui Kang</a>, 
<a href="/search/cs?searchtype=author&query=Huai%2C+M">Mengdi Huai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08968" title="Abstract">arXiv:2308.08968</a> (replaced) [<a href="/pdf/2308.08968" title="Download PDF">pdf</a>, <a href="/format/2308.08968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Performance of Multidimensional Constellation Shaping for Linear  and Nonlinear Optical Fiber Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+Z">Zhiwei Liang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shen Li</a>, 
<a href="/search/eess?searchtype=author&query=Lei%2C+Y">Yi Lei</a>, 
<a href="/search/eess?searchtype=author&query=Liga%2C+G">Gabriele Liga</a>, 
<a href="/search/eess?searchtype=author&query=Alvarado%2C+A">Alex Alvarado</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by the ECOC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00608" title="Abstract">arXiv:2309.00608</a> (replaced) [<a href="/pdf/2309.00608" title="Download PDF">pdf</a>, <a href="/format/2309.00608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copiloting the Copilots: Fusing Large Language Models with Completion  Engines for Automated Program Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yuxiang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C+S">Chunqiu Steven Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingming Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at ESEC/FSE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00857" title="Abstract">arXiv:2309.00857</a> (replaced) [<a href="/pdf/2309.00857" title="Download PDF">pdf</a>, <a href="/format/2309.00857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Transformer&#x27;s Ability to Learn Mildly Context-Sensitive  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shunjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Steinert-Threlkeld%2C+S">Shane Steinert-Threlkeld</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at BlackboxNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01319" title="Abstract">arXiv:2309.01319</a> (replaced) [<a href="/pdf/2309.01319" title="Download PDF">pdf</a>, <a href="/format/2309.01319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An ML-assisted OTFS vs. OFDM adaptable modem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+I+Z">I. Zakir Ahmed</a>, 
<a href="/search/eess?searchtype=author&query=Sadjadpour%2C+H+R">Hamid R. Sadjadpour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Future Networks World Forum 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02072" title="Abstract">arXiv:2309.02072</a> (replaced) [<a href="/pdf/2309.02072" title="Download PDF">pdf</a>, <a href="/format/2309.02072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepVol: A Pre-Trained Universal Asset Volatility Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/econ?searchtype=author&query=Tran%2C+M">Minh-Ngoc Tran</a>, 
<a href="/search/econ?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/econ?searchtype=author&query=Gerlach%2C+R">Richard Gerlach</a>, 
<a href="/search/econ?searchtype=author&query=Kohn%2C+R">Robert Kohn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Artificial Intelligence (cs.AI); Computational Finance (q-fin.CP)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03308" title="Abstract">arXiv:2309.03308</a> (replaced) [<a href="/pdf/2309.03308" title="Download PDF">pdf</a>, <a href="/format/2309.03308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Sampling of 3D Spatial Correlations for Focus+Context  Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neuhauser%2C+C">Christoph Neuhauser</a>, 
<a href="/search/cs?searchtype=author&query=Stumpfegger%2C+J">Josef Stumpfegger</a>, 
<a href="/search/cs?searchtype=author&query=Westermann%2C+R">R&#xfc;diger Westermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at IEEE Transactions on Visualization and Computer Graphics (TVCG). This version will be replaced upon publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05138" title="Abstract">arXiv:2309.05138</a> (replaced) [<a href="/pdf/2309.05138" title="Download PDF">pdf</a>, <a href="/format/2309.05138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenAIPABench: A Benchmark for Generative AI-based Privacy Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamid%2C+A">Aamir Hamid</a>, 
<a href="/search/cs?searchtype=author&query=Samidi%2C+H+R">Hemanth Reddy Samidi</a>, 
<a href="/search/cs?searchtype=author&query=Finin%2C+T">Tim Finin</a>, 
<a href="/search/cs?searchtype=author&query=Pappachan%2C+P">Primal Pappachan</a>, 
<a href="/search/cs?searchtype=author&query=Yus%2C+R">Roberto Yus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05270" title="Abstract">arXiv:2309.05270</a> (replaced) [<a href="/pdf/2309.05270" title="Download PDF">pdf</a>, <a href="/format/2309.05270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CONFLATOR: Incorporating Switching Point based Rotatory Positional  Encodings for Code-Mixed Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+M">Mohsin Ali</a>, 
<a href="/search/cs?searchtype=author&query=Teja%2C+K+S">Kandukuri Sai Teja</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Neeharika Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Patwa%2C+P">Parth Patwa</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Anubhab Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Workshop on Computational Approaches to Linguistic Code-Switching @EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09169" title="Abstract">arXiv:2309.09169</a> (replaced) [<a href="/pdf/2309.09169" title="Download PDF">pdf</a>, <a href="/format/2309.09169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Throughput Analysis of IEEE 802.11bn Coordinated Spatial Reuse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilhelmi%2C+F">Francesc Wilhelmi</a>, 
<a href="/search/cs?searchtype=author&query=Galati-Giordano%2C+L">Lorenzo Galati-Giordano</a>, 
<a href="/search/cs?searchtype=author&query=Geraci%2C+G">Giovanni Geraci</a>, 
<a href="/search/cs?searchtype=author&query=Bellalta%2C+B">Boris Bellalta</a>, 
<a href="/search/cs?searchtype=author&query=Fontanesi%2C+G">Gianluca Fontanesi</a>, 
<a href="/search/cs?searchtype=author&query=Nu%C3%B1ez%2C+D">David Nu&#xf1;ez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10194" title="Abstract">arXiv:2309.10194</a> (replaced) [<a href="/pdf/2309.10194" title="Download PDF">pdf</a>, <a href="/format/2309.10194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Kernel Density Integral Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=McCarter%2C+C">Calvin McCarter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Transactions on Machine Learning Research (10/2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10323" title="Abstract">arXiv:2309.10323</a> (replaced) [<a href="/pdf/2309.10323" title="Download PDF">pdf</a>, <a href="/ps/2309.10323" title="Download PostScript">ps</a>, <a href="/format/2309.10323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Exposed Passwords on Honeyword Efficacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zonghao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+L">Lujo Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Reiter%2C+M+K">Michael K. Reiter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10444" title="Abstract">arXiv:2309.10444</a> (replaced) [<a href="/pdf/2309.10444" title="Download PDF">pdf</a>, <a href="/format/2309.10444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Self-Reinforcement for Improving Learnersourced  Multiple-Choice Question Explanations with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Q">Qiming Bao</a>, 
<a href="/search/cs?searchtype=author&query=Leinonen%2C+J">Juho Leinonen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+A+Y">Alex Yuxuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Pistotti%2C+T">Tim Pistotti</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+A">Alice Huang</a>, 
<a href="/search/cs?searchtype=author&query=Denny%2C+P">Paul Denny</a>, 
<a href="/search/cs?searchtype=author&query=Witbrock%2C+M">Michael Witbrock</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10491" title="Abstract">arXiv:2309.10491</a> (replaced) [<a href="/pdf/2309.10491" title="Download PDF">pdf</a>, <a href="/format/2309.10491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiawen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Huayi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jun-Yan He</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shihao Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12645" title="Abstract">arXiv:2309.12645</a> (replaced) [<a href="/pdf/2309.12645" title="Download PDF">pdf</a>, <a href="/format/2309.12645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KuaiSim: A Comprehensive Simulator for Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kesen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Q">Qingpeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziru Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12775" title="Abstract">arXiv:2309.12775</a> (replaced) [<a href="/pdf/2309.12775" title="Download PDF">pdf</a>, <a href="/format/2309.12775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Change Driven Generative Semantic Communication Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wanting Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zehui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yanli Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12871" title="Abstract">arXiv:2309.12871</a> (replaced) [<a href="/pdf/2309.12871" title="Download PDF">pdf</a>, <a href="/format/2309.12871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnglE-optimized Text Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianming Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> add unsupervised-llama results and some citations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13218" title="Abstract">arXiv:2309.13218</a> (replaced) [<a href="/pdf/2309.13218" title="Download PDF">pdf</a>, <a href="/format/2309.13218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Copilot for Business Optimisation: A Framework and A Case Study in  Production Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amarasinghe%2C+P+T">Pivithuru Thejan Amarasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+S">Su Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Alahakoon%2C+D">Damminda Alahakoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15630" title="Abstract">arXiv:2309.15630</a> (replaced) [<a href="/pdf/2309.15630" title="Download PDF">pdf</a>, <a href="/format/2309.15630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLPBench: Evaluating Large Language Models on Solving NLP Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lechao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pengyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+I">Irene Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16128" title="Abstract">arXiv:2309.16128</a> (replaced) [<a href="/pdf/2309.16128" title="Download PDF">pdf</a>, <a href="/format/2309.16128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Correcting and Refinement for Balanced Low-Light Image Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nana Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Hong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yahong Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00378" title="Abstract">arXiv:2310.00378</a> (replaced) [<a href="/pdf/2310.00378" title="Download PDF">pdf</a>, <a href="/format/2310.00378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Value Understanding in Language Models through  Discriminator-Critique Gap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+F">Fengshuo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01225" title="Abstract">arXiv:2310.01225</a> (replaced) [<a href="/pdf/2310.01225" title="Download PDF">pdf</a>, <a href="/format/2310.01225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A path-norm toolkit for modern networks: consequences, promises and  challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gonon%2C+A">Antoine Gonon</a>, 
<a href="/search/stat?searchtype=author&query=Brisebarre%2C+N">Nicolas Brisebarre</a>, 
<a href="/search/stat?searchtype=author&query=Riccietti%2C+E">Elisa Riccietti</a>, 
<a href="/search/stat?searchtype=author&query=Gribonval%2C+R">R&#xe9;mi Gribonval</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01448" title="Abstract">arXiv:2310.01448</a> (replaced) [<a href="/pdf/2310.01448" title="Download PDF">pdf</a>, <a href="/format/2310.01448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta Semantic Template for Evaluation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yachuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress; 7 pages; more work at: <a href="https://llm-eval.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01506" title="Abstract">arXiv:2310.01506</a> (replaced) [<a href="/pdf/2310.01506" title="Download PDF">pdf</a>, <a href="/format/2310.01506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+X">Xuan Ju</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Ailing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yuxuan Bian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shaoteng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01680" title="Abstract">arXiv:2310.01680</a> (replaced) [<a href="/pdf/2310.01680" title="Download PDF">pdf</a>, <a href="/format/2310.01680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keypoint-Augmented Self-Supervised Learning for Medical Image  Segmentation with Limited Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhangsihao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+M">Mengwei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kaize Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gerig%2C+G">Guido Gerig</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yalin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera ready for NeurIPS 2023. Code available at <a href="https://github.com/zshyang/kaf.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01799" title="Abstract">arXiv:2310.01799</a> (replaced) [<a href="/pdf/2310.01799" title="Download PDF">pdf</a>, <a href="/format/2310.01799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ozturkler%2C+B">Batu Ozturkler</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+C">Chao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Eckart%2C+B">Benjamin Eckart</a>, 
<a href="/search/eess?searchtype=author&query=Mardani%2C+M">Morteza Mardani</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+J">Jiaming Song</a>, 
<a href="/search/eess?searchtype=author&query=Kautz%2C+J">Jan Kautz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01851" title="Abstract">arXiv:2310.01851</a> (replaced) [<a href="/pdf/2310.01851" title="Download PDF">pdf</a>, <a href="/format/2310.01851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An equioscillation theorem for multivariate Chebyshev approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goldsztejn%2C+A">Alexandre Goldsztejn</a> (LS2N - &#xe9;quipe OGRE, LS2N)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02031" title="Abstract">arXiv:2310.02031</a> (replaced) [<a href="/pdf/2310.02031" title="Download PDF">pdf</a>, <a href="/format/2310.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OceanGPT: A Large Language Model for Ocean Science Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+D">Daxiong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guozhou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Project Website: <a href="https://zjunlp.github.io/project/OceanGPT/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02227" title="Abstract">arXiv:2310.02227</a> (replaced) [<a href="/pdf/2310.02227" title="Download PDF">pdf</a>, <a href="/format/2310.02227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meidani%2C+K">Kazem Meidani</a>, 
<a href="/search/cs?searchtype=author&query=Shojaee%2C+P">Parshin Shojaee</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+C+K">Chandan K. Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02554" title="Abstract">arXiv:2310.02554</a> (replaced) [<a href="/pdf/2310.02554" title="Download PDF">pdf</a>, <a href="/format/2310.02554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhipeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+N">Nanqing Dong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiahao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Knottenbelt%2C+W">William Knottenbelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02658" title="Abstract">arXiv:2310.02658</a> (replaced) [<a href="/pdf/2310.02658" title="Download PDF">pdf</a>, <a href="/format/2310.02658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Multi-Configuration Problems: A Performance Analysis with Choco  Solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ritz%2C+B">Benjamin Ritz</a>, 
<a href="/search/cs?searchtype=author&query=Felfernig%2C+A">Alexander Felfernig</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Viet-Man Le</a>, 
<a href="/search/cs?searchtype=author&query=Lubos%2C+S">Sebastian Lubos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper was presented at ConfWS'23: 25th International Workshop on Configuration, September 6-7, 2023, M\'alaga, Spain and is published in the conference proceedings: <a href="https://ceur-ws.org/Vol-3509/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02954" title="Abstract">arXiv:2310.02954</a> (replaced) [<a href="/pdf/2310.02954" title="Download PDF">pdf</a>, <a href="/format/2310.02954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for  In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhicheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiongwei Han</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03178" title="Abstract">arXiv:2310.03178</a> (replaced) [<a href="/pdf/2310.03178" title="Download PDF">pdf</a>, <a href="/format/2310.03178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Ethics in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Liangqi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03559" title="Abstract">arXiv:2310.03559</a> (replaced) [<a href="/pdf/2310.03559" title="Download PDF">pdf</a>, <a href="/format/2310.03559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yanwu Xu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+L">Li Sun</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+W">Wei Peng</a>, 
<a href="/search/eess?searchtype=author&query=Visweswaran%2C+S">Shyam Visweswaran</a>, 
<a href="/search/eess?searchtype=author&query=Batmanghelich%2C+K">Kayhan Batmanghelich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05069" title="Abstract">arXiv:2310.05069</a> (replaced) [<a href="/pdf/2310.05069" title="Download PDF">pdf</a>, <a href="/format/2310.05069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot  Performance via Probability Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+E">Ercong Nie</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+H">Helmut Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05161" title="Abstract">arXiv:2310.05161</a> (replaced) [<a href="/pdf/2310.05161" title="Download PDF">pdf</a>, <a href="/format/2310.05161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Neural Language Models as Probabilistic Finite-state Automata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Svete%2C+A">Anej Svete</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Complexity (cs.CC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05199" title="Abstract">arXiv:2310.05199</a> (replaced) [<a href="/pdf/2310.05199" title="Download PDF">pdf</a>, <a href="/format/2310.05199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning  from Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+W">Wenyu Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP findings 2023 (camera-ready) Length Bias in RLHF
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05483" title="Abstract">arXiv:2310.05483</a> (replaced) [<a href="/pdf/2310.05483" title="Download PDF">pdf</a>, <a href="/format/2310.05483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with  Sparse Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiawei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chuming Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05991" title="Abstract">arXiv:2310.05991</a> (replaced) [<a href="/e-print/2310.05991" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Document-level Event Argument Extraction with Contextual Clues  and Role Relevance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wanlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shaohuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dingyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Hong Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are some mistakes in the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05995" title="Abstract">arXiv:2310.05995</a> (replaced) [<a href="/pdf/2310.05995" title="Download PDF">pdf</a>, <a href="/format/2310.05995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y+E">Yixuan Even Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jecmen%2C+S">Steven Jecmen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zimeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 8 figures, 3 tables, neurips 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06165" title="Abstract">arXiv:2310.06165</a> (replaced) [<a href="/pdf/2310.06165" title="Download PDF">pdf</a>, <a href="/format/2310.06165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAW-coref: Conjunction-Aware Word-level Coreference Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Oosterlinck%2C+K">Karel D&#x27;Oosterlinck</a>, 
<a href="/search/cs?searchtype=author&query=Bitew%2C+S+K">Semere Kiros Bitew</a>, 
<a href="/search/cs?searchtype=author&query=Papineau%2C+B">Brandon Papineau</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Demeester%2C+T">Thomas Demeester</a>, 
<a href="/search/cs?searchtype=author&query=Develder%2C+C">Chris Develder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CRAC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06196" title="Abstract">arXiv:2310.06196</a> (replaced) [<a href="/pdf/2310.06196" title="Download PDF">pdf</a>, <a href="/format/2310.06196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised  Transformers for Weakly Supervised Object Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murtaza%2C+S">Shakeeb Murtaza</a>, 
<a href="/search/cs?searchtype=author&query=Belharbi%2C+S">Soufiane Belharbi</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>, 
<a href="/search/cs?searchtype=author&query=Sarraf%2C+A">Aydin Sarraf</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Image and Vision Computing 140C (2023) 104838
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07091" title="Abstract">arXiv:2310.07091</a> (replaced) [<a href="/pdf/2310.07091" title="Download PDF">pdf</a>, <a href="/format/2310.07091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jaeger: A Concatenation-Based Multi-Transformer VQA Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+J">Jieting Long</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zewei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Penghao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Y">Yidong Gan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the technical research paper of CIKM 2023 DocIU challenges. The authors received the CIKM 2023 DocIU Winner Award, sponsored by Google, Microsoft, and the Centre for data-driven geoscience
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07141" title="Abstract">arXiv:2310.07141</a> (replaced) [<a href="/pdf/2310.07141" title="Download PDF">pdf</a>, <a href="/ps/2310.07141" title="Download PostScript">ps</a>, <a href="/format/2310.07141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time and Frequency Offset Estimation and Intercarrier Interference  Cancellation for AFDM Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuankun Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Anjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Miaowen Wen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+F">Fei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jinming Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07488" title="Abstract">arXiv:2310.07488</a> (replaced) [<a href="/pdf/2310.07488" title="Download PDF">pdf</a>, <a href="/format/2310.07488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KwaiYiiMath: Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jiayi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiaoyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengli Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengzong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhirui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengnan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xue Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xucheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yiqiao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Chao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chengru Song</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Junchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zijia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> technical report. arXiv admin note: text overlap with <a href="/abs/2306.16636">arXiv:2306.16636</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07560" title="Abstract">arXiv:2310.07560</a> (replaced) [<a href="/pdf/2310.07560" title="Download PDF">pdf</a>, <a href="/format/2310.07560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROMO: Retrieval-enhanced Offline Model-based Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingcheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haoran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuxiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Hulei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongqiao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zheng Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07570" title="Abstract">arXiv:2310.07570</a> (replaced) [<a href="/pdf/2310.07570" title="Download PDF">pdf</a>, <a href="/format/2310.07570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT for Computational Topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jian Liu</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/math?searchtype=author&query=Wei%2C+G">Guo-Wei Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07587" title="Abstract">arXiv:2310.07587</a> (replaced) [<a href="/pdf/2310.07587" title="Download PDF">pdf</a>, <a href="/format/2310.07587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient  Balancer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zikai Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zihan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Songshang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jin Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H+H">Howard Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07940" title="Abstract">arXiv:2310.07940</a> (replaced) [<a href="/pdf/2310.07940" title="Download PDF">pdf</a>, <a href="/format/2310.07940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Driven Hardware-Software Co-Optimization of Machine Learning  Pipelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Ravit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Romaszkan%2C+W">Wojciech Romaszkan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Feiqian Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Puneet Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+A">Ankur Mehta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08099" title="Abstract">arXiv:2310.08099</a> (replaced) [<a href="/pdf/2310.08099" title="Download PDF">pdf</a>, <a href="/format/2310.08099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using  Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+A">Ajay Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Anoop%2C+V+S">V. S. Anoop</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08237" title="Abstract">arXiv:2310.08237</a> (replaced) [<a href="/pdf/2310.08237" title="Download PDF">pdf</a>, <a href="/format/2310.08237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Feng%2C+X">Xingdong Feng</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Caixing Wang</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+J">Jingnan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Poster to appear in Thirty-seventh Conference on Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08395" title="Abstract">arXiv:2310.08395</a> (replaced) [<a href="/pdf/2310.08395" title="Download PDF">pdf</a>, <a href="/format/2310.08395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Large Language Models with Chain-of-Thought for Few-Shot  Knowledge Base Question Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuanyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weining Qian</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yunshi Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08854" title="Abstract">arXiv:2310.08854</a> (replaced) [<a href="/pdf/2310.08854" title="Download PDF">pdf</a>, <a href="/format/2310.08854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rank-DETR for High Quality Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yifan Pu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+W">Weicong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yiduo Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuhui Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yukang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09168" title="Abstract">arXiv:2310.09168</a> (replaced) [<a href="/pdf/2310.09168" title="Download PDF">pdf</a>, <a href="/format/2310.09168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through  Active Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fanqi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09294" title="Abstract">arXiv:2310.09294</a> (replaced) [<a href="/pdf/2310.09294" title="Download PDF">pdf</a>, <a href="/format/2310.09294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of Synthetic Fuel Production: Coupled  Optimization of Heat Exchanger Network and Operating Parameters of a 1 MW  Power-to-Liquid Plant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huber%2C+D">David Huber</a>, 
<a href="/search/math?searchtype=author&query=Birkelbach%2C+F">Felix Birkelbach</a>, 
<a href="/search/math?searchtype=author&query=Hofmann%2C+R">Ren&#xe9; Hofmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09342" title="Abstract">arXiv:2310.09342</a> (replaced) [<a href="/pdf/2310.09342" title="Download PDF">pdf</a>, <a href="/format/2310.09342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranking LLM-Generated Loop Invariants for Program Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Saikat Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Lahiri%2C+S+K">Shuvendu K. Lahiri</a>, 
<a href="/search/cs?searchtype=author&query=Fakhoury%2C+S">Sarah Fakhoury</a>, 
<a href="/search/cs?searchtype=author&query=Musuvathi%2C+M">Madanlal Musuvathi</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+A">Akash Lal</a>, 
<a href="/search/cs?searchtype=author&query=Rastogi%2C+A">Aseem Rastogi</a>, 
<a href="/search/cs?searchtype=author&query=Senthilnathan%2C+A">Aditya Senthilnathan</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rahul Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Swamy%2C+N">Nikhil Swamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP-findings 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09430" title="Abstract">arXiv:2310.09430</a> (replaced) [<a href="/pdf/2310.09430" title="Download PDF">pdf</a>, <a href="/ps/2310.09430" title="Download PostScript">ps</a>, <a href="/format/2310.09430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Evaluation of Large Language Models on Out-of-Distribution  Logical Reasoning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Q">Qiming Bao</a>, 
<a href="/search/cs?searchtype=author&query=Gendron%2C+G">Gael Gendron</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+A+Y">Alex Yuxuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+N">Neset Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Witbrock%2C+M">Michael Witbrock</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiamou Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for oral presentation at the LLM@IJCAI 2023 non-archival symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10060" title="Abstract">arXiv:2310.10060</a> (replaced) [<a href="/pdf/2310.10060" title="Download PDF">pdf</a>, <a href="/format/2310.10060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation for Time-Series Classification: An Extensive Empirical  Study and Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zijun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianhua Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10191" title="Abstract">arXiv:2310.10191</a> (replaced) [<a href="/pdf/2310.10191" title="Download PDF">pdf</a>, <a href="/format/2310.10191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VIBE: Topic-Driven Temporal Adaptation for Twitter Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10200" title="Abstract">arXiv:2310.10200</a> (replaced) [<a href="/pdf/2310.10200" title="Download PDF">pdf</a>, <a href="/ps/2310.10200" title="Download PostScript">ps</a>, <a href="/format/2310.10200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circular External Difference Families: Construction and Non-Existence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+H">Huawei Wu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Jing Yang</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+K">Keqin Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10404" title="Abstract">arXiv:2310.10404</a> (replaced) [<a href="/pdf/2310.10404" title="Download PDF">pdf</a>, <a href="/format/2310.10404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM4SGG: Large Language Model for Weakly Supervised Scene Graph  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kibum Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kanghoon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+J">Jaehyeong Jeon</a>, 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jinyoung Moon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10537" title="Abstract">arXiv:2310.10537</a> (replaced) [<a href="/pdf/2310.10537" title="Download PDF">pdf</a>, <a href="/format/2310.10537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Microscaling Data Formats for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rouhani%2C+B+D">Bita Darvish Rouhani</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ritchie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+A">Ankit More</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+M">Mathew Hall</a>, 
<a href="/search/cs?searchtype=author&query=Khodamoradi%2C+A">Alireza Khodamoradi</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+S">Summer Deng</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+D">Dhruv Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Cornea%2C+M">Marius Cornea</a>, 
<a href="/search/cs?searchtype=author&query=Dellinger%2C+E">Eric Dellinger</a>, 
<a href="/search/cs?searchtype=author&query=Denolf%2C+K">Kristof Denolf</a>, 
<a href="/search/cs?searchtype=author&query=Dusan%2C+S">Stosic Dusan</a>, 
<a href="/search/cs?searchtype=author&query=Elango%2C+V">Venmugil Elango</a>, 
<a href="/search/cs?searchtype=author&query=Golub%2C+M">Maximilian Golub</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+A">Alexander Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=James-Roxby%2C+P">Phil James-Roxby</a>, 
<a href="/search/cs?searchtype=author&query=Jani%2C+D">Dharmesh Jani</a>, 
<a href="/search/cs?searchtype=author&query=Kolhe%2C+G">Gaurav Kolhe</a>, 
<a href="/search/cs?searchtype=author&query=Langhammer%2C+M">Martin Langhammer</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ada Li</a>, 
<a href="/search/cs?searchtype=author&query=Melnick%2C+L">Levi Melnick</a>, 
<a href="/search/cs?searchtype=author&query=Mesmakhosroshahi%2C+M">Maral Mesmakhosroshahi</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Andres Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Schulte%2C+M">Michael Schulte</a>, 
<a href="/search/cs?searchtype=author&query=Shafipour%2C+R">Rasoul Shafipour</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Lei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Siu%2C+M">Michael Siu</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+P">Pradeep Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Micikevicius%2C+P">Paulius Micikevicius</a>, 
<a href="/search/cs?searchtype=author&query=Naumov%2C+M">Maxim Naumov</a>, 
<a href="/search/cs?searchtype=author&query=Verrilli%2C+C">Colin Verrilli</a>, 
<a href="/search/cs?searchtype=author&query=Wittig%2C+R">Ralph Wittig</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+D">Doug Burger</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+E">Eric Chung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10638" title="Abstract">arXiv:2310.10638</a> (replaced) [<a href="/pdf/2310.10638" title="Download PDF">pdf</a>, <a href="/format/2310.10638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Pretraining: Language Modeling Beyond Document Boundaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Sewon Min</a>, 
<a href="/search/cs?searchtype=author&query=Lomeli%2C+M">Maria Lomeli</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunting Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Margaret Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+V">Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+N+A">Noah A. Smith</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Yih%2C+S">Scott Yih</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Mike Lewis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10692" title="Abstract">arXiv:2310.10692</a> (replaced) [<a href="/pdf/2310.10692" title="Download PDF">pdf</a>, <a href="/format/2310.10692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACES: Generating Diverse Programming Puzzles with Autotelic Language  Models and Semantic Descriptors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pourcel%2C+J">Julien Pourcel</a>, 
<a href="/search/cs?searchtype=author&query=Colas%2C+C">C&#xe9;dric Colas</a>, 
<a href="/search/cs?searchtype=author&query=Oudeyer%2C+P">Pierre-Yves Oudeyer</a>, 
<a href="/search/cs?searchtype=author&query=Teodorescu%2C+L">Laetitia Teodorescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10737" title="Abstract">arXiv:2310.10737</a> (replaced) [<a href="/pdf/2310.10737" title="Download PDF">pdf</a>, <a href="/ps/2310.10737" title="Download PostScript">ps</a>, <a href="/format/2310.10737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outperforming 5G LDPC codes with GRAND over long, low rate codes --  making a long story short
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+P">Peihong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Medard%2C+M">Muriel Medard</a>, 
<a href="/search/cs?searchtype=author&query=Galligan%2C+K">Kevin Galligan</a>, 
<a href="/search/cs?searchtype=author&query=Duffy%2C+K+R">Ken R. Duffy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2305.05777">arXiv:2305.05777</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10765" title="Abstract">arXiv:2310.10765</a> (replaced) [<a href="/pdf/2310.10765" title="Download PDF">pdf</a>, <a href="/format/2310.10765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiomedJourney: Counterfactual Biomedical Image Generation by  Instruction-Learning from Multimodal Patient Journeys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yu Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Usuyama%2C+N">Naoto Usuyama</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lungren%2C+M+P">Matthew P. Lungren</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+H">Hoifung Poon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page &amp; demo: <a href="https://aka.ms/biomedjourney">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10917" title="Abstract">arXiv:2310.10917</a> (replaced) [<a href="/pdf/2310.10917" title="Download PDF">pdf</a>, <a href="/ps/2310.10917" title="Download PostScript">ps</a>, <a href="/format/2310.10917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Performance of Near-Field ISAC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Boqun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+C">Chongjun Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10925" title="Abstract">arXiv:2310.10925</a> (replaced) [<a href="/pdf/2310.10925" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Following Control of Automated Vehicle Considering Uncertainties  and Disturbances with Parametric Varying
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shen%2C+D">Dan Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10988" title="Abstract">arXiv:2310.10988</a> (replaced) [<a href="/pdf/2310.10988" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HCI in e-Government and e-Democracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianmu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11009" title="Abstract">arXiv:2310.11009</a> (replaced) [<a href="/pdf/2310.11009" title="Download PDF">pdf</a>, <a href="/format/2310.11009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Pairwise Encodings for Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shomer%2C+H">Harry Shomer</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11040" title="Abstract">arXiv:2310.11040</a> (replaced) [<a href="/pdf/2310.11040" title="Download PDF">pdf</a>, <a href="/format/2310.11040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-Learning Semantic-aware Unsupervised Segmentation for Pathological  Image Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+S">Shi Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, published in Medical Image Computing and Computer Assisted Intervention (MICCAI) 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Medical Image Computing and
  Computer-Assisted Intervention, pp. 537-547. Cham: Springer Nature
  Switzerland, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11092" title="Abstract">arXiv:2310.11092</a> (replaced) [<a href="/pdf/2310.11092" title="Download PDF">pdf</a>, <a href="/format/2310.11092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DORec: Decomposed Object Reconstruction Utilizing 2D Self-Supervised  Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sicheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Sihui Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R">Rong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yiyi Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11097" title="Abstract">arXiv:2310.11097</a> (replaced) [<a href="/pdf/2310.11097" title="Download PDF">pdf</a>, <a href="/format/2310.11097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimenting AI Technologies for Disinformation Combat: the IDMO  Project
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Canale%2C+L">Lorenzo Canale</a>, 
<a href="/search/cs?searchtype=author&query=Messina%2C+A">Alberto Messina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11102" title="Abstract">arXiv:2310.11102</a> (replaced) [<a href="/pdf/2310.11102" title="Download PDF">pdf</a>, <a href="/format/2310.11102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HGCVAE: Integrating Generative and Contrastive Learning for  Heterogeneous Graph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yulan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhirui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Sheng Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Junchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11113" title="Abstract">arXiv:2310.11113</a> (replaced) [<a href="/pdf/2310.11113" title="Download PDF">pdf</a>, <a href="/format/2310.11113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Sentiment Analysis for Software Engineering in the Era of  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Ting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Irsan%2C+I+C">Ivana Clairine Irsan</a>, 
<a href="/search/cs?searchtype=author&query=Thung%2C+F">Ferdian Thung</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+D">David Lo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to TOSEM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11368" title="Abstract">arXiv:2310.11368</a> (replaced) [<a href="/pdf/2310.11368" title="Download PDF">pdf</a>, <a href="/format/2310.11368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VECHR: A Dataset for Explainable and Robust Classification of  Vulnerability Type in the European Court of Human Rights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Staufer%2C+L">Leon Staufer</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S+T+Y+S">Santosh T.Y.S.S</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Heri%2C+C">Corina Heri</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11426" title="Abstract">arXiv:2310.11426</a> (replaced) [<a href="/pdf/2310.11426" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underwater and Surface Aquatic Locomotion of Soft Biomimetic Robot Based  on Bending Rolled Dielectric Elastomer Actuators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Juntian Qu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xiang Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages, 12 Figures, Published at IROS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11466" title="Abstract">arXiv:2310.11466</a> (replaced) [<a href="/pdf/2310.11466" title="Download PDF">pdf</a>, <a href="/format/2310.11466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Protein 3D Graph Structure Learning for Robust Structure-based Protein  Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jin Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+O">Odin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Jingqi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiangbin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan.ZQ.Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11526" title="Abstract">arXiv:2310.11526</a> (replaced) [<a href="/pdf/2310.11526" title="Download PDF">pdf</a>, <a href="/ps/2310.11526" title="Download PostScript">ps</a>, <a href="/format/2310.11526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Commitments from Quantum One-Wayness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Khurana%2C+D">Dakshita Khurana</a> (UIUC), 
<a href="/search/quant-ph?searchtype=author&query=Tomer%2C+K">Kabir Tomer</a> (UIUC)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 68 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11569" title="Abstract">arXiv:2310.11569</a> (replaced) [<a href="/e-print/2310.11569" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Rigidity Hurts: Soft Consistency Regularization for Probabilistic  Hierarchical Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamarthi%2C+H">Harshavardhan Kamarthi</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingkai Kong</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+A">Alexander Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> I have submitted this paper as a revision of <a href="/abs/2206.07940">arXiv:2206.07940</a>. Apologies for the confusion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11595" title="Abstract">arXiv:2310.11595</a> (replaced) [<a href="/pdf/2310.11595" title="Download PDF">pdf</a>, <a href="/format/2310.11595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks  Against Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+Z">Zhihao Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhiwei Ling</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xian Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingsong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11670" title="Abstract">arXiv:2310.11670</a> (replaced) [<a href="/pdf/2310.11670" title="Download PDF">pdf</a>, <a href="/format/2310.11670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11709" title="Abstract">arXiv:2310.11709</a> (replaced) [<a href="/pdf/2310.11709" title="Download PDF">pdf</a>, <a href="/format/2310.11709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with  NFT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bingqiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shengliang Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bingsheng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023, Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11762" title="Abstract">arXiv:2310.11762</a> (replaced) [<a href="/pdf/2310.11762" title="Download PDF">pdf</a>, <a href="/format/2310.11762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quasi-Wasserstein Loss for Learning Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Minjie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongteng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11804" title="Abstract">arXiv:2310.11804</a> (replaced) [<a href="/pdf/2310.11804" title="Download PDF">pdf</a>, <a href="/format/2310.11804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed Neural Network for Acoustic Resonance Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yokota%2C+K">Kazuya Yokota</a>, 
<a href="/search/cs?searchtype=author&query=Kurahashi%2C+T">Takahiko Kurahashi</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+M">Masajiro Abe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 14 figures. The following article has been submitted to the Journal of the Acoustical Society of America. After it is published, it will be found at <a href="https://pubs.aip.org/asa/jasa">this https URL</a> . v2: Corrected a typo in Eq. (22)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11878" title="Abstract">arXiv:2310.11878</a> (replaced) [<a href="/pdf/2310.11878" title="Download PDF">pdf</a>, <a href="/format/2310.11878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Dissonance to Insights: Dissecting Disagreements in Rationale  Construction for Case Outcome Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S+T+Y+S">Santosh T.Y.S.S</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Risini%2C+I">Isabella Risini</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11971" title="Abstract">arXiv:2310.11971</a> (replaced) [<a href="/pdf/2310.11971" title="Download PDF">pdf</a>, <a href="/format/2310.11971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Generalization of Alignment with Human Preferences through  Group Invariant Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yuan Hua</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+W">Wenbin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Z">Zhiheng Xi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haoran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12053" title="Abstract">arXiv:2310.12053</a> (replaced) [<a href="/pdf/2310.12053" title="Download PDF">pdf</a>, <a href="/format/2310.12053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rational function approximation with normalized positive denominators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chok%2C+J">James Chok</a>, 
<a href="/search/math?searchtype=author&query=Vasil%2C+G+M">Geoffrey M. Vasil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12060" title="Abstract">arXiv:2310.12060</a> (replaced) [<a href="/pdf/2310.12060" title="Download PDF">pdf</a>, <a href="/format/2310.12060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Class-Conditional Distribution Alignment for Partial Domain  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choudhuri%2C+S">Sandipan Choudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+A">Arunabha Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12069" title="Abstract">arXiv:2310.12069</a> (replaced) [<a href="/pdf/2310.12069" title="Download PDF">pdf</a>, <a href="/format/2310.12069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers for scientific data: a pedagogical review for astronomers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Tanoglidis%2C+D">Dimitrios Tanoglidis</a>, 
<a href="/search/astro-ph?searchtype=author&query=Jain%2C+B">Bhuvnesh Jain</a>, 
<a href="/search/astro-ph?searchtype=author&query=Qu%2C+H">Helen Qu</a> (University of Pennsylvania)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12142" title="Abstract">arXiv:2310.12142</a> (replaced) [<a href="/pdf/2310.12142" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electromechanical System Design for Self-Balancing Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Morshed%2C+M+A+A">Md. Abid Al Morshed</a>, 
<a href="/search/eess?searchtype=author&query=Hayder%2C+M+M">Md. Mustakim Hayder</a>, 
<a href="/search/eess?searchtype=author&query=Maruf%2C+T+R">Tayfur Rahman Maruf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item335">Cross-lists</a></li>
<li><a href="#item383">Replacements</a></li>
</ul>
<small>[ total of 638 entries:  <b>1-638</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
