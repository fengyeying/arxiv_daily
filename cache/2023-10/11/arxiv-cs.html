<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon  9 Oct 23  to  Tue 10 Oct 23, announced Wed, 11 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item399">Cross-lists</a></li>
<li><a href="#item453">Replacements</a></li>
</ul>
<small>[ total of 751 entries:  <b>1-751</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed, 11 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05929" title="Abstract">arXiv:2310.05929</a> [<a href="/pdf/2310.05929" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning based Tomato Disease Detection and Remedy Suggestions  using Mobile Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandeya%2C+Y+R">Yagya Raj Pandeya</a>, 
<a href="/search/cs?searchtype=author&query=Karki%2C+S">Samin Karki</a>, 
<a href="/search/cs?searchtype=author&query=Dangol%2C+I">Ishan Dangol</a>, 
<a href="/search/cs?searchtype=author&query=Rajbanshi%2C+N">Nitesh Rajbanshi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We have developed a comprehensive computer system to assist farmers who
practice traditional farming methods and have limited access to agricultural
experts for addressing crop diseases. Our system utilizes artificial
intelligence (AI) to identify and provide remedies for vegetable diseases. To
ensure ease of use, we have created a mobile application that offers a
user-friendly interface, allowing farmers to inquire about vegetable diseases
and receive suitable solutions in their local language. The developed system
can be utilized by any farmer with a basic understanding of a smartphone.
Specifically, we have designed an AI-enabled mobile application for identifying
and suggesting remedies for vegetable diseases, focusing on tomato diseases to
benefit the local farming community in Nepal. Our system employs
state-of-the-art object detection methodology, namely You Only Look Once
(YOLO), to detect tomato diseases. The detected information is then relayed to
the mobile application, which provides remedy suggestions guided by domain
experts. In order to train our system effectively, we curated a dataset
consisting of ten classes of tomato diseases. We utilized various data
augmentation methods to address overfitting and trained a YOLOv5 object
detector. The proposed method achieved a mean average precision of 0.76 and
offers an efficient mobile interface for interacting with the AI system. While
our system is currently in the development phase, we are actively working
towards enhancing its robustness and real-time usability by accumulating more
training samples.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05930" title="Abstract">arXiv:2310.05930</a> [<a href="/pdf/2310.05930" title="Download PDF">pdf</a>, <a href="/ps/2310.05930" title="Download PostScript">ps</a>, <a href="/format/2310.05930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of Clustered Phased Arrays by Means of an Innovative Power  Pattern Matching-Driven Method -- The Linear Array Case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benoni%2C+A">Arianna Benoni</a>, 
<a href="/search/cs?searchtype=author&query=Poli%2C+L">Lorenzo Poli</a>, 
<a href="/search/cs?searchtype=author&query=Rocca%2C+P">Paolo Rocca</a>, 
<a href="/search/cs?searchtype=author&query=Massa%2C+A">Andrea Massa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">The design of sub-arrayed phased arrays (PAs) with sub-array-only amplitude
and phase controls that afford arbitrary-shaped power patterns matching
reference ones is addressed. Such a synthesis problem is formulated in the
power pattern domain and an innovative complex-excitations clustering method,
which is based on the decomposition of the reference power pattern in a number
of elementary patterns equal to the array elements, is presented. A set of
representative results is reported to illustrate the features of the proposed
approach as well as to assess its effectiveness in comparison with benchmark
results from the state-of-the-art (SoA) excitation matching-based clustering
methods.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05932" title="Abstract">arXiv:2310.05932</a> [<a href="/pdf/2310.05932" title="Download PDF">pdf</a>, <a href="/format/2310.05932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Agent Systems Approach for Peer-to-Peer Energy Trading in Dairy  Farming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+M+I+A">Mian Ibad Ali Shah</a>, 
<a href="/search/cs?searchtype=author&query=Wahid%2C+A">Abdul Wahid</a>, 
<a href="/search/cs?searchtype=author&query=Barrett%2C+E">Enda Barrett</a>, 
<a href="/search/cs?searchtype=author&query=Mason%2C+K">Karl Mason</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proc. of the Artificial Intelligence for Sustainability, ECAI 2023, Eunika et al. (eds.), Sep 30- Oct 1, 2023, <a href="https://sites.google.com/view/ai4s.">this https URL</a> 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">To achieve desired carbon emission reductions, integrating renewable
generation and accelerating the adoption of peer-to-peer energy trading is
crucial. This is especially important for energy-intensive farming, like dairy
farming. However, integrating renewables and peer-to-peer trading presents
challenges. To address this, we propose the Multi-Agent Peer-to-Peer Dairy Farm
Energy Simulator (MAPDES), enabling dairy farms to participate in peer-to-peer
markets. Our strategy reduces electricity costs and peak demand by
approximately 30% and 24% respectively, while increasing energy sales by 37%
compared to the baseline scenario without P2P trading. This demonstrates the
effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05934" title="Abstract">arXiv:2310.05934</a> [<a href="/pdf/2310.05934" title="Download PDF">pdf</a>, <a href="/format/2310.05934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with  Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S+J">Se Jin Park</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Joanna Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+Y+M">Yong Man Ro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Speech-driven 3D facial animation has gained significant attention for its
ability to create realistic and expressive facial animations in 3D space based
on speech. Learning-based methods have shown promising progress in achieving
accurate facial motion synchronized with speech. However, one-to-many nature of
speech-to-3D facial synthesis has not been fully explored: while the lip
accurately synchronizes with the speech content, other facial attributes beyond
speech-related motions are variable with respect to the speech. To account for
the potential variance in the facial attributes within a single speech, we
propose DF-3DFace, a diffusion-driven speech-to-3D face mesh synthesis.
DF-3DFace captures the complex one-to-many relationships between speech and 3D
face based on diffusion. It concurrently achieves aligned lip motion by
exploiting audio-mesh synchronization and masked conditioning. Furthermore, the
proposed method jointly models identity and pose in addition to facial motions
so that it can generate 3D face animation without requiring a reference
identity mesh and produce natural head poses. We contribute a new large-scale
3D facial mesh dataset, 3D-HDTF to enable the synthesis of variations in
identities, poses, and facial motions of 3D face mesh. Extensive experiments
demonstrate that our method successfully generates highly variable facial
shapes and motions from speech and simultaneously achieves more realistic
facial animation than the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05935" title="Abstract">arXiv:2310.05935</a> [<a href="/pdf/2310.05935" title="Download PDF">pdf</a>, <a href="/format/2310.05935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vulnerability Clustering and other Machine Learning Applications of  Semantic Vulnerability Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stehr%2C+M">Mark-Oliver Stehr</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minyoung Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Cyber-security vulnerabilities are usually published in form of short natural
language descriptions (e.g., in form of MITRE's CVE list) that over time are
further manually enriched with labels such as those defined by the Common
Vulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and
Intelligence) project, we investigated different types of semantic
vulnerability embeddings based on natural language processing (NLP) techniques
to obtain a concise representation of the vulnerability space. We also
evaluated their use as a foundation for machine learning applications that can
support cyber-security researchers and analysts in risk assessment and other
related activities. The particular applications we explored and briefly
summarize in this report are clustering, classification, and visualization, as
well as a new logic-based approach to evaluate theories about the vulnerability
space.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05936" title="Abstract">arXiv:2310.05936</a> [<a href="/pdf/2310.05936" title="Download PDF">pdf</a>, <a href="/ps/2310.05936" title="Download PostScript">ps</a>, <a href="/format/2310.05936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technocracy, pseudoscience and performative compliance: the risks of  privacy risk assessments. Lessons from NIST&#x27;s Privacy Risk Assessment  Methodology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balsa%2C+E">Ero Balsa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working draft. A version of this paper was presented at the 16th International Conference on Computers, Privacy and Data Protection, May 24-26, 2023 in Brussels (Belgium)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Privacy risk assessments have been touted as an objective, principled way to
encourage organizations to implement privacy-by-design. They are central to a
new regulatory model of collaborative governance, as embodied by the GDPR.
However, existing guidelines and methods remain vague, and there is little
empirical evidence on privacy harms. In this paper we conduct a close analysis
of US NIST's Privacy Risk Assessment Methodology, highlighting multiple sites
of discretion that create countless opportunities for adversarial organizations
to engage in performative compliance. Our analysis shows that the premises on
which the success of privacy risk assessments depends do not hold, particularly
in regard to organizations' incentives and regulators auditing capabilities. We
highlight the limitations and pitfalls of what is essentially a utilitarian and
technocratic approach, leading us to discuss alternatives and a realignment of
our policy and research objectives.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05938" title="Abstract">arXiv:2310.05938</a> [<a href="/pdf/2310.05938" title="Download PDF">pdf</a>, <a href="/format/2310.05938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Component attention network for multimodal dance improvisation  recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jia Fu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiarui Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wenjie Yin</a>, 
<a href="/search/cs?searchtype=author&query=Pashami%2C+S">Sepideh Pashami</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rkman%2C+M">M&#xe5;rten Bj&#xf6;rkman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 25th ACM International Conference on Multimodal Interaction (ICMI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Dance improvisation is an active research topic in the arts. Motion analysis
of improvised dance can be challenging due to its unique dynamics. Data-driven
dance motion analysis, including recognition and generation, is often limited
to skeletal data. However, data of other modalities, such as audio, can be
recorded and benefit downstream tasks. This paper explores the application and
performance of multimodal fusion methods for human motion recognition in the
context of dance improvisation. We propose an attention-based model, component
attention network (CANet), for multimodal fusion on three levels: 1) feature
fusion with CANet, 2) model fusion with CANet and graph convolutional network
(GCN), and 3) late fusion with a voting strategy. We conduct thorough
experiments to analyze the impact of each modality in different fusion methods
and distinguish critical temporal or component features. We show that our
proposed model outperforms the two baseline methods, demonstrating its
potential for analyzing improvisation in dance.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05939" title="Abstract">arXiv:2310.05939</a> [<a href="/pdf/2310.05939" title="Download PDF">pdf</a>, <a href="/format/2310.05939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Cyber Defence Tactics from Scratch with Multi-Agent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiebe%2C+J">Jacob Wiebe</a>, 
<a href="/search/cs?searchtype=author&query=Mallah%2C+R+A">Ranwa Al Mallah</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at 2nd International Workshop on Adaptive Cyber Defense, 2023 (<a href="/abs/2308.09520">arXiv:2308.09520</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advancements in deep learning techniques have opened new possibilities
for designing solutions for autonomous cyber defence. Teams of intelligent
agents in computer network defence roles may reveal promising avenues to
safeguard cyber and kinetic assets. In a simulated game environment, agents are
evaluated on their ability to jointly mitigate attacker activity in host-based
defence scenarios. Defender systems are evaluated against heuristic attackers
with the goals of compromising network confidentiality, integrity, and
availability. Value-based Independent Learning and Centralized Training
Decentralized Execution (CTDE) cooperative Multi-Agent Reinforcement Learning
(MARL) methods are compared revealing that both approaches outperform a simple
multi-agent heuristic defender. This work demonstrates the ability of
cooperative MARL to learn effective cyber defence tactics against varied
threats.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05940" title="Abstract">arXiv:2310.05940</a> [<a href="/pdf/2310.05940" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic S-BOX using Chaotic Map for VPN Data Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ishaq%2C+K">Kashif Ishaq</a>, 
<a href="/search/cs?searchtype=author&query=Hassan%2C+K+A">Khwaja Ahmad Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Bhatti%2C+Y+T">Yahya Tauqeer Bhatti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">A dynamic SBox using a chaotic map is a cryptography technique that changes
the SBox during encryption based on iterations of a chaotic map, adding an
extra layer of confusion and security to symmetric encryption algorithms like
AES. The chaotic map introduces unpredictability, non-linearity, and key
dependency, enhancing the overall security of the encryption process. The
existing work on dynamic SBox using chaotic maps lacks standardized guidelines
and extensive security analysis, leaving potential vulnerabilities and
performance concerns unaddressed. Key management and the sensitivity of chaotic
maps to initial conditions are challenges that need careful consideration. The
main objective of using a dynamic SBox with a chaotic map in cryptography
systems is to enhance the security and robustness of symmetric encryption
algorithms. The method of dynamic SBox using a chaotic map involves
initializing the SBox, selecting a chaotic map, iterating the map to generate
chaotic values, and updating the SBox based on these values during the
encryption process to enhance security and resist cryptanalytic attacks. This
article proposes a novel chaotic map that can be utilized to create a fresh,
lively SBox. The performance assessment of the suggested S resilience Box
against various attacks involves metrics such as nonlinearity (NL), strict
avalanche criterion (SAC), bit independence criterion (BIC), linear
approximation probability (LP), and differential approximation probability
(DP). These metrics help gauge the Box ability to handle and respond to
different attack scenarios. Assess the cryptography strength of the proposed
S-Box for usage in practical security applications, it is compared to other
recently developed SBoxes. The comparative research shows that the suggested
SBox has the potential to be an important advancement in the field of data
security.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05941" title="Abstract">arXiv:2310.05941</a> [<a href="/pdf/2310.05941" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Ripple Effect of Retraction on an Author&#x27;s Collaboration Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Kiran Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Satyam Mukherjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">Scientists involved in scientific misconduct may face social stigmatization,
leading to isolation and limited opportunities for collaboration. The
reputation of every individual is reflected on the team, as the fraud attempted
by any member will be reflected on the team. Earlier studies pointed out the
impact of citation penalty on the prior work of coauthors, the effect of
retraction on a co-author's research career, and stigmatization through mere
association. This paper explores the formation and dynamics of the networks of
authors who faced retractions and their "innocent coauthors" who never faced
retractions in their careers. Leveraging a dataset of 5972 retracted papers
involving 24209 authors, we investigate whether scientific misconduct reduces
collaborative ties of misconducting authors as opposed to those who never faced
allegations of academic misconduct. We observe that the network structure of
authors involved in retractions does not change significantly over the years
compared to that of the "innocent coauthors". Our results suggest that
stigmatization rarely affects the collaboration network of stigmatized authors.
Our findings have implications for institutions adopting stringent measures and
fostering ethical practices research.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05942" title="Abstract">arXiv:2310.05942</a> [<a href="/pdf/2310.05942" title="Download PDF">pdf</a>, <a href="/format/2310.05942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transactive Multi-Agent Systems over Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yijun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+Z">Zeinab Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Ratnam%2C+E+L">Elizabeth L. Ratnam</a>, 
<a href="/search/cs?searchtype=author&query=Petersen%2C+I+R">Ian R. Petersen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guodong Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">This paper presented insights into the implementation of transactive
multi-agent systems over flow networks where local resources are decentralized.
Agents have local resource demand and supply, and are interconnected through a
flow network to support the sharing of local resources while respecting
restricted sharing/flow capacity. We first establish a competitive market with
a pricing mechanism that internalizes flow capacity constraints into agents'
private decisions. We then demonstrate through duality theory that competitive
equilibrium and social welfare equilibrium exist and agree under convexity
assumptions, indicating the efficiency of the pricing mechanism. Additionally,
a new social acceptance sharing problem is defined to investigate homogeneous
pricing when the optimal sharing prices at all agents under competitive
equilibrium are always equal for social acceptance. A conceptual computation
method is proposed, prescribing a class of socially admissible utility
functions to solve the social acceptance problem. A special case of
linear-quadratic multi-agent systems over undirected star graphs is provided as
a pedagogical example of how to explicitly prescribe socially admissible
utility functions. Finally, extensive experiments are provided to validate the
results.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05943" title="Abstract">arXiv:2310.05943</a> [<a href="/pdf/2310.05943" title="Download PDF">pdf</a>, <a href="/format/2310.05943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Learned Features and Framework for Potato Disease Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Shikha Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Soma Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Rameshan%2C+R">Renu Rameshan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">For applications like plant disease detection, usually, a model is trained on
publicly available data and tested on field data. This means that the test data
distribution is not the same as the training data distribution, which affects
the classifier performance adversely. We handle this dataset shift by ensuring
that the features are learned from disease spots in the leaf or healthy
regions, as applicable. This is achieved using a faster Region-based
convolutional neural network (RCNN) as one of the solutions and an
attention-based network as the other. The average classification accuracies of
these classifiers are approximately 95% while evaluated on the test set
corresponding to their training dataset. These classifiers also performed
equivalently, with an average score of 84% on a dataset not seen during the
training phase.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05947" title="Abstract">arXiv:2310.05947</a> [<a href="/pdf/2310.05947" title="Download PDF">pdf</a>, <a href="/format/2310.05947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and Efficient Interference Neural Networks for Defending Against  Adversarial Attacks in ImageNet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yunuo Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shujuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hongwei Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The existence of adversarial images has seriously affected the task of image
recognition and practical application of deep learning, it is also a key
scientific problem that deep learning urgently needs to solve. By far the most
effective approach is to train the neural network with a large number of
adversarial examples. However, this adversarial training method requires a huge
amount of computing resources when applied to ImageNet, and has not yet
achieved satisfactory results for high-intensity adversarial attacks. In this
paper, we construct an interference neural network by applying additional
background images and corresponding labels, and use pre-trained ResNet-152 to
efficiently complete the training. Compared with the state-of-the-art results
under the PGD attack, it has a better defense effect with much smaller
computing resources. This work provides new ideas for academic research and
practical applications of effective defense against adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05951" title="Abstract">arXiv:2310.05951</a> [<a href="/pdf/2310.05951" title="Download PDF">pdf</a>, <a href="/format/2310.05951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing the False Positive Rate Using Bayesian Inference in Autonomous  Driving Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bastos%2C+J+J+S">Johann J. S. Bastos</a>, 
<a href="/search/cs?searchtype=author&query=da+Silva%2C+B+L+S">Bruno L. S. da Silva</a>, 
<a href="/search/cs?searchtype=author&query=Zanotelli%2C+T">Tiago Zanotelli</a>, 
<a href="/search/cs?searchtype=author&query=Premebida%2C+C">Cristiano Premebida</a>, 
<a href="/search/cs?searchtype=author&query=Melotti%2C+G">Gledson Melotti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Object recognition is a crucial step in perception systems for autonomous and
intelligent vehicles, as evidenced by the numerous research works in the topic.
In this paper, object recognition is explored by using multisensory and
multimodality approaches, with the intention of reducing the false positive
rate (FPR). The reduction of the FPR becomes increasingly important in
perception systems since the misclassification of an object can potentially
cause accidents. In particular, this work presents a strategy through Bayesian
inference to reduce the FPR considering the likelihood function as a cumulative
distribution function from Gaussian kernel density estimations, and the prior
probabilities as cumulative functions of normalized histograms. The validation
of the proposed methodology is performed on the KITTI dataset using deep
networks (DenseNet, NasNet, and EfficientNet), and recent 3D point cloud
networks (PointNet, and PintNet++), by considering three object-categories
(cars, cyclists, pedestrians) and the RGB and LiDAR sensor modalities.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05952" title="Abstract">arXiv:2310.05952</a> [<a href="/pdf/2310.05952" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Denial of Service Attacks in Fog-Based Wireless Sensor  Networks Using Machine Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abidoye%2C+A">Ademola Abidoye</a>, 
<a href="/search/cs?searchtype=author&query=Obagbuwa%2C+I">Ibidun Obagbuwa</a>, 
<a href="/search/cs?searchtype=author&query=Azeez%2C+N">Nureni Azeez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Wireless sensor networks are considered to be among the most significant and
innovative technologies in the 21st century due to their wide range of
industrial applications. Sensor nodes in these networks are susceptible to a
variety of assaults due to their special qualities and method of deployment. In
WSNs, denial of service attacks are common attacks in sensor networks. It is
difficult to design a detection and prevention system that would effectively
reduce the impact of these attacks on WSNs. In order to identify assaults on
WSNs, this study suggests using two machine learning models: decision trees and
XGBoost. The WSNs dataset was the subject of extensive tests to identify denial
of service attacks. The experimental findings demonstrate that the XGBoost
model, when applied to the entire dataset, has a higher true positive rate
(98.3%) than the Decision tree approach (97.3%) and a lower false positive rate
(1.7%) than the Decision tree technique (2.7%). Like this, with selected
dataset assaults, the XGBoost approach has a higher true positive rate (99.01%)
than the Decision tree technique (97.50%) and a lower false positive rate
(0.99%) than the Decision tree technique (2.50%).
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05953" title="Abstract">arXiv:2310.05953</a> [<a href="/pdf/2310.05953" title="Download PDF">pdf</a>, <a href="/format/2310.05953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification of Spam URLs Using Machine Learning Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Odeh%2C+O+H">Omar Husni Odeh</a>, 
<a href="/search/cs?searchtype=author&query=Arram%2C+A">Anas Arram</a>, 
<a href="/search/cs?searchtype=author&query=Njoum%2C+M">Murad Njoum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Internet is used by billions of users daily because it offers fast and
free communication tools and platforms. Nevertheless, with this significant
increase in usage, huge amounts of spam are generated every second, which
wastes internet resources and, more importantly, users time. This study
investigates using machine learning models to classify URLs as spam or
non-spam. We first extract the features from the URL as it has only one
feature, and then we compare the performance of several models, including
k-nearest neighbors, bagging, random forest, logistic regression, and others.
We find that bagging achieves the best accuracy, with an accuracy of 96.5%.
This suggests that bagging is a promising approach for classifying URLs as spam
or nonspam.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05956" title="Abstract">arXiv:2310.05956</a> [<a href="/pdf/2310.05956" title="Download PDF">pdf</a>, <a href="/format/2310.05956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Network Representation for GNN-based Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Friji%2C+H">Hamdi Friji</a>, 
<a href="/search/cs?searchtype=author&query=Olivereau%2C+A">Alexis Olivereau</a>, 
<a href="/search/cs?searchtype=author&query=Sarkiss%2C+M">Mireille Sarkiss</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Tibouchi, M., Wang, X. (eds) Applied Cryptography and Network
  Security. ACNS 2023. Lecture Notes in Computer Science, vol 13905. Springer,
  Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The last decades have seen a growth in the number of cyber-attacks with
severe economic and privacy damages, which reveals the need for network
intrusion detection approaches to assist in preventing cyber-attacks and
reducing their risks. In this work, we propose a novel network representation
as a graph of flows that aims to provide relevant topological information for
the intrusion detection task, such as malicious behavior patterns, the relation
between phases of multi-step attacks, and the relation between spoofed and
pre-spoofed attackers activities. In addition, we present a Graph Neural
Network (GNN) based framework responsible for exploiting the proposed graph
structure to classify communication flows by assigning them a maliciousness
score. The framework comprises three main steps that aim to embed nodes
features and learn relevant attack patterns from the network representation.
Finally, we highlight a potential data leakage issue with classical evaluation
procedures and suggest a solution to ensure a reliable validation of intrusion
detection systems performance. We implement the proposed framework and prove
that exploiting the flow-based graph structure outperforms the classical
machine learning-based and the previous GNN-based solutions.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05959" title="Abstract">arXiv:2310.05959</a> [<a href="/pdf/2310.05959" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating global landslide detection with heterogeneous ensemble  deep-learning classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganer%C3%B8d%2C+A+J">Alexandra Jarna Ganer&#xf8;d</a>, 
<a href="/search/cs?searchtype=author&query=Franch%2C+G">Gabriele Franch</a>, 
<a href="/search/cs?searchtype=author&query=Lindsay%2C+E">Erin Lindsay</a>, 
<a href="/search/cs?searchtype=author&query=Calovi%2C+M">Martina Calovi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Author 1 and Author 2 contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With changing climatic conditions, we are already seeing an increase in
extreme weather events and their secondary consequences, including landslides.
Landslides threaten infrastructure, including roads, railways, buildings, and
human life. Hazard-based spatial planning and early warning systems are
cost-effective strategies to reduce the risk to society from landslides.
However, these both rely on data from previous landslide events, which is often
scarce. Many deep learning (DL) models have recently been applied for landside
mapping using medium- to high-resolution satellite images as input. However,
they often suffer from sensitivity problems, overfitting, and low mapping
accuracy. This study addresses some of these limitations by using a diverse
global landslide dataset, using different segmentation models, such as Unet,
Linknet, PSP-Net, PAN, and DeepLab and based on their performances, building an
ensemble model. The ensemble model achieved the highest F1-score (0.69) when
combining both Sentinel-1 and Sentinel-2 bands, with the highest average
improvement of 6.87 % when the ensemble size was 20. On the other hand,
Sentinel-2 bands only performed very well, with an F1 score of 0.61 when the
ensemble size is 20 with an improvement of 14.59 % when the ensemble size is
20. This result shows considerable potential in building a robust and reliable
monitoring system based on changes in vegetation index dNDVI only.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05960" title="Abstract">arXiv:2310.05960</a> [<a href="/pdf/2310.05960" title="Download PDF">pdf</a>, <a href="/format/2310.05960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fingerprint Attack: Client De-Anonymization in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiongkai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cohn%2C+T">Trevor Cohn</a>, 
<a href="/search/cs?searchtype=author&query=Ohrimenko%2C+O">Olga Ohrimenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated Learning allows collaborative training without data sharing in
settings where participants do not trust the central server and one another.
Privacy can be further improved by ensuring that communication between the
participants and the server is anonymized through a shuffle; decoupling the
participant identity from their data. This paper seeks to examine whether such
a defense is adequate to guarantee anonymity, by proposing a novel
fingerprinting attack over gradients sent by the participants to the server. We
show that clustering of gradients can easily break the anonymization in an
empirical study of learning federated language models on two language corpora.
We then show that training with differential privacy can provide a practical
defense against our fingerprint attack.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05962" title="Abstract">arXiv:2310.05962</a> [<a href="/pdf/2310.05962" title="Download PDF">pdf</a>, <a href="/format/2310.05962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Performance of R17 Type-II Codebook with Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Ke Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+Y">Yiliang Sang</a>, 
<a href="/search/cs?searchtype=author&query=Ming%2C+Y">Yang Ming</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jin Lian</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+C">Chang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaocheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE GLOBECOM 2023, conference version of Arxiv:<a href="/abs/2305.08081">2305.08081</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">The Type-II codebook in Release 17 (R17) exploits the angular-delay-domain
partial reciprocity between uplink and downlink channels to select part of
angular-delay-domain ports for measuring and feeding back the downlink channel
state information (CSI), where the performance of existing deep learning
enhanced CSI feedback methods is limited due to the deficiency of sparse
structures. To address this issue, we propose two new perspectives of adopting
deep learning to improve the R17 Type-II codebook. Firstly, considering the low
signal-to-noise ratio of uplink channels, deep learning is utilized to
accurately select the dominant angular-delay-domain ports, where the focal loss
is harnessed to solve the class imbalance problem. Secondly, we propose to
adopt deep learning to reconstruct the downlink CSI based on the feedback of
the R17 Type-II codebook at the base station, where the information of sparse
structures can be effectively leveraged. Besides, a weighted shortcut module is
designed to facilitate the accurate reconstruction. Simulation results
demonstrate that our proposed methods could improve the sum rate performance
compared with its traditional R17 Type-II codebook and deep learning
benchmarks.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05963" title="Abstract">arXiv:2310.05963</a> [<a href="/pdf/2310.05963" title="Download PDF">pdf</a>, <a href="/format/2310.05963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFDBench: A Comprehensive Benchmark for Machine Learning Methods in  Fluid Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yining Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingfa Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 11 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">In recent years, applying deep learning to solve physics problems has
attracted much attention. Data-driven deep learning methods produce operators
that can learn solutions to the whole system of partial differential equations.
However, the existing methods are only evaluated on simple flow equations
(e.g., Burger's equation), and only consider the generalization ability on
different initial conditions. In this paper, we construct CFDBench, a benchmark
with four classic problems in computational fluid dynamics (CFD): lid-driven
cavity flow, laminar boundary layer flow in circular tubes, dam flows through
the steps, and periodic Karman vortex street. Each flow problem includes data
with different boundary conditions, fluid physical properties, and domain
geometry. Compared to existing datasets, the advantages of CFDBench are (1)
comprehensive. It contains common physical parameters such as velocity,
pressure, and cavity fraction. (2) realistic. It is very suitable for deep
learning solutions of fluid mechanics equations. (3) challenging. It has a
certain learning difficulty, prompting to find models with strong learning
ability. (4) standardized. CFDBench facilitates a comprehensive and fair
comparison of different deep learning methods for CFD. We make appropriate
modifications to popular deep neural networks to apply them to CFDBench and
enable the accommodation of more changing inputs. The evaluation on CFDBench
reveals some new shortcomings of existing works and we propose possible
directions for solving such problems.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05964" title="Abstract">arXiv:2310.05964</a> [<a href="/pdf/2310.05964" title="Download PDF">pdf</a>, <a href="/format/2310.05964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Embeddings for Measuring Text Relatedness: Unveiling  Sentiments and Relationships in Online Comments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olakangil%2C+A">Anthony Olakangil</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cindy Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+J">Justin Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qunbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jethwa%2C+K">Kaavya Jethwa</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jason Li</a>, 
<a href="/search/cs?searchtype=author&query=Narendra%2C+A">Aryan Narendra</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nishk Patel</a>, 
<a href="/search/cs?searchtype=author&query=Rajaram%2C+A">Arjun Rajaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 3 tables, to be published in the Second International Conference on Informatics (ICI-2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">After a pandemic that caused internet usage to grow by 70%, there has been an
increased number of people all across the world using social media.
Applications like Twitter, Meta Threads, YouTube, and Reddit have become
increasingly pervasive, leaving almost no digital space where public opinion is
not expressed. This paper investigates sentiment and semantic relationships
among comments across various social media platforms, as well as discusses the
importance of shared opinions across these different media platforms, using
word embeddings to analyze components in sentences and documents. It allows
researchers, politicians, and business representatives to trace a path of
shared sentiment among users across the world. This research paper presents
multiple approaches that measure the relatedness of text extracted from user
comments on these popular online platforms. By leveraging embeddings, which
capture semantic relationships between words and help analyze sentiments across
the web, we can uncover connections regarding public opinion as a whole. The
study utilizes pre-existing datasets from YouTube, Reddit, Twitter, and more.
We made use of popular natural language processing models like Bidirectional
Encoder Representations from Transformers (BERT) to analyze sentiments and
explore relationships between comment embeddings. Additionally, we aim to
utilize clustering and Kl-divergence to find semantic relationships within
these comment embeddings across various social media platforms. Our analysis
will enable a deeper understanding of the interconnectedness of online comments
and will investigate the notion of the internet functioning as a large
interconnected brain.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05972" title="Abstract">arXiv:2310.05972</a> [<a href="/pdf/2310.05972" title="Download PDF">pdf</a>, <a href="/format/2310.05972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normality of I-V Measurements Using ML
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Najjar%2C+A">Anees Al-Najjar</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+N+S+V">Nageswara S. V. Rao</a>, 
<a href="/search/cs?searchtype=author&query=Bridges%2C+C+A">Craig A. Bridges</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+S">Sheng Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published at eScience 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in 2023 IEEE 19th International Conference on e-Science
  (e-Science), Limassol, Cyprus, 2023 pp. 1-2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
<p class="mathjax">Electrochemistry ecosystems are promising for accelerating the design and
discovery of electrochemical systems for energy storage and conversion, by
automating significant parts of workflows that combine synthesis and
characterization experiments with computations. They require the integration of
flow controllers, solvent containers, pumps, fraction collectors, and
potentiostats, all connected to an electrochemical cell. These are specialized
instruments with custom software that is not originally designed for network
integration. We developed network and software solutions for electrochemical
workflows that adapt system and instrument settings in real-time for multiple
rounds of experiments. We demonstrate this automated workflow by remotely
operating the instruments and collecting their measurements to generate a
voltammogram (I-V profile) of an electrolyte solution in an electrochemical
cell. These measurements are made available at the remote computing system and
used for subsequent analysis. In this paper, we focus on a novel, analytically
validated machine learning (ML) method for an electrochemistry ecosystem to
ensure that I-V measurements are consistent with the normal experimental
conditions, and to detect abnormal conditions, such as disconnected electrodes
or low cell content volume.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05978" title="Abstract">arXiv:2310.05978</a> [<a href="/pdf/2310.05978" title="Download PDF">pdf</a>, <a href="/format/2310.05978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Key Users&#x27; behavior trends in Volunteer-Based Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piterman%2C+N">Nofar Piterman</a>, 
<a href="/search/cs?searchtype=author&query=Makov%2C+T">Tamar Makov</a>, 
<a href="/search/cs?searchtype=author&query=Fire%2C+M">Michael Fire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Online social networks usage has increased significantly in the last decade
and continues to grow in popularity. Multiple social platforms use volunteers
as a central component. The behavior of volunteers in volunteer-based networks
has been studied extensively in recent years. Here, we explore the development
of volunteer-based social networks, primarily focusing on their key users'
behaviors and activities. We developed two novel algorithms: the first reveals
key user behavior patterns over time; the second utilizes machine learning
methods to generate a forecasting model that can predict the future behavior of
key users, including whether they will remain active donors or change their
behavior to become mainly recipients, and vice-versa. These algorithms allowed
us to analyze the factors that significantly influence behavior predictions.
<br />To evaluate our algorithms, we utilized data from over 2.4 million users on a
peer-to-peer food-sharing online platform. Using our algorithm, we identified
four main types of key user behavior patterns that occur over time. Moreover,
we succeeded in forecasting future active donor key users and predicting the
key users that would change their behavior to donors, with an accuracy of up to
89.6%. These findings provide valuable insights into the behavior of key users
in volunteer-based social networks and pave the way for more effective
communities-building in the future, while using the potential of machine
learning for this goal.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05982" title="Abstract">arXiv:2310.05982</a> [<a href="/pdf/2310.05982" title="Download PDF">pdf</a>, <a href="/ps/2310.05982" title="Download PostScript">ps</a>, <a href="/format/2310.05982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On matrix rank function over bounded arithmetics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ken%2C+E">Eitetsu Ken</a>, 
<a href="/search/cs?searchtype=author&query=Kuroda%2C+S">Satoru Kuroda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, no figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">In <a href="/abs/1811.04313">arXiv:1811.04313</a>, a definition of determinant is formalized in the bounded
arithmetic $VNC^{2}$. Following the presentation of [Gathen, 1993], we can
formalize a definition of matrix rank in the same bounded arithmetic. In this
article, we define a bounded arithmetic $LAPPD$, and show that $LAPPD$ seems to
be a natural arithmetic theory formalizing the treatment of rank function
following Mulmuley's algorithm. Furthermore, we give a formalization of rank in
$VNC^{2}$ by interpreting $LAPPD$ by $VNC^{2}$.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05984" title="Abstract">arXiv:2310.05984</a> [<a href="/pdf/2310.05984" title="Download PDF">pdf</a>, <a href="/format/2310.05984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulating Social Media Using Large Language Models to Evaluate  Alternative News Feed Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T%C3%B6rnberg%2C+P">Petter T&#xf6;rnberg</a>, 
<a href="/search/cs?searchtype=author&query=Valeeva%2C+D">Diliara Valeeva</a>, 
<a href="/search/cs?searchtype=author&query=Uitermark%2C+J">Justus Uitermark</a>, 
<a href="/search/cs?searchtype=author&query=Bail%2C+C">Christopher Bail</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Social media is often criticized for amplifying toxic discourse and
discouraging constructive conversations. But designing social media platforms
to promote better conversations is inherently challenging. This paper asks
whether simulating social media through a combination of Large Language Models
(LLM) and Agent-Based Modeling can help researchers study how different news
feed algorithms shape the quality of online conversations. We create realistic
personas using data from the American National Election Study to populate
simulated social media platforms. Next, we prompt the agents to read and share
news articles - and like or comment upon each other's messages - within three
platforms that use different news feed algorithms. In the first platform, users
see the most liked and commented posts from users whom they follow. In the
second, they see posts from all users - even those outside their own network.
The third platform employs a novel "bridging" algorithm that highlights posts
that are liked by people with opposing political views. We find this bridging
algorithm promotes more constructive, non-toxic, conversation across political
divides than the other two models. Though further research is needed to
evaluate these findings, we argue that LLMs hold considerable potential to
improve simulation research on social media and many other complex social
settings.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05986" title="Abstract">arXiv:2310.05986</a> [<a href="/pdf/2310.05986" title="Download PDF">pdf</a>, <a href="/format/2310.05986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Unreasonable Effectiveness of Linear Prediction as a Perceptual  Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Severo%2C+D">Daniel Severo</a>, 
<a href="/search/cs?searchtype=author&query=Theis%2C+L">Lucas Theis</a>, 
<a href="/search/cs?searchtype=author&query=Ball%C3%A9%2C+J">Johannes Ball&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We show how perceptual embeddings of the visual system can be constructed at
inference-time with no training data or deep neural network features. Our
perceptual embeddings are solutions to a weighted least squares (WLS) problem,
defined at the pixel-level, and solved at inference-time, that can capture
global and local image characteristics. The distance in embedding space is used
to define a perceptual similarity metric which we call LASI: Linear
Autoregressive Similarity Index. Experiments on full-reference image quality
assessment datasets show LASI performs competitively with learned deep feature
based methods like LPIPS (Zhang et al., 2018) and PIM (Bhardwaj et al., 2020),
at a similar computational cost to hand-crafted methods such as MS-SSIM (Wang
et al., 2003). We found that increasing the dimensionality of the embedding
space consistently reduces the WLS loss while increasing performance on
perceptual tasks, at the cost of increasing the computational complexity. LASI
is fully differentiable, scales cubically with the number of embedding
dimensions, and can be parallelized at the pixel-level. A Maximum
Differentiation (MAD) competition (Wang &amp; Simoncelli, 2008) between LASI and
LPIPS shows that both methods are capable of finding failure points for the
other, suggesting these metrics can be combined.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05988" title="Abstract">arXiv:2310.05988</a> [<a href="/pdf/2310.05988" title="Download PDF">pdf</a>, <a href="/format/2310.05988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dual Latent State Learning Approach: Exploiting Regional Network  Similarities for QoS Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziliang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaohong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Meng Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Individual objects, whether users or services, within a specific region often
exhibit similar network states due to their shared origin from the same city or
autonomous system (AS). Despite this regional network similarity, many existing
techniques overlook its potential, resulting in subpar performance arising from
challenges such as data sparsity and label imbalance. In this paper, we
introduce the regional-based dual latent state learning network(R2SL), a novel
deep learning framework designed to overcome the pitfalls of traditional
individual object-based prediction techniques in Quality of Service (QoS)
prediction. Unlike its predecessors, R2SL captures the nuances of regional
network behavior by deriving two distinct regional network latent states: the
city-network latent state and the AS-network latent state. These states are
constructed utilizing aggregated data from common regions rather than
individual object data. Furthermore, R2SL adopts an enhanced Huber loss
function that adjusts its linear loss component, providing a remedy for
prevalent label imbalance issues. To cap off the prediction process, a
multi-scale perception network is leveraged to interpret the integrated feature
map, a fusion of regional network latent features and other pertinent
information, ultimately accomplishing the QoS prediction. Through rigorous
testing on real-world QoS datasets, R2SL demonstrates superior performance
compared to prevailing state-of-the-art methods. Our R2SL approach ushers in an
innovative avenue for precise QoS predictions by fully harnessing the regional
network similarities inherent in objects.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05989" title="Abstract">arXiv:2310.05989</a> [<a href="/pdf/2310.05989" title="Download PDF">pdf</a>, <a href="/format/2310.05989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DynamicBEV: Leveraging Dynamic Queries and Temporal Context for 3D  Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiawei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yingxin Lai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D object detection is crucial for applications like autonomous driving and
robotics. While query-based 3D object detection for BEV (Bird's Eye View)
images has seen significant advancements, most existing methods follows the
paradigm of static query. Such paradigm is incapable of adapting to complex
spatial-temporal relationships in the scene. To solve this problem, we
introduce a new paradigm in DynamicBEV, a novel approach that employs dynamic
queries for BEV-based 3D object detection. In contrast to static queries, the
proposed dynamic queries exploit K-means clustering and Top-K Attention in a
creative way to aggregate information more effectively from both local and
distant feature, which enable DynamicBEV to adapt iteratively to complex
scenes. To further boost efficiency, DynamicBEV incorporates a Lightweight
Temporal Fusion Module (LTFM), designed for efficient temporal context
integration with a significant computation reduction. Additionally, a
custom-designed Diversity Loss ensures a balanced feature representation across
scenarios. Extensive experiments on the nuScenes dataset validate the
effectiveness of DynamicBEV, establishing a new state-of-the-art and heralding
a paradigm-level breakthrough in query-based BEV object detection.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05991" title="Abstract">arXiv:2310.05991</a> [<a href="/pdf/2310.05991" title="Download PDF">pdf</a>, <a href="/format/2310.05991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Document-level Event Argument Extraction with Contextual Clues  and Role Relevance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wanlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shaohuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dingyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+H">Hong Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of ACL 2023. arXiv admin note: text overlap with <a href="/abs/2310.05116">arXiv:2310.05116</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Document-level event argument extraction poses new challenges of long input
and cross-sentence inference compared to its sentence-level counterpart.
However, most prior works focus on capturing the relations between candidate
arguments and the event trigger in each event, ignoring two crucial points: a)
non-argument contextual clue information; b) the relevance among argument
roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling
and latent Role Guidance) model, which contains two novel and effective modules
for the above problem. The Span-Trigger-based Contextual Pooling(STCP)
adaptively selects and aggregates the information of non-argument clue words
based on the context attention weights of specific argument-trigger pairs from
pre-trained model. The Role-based Latent Information Guidance (RLIG) module
constructs latent role representations, makes them interact through
role-interactive encoding to capture semantic relevance, and merges them into
candidate arguments. Both STCP and RLIG introduce no more than 1% new
parameters compared with the base model and can be easily applied to other
event extraction models, which are compact and transplantable. Experiments on
two public datasets show that our SCPRG outperforms previous state-of-the-art
methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents
respectively. Further analyses illustrate the interpretability of our model.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05993" title="Abstract">arXiv:2310.05993</a> [<a href="/pdf/2310.05993" title="Download PDF">pdf</a>, <a href="/format/2310.05993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring reasoning capabilities of ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Groza%2C+A">Adrian Groza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">I shall quantify the logical faults generated by ChatGPT when applied to
reasoning tasks. For experiments, I use the 144 puzzles from the library
\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\cite{groza:fol}. The
library contains puzzles of various types, including arithmetic puzzles,
logical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling
puzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct
solutions for these puzzles were checked using the theorem prover
Prover9~\cite{mccune2005release} and the finite models finder
Mace4~\cite{mccune2003mace4} based on human-modelling in Equational First Order
Logic. A first output of this study is the benchmark of 100 logical puzzles.
For this dataset ChatGPT provided both correct answer and justification for 7\%
only. %, while BARD for 5\%. Since the dataset seems challenging, the
researchers are invited to test the dataset on more advanced or tuned models
than ChatGPT3.5 with more crafted prompts. A second output is the
classification of reasoning faults conveyed by ChatGPT. This classification
forms a basis for a taxonomy of reasoning faults generated by large language
models. I have identified 67 such logical faults, among which: inconsistencies,
implication does not hold, unsupported claim, lack of commonsense, wrong
justification. The 100 solutions generated by ChatGPT contain 698 logical
faults. That is on average, 7 fallacies for each reasoning task. A third ouput
is the annotated answers of the ChatGPT with the corresponding logical faults.
Each wrong statement within the ChatGPT answer was manually annotated, aiming
to quantify the amount of faulty text generated by the language model. On
average, 26.03\% from the generated text was a logical fault.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05995" title="Abstract">arXiv:2310.05995</a> [<a href="/pdf/2310.05995" title="Download PDF">pdf</a>, <a href="/format/2310.05995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y+E">Yixuan Even Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jecmen%2C+S">Steven Jecmen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zimeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 8 figures, 3 tables, neurips 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">The assignment of papers to reviewers is a crucial part of the peer review
processes of large publication venues, where organizers (e.g., conference
program chairs) rely on algorithms to perform automated paper assignment. As
such, a major challenge for the organizers of these processes is to specify
paper assignment algorithms that find appropriate assignments with respect to
various desiderata. Although the main objective when choosing a good paper
assignment is to maximize the expertise of each reviewer for their assigned
papers, several other considerations make introducing randomization into the
paper assignment desirable: robustness to malicious behavior, the ability to
evaluate alternative paper assignments, reviewer diversity, and reviewer
anonymity. However, it is unclear in what way one should randomize the paper
assignment in order to best satisfy all of these considerations simultaneously.
In this work, we present a practical, one-size-fits-all method for randomized
paper assignment intended to perform well across different motivations for
randomness. We show theoretically and experimentally that our method
outperforms currently-deployed methods for randomized paper assignment on
several intuitive randomness metrics, demonstrating that the randomized
assignments produced by our method are general-purpose.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05996" title="Abstract">arXiv:2310.05996</a> [<a href="/pdf/2310.05996" title="Download PDF">pdf</a>, <a href="/format/2310.05996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel Network Science Algorithm for Improving Triage of Patients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guzzi%2C+P+H">Pietro Hiram Guzzi</a>, 
<a href="/search/cs?searchtype=author&query=De+Filippo%2C+A">Annamaria De Filippo</a>, 
<a href="/search/cs?searchtype=author&query=Veltri%2C+P">Pierangelo Veltri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Patient triage plays a crucial role in healthcare, ensuring timely and
appropriate care based on the urgency of patient conditions. Traditional triage
methods heavily rely on human judgment, which can be subjective and prone to
errors. Recently, a growing interest has been in leveraging artificial
intelligence (AI) to develop algorithms for triaging patients. This paper
presents the development of a novel algorithm for triaging patients. It is
based on the analysis of patient data to produce decisions regarding their
prioritization. The algorithm was trained on a comprehensive data set
containing relevant patient information, such as vital signs, symptoms, and
medical history. The algorithm was designed to accurately classify patients
into triage categories through rigorous preprocessing and feature engineering.
Experimental results demonstrate that our algorithm achieved high accuracy and
performance, outperforming traditional triage methods. By incorporating
computer science into the triage process, healthcare professionals can benefit
from improved efficiency, accuracy, and consistency, prioritizing patients
effectively and optimizing resource allocation. Although further research is
needed to address challenges such as biases in training data and model
interpretability, the development of AI-based algorithms for triaging patients
shows great promise in enhancing healthcare delivery and patient outcomes.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05998" title="Abstract">arXiv:2310.05998</a> [<a href="/pdf/2310.05998" title="Download PDF">pdf</a>, <a href="/format/2310.05998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Skills Do You Need When Developing Software Using ChatGPT?  (Discussion Paper)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeuring%2C+J">Johan Jeuring</a>, 
<a href="/search/cs?searchtype=author&query=Groot%2C+R">Roel Groot</a>, 
<a href="/search/cs?searchtype=author&query=Keuning%2C+H">Hieke Keuning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Since the release of LLM-based tools such as GitHub Copilot and ChatGPT the
media and popular scientific literature, but also journals such as the
Communications of the ACM, have been flooded with opinions how these tools will
change programming. The opinions range from ``machines will program
themselves'', to ``AI does not help programmers''. Of course, these statements
are meant to to stir up a discussion, and should be taken with a grain of salt,
but we argue that such unfounded statements are potentially harmful. Instead,
we propose to investigate which skills are required to develop software using
LLM-based tools.
<br />In this paper we report on an experiment in which we explore if Computational
Thinking (CT) skills predict the ability to develop software using LLM-based
tools. Our results show that the ability to develop software using LLM-based
tools can indeed be predicted by the score on a CT assessment. There are many
limitations to our experiment, and this paper is also a call to discuss how to
approach, preferably experimentally, the question of which skills are required
to develop software using LLM-based tools. We propose to rephrase this question
to include by what kind of people/programmers, to develop what kind of software
using what kind of LLM-based tools.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05999" title="Abstract">arXiv:2310.05999</a> [<a href="/pdf/2310.05999" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two stage Robust Nash Bargaining based Benefit Sharing between Electric  and HCNG Distribution Networks Bridged with SOFC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wenwen Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+G">Gao Qiu</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+H">Hongjun Gao</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tingjian Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Junyong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yaping Li</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+S">Shengchun Yang</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+J">Jiahao Yan</a>, 
<a href="/search/eess?searchtype=author&query=Mao%2C+W">Wenbo Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Hydrogen-enriched compressed natural gas (HCNG) networks have potentized
sustainability and efficiency of integrated electricity and natural gas
systems. However, paucity of benefit sharing risks the IENGS's development in
multiple entities and bottlenecks its efficacy. To fill the gap, a robust Nash
bargaining-based benefit sharing mechanism for HCNG-enabled IENGS is proposed.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06002" title="Abstract">arXiv:2310.06002</a> [<a href="/pdf/2310.06002" title="Download PDF">pdf</a>, <a href="/format/2310.06002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LCOT: Linear circular optimal transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martin%2C+R+D">Rocio Diaz Martin</a>, 
<a href="/search/cs?searchtype=author&query=Medri%2C+I">Ivan Medri</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yikun Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Kangbai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Rohde%2C+G+K">Gustavo K. Rohde</a>, 
<a href="/search/cs?searchtype=author&query=Kolouri%2C+S">Soheil Kolouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The optimal transport problem for measures supported on non-Euclidean spaces
has recently gained ample interest in diverse applications involving
representation learning. In this paper, we focus on circular probability
measures, i.e., probability measures supported on the unit circle, and
introduce a new computationally efficient metric for these measures, denoted as
Linear Circular Optimal Transport (LCOT). The proposed metric comes with an
explicit linear embedding that allows one to apply Machine Learning (ML)
algorithms to the embedded measures and seamlessly modify the underlying metric
for the ML algorithm to LCOT. We show that the proposed metric is rooted in the
Circular Optimal Transport (COT) and can be considered the linearization of the
COT metric with respect to a fixed reference measure. We provide a theoretical
analysis of the proposed metric and derive the computational complexities for
pairwise comparison of circular probability measures. Lastly, through a set of
numerical experiments, we demonstrate the benefits of LCOT in learning
representations of circular measures.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06003" title="Abstract">arXiv:2310.06003</a> [<a href="/pdf/2310.06003" title="Download PDF">pdf</a>, <a href="/format/2310.06003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Memory and Communication Cost for Efficient Large Language  Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+L">Lin Ju</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Youshao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Huan%2C+Z">Zhaoxin Huan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanzhuang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaolu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As model sizes and training datasets continue to increase, large-scale model
training frameworks reduce memory consumption by various sharding techniques.
However, the huge communication overhead reduces the training efficiency,
especially in public cloud environments with varying network bandwidths. In
this paper, we rethink the impact of memory consumption and communication
overhead on the training speed of large language model, and propose a
memory-communication balanced \underline{Pa}rtial \underline{R}edundancy
\underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of
inter-group communication by grouping GPU clusters and introducing minor
intra-group memory redundancy, thereby improving the training efficiency of the
model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring)
communication topology to enhance communication efficiency between nodes or
across switches in large model training. Our experiments demonstrate that the
HO-Ring algorithm improves communication efficiency by 32.6\% compared to the
traditional Ring algorithm. Compared to the baseline ZeRO, PaRO significantly
improves training throughput by 1.2x-2.6x and achieves a near-linear
scalability. Therefore, the PaRO strategy provides more fine-grained options
for the trade-off between memory consumption and communication overhead in
different training scenarios.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06006" title="Abstract">arXiv:2310.06006</a> [<a href="/pdf/2310.06006" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of control algorithms for mobile robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suarez-Gomez%2C+A">Andres-David Suarez-Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Ortega%2C+A+A+H">Andres A. Hernandez Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, in Spanish
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This article presents a comprehensive review of control algorithms used in
mobile robotics, a field in constant evolution. Mobile robotics has seen
significant advances in recent years, driven by the demand for applications in
various sectors, such as industrial automation, space exploration, and medical
care. The review focuses on control algorithms that address specific challenges
in navigation, localization, mapping, and path planning in changing and unknown
environments. Classical approaches, such as PID control and methods based on
classical control theory, as well as modern techniques, including deep learning
and model-based planning, are discussed in detail. In addition, practical
applications and remaining challenges in implementing these algorithms in
real-world mobile robots are highlighted. Ultimately, this review provides a
comprehensive overview of the diversity and complexity of control algorithms in
mobile robotics, helping researchers and practitioners to better understand the
options available to address specific problems in this exciting area of study.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06008" title="Abstract">arXiv:2310.06008</a> [<a href="/pdf/2310.06008" title="Download PDF">pdf</a>, <a href="/format/2310.06008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoBEVFusion: Cooperative Perception with LiDAR-Camera Bird&#x27;s-Eye View  Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+D">Donghao Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zulkernine%2C+F">Farhana Zulkernine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Autonomous Vehicles (AVs) use multiple sensors to gather information about
their surroundings. By sharing sensor data between Connected Autonomous
Vehicles (CAVs), the safety and reliability of these vehicles can be improved
through a concept known as cooperative perception. However, recent approaches
in cooperative perception only share single sensor information such as cameras
or LiDAR. In this research, we explore the fusion of multiple sensor data
sources and present a framework, called CoBEVFusion, that fuses LiDAR and
camera data to create a Bird's-Eye View (BEV) representation. The CAVs process
the multi-modal data locally and utilize a Dual Window-based Cross-Attention
(DWCA) module to fuse the LiDAR and camera features into a unified BEV
representation. The fused BEV feature maps are shared among the CAVs, and a 3D
Convolutional Neural Network is applied to aggregate the features from the
CAVs. Our CoBEVFusion framework was evaluated on the cooperative perception
dataset OPV2V for two perception tasks: BEV semantic segmentation and 3D object
detection. The results show that our DWCA LiDAR-camera fusion model outperforms
perception models with single-modal data and state-of-the-art BEV fusion
models. Our overall cooperative perception architecture, CoBEVFusion, also
achieves comparable performance with other cooperative perception models.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06009" title="Abstract">arXiv:2310.06009</a> [<a href="/pdf/2310.06009" title="Download PDF">pdf</a>, <a href="/format/2310.06009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide-and-Conquer Dynamics in AI-Driven Disempowerment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+P+S">Peter S. Park</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, nine visualizations (seven figures and two tables)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">AI companies are attempting to create AI systems that outperform humans at
most economically valuable work. Current AI models are already automating away
the livelihoods of some artists, actors, and writers. But there is infighting
between those who prioritize current harms and future harms. We construct a
game-theoretic model of conflict to study the causes and consequences of this
disunity. Our model also helps explain why throughout history, stakeholders
sharing a common threat have found it advantageous to unite against it, and why
the common threat has in turn found it advantageous to divide and conquer.
<br />Under realistic parameter assumptions, our model makes several predictions
that find preliminary corroboration in the historical-empirical record. First,
current victims of AI-driven disempowerment need the future victims to realize
that their interests are also under serious and imminent threat, so that future
victims are incentivized to support current victims in solidarity. Second, the
movement against AI-driven disempowerment can become more united, and thereby
more likely to prevail, if members believe that their efforts will be
successful as opposed to futile. Finally, the movement can better unite and
prevail if its members are less myopic. Myopic members prioritize their future
well-being less than their present well-being, and are thus disinclined to
solidarily support current victims today at personal cost, even if this is
necessary to counter the shared threat of AI-driven disempowerment.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06020" title="Abstract">arXiv:2310.06020</a> [<a href="/pdf/2310.06020" title="Download PDF">pdf</a>, <a href="/format/2310.06020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DyST: Towards Dynamic Neural Scene Representations on Real-World Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seitzer%2C+M">Maximilian Seitzer</a>, 
<a href="/search/cs?searchtype=author&query=van+Steenkiste%2C+S">Sjoerd van Steenkiste</a>, 
<a href="/search/cs?searchtype=author&query=Kipf%2C+T">Thomas Kipf</a>, 
<a href="/search/cs?searchtype=author&query=Greff%2C+K">Klaus Greff</a>, 
<a href="/search/cs?searchtype=author&query=Sajjadi%2C+M+S+M">Mehdi S. M. Sajjadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://dyst-paper.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Visual understanding of the world goes beyond the semantics and flat
structure of individual images. In this work, we aim to capture both the 3D
structure and dynamics of real-world scenes from monocular real-world videos.
Our Dynamic Scene Transformer (DyST) model leverages recent work in neural
scene representation to learn a latent decomposition of monocular real-world
videos into scene content, per-view scene dynamics, and camera pose. This
separation is achieved through a novel co-training scheme on monocular videos
and our new synthetic dataset DySO. DyST learns tangible latent representations
for dynamic scenes that enable view generation with separate control over the
camera and the content of the scene.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06045" title="Abstract">arXiv:2310.06045</a> [<a href="/pdf/2310.06045" title="Download PDF">pdf</a>, <a href="/format/2310.06045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative ensemble deep learning severe weather prediction from a  deterministic convection-allowing model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sha%2C+Y">Yingkai Sha</a>, 
<a href="/search/cs?searchtype=author&query=Sobash%2C+R+A">Ryan A. Sobash</a>, 
<a href="/search/cs?searchtype=author&query=Gagne%2C+D+J">David John Gagne II</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">An ensemble post-processing method is developed for the probabilistic
prediction of severe weather (tornadoes, hail, and wind gusts) over the
conterminous United States (CONUS). The method combines conditional generative
adversarial networks (CGANs), a type of deep generative model, with a
convolutional neural network (CNN) to post-process convection-allowing model
(CAM) forecasts. The CGANs are designed to create synthetic ensemble members
from deterministic CAM forecasts, and their outputs are processed by the CNN to
estimate the probability of severe weather. The method is tested using
High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm
Prediction Center (SPC) severe weather reports as targets. The method produced
skillful predictions with up to 20% Brier Skill Score (BSS) increases compared
to other neural-network-based reference methods using a testing dataset of HRRR
forecasts in 2021. For the evaluation of uncertainty quantification, the method
is overconfident but produces meaningful ensemble spreads that can distinguish
good and bad forecasts. The quality of CGAN outputs is also evaluated. Results
show that the CGAN outputs behave similarly to a numerical ensemble; they
preserved the inter-variable correlations and the contribution of influential
predictors as in the original HRRR forecasts. This work provides a novel
approach to post-process CAM output using neural networks that can be applied
to severe weather prediction.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06046" title="Abstract">arXiv:2310.06046</a> [<a href="/pdf/2310.06046" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM for SoC Security: A Paradigm Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+D">Dipayan Saha</a>, 
<a href="/search/cs?searchtype=author&query=Tarek%2C+S">Shams Tarek</a>, 
<a href="/search/cs?searchtype=author&query=Yahyaei%2C+K">Katayoon Yahyaei</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S+K">Sujan Kumar Saha</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingbo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tehranipoor%2C+M">Mark Tehranipoor</a>, 
<a href="/search/cs?searchtype=author&query=Farahmandi%2C+F">Farimah Farahmandi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">As the ubiquity and complexity of system-on-chip (SoC) designs increase
across electronic devices, the task of incorporating security into an SoC
design flow poses significant challenges. Existing security solutions are
inadequate to provide effective verification of modern SoC designs due to their
limitations in scalability, comprehensiveness, and adaptability. On the other
hand, Large Language Models (LLMs) are celebrated for their remarkable success
in natural language understanding, advanced reasoning, and program synthesis
tasks. Recognizing an opportunity, our research delves into leveraging the
emergent capabilities of Generative Pre-trained Transformers (GPTs) to address
the existing gaps in SoC security, aiming for a more efficient, scalable, and
adaptable methodology. By integrating LLMs into the SoC security verification
paradigm, we open a new frontier of possibilities and challenges to ensure the
security of increasingly complex SoCs. This paper offers an in-depth analysis
of existing works, showcases practical case studies, demonstrates comprehensive
experiments, and provides useful promoting guidelines. We also present the
achievements, prospects, and challenges of employing LLM in different SoC
security verification tasks.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06047" title="Abstract">arXiv:2310.06047</a> [<a href="/pdf/2310.06047" title="Download PDF">pdf</a>, <a href="/format/2310.06047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Distillation for Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pol%2C+A+A">Adrian Alan Pol</a>, 
<a href="/search/cs?searchtype=author&query=Govorkova%2C+E">Ekaterina Govorkova</a>, 
<a href="/search/cs?searchtype=author&query=Gronroos%2C+S">Sonja Gronroos</a>, 
<a href="/search/cs?searchtype=author&query=Chernyavskaya%2C+N">Nadezda Chernyavskaya</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+P">Philip Harris</a>, 
<a href="/search/cs?searchtype=author&query=Pierini%2C+M">Maurizio Pierini</a>, 
<a href="/search/cs?searchtype=author&query=Ojalvo%2C+I">Isobel Ojalvo</a>, 
<a href="/search/cs?searchtype=author&query=Elmer%2C+P">Peter Elmer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Unsupervised deep learning techniques are widely used to identify anomalous
behaviour. The performance of such methods is a product of the amount of
training data and the model size. However, the size is often a limiting factor
for the deployment on resource-constrained devices. We present a novel
procedure based on knowledge distillation for compressing an unsupervised
anomaly detection model into a supervised deployable one and we suggest a set
of techniques to improve the detection sensitivity. Compressed models perform
comparably to their larger counterparts while significantly reducing the size
and memory footprint.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06056" title="Abstract">arXiv:2310.06056</a> [<a href="/pdf/2310.06056" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Automated Tool to Detect Suicidal Susceptibility from Social Media  Posts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dus%2C+Y">Yasin Dus</a>, 
<a href="/search/cs?searchtype=author&query=Nefedov%2C+G">Georgiy Nefedov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, 1 table. Submitted to PeerJ
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">According to the World Health Organization (WHO), approximately 1.4 million
individuals died by suicide in 2022. This means that one person dies by suicide
every 20 seconds. Globally, suicide ranks as the 10th leading cause of death,
while it ranks second for young people aged 15-29. In the year 2022, it was
estimated that about 10.5 million suicide attempts occurred. The WHO suggests
that alongside each completed suicide, there are many individuals who make
attempts. Today, social media is a place where people share their feelings,
such as happiness, sadness, anger, and love. This helps us understand how they
are thinking or what they might do. This study takes advantage of this
opportunity and focuses on developing an automated tool to find if someone may
be thinking about harming themselves. It is developed based on the
Suicidal-Electra model. We collected datasets of social media posts, processed
them, and used them to train and fine-tune the model. Upon evaluating the
refined model with a testing dataset, we consistently observed outstanding
results. The model demonstrated an impressive accuracy rate of 93% and a
commendable F1 score of 0.93. Additionally, we developed an API enabling
seamless integration with third-party platforms, enhancing its potential for
implementation to address the growing concern of rising suicide rates.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06059" title="Abstract">arXiv:2310.06059</a> [<a href="/pdf/2310.06059" title="Download PDF">pdf</a>, <a href="/format/2310.06059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early Warning via tipping-preserving latent stochastic dynamical system  and meta label correcting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Ting Gao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinqiao Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Early warning for epilepsy patients is crucial for their safety and
well-being, in terms of preventing or minimizing the severity of seizures.
Through the patients' EEG data, we propose a meta learning framework for
improving prediction on early ictal signals. To better utilize the meta label
corrector method, we fuse the information from both the real data and the
augmented data from the latent Stochastic differential equation(SDE). Besides,
we also optimally select the latent dynamical system via distribution of
transition time between real data and that from the latent SDE. In this way,
the extracted tipping dynamical feature is also integrated into the meta
network to better label the noisy data. To validate our method, LSTM is
implemented as the baseline model. We conduct a series of experiments to
predict seizure in various long-term window from 1-2 seconds input data and
find surprisingly increment of prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06061" title="Abstract">arXiv:2310.06061</a> [<a href="/pdf/2310.06061" title="Download PDF">pdf</a>, <a href="/format/2310.06061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auditing Gender Analyzers on Text Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+S+D">Siddharth D Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+A+K">Ankit Kumar Verma</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted at IEEE/ACM ASONAM 2023. Please cite the version appearing in the ASONAM proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">AI models have become extremely popular and accessible to the general public.
However, they are continuously under the scanner due to their demonstrable
biases toward various sections of the society like people of color and
non-binary people. In this study, we audit three existing gender analyzers --
uClassify, Readable and HackerFactor, for biases against non-binary
individuals. These tools are designed to predict only the cisgender binary
labels, which leads to discrimination against non-binary members of the
society. We curate two datasets -- Reddit comments (660k) and, Tumblr posts
(2.05M) and our experimental evaluation shows that the tools are highly
inaccurate with the overall accuracy being ~50% on all platforms. Predictions
for non-binary comments on all platforms are mostly female, thus propagating
the societal bias that non-binary individuals are effeminate. To address this,
we fine-tune a BERT multi-label classifier on the two datasets in multiple
combinations, observe an overall performance of ~77% on the most realistically
deployable setting and a surprisingly higher performance of 90% for the
non-binary class. We also audit ChatGPT using zero-shot prompts on a small
dataset (due to high pricing) and observe an average accuracy of 58% for Reddit
and Tumblr combined (with overall better results for Reddit).
<br />Thus, we show that existing systems, including highly advanced ones like
ChatGPT are biased, and need better audits and moderation and, that such
societal biases can be addressed and alleviated through simple off-the-shelf
models like BERT trained on more gender inclusive datasets.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06066" title="Abstract">arXiv:2310.06066</a> [<a href="/pdf/2310.06066" title="Download PDF">pdf</a>, <a href="/ps/2310.06066" title="Download PostScript">ps</a>, <a href="/format/2310.06066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stabilized finite elements for the solution of the Reynolds equation  considering cavitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gravenkamp%2C+H">Hauke Gravenkamp</a>, 
<a href="/search/math?searchtype=author&query=Pfeil%2C+S">Simon Pfeil</a>, 
<a href="/search/math?searchtype=author&query=Codina%2C+R">Ramon Codina</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Methods in Applied Mechanics and Engineering, vol. 418,
  p. 116488, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The Reynolds equation, combined with the Elrod algorithm for including the
effect of cavitation, resembles a nonlinear convection-diffusion-reaction (CDR)
equation. Its solution by finite elements is prone to oscillations in
convection-dominated regions, which are present whenever cavitation occurs. We
propose a stabilized finite-element method that is based on the variational
multiscale method and exploits the concept of orthogonal subgrid scales. We
demonstrate that this approach only requires one additional term in the weak
form to obtain a stable method that converges optimally when performing mesh
refinement.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06068" title="Abstract">arXiv:2310.06068</a> [<a href="/pdf/2310.06068" title="Download PDF">pdf</a>, <a href="/format/2310.06068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Vision-Based Human Pose Estimation with Rotation Matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vazan%2C+M">Milad Vazan</a>, 
<a href="/search/cs?searchtype=author&query=Masoumi%2C+F+S">Fatemeh Sadat Masoumi</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+R">Ruizhi Ou</a>, 
<a href="/search/cs?searchtype=author&query=Rawassizadeh%2C+R">Reza Rawassizadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fitness applications are commonly used to monitor activities within the gym,
but they often fail to automatically track indoor activities inside the gym.
This study proposes a model that utilizes pose estimation combined with a novel
data augmentation method, i.e., rotation matrix. We aim to enhance the
classification accuracy of activity recognition based on pose estimation data.
Through our experiments, we experiment with different classification algorithms
along with image augmentation approaches. Our findings demonstrate that the SVM
with SGD optimization, using data augmentation with the Rotation Matrix, yields
the most accurate results, achieving a 96% accuracy rate in classifying five
physical activities. Conversely, without implementing the data augmentation
techniques, the baseline accuracy remains at a modest 64%.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06072" title="Abstract">arXiv:2310.06072</a> [<a href="/pdf/2310.06072" title="Download PDF">pdf</a>, <a href="/format/2310.06072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and  Nonverbal Expressions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+D">Detai Xin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junfeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Takamichi%2C+S">Shinnosuke Takamichi</a>, 
<a href="/search/cs?searchtype=author&query=Saito%2C+Y">Yuki Saito</a>, 
<a href="/search/cs?searchtype=author&query=Aizawa%2C+A">Akiko Aizawa</a>, 
<a href="/search/cs?searchtype=author&query=Saruwatari%2C+H">Hiroshi Saruwatari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present the JVNV, a Japanese emotional speech corpus with verbal content
and nonverbal vocalizations whose scripts are generated by a large-scale
language model. Existing emotional speech corpora lack not only proper
emotional scripts but also nonverbal vocalizations (NVs) that are essential
expressions in spoken language to express emotions. We propose an automatic
script generation method to produce emotional scripts by providing seed words
with sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using
prompt engineering. We select 514 scripts with balanced phoneme coverage from
the generated candidate scripts with the assistance of emotion confidence
scores and language fluency scores. We demonstrate the effectiveness of JVNV by
showing that JVNV has better phoneme coverage and emotion recognizability than
previous Japanese emotional speech corpora. We then benchmark JVNV on emotional
text-to-speech synthesis using discrete codes to represent NVs. We show that
there still exists a gap between the performance of synthesizing read-aloud
speech and emotional speech, and adding NVs in the speech makes the task even
harder, which brings new challenges for this task and makes JVNV a valuable
resource for relevant works in the future. To our best knowledge, JVNV is the
first speech corpus that generates scripts automatically using large language
models.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06074" title="Abstract">arXiv:2310.06074</a> [<a href="/pdf/2310.06074" title="Download PDF">pdf</a>, <a href="/format/2310.06074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Agility: A Momentum Aware Trajectory Optimisation Framework  using Full-Centroidal Dynamics &amp; Implicit Inverse Kinematics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papatheodorou%2C+A">Aristotelis Papatheodorou</a>, 
<a href="/search/cs?searchtype=author&query=Merkt%2C+W">Wolfgang Merkt</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+A+L">Alexander L. Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Havoutis%2C+I">Ioannis Havoutis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Online planning and execution of acrobatic maneuvers pose significant
challenges in legged locomotion. Their underlying combinatorial nature, along
with the current hardware's limitations constitute the main obstacles in
unlocking the true potential of legged-robots. This letter tries to expose the
intricacies of these optimal control problems in a tangible way, directly
applicable to the creation of more efficient online trajectory optimisation
frameworks. By analysing the fundamental principles that shape the behaviour of
the system, the dynamics themselves can be exploited to surpass its hardware
limitations. More specifically, a trajectory optimisation formulation is
proposed that exploits the system's high-order nonlinearities, such as the
nonholonomy of the angular momentum, and phase-space symmetries in order to
produce feasible high-acceleration maneuvers. By leveraging the full-centroidal
dynamics of the quadruped ANYmal C and directly optimising its footholds and
contact forces, the framework is capable of producing efficient motion plans
with low computational overhead. The feasibility of the produced trajectories
is ensured by taking into account the configuration-dependent inertial
properties of the robot during the planning process, while its robustness is
increased by supplying the full analytic derivatives &amp; hessians to the solver.
Finally, a significant portion of the discussion is centred around the
deployment of the proposed framework on the ANYmal C platform, while its true
capabilities are demonstrated through real-world experiments, with the
successful execution of high-acceleration motion scenarios like the squat-jump.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06075" title="Abstract">arXiv:2310.06075</a> [<a href="/pdf/2310.06075" title="Download PDF">pdf</a>, <a href="/format/2310.06075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pain Forecasting using Self-supervised Learning and Patient Phenotyping:  An attempt to prevent Opioid Addiction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padhee%2C+S">Swati Padhee</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+T">Tanvi Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Abrams%2C+D+M">Daniel M. Abrams</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Nirmish Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by
recurrent acute painful episodes. Opioids are often used to manage these
painful episodes; the extent of their use in managing pain in this disorder is
an issue of debate. The risk of addiction and side effects of these opioid
treatments can often lead to more pain episodes in the future. Hence, it is
crucial to forecast future patient pain trajectories to help patients manage
their SCD to improve their quality of life without compromising their
treatment. It is challenging to obtain many pain records to design forecasting
models since it is mainly recorded by patients' self-report. Therefore, it is
expensive and painful (due to the need for patient compliance) to solve pain
forecasting problems in a purely supervised manner. In light of this challenge,
we propose to solve the pain forecasting problem using self-supervised learning
methods. Also, clustering such time-series data is crucial for patient
phenotyping, anticipating patients' prognoses by identifying "similar"
patients, and designing treatment guidelines tailored to homogeneous patient
subgroups. Hence, we propose a self-supervised learning approach for clustering
time-series data, where each cluster comprises patients who share similar
future pain profiles. Experiments on five years of real-world datasets show
that our models achieve superior performance over state-of-the-art benchmarks
and identify meaningful clusters that can be translated into actionable
information for clinical decision-making.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06076" title="Abstract">arXiv:2310.06076</a> [<a href="/pdf/2310.06076" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFPB Consumer Complaints Analysis Using Hadoop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaishnav%2C+D">Dhwani Vaishnav</a>, 
<a href="/search/cs?searchtype=author&query=Neethinayagam%2C+M">Manimozhi Neethinayagam</a>, 
<a href="/search/cs?searchtype=author&query=Khaire%2C+A+S">Akanksha S Khaire</a>, 
<a href="/search/cs?searchtype=author&query=Dhoke%2C+M+V">Mansi Vivekanand Dhoke</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+J">Jongwook Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 7 figures, 2 Ttables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Consumer complaints are a crucial source of information for companies,
policymakers, and consumers alike. They provide insight into the problems faced
by consumers and help identify areas for improvement in products, services, and
regulatory frameworks. This paper aims to analyze Consumer Complaints Dataset
provided by Consumer Financial Protection Bureau (CFPB) and provide insights
into the nature and patterns of consumer complaints in the USA. We begin by
describing the dataset and its features, including the types of complaints,
companies involved, and geographic distribution. We then conduct exploratory
data analysis to identify trends and patterns in the data, such as the most
common types of complaints, the companies with the highest number of
complaints, and the states with the most complaints. We have also performed
descriptive and inferential statistics to test hypotheses and draw conclusions
about the data. We have investigated whether there are significant differences
in the types of complaints or companies involved based on geographic location.
Overall, our analysis provides valuable insights into the nature of consumer
complaints in the USA and helps stakeholders make informed decisions to improve
the consumer experience.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06077" title="Abstract">arXiv:2310.06077</a> [<a href="/pdf/2310.06077" title="Download PDF">pdf</a>, <a href="/format/2310.06077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performative Time-Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhiyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+A">Alexander Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B.Aditya Prakash</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages (7 main text, 2 reference, 3 appendix), 3 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time-series forecasting is a critical challenge in various domains and has
witnessed substantial progress in recent years. Many real-life scenarios, such
as public health, economics, and social applications, involve feedback loops
where predictions can influence the predicted outcome, subsequently altering
the target variable's distribution. This phenomenon, known as performativity,
introduces the potential for 'self-negating' or 'self-fulfilling' predictions.
Despite extensive studies in classification problems across domains,
performativity remains largely unexplored in the context of time-series
forecasting from a machine-learning perspective.
<br />In this paper, we formalize performative time-series forecasting (PeTS),
addressing the challenge of accurate predictions when performativity-induced
distribution shifts are possible. We propose a novel approach, Feature
Performative-Shifting (FPS), which leverages the concept of delayed response to
anticipate distribution shifts and subsequently predicts targets accordingly.
We provide theoretical insights suggesting that FPS can potentially lead to
reduced generalization error. We conduct comprehensive experiments using
multiple time-series models on COVID-19 and traffic forecasting tasks. The
results demonstrate that FPS consistently outperforms conventional time-series
forecasting methods, highlighting its efficacy in handling
performativity-induced challenges.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06083" title="Abstract">arXiv:2310.06083</a> [<a href="/pdf/2310.06083" title="Download PDF">pdf</a>, <a href="/format/2310.06083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers and Large Language Models for Chemistry and Drug Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bran%2C+A+M">Andres M Bran</a>, 
<a href="/search/cs?searchtype=author&query=Schwaller%2C+P">Philippe Schwaller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Language modeling has seen impressive progress over the last years, mainly
prompted by the invention of the Transformer architecture, sparking a
revolution in many fields of machine learning, with breakthroughs in chemistry
and biology. In this chapter, we explore how analogies between chemical and
natural language have inspired the use of Transformers to tackle important
bottlenecks in the drug discovery process, such as retrosynthetic planning and
chemical space exploration. The revolution started with models able to perform
particular tasks with a single type of data, like linearised molecular graphs,
which then evolved to include other types of data, like spectra from analytical
instruments, synthesis actions, and human language. A new trend leverages
recent developments in large language models, giving rise to a wave of models
capable of solving generic tasks in chemistry, all facilitated by the
flexibility of natural language. As we continue to explore and harness these
capabilities, we can look forward to a future where machine learning plays an
even more integral role in accelerating scientific discovery.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06084" title="Abstract">arXiv:2310.06084</a> [<a href="/pdf/2310.06084" title="Download PDF">pdf</a>, <a href="/format/2310.06084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exoskeleton-Mediated Physical Human-Human Interaction for a Sit-to-Stand  Rehabilitation Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vianello%2C+L">Lorenzo Vianello</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BC%C3%A7%C3%BCktabak%2C+E+B">Emek Bar&#x131;&#x15f; K&#xfc;&#xe7;&#xfc;ktabak</a>, 
<a href="/search/cs?searchtype=author&query=Short%2C+M">Matthew Short</a>, 
<a href="/search/cs?searchtype=author&query=Lhoste%2C+C">Cl&#xe9;ment Lhoste</a>, 
<a href="/search/cs?searchtype=author&query=Amato%2C+L">Lorenzo Amato</a>, 
<a href="/search/cs?searchtype=author&query=Lynch%2C+K">Kevin Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Pons%2C+J">Jose Pons</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, submitted to 2024 IEEE The International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Sit-to-Stand (StS) is a fundamental daily activity that can be challenging
for stroke survivors due to strength, motor control, and proprioception
deficits in their lower limbs. Existing therapies involve repetitive StS
exercises, but these can be physically demanding for therapists while assistive
devices may limit patient participation and hinder motor learning. To address
these challenges, this work proposes the use of two lower-limb exoskeletons to
mediate physical interaction between therapists and patients during a StS
rehabilitative task. This approach offers several advantages, including
improved therapist-patient interaction, safety enforcement, and performance
quantification. The whole body control of the two exoskeletons transmits online
feedback between the two users, but at the same time assists in movement and
ensures balance, and thus helping subjects with greater difficulty. In this
study we present the architecture of the framework, presenting and discussing
some technical choices made in the design.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06085" title="Abstract">arXiv:2310.06085</a> [<a href="/pdf/2310.06085" title="Download PDF">pdf</a>, <a href="/format/2310.06085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantile-based Maximum Likelihood Training for Outlier Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taghikhah%2C+M">Masoud Taghikhah</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nishant Kumar</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0egvi%C4%87%2C+S">Sini&#x161;a &#x160;egvi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Eslami%2C+A">Abouzar Eslami</a>, 
<a href="/search/cs?searchtype=author&query=Gumhold%2C+S">Stefan Gumhold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/taghikhah/QuantOD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Discriminative learning effectively predicts true object class for image
classification. However, it often results in false positives for outliers,
posing critical concerns in applications like autonomous driving and video
surveillance systems. Previous attempts to address this challenge involved
training image classifiers through contrastive learning using actual outlier
data or synthesizing outliers for self-supervised learning. Furthermore,
unsupervised generative modeling of inliers in pixel space has shown limited
success for outlier detection. In this work, we introduce a quantile-based
maximum likelihood objective for learning the inlier distribution to improve
the outlier separation during inference. Our approach fits a normalizing flow
to pre-trained discriminative features and detects the outliers according to
the evaluated log-likelihood. The experimental evaluation demonstrates the
effectiveness of our method as it surpasses the performance of the
state-of-the-art unsupervised methods for outlier detection. The results are
also competitive compared with a recent self-supervised approach for outlier
detection. Our work allows to reduce dependency on well-sampled negative
training data, which is especially important for domains like medical
diagnostics or remote sensing.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06089" title="Abstract">arXiv:2310.06089</a> [<a href="/pdf/2310.06089" title="Download PDF">pdf</a>, <a href="/format/2310.06089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive auxiliary objectives in deep RL mimic learning in the brain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Ching Fang</a>, 
<a href="/search/cs?searchtype=author&query=Stachenfeld%2C+K+L">Kimberly L Stachenfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The ability to predict upcoming events has been hypothesized to comprise a
key aspect of natural and machine cognition. This is supported by trends in
deep reinforcement learning (RL), where self-supervised auxiliary objectives
such as prediction are widely used to support representation learning and
improve task performance. Here, we study the effects predictive auxiliary
objectives have on representation learning across different modules of an RL
system and how these mimic representational changes observed in the brain. We
find that predictive objectives improve and stabilize learning particularly in
resource-limited architectures, and we identify settings where longer
predictive horizons better support representational transfer. Furthermore, we
find that representational changes in this RL system bear a striking
resemblance to changes in neural activity observed in the brain across various
experiments. Specifically, we draw a connection between the auxiliary
predictive model of the RL system and hippocampus, an area thought to learn a
predictive model to support memory-guided behavior. We also connect the encoder
network and the value learning network of the RL system to visual cortex and
striatum in the brain, respectively. This work demonstrates how representation
learning in deep RL systems can provide an interpretable framework for modeling
multi-region interactions in the brain. The deep RL perspective taken here also
suggests an additional role of the hippocampus in the brain -- that of an
auxiliary learning system that benefits representation learning in other
regions.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06092" title="Abstract">arXiv:2310.06092</a> [<a href="/pdf/2310.06092" title="Download PDF">pdf</a>, <a href="/format/2310.06092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Analysis of time-dependent Hamilton-Jacobi Equations on  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carlini%2C+E">Elisabetta Carlini</a>, 
<a href="/search/math?searchtype=author&query=Siconolfi%2C+A">Antonio Siconolfi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">A new algorithm for time dependent Hamilton Jacobi equations on networks,
based on semi Lagrangian scheme, is proposed. It is based on the definition of
viscosity solution for this kind of problems recently given in. A thorough
convergence analysis, not requiring weak semilimits, is provided. In
particular, the check of the supersolution property at the vertices is
performed through a dynamical technique which seems new. The scheme is
efficient, explicit, allows long time steps, and is suitable to be implemented
in a parallel algorithm. We present some numerical tests, showing the advantage
in terms of computational cost over the one proposed in [7]
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06100" title="Abstract">arXiv:2310.06100</a> [<a href="/pdf/2310.06100" title="Download PDF">pdf</a>, <a href="/format/2310.06100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Dimensional Causal Inference with Variational Backdoor Adjustment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Israel%2C+D">Daniel Israel</a>, 
<a href="/search/cs?searchtype=author&query=Grover%2C+A">Aditya Grover</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Backdoor adjustment is a technique in causal inference for estimating
interventional quantities from purely observational data. For example, in
medical settings, backdoor adjustment can be used to control for confounding
and estimate the effectiveness of a treatment. However, high dimensional
treatments and confounders pose a series of potential pitfalls: tractability,
identifiability, optimization. In this work, we take a generative modeling
approach to backdoor adjustment for high dimensional treatments and
confounders. We cast backdoor adjustment as an optimization problem in
variational inference without reliance on proxy variables and hidden
confounders. Empirically, our method is able to estimate interventional
likelihood in a variety of high dimensional settings, including semi-synthetic
X-ray medical data. To the best of our knowledge, this is the first application
of backdoor adjustment in which all the relevant variables are high
dimensional.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06103" title="Abstract">arXiv:2310.06103</a> [<a href="/pdf/2310.06103" title="Download PDF">pdf</a>, <a href="/format/2310.06103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Multilingual Self-Supervised Pretrained Models for  Sequence-to-Sequence End-to-End Spoken Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Denisov%2C+P">Pavel Denisov</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+N+T">Ngoc Thang Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">A number of methods have been proposed for End-to-End Spoken Language
Understanding (E2E-SLU) using pretrained models, however their evaluation often
lacks multilingual setup and tasks that require prediction of lexical fillers,
such as slot filling. In this work, we propose a unified method that integrates
multilingual pretrained speech and text models and performs E2E-SLU on six
datasets in four languages in a generative manner, including the prediction of
lexical fillers. We investigate how the proposed method can be improved by
pretraining on widely available speech recognition data using several training
objectives. Pretraining on 7000 hours of multilingual data allows us to
outperform the state-of-the-art ultimately on two SLU datasets and partly on
two more SLU datasets. Finally, we examine the cross-lingual capabilities of
the proposed model and improve on the best known result on the
PortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate
of 23.65%.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06107" title="Abstract">arXiv:2310.06107</a> [<a href="/pdf/2310.06107" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing and Refining a Multifunctional Facial Recognition System for  Older Adults with Cognitive Impairments: A Journey Towards Enhanced Quality  of Life
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+L">Li He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In an era where the global population is aging significantly, cognitive
impairments among the elderly have become a major health concern. The need for
effective assistive technologies is clear, and facial recognition systems are
emerging as promising tools to address this issue. This document discusses the
development and evaluation of a new Multifunctional Facial Recognition System
(MFRS), designed specifically to assist older adults with cognitive
impairments. The MFRS leverages face_recognition [1], a powerful open-source
library capable of extracting, identifying, and manipulating facial features.
Our system integrates the face recognition and retrieval capabilities of
face_recognition, along with additional functionalities to capture images and
record voice memos. This combination of features notably enhances the system's
usability and versatility, making it a more user-friendly and universally
applicable tool for end-users. The source code for this project can be accessed
at https://github.com/Li-8023/Multi-function-face-recognition.git.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06109" title="Abstract">arXiv:2310.06109</a> [<a href="/pdf/2310.06109" title="Download PDF">pdf</a>, <a href="/format/2310.06109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QR-Tag: Angular Measurement and Tracking with a QR-Design Marker
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Simeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Amata%2C+H">Hadi Amata</a>, 
<a href="/search/cs?searchtype=author&query=Heidrich%2C+W">Wolfgang Heidrich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Directional information measurement has many applications in domains such as
robotics, virtual and augmented reality, and industrial computer vision.
Conventional methods either require pre-calibration or necessitate controlled
environments. The state-of-the-art MoireTag approach exploits the Moire effect
and QR-design to continuously track the angular shift precisely. However, it is
still not a fully QR code design. To overcome the above challenges, we propose
a novel snapshot method for discrete angular measurement and tracking with
scannable QR-design patterns that are generated by binary structures printed on
both sides of a glass plate. The QR codes, resulting from the parallax effect
due to the geometry alignment between two layers, can be readily measured as
angular information using a phone camera. The simulation results show that the
proposed non-contact object tracking framework is computationally efficient
with high accuracy.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06111" title="Abstract">arXiv:2310.06111</a> [<a href="/pdf/2310.06111" title="Download PDF">pdf</a>, <a href="/format/2310.06111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BYOC: Personalized Few-Shot Classification with Co-Authored Class  Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bohra%2C+A">Arth Bohra</a>, 
<a href="/search/cs?searchtype=author&query=Verkes%2C+G">Govert Verkes</a>, 
<a href="/search/cs?searchtype=author&query=Harutyunyan%2C+A">Artem Harutyunyan</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+P">Pascal Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Campagna%2C+G">Giovanni Campagna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Text classification is a well-studied and versatile building block for many
NLP applications. Yet, existing approaches require either large annotated
corpora to train a model with or, when using large language models as a base,
require carefully crafting the prompt as well as using a long context that can
fit many examples. As a result, it is not possible for end-users to build
classifiers for themselves. To address this issue, we propose a novel approach
to few-shot text classification using an LLM. Rather than few-shot examples,
the LLM is prompted with descriptions of the salient features of each class.
These descriptions are coauthored by the user and the LLM interactively: while
the user annotates each few-shot example, the LLM asks relevant questions that
the user answers. Examples, questions, and answers are summarized to form the
classification prompt. Our experiments show that our approach yields high
accuracy classifiers, within 82% of the performance of models trained with
significantly larger datasets while using only 1% of their training sets.
Additionally, in a study with 30 participants, we show that end-users are able
to build classifiers to suit their specific needs. The personalized classifiers
show an average accuracy of 90%, which is 15% higher than the state-of-the-art
approach.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06112" title="Abstract">arXiv:2310.06112</a> [<a href="/pdf/2310.06112" title="Download PDF">pdf</a>, <a href="/ps/2310.06112" title="Download PostScript">ps</a>, <a href="/format/2310.06112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shaopeng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Adversarial training (AT) is a canonical method for enhancing the robustness
of deep neural networks (DNNs). However, recent studies empirically
demonstrated that it suffers from robust overfitting, i.e., a long time AT can
be detrimental to the robustness of DNNs. This paper presents a theoretical
explanation of robust overfitting for DNNs. Specifically, we non-trivially
extend the neural tangent kernel (NTK) theory to AT and prove that an
adversarially trained wide DNN can be well approximated by a linearized DNN.
Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can
be derived, which reveals a new AT degeneration phenomenon: a long-term AT will
result in a wide DNN degenerates to that obtained without AT and thus cause
robust overfitting. Based on our theoretical results, we further design a
method namely Adv-NTK, the first AT algorithm for infinite-width DNNs.
Experiments on real-world datasets show that Adv-NTK can help infinite-width
DNNs enhance comparable robustness to that of their finite-width counterparts,
which in turn justifies our theoretical findings. The code is available at
https://github.com/fshp971/adv-ntk.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06113" title="Abstract">arXiv:2310.06113</a> [<a href="/pdf/2310.06113" title="Download PDF">pdf</a>, <a href="/format/2310.06113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When is Agnostic Reinforcement Learning Statistically Tractable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+Z">Zeyu Jia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gene Li</a>, 
<a href="/search/cs?searchtype=author&query=Rakhlin%2C+A">Alexander Rakhlin</a>, 
<a href="/search/cs?searchtype=author&query=Sekhari%2C+A">Ayush Sekhari</a>, 
<a href="/search/cs?searchtype=author&query=Srebro%2C+N">Nathan Srebro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of agnostic PAC reinforcement learning (RL): given a
policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a
potentially large state and action space) are required to learn an
$\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we
introduce a new complexity measure, called the \emph{spanning capacity}, that
depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a
generative model, we show that for any policy class $\Pi$, bounded spanning
capacity characterizes PAC learnability. However, for online RL, the situation
is more subtle. We show there exists a policy class $\Pi$ with a bounded
spanning capacity that requires a superpolynomial number of samples to learn.
This reveals a surprising separation for agnostic learnability between
generative access and online access models (as well as between
deterministic/stochastic MDPs under online access). On the positive side, we
identify an additional \emph{sunflower} structure, which in conjunction with
bounded spanning capacity enables statistically efficient online RL via a new
algorithm called POPLER, which takes inspiration from classical importance
sampling methods as well as techniques for reachable-state identification and
policy evaluation in reward-free exploration.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06114" title="Abstract">arXiv:2310.06114</a> [<a href="/pdf/2310.06114" title="Download PDF">pdf</a>, <a href="/format/2310.06114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interactive Real-World Simulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengjiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Ghasemipour%2C+K">Kamyar Ghasemipour</a>, 
<a href="/search/cs?searchtype=author&query=Tompson%2C+J">Jonathan Tompson</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://universal-simulator.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Generative models trained on internet data have revolutionized how text,
image, and video content can be created. Perhaps the next milestone for
generative models is to simulate realistic experience in response to actions
taken by humans, robots, and other interactive agents. Applications of a
real-world simulator range from controllable content creation in games and
movies, to training embodied agents purely in simulation that can be directly
deployed in the real world. We explore the possibility of learning a universal
simulator (UniSim) of real-world interaction through generative modeling. We
first make the important observation that natural datasets available for
learning a real-world simulator are often rich along different axes (e.g.,
abundant objects in image data, densely sampled actions in robotics data, and
diverse movements in navigation data). With careful orchestration of diverse
datasets, each providing a different aspect of the overall experience, UniSim
can emulate how humans and agents interact with the world by simulating the
visual outcome of both high-level instructions such as "open the drawer" and
low-level controls such as "move by x, y" from otherwise static scenes and
objects. There are numerous use cases for such a real-world simulator. As an
example, we use UniSim to train both high-level vision-language planners and
low-level reinforcement learning policies, each of which exhibit zero-shot
real-world transfer after training purely in a learned real-world simulator. We
also show that other types of intelligence such as video captioning models can
benefit from training with simulated experience in UniSim, opening up even
wider applications. Video demos can be found at
https://universal-simulator.github.io.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06116" title="Abstract">arXiv:2310.06116</a> [<a href="/pdf/2310.06116" title="Download PDF">pdf</a>, <a href="/format/2310.06116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OptiMUS: Optimization Modeling Using mip Solvers and large language  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AhmadiTeshnizi%2C+A">Ali AhmadiTeshnizi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wenzhi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Udell%2C+M">Madeleine Udell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Optimization problems are pervasive across various sectors, from
manufacturing and distribution to healthcare. However, most such problems are
still solved heuristically by hand rather than optimally by state-of-the-art
solvers, as the expertise required to formulate and solve these problems limits
the widespread adoption of optimization tools and techniques. We introduce
OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and
solve MILP problems from their natural language descriptions. OptiMUS is
capable of developing mathematical models, writing and debugging solver code,
developing tests, and checking the validity of generated solutions. To
benchmark our agent, we present NLP4LP, a novel dataset of linear programming
(LP) and mixed integer linear programming (MILP) problems. Our experiments
demonstrate that OptiMUS is able to solve 67\% more problems compared to a
basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at
\href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06117" title="Abstract">arXiv:2310.06117</a> [<a href="/pdf/2310.06117" title="Download PDF">pdf</a>, <a href="/format/2310.06117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Take a Step Back: Evoking Reasoning via Abstraction in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H+S">Huaixiu Steven Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Heng-Tze Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E+H">Ed H. Chi</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+Q+V">Quoc V Le</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Denny Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We present Step-Back Prompting, a simple prompting technique that enables
LLMs to do abstractions to derive high-level concepts and first principles from
instances containing specific details. Using the concepts and principles to
guide the reasoning steps, LLMs significantly improve their abilities in
following a correct reasoning path towards the solution. We conduct experiments
of Step-Back Prompting with PaLM-2L models and observe substantial performance
gains on a wide range of challenging reasoning-intensive tasks including STEM,
Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting
improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,
TimeQA by 27%, and MuSiQue by 7%.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06119" title="Abstract">arXiv:2310.06119</a> [<a href="/pdf/2310.06119" title="Download PDF">pdf</a>, <a href="/format/2310.06119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Progress in Multivariate Time Series Forecasting:  Comprehensive Benchmarking and Heterogeneity Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zezhi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yongjun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengqing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+D">Di Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+G">Guangyin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+G">Gao Cong</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+C+S">Christian S. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multivariate Time Series (MTS) widely exists in real-word complex systems,
such as traffic and energy systems, making their forecasting crucial for
understanding and influencing these systems. Recently, deep learning-based
approaches have gained much popularity for effectively modeling temporal and
spatial dependencies in MTS, specifically in Long-term Time Series Forecasting
(LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking
issue and the choice of technical approaches have been hotly debated in related
work. Such controversies significantly hinder our understanding of progress in
this field. Thus, this paper aims to address these controversies to present
insights into advancements achieved. To resolve benchmarking issues, we
introduce BasicTS, a benchmark designed for fair comparisons in MTS
forecasting. BasicTS establishes a unified training pipeline and reasonable
evaluation settings, enabling an unbiased evaluation of over 30 popular MTS
forecasting models on more than 18 datasets. Furthermore, we highlight the
heterogeneity among MTS datasets and classify them based on temporal and
spatial characteristics. We further prove that neglecting heterogeneity is the
primary reason for generating controversies in technical approaches. Moreover,
based on the proposed BasicTS and rich heterogeneous MTS datasets, we conduct
an exhaustive and reproducible performance and efficiency comparison of popular
models, providing insights for researchers in selecting and designing MTS
forecasting models.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06121" title="Abstract">arXiv:2310.06121</a> [<a href="/pdf/2310.06121" title="Download PDF">pdf</a>, <a href="/ps/2310.06121" title="Download PostScript">ps</a>, <a href="/format/2310.06121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Innermost to Full Almost-Sure Termination of Probabilistic Term  Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kassing%2C+J">Jan-Christoph Kassing</a>, 
<a href="/search/cs?searchtype=author&query=Frohn%2C+F">Florian Frohn</a>, 
<a href="/search/cs?searchtype=author&query=Giesl%2C+J">J&#xfc;rgen Giesl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">There are many evaluation strategies for term rewrite systems, but proving
termination automatically is usually easiest for innermost rewriting. Several
syntactic criteria exist when innermost termination implies full termination.
We adapt these criteria to the probabilistic setting, e.g., we show when it
suffices to analyze almost-sure termination (AST) w.r.t. innermost rewriting to
prove full AST of probabilistic term rewrite systems (PTRSs). These criteria
also apply to other notions of termination like positive AST. We implemented
and evaluated our new contributions in the tool AProVE.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06122" title="Abstract">arXiv:2310.06122</a> [<a href="/pdf/2310.06122" title="Download PDF">pdf</a>, <a href="/format/2310.06122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text to Knowledge with Graphs: modelling, querying and exploiting  textual content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vargas-Solar%2C+G">Genoveva Vargas-Solar</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+M+H+F">Mirian Halfeld Ferrari Alves</a>, 
<a href="/search/cs?searchtype=author&query=Forst%2C+A+M">Anne-Lyse Minard Forst</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">This paper highlights the challenges, current trends, and open issues related
to the representation, querying and analytics of content extracted from texts.
The internet contains vast text-based information on various subjects,
including commercial documents, medical records, scientific experiments,
engineering tests, and events that impact urban and natural environments.
Extracting knowledge from this text involves understanding the nuances of
natural language and accurately representing the content without losing
information. This allows knowledge to be accessed, inferred, or discovered. To
achieve this, combining results from various fields, such as linguistics,
natural language processing, knowledge representation, data storage, querying,
and analytics, is necessary. The vision in this paper is that graphs can be a
well-suited text content representation once annotated and the right querying
and analytics techniques are applied. This paper discusses this hypothesis from
the perspective of linguistics, natural language processing, graph models and
databases and artificial intelligence provided by the panellists of the DOING
session in the MADICS Symposium 2022.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06123" title="Abstract">arXiv:2310.06123</a> [<a href="/pdf/2310.06123" title="Download PDF">pdf</a>, <a href="/format/2310.06123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-driven Prompt Generation for Vision-Language Models in Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+C">Chen Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Mummadi%2C+C+K">Chaithanya Kumar Mummadi</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+M+R">Madan Ravi Ganesh</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenzhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Lu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wan-Yi Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prompt learning for vision-language models, e.g., CoOp, has shown great
success in adapting CLIP to different downstream tasks, making it a promising
solution for federated learning due to computational reasons. Existing prompt
learning techniques replace hand-crafted text prompts with learned vectors that
offer improvements on seen classes, but struggle to generalize to unseen
classes. Our work addresses this challenge by proposing Federated Text-driven
Prompt Generation (FedTPG), which learns a unified prompt generation network
across multiple remote clients in a scalable manner. The prompt generation
network is conditioned on task-related text input, thus is context-aware,
making it suitable to generalize for both seen and unseen classes. Our
comprehensive empirical evaluations on nine diverse image classification
datasets show that our method is superior to existing federated prompt learning
methods, that achieve overall better generalization on both seen and unseen
classes and is also generalizable to unseen datasets.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06124" title="Abstract">arXiv:2310.06124</a> [<a href="/pdf/2310.06124" title="Download PDF">pdf</a>, <a href="/format/2310.06124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factorized Tensor Networks for Multi-Task and Multi-Domain Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garg%2C+Y">Yash Garg</a>, 
<a href="/search/cs?searchtype=author&query=Yismaw%2C+N">Nebiyou Yismaw</a>, 
<a href="/search/cs?searchtype=author&query=Hyder%2C+R">Rakib Hyder</a>, 
<a href="/search/cs?searchtype=author&query=Prater-Bennette%2C+A">Ashley Prater-Bennette</a>, 
<a href="/search/cs?searchtype=author&query=Asif%2C+M+S">M. Salman Asif</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multi-task and multi-domain learning methods seek to learn multiple
tasks/domains, jointly or one after another, using a single unified network.
The key challenge and opportunity is to exploit shared information across tasks
and domains to improve the efficiency of the unified network. The efficiency
can be in terms of accuracy, storage cost, computation, or sample complexity.
In this paper, we propose a factorized tensor network (FTN) that can achieve
accuracy comparable to independent single-task/domain networks with a small
number of additional parameters. FTN uses a frozen backbone network from a
source model and incrementally adds task/domain-specific low-rank tensor
factors to the shared frozen network. This approach can adapt to a large number
of target domains and tasks without catastrophic forgetting. Furthermore, FTN
requires a significantly smaller number of task-specific parameters compared to
existing methods. We performed experiments on widely used multi-domain and
multi-task datasets. We show the experiments on convolutional-based
architecture with different backbones and on transformer-based architecture. We
observed that FTN achieves similar accuracy as single-task/domain methods while
using only a fraction of additional parameters per task.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06125" title="Abstract">arXiv:2310.06125</a> [<a href="/pdf/2310.06125" title="Download PDF">pdf</a>, <a href="/format/2310.06125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Time Domain Conformer Models for Monaural Speech Separation in Noisy  Reverberant Acoustic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravenscroft%2C+W">William Ravenscroft</a>, 
<a href="/search/cs?searchtype=author&query=Goetze%2C+S">Stefan Goetze</a>, 
<a href="/search/cs?searchtype=author&query=Hain%2C+T">Thomas Hain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ASRU Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech separation remains an important topic for multi-speaker technology
researchers. Convolution augmented transformers (conformers) have performed
well for many speech processing tasks but have been under-researched for speech
separation. Most recent state-of-the-art (SOTA) separation models have been
time-domain audio separation networks (TasNets). A number of successful models
have made use of dual-path (DP) networks which sequentially process local and
global information. Time domain conformers (TD-Conformers) are an analogue of
the DP approach in that they also process local and global context sequentially
but have a different time complexity function. It is shown that for realistic
shorter signal lengths, conformers are more efficient when controlling for
feature dimension. Subsampling layers are proposed to further improve
computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB
SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06131" title="Abstract">arXiv:2310.06131</a> [<a href="/pdf/2310.06131" title="Download PDF">pdf</a>, <a href="/format/2310.06131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Layer-wise Equivariances Automatically using Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Ouderaa%2C+T+F+A">Tycho F.A. van der Ouderaa</a>, 
<a href="/search/cs?searchtype=author&query=Immer%2C+A">Alexander Immer</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Wilk%2C+M">Mark van der Wilk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Convolutions encode equivariance symmetries into neural networks leading to
better generalisation performance. However, symmetries provide fixed hard
constraints on the functions a network can represent, need to be specified in
advance, and can not be adapted. Our goal is to allow flexible symmetry
constraints that can automatically be learned from data using gradients.
Learning symmetry and associated weight connectivity structures from scratch is
difficult for two reasons. First, it requires efficient and flexible
parameterisations of layer-wise equivariances. Secondly, symmetries act as
constraints and are therefore not encouraged by training losses measuring data
fit. To overcome these challenges, we improve parameterisations of soft
equivariance and learn the amount of equivariance in layers by optimising the
marginal likelihood, estimated using differentiable Laplace approximations. The
objective balances data fit and model complexity enabling layer-wise symmetry
discovery in deep networks. We demonstrate the ability to automatically learn
layer-wise equivariances on image classification tasks, achieving equivalent or
improved performance over baselines with hard-coded symmetry.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06136" title="Abstract">arXiv:2310.06136</a> [<a href="/pdf/2310.06136" title="Download PDF">pdf</a>, <a href="/format/2310.06136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Player Engagement in Tom Clancy&#x27;s The Division 2: A  Multimodal Approach via Pixels and Gamepad Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pinitas%2C+K">Kosmas Pinitas</a>, 
<a href="/search/cs?searchtype=author&query=Renaudie%2C+D">David Renaudie</a>, 
<a href="/search/cs?searchtype=author&query=Thomsen%2C+M">Mike Thomsen</a>, 
<a href="/search/cs?searchtype=author&query=Barthet%2C+M">Matthew Barthet</a>, 
<a href="/search/cs?searchtype=author&query=Makantasis%2C+K">Konstantinos Makantasis</a>, 
<a href="/search/cs?searchtype=author&query=Liapis%2C+A">Antonios Liapis</a>, 
<a href="/search/cs?searchtype=author&query=Yannakakis%2C+G+N">Georgios N. Yannakakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, accepted for publication and presentation at 2023 25th ACM International Conference on Multimodal Interaction (ICMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper introduces a large scale multimodal corpus collected for the
purpose of analysing and predicting player engagement in commercial-standard
games. The corpus is solicited from 25 players of the action role-playing game
Tom Clancy's The Division 2, who annotated their level of engagement using a
time-continuous annotation tool. The cleaned and processed corpus presented in
this paper consists of nearly 20 hours of annotated gameplay videos accompanied
by logged gamepad actions. We report preliminary results on predicting
long-term player engagement based on in-game footage and game controller
actions using Convolutional Neural Network architectures. Results obtained
suggest we can predict the player engagement with up to 72% accuracy on average
(88% at best) when we fuse information from the game footage and the player's
controller input. Our findings validate the hypothesis that long-term (i.e. 1
hour of play) engagement can be predicted efficiently solely from pixels and
gamepad actions.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06138" title="Abstract">arXiv:2310.06138</a> [<a href="/pdf/2310.06138" title="Download PDF">pdf</a>, <a href="/format/2310.06138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Layout Sequence Prediction From Noisy Mobile Modality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haichao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongsheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Shimizu%2C+T">Takayuki Shimizu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yun Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 31st ACM International Conference on Multimedia 2023 (MM 23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Robotics (cs.RO)

</div>
<p class="mathjax">Trajectory prediction plays a vital role in understanding pedestrian movement
for applications such as autonomous driving and robotics. Current trajectory
prediction models depend on long, complete, and accurately observed sequences
from visual modalities. Nevertheless, real-world situations often involve
obstructed cameras, missed objects, or objects out of sight due to
environmental factors, leading to incomplete or noisy trajectories. To overcome
these limitations, we propose LTrajDiff, a novel approach that treats objects
obstructed or out of sight as equally important as those with fully visible
trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount
out-of-sight constraints, albeit introducing new challenges such as modality
fusion, noisy data, and the absence of spatial layout and object size
information. We employ a denoising diffusion model to predict precise layout
sequences from noisy mobile data using a coarse-to-fine diffusion strategy,
incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model
predicts layout sequences by implicitly inferring object size and projection
status from a single reference timestamp or significantly obstructed sequences.
Achieving SOTA results in randomly obstructed experiments and extremely short
input experiments, our model illustrates the effectiveness of leveraging noisy
mobile data. In summary, our approach offers a promising solution to the
challenges faced by layout sequence and trajectory prediction models in
real-world settings, paving the way for utilizing sensor data from mobile
phones to accurately predict pedestrian bounding box trajectories. To the best
of our knowledge, this is the first work that addresses severely obstructed and
extremely short layout sequences by combining vision with noisy mobile
modality, making it the pioneering work in the field of layout sequence
trajectory prediction.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06139" title="Abstract">arXiv:2310.06139</a> [<a href="/pdf/2310.06139" title="Download PDF">pdf</a>, <a href="/ps/2310.06139" title="Download PostScript">ps</a>, <a href="/format/2310.06139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Correlation between Random Variables and their Principal  Components
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gniazdowski%2C+Z">Zenon Gniazdowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Zeszyty Naukowe WWSI, No 28, Vol. 17, 2023, pp. 41-55
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">The article attempts to find an algebraic formula describing the correlation
coefficients between random variables and the principal components representing
them. As a result of the analysis, starting from selected statistics relating
to individual random variables, the equivalents of these statistics relating to
a set of random variables were presented in the language of linear algebra,
using the concepts of vector and matrix. This made it possible, in subsequent
steps, to derive the expected formula. The formula found is identical to the
formula used in Factor Analysis to calculate factor loadings. The discussion
showed that it is possible to apply this formula to optimize the number of
principal components in Principal Component Analysis, as well as to optimize
the number of factors in Factor Analysis.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06140" title="Abstract">arXiv:2310.06140</a> [<a href="/pdf/2310.06140" title="Download PDF">pdf</a>, <a href="/format/2310.06140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NP-Hardness of Tensor Network Contraction Ordering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Ling Liang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+L">Lei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoqi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Jianyu Xu and Hanwen Zhang are equal contributors. 10 pages (reference and appendix excluded), 20 pages in total, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">We study the optimal order (or sequence) of contracting a tensor network with
a minimal computational cost. We conclude 2 different versions of this optimal
sequence: that minimize the operation number (OMS) and that minimize the time
complexity (CMS). Existing results only shows that OMS is NP-hard, but no
conclusion on CMS problem. In this work, we firstly reduce CMS to CMS-0, which
is a sub-problem of CMS with no free indices. Then we prove that CMS is easier
than OMS, both in general and in tree cases. Last but not least, we prove that
CMS is still NP-hard. Based on our results, we have built up relationships of
hardness of different tensor network contraction problems.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06141" title="Abstract">arXiv:2310.06141</a> [<a href="/pdf/2310.06141" title="Download PDF">pdf</a>, <a href="/format/2310.06141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delay-Optimal Service Chain Forwarding and Offloading in Collaborative  Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinkun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+E">Edmund Yeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Collaborative edge computing (CEC) is an emerging paradigm for heterogeneous
devices to collaborate on edge computation jobs. For congestible links and
computing units, delay-optimal forwarding and offloading for service chain
tasks (e.g., DNN with vertical split) in CEC remains an open problem. In this
paper, we formulate the service chain forwarding and offloading in CEC with
arbitrary topology and heterogeneous transmission/computation capability, and
aim to minimize the network aggregated cost. We consider congestion-aware
nonlinear cost functions that cover various performance metrics and
constraints, such as average queueing delay with limited processor capacity. We
solve the non-convex optimization problem globally by analyzing the KKT
condition and proposing a sufficiency optimality condition. We propose a
polynomial-time distributed algorithm that converges to the global optimum. The
algorithm adapts to changes in input rates and network topology, and can be
implemented as an online algorithm. Numerical evaluation shows that our method
significantly outperforms baselines in multiple network instances, especially
in congested scenarios.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06147" title="Abstract">arXiv:2310.06147</a> [<a href="/pdf/2310.06147" title="Download PDF">pdf</a>, <a href="/format/2310.06147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning in the Era of LLMs: What is Essential? What is  needed? An RL Perspective on RLHF, Prompting, and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in Large Language Models (LLMs) have garnered wide
attention and led to successful products such as ChatGPT and GPT-4. Their
proficiency in adhering to instructions and delivering harmless, helpful, and
honest (3H) responses can largely be attributed to the technique of
Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to
link the research in conventional RL to RL techniques used in LLM research.
Demystify this technique by discussing why, when, and how RL excels.
Furthermore, we explore potential future avenues that could either benefit from
or contribute to RLHF research.
<br />Highlighted Takeaways:
<br />1. RLHF is Online Inverse RL with Offline Demonstration Data.
<br />2. RLHF $&gt;$ SFT because Imitation Learning (and Inverse RL) $&gt;$ Behavior
Cloning (BC) by alleviating the problem of compounding error.
<br />3. The RM step in RLHF generates a proxy of the expensive human feedback,
such an insight can be generalized to other LLM tasks such as prompting
evaluation and optimization where feedback is also expensive.
<br />4. The policy learning in RLHF is more challenging than conventional problems
studied in IRL due to their high action dimensionality and feedback sparsity.
<br />5. The main superiority of PPO over off-policy value-based methods is its
stability gained from (almost) on-policy data and conservative policy updates.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06148" title="Abstract">arXiv:2310.06148</a> [<a href="/pdf/2310.06148" title="Download PDF">pdf</a>, <a href="/format/2310.06148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Transfer Learning and Gradient-Based Meta-Learning  Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huisman%2C+M">Mike Huisman</a>, 
<a href="/search/cs?searchtype=author&query=Plaat%2C+A">Aske Plaat</a>, 
<a href="/search/cs?searchtype=author&query=van+Rijn%2C+J+N">Jan N. van Rijn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Machine Learning Journal, Special Issue on Discovery Science 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep neural networks can yield good performance on various tasks but often
require large amounts of data to train them. Meta-learning received
considerable attention as one approach to improve the generalization of these
networks from a limited amount of data. Whilst meta-learning techniques have
been observed to be successful at this in various scenarios, recent results
suggest that when evaluated on tasks from a different data distribution than
the one used for training, a baseline that simply finetunes a pre-trained
network may be more effective than more complicated meta-learning techniques
such as MAML, which is one of the most popular meta-learning techniques. This
is surprising as the learning behaviour of MAML mimics that of finetuning: both
rely on re-using learned features. We investigate the observed performance
differences between finetuning, MAML, and another meta-learning technique
called Reptile, and show that MAML and Reptile specialize for fast adaptation
in low-data regimes of similar data distribution as the one used for training.
Our findings show that both the output layer and the noisy training conditions
induced by data scarcity play important roles in facilitating this
specialization for MAML. Lastly, we show that the pre-trained features as
obtained by the finetuning baseline are more diverse and discriminative than
those learned by MAML and Reptile. Due to this lack of diversity and
distribution specialization, MAML and Reptile may fail to generalize to
out-of-distribution tasks whereas finetuning can fall back on the diversity of
the learned features.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06150" title="Abstract">arXiv:2310.06150</a> [<a href="/pdf/2310.06150" title="Download PDF">pdf</a>, <a href="/format/2310.06150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Diffusion Model for DNA Sequence Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zehui Li</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yuhao Ni</a>, 
<a href="/search/cs?searchtype=author&query=Huygelen%2C+T+A+B">Tim August B. Huygelen</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Akashaditya Das</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Guoxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Stan%2C+G">Guy-Bart Stan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The harnessing of machine learning, especially deep generative models, has
opened up promising avenues in the field of synthetic DNA sequence generation.
Whilst Generative Adversarial Networks (GANs) have gained traction for this
application, they often face issues such as limited sample diversity and mode
collapse. On the other hand, Diffusion Models are a promising new class of
generative models that are not burdened with these problems, enabling them to
reach the state-of-the-art in domains such as image generation. In light of
this, we propose a novel latent diffusion model, DiscDiff, tailored for
discrete DNA sequence generation. By simply embedding discrete DNA sequences
into a continuous latent space using an autoencoder, we are able to leverage
the powerful generative abilities of continuous diffusion models for the
generation of discrete data. Additionally, we introduce Fr\'echet
Reconstruction Distance (FReD) as a new metric to measure the sample quality of
DNA sequence generations. Our DiscDiff model demonstrates an ability to
generate synthetic DNA sequences that align closely with real DNA in terms of
Motif Distribution, Latent Embedding Distribution (FReD), and Chromatin
Profiles. Additionally, we contribute a comprehensive cross-species dataset of
150K unique promoter-gene sequences from 15 species, enriching resources for
future generative modelling in genomics. We will make our code public upon
publication.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06153" title="Abstract">arXiv:2310.06153</a> [<a href="/pdf/2310.06153" title="Download PDF">pdf</a>, <a href="/format/2310.06153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Robot Task Assignment and Path Finding for Time-Sensitive Missions  with Online Task Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thorne%2C+D">David Thorne</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+B+T">Brett T. Lopez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Executing time-sensitive multi-robot missions involves two distinct problems:
Multi-Robot Task Assignment (MRTA) and Multi-Agent Path Finding (MAPF).
Computing safe paths that complete every task and minimize the time to mission
completion, or makespan, is a significant computational challenge even for
small teams. In many missions, tasks can be generated during execution which is
typically handled by either recomputing task assignments and paths from
scratch, or by modifying existing plans using approximate approaches. While
performing task reassignment and path finding from scratch produces
theoretically optimal results, the computational load makes it too expensive
for online implementation. In this work, we present Time-Sensitive Online Task
Assignment and Navigation (TSOTAN), a framework which can quickly incorporate
online generated tasks while guaranteeing bounded suboptimal task assignment
makespans. It does this by assessing the quality of partial task reassignments
and only performing a complete reoptimization when the makespan exceeds a user
specified suboptimality bound. Through experiments in 2D environments we
demonstrate TSOTAN's ability to produce quality solutions with computation
times suitable for online implementation.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06155" title="Abstract">arXiv:2310.06155</a> [<a href="/pdf/2310.06155" title="Download PDF">pdf</a>, <a href="/format/2310.06155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How AI Processing Delays Foster Creativity: Exploring Research Question  Co-Creation with an LLM-based Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiren Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Si Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haocong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mengxia Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+X">Xiao Ran</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+A">Andrew Mo</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yiliu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yun Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Developing novel research questions (RQs) often requires extensive literature
reviews, especially for interdisciplinary fields. Leveraging Large Language
Models (LLMs), we built an LLM-based agent system, called CoQuest, supporting
RQ development through human-AI co-creation. We conducted an experimental
design with 20 participants to examine the effect of two interaction designs:
breadth-first and depth-first RQ generation. The results showed that
participants found the breadth-first approach more creative and trustworthy
upon task completion. However, during the task, they rated the RQs generated
through the depth-first approach as more creative. We also discovered that AI
processing delays allowed users to contemplate multiple RQs simultaneously,
resulting in more generated RQs and an increased sense of perceived control.
Our work makes both theoretical and practical contributions by proposing and
assessing a mental model for human-AI co-creation RQs.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06157" title="Abstract">arXiv:2310.06157</a> [<a href="/pdf/2310.06157" title="Download PDF">pdf</a>, <a href="/format/2310.06157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on  Differentiable Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kelshaw%2C+D">Daniel Kelshaw</a>, 
<a href="/search/cs?searchtype=author&query=Magri%2C+L">Luca Magri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to NeurIPS 2023: Symmetry and Geometry in Neural Representations Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Manifolds discovered by machine learning models provide a compact
representation of the underlying data. Geodesics on these manifolds define
locally length-minimising curves and provide a notion of distance, which are
key for reduced-order modelling, statistical inference, and interpolation. In
this work, we propose a model-based parameterisation for distance fields and
geodesic flows on manifolds, exploiting solutions of a manifold-augmented
Eikonal equation. We demonstrate how the geometry of the manifold impacts the
distance field, and exploit the geodesic flow to obtain globally
length-minimising curves directly. This work opens opportunities for statistics
and reduced-order modelling on differentiable manifolds.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06159" title="Abstract">arXiv:2310.06159</a> [<a href="/pdf/2310.06159" title="Download PDF">pdf</a>, <a href="/format/2310.06159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled  Gradient Descent, Even with Overparameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Cong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xingyu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+T">Tian Tong</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Book chapter for "Explorations in the Mathematics of Data Science - The Inaugural Volume of the Center for Approximation and Mathematical Data Analytics". arXiv admin note: text overlap with <a href="/abs/2104.14526">arXiv:2104.14526</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Many problems encountered in science and engineering can be formulated as
estimating a low-rank object (e.g., matrices and tensors) from incomplete, and
possibly corrupted, linear measurements. Through the lens of matrix and tensor
factorization, one of the most popular approaches is to employ simple iterative
algorithms such as gradient descent (GD) to recover the low-rank factors
directly, which allow for small memory and computation footprints. However, the
convergence rate of GD depends linearly, and sometimes even quadratically, on
the condition number of the low-rank object, and therefore, GD slows down
painstakingly when the problem is ill-conditioned. This chapter introduces a
new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that
provably converges linearly at a constant rate independent of the condition
number of the low-rank object, while maintaining the low per-iteration cost of
gradient descent for a variety of tasks including sensing, robust principal
component analysis and completion. In addition, ScaledGD continues to admit
fast global convergence to the minimax-optimal solution, again almost
independent of the condition number, from a small random initialization when
the rank is over-specified in the presence of Gaussian noise. In total,
ScaledGD highlights the power of appropriate preconditioning in accelerating
nonconvex statistical estimation, where the iteration-varying preconditioners
promote desirable invariance properties of the trajectory with respect to the
symmetry in low-rank factorization without hurting generalization.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06160" title="Abstract">arXiv:2310.06160</a> [<a href="/pdf/2310.06160" title="Download PDF">pdf</a>, <a href="/format/2310.06160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy Based Multi-robot Active SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M+F">Muhammad Farhan Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Maragliano%2C+M">Matteo Maragliano</a>, 
<a href="/search/cs?searchtype=author&query=Fr%C3%A9mont%2C+V">Vincent Fr&#xe9;mont</a>, 
<a href="/search/cs?searchtype=author&query=Recchiuto%2C+C+T">Carmine Tommaso Recchiuto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this article, we present an efficient multi-robot active SLAM framework
that involves a frontier-sharing method for maximum exploration of an unknown
environment. It encourages the robots to spread into the environment while
weighting the goal frontiers with the pose graph SLAM uncertainly and path
entropy. Our approach works on a limited number of frontier points and weights
the goal frontiers with a utility function that encapsulates both the SLAM and
map uncertainties, thus providing an efficient and not computationally
expensive solution. Our approach has been tested on publicly available
simulation environments and on real robots. An accumulative 31% more coverage
than similar state-of-the-art approaches has been obtained, proving the
capability of our approach for efficient environment exploration.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06161" title="Abstract">arXiv:2310.06161</a> [<a href="/pdf/2310.06161" title="Download PDF">pdf</a>, <a href="/format/2310.06161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Simplicity Bias in Deep Learning for Improved OOD  Generalization and Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vasudeva%2C+B">Bhavya Vasudeva</a>, 
<a href="/search/cs?searchtype=author&query=Shahabi%2C+K">Kameron Shahabi</a>, 
<a href="/search/cs?searchtype=author&query=Sharan%2C+V">Vatsal Sharan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Neural networks (NNs) are known to exhibit simplicity bias where they tend to
prefer learning 'simple' features over more 'complex' ones, even when the
latter may be more informative. Simplicity bias can lead to the model making
biased predictions which have poor out-of-distribution (OOD) generalization. To
address this, we propose a framework that encourages the model to use a more
diverse set of features to make predictions. We first train a simple model, and
then regularize the conditional mutual information with respect to it to obtain
the final model. We demonstrate the effectiveness of this framework in various
problem settings and real-world applications, showing that it effectively
addresses simplicity bias and leads to more features being used, enhances OOD
generalization, and improves subgroup robustness and fairness. We complement
these results with theoretical analyses of the effect of the regularization and
its OOD generalization properties.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06164" title="Abstract">arXiv:2310.06164</a> [<a href="/pdf/2310.06164" title="Download PDF">pdf</a>, <a href="/format/2310.06164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEUX: Active Exploration for Learning Unsupervised Depth Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chanc%C3%A1n%2C+M">Marvin Chanc&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Alex Wong</a>, 
<a href="/search/cs?searchtype=author&query=Abraham%2C+I">Ian Abraham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Depth perception models are typically trained on non-interactive datasets
with predefined camera trajectories. However, this often introduces systematic
biases into the learning process correlated to specific camera paths chosen
during data acquisition. In this paper, we investigate the role of how data is
collected for learning depth completion, from a robot navigation perspective,
by leveraging 3D interactive environments. First, we evaluate four depth
completion models trained on data collected using conventional navigation
techniques. Our key insight is that existing exploration paradigms do not
necessarily provide task-specific data points to achieve competent unsupervised
depth completion learning. We then find that data collected with respect to
photometric reconstruction has a direct positive influence on model
performance. As a result, we develop an active, task-informed, depth
uncertainty-based motion planning approach for learning depth completion, which
we call DEpth Uncertainty-guided eXploration (DEUX). Training with data
collected by our approach improves depth completion by an average greater than
18% across four depth completion models compared to existing exploration
methods on the MP3D test set. We show that our approach further improves
zero-shot generalization, while offering new insights into integrating robot
learning-based depth estimation.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06165" title="Abstract">arXiv:2310.06165</a> [<a href="/pdf/2310.06165" title="Download PDF">pdf</a>, <a href="/format/2310.06165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAW-coref: Conjunction-Aware Word-level Coreference Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Oosterlinck%2C+K">Karel D&#x27;Oosterlinck</a>, 
<a href="/search/cs?searchtype=author&query=Bitew%2C+S+K">Semere Kiros Bitew</a>, 
<a href="/search/cs?searchtype=author&query=Papineau%2C+B">Brandon Papineau</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C">Christopher Potts</a>, 
<a href="/search/cs?searchtype=author&query=Demeester%2C+T">Thomas Demeester</a>, 
<a href="/search/cs?searchtype=author&query=Develder%2C+C">Chris Develder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CRAC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">State-of-the-art coreference resolutions systems depend on multiple LLM calls
per document and are thus prohibitively expensive for many use cases (e.g.,
information extraction with large corpora). The leading word-level coreference
system (WL-coref) attains 96.6% of these SOTA systems' performance while being
much more efficient. In this work, we identify a routine yet important failure
case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We
offer a simple yet effective solution that improves the performance on the
OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level
coreference resolution and expensive SOTA approaches by 34.6%. Our
Conjunction-Aware Word-level coreference model (CAW-coref) and code is
available at https://github.com/KarelDO/wl-coref.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06166" title="Abstract">arXiv:2310.06166</a> [<a href="/pdf/2310.06166" title="Download PDF">pdf</a>, <a href="/format/2310.06166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Threshold Policies with Tight Guarantees for Online Selection with  Convex Costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xiaoqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Siyuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Boutaba%2C+R">Raouf Boutaba</a>, 
<a href="/search/cs?searchtype=author&query=Leon-Garcia%2C+A">Alberto Leon-Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">This paper provides threshold policies with tight guarantees for online
selection with convex cost (OSCC). In OSCC, a seller wants to sell some asset
to a sequence of buyers with the goal of maximizing her profit. The seller can
produce additional units of the asset, but at non-decreasing marginal costs. At
each time, a buyer arrives and offers a price. The seller must make an
immediate and irrevocable decision in terms of whether to accept the offer and
produce/sell one unit of the asset to this buyer. The goal is to develop an
online algorithm that selects a subset of buyers to maximize the seller's
profit, namely, the total selling revenue minus the total production cost. Our
main result is the development of a class of simple threshold policies that are
logistically simple and easy to implement, but have provable optimality
guarantees among all deterministic algorithms. We also derive a lower bound on
competitive ratios of randomized algorithms and prove that the competitive
ratio of our threshold policy asymptotically converges to this lower bound when
the total production output is sufficiently large. Our results generalize and
unify various online search, pricing, and auction problems, and provide a new
perspective on the impact of non-decreasing marginal costs on real-world online
resource allocation problems.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06167" title="Abstract">arXiv:2310.06167</a> [<a href="/pdf/2310.06167" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictable Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lexin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Moreno-Casares%2C+P+A">Pablo A. Moreno-Casares</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Plumed%2C+F">Fernando Mart&#xed;nez-Plumed</a>, 
<a href="/search/cs?searchtype=author&query=Burden%2C+J">John Burden</a>, 
<a href="/search/cs?searchtype=author&query=Burnell%2C+R">Ryan Burnell</a>, 
<a href="/search/cs?searchtype=author&query=Cheke%2C+L">Lucy Cheke</a>, 
<a href="/search/cs?searchtype=author&query=Ferri%2C+C">C&#xe8;sar Ferri</a>, 
<a href="/search/cs?searchtype=author&query=Marcoci%2C+A">Alexandru Marcoci</a>, 
<a href="/search/cs?searchtype=author&query=Mehrbakhsh%2C+B">Behzad Mehrbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Moros-Daval%2C+Y">Yael Moros-Daval</a>, 
<a href="/search/cs?searchtype=author&query=h%C3%89igeartaigh%2C+S+%C3%93">Se&#xe1;n &#xd3; h&#xc9;igeartaigh</a>, 
<a href="/search/cs?searchtype=author&query=Rutar%2C+D">Danaja Rutar</a>, 
<a href="/search/cs?searchtype=author&query=Schellaert%2C+W">Wout Schellaert</a>, 
<a href="/search/cs?searchtype=author&query=Voudouris%2C+K">Konstantinos Voudouris</a>, 
<a href="/search/cs?searchtype=author&query=Hern%C3%A1ndez-Orallo%2C+J">Jos&#xe9; Hern&#xe1;ndez-Orallo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages excluding references, 4 figures, and 2 tables. Paper Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We introduce the fundamental ideas and challenges of Predictable AI, a
nascent research area that explores the ways in which we can anticipate key
indicators of present and future AI ecosystems. We argue that achieving
predictability is crucial for fostering trust, liability, control, alignment
and safety of AI ecosystems, and thus should be prioritised over performance.
While distinctive from other areas of technical and non-technical AI research,
the questions, hypotheses and challenges relevant to Predictable AI were yet to
be clearly described. This paper aims to elucidate them, calls for identifying
paths towards AI predictability and outlines the potential impact of this
emergent field.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06169" title="Abstract">arXiv:2310.06169</a> [<a href="/pdf/2310.06169" title="Download PDF">pdf</a>, <a href="/format/2310.06169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Robust Walking Gaits via Discrete-Time Barrier Functions  with Application to Multi-Contact Exoskeleton Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tucker%2C+M">Maegan Tucker</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kejun Li</a>, 
<a href="/search/cs?searchtype=author&query=Ames%2C+A+D">Aaron D. Ames</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Successfully achieving bipedal locomotion remains challenging due to
real-world factors such as model uncertainty, random disturbances, and
imperfect state estimation. In this work, we propose the use of discrete-time
barrier functions to certify hybrid forward invariance of reduced step-to-step
dynamics. The size of these invariant sets can then be used as a metric for
locomotive robustness. We demonstrate an application of this metric towards
synthesizing robust nominal walking gaits using a simulation-in-the-loop
approach. This procedure produces reference motions with step-to-step dynamics
that are maximally forward-invariant with respect to the reduced representation
of choice. The results demonstrate robust locomotion for both flat-foot walking
and multi-contact walking on the Atalante lower-body exoskeleton.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06170" title="Abstract">arXiv:2310.06170</a> [<a href="/pdf/2310.06170" title="Download PDF">pdf</a>, <a href="/ps/2310.06170" title="Download PostScript">ps</a>, <a href="/format/2310.06170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward More Accurate and Robust Optimal Power Flow for Distribution  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hamilton%2C+D">Dakota Hamilton</a>, 
<a href="/search/eess?searchtype=author&query=Navarro%2C+L">Loraine Navarro</a>, 
<a href="/search/eess?searchtype=author&query=Aliprantis%2C+D">Dionysios Aliprantis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The objective of this paper is to improve the accuracy and robustness of
optimal power flow (OPF) formulations for distribution systems modeled down to
the low-voltage point of connection of individual buildings. An approach for
addressing the uncertain switching behavior of building loads(e.g., air
conditioners, water heaters, or pool pumps) and variable renewable generation
(e.g., rooftop solar) in the OPF is proposed. Rather than using time-averaged
forecasts to determine voltage magnitude constraints, we leverage worst-case
minimum and maximum forecasts of loads and distributed energy resource
generation. Sensitivities of the power flow equations are used to predict how
these deviations in load and renewable generation will impact system voltages,
and the voltage constraints in the OPF are dynamically adjusted to mitigate
voltage violations due to this uncertainty. A methodology for incorporating
models of split-phase components and transformer core losses in the OPF
formulation is also proposed. The proposed approach is validated through
numerical case studies on a realistic distribution feeder using GridLAB-D, a
distribution system simulation software.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06171" title="Abstract">arXiv:2310.06171</a> [<a href="/pdf/2310.06171" title="Download PDF">pdf</a>, <a href="/format/2310.06171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-Consistent Neural Networks for Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+K">Kaustubh Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Souradeep Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Jayaraman%2C+D">Dinesh Jayaraman</a>, 
<a href="/search/cs?searchtype=author&query=Weimer%2C+J">James Weimer</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Insup Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages (9 main pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Imitation learning considerably simplifies policy synthesis compared to
alternative approaches by exploiting access to expert demonstrations. For such
imitation policies, errors away from the training samples are particularly
critical. Even rare slip-ups in the policy action outputs can compound quickly
over time, since they lead to unfamiliar future states where the policy is
still more likely to err, eventually causing task failures. We revisit simple
supervised ``behavior cloning'' for conveniently training the policy from
nothing more than pre-recorded demonstrations, but carefully design the model
class to counter the compounding error phenomenon. Our ``memory-consistent
neural network'' (MCNN) outputs are hard-constrained to stay within clearly
specified permissible regions anchored to prototypical ``memory'' training
samples. We provide a guaranteed upper bound for the sub-optimality gap induced
by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP,
Transformer, and Diffusion backbones, spanning dexterous robotic manipulation
and driving, proprioceptive inputs and visual inputs, and varying sizes and
types of demonstration data, we find large and consistent gains in performance,
validating that MCNNs are better-suited than vanilla deep neural networks for
imitation learning applications. Website:
https://sites.google.com/view/mcnn-imitation
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06174" title="Abstract">arXiv:2310.06174</a> [<a href="/pdf/2310.06174" title="Download PDF">pdf</a>, <a href="/format/2310.06174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How does prompt engineering affect ChatGPT performance on unsupervised  entity resolution?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sisaengsuwanchai%2C+K">Khanin Sisaengsuwanchai</a>, 
<a href="/search/cs?searchtype=author&query=Nananukul%2C+N">Navapat Nananukul</a>, 
<a href="/search/cs?searchtype=author&query=Kejriwal%2C+M">Mayank Kejriwal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Entity Resolution (ER) is the problem of semi-automatically determining when
two entities refer to the same underlying entity, with applications ranging
from healthcare to e-commerce. Traditional ER solutions required considerable
manual expertise, including feature engineering, as well as identification and
curation of training data. In many instances, such techniques are highly
dependent on the domain. With recent advent in large language models (LLMs),
there is an opportunity to make ER much more seamless and domain-independent.
However, it is also well known that LLMs can pose risks, and that the quality
of their outputs can depend on so-called prompt engineering. Unfortunately, a
systematic experimental study on the effects of different prompting methods for
addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper
aims to address this gap by conducting such a study. Although preliminary in
nature, our results show that prompting can significantly affect the quality of
ER, although it affects some metrics more than others, and can also be dataset
dependent.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06176" title="Abstract">arXiv:2310.06176</a> [<a href="/pdf/2310.06176" title="Download PDF">pdf</a>, <a href="/format/2310.06176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factual and Personalized Recommendations using Language Models and  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+J">Jihwan Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+Y">Yinlam Chow</a>, 
<a href="/search/cs?searchtype=author&query=Tennenholtz%2C+G">Guy Tennenholtz</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chih-Wei Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Tulepbergenov%2C+A">Azamat Tulepbergenov</a>, 
<a href="/search/cs?searchtype=author&query=Ghavamzadeh%2C+M">Mohammad Ghavamzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Recommender systems (RSs) play a central role in connecting users to content,
products, and services, matching candidate items to users based on their
preferences. While traditional RSs rely on implicit user feedback signals,
conversational RSs interact with users in natural language. In this work, we
develop a comPelling, Precise, Personalized, Preference-relevant language model
(P4LM) that recommends items to users while putting emphasis on explaining item
characteristics and their relevance. P4LM uses the embedding space
representation of a user's preferences to generate compelling responses that
are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we
develop a joint reward function that measures precision, appeal, and
personalization, which we use as AI-based feedback in a reinforcement
learning-based language model framework. Using the MovieLens 25M dataset, we
demonstrate that P4LM delivers compelling, personalized movie narratives to
users.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06177" title="Abstract">arXiv:2310.06177</a> [<a href="/pdf/2310.06177" title="Download PDF">pdf</a>, <a href="/format/2310.06177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DockGame: Cooperative Games for Multimeric Rigid Protein Docking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Somnath%2C+V+R">Vignesh Ram Somnath</a>, 
<a href="/search/cs?searchtype=author&query=Sessa%2C+P+G">Pier Giuseppe Sessa</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M+R">Maria Rodriguez Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Protein interactions and assembly formation are fundamental to most
biological processes. Predicting the assembly structure from constituent
proteins -- referred to as the protein docking task -- is thus a crucial step
in protein design applications. Most traditional and deep learning methods for
docking have focused mainly on binary docking, following either a search-based,
regression-based, or generative modeling paradigm. In this paper, we focus on
the less-studied multimeric (i.e., two or more proteins) docking problem. We
introduce DockGame, a novel game-theoretic framework for docking -- we view
protein docking as a cooperative game between proteins, where the final
assembly structure(s) constitute stable equilibria w.r.t. the underlying game
potential. Since we do not have access to the true potential, we consider two
approaches - i) learning a surrogate game potential guided by physics-based
energy functions and computing equilibria by simultaneous gradient updates, and
ii) sampling from the Gibbs distribution of the true potential by learning a
diffusion generative model over the action spaces (rotations and translations)
of all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset,
DockGame has much faster runtimes than traditional docking methods, can
generate multiple plausible assembly structures, and achieves comparable
performance to existing binary docking baselines, despite solving the harder
task of coordinating multiple protein chains.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06178" title="Abstract">arXiv:2310.06178</a> [<a href="/pdf/2310.06178" title="Download PDF">pdf</a>, <a href="/format/2310.06178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via  msGeMM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maleki%2C+S">Saeed Maleki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">AI models are increasing in size and recent advancement in the community has
shown that unlike HPC applications where double precision datatype are
required, lower-precision datatypes such as fp8 or int4 are sufficient to bring
the same model quality both for training and inference. Following these trends,
GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8
and int8 GeMM operations with an exceptional performance via Tensor Cores.
However, this paper proposes a new algorithm called msGeMM which shows that AI
models with low-precision datatypes can run with ~2.5x fewer multiplication and
add instructions. Efficient implementation of this algorithm requires special
CUDA cores with the ability to add elements from a small look-up table at the
rate of Tensor Cores.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06179" title="Abstract">arXiv:2310.06179</a> [<a href="/pdf/2310.06179" title="Download PDF">pdf</a>, <a href="/format/2310.06179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Integration for Spatiotemporal Neural Point Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Learning continuous-time point processes is essential to many discrete event
forecasting tasks. However, integration poses a major challenge, particularly
for spatiotemporal point processes (STPPs), as it involves calculating the
likelihood through triple integrals over space and time. Existing methods for
integrating STPP either assume a parametric form of the intensity function,
which lacks flexibility; or approximating the intensity with Monte Carlo
sampling, which introduces numerical errors. Recent work by Omi et al. [2019]
proposes a dual network or AutoInt approach for efficient integration of
flexible intensity function. However, the method only focuses on the 1D
temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP
(Automatic Integration for Spatiotemporal Neural Point Processes) that extends
the AutoInt approach to 3D STPP. We show that direct extension of the previous
work overly constrains the intensity function, leading to poor performance. We
prove consistency of AutoSTPP and validate it on synthetic data and benchmark
real world datasets, showcasing its significant advantage in recovering complex
intensity functions from irregular spatiotemporal events, particularly when the
intensity is sharply localized.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06182" title="Abstract">arXiv:2310.06182</a> [<a href="/pdf/2310.06182" title="Download PDF">pdf</a>, <a href="/format/2310.06182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jiancong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhi-quan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is
found empirically that adversarially robust generalization is crucial in
establishing defense algorithms against adversarial attacks. Therefore, it is
interesting to study the theoretical guarantee of robust generalization. This
paper focuses on norm-based complexity, based on a PAC-Bayes approach
(Neyshabur et al., 2017). The main challenge lies in extending the key
ingredient, which is a weight perturbation bound in standard settings, to the
robust settings. Existing attempts heavily rely on additional strong
assumptions, leading to loose bounds. In this paper, we address this issue and
provide a spectrally-normalized robust generalization bound for DNNs. Compared
to existing bounds, our bound offers two significant advantages: Firstly, it
does not depend on additional assumptions. Secondly, it is considerably
tighter, aligning with the bounds of standard generalization. Therefore, our
result provides a different perspective on understanding robust generalization:
The mismatch terms between standard and robust generalization bounds shown in
previous studies do not contribute to the poor robust generalization. Instead,
these disparities solely due to mathematical issues. Finally, we extend the
main result to adversarial robustness against general non-$\ell_p$ attacks and
other neural network architectures.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06187" title="Abstract">arXiv:2310.06187</a> [<a href="/pdf/2310.06187" title="Download PDF">pdf</a>, <a href="/ps/2310.06187" title="Download PostScript">ps</a>, <a href="/format/2310.06187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-Monte Carlo sparse grid Galerkin finite element methods for linear  elasticity equations with uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dick%2C+J">J. Dick</a>, 
<a href="/search/math?searchtype=author&query=Gia%2C+Q+T+L">Q. T. Le Gia</a>, 
<a href="/search/math?searchtype=author&query=Mustapha%2C+K">K. Mustapha</a>, 
<a href="/search/math?searchtype=author&query=Tran%2C+T">T. Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We explore a linear inhomogeneous elasticity equation with random Lam\'e
parameters. The latter are parameterized by a countably infinite number of
terms in separated expansions. The main aim of this work is to estimate
expected values (considered as an infinite dimensional integral on the
parametric space corresponding to the random coefficients) of linear
functionals acting on the solution of the elasticity equation. To achieve this,
the expansions of the random parameters are truncated, a high-order quasi-Monte
Carlo (QMC) is combined with a sparse grid approach to approximate the high
dimensional integral, and a Galerkin finite element method (FEM) is introduced
to approximate the solution of the elasticity equation over the physical
domain. The error estimates from (1) truncating the infinite expansion, (2) the
Galerkin FEM, and (3) the QMC sparse grid quadrature rule are all studied. For
this purpose, we show certain required regularity properties of the continuous
solution with respect to both the parametric and physical variables. To achieve
our theoretical regularity and convergence results, some reasonable assumptions
on the expansions of the random coefficients are imposed. Finally, some
numerical results are delivered.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06193" title="Abstract">arXiv:2310.06193</a> [<a href="/pdf/2310.06193" title="Download PDF">pdf</a>, <a href="/format/2310.06193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Natural Indirect Adaptive Controller for a Satellite-Mounted  Manipulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Giordano%2C+J">Jacopo Giordano</a>, 
<a href="/search/eess?searchtype=author&query=Cenedese%2C+A">Angelo Cenedese</a>, 
<a href="/search/eess?searchtype=author&query=Serrani%2C+A">Andrea Serrani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The work considers the design of an indirect adaptive controller for a
satellite equipped with a robotic arm manipulating an object. Uncertainty on
the manipulated object can considerably impact the overall behavior of the
system. In addition, the dynamics of the actuators of the base satellite are
non-linear and can be affected by malfunctioning. Neglecting these two
phenomena may lead to excessive control effort or degrade performance. An
indirect adaptive control approach is pursued, which allows consideration of
relevant features of the actuators dynamics, such as loss of effectiveness.
Furthermore, an adaptive law that preserves the physical consistency of the
inertial parameters of the various rigid bodies comprising the system is
employed. The performance and robustness of the controller are first analyzed
and then validated in simulation.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06195" title="Abstract">arXiv:2310.06195</a> [<a href="/pdf/2310.06195" title="Download PDF">pdf</a>, <a href="/format/2310.06195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale hierarchical decomposition methods for images corrupted by  multiplicative noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Barnett%2C+J">Joel Barnett</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wen Li</a>, 
<a href="/search/math?searchtype=author&query=Resmerita%2C+E">Elena Resmerita</a>, 
<a href="/search/math?searchtype=author&query=Vese%2C+L">Luminita Vese</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Recovering images corrupted by multiplicative noise is a well known
challenging task. Motivated by the success of multiscale hierarchical
decomposition methods (MHDM) in image processing, we adapt a variety of both
classical and new multiplicative noise removing models to the MHDM form. On the
basis of previous work, we further present a tight and a refined version of the
corresponding multiplicative MHDM. We discuss existence and uniqueness of
solutions for the proposed models, and additionally, provide convergence
properties. Moreover, we present a discrepancy principle stopping criterion
which prevents recovering excess noise in the multiscale reconstruction.
Through comprehensive numerical experiments and comparisons, we qualitatively
and quantitatively evaluate the validity of all proposed models for denoising
and deblurring images degraded by multiplicative noise. By construction, these
multiplicative multiscale hierarchical decomposition methods have the added
benefit of recovering many scales of an image, which can provide features of
interest beyond image denoising.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06196" title="Abstract">arXiv:2310.06196</a> [<a href="/pdf/2310.06196" title="Download PDF">pdf</a>, <a href="/format/2310.06196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised  Transformers for Weakly Supervised Object Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murtaza%2C+S">Shakeeb Murtaza</a>, 
<a href="/search/cs?searchtype=author&query=Belharbi%2C+S">Soufiane Belharbi</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>, 
<a href="/search/cs?searchtype=author&query=Sarraf%2C+A">Aydin Sarraf</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Self-supervised vision transformers (SSTs) have shown great potential to
yield rich localization maps that highlight different objects in an image.
However, these maps remain class-agnostic since the model is unsupervised. They
often tend to decompose the image into multiple maps containing different
objects while being unable to distinguish the object of interest from
background noise objects. In this paper, Discriminative Pseudo-label Sampling
(DiPS) is introduced to leverage these class-agnostic maps for
weakly-supervised object localization (WSOL), where only image-class labels are
available. Given multiple attention maps, DiPS relies on a pre-trained
classifier to identify the most discriminative regions of each attention map.
This ensures that the selected ROIs cover the correct image object while
discarding the background ones, and, as such, provides a rich pool of diverse
and discriminative proposals to cover different parts of the object.
Subsequently, these proposals are used as pseudo-labels to train our new
transformer-based WSOL model designed to perform classification and
localization tasks. Unlike standard WSOL methods, DiPS optimizes performance in
both tasks by using a transformer encoder and a dedicated output head for each
task, each trained using dedicated loss functions. To avoid overfitting a
single proposal and promote better object coverage, a single proposal is
randomly selected among the top ones for a training image at each training
step. Experimental results on the challenging CUB, ILSVRC, OpenImages, and
TelDrone datasets indicate that our architecture, in combination with our
transformer-based proposals, can yield better localization performance than
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06198" title="Abstract">arXiv:2310.06198</a> [<a href="/pdf/2310.06198" title="Download PDF">pdf</a>, <a href="/format/2310.06198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion Memory: Leveraging Past Experiences to Accelerate Future Motion  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+D">Dibyendu Das</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuanjie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Plaku%2C+E">Erion Plaku</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xuesu Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">When facing a new motion-planning problem, most motion planners solve it from
scratch, e.g., via sampling and exploration or starting optimization from a
straight-line path. However, most motion planners have to experience a variety
of planning problems throughout their lifetimes, which are yet to be leveraged
for future planning. In this paper, we present a simple but efficient method
called Motion Memory, which allows different motion planners to accelerate
future planning using past experiences. Treating existing motion planners as
either a closed or open box, we present a variety of ways that Motion Memory
can contribute to reduce the planning time when facing a new planning problem.
We provide extensive experiment results with three different motion planners on
three classes of planning problems with over 30,000 problem instances and show
that planning speed can be significantly reduced by up to 89% with the proposed
Motion Memory technique and with increasing past planning experiences.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06200" title="Abstract">arXiv:2310.06200</a> [<a href="/pdf/2310.06200" title="Download PDF">pdf</a>, <a href="/format/2310.06200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Importance of Prompt Tuning for Automated Neuron Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Justin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oikarinen%2C+T">Tuomas Oikarinen</a>, 
<a href="/search/cs?searchtype=author&query=Chantha%2C+A">Arjun Chantha</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Keng-Chi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yilan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+T">Tsui-Wei Weng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances have greatly increased the capabilities of large language
models (LLMs), but our understanding of the models and their safety has not
progressed as fast. In this paper we aim to understand LLMs deeper by studying
their individual neurons. We build upon previous work showing large language
models such as GPT-4 can be useful in explaining what each neuron in a language
model does. Specifically, we analyze the effect of the prompt used to generate
explanations and show that reformatting the explanation prompt in a more
natural way can significantly improve neuron explanation quality and greatly
reduce computational cost. We demonstrate the effects of our new prompts in
three different ways, incorporating both automated and human evaluations.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06201" title="Abstract">arXiv:2310.06201</a> [<a href="/pdf/2310.06201" title="Download PDF">pdf</a>, <a href="/format/2310.06201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressing Context to Enhance Inference Efficiency of Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yucheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Guerin%2C+F">Frank Guerin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. arXiv admin note: substantial text overlap with <a href="/abs/2304.12102">arXiv:2304.12102</a>; text overlap with <a href="/abs/2303.11076">arXiv:2303.11076</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) achieved remarkable performance across various
tasks. However, they face challenges in managing long documents and extended
conversations, due to significantly increased computational requirements, both
in memory and inference time, and potential context truncation when the input
exceeds the LLM's fixed context length. This paper proposes a method called
Selective Context that enhances the inference efficiency of LLMs by identifying
and pruning redundancy in the input context to make the input more compact. We
test our approach using common data sources requiring long context processing:
arXiv papers, news articles, and long conversations, on tasks of summarisation,
question answering, and response generation. Experimental results show that
Selective Context significantly reduces memory cost and decreases generation
latency while maintaining comparable performance compared to that achieved when
full context is used. Specifically, we achieve a 50\% reduction in context
cost, resulting in a 36\% reduction in inference memory usage and a 32\%
reduction in inference time, while observing only a minor drop of .023 in
BERTscore and .038 in faithfulness on four downstream applications, indicating
that our method strikes a good balance between efficiency and performance.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06202" title="Abstract">arXiv:2310.06202</a> [<a href="/pdf/2310.06202" title="Download PDF">pdf</a>, <a href="/format/2310.06202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-who: An Information Density-based Machine-Generated Text Detector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Venkatraman%2C+S">Saranya Venkatraman</a>, 
<a href="/search/cs?searchtype=author&query=Uchendu%2C+A">Adaku Uchendu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The Uniform Information Density principle posits that humans prefer to spread
information evenly during language production. In this work, we examine if the
UID principle can help capture differences between Large Language Models (LLMs)
and human-generated text. We propose GPT-who, the first
psycholinguistically-aware multi-class domain-agnostic statistical-based
detector. This detector employs UID-based features to model the unique
statistical signature of each LLM and human author for accurate authorship
attribution. We evaluate our method using 4 large-scale benchmark datasets and
find that GPT-who outperforms state-of-the-art detectors (both statistical- &amp;
non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by
over $20$% across domains. In addition to superior performance, it is
computationally inexpensive and utilizes an interpretable representation of
text articles. We present the largest analysis of the UID-based representations
of human and machine-generated texts (over 400k articles) to demonstrate how
authors distribute information differently, and in ways that enable their
detection using an off-the-shelf LM without any fine-tuning. We find that
GPT-who can distinguish texts generated by very sophisticated LLMs, even when
the overlying text is indiscernible.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06204" title="Abstract">arXiv:2310.06204</a> [<a href="/pdf/2310.06204" title="Download PDF">pdf</a>, <a href="/format/2310.06204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Numbers without Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thawani%2C+A">Avijit Thawani</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>, 
<a href="/search/cs?searchtype=author&query=Kalyan%2C+A">Ashwin Kalyan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Workshop on Insights from Negative Results in NLP at EACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite recent successes in language models, their ability to represent
numbers is insufficient. Humans conceptualize numbers based on their
magnitudes, effectively projecting them on a number line; whereas subword
tokenization fails to explicitly capture magnitude by splitting numbers into
arbitrary chunks. To alleviate this shortcoming, alternative approaches have
been proposed that modify numbers at various stages of the language modeling
pipeline. These methods change either the (1) notation in which numbers are
written (\eg scientific vs decimal), the (2) vocabulary used to represent
numbers or the entire (3) architecture of the underlying language model, to
directly regress to a desired number.
<br />Previous work suggests that architectural change helps achieve
state-of-the-art on number estimation but we find an insightful ablation:
changing the model's vocabulary instead (\eg introduce a new token for numbers
in range 10-100) is a far better trade-off. In the context of masked number
prediction, a carefully designed tokenization scheme is both the simplest to
implement and sufficient, \ie with similar performance to the state-of-the-art
approach that requires making significant architectural changes. Finally, we
report similar trends on the downstream task of numerical fact estimation (for
Fermi Problems) and discuss reasons behind our findings.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06205" title="Abstract">arXiv:2310.06205</a> [<a href="/pdf/2310.06205" title="Download PDF">pdf</a>, <a href="/format/2310.06205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Classifiers that Abstain without Harm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+T">Tongxin Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ton%2C+J">Jean-Fran&#xe7;ois Ton</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruocheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuanshun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In critical applications, it is vital for classifiers to defer
decision-making to humans. We propose a post-hoc method that makes existing
classifiers selectively abstain from predicting certain samples. Our abstaining
classifier is incentivized to maintain the original accuracy for each
sub-population (i.e. no harm) while achieving a set of group fairness
definitions to a user specified degree. To this end, we design an Integer
Programming (IP) procedure that assigns abstention decisions for each training
sample to satisfy a set of constraints. To generalize the abstaining decisions
to test samples, we then train a surrogate model to learn the abstaining
decisions based on the IP solutions in an end-to-end manner. We analyze the
feasibility of the IP procedure to determine the possible abstention rate for
different levels of unfairness tolerance and accuracy constraint for achieving
no harm. To the best of our knowledge, this work is the first to identify the
theoretical relationships between the constraint parameters and the required
abstention rate. Our theoretical results are important since a high abstention
rate is often infeasible in practice due to a lack of human resources. Our
framework outperforms existing methods in terms of fairness disparity without
sacrificing accuracy at similar abstention rates.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06208" title="Abstract">arXiv:2310.06208</a> [<a href="/pdf/2310.06208" title="Download PDF">pdf</a>, <a href="/format/2310.06208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot  Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thumm%2C+J">Jakob Thumm</a>, 
<a href="/search/cs?searchtype=author&query=Trost%2C+F">Felix Trost</a>, 
<a href="/search/cs?searchtype=author&query=Althoff%2C+M">Matthias Althoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Deep reinforcement learning (RL) has shown promising results in robot motion
planning with first attempts in human-robot collaboration (HRC). However, a
fair comparison of RL approaches in HRC under the constraint of guaranteed
safety is yet to be made. We, therefore, present human-robot gym, a benchmark
for safe RL in HRC. Our benchmark provides eight challenging, realistic HRC
tasks in a modular simulation framework. Most importantly, human-robot gym
includes a safety shield that provably guarantees human safety. We are,
thereby, the first to provide a benchmark to train RL agents that adhere to the
safety specifications of real-world HRC. This bridges a critical gap between
theoretic RL research and its real-world deployment. Our evaluation of six
environments led to three key results: (a) the diverse nature of the tasks
offered by human-robot gym creates a challenging benchmark for state-of-the-art
RL methods, (b) incorporating expert knowledge in the RL training in the form
of an action-based reward can outperform the expert, and (c) our agents
negligibly overfit to training data.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06210" title="Abstract">arXiv:2310.06210</a> [<a href="/pdf/2310.06210" title="Download PDF">pdf</a>, <a href="/format/2310.06210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAT-RRT: Motion Planning that Admits Contact One Link at a Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nechyporenko%2C+N">Nataliya Nechyporenko</a>, 
<a href="/search/cs?searchtype=author&query=Escobedo%2C+C">Caleb Escobedo</a>, 
<a href="/search/cs?searchtype=author&query=Kadekodi%2C+S">Shreyas Kadekodi</a>, 
<a href="/search/cs?searchtype=author&query=Roncone%2C+A">Alessandro Roncone</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Current motion planning approaches rely on binary collision checking to
evaluate the validity of a state and thereby dictate where the robot is allowed
to move. This approach leaves little room for robots to engage in contact with
an object, as is often necessary when operating in densely cluttered spaces. In
this work, we propose an alternative method that considers contact states as
high-cost states that the robot should avoid but can traverse if necessary to
complete a task. More specifically, we introduce Contact Admissible
Transition-based Rapidly exploring Random Trees (CAT-RRT), a planner that uses
a novel per-link cost heuristic to find a path by traversing high-cost obstacle
regions. Through extensive testing, we find that state-of-the-art optimization
planners tend to over-explore low-cost states, which leads to slow and
inefficient convergence to contact regions. Conversely, CAT-RRT searches both
low and high-cost regions simultaneously with an adaptive thresholding
mechanism carried out at each robot link. This leads to paths with a balance
between efficiency, path length, and contact cost.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06213" title="Abstract">arXiv:2310.06213</a> [<a href="/pdf/2310.06213" title="Download PDF">pdf</a>, <a href="/format/2310.06213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoLLM: Extracting Geospatial Knowledge from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manvi%2C+R">Rohin Manvi</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S">Samar Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+G">Gengchen Mai</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+M">Marshall Burke</a>, 
<a href="/search/cs?searchtype=author&query=Lobell%2C+D">David Lobell</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The application of machine learning (ML) in a range of geospatial tasks is
increasingly common but often relies on globally available covariates such as
satellite imagery that can either be expensive or lack predictive power. Here
we explore the question of whether the vast amounts of knowledge found in
Internet language corpora, now compressed within large language models (LLMs),
can be leveraged for geospatial prediction tasks. We first demonstrate that
LLMs embed remarkable spatial information about locations, but naively querying
LLMs using geographic coordinates alone is ineffective in predicting key
indicators like population density. We then present GeoLLM, a novel method that
can effectively extract geospatial knowledge from LLMs with auxiliary map data
from OpenStreetMap. We demonstrate the utility of our approach across multiple
tasks of central interest to the international community, including the
measurement of population density and economic livelihoods. Across these tasks,
our method demonstrates a 70% improvement in performance (measured using
Pearson's $r^2$) relative to baselines that use nearest neighbors or use
information directly from the prompt, and performance equal to or exceeding
satellite-based benchmarks in the literature. With GeoLLM, we observe that
GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting
that the performance of our method scales well with the size of the model and
its pretraining dataset. Our experiments reveal that LLMs are remarkably
sample-efficient, rich in geospatial information, and robust across the globe.
Crucially, GeoLLM shows promise in mitigating the limitations of existing
geospatial covariates and complementing them well.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06214" title="Abstract">arXiv:2310.06214</a> [<a href="/pdf/2310.06214" title="Download PDF">pdf</a>, <a href="/format/2310.06214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakr%2C+E+M">Eslam Mohamed Bakr</a>, 
<a href="/search/cs?searchtype=author&query=Ayman%2C+M">Mohamed Ayman</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+M">Mahmoud Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Slim%2C+H">Habib Slim</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D visual grounding is the ability to localize objects in 3D scenes
conditioned by utterances. Most existing methods devote the referring head to
localize the referred object directly, causing failure in complex scenarios. In
addition, it does not illustrate how and why the network reaches the final
decision. In this paper, we address this question Can we design an
interpretable 3D visual grounding framework that has the potential to mimic the
human perception system?. To this end, we formulate the 3D visual grounding
problem as a sequence-to-sequence task by first predicting a chain of anchors
and then the final target. Interpretability not only improves the overall
performance but also helps us identify failure cases. Following the chain of
thoughts approach enables us to decompose the referring task into interpretable
intermediate steps, boosting the performance and making our framework extremely
data-efficient. Moreover, our proposed framework can be easily integrated into
any existing architecture. We validate our approach through comprehensive
experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent
performance gains compared to existing methods without requiring manually
annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is
significantly data-efficient, whereas on the Sr3D dataset, when trained only on
10% of the data, we match the SOTA performance that trained on the entire data.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06217" title="Abstract">arXiv:2310.06217</a> [<a href="/pdf/2310.06217" title="Download PDF">pdf</a>, <a href="/format/2310.06217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Multi-Level Optimization over Decentralized Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuoguang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuezhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2206.10870">arXiv:2206.10870</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Multi-level optimization has gained increasing attention in recent years, as
it provides a powerful framework for solving complex optimization problems that
arise in many fields, such as meta-learning, multi-player games, reinforcement
learning, and nested composition optimization. In this paper, we study the
problem of distributed multi-level optimization over a network, where agents
can only communicate with their immediate neighbors. This setting is motivated
by the need for distributed optimization in large-scale systems, where
centralized optimization may not be practical or feasible. To address this
problem, we propose a novel gossip-based distributed multi-level optimization
algorithm that enables networked agents to solve optimization problems at
different levels in a single timescale and share information through network
propagation. Our algorithm achieves optimal sample complexity, scaling linearly
with the network size, and demonstrates state-of-the-art performance on various
applications, including hyper-parameter tuning, decentralized reinforcement
learning, and risk-averse optimization.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06218" title="Abstract">arXiv:2310.06218</a> [<a href="/pdf/2310.06218" title="Download PDF">pdf</a>, <a href="/format/2310.06218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading  Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jingyang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+S">Shipeng Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yukai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+G">Guang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, Accepted by 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The study of sparsity in Convolutional Neural Networks (CNNs) has become
widespread to compress and accelerate models in environments with limited
resources. By constraining N consecutive weights along the output channel to be
group-wise non-zero, the recent network with 1$\times$N sparsity has received
tremendous popularity for its three outstanding advantages: 1) A large amount
of storage space saving by a \emph{Block Sparse Row} matrix. 2) Excellent
performance at a high sparsity. 3) Significant speedups on CPUs with Advanced
Vector Extensions. Recent work requires selecting and fine-tuning 1$\times$N
sparse weights based on dense pre-trained weights, leading to the problems such
as expensive training cost and memory access, sub-optimal model quality, as
well as unbalanced workload across threads (different sparsity across output
channels). To overcome them, this paper proposes a novel \emph{\textbf{S}oft
\textbf{U}niform \textbf{B}lock \textbf{P}runing} (SUBP) approach to train a
uniform 1$\times$N sparse structured network from scratch. Specifically, our
approach tends to repeatedly allow pruned blocks to regrow to the network based
on block angular redundancy and importance sampling in a uniform manner
throughout the training process. It not only makes the model less dependent on
pre-training, reduces the model redundancy and the risk of pruning the
important blocks permanently but also achieves balanced workload. Empirically,
on ImageNet, comprehensive experiments across various CNN architectures show
that our SUBP consistently outperforms existing 1$\times$N and structured
sparsity methods based on pre-trained models or training from scratch. Source
codes and models are available at \url{https://github.com/JingyangXiang/SUBP}.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06219" title="Abstract">arXiv:2310.06219</a> [<a href="/pdf/2310.06219" title="Download PDF">pdf</a>, <a href="/format/2310.06219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Runtime Monitoring of Human-centric Requirements in Machine Learning  Components: A Model-driven Engineering Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naveed%2C+H">Hira Naveed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">As machine learning (ML) components become increasingly integrated into
software systems, the emphasis on the ethical or responsible aspects of their
use has grown significantly. This includes building ML-based systems that
adhere to human-centric requirements, such as fairness, privacy,
explainability, well-being, transparency and human values. Meeting these
human-centric requirements is not only essential for maintaining public trust
but also a key factor determining the success of ML-based systems. However, as
these requirements are dynamic in nature and continually evolve, pre-deployment
monitoring of these models often proves insufficient to establish and sustain
trust in ML components. Runtime monitoring approaches for ML are potentially
valuable solutions to this problem. Existing state-of-the-art techniques often
fall short as they seldom consider more than one human-centric requirement,
typically focusing on fairness, safety, and trust. The technical expertise and
effort required to set up a monitoring system are also challenging. In my PhD
research, I propose a novel approach for the runtime monitoring of multiple
human-centric requirements. This approach leverages model-driven engineering to
more comprehensively monitor ML components. This doctoral symposium paper
outlines the motivation for my PhD work, a potential solution, progress so far
and future plans.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06221" title="Abstract">arXiv:2310.06221</a> [<a href="/pdf/2310.06221" title="Download PDF">pdf</a>, <a href="/format/2310.06221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting and Learning Out-of-Distribution Data in the Open world:  Algorithm and Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiyou Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ph.D. thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This thesis makes considerable contributions to the realm of machine
learning, specifically in the context of open-world scenarios where systems
face previously unseen data and contexts. Traditional machine learning models
are usually trained and tested within a fixed and known set of classes, a
condition known as the closed-world setting. While this assumption works in
controlled environments, it falls short in real-world applications where new
classes or categories of data can emerge dynamically and unexpectedly. To
address this, our research investigates two intertwined steps essential for
open-world machine learning: Out-of-distribution (OOD) Detection and Open-world
Representation Learning (ORL). OOD detection focuses on identifying instances
from unknown classes that fall outside the model's training distribution. This
process reduces the risk of making overly confident, erroneous predictions
about unfamiliar inputs. Moving beyond OOD detection, ORL extends the
capabilities of the model to not only detect unknown instances but also learn
from and incorporate knowledge about these new classes. By delving into these
research problems of open-world learning, this thesis contributes both
algorithmic solutions and theoretical foundations, which pave the way for
building machine learning models that are not only performant but also reliable
in the face of the evolving complexities of the real world.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06224" title="Abstract">arXiv:2310.06224</a> [<a href="/pdf/2310.06224" title="Download PDF">pdf</a>, <a href="/ps/2310.06224" title="Download PostScript">ps</a>, <a href="/format/2310.06224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-aware Status Updating: Wireless Scheduling for Maximizing  Situational Awareness in Safety-critical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ornee%2C+T+Z">Tasmeen Zaman Ornee</a>, 
<a href="/search/cs?searchtype=author&query=Shisher%2C+M+K+C">Md Kamran Chowdhury Shisher</a>, 
<a href="/search/cs?searchtype=author&query=Kam%2C+C">Clement Kam</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yin Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, part of this manuscript has been accepted by IEEE MILCOM 2023 Workshop on QuAVoI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this study, we investigate a context-aware status updating system
consisting of multiple sensor-estimator pairs. A centralized monitor pulls
status updates from multiple sensors that are monitoring several
safety-critical situations (e.g., carbon monoxide density in forest fire
detection, machine safety in industrial automation, and road safety). Based on
the received sensor updates, multiple estimators determine the current
safety-critical situations. Due to transmission errors and limited
communication resources, the sensor updates may not be timely, resulting in the
possibility of misunderstanding the current situation. In particular, if a
dangerous situation is misinterpreted as safe, the safety risk is high. In this
paper, we introduce a novel framework that quantifies the penalty due to the
unawareness of a potentially dangerous situation. This situation-unaware
penalty function depends on two key factors: the Age of Information (AoI) and
the observed signal value. For optimal estimators, we provide an
information-theoretic bound of the penalty function that evaluates the
fundamental performance limit of the system. To minimize the penalty, we study
a pull-based multi-sensor, multi-channel transmission scheduling problem. Our
analysis reveals that for optimal estimators, it is always beneficial to keep
the channels busy. Due to communication resource constraints, the scheduling
problem can be modelled as a Restless Multi-armed Bandit (RMAB) problem. By
utilizing relaxation and Lagrangian decomposition of the RMAB, we provide a
low-complexity scheduling algorithm which is asymptotically optimal. Our
results hold for both reliable and unreliable channels. Numerical evidence
shows that our scheduling policy can achieve up to 100 times performance gain
over periodic updating and up to 10 times over randomized policy.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06225" title="Abstract">arXiv:2310.06225</a> [<a href="/pdf/2310.06225" title="Download PDF">pdf</a>, <a href="/format/2310.06225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+B">Bruno Silva</a>, 
<a href="/search/cs?searchtype=author&query=Nunes%2C+L">Leonardo Nunes</a>, 
<a href="/search/cs?searchtype=author&query=Estev%C3%A3o%2C+R">Roberto Estev&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+R">Ranveer Chandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs' performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4's ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models' capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06226" title="Abstract">arXiv:2310.06226</a> [<a href="/pdf/2310.06226" title="Download PDF">pdf</a>, <a href="/format/2310.06226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Words into Action: Learning Diverse Humanoid Robot Behaviors using  Language Guided Iterative Motion Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+K+N">K. Niranjan Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Essa%2C+I">Irfan Essa</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+S">Sehoon Ha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Humanoid robots are well suited for human habitats due to their morphological
similarity, but developing controllers for them is a challenging task that
involves multiple sub-problems, such as control, planning and perception. In
this paper, we introduce a method to simplify controller design by enabling
users to train and fine-tune robot control policies using natural language
commands. We first learn a neural network policy that generates behaviors given
a natural language command, such as "walk forward", by combining Large Language
Models (LLMs), motion retargeting, and motion imitation. Based on the
synthesized motion, we iteratively fine-tune by updating the text prompt and
querying LLMs to find the best checkpoint associated with the closest motion in
history. We validate our approach using a simulated Digit humanoid robot and
demonstrate learning of diverse motions, such as walking, hopping, and kicking,
without the burden of complex reward engineering. In addition, we show that our
iterative refinement enables us to learn 3x times faster than a naive
formulation that learns from scratch.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06227" title="Abstract">arXiv:2310.06227</a> [<a href="/pdf/2310.06227" title="Download PDF">pdf</a>, <a href="/format/2310.06227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring adversarial attacks in federated learning for medical imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darzi%2C+E">Erfan Darzi</a>, 
<a href="/search/cs?searchtype=author&query=Dubost%2C+F">Florian Dubost</a>, 
<a href="/search/cs?searchtype=author&query=Sijtsema%2C+N+M">N.M. Sijtsema</a>, 
<a href="/search/cs?searchtype=author&query=van+Ooijen%2C+P+M+A">P.M.A van Ooijen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Federated learning offers a privacy-preserving framework for medical image
analysis but exposes the system to adversarial attacks. This paper aims to
evaluate the vulnerabilities of federated learning networks in medical image
analysis against such attacks. Employing domain-specific MRI tumor and
pathology imaging datasets, we assess the effectiveness of known threat
scenarios in a federated learning environment. Our tests reveal that
domain-specific configurations can increase the attacker's success rate
significantly. The findings emphasize the urgent need for effective defense
mechanisms and suggest a critical re-evaluation of current security protocols
in federated medical image analysis systems.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06228" title="Abstract">arXiv:2310.06228</a> [<a href="/pdf/2310.06228" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolution of Natural Language Processing Technology: Not Just Language  Processing Towards General Purpose AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamamoto%2C+M">Masahiro Yamamoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Since the invention of computers, communication through natural language
(actual human language) has been a dream technology. However, natural language
is extremely difficult to mathematically formulate, making it difficult to
realize as an algorithm without considering programming. While there have been
numerous technological developments, one cannot say that any results allowing
free utilization have been achieved thus far. In the case of language learning
in humans, for instance when learning one's mother tongue or foreign language,
one must admit that this process is similar to the adage "practice makes
perfect" in principle, even though the learning method is significant up to a
point. Deep learning has played a central role in contemporary AI technology in
recent years. When applied to natural language processing (NLP), this produced
unprecedented results. Achievements exceeding the initial predictions have been
reported from the results of learning vast amounts of textual data using deep
learning. For instance, four arithmetic operations could be performed without
explicit learning, thereby enabling the explanation of complex images and the
generation of images from corresponding explanatory texts. It is an accurate
example of the learner embodying the concept of "practice makes perfect" by
using vast amounts of textual data. This report provides a technological
explanation of how cutting-edge NLP has made it possible to realize the
"practice makes perfect" principle. Additionally, examples of how this can be
applied to business are provided. We reported in June 2022 in Japanese on the
NLP movement from late 2021 to early 2022. We would like to summarize this as a
memorandum since this is just the initial movement leading to the current large
language models (LLMs).
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06229" title="Abstract">arXiv:2310.06229</a> [<a href="/pdf/2310.06229" title="Download PDF">pdf</a>, <a href="/format/2310.06229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Stable and Structure-Preserving Schemes for the Stochastic  Galerkin Shallow Water Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dai%2C+D">Dihan Dai</a>, 
<a href="/search/math?searchtype=author&query=Epshteyn%2C+Y">Yekaterina Epshteyn</a>, 
<a href="/search/math?searchtype=author&query=Narayan%2C+A">Akil Narayan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The shallow water flow model is widely used to describe water flows in
rivers, lakes, and coastal areas. Accounting for uncertainty in the
corresponding transport-dominated nonlinear PDE models presents theoretical and
numerical challenges that motivate the central advances of this paper. Starting
with a spatially one-dimensional hyperbolicity-preserving,
positivity-preserving stochastic Galerkin formulation of the
parametric/uncertain shallow water equations, we derive an entropy-entropy flux
pair for the system. We exploit this entropy-entropy flux pair to construct
structure-preserving second-order energy conservative, and first- and
second-order energy stable finite volume schemes for the stochastic Galerkin
shallow water system. The performance of the methods is illustrated on several
numerical experiments.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06231" title="Abstract">arXiv:2310.06231</a> [<a href="/pdf/2310.06231" title="Download PDF">pdf</a>, <a href="/format/2310.06231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transmission Investment Coordination using MILP Lagrange Dual  Decomposition and Auxiliary Problem Principle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chakrabarti%2C+S">Sambuddha Chakrabarti</a>, 
<a href="/search/eess?searchtype=author&query=Khajeh%2C+H">Hosna Khajeh</a>, 
<a href="/search/eess?searchtype=author&query=Nudell%2C+T+R">Thomas R Nudell</a>, 
<a href="/search/eess?searchtype=author&query=Hesamzadeh%2C+M+R">Mohammad Reza Hesamzadeh</a>, 
<a href="/search/eess?searchtype=author&query=Baldick%2C+R">Ross Baldick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper considers the investment coordination problem for the long term
transmission capacity expansion in a situation where there are multiple
regional Transmission Planners (TPs), each acting in order to maximize the
utility in only its own region. In such a setting, any particular TP does not
normally have any incentive to cooperate with the neighboring TP(s), although
the optimal investment decision of each TP is contingent upon those of the
neighboring TPs. A game-theoretic interaction among the TPs does not
necessarily lead to this overall social optimum. We, therefore, introduce a
social planner and call it the Transmission Planning Coordinator (TPC) whose
goal is to attain the optimal possible social welfare for the bigger
geographical region. In order to achieve this goal, this paper introduces a new
incentive mechanism, based on distributed optimization theory. This incentive
mechanism can be viewed as a set of rules of the transmission expansion
investment coordination game, set by the social planner TPC, such that, even if
the individual TPs act selfishly, it will still lead to the TPC's goal of
attaining overall social optimum. Finally, the effectiveness of our approach is
demonstrated through several simulation studies.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06232" title="Abstract">arXiv:2310.06232</a> [<a href="/pdf/2310.06232" title="Download PDF">pdf</a>, <a href="/format/2310.06232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking PointNet: Spiking Neural Networks for Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+D">Dayong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhe Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuanpei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Weihang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaode Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yufei Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, Spiking Neural Networks (SNNs), enjoying extreme energy efficiency,
have drawn much research attention on 2D visual recognition and shown gradually
increasing application potential. However, it still remains underexplored
whether SNNs can be generalized to 3D recognition. To this end, we present
Spiking PointNet in the paper, the first spiking neural model for efficient
deep learning on point clouds. We discover that the two huge obstacles limiting
the application of SNNs in point clouds are: the intrinsic optimization
obstacle of SNNs that impedes the training of a big spiking model with large
time steps, and the expensive memory and computation cost of PointNet that
makes training a big spiking point model unrealistic. To solve the problems
simultaneously, we present a trained-less but learning-more paradigm for
Spiking PointNet with theoretical justifications and in-depth experimental
analysis. In specific, our Spiking PointNet is trained with only a single time
step but can obtain better performance with multiple time steps inference,
compared to the one trained directly with multiple time steps. We conduct
various experiments on ModelNet10, ModelNet40 to demonstrate the effectiveness
of Spiking PointNet. Notably, our Spiking PointNet even can outperform its ANN
counterpart, which is rare in the SNN field thus providing a potential research
direction for the following work. Moreover, Spiking PointNet shows impressive
speedup and storage saving in the training phase.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06233" title="Abstract">arXiv:2310.06233</a> [<a href="/pdf/2310.06233" title="Download PDF">pdf</a>, <a href="/format/2310.06233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhi-Yong Wang</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+H+C">Hing Cheung So</a>, 
<a href="/search/cs?searchtype=author&query=Zoubir%2C+A+M">Abdelhak M. Zoubir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV); Signal Processing (eess.SP)

</div>
<p class="mathjax">To alleviate the bias generated by the l1-norm in the low-rank tensor
completion problem, nonconvex surrogates/regularizers have been suggested to
replace the tensor nuclear norm, although both can achieve sparsity. However,
the thresholding functions of these nonconvex regularizers may not have
closed-form expressions and thus iterations are needed, which increases the
computational loads. To solve this issue, we devise a framework to generate
sparsity-inducing regularizers with closed-form thresholding functions. These
regularizers are applied to low-tubal-rank tensor completion, and efficient
algorithms based on the alternating direction method of multipliers are
developed. Furthermore, convergence of our methods is analyzed and it is proved
that the generated sequences are bounded and any limit point is a stationary
point. Experimental results using synthetic and real-world datasets show that
the proposed algorithms outperform the state-of-the-art methods in terms of
restoration performance.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06234" title="Abstract">arXiv:2310.06234</a> [<a href="/pdf/2310.06234" title="Download PDF">pdf</a>, <a href="/format/2310.06234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Adaptation of Large Vision Transformer via Adapter  Re-Composing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Dawei Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhijun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper is accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The advent of high-capacity pre-trained models has revolutionized
problem-solving in computer vision, shifting the focus from training
task-specific models to adapting pre-trained models. Consequently, effectively
adapting large pre-trained models to downstream tasks in an efficient manner
has become a prominent research area. Existing solutions primarily concentrate
on designing lightweight adapters and their interaction with pre-trained
models, with the goal of minimizing the number of parameters requiring updates.
In this study, we propose a novel Adapter Re-Composing (ARC) strategy that
addresses efficient pre-trained model adaptation from a fresh perspective. Our
approach considers the reusability of adaptation parameters and introduces a
parameter-sharing scheme. Specifically, we leverage symmetric
down-/up-projections to construct bottleneck operations, which are shared
across layers. By learning low-dimensional re-scaling coefficients, we can
effectively re-compose layer-adaptive adapters. This parameter-sharing strategy
in adapter design allows us to significantly reduce the number of new
parameters while maintaining satisfactory performance, thereby offering a
promising approach to compress the adaptation cost. We conduct experiments on
24 downstream image classification tasks using various Vision Transformer
variants to evaluate our method. The results demonstrate that our approach
achieves compelling transfer learning performance with a reduced parameter
count. Our code is available at
\href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06237" title="Abstract">arXiv:2310.06237</a> [<a href="/pdf/2310.06237" title="Download PDF">pdf</a>, <a href="/format/2310.06237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Multi-Site Treatment Effect Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koga%2C+T">Tatsuki Koga</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+K">Kamalika Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Page%2C+D">David Page</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Patient privacy is a major barrier to healthcare AI. For confidentiality
reasons, most patient data remains in silo in separate hospitals, preventing
the design of data-driven healthcare AI systems that need large volumes of
patient data to make effective decisions. A solution to this is collective
learning across multiple sites through federated learning with differential
privacy. However, literature in this space typically focuses on differentially
private statistical estimation and machine learning, which is different from
the causal inference-related problems that arise in healthcare. In this work,
we take a fresh look at federated learning with a focus on causal inference;
specifically, we look at estimating the average treatment effect (ATE), an
important task in causal inference for healthcare applications, and provide a
federated analytics approach to enable ATE estimation across multiple sites
along with differential privacy (DP) guarantees at each site. The main
challenge comes from site heterogeneity -- different sites have different
sample sizes and privacy budgets. We address this through a class of per-site
estimation algorithms that reports the ATE estimate and its variance as a
quality measure, and an aggregation algorithm on the server side that minimizes
the overall variance of the final ATE estimate. Our experiments on real and
synthetic data show that our method reliably aggregates private statistics
across sites and provides better privacy-utility tradeoff under site
heterogeneity than baselines.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06238" title="Abstract">arXiv:2310.06238</a> [<a href="/pdf/2310.06238" title="Download PDF">pdf</a>, <a href="/format/2310.06238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for  Unbiased Question-Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiulong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhikang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In recent years, there has been a growing emphasis on the intersection of
audio, vision, and text modalities, driving forward the advancements in
multimodal research. However, strong bias that exists in any modality can lead
to the model neglecting the others. Consequently, the model's ability to
effectively reason across these diverse modalities is compromised, impeding
further advancement. In this paper, we meticulously review each question type
from the original dataset, selecting those with pronounced answer biases. To
counter these biases, we gather complementary videos and questions, ensuring
that no answers have outstanding skewed distribution. In particular, for binary
questions, we strive to ensure that both answers are almost uniformly spread
within each question category. As a result, we construct a new dataset, named
MUSIC-AVQA v2.0, which is more challenging and we believe could better foster
the progress of AVQA task. Furthermore, we present a novel baseline model that
delves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,
this model surpasses all the existing benchmarks, improving accuracy by 2% on
MUSIC-AVQA v2.0, setting a new state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06239" title="Abstract">arXiv:2310.06239</a> [<a href="/pdf/2310.06239" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Tuning or Prompt Tuning? A Study of Large Language Models for  Clinical Concept and Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Cheng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+K+E">Kaleb E Smith</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zehao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yonghui Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Objective To develop soft prompt-based learning algorithms for large language
models (LLMs), examine the shape of prompts, prompt-tuning using
frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities.
Methods We developed a soft prompt-based LLM model and compared 4 training
strategies including (1) fine-tuning without prompts; (2) hard-prompt with
unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with
frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for
clinical concept and relation extraction on two benchmark datasets. We
evaluated the transfer learning ability of the prompt-based learning algorithms
in a cross-institution setting. We also assessed the few-shot learning ability.
Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft
prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept
extraction, outperforming the traditional fine-tuning and hard prompt-based
models by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft
prompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end
relation extraction, outperforming the other two models by 0.2~2% and
0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million
parameters) LLMs have a big gap to be competitive with unfrozen models; scaling
LLMs up to billions of parameters makes frozen LLMs competitive with unfrozen
LLMs. For cross-institute evaluation, soft prompting with a frozen
GatorTron-8.9B model achieved the best performance. This study demonstrates
that (1) machines can learn soft prompts better than humans, (2) frozen LLMs
have better few-shot learning ability and transfer learning ability to
facilitate muti-institution applications, and (3) frozen LLMs require large
models.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06240" title="Abstract">arXiv:2310.06240</a> [<a href="/pdf/2310.06240" title="Download PDF">pdf</a>, <a href="/ps/2310.06240" title="Download PostScript">ps</a>, <a href="/format/2310.06240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Multi-Time Slot Power Balancing Control of Power Systems  with Energy Storage Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+L">Luwei Yang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Hill%2C+D+J">David J. Hill</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper studies a crucial problem in power system balancing control, i.e.,
the multi-time slot economic dispatch (MTSED) problem, for power grids with
substantial renewables, synchronous generators (SGs), and energy storage
devices (ESDs). The target of MTSED is to optimally coordinate active/reactive
power outputs of all controllable units to meet a forecast net demand profile
over multiple time slots within a receding finite time horizon. Firstly, the
MTSED is formulated as an optimization problem with operational constraints,
including the limits on the output of each controllable unit, ramping rates of
SGss, energy levels of ESDs, and bus voltages. Then, a novel projection-based
algorithm is developed to solve the problem in a distributed way. In
particular, the distributed algorithm is not limited to solving the MTSED
problem but also applies to more general optimization problems with both
generic convex objective functions and hard feasibility constraints. Finally,
case studies verify the effectiveness of the proposed method.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06243" title="Abstract">arXiv:2310.06243</a> [<a href="/pdf/2310.06243" title="Download PDF">pdf</a>, <a href="/ps/2310.06243" title="Download PostScript">ps</a>, <a href="/format/2310.06243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-Efficient Multi-Agent RL: An Optimization Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+N">Nuoya Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study multi-agent reinforcement learning (MARL) for the general-sum Markov
Games (MGs) under the general function approximation. In order to find the
minimum assumption for sample-efficient learning, we introduce a novel
complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for
general-sum MGs. Using this measure, we propose the first unified algorithmic
framework that ensures sample efficiency in learning Nash Equilibrium, Coarse
Correlated Equilibrium, and Correlated Equilibrium for both model-based and
model-free MARL problems with low MADC. We also show that our algorithm
provides comparable sublinear regret to the existing works. Moreover, our
algorithm combines an equilibrium-solving oracle with a single objective
optimization subprocedure that solves for the regularized payoff of each
deterministic joint policy, which avoids solving constrained optimization
problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023)
or executing sampling procedures with complex multi-objective optimization
problems (Foster et al. 2023), thus being more amenable to empirical
implementation.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06245" title="Abstract">arXiv:2310.06245</a> [<a href="/pdf/2310.06245" title="Download PDF">pdf</a>, <a href="/format/2310.06245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> We are what we repeatedly do: Inducing and deploying habitual schemas in  persona-based responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kane%2C+B">Benjamin Kane</a>, 
<a href="/search/cs?searchtype=author&query=Schubert%2C+L">Lenhart Schubert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Many practical applications of dialogue technology require the generation of
responses according to a particular developer-specified persona. While a
variety of personas can be elicited from recent large language models, the
opaqueness and unpredictability of these models make it desirable to be able to
specify personas in an explicit form. In previous work, personas have typically
been represented as sets of one-off pieces of self-knowledge that are retrieved
by the dialogue system for use in generation. However, in realistic human
conversations, personas are often revealed through story-like narratives that
involve rich habitual knowledge -- knowledge about kinds of events that an
agent often participates in (e.g., work activities, hobbies, sporting
activities, favorite entertainments, etc.), including typical goals,
sub-events, preconditions, and postconditions of those events. We capture such
habitual knowledge using an explicit schema representation, and propose an
approach to dialogue generation that retrieves relevant schemas to condition a
large language model to generate persona-based responses. Furthermore, we
demonstrate a method for bootstrapping the creation of such schemas by first
generating generic passages from a set of simple facts, and then inducing
schemas from the generated passages.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06249" title="Abstract">arXiv:2310.06249</a> [<a href="/pdf/2310.06249" title="Download PDF">pdf</a>, <a href="/format/2310.06249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> l-dyno: framework to learn consistent visual features using robot&#x27;s  motion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Kartikeya Singh</a>, 
<a href="/search/cs?searchtype=author&query=Adhivarahan%2C+C">Charuvaran Adhivarahan</a>, 
<a href="/search/cs?searchtype=author&query=Dantu%2C+K">Karthik Dantu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Historically, feature-based approaches have been used extensively for
camera-based robot perception tasks such as localization, mapping, tracking,
and others. Several of these approaches also combine other sensors (inertial
sensing, for example) to perform combined state estimation. Our work rethinks
this approach; we present a representation learning mechanism that identifies
visual features that best correspond to robot motion as estimated by an
external signal. Specifically, we utilize the robot's transformations through
an external signal (inertial sensing, for example) and give attention to image
space that is most consistent with the external signal. We use a pairwise
consistency metric as a representation to keep the visual features consistent
through a sequence with the robot's relative pose transformations. This
approach enables us to incorporate information from the robot's perspective
instead of solely relying on the image attributes. We evaluate our approach on
real-world datasets such as KITTI &amp; EuRoC and compare the refined features with
existing feature descriptors. We also evaluate our method using our real robot
experiment. We notice an average of 49% reduction in the image search space
without compromising the trajectory estimation accuracy. Our method reduces the
execution time of visual odometry by 4.3% and also reduces reprojection errors.
We demonstrate the need to select only the most important features and show the
competitiveness using various feature detection baselines.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06253" title="Abstract">arXiv:2310.06253</a> [<a href="/pdf/2310.06253" title="Download PDF">pdf</a>, <a href="/format/2310.06253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified View on Solving Objective Mismatch in Model-Based  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+R">Ran Wei</a>, 
<a href="/search/cs?searchtype=author&query=Lambert%2C+N">Nathan Lambert</a>, 
<a href="/search/cs?searchtype=author&query=McDonald%2C+A">Anthony McDonald</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+A">Alfredo Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Calandra%2C+R">Roberto Calandra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Model-based Reinforcement Learning (MBRL) aims to make agents more
sample-efficient, adaptive, and explainable by learning an explicit model of
the environment. While the capabilities of MBRL agents have significantly
improved in recent years, how to best learn the model is still an unresolved
question. The majority of MBRL algorithms aim at training the model to make
accurate predictions about the environment and subsequently using the model to
determine the most rewarding actions. However, recent research has shown that
model predictive accuracy is often not correlated with action quality, tracing
the root cause to the \emph{objective mismatch} between accurate dynamics model
learning and policy optimization of rewards. A number of interrelated solution
categories to the objective mismatch problem have emerged as MBRL continues to
mature as a research area. In this work, we provide an in-depth survey of these
solution categories and propose a taxonomy to foster future research.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06254" title="Abstract">arXiv:2310.06254</a> [<a href="/pdf/2310.06254" title="Download PDF">pdf</a>, <a href="/format/2310.06254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Get the gist? Using large language models for few-shot  decontextualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kane%2C+B">Benjamin Kane</a>, 
<a href="/search/cs?searchtype=author&query=Schubert%2C+L">Lenhart Schubert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In many NLP applications that involve interpreting sentences within a rich
context -- for instance, information retrieval systems or dialogue systems --
it is desirable to be able to preserve the sentence in a form that can be
readily understood without context, for later reuse -- a process known as
``decontextualization''. While previous work demonstrated that generative
Seq2Seq models could effectively perform decontextualization after being
fine-tuned on a specific dataset, this approach requires expensive human
annotations and may not transfer to other domains. We propose a few-shot method
of decontextualization using a large language model, and present preliminary
results showing that this method achieves viable performance on multiple
domains using only a small set of examples.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06257" title="Abstract">arXiv:2310.06257</a> [<a href="/pdf/2310.06257" title="Download PDF">pdf</a>, <a href="/format/2310.06257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCAR: Power Side-Channel Analysis at RTL-Level
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Amisha Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sanjay Das</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+N">Navnil Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Psiakis%2C+R">Rafail Psiakis</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+P+H">Pedro Henrique Silva</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+D">Debjit Pal</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+K">Kanad Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Power side-channel attacks exploit the dynamic power consumption of
cryptographic operations to leak sensitive information of encryption hardware.
Therefore, it is necessary to conduct power side-channel analysis for assessing
the susceptibility of cryptographic systems and mitigating potential risks.
Existing power side-channel analysis primarily focuses on post-silicon
implementations, which are inflexible in addressing design flaws, leading to
costly and time-consuming post-fabrication design re-spins. Hence, pre-silicon
power side-channel analysis is required for early detection of vulnerabilities
to improve design robustness. In this paper, we introduce SCAR, a novel
pre-silicon power side-channel analysis framework based on Graph Neural
Networks (GNN). SCAR converts register-transfer level (RTL) designs of
encryption hardware into control-data flow graphs and use that to detect the
design modules susceptible to side-channel leakage. Furthermore, we incorporate
a deep learning-based explainer in SCAR to generate quantifiable and
human-accessible explanation of our detection and localization decisions. We
have also developed a fortification component as a part of SCAR that uses
large-language models (LLM) to automatically generate and insert additional
design code at the localized zone to shore up the side-channel leakage. When
evaluated on popular encryption algorithms like AES, RSA, and PRESENT, and
postquantum cryptography algorithms like Saber and CRYSTALS-Kyber, SCAR,
achieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall.
Additionally, through explainability analysis, SCAR reduces features for GNN
model training by 57% while maintaining comparable accuracy. We believe that
SCAR will transform the security-critical hardware design cycle, resulting in
faster design closure at a reduced design cost.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06258" title="Abstract">arXiv:2310.06258</a> [<a href="/pdf/2310.06258" title="Download PDF">pdf</a>, <a href="/ps/2310.06258" title="Download PostScript">ps</a>, <a href="/format/2310.06258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Participants&#x27; Utility Functions to Compare Versions of  Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kohli%2C+N">Nitin Kohli</a>, 
<a href="/search/cs?searchtype=author&query=Tschantz%2C+M+C">Michael Carl Tschantz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">We use decision theory to compare variants of differential privacy from the
perspective of prospective study participants. We posit the existence of a
preference ordering on the set of potential consequences that study
participants can incur, which enables the analysis of individual utility
functions. Drawing upon the theory of measurement, we argue that changes in
expected utilities should be measured via the classic Euclidean metric. We then
consider the question of which privacy guarantees would be more appealing for
individuals under different decision settings. Through our analysis, we found
that the nature of the potential participant's utility function, along with the
specific values of $\epsilon$ and $\delta$, can greatly alter which privacy
guarantees are preferable.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06260" title="Abstract">arXiv:2310.06260</a> [<a href="/pdf/2310.06260" title="Download PDF">pdf</a>, <a href="/format/2310.06260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An experiment on an automated literature survey of data-driven speech  enhancement methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+A+d">Arthur dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+J">Jayr Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Nogueira%2C+R">Rodrigo Nogueira</a>, 
<a href="/search/cs?searchtype=author&query=Masiero%2C+B">Bruno Masiero</a>, 
<a href="/search/cs?searchtype=author&query=Sander-Tavallaey%2C+S">Shiva Sander-Tavallaey</a>, 
<a href="/search/cs?searchtype=author&query=Zea%2C+E">Elias Zea</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The increasing number of scientific publications in acoustics, in general,
presents difficulties in conducting traditional literature surveys. This work
explores the use of a generative pre-trained transformer (GPT) model to
automate a literature survey of 116 articles on data-driven speech enhancement
methods. The main objective is to evaluate the capabilities and limitations of
the model in providing accurate responses to specific queries about the papers
selected from a reference human-based survey. While we see great potential to
automate literature surveys in acoustics, improvements are needed to address
technical questions more clearly and accurately.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06261" title="Abstract">arXiv:2310.06261</a> [<a href="/pdf/2310.06261" title="Download PDF">pdf</a>, <a href="/format/2310.06261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Discriminative Modeling for Anomalous Graph Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Jicong Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work was submitted to NeurIPS 2023 but was unfortunately rejected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper studies the problem of detecting anomalous graphs using a machine
learning model trained on only normal graphs, which has many applications in
molecule, biology, and social network data analysis. We present a
self-discriminative modeling framework for anomalous graph detection. The key
idea, mathematically and numerically illustrated, is to learn a discriminator
(classifier) from the given normal graphs together with pseudo-anomalous graphs
generated by a model jointly trained, where we never use any true anomalous
graphs and we hope that the generated pseudo-anomalous graphs interpolate
between normal ones and (real) anomalous ones. Under the framework, we provide
three algorithms with different computational efficiencies and stabilities for
anomalous graph detection. The three algorithms are compared with several
state-of-the-art graph-level anomaly detection baselines on nine popular graph
datasets (four with small size and five with moderate size) and show
significant improvement in terms of AUC. The success of our algorithms stems
from the integration of the discriminative classifier and the well-posed
pseudo-anomalous graphs, which provide new insights for anomaly detection.
Moreover, we investigate our algorithms for large-scale imbalanced graph
datasets. Surprisingly, our algorithms, though fully unsupervised, are able to
significantly outperform supervised learning algorithms of anomalous graph
detection. The corresponding reason is also analyzed.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06266" title="Abstract">arXiv:2310.06266</a> [<a href="/pdf/2310.06266" title="Download PDF">pdf</a>, <a href="/format/2310.06266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di%2C+P">Peng Di</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Wenting Cai</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chaoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+G">Gang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jie Gong</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tingting Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zhichao Lei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Ting Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Ming Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Cong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiachen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shaojun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Min Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangpei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaogui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiawei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qing Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gehao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zelin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xunjin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hailian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lifu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xianying Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages with 2 pages for references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Code Large Language Models (Code LLMs) have gained significant attention in
the industry due to their wide applications in the full lifecycle of software
engineering. However, the effectiveness of existing models in understanding
non-English inputs for multi-lingual code-related tasks is still far from well
studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code
LLM. It is specifically designed for code-related tasks with both English and
Chinese prompts and supports over 40 programming languages. CodeFuse achieves
its effectiveness by utilizing a high quality pre-training dataset that is
carefully filtered by program analyzers and optimized during the training
process. Extensive experiments are conducted using real-world usage scenarios,
the industry-standard benchmark HumanEval-x, and the specially designed
CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we
actively collected valuable human feedback from the AntGroup's software
development process where CodeFuse has been successfully deployed. The results
demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%,
positioning it as one of the top multi-lingual code LLMs with similar parameter
sizes. In practical scenarios, such as code generation, code translation, code
comments, and testcase generation, CodeFuse performs better than other models
when confronted with Chinese prompts.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06268" title="Abstract">arXiv:2310.06268</a> [<a href="/pdf/2310.06268" title="Download PDF">pdf</a>, <a href="/format/2310.06268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-Level Offline Policy Optimization with Limited Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenzhuo Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">We study offline reinforcement learning (RL) which seeks to learn a good
policy based on a fixed, pre-collected dataset. A fundamental challenge behind
this task is the distributional shift due to the dataset lacking sufficient
exploration, especially under function approximation. To tackle this issue, we
propose a bi-level structured policy optimization algorithm that models a
hierarchical interaction between the policy (upper-level) and the value
function (lower-level). The lower level focuses on constructing a confidence
set of value estimates that maintain sufficiently small weighted average
Bellman errors, while controlling uncertainty arising from distribution
mismatch. Subsequently, at the upper level, the policy aims to maximize a
conservative value estimate from the confidence set formed at the lower level.
This novel formulation preserves the maximum flexibility of the implicitly
induced exploratory data distribution, enabling the power of model
extrapolation. In practice, it can be solved through a computationally
efficient, penalized adversarial estimation procedure. Our theoretical regret
guarantees do not rely on any data-coverage and completeness-type assumptions,
only requiring realizability. These guarantees also demonstrate that the
learned policy represents the "best effort" among all policies, as no other
policies can outperform it. We evaluate our model using a blend of synthetic,
benchmark, and real-world datasets for offline RL, showing that it performs
competitively with state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06269" title="Abstract">arXiv:2310.06269</a> [<a href="/pdf/2310.06269" title="Download PDF">pdf</a>, <a href="/format/2310.06269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The AI Incident Database as an Educational Tool to Raise Awareness of AI  Harms: A Classroom Exploration of Efficacy, Limitations, &amp; Future  Improvements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feffer%2C+M">Michael Feffer</a>, 
<a href="/search/cs?searchtype=author&query=Martelaro%2C+N">Nikolas Martelaro</a>, 
<a href="/search/cs?searchtype=author&query=Heidari%2C+H">Hoda Heidari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 11 figures; To appear in the proceedings of EAAMO 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Prior work has established the importance of integrating AI ethics topics
into computer and data sciences curricula. We provide evidence suggesting that
one of the critical objectives of AI Ethics education must be to raise
awareness of AI harms. While there are various sources to learn about such
harms, The AI Incident Database (AIID) is one of the few attempts at offering a
relatively comprehensive database indexing prior instances of harms or near
harms stemming from the deployment of AI technologies in the real world. This
study assesses the effectiveness of AIID as an educational tool to raise
awareness regarding the prevalence and severity of AI harms in socially
high-stakes domains. We present findings obtained through a classroom study
conducted at an R1 institution as part of a course focused on the societal and
ethical considerations around AI and ML. Our qualitative findings characterize
students' initial perceptions of core topics in AI ethics and their desire to
close the educational gap between their technical skills and their ability to
think systematically about ethical and societal aspects of their work. We find
that interacting with the database helps students better understand the
magnitude and severity of AI harms and instills in them a sense of urgency
around (a) designing functional and safe AI and (b) strengthening governance
and accountability mechanisms. Finally, we compile students' feedback about the
tool and our class activity into actionable recommendations for the database
development team and the broader community to improve awareness of AI harms in
AI ethics education.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06271" title="Abstract">arXiv:2310.06271</a> [<a href="/pdf/2310.06271" title="Download PDF">pdf</a>, <a href="/format/2310.06271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Mitigating Hallucination in Large Language Models via  Self-Reflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ziwei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tiezheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Nayeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ishii%2C+E">Etsuko Ishii</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+P">Pascale Fung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have shown promise for generative and
knowledge-intensive tasks including question-answering (QA) tasks. However, the
practical deployment still faces challenges, notably the issue of
"hallucination", where models generate plausible-sounding but unfaithful or
nonsensical information. This issue becomes particularly critical in the
medical domain due to the uncommon professional concepts and potential social
risks involved. This paper analyses the phenomenon of hallucination in medical
generative QA systems using widely adopted LLMs and datasets. Our investigation
centers on the identification and comprehension of common problematic answers,
with a specific emphasis on hallucination. To tackle this challenge, we present
an interactive self-reflection methodology that incorporates knowledge
acquisition and answer generation. Through this feedback process, our approach
steadily enhances the factuality, consistency, and entailment of the generated
answers. Consequently, we harness the interactivity and multitasking ability of
LLMs and produce progressively more precise and accurate answers. Experimental
results on both automatic and human evaluation demonstrate the superiority of
our approach in hallucination reduction compared to baselines.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06272" title="Abstract">arXiv:2310.06272</a> [<a href="/pdf/2310.06272" title="Download PDF">pdf</a>, <a href="/format/2310.06272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let Models Speak Ciphers: Multiagent Debate through Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+C">Chau Pham</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Boyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yingxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Plummer%2C+B+A">Bryan A. Plummer</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Discussion and debate among Large Language Models (LLMs) have gained
considerable attention due to their potential to enhance the reasoning ability
of LLMs. Although natural language is an obvious choice for communication due
to LLM's language understanding capability, the token sampling step needed when
generating natural language poses a potential risk of information loss, as it
uses only one token to represent the model's belief across the entire
vocabulary. In this paper, we introduce a communication regime named CIPHER
(Communicative Inter-Model Protocol Through Embedding Representation) to
address this issue. Specifically, we remove the token sampling step from LLMs
and let them communicate their beliefs across the vocabulary through the
expectation of the raw transformer output embeddings. Remarkably, by deviating
from natural language, CIPHER offers an advantage of encoding a broader
spectrum of information without any modification to the model weights. While
the state-of-the-art LLM debate methods using natural language outperforms
traditional inference by a margin of 1.5-8%, our experiment results show that
CIPHER debate further extends this lead by 1-3.5% across five reasoning tasks
and multiple open-source LLMs of varying sizes. This showcases the superiority
and robustness of embeddings as an alternative "language" for communication
among LLMs.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06275" title="Abstract">arXiv:2310.06275</a> [<a href="/pdf/2310.06275" title="Download PDF">pdf</a>, <a href="/format/2310.06275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying  Expression Conditioned Neural Radiance Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+M">Minghan Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuelang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaochen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yebin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoqian Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">One crucial aspect of 3D head avatar reconstruction lies in the details of
facial expressions. Although recent NeRF-based photo-realistic 3D head avatar
methods achieve high-quality avatar rendering, they still encounter challenges
retaining intricate facial expression details because they overlook the
potential of specific expression variations at different spatial positions when
conditioning the radiance field. Motivated by this observation, we introduce a
novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained
by a simple MLP-based generation network, encompassing both spatial positional
features and global expression information. Benefiting from rich and diverse
information of the SVE at different positions, the proposed SVE-conditioned
neural radiance field can deal with intricate facial expressions and achieve
realistic rendering and geometry details of high-fidelity 3D head avatars.
Additionally, to further elevate the geometric and rendering quality, we
introduce a new coarse-to-fine training strategy, including a geometry
initialization strategy at the coarse stage and an adaptive importance sampling
strategy at the fine stage. Extensive experiments indicate that our method
outperforms other state-of-the-art (SOTA) methods in rendering and geometry
quality on mobile phone-collected and public datasets.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06278" title="Abstract">arXiv:2310.06278</a> [<a href="/pdf/2310.06278" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haoxiang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Vasilakos%2C+A+V">Athanasios V. Vasilakos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, artificial intelligence (AI) and machine learning (ML) are
reshaping society's production methods and productivity, and also changing the
paradigm of scientific research. Among them, the AI language model represented
by ChatGPT has made great progress. Such large language models (LLMs) serve
people in the form of AI-generated content (AIGC) and are widely used in
consulting, healthcare, and education. However, it is difficult to guarantee
the authenticity and reliability of AIGC learning data. In addition, there are
also hidden dangers of privacy disclosure in distributed AI training. Moreover,
the content generated by LLMs is difficult to identify and trace, and it is
difficult to cross-platform mutual recognition. The above information security
issues in the coming era of AI powered by LLMs will be infinitely amplified and
affect everyone's life. Therefore, we consider empowering LLMs using blockchain
technology with superior security features to propose a vision for trusted AI.
This paper mainly introduces the motivation and technical route of blockchain
for LLM (BC4LLM), including reliable learning corpus, secure training process,
and identifiable generated content. Meanwhile, this paper also reviews the
potential applications and future challenges, especially in the frontier
communication networks field, including network resource allocation, dynamic
spectrum sharing, and semantic communication. Based on the above work combined
and the prospect of blockchain and LLMs, it is expected to help the early
realization of trusted AI and provide guidance for the academic community.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06279" title="Abstract">arXiv:2310.06279</a> [<a href="/pdf/2310.06279" title="Download PDF">pdf</a>, <a href="/format/2310.06279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEC-Intelligent Agent Support for Low-Latency Data Plane in Private  NextG Core
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Shalini Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sushovan Das</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+S">Sanjoy Paul</a>, 
<a href="/search/cs?searchtype=author&query=Maddala%2C+P">Prasanthi Maddala</a>, 
<a href="/search/cs?searchtype=author&query=Seskar%2C+I">Ivan Seskar</a>, 
<a href="/search/cs?searchtype=author&query=Raychaudhuri%2C+D">Dipankar Raychaudhuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Private 5G networks will soon be ubiquitous across the future-generation
smart wireless access infrastructures hosting a wide range of
performance-critical applications. A high-performing User Plane Function (UPF)
in the data plane is critical to achieving such stringent performance goals, as
it governs fast packet processing and supports several key control-plane
operations. Based on a private 5G prototype implementation and analysis, it is
imperative to perform dynamic resource management and orchestration at the UPF.
This paper leverages Mobile Edge Cloud-Intelligent Agent (MEC-IA), a logically
centralized entity that proactively distributes resources at UPF for various
service types, significantly reducing the tail latency experienced by the user
requests while maximizing resource utilization. Extending the MEC-IA
functionality to MEC layers further incurs data plane latency reduction. Based
on our extensive simulations, under skewed uRLLC traffic arrival, the MEC-IA
assisted bestfit UPF-MEC scheme reduces the worst-case latency of UE requests
by up to 77.8% w.r.t. baseline. Additionally, the system can increase uRLLC
connectivity gain by 2.40x while obtaining 40% CapEx savings.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06282" title="Abstract">arXiv:2310.06282</a> [<a href="/pdf/2310.06282" title="Download PDF">pdf</a>, <a href="/format/2310.06282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuseChat: A Conversational Music Recommendation System for Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhikang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiulong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Polak%2C+P">Pawel Polak</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
<p class="mathjax">We introduce MuseChat, an innovative dialog-based music recommendation
system. This unique platform not only offers interactive user engagement but
also suggests music tailored for input videos, so that users can refine and
personalize their music selections. In contrast, previous systems predominantly
emphasized content compatibility, often overlooking the nuances of users'
individual preferences. For example, all the datasets only provide basic
music-video pairings or such pairings with textual music descriptions. To
address this gap, our research offers three contributions. First, we devise a
conversation-synthesis method that simulates a two-turn interaction between a
user and a recommendation system, which leverages pre-trained music tags and
artist information. In this interaction, users submit a video to the system,
which then suggests a suitable music piece with a rationale. Afterwards, users
communicate their musical preferences, and the system presents a refined music
recommendation with reasoning. Second, we introduce a multi-modal
recommendation engine that matches music either by aligning it with visual cues
from the video or by harmonizing visual information, feedback from previously
recommended music, and the user's textual input. Third, we bridge music
representations and textual data with a Large Language Model(Vicuna-7B). This
alignment equips MuseChat to deliver music recommendations and their underlying
reasoning in a manner resembling human communication. Our evaluations show that
MuseChat surpasses existing state-of-the-art models in music retrieval tasks
and pioneers the integration of the recommendation process within a natural
language framework.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06283" title="Abstract">arXiv:2310.06283</a> [<a href="/pdf/2310.06283" title="Download PDF">pdf</a>, <a href="/format/2310.06283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards More Efficient Depression Risk Recognition via Gait
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+M">Min Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Muchan Tao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuecai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaotong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiong Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongzhen Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Depression, a highly prevalent mental illness, affects over 280 million
individuals worldwide. Early detection and timely intervention are crucial for
promoting remission, preventing relapse, and alleviating the emotional and
financial burdens associated with depression. However, patients with depression
often go undiagnosed in the primary care setting. Unlike many physiological
illnesses, depression lacks objective indicators for recognizing depression
risk, and existing methods for depression risk recognition are time-consuming
and often encounter a shortage of trained medical professionals. The
correlation between gait and depression risk has been empirically established.
Gait can serve as a promising objective biomarker, offering the advantage of
efficient and convenient data collection. However, current methods for
recognizing depression risk based on gait have only been validated on small,
private datasets, lacking large-scale publicly available datasets for research
purposes. Additionally, these methods are primarily limited to hand-crafted
approaches. Gait is a complex form of motion, and hand-crafted gait features
often only capture a fraction of the intricate associations between gait and
depression risk. Therefore, this study first constructs a large-scale gait
database, encompassing over 1,200 individuals, 40,000 gait sequences, and
covering six perspectives and three types of attire. Two commonly used
psychological scales are provided as depression risk annotations. Subsequently,
a deep learning-based depression risk recognition model is proposed, overcoming
the limitations of hand-crafted approaches. Through experiments conducted on
the constructed large-scale database, the effectiveness of the proposed method
is validated, and numerous instructive insights are presented in the paper,
highlighting the significant potential of gait-based depression risk
recognition.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06285" title="Abstract">arXiv:2310.06285</a> [<a href="/pdf/2310.06285" title="Download PDF">pdf</a>, <a href="/ps/2310.06285" title="Download PostScript">ps</a>, <a href="/format/2310.06285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Neighbor Discovery for Wireless Ad Hoc Network with Successive  Interference Cancellation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yueyue Liang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zeyang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kaifeng Han</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huici Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Neighbor discovery (ND) is a key step in wireless ad hoc network, which
directly affects the efficiency of wireless networking. Improving the speed of
ND has always been the goal of ND algorithms. The classical ND algorithms lose
packets due to the collision of multiple packets, which greatly affects the
speed of the ND algorithms. Traditional methods detect packet collision and
implement retransmission when encountering packet loss. However, they does not
solve the packet collision problem and the performance improvement of ND
algorithms is limited. In this paper, the successive interference cancellation
(SIC) technology is introduced into the ND algorithms to unpack multiple
collision packets by distinguishing multiple packets in the power domain.
Besides, the multi-packet reception (MPR) is further applied to reduce the
probability of packet collision by distinguishing multiple received packets,
thus further improving the speed of ND algorithms. Six ND algorithms, namely
completely random algorithm (CRA), CRA based on SIC (CRA-SIC), CRA based on SIC
and MPR (CRA-SIC-MPR), scan-based algorithm (SBA), SBA based on SIC (SBA-SIC),
and SBA based on SIC and MPR (SBA-SIC-MPR), are theoretically analyzed and
verified by simulation. The simulation results show that SIC and MPR reduce the
ND time of SBA by 69.02% and CRA by 66.03% averagely.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06286" title="Abstract">arXiv:2310.06286</a> [<a href="/pdf/2310.06286" title="Download PDF">pdf</a>, <a href="/format/2310.06286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Suppressing Overestimation in Q-Learning through Adversarial Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">HyeAnn Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The goal of this paper is to propose a new Q-learning algorithm with a dummy
adversarial player, which is called dummy adversarial Q-learning (DAQ), that
can effectively regulate the overestimation bias in standard Q-learning. With
the dummy player, the learning can be formulated as a two-player zero-sum game.
The proposed DAQ unifies several Q-learning variations to control
overestimation biases, such as maxmin Q-learning and minmax Q-learning
(proposed in this paper) in a single framework. The proposed DAQ is a simple
but effective way to suppress the overestimation bias thourgh dummy adversarial
behaviors and can be easily applied to off-the-shelf reinforcement learning
algorithms to improve the performances. A finite-time convergence of DAQ is
analyzed from an integrated perspective by adapting an adversarial Q-learning.
The performance of the suggested DAQ is empirically demonstrated under various
benchmark environments.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06287" title="Abstract">arXiv:2310.06287</a> [<a href="/pdf/2310.06287" title="Download PDF">pdf</a>, <a href="/format/2310.06287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability of FFLS-based diffusion adaptive filter under a cooperative  excitation condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gan%2C+D">Die Gan</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+S">Siyu Xie</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zhixin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+J">Jinhu Lv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we consider the distributed filtering problem over sensor
networks such that all sensors cooperatively track unknown time-varying
parameters by using local information. A distributed forgetting factor least
squares (FFLS) algorithm is proposed by minimizing a local cost function
formulated as a linear combination of accumulative estimation error. Stability
analysis of the algorithm is provided under a cooperative excitation condition
which contains spatial union information to reflect the cooperative effect of
all sensors. Furthermore, we generalize theoretical results to the case of
Markovian switching directed graphs. The main difficulties of theoretical
analysis lie in how to analyze properties of the product of non-independent and
non-stationary random matrices. Some techniques such as stability theory,
algebraic graph theory and Markov chain theory are employed to deal with the
above issue. Our theoretical results are obtained without relying on the
independency or stationarity assumptions of regression vectors which are
commonly used in existing literature.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06290" title="Abstract">arXiv:2310.06290</a> [<a href="/pdf/2310.06290" title="Download PDF">pdf</a>, <a href="/format/2310.06290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gem5Pred: Predictive Approaches For Gem5 Simulation Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tian Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xueyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Taki%2C+S+U">Sifat Ut Taki</a>, 
<a href="/search/cs?searchtype=author&query=Mehrdad%2C+S">Saeid Mehrdad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Gem5, an open-source, flexible, and cost-effective simulator, is widely
recognized and utilized in both academic and industry fields for hardware
simulation. However, the typically time-consuming nature of simulating programs
on Gem5 underscores the need for a predictive model that can estimate
simulation time. As of now, no such dataset or model exists. In response to
this gap, this paper makes a novel contribution by introducing a unique dataset
specifically created for this purpose. We also conducted analysis of the
effects of different instruction types on the simulation time in Gem5. After
this, we employ three distinct models leveraging CodeBERT to execute the
prediction task based on the developed dataset. Our superior regression model
achieves a Mean Absolute Error (MAE) of 0.546, while our top-performing
classification model records an Accuracy of 0.696. Our models establish a
foundation for future investigations on this topic, serving as benchmarks
against which subsequent models can be compared. We hope that our contribution
can simulate further research in this field. The dataset we used is available
at https://github.com/XueyangLiOSU/Gem5Pred.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06293" title="Abstract">arXiv:2310.06293</a> [<a href="/pdf/2310.06293" title="Download PDF">pdf</a>, <a href="/format/2310.06293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetShaper: A Differentially Private Network Side-Channel Mitigation  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabzi%2C+A">Amir Sabzi</a>, 
<a href="/search/cs?searchtype=author&query=Vora%2C+R">Rut Vora</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+S">Swati Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Seltzer%2C+M">Margo Seltzer</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A9cuyer%2C+M">Mathias L&#xe9;cuyer</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+A">Aastha Mehta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The widespread adoption of encryption in network protocols has significantly
improved the overall security of many Internet applications. However, these
protocols cannot prevent network side-channel leaks -- leaks of sensitive
information through the sizes and timing of network packets. We present
NetShaper, a system that mitigates such leaks based on the principle of traffic
shaping. NetShaper's traffic shaping provides differential privacy guarantees
while adapting to the prevailing workload and congestion condition, and allows
configuring a tradeoff between privacy guarantees, bandwidth and latency
overheads. Furthermore, NetShaper provides a modular and portable tunnel
endpoint design that can support diverse applications. We present a
middlebox-based implementation of NetShaper and demonstrate its applicability
in a video streaming and a web service application.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06296" title="Abstract">arXiv:2310.06296</a> [<a href="/pdf/2310.06296" title="Download PDF">pdf</a>, <a href="/format/2310.06296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Users Pointing Performance on Large Displays with Different  Curvatures in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ullah%2C+A+K+M+A">A K M Amanat Ullah</a>, 
<a href="/search/cs?searchtype=author&query=Delamare%2C+W">William Delamare</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+K">Khalad Hasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Visualization and Computer Graphics (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Large curved displays inside Virtual Reality environments are becoming
popular for visualizing high-resolution content during analytical tasks, gaming
or entertainment. Prior research showed that such displays provide a wide field
of view and offer users a high level of immersion. However, little is known
about users' performance (e.g., pointing speed and accuracy) on them. We
explore users' pointing performance on large virtual curved displays. We
investigate standard pointing factors (e.g., target width and amplitude) in
combination with relevant curve-related factors, namely display curvature and
both linear and angular measures. Our results show that the less curved the
display, the higher the performance, i.e., faster movement time. This result
holds for pointing tasks controlled via their visual properties (linear widths
and amplitudes) or their motor properties (angular widths and amplitudes).
Additionally, display curvatures significantly affect the error rate for both
linear and angular conditions. Furthermore, we observe that curved displays
perform better or similar to flat displays based on throughput analysis.
Finally, we discuss our results and provide suggestions regarding pointing
tasks on large curved displays in VR.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06297" title="Abstract">arXiv:2310.06297</a> [<a href="/pdf/2310.06297" title="Download PDF">pdf</a>, <a href="/format/2310.06297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Detailed Vehicle Energy Dynamics to Physics-Like Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khoudari%2C+N">Nour Khoudari</a>, 
<a href="/search/eess?searchtype=author&query=Almatrudi%2C+S">Sulaiman Almatrudi</a>, 
<a href="/search/eess?searchtype=author&query=Ramadan%2C+R">Rabie Ramadan</a>, 
<a href="/search/eess?searchtype=author&query=Carpio%2C+J">Joy Carpio</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+M">Mengsha Yao</a>, 
<a href="/search/eess?searchtype=author&query=Butts%2C+K">Kenneth Butts</a>, 
<a href="/search/eess?searchtype=author&query=Bayen%2C+A+M">Alexandre M. Bayen</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+W">Jonathan W. Lee</a>, 
<a href="/search/eess?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The energy demand of vehicles, particularly in unsteady drive cycles, is
affected by complex dynamics internal to the engine and other powertrain
components. Yet, in many applications, particularly macroscopic traffic flow
modeling and optimization, structurally simple approximations to the complex
vehicle dynamics are needed that nevertheless reproduce the correct effective
energy behavior. This work presents a systematic model reduction pipeline that
starts from complex vehicle models based on the Autonomie software and derives
a hierarchy of simplified models that are fast to evaluate, easy to disseminate
in open-source frameworks, and compatible with optimization frameworks. The
pipeline, based on a virtual chassis dynamometer and subsequent approximation
strategies, is reproducible and is applied to six different vehicle classes to
produce concrete explicit energy models that represent an average vehicle in
each class and leverage the accuracy and validation work of the Autonomie
software.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06298" title="Abstract">arXiv:2310.06298</a> [<a href="/pdf/2310.06298" title="Download PDF">pdf</a>, <a href="/format/2310.06298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Just-in-Time Flaky Test Detection via Abstracted Failure Symptom  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+G">Gabin An</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Juyeon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+T">Thomas Bach</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jingun Hong</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shin Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">We report our experience of using failure symptoms, such as error messages or
stack traces, to identify flaky test failures in a Continuous Integration (CI)
pipeline for a large industrial software system, SAP HANA. Although failure
symptoms are commonly used to identify similar failures, they have not
previously been employed to detect flaky test failures. Our hypothesis is that
flaky failures will exhibit symptoms distinct from those of non-flaky failures.
Consequently, we can identify recurring flaky failures, without rerunning the
tests, by matching the failure symptoms to those of historical flaky runs. This
can significantly reduce the need for test reruns, ultimately resulting in
faster delivery of test results to developers. To facilitate the process of
matching flaky failures across different execution instances, we abstract newer
test failure symptoms before matching them to the known patterns of flaky
failures, inspired by previous research in the fields of failure deduplication
and log analysis. We evaluate our symptom-based flakiness detection method
using actual failure symptoms gathered from CI data of SAP HANA during a
six-month period. Our method shows the potential of using failure symptoms to
identify recurring flaky failures, achieving a precision of at least 96%, while
saving approximately 58% of the machine time compared to the traditional rerun
strategy. Analysis of the false positives and the feedback from developers
underscore the importance of having descriptive and informative failure
symptoms for both the effective deployment of this symptom-based approach and
the debugging of flaky tests.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06300" title="Abstract">arXiv:2310.06300</a> [<a href="/pdf/2310.06300" title="Download PDF">pdf</a>, <a href="/format/2310.06300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward a Reference Architecture for Software Supply Chain Metadata  Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+N+K">Nguyen Khoi Tran</a>, 
<a href="/search/cs?searchtype=author&query=Pallewatta%2C+S">Samodha Pallewatta</a>, 
<a href="/search/cs?searchtype=author&query=Babar%2C+M+A">M. Ali Babar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">An Software Supply Chain (SSC) attack combines an upstream attack, where
malicious codes are injected into a software artefact via a compromised life
cycle activity, and a downstream attack on the consumers who use the
compromised artefact. Organisations need thorough and trustworthy visibility
over the entire SSC of their software inventory to detect risks early and
rapidly identify compromised assets in the event of an SSC attack. One way to
achieve such visibility is through SSC metadata, machine-readable and
authenticated documents describing an artefact's lifecycle, such as how it was
constructed and the utilised ``ingredients''. Adopting SSC metadata requires
organisations to procure or develop a Software Supply Chain Metadata Management
system (SCM2), a suite of software tools for performing life cycle activities
of SSC metadata documents such as creation, signing, distribution, and
consumption. Selecting or developing an SCM2 is challenging due to the lack of
a comprehensive domain model and architectural blueprint to aid practitioners
in navigating the vast design space of SSC metadata terminologies, frameworks,
and solutions. This paper addresses the above-mentioned challenge with a
Systematisation of Knowledge about SSC metadata and SCM2, presented as a
Reference Architecture (RA). The RA comprises a domain model and an
architectural blueprint for SCM2 systems, constructed from the concepts and
building blocks scattered across existing SSC security frameworks and
standards. Our evaluation shows that the RA framework is effective for
analysing existing SCM2 solutions and guiding the engineering of new SCM2.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06301" title="Abstract">arXiv:2310.06301</a> [<a href="/pdf/2310.06301" title="Download PDF">pdf</a>, <a href="/format/2310.06301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamical versus Bayesian Phase Transitions in a Toy Model of  Superposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhongtian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+E">Edmund Lau</a>, 
<a href="/search/cs?searchtype=author&query=Mendel%2C+J">Jake Mendel</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Susan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Murfet%2C+D">Daniel Murfet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We investigate phase transitions in a Toy Model of Superposition (TMS) using
Singular Learning Theory (SLT). We derive a closed formula for the theoretical
loss and, in the case of two hidden dimensions, discover that regular $k$-gons
are critical points. We present supporting theory indicating that the local
learning coefficient (a geometric invariant) of these $k$-gons determines phase
transitions in the Bayesian posterior as a function of training sample size. We
then show empirically that the same $k$-gon critical points also determine the
behavior of SGD training. The picture that emerges adds evidence to the
conjecture that the SGD learning trajectory is subject to a sequential learning
mechanism. Specifically, we find that the learning process in TMS, be it
through SGD or Bayesian learning, can be characterized by a journey through
parameter space from regions of high loss and low complexity to regions of low
loss and high complexity.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06302" title="Abstract">arXiv:2310.06302</a> [<a href="/pdf/2310.06302" title="Download PDF">pdf</a>, <a href="/format/2310.06302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Demonstrations for Cross-domain Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shuaichen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Fosler-Lussier%2C+E">Eric Fosler-Lussier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) with in-context learning have demonstrated
impressive generalization capabilities in the cross-domain text-to-SQL task,
without the use of in-domain annotations. However, incorporating in-domain
demonstration examples has been found to greatly enhance LLMs' performance. In
this paper, we delve into the key factors within in-domain examples that
contribute to the improvement and explore whether we can harness these benefits
without relying on in-domain annotations. Based on our findings, we propose a
demonstration selection framework ODIS which utilizes both out-of-domain
examples and synthetically generated in-domain examples to construct
demonstrations. By retrieving demonstrations from hybrid sources, ODIS
leverages the advantages of both, showcasing its effectiveness compared to
baseline methods that rely on a single data source. Furthermore, ODIS
outperforms state-of-the-art approaches on two cross-domain text-to-SQL
datasets, with improvements of 1.1 and 11.8 points in execution accuracy,
respectively.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06303" title="Abstract">arXiv:2310.06303</a> [<a href="/pdf/2310.06303" title="Download PDF">pdf</a>, <a href="/format/2310.06303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dobby: A Conversational Service Robot Driven by GPT-4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stark%2C+C">Carson Stark</a>, 
<a href="/search/cs?searchtype=author&query=Chun%2C+B">Bohkyung Chun</a>, 
<a href="/search/cs?searchtype=author&query=Charleston%2C+C">Casey Charleston</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+V">Varsha Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Pabon%2C+L">Luis Pabon</a>, 
<a href="/search/cs?searchtype=author&query=Sunkari%2C+S">Surya Sunkari</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+T">Tarun Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Hart%2C+J">Justin Hart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This work introduces a robotics platform which embeds a conversational AI
agent in an embodied system for natural language understanding and intelligent
decision-making for service tasks; integrating task planning and human-like
conversation. The agent is derived from a large language model, which has
learned from a vast corpus of general knowledge. In addition to generating
dialogue, this agent can interface with the physical world by invoking commands
on the robot; seamlessly merging communication and behavior. This system is
demonstrated in a free-form tour-guide scenario, in an HRI study combining
robots with and without conversational AI capabilities. Performance is measured
along five dimensions: overall effectiveness, exploration abilities,
scrutinization abilities, receptiveness to personification, and adaptability.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06306" title="Abstract">arXiv:2310.06306</a> [<a href="/pdf/2310.06306" title="Download PDF">pdf</a>, <a href="/format/2310.06306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble Active Learning by Contextual Bandits for AI Incubation in  Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yingyan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Ran Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Online sensing and computational resources in Industrial Cyber-physical
Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data
quality, such as imbalanced classes, hinder AI models trained offline. To
address this, AI models are updated online with streaming data for continuous
improvement. Supervised learning models, however, face challenges in selecting
quality streaming samples for updates due to annotation constraints. Active
learning methods in literature offer solutions by focusing on under-represented
or well-represented regions. Balancing these strategies in changing
manufacturing contexts is challenging. Some acquisition criteria learned by AI
dynamically adapt but may not consistently handle frequent changes. We
introduce an ensemble active learning method, CBEAL, employing active learning
agents specifically for exploration or exploitation. Weights of agents are
adjusted based on agent decision effectiveness. CBEAL optimally guides data
acquisition, minimizing human annotation. Our theoretical analysis and
empirical studies validate CBEAL's efficiency in ICPS manufacturing process
modeling.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06307" title="Abstract">arXiv:2310.06307</a> [<a href="/pdf/2310.06307" title="Download PDF">pdf</a>, <a href="/format/2310.06307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Users&#x27; Pointing Performance on Virtual and Physical Large  Curved Displays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ullah%2C+A+K+M+A">A K M Amanat Ullah</a>, 
<a href="/search/cs?searchtype=author&query=Delamare%2C+W">William Delamare</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+K">Khalad Hasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In 29th ACM Symposium on Virtual Reality Software and Technology (VRST 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Large curved displays have emerged as a powerful platform for collaboration,
data visualization, and entertainment. These displays provide highly immersive
experiences, a wider field of view, and higher satisfaction levels. Yet, large
curved displays are not commonly available due to their high costs. With the
recent advancement of Head Mounted Displays (HMDs), large curved displays can
be simulated in Virtual Reality (VR) with minimal cost and space requirements.
However, to consider the virtual display as an alternative to the physical
display, it is necessary to uncover user performance differences (e.g.,
pointing speed and accuracy) between these two platforms. In this paper, we
explored users' pointing performance on both physical and virtual large curved
displays. Specifically, with two studies, we investigate users' performance
between the two platforms for standard pointing factors such as target width,
target amplitude as well as users' position relative to the screen. Results
from user studies reveal no significant difference in pointing performance
between the two platforms when users are located at the same position relative
to the screen. In addition, we observe users' pointing performance improves
when they are located at the center of a semi-circular display compared to
off-centered positions. We conclude by outlining design implications for
pointing on large curved virtual displays. These findings show that large
curved virtual displays are a viable alternative to physical displays for
pointing tasks.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06308" title="Abstract">arXiv:2310.06308</a> [<a href="/pdf/2310.06308" title="Download PDF">pdf</a>, <a href="/format/2310.06308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unit Testing Challenges with Automated Marking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Norman Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, accepted at the 30th Asia-Pacific Software Engineering Conference (APSEC 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Teaching software testing presents difficulties due to its abstract and
conceptual nature. The lack of tangible outcomes and limited emphasis on
hands-on experience further compound the challenge, often leading to
difficulties in comprehension for students. This can result in waning
engagement and diminishing motivation over time. In this paper, we introduce
online unit testing challenges with automated marking as a learning tool via
the EdStem platform to enhance students' software testing skills and
understanding of software testing concepts. Then, we conducted a survey to
investigate the impact of the unit testing challenges with automated marking on
student learning. The results from 92 participants showed that our unit testing
challenges have kept students more engaged and motivated, fostering deeper
understanding and learning, while the automated marking mechanism enhanced
students' learning progress, helping them to understand their mistakes and
misconceptions quicker than traditional-style human-written manual feedback.
Consequently, these results inform educators that the online unit testing
challenges with automated marking improve overall student learning experience,
and are an effective pedagogical practice in software testing.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06309" title="Abstract">arXiv:2310.06309</a> [<a href="/pdf/2310.06309" title="Download PDF">pdf</a>, <a href="/format/2310.06309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoding and Decoding Narratives: Datafication and Alternative Access  Models for Audiovisual Archives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuchen Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2310.05825">arXiv:2310.05825</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Situated in the intersection of audiovisual archives, computational methods,
and immersive interactions, this work probes the increasingly important
accessibility issues from a two-fold approach. Firstly, the work proposes an
ontological data model to handle complex descriptors (metadata, feature
vectors, etc.) with regard to user interactions. Secondly, this work examines
text-to-video retrieval from an implementation perspective by proposing a
classifier-enhanced workflow to deal with complex and hybrid queries and a
training data augmentation workflow to improve performance. This work serves as
the foundation for experimenting with novel public-facing access models to
large audiovisual archives
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06310" title="Abstract">arXiv:2310.06310</a> [<a href="/pdf/2310.06310" title="Download PDF">pdf</a>, <a href="/format/2310.06310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Demystify Bug Reports?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plein%2C+L">Laura Plein</a>, 
<a href="/search/cs?searchtype=author&query=Bissyand%C3%A9%2C+T+F">Tegawend&#xe9; F. Bissyand&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Bugs are notoriously challenging: they slow down software users and result in
time-consuming investigations for developers. These challenges are exacerbated
when bugs must be reported in natural language by users. Indeed, we lack
reliable tools to automatically address reported bugs (i.e., enabling their
analysis, reproduction, and bug fixing). With the recent promises created by
LLMs such as ChatGPT for various tasks, including in software engineering, we
ask ourselves: What if ChatGPT could understand bug reports and reproduce them?
This question will be the main focus of this study. To evaluate whether ChatGPT
is capable of catching the semantics of bug reports, we used the popular
Defects4J benchmark with its bug reports. Our study has shown that ChatGPT was
able to demystify and reproduce 50% of the reported bugs. ChatGPT being able to
automatically address half of the reported bugs shows promising potential in
the direction of applying machine learning to address bugs with only a
human-in-the-loop to report the bug.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06311" title="Abstract">arXiv:2310.06311</a> [<a href="/pdf/2310.06311" title="Download PDF">pdf</a>, <a href="/format/2310.06311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Compositional Text-to-image Generation with Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Song Wen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+G">Guian Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Metaxas%2C+D">Dimitris Metaxas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Recent advancements in text-to-image models, particularly diffusion models,
have shown significant promise. However, compositional text-to-image models
frequently encounter difficulties in generating high-quality images that
accurately align with input texts describing multiple objects, variable
attributes, and intricate spatial relationships. To address this limitation, we
employ large vision-language models (LVLMs) for multi-dimensional assessment of
the alignment between generated images and their corresponding input texts.
Utilizing this assessment, we fine-tune the diffusion model to enhance its
alignment capabilities. During the inference phase, an initial image is
produced using the fine-tuned diffusion model. The LVLM is then employed to
pinpoint areas of misalignment in the initial image, which are subsequently
corrected using the image editing algorithm until no further misalignments are
detected by the LVLM. The resultant image is consequently more closely aligned
with the input text. Our experimental results validate that the proposed
methodology significantly improves text-image alignment in compositional image
generation, particularly with respect to object number, attribute binding,
spatial relationships, and aesthetic quality.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06312" title="Abstract">arXiv:2310.06312</a> [<a href="/pdf/2310.06312" title="Download PDF">pdf</a>, <a href="/format/2310.06312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Mixtures of Structural Causal Models from Time Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varambally%2C+S">Sumanth Varambally</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi-An Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In fields such as finance, climate science, and neuroscience, inferring
causal relationships from time series data poses a formidable challenge. While
contemporary techniques can handle nonlinear relationships between variables
and flexible noise distributions, they rely on the simplifying assumption that
data originates from the same underlying causal model. In this work, we relax
this assumption and perform causal discovery from time series data originating
from mixtures of different causal models. We infer both the underlying
structural causal models and the posterior probability for each sample
belonging to a specific mixture component. Our approach employs an end-to-end
training process that maximizes an evidence-lower bound for data likelihood.
Through extensive experimentation on both synthetic and real-world datasets, we
demonstrate that our method surpasses state-of-the-art benchmarks in causal
discovery tasks, particularly when the data emanates from diverse underlying
causal graphs. Theoretically, we prove the identifiability of such a model
under some mild assumptions.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06313" title="Abstract">arXiv:2310.06313</a> [<a href="/pdf/2310.06313" title="Download PDF">pdf</a>, <a href="/format/2310.06313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Pose-Guided Image Synthesis with Progressive Conditional  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+F">Fei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent work has showcased the significant potential of diffusion models in
pose-guided person image synthesis. However, owing to the inconsistency in pose
between the source and target images, synthesizing an image with a distinct
pose, relying exclusively on the source image and target pose information,
remains a formidable challenge. This paper presents Progressive Conditional
Diffusion Models (PCDMs) that incrementally bridge the gap between person
images under the target and source poses through three stages. Specifically, in
the first stage, we design a simple prior conditional diffusion model that
predicts the global features of the target image by mining the global alignment
relationship between pose coordinates and image appearance. Then, the second
stage establishes a dense correspondence between the source and target images
using the global features from the previous stage, and an inpainting
conditional diffusion model is proposed to further align and enhance the
contextual features, generating a coarse-grained person image. In the third
stage, we propose a refining conditional diffusion model to utilize the
coarsely generated image from the previous stage as a condition, achieving
texture restoration and enhancing fine-detail consistency. The three-stage
PCDMs work progressively to generate the final high-quality and high-fidelity
synthesized image. Both qualitative and quantitative results demonstrate the
consistency and photorealism of our proposed PCDMs under challenging
scenarios.The code and model will be available at
https://github.com/muzishen/PCDMs.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06319" title="Abstract">arXiv:2310.06319</a> [<a href="/pdf/2310.06319" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer learning-based physics-informed convolutional neural network  for simulating flow in porous media with time-varying controls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jungang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gildin%2C+E">Eduardo Gildin</a>, 
<a href="/search/cs?searchtype=author&query=Killough%2C+J+E">John E. Killough</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">A physics-informed convolutional neural network is proposed to simulate two
phase flow in porous media with time-varying well controls. While most of
PICNNs in existing literatures worked on parameter-to-state mapping, our
proposed network parameterizes the solution with time-varying controls to
establish a control-to-state regression. Firstly, finite volume scheme is
adopted to discretize flow equations and formulate loss function that respects
mass conservation laws. Neumann boundary conditions are seamlessly incorporated
into the semi-discretized equations so no additional loss term is needed. The
network architecture comprises two parallel U-Net structures, with network
inputs being well controls and outputs being the system states. To capture the
time-dependent relationship between inputs and outputs, the network is well
designed to mimic discretized state space equations. We train the network
progressively for every timestep, enabling it to simultaneously predict oil
pressure and water saturation at each timestep. After training the network for
one timestep, we leverage transfer learning techniques to expedite the training
process for subsequent timestep. The proposed model is used to simulate
oil-water porous flow scenarios with varying reservoir gridblocks and aspects
including computation efficiency and accuracy are compared against
corresponding numerical approaches. The results underscore the potential of
PICNN in effectively simulating systems with numerous grid blocks, as
computation time does not scale with model dimensionality. We assess the
temporal error using 10 different testing controls with variation in magnitude
and another 10 with higher alternation frequency with proposed control-to-state
architecture. Our observations suggest the need for a more robust and reliable
model when dealing with controls that exhibit significant variations in
magnitude or frequency.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06320" title="Abstract">arXiv:2310.06320</a> [<a href="/pdf/2310.06320" title="Download PDF">pdf</a>, <a href="/format/2310.06320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Generation of Test Cases based on Bug Reports: a Feasibility  Study with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plein%2C+L">Laura Plein</a>, 
<a href="/search/cs?searchtype=author&query=Ou%C3%A9draogo%2C+W+C">Wendk&#xfb;uni C. Ou&#xe9;draogo</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jacques Klein</a>, 
<a href="/search/cs?searchtype=author&query=Bissyand%C3%A9%2C+T+F">Tegawend&#xe9; F. Bissyand&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software testing is a core discipline in software engineering where a large
array of research results has been produced, notably in the area of automatic
test generation. Because existing approaches produce test cases that either can
be qualified as simple (e.g. unit tests) or that require precise
specifications, most testing procedures still rely on test cases written by
humans to form test suites. Such test suites, however, are incomplete: they
only cover parts of the project or they are produced after the bug is fixed.
Yet, several research challenges, such as automatic program repair, and
practitioner processes, build on the assumption that available test suites are
sufficient. There is thus a need to break existing barriers in automatic test
case generation. While prior work largely focused on random unit testing
inputs, we propose to consider generating test cases that realistically
represent complex user execution scenarios, which reveal buggy behaviour. Such
scenarios are informally described in bug reports, which should therefore be
considered as natural inputs for specifying bug-triggering test cases. In this
work, we investigate the feasibility of performing this generation by
leveraging large language models (LLMs) and using bug reports as inputs. Our
experiments include the use of ChatGPT, as an online service, as well as
CodeGPT, a code-related pre-trained LLM that was fine-tuned for our task.
Overall, we experimentally show that bug reports associated to up to 50% of
Defects4J bugs can prompt ChatGPT to generate an executable test case. We show
that even new bug reports can indeed be used as input for generating executable
test cases. Finally, we report experimental results which confirm that
LLM-generated test cases are immediately useful in software engineering tasks
such as fault localization as well as patch validation in automated program
repair.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06322" title="Abstract">arXiv:2310.06322</a> [<a href="/pdf/2310.06322" title="Download PDF">pdf</a>, <a href="/format/2310.06322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Three Types of Freezing of Gait Events Using Deep Learning  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+W+T">Wen Tao Mo</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J+H">Jonathan H. Chan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Freezing of gait is a Parkinson's Disease symptom that episodically inflicts
a patient with the inability to step or turn while walking. While medical
experts have discovered various triggers and alleviating actions for freezing
of gait, the underlying causes and prediction models are still being explored
today. Current freezing of gait prediction models that utilize machine learning
achieve high sensitivity and specificity in freezing of gait predictions based
on time-series data; however, these models lack specifications on the type of
freezing of gait events. We develop various deep learning models using the
transformer encoder architecture plus Bidirectional LSTM layers and different
feature sets to predict the three different types of freezing of gait events.
The best performing model achieves a score of 0.427 on testing data, which
would rank top 5 in Kaggle's Freezing of Gait prediction competition, hosted by
THE MICHAEL J. FOX FOUNDATION. However, we also recognize overfitting in
training data that could be potentially improved through pseudo labelling on
additional data and model architecture simplification.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06326" title="Abstract">arXiv:2310.06326</a> [<a href="/pdf/2310.06326" title="Download PDF">pdf</a>, <a href="/format/2310.06326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal  Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yusheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouhan Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Multimodal information extraction is attracting research attention nowadays,
which requires aggregating representations from different modalities. In this
paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM)
method for this task, which contains two modules. Firstly, the intra-sample
relationship modeling module operates on a single sample and aims to learn
effective representations. Embeddings from textual and visual modalities are
shifted to bridge the modality gap caused by distinct pre-trained language and
image models. Secondly, the inter-sample relationship modeling module considers
relationships among multiple samples and focuses on capturing the interactions.
An AttnMixup strategy is proposed, which not only enables collaboration among
samples but also augments data to improve generalization. We conduct extensive
experiments on the multimodal named entity recognition datasets Twitter-2015
and Twitter-2017, and the multimodal relation extraction dataset MNRE. Our
proposed method I2SRM achieves competitive results, 77.12% F1-score on
Twitter-2015, 88.40% F1-score on Twitter-2017, and 84.12% F1-score on MNRE.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06328" title="Abstract">arXiv:2310.06328</a> [<a href="/pdf/2310.06328" title="Download PDF">pdf</a>, <a href="/format/2310.06328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploit the antenna response consistency to define the alignment  criteria for CSI data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiangtao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hongyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dingchang Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Self-supervised learning (SSL) for WiFi-based human activity recognition
(HAR) holds great promise due to its ability to address the challenge of
insufficient labeled data. However, directly transplanting SSL algorithms,
especially contrastive learning, originally designed for other domains to CSI
data, often fails to achieve the expected performance. We attribute this issue
to the inappropriate alignment criteria, which disrupt the semantic distance
consistency between the feature space and the input space. To address this
challenge, we introduce \textbf{A}netenna \textbf{R}esponse
\textbf{C}onsistency (ARC) as a solution to define proper alignment criteria.
ARC is designed to retain semantic information from the input space while
introducing robustness to real-world noise. We analyze ARC from the perspective
of CSI data structure, demonstrating that its optimal solution leads to a
direct mapping from input CSI data to action vectors in the feature map.
Furthermore, we provide extensive experimental evidence to validate the
effectiveness of ARC in improving the performance of self-supervised learning
for WiFi-based HAR.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06329" title="Abstract">arXiv:2310.06329</a> [<a href="/pdf/2310.06329" title="Download PDF">pdf</a>, <a href="/format/2310.06329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise Payload Delivery via Unmanned Aerial Vehicles: An Approach Using  Object Detection Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vadduri%2C+A">Aditya Vadduri</a>, 
<a href="/search/cs?searchtype=author&query=Benjwal%2C+A">Anagh Benjwal</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+A">Abhishek Pai</a>, 
<a href="/search/cs?searchtype=author&query=Quadros%2C+E">Elkan Quadros</a>, 
<a href="/search/cs?searchtype=author&query=Kammar%2C+A">Aniruddh Kammar</a>, 
<a href="/search/cs?searchtype=author&query=Uday%2C+P">Prajwal Uday</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Second International Conference on Artificial Intelligence, Computational Electronics and Communication System (AICECS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent years have seen tremendous advancements in the area of autonomous
payload delivery via unmanned aerial vehicles, or drones. However, most of
these works involve delivering the payload at a predetermined location using
its GPS coordinates. By relying on GPS coordinates for navigation, the
precision of payload delivery is restricted to the accuracy of the GPS network
and the availability and strength of the GPS connection, which may be severely
restricted by the weather condition at the time and place of operation. In this
work we describe the development of a micro-class UAV and propose a novel
navigation method that improves the accuracy of conventional navigation methods
by incorporating a deep-learning-based computer vision approach to identify and
precisely align the UAV with a target marked at the payload delivery position.
This proposed method achieves a 500% increase in average horizontal precision
over conventional GPS-based approaches.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06332" title="Abstract">arXiv:2310.06332</a> [<a href="/pdf/2310.06332" title="Download PDF">pdf</a>, <a href="/format/2310.06332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrowdRec: 3D Crowd Reconstruction from Single Color Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Buzhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+J">Jingyi Ju</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yangang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This is a technical report for the GigaCrowd challenge. Reconstructing 3D
crowds from monocular images is a challenging problem due to mutual occlusions,
server depth ambiguity, and complex spatial distribution. Since no large-scale
3D crowd dataset can be used to train a robust model, the current multi-person
mesh recovery methods can hardly achieve satisfactory performance in crowded
scenes. In this paper, we exploit the crowd features and propose a
crowd-constrained optimization to improve the common single-person method on
crowd images. To avoid scale variations, we first detect human bounding-boxes
and 2D poses from the original images with off-the-shelf detectors. Then, we
train a single-person mesh recovery network using existing in-the-wild image
datasets. To promote a more reasonable spatial distribution, we further propose
a crowd constraint to refine the single-person network parameters. With the
optimization, we can obtain accurate body poses and shapes with reasonable
absolute positions from a large-scale crowd image using a single-person
backbone. The code will be publicly available
at~\url{https://github.com/boycehbz/CrowdRec}.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06333" title="Abstract">arXiv:2310.06333</a> [<a href="/pdf/2310.06333" title="Download PDF">pdf</a>, <a href="/ps/2310.06333" title="Download PostScript">ps</a>, <a href="/format/2310.06333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning bounded-degree polytrees with known skeleton
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choo%2C+D">Davin Choo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J+Q">Joy Qiping Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+A">Arnab Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Canonne%2C+C+L">Cl&#xe9;ment L. Canonne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We establish finite-sample guarantees for efficient proper learning of
bounded-degree polytrees, a rich class of high-dimensional probability
distributions and a subclass of Bayesian networks, a widely-studied type of
graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample
guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees.
We extend their results by providing an efficient algorithm which learns
$d$-polytrees in polynomial time and sample complexity for any bounded $d$ when
the underlying undirected graph (skeleton) is known. We complement our
algorithm with an information-theoretic sample complexity lower bound, showing
that the dependence on the dimension and target accuracy parameters are nearly
tight.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06335" title="Abstract">arXiv:2310.06335</a> [<a href="/pdf/2310.06335" title="Download PDF">pdf</a>, <a href="/format/2310.06335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BBCA-CHAIN: One-Message, Low Latency BFT Consensus on a DAG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malkhi%2C+D">Dahlia Malkhi</a>, 
<a href="/search/cs?searchtype=author&query=Stathakopoulou%2C+C">Chrysoula Stathakopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+M">Maofan Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">This paper presents a partially synchronous BFT consensus protocol powered by
BBCA, a lightly modified Byzantine Consistent Broadcast (CBC) primitive. BBCA
provides a Complete-Adopt semantic through an added probing interface to allow
either aborting the broadcast by correct nodes or exclusively, adopting the
message consistently in case of a potential delivery. It does not introduce any
extra type of messages or communication cost to CBC.
<br />BBCA is harnessed into BBCA-CHAIN to make direct commits on a chained
backbone of a causally ordered graph of blocks, without any additional voting
blocks or artificial layering. With the help of Complete-Adopt, the additional
knowledge gained from the underlying CBC completely removes the voting latency
in popular DAG-based protocols. At the same time, causal ordering allows nodes
to propose blocks in parallel and achieve high throughput.
<br />BBCA-CHAIN thus closes up the gap between protocols built by consistent
broadcasts (e.g., Bullshark) to those without such an abstraction (e.g.,
PBFT/HotStuff), emphasizing their shared fundamental principles. Using a
Bracha-style CBC as an example, we fully specify BBCA-CHAIN with simplicity,
serving as a solid basis for high-performance replication systems (and
blockchains).
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06337" title="Abstract">arXiv:2310.06337</a> [<a href="/pdf/2310.06337" title="Download PDF">pdf</a>, <a href="/format/2310.06337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Style Awareness of Font Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haraguchi%2C+D">Daichi Haraguchi</a>, 
<a href="/search/cs?searchtype=author&query=Uchida%2C+S">Seiichi Uchida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICDAR WML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">When we compare fonts, we often pay attention to styles of local parts, such
as serifs and curvatures. This paper proposes an attention mechanism to find
important local parts. The local parts with larger attention are then
considered important. The proposed mechanism can be trained in a
quasi-self-supervised manner that requires no manual annotation other than
knowing that a set of character images is from the same font, such as
Helvetica. After confirming that the trained attention mechanism can find
style-relevant local parts, we utilize the resulting attention for local
style-aware font generation. Specifically, we design a new reconstruction loss
function to put more weight on the local parts with larger attention for
generating character images with more accurate style realization. This loss
function has the merit of applicability to various font generation models. Our
experimental results show that the proposed loss function improves the quality
of generated character images by several few-shot font generation models.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06338" title="Abstract">arXiv:2310.06338</a> [<a href="/pdf/2310.06338" title="Download PDF">pdf</a>, <a href="/format/2310.06338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Safe than Sorry: Recovering after Adversarial Majority
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+S">Srivatsan Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Zindros%2C+D">Dionysis Zindros</a>, 
<a href="/search/cs?searchtype=author&query=Tse%2C+D">David Tse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The security of blockchain protocols is a combination of two properties:
safety and liveness. It is well known that no blockchain protocol can provide
both to sleepy (intermittently online) clients under adversarial majority.
However, safety is more critical in that a single safety violation can cause
users to lose money. At the same time, liveness must not be lost forever. We
show that, in a synchronous network, it is possible to maintain safety for all
clients even during adversarial majority, and recover liveness after honest
majority is restored. Our solution takes the form of a recovery gadget that can
be applied to any protocol with certificates (such as HotStuff, Streamlet,
Tendermint, and their variants).
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06341" title="Abstract">arXiv:2310.06341</a> [<a href="/pdf/2310.06341" title="Download PDF">pdf</a>, <a href="/format/2310.06341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Reduced Information Leakage and Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+T">Tongxin Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xueru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khalili%2C+M+M">Mohammad Mahdi Khalili</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingyan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated learning (FL) is a distributed learning paradigm that allows
multiple decentralized clients to collaboratively learn a common model without
sharing local data. Although local data is not exposed directly, privacy
concerns nonetheless exist as clients' sensitive information can be inferred
from intermediate computations. Moreover, such information leakage accumulates
substantially over time as the same data is repeatedly used during the
iterative learning process. As a result, it can be particularly difficult to
balance the privacy-accuracy trade-off when designing privacy-preserving FL
algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning
framework with first-order approximation applied at every even iteration. Under
this framework, half of the FL updates incur no information leakage and require
much less computation. We first conduct the theoretical analysis on the
convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to
preserve privacy. Experiments on real-world data show that Upcycled-FL
consistently outperforms existing methods over heterogeneous data, and
significantly improves privacy-accuracy trade-off while reducing 48% of the
training time on average.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06342" title="Abstract">arXiv:2310.06342</a> [<a href="/pdf/2310.06342" title="Download PDF">pdf</a>, <a href="/format/2310.06342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Prompt Learning-based Code Search based on Interaction  Matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yubo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanfang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xinxin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yunfeng Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Code search aims to retrieve the code snippet that highly matches the given
query described in natural language. Recently, many code pre-training
approaches have demonstrated impressive performance on code search. However,
existing code search methods still suffer from two performance constraints:
inadequate semantic representation and the semantic gap between natural
language (NL) and programming language (PL). In this paper, we propose CPLCS, a
contrastive prompt learning-based code search method based on the cross-modal
interaction mechanism. CPLCS comprises:(1) PL-NL contrastive learning, which
learns the semantic matching relationship between PL and NL representations;
(2) a prompt learning design for a dual-encoder structure that can alleviate
the problem of inadequate semantic representation; (3) a cross-modal
interaction mechanism to enhance the fine-grained mapping between NL and PL. We
conduct extensive experiments to evaluate the effectiveness of our approach on
a real-world dataset across six programming languages. The experiment results
demonstrate the efficacy of our approach in improving semantic representation
quality and mapping ability between PL and NL.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06343" title="Abstract">arXiv:2310.06343</a> [<a href="/pdf/2310.06343" title="Download PDF">pdf</a>, <a href="/format/2310.06343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Continuous Control with Consistency Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongbin Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Due to its training stability and strong expression, the diffusion model has
attracted considerable attention in offline reinforcement learning. However,
several challenges have also come with it: 1) The demand for a large number of
diffusion steps makes the diffusion-model-based methods time inefficient and
limits their applications in real-time control; 2) How to achieve policy
improvement with accurate guidance for diffusion model-based policy is still an
open problem. Inspired by the consistency model, we propose a novel
time-efficiency method named Consistency Policy with Q-Learning (CPQL), which
derives action from noise by a single step. By establishing a mapping from the
reverse diffusion trajectories to the desired policy, we simultaneously address
the issues of time efficiency and inaccurate guidance when updating diffusion
model-based policy with the learned Q-function. We demonstrate that CPQL can
achieve policy improvement with accurate guidance for offline reinforcement
learning, and can be seamlessly extended for online RL tasks. Experimental
results indicate that CPQL achieves new state-of-the-art performance on 11
offline and 21 online tasks, significantly improving inference speed by nearly
45 times compared to Diffusion-QL. We will release our code later.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06344" title="Abstract">arXiv:2310.06344</a> [<a href="/pdf/2310.06344" title="Download PDF">pdf</a>, <a href="/format/2310.06344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filter Pruning For CNN With Enhanced Linear Representation Redundancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bojue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chunmei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nianbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinqi Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Structured network pruning excels non-structured methods because they can
take advantage of the thriving developed parallel computing techniques. In this
paper, we propose a new structured pruning method. Firstly, to create more
structured redundancy, we present a data-driven loss function term calculated
from the correlation coefficient matrix of different feature maps in the same
layer, named CCM-loss. This loss term can encourage the neural network to learn
stronger linear representation relations between feature maps during the
training from the scratch so that more homogenous parts can be removed later in
pruning. CCM-loss provides us with another universal transcendental
mathematical tool besides L*-norm regularization, which concentrates on
generating zeros, to generate more redundancy but for the different genres.
Furthermore, we design a matching channel selection strategy based on principal
components analysis to exploit the maximum potential ability of CCM-loss. In
our new strategy, we mainly focus on the consistency and integrality of the
information flow in the network. Instead of empirically hard-code the retain
ratio for each layer, our channel selection strategy can dynamically adjust
each layer's retain ratio according to the specific circumstance of a
per-trained model to push the prune ratio to the limit. Notably, on the
Cifar-10 dataset, our method brings 93.64% accuracy for pruned VGG-16 with only
1.40M parameters and 49.60M FLOPs, the pruned ratios for parameters and FLOPs
are 90.6% and 84.2%, respectively. For ResNet-50 trained on the ImageNet
dataset, our approach achieves 42.8% and 47.3% storage and computation
reductions, respectively, with an accuracy of 76.23%. Our code is available at
https://github.com/Bojue-Wang/CCM-LRR.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06347" title="Abstract">arXiv:2310.06347</a> [<a href="/pdf/2310.06347" title="Download PDF">pdf</a>, <a href="/format/2310.06347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JointNet: Extending Text-to-Image Diffusion for Dense Distribution  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuanxun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tian Fang</a>, 
<a href="/search/cs?searchtype=author&query=McKinnon%2C+D">David McKinnon</a>, 
<a href="/search/cs?searchtype=author&query=Tsin%2C+Y">Yanghai Tsin</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+L">Long Quan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yao Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce JointNet, a novel neural network architecture for modeling the
joint distribution of images and an additional dense modality (e.g., depth
maps). JointNet is extended from a pre-trained text-to-image diffusion model,
where a copy of the original network is created for the new dense modality
branch and is densely connected with the RGB branch. The RGB branch is locked
during network fine-tuning, which enables efficient learning of the new
modality distribution while maintaining the strong generalization ability of
the large-scale pre-trained diffusion model. We demonstrate the effectiveness
of JointNet by using RGBD diffusion as an example and through extensive
experiments, showcasing its applicability in a variety of applications,
including joint RGBD generation, dense depth prediction, depth-conditioned
image generation, and coherent tile-based 3D panorama generation.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06349" title="Abstract">arXiv:2310.06349</a> [<a href="/pdf/2310.06349" title="Download PDF">pdf</a>, <a href="/ps/2310.06349" title="Download PostScript">ps</a>, <a href="/format/2310.06349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards immersive generosity: The need for a novel framework to explore  large audiovisual archives through embodied experiences in immersive  environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alliata%2C+G">Giacomo Alliata</a>, 
<a href="/search/cs?searchtype=author&query=Kenderdine%2C+S">Sarah Kenderdine</a>, 
<a href="/search/cs?searchtype=author&query=Hibberd%2C+L">Lily Hibberd</a>, 
<a href="/search/cs?searchtype=author&query=Mason%2C+I">Ingrid Mason</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the pre-published version (after peer-review)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PRESENCE: Virtual and Augmented Reality (2021) Volume 30
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">This article proposes an innovative framework to explore large audiovisual
archives using Immersive Environments to place users inside a dataset and
create an embodied experience. It starts by outlining the need for such a novel
interface to meet the needs of archival scholars and the GLAM sector, and
discusses issues in the current modes of access, mostly restrained to
traditional information retrieval systems based on metadata. The paper presents
the concept of ``generous interfaces" as a preliminary approach to address
these issues, and argues some of the key reasons why employing Immersive Visual
Storytelling might benefit such frameworks. The theory of embodiment is
leveraged to justify this claim, showing how a more embodied understanding of a
collection can result in a stronger engagement for the public. By placing users
as actors in the experience rather than mere spectators, the emergence of
narrative is driven by their interactions, with benefits in terms of engagement
with the public and understanding of the cultural component. The framework we
propose is applied to two existing installations to analyze them in-depth and
critique them, highlighting the key directions to pursue for further
development.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06351" title="Abstract">arXiv:2310.06351</a> [<a href="/pdf/2310.06351" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fire Detection From Image and Video Using YOLOv5
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+A">Arafat Islam</a>, 
<a href="/search/cs?searchtype=author&query=Habib%2C+M+I">Md. Imtiaz Habib</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 sections, unpublished paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For the detection of fire-like targets in indoor, outdoor and forest fire
images, as well as fire detection under different natural lights, an improved
YOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection
model expands the feature extraction network from three dimensions, which
enhances feature propagation of fire small targets identification, improves
network performance, and reduces model parameters. Furthermore, through the
promotion of the feature pyramid, the top-performing prediction box is
obtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art
object detection networks, notably in the detection of small targets of fire
and smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection
model can effectively deal with the inspection of small fire targets, as well
as fire-like and smoke-like objects with F1 score 0.88. When the input image
size is 416 x 416 resolution, the average detection time is 0.12 s per frame,
which can provide real-time forest fire detection. Moreover, the algorithm
proposed in this paper can also be applied to small target detection under
other complicated situations. The proposed system shows an improved approach in
all fire detection metrics such as precision, recall, and mean average
precision.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06356" title="Abstract">arXiv:2310.06356</a> [<a href="/pdf/2310.06356" title="Download PDF">pdf</a>, <a href="/format/2310.06356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Semantic Invariant Robust Watermark for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Leyi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+S">Shiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lijie Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Watermark algorithms for large language models (LLMs) have achieved extremely
high accuracy in detecting text generated by LLMs. Such algorithms typically
involve adding extra watermark logits to the LLM's logits at each generation
step. However, prior algorithms face a trade-off between attack robustness and
security robustness. This is because the watermark logits for a token are
determined by a certain number of preceding tokens; a small number leads to low
security robustness, while a large number results in insufficient attack
robustness. In this work, we propose a semantic invariant watermarking method
for LLMs that provides both attack robustness and security robustness. The
watermark logits in our work are determined by the semantics of all preceding
tokens. Specifically, we utilize another embedding LLM to generate semantic
embeddings for all preceding tokens, and then these semantic embeddings are
transformed into the watermark logits through our trained watermark model.
Subsequent analyses and experiments demonstrated the attack robustness of our
method in semantically invariant settings: synonym substitution and text
paraphrasing settings. Finally, we also show that our watermark possesses
adequate security robustness. Our code and data are available at
https://github.com/THU-BPM/Robust_Watermark.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06358" title="Abstract">arXiv:2310.06358</a> [<a href="/pdf/2310.06358" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and  Shortest Paths-based Centrality Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meghanathan%2C+N">Natarajan Meghanathan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We perform factor analysis on the raw data of the four major neighborhood and
shortest paths-based centrality metrics (Degree, Eigenvector, Betweeenness and
Closeness) and propose a novel quantitative measure called the
Core-Intermediate-Peripheral (CIP) Index to capture the extent with which a
node could play the role of a core node (nodes at the center of a network with
larger values for any centrality metric) vis-a-vis a peripheral node (nodes
that exist at the periphery of a network with lower values for any centrality
metric). We conduct factor analysis (varimax-based rotation of the
Eigenvectors) on the transpose matrix of the raw centrality metrics dataset,
with the node ids as features, under the hypothesis that there are two factors
(core and peripheral) that drive the values incurred by the nodes with respect
to the centrality metrics. We test our approach on a diverse suite of 12
complex real-world networks.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06361" title="Abstract">arXiv:2310.06361</a> [<a href="/pdf/2310.06361" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anticipating Impacts: Using Large-Scale Scenario Writing to Explore  Diverse Implications of Generative AI in the News Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kieslich%2C+K">Kimon Kieslich</a>, 
<a href="/search/cs?searchtype=author&query=Diakopoulos%2C+N">Nicholas Diakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Helberger%2C+N">Natali Helberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The tremendous rise of generative AI has reached every part of society -
including the news environment. There are many concerns about the individual
and societal impact of the increasing use of generative AI, including issues
such as disinformation and misinformation, discrimination, and the promotion of
social tensions. However, research on anticipating the impact of generative AI
is still in its infancy and mostly limited to the views of technology
developers and/or researchers. In this paper, we aim to broaden the perspective
and capture the expectations of three stakeholder groups (news consumers;
technology developers; content creators) about the potential negative impacts
of generative AI, as well as mitigation strategies to address these.
Methodologically, we apply scenario writing and use participatory foresight in
the context of a survey (n=119) to delve into cognitively diverse imaginations
of the future. We qualitatively analyze the scenarios using thematic analysis
to systematically map potential impacts of generative AI on the news
environment, potential mitigation strategies, and the role of stakeholders in
causing and mitigating these impacts. In addition, we measure respondents'
opinions on a specific mitigation strategy, namely transparency obligations as
suggested in Article 52 of the draft EU AI Act. We compare the results across
different stakeholder groups and elaborate on the (non-) presence of different
expected impacts across these groups. We conclude by discussing the usefulness
of scenario-writing and participatory foresight as a toolbox for generative AI
impact assessment.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06362" title="Abstract">arXiv:2310.06362</a> [<a href="/pdf/2310.06362" title="Download PDF">pdf</a>, <a href="/format/2310.06362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoCL: Alleviating Catastrophic Forgetting in Continual Text  Classification from An Information Theoretic Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Weimin Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dawei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sujian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023. An improved version of <a href="/abs/2305.07289">arXiv:2305.07289</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Continual learning (CL) aims to constantly learn new knowledge over time
while avoiding catastrophic forgetting on old tasks. We focus on continual text
classification under the class-incremental setting. Recent CL studies have
identified the severe performance decrease on analogous classes as a key factor
for catastrophic forgetting. In this paper, through an in-depth exploration of
the representation learning process in CL, we discover that the compression
effect of the information bottleneck leads to confusion on analogous classes.
To enable the model learn more sufficient representations, we propose a novel
replay-based continual text classification method, InfoCL. Our approach
utilizes fast-slow and current-past contrastive learning to perform mutual
information maximization and better recover the previously learned
representations. In addition, InfoCL incorporates an adversarial memory
augmentation strategy to alleviate the overfitting problem of replay.
Experimental results demonstrate that InfoCL effectively mitigates forgetting
and achieves state-of-the-art performance on three text classification tasks.
The code is publicly available at https://github.com/Yifan-Song793/InfoCL.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06364" title="Abstract">arXiv:2310.06364</a> [<a href="/pdf/2310.06364" title="Download PDF">pdf</a>, <a href="/format/2310.06364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup  Anomalous Sound Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Soonhyeon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jung-Woo Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Unsupervised anomalous sound detection (ASD) aims to identify anomalous
sounds by learning the features of normal operational sounds and sensing their
deviations. Recent approaches have focused on the self-supervised task
utilizing the classification of normal data, and advanced models have shown
that securing representation space for anomalous data is important through
representation learning yielding compact intra-class and well-separated
intra-class distributions. However, we show that conventional approaches often
fail to ensure sufficient intra-class compactness and exhibit angular disparity
between samples and their corresponding centers. In this paper, we propose a
training technique aimed at ensuring intra-class compactness and increasing the
angle gap between normal and abnormal samples. Furthermore, we present an
architecture that extracts features for important temporal regions, enabling
the model to learn which time frames should be emphasized or suppressed.
Experimental results demonstrate that the proposed method achieves the best
performance giving 0.90%, 0.83%, and 2.16% improvement in terms of AUC, pAUC,
and mAUC, respectively, compared to the state-of-the-art method on DCASE 2020
Challenge Task2 dataset.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06365" title="Abstract">arXiv:2310.06365</a> [<a href="/pdf/2310.06365" title="Download PDF">pdf</a>, <a href="/format/2310.06365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+C">Cheng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhaoji Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lihong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify
equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However,
this task faces challenges due to the presence of different types of
information, including neighboring entities, multi-modal attributes, and entity
types. Directly incorporating the above information (e.g., concatenation or
attention) can lead to an unaligned information space. To address these
challenges, we propose a novel MMEA transformer, called MoAlign, that
hierarchically introduces neighbor features, multi-modal attributes, and entity
types to enhance the alignment task. Taking advantage of the transformer's
ability to better integrate multiple information, we design a hierarchical
modifiable self-attention block in a transformer encoder to preserve the unique
semantics of different information. Furthermore, we design two entity-type
prefix injection methods to integrate entity-type information using type
prefixes, which help to restrict the global information of entities not present
in the MMKGs. Our extensive experiments on benchmark datasets demonstrate that
our approach outperforms strong competitors and achieves excellent entity
alignment performance.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06366" title="Abstract">arXiv:2310.06366</a> [<a href="/pdf/2310.06366" title="Download PDF">pdf</a>, <a href="/format/2310.06366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Peak AoI of UAV-assisted IoT Networks: A Stochastic Geometry  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qin%2C+Y">Yujie Qin</a>, 
<a href="/search/eess?searchtype=author&query=Kishk%2C+M+A">Mustafa A. Kishk</a>, 
<a href="/search/eess?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we analyze the peak age of information (PAoI) in UAV-assisted
internet of thing (IoT) networks, in which the locations of IoT devices are
modeled by a Mat\'{e}rn cluster process (MCP) and UAVs are deployed at the
cluster centers to collect the status updates from the devices. Specifically,
we consider that IoT devices can either monitor the same physical process or
different physical processes and UAVs split their resources, time or bandwidth,
to serve the devices to avoid inter-cluster interference. Using tools from
stochastic geometry, we are able to compute the mean activity probability of
IoT devices and the conditional success probability of an individual device. We
then use tools from queuing theory to compute the PAoI under two load models
and two scenarios for devices, respectively. Our numerical results show
interesting system insights. We first show that for a low data arrival rate,
increasing the number of correlated devices can improve the PAoI for both load
models. Next, we show that even though the time-splitting technique causes
higher interference, it has a limited impact on the mean PAoI, and the mean
PAoI benefits more from the time-splitting technique. This is because of the
nature of UAV communication, especially at places where devices (users) are
spatially-clustered: shorter transmission distances and better communication
channels, comparing the links established by the cluster UAV and serving
devices (users) to links established by interferers.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06367" title="Abstract">arXiv:2310.06367</a> [<a href="/pdf/2310.06367" title="Download PDF">pdf</a>, <a href="/format/2310.06367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrugCLIP: Contrastive Protein-Molecule Representation Learning for  Virtual Screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+B">Bowen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Qiang%2C+B">Bo Qiang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haichuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+M">Minsi Ren</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yinjun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+M">Minsi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingjing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weiying Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yanyan Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Virtual screening, which identifies potential drugs from vast compound
databases to bind with a particular protein pocket, is a critical step in
AI-assisted drug discovery. Traditional docking methods are highly
time-consuming, and can only work with a restricted search library in real-life
applications. Recent supervised learning approaches using scoring functions for
binding-affinity prediction, although promising, have not yet surpassed docking
methods due to their strong dependency on limited data with reliable
binding-affinity labels. In this paper, we propose a novel contrastive learning
framework, DrugCLIP, by reformulating virtual screening as a dense retrieval
task and employing contrastive learning to align representations of binding
protein pockets and molecules from a large quantity of pairwise data without
explicit binding-affinity scores. We also introduce a biological-knowledge
inspired data augmentation strategy to learn better protein-molecule
representations. Extensive experiments show that DrugCLIP significantly
outperforms traditional docking and supervised learning methods on diverse
virtual screening benchmarks with highly reduced computation time, especially
in zero-shot setting.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06368" title="Abstract">arXiv:2310.06368</a> [<a href="/pdf/2310.06368" title="Download PDF">pdf</a>, <a href="/format/2310.06368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoinSeg: Contrast Inter- and Intra- Class Representations for  Incremental Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zekang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Guangyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jianbo Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C+H">Chi Harold Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yunchao Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Class incremental semantic segmentation aims to strike a balance between the
model's stability and plasticity by maintaining old knowledge while adapting to
new concepts. However, most state-of-the-art methods use the freeze strategy
for stability, which compromises the model's plasticity.In contrast, releasing
parameter training for plasticity could lead to the best performance for all
categories, but this requires discriminative feature representation.Therefore,
we prioritize the model's plasticity and propose the Contrast inter- and
intra-class representations for Incremental Segmentation (CoinSeg), which
pursues discriminative representations for flexible parameter tuning. Inspired
by the Gaussian mixture model that samples from a mixture of Gaussian
distributions, CoinSeg emphasizes intra-class diversity with multiple
contrastive representation centroids. Specifically, we use mask proposals to
identify regions with strong objectness that are likely to be diverse
instances/centroids of a category. These mask proposals are then used for
contrastive representations to reinforce intra-class diversity. Meanwhile, to
avoid bias from intra-class diversity, we also apply category-level
pseudo-labels to enhance category-level consistency and inter-category
diversity. Additionally, CoinSeg ensures the model's stability and alleviates
forgetting through a specific flexible tuning strategy. We validate CoinSeg on
Pascal VOC 2012 and ADE20K datasets with multiple incremental scenarios and
achieve superior results compared to previous state-of-the-art methods,
especially in more challenging and realistic long-term scenarios. Code is
available at https://github.com/zkzhang98/CoinSeg.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06369" title="Abstract">arXiv:2310.06369</a> [<a href="/pdf/2310.06369" title="Download PDF">pdf</a>, <a href="/format/2310.06369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometrically Aligned Transfer Encoder for Inductive Transfer in  Regression Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+S+M">Sung Moon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sumin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+D">Dae-Woong Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+W">Woohyung Lim</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Sehui Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12+11 pages, 6+1 figures, 0+7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Transfer learning is a crucial technique for handling a small amount of data
that is potentially related to other abundant data. However, most of the
existing methods are focused on classification tasks using images and language
datasets. Therefore, in order to expand the transfer learning scheme to
regression tasks, we propose a novel transfer technique based on differential
geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this
method, we interpret the latent vectors from the model to exist on a Riemannian
curved manifold. We find a proper diffeomorphism between pairs of tasks to
ensure that every arbitrary point maps to a locally flat coordinate in the
overlapping region, allowing the transfer of knowledge from the source to the
target data. This also serves as an effective regularizer for the model to
behave in extrapolation regions. In this article, we demonstrate that GATE
outperforms conventional methods and exhibits stable behavior in both the
latent space and extrapolation regions for various molecular graph datasets.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06370" title="Abstract">arXiv:2310.06370</a> [<a href="/pdf/2310.06370" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advanced Efficient Strategy for Detection of Dark Objects Based on  Spiking Network with Multi-Box Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ali%2C+M">Munawar Ali</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baoqun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Bilal%2C+H">Hazrat Bilal</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aakash Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Muhammad%2C+A">Ali Muhammad</a>, 
<a href="/search/cs?searchtype=author&query=Rohra%2C+A">Avinash Rohra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Several deep learning algorithms have shown amazing performance for existing
object detection tasks, but recognizing darker objects is the largest
challenge. Moreover, those techniques struggled to detect or had a slow
recognition rate, resulting in significant performance losses. As a result, an
improved and accurate detection approach is required to address the above
difficulty. The whole study proposes a combination of spiked and normal
convolution layers as an energy-efficient and reliable object detector model.
The proposed model is split into two sections. The first section is developed
as a feature extractor, which utilizes pre-trained VGG16, and the second
section of the proposal structure is the combination of spiked and normal
Convolutional layers to detect the bounding boxes of images. We drew a
pre-trained model for classifying detected objects. With state of the art
Python libraries, spike layers can be trained efficiently. The proposed spike
convolutional object detector (SCOD) has been evaluated on VOC and Ex-Dark
datasets. SCOD reached 66.01% and 41.25% mAP for detecting 20 different objects
in the VOC-12 and 12 objects in the Ex-Dark dataset. SCOD uses 14 Giga FLOPS
for its forward path calculations. Experimental results indicated superior
performance compared to Tiny YOLO, Spike YOLO, YOLO-LITE, Tinier YOLO and
Center of loc+Xception based on mAP for the VOC dataset.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06371" title="Abstract">arXiv:2310.06371</a> [<a href="/pdf/2310.06371" title="Download PDF">pdf</a>, <a href="/format/2310.06371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partition-based differentially private synthetic data generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+D">Dihang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lihua Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Private synthetic data sharing is preferred as it keeps the distribution and
nuances of original data compared to summary statistics. The state-of-the-art
methods adopt a select-measure-generate paradigm, but measuring large domain
marginals still results in much error and allocating privacy budget iteratively
is still difficult. To address these issues, our method employs a
partition-based approach that effectively reduces errors and improves the
quality of synthetic data, even with a limited privacy budget. Results from our
experiments demonstrate the superiority of our method over existing approaches.
The synthetic data produced using our approach exhibits improved quality and
utility, making it a preferable choice for private synthetic data sharing.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06372" title="Abstract">arXiv:2310.06372</a> [<a href="/pdf/2310.06372" title="Download PDF">pdf</a>, <a href="/format/2310.06372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Diffusion-Based Image Variations for Robust Training on  Poisoned Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Hentschel%2C+M+B">Martin B. Hentschel</a>, 
<a href="/search/cs?searchtype=author&query=Poth%2C+C">Clifton Poth</a>, 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Backdoor attacks pose a serious security threat for training neural networks
as they surreptitiously introduce hidden functionalities into a model. Such
backdoors remain silent during inference on clean inputs, evading detection due
to inconspicuous behavior. However, once a specific trigger pattern appears in
the input data, the backdoor activates, causing the model to execute its
concealed function. Detecting such poisoned samples within vast datasets is
virtually impossible through manual inspection. To address this challenge, we
propose a novel approach that enables model training on potentially poisoned
datasets by utilizing the power of recent diffusion models. Specifically, we
create synthetic variations of all training samples, leveraging the inherent
resilience of diffusion models to potential trigger patterns in the data. By
combining this generative approach with knowledge distillation, we produce
student models that maintain their general performance on the task while
exhibiting robust resistance to backdoor triggers.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06374" title="Abstract">arXiv:2310.06374</a> [<a href="/pdf/2310.06374" title="Download PDF">pdf</a>, <a href="/format/2310.06374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Model Selection and Decoding for Keyphrase Generation with  Pre-trained Sequence-to-Sequence Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+W+U">Wasi Uddin Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Keyphrase Generation (KPG) is a longstanding task in NLP with widespread
applications. The advent of sequence-to-sequence (seq2seq) pre-trained language
models (PLMs) has ushered in a transformative era for KPG, yielding promising
performance improvements. However, many design decisions remain unexplored and
are often made arbitrarily. This paper undertakes a systematic analysis of the
influence of model selection and decoding strategies on PLM-based KPG. We begin
by elucidating why seq2seq PLMs are apt for KPG, anchored by an
attention-driven hypothesis. We then establish that conventional wisdom for
selecting seq2seq PLMs lacks depth: (1) merely increasing model size or
performing task-specific adaptation is not parameter-efficient; (2) although
combining in-domain pre-training with task adaptation benefits KPG, it does
partially hinder generalization. Regarding decoding, we demonstrate that while
greedy search delivers strong F1 scores, it lags in recall compared with
sampling-based methods. From our insights, we propose DeSel, a likelihood-based
decode-select algorithm that improves greedy search by an average of 4.7%
semantic F1 across five datasets. Our collective findings pave the way for
deeper future investigations into PLM-based KPG.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06376" title="Abstract">arXiv:2310.06376</a> [<a href="/pdf/2310.06376" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Martin-L&#xf6;f &#xe0; la Coq
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adjedj">Adjedj</a>, 
<a href="/search/cs?searchtype=author&query=Arthur">Arthur</a>, 
<a href="/search/cs?searchtype=author&query=Lennon-Bertrand">Lennon-Bertrand</a>, 
<a href="/search/cs?searchtype=author&query=Meven">Meven</a>, 
<a href="/search/cs?searchtype=author&query=Maillard">Maillard</a>, 
<a href="/search/cs?searchtype=author&query=Kenji">Kenji</a>, 
<a href="/search/cs?searchtype=author&query=P%5C%27edrot">P&#xe9;drot</a>, 
<a href="/search/cs?searchtype=author&query=Pierre-Marie">Pierre-Marie</a>, 
<a href="/search/cs?searchtype=author&query=Pujet">Pujet</a>, 
<a href="/search/cs?searchtype=author&query=Lo%5C%22ic">Lo&#xef;c</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We present an extensive mechanization of the meta-theory of Martin-L\"of Type
Theory (MLTT) in the Coq proof assistant. Our development builds on
pre-existing work in Agda to show not only the decidability of conversion, but
also the decidability of type checking, using an approach guided by
bidirectional type checking. From our proof of decidability, we obtain a
certified and executable type checker for a full-fledged version of MLTT with
support for $\Pi$, $\Sigma$, $\mathbb{N}$, and identity types, and one
universe. Furthermore, our development does not rely on impredicativity,
induction-recursion or any axiom beyond MLTT with a schema for indexed
inductive types and a handful of predicative universes, narrowing the gap
between the object theory and the meta-theory to a mere difference in
universes. Finally, we explain our formalization choices, geared towards a
modular development relying on Coq's features, e.g. meta-programming facilities
provided by tactics and universe polymorphism.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06379" title="Abstract">arXiv:2310.06379</a> [<a href="/pdf/2310.06379" title="Download PDF">pdf</a>, <a href="/format/2310.06379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initialization Bias of Fourier Neural Operator: Revisiting the Edge of  Chaos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koshizuka%2C+T">Takeshi Koshizuka</a>, 
<a href="/search/cs?searchtype=author&query=Fujisawa%2C+M">Masahiro Fujisawa</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+Y">Yusuke Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+I">Issei Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper investigates the initialization bias of the Fourier neural
operator (FNO). A mean-field theory for FNO is established, analyzing the
behavior of the random FNO from an ``edge of chaos'' perspective. We uncover
that the forward and backward propagation behaviors exhibit characteristics
unique to FNO, induced by mode truncation, while also showcasing similarities
to those of densely connected networks. Building upon this observation, we also
propose a FNO version of the He initialization scheme to mitigate the negative
initialization bias leading to training instability. Experimental results
demonstrate the effectiveness of our initialization scheme, enabling stable
training of a 32-layer FNO without the need for additional techniques or
significant performance degradation.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06380" title="Abstract">arXiv:2310.06380</a> [<a href="/pdf/2310.06380" title="Download PDF">pdf</a>, <a href="/format/2310.06380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAST: Cluster-Aware Self-Training for Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minwook Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juseong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kibeom Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Donggil Kang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Giltae Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages with appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-training has gained attraction because of its simplicity and
versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have
proposed successful approaches to tackle this issue, but they have diminished
the advantages of self-training because they require specific modifications in
self-training algorithms or model architectures. Furthermore, most of them are
incompatible with gradient boosting decision trees, which dominate the tabular
domain. To address this, we revisit the cluster assumption, which states that
data samples that are close to each other tend to belong to the same class.
Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for
tabular data. CAST is a simple and universally adaptable approach for enhancing
existing self-training algorithms without significant modifications.
Concretely, our method regularizes the confidence of the classifier, which
represents the value of the pseudo-label, forcing the pseudo-labels in
low-density regions to have lower confidence by leveraging prior knowledge for
each class within the training data. Extensive empirical evaluations on up to
20 real-world datasets confirm not only the superior performance of CAST but
also its robustness in various setups in self-training contexts.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06382" title="Abstract">arXiv:2310.06382</a> [<a href="/pdf/2310.06382" title="Download PDF">pdf</a>, <a href="/format/2310.06382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mutual Information Metrics for Uplink MIMO-OFDM Integrated Sensing and  Communication System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piao%2C+J">Jinghui Piao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huici Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">As the uplink sensing has the advantage of easy implementation, it attracts
great attention in integrated sensing and communication (ISAC) system. This
paper presents an uplink ISAC system based on multi-input multi-output
orthogonal frequency division multiplexing (MIMO-OFDM) technology. The mutual
information (MI) is introduced as a unified metric to evaluate the performance
of communication and sensing. In this paper, firstly, the upper and lower
bounds of communication and sensing MI are derived in details based on the
interaction between communication and sensing. And the ISAC waveform is
optimized by maximizing the weighted sum of sensing and communication MI. The
Monte Carlo simulation results show that, compared with other waveform
optimization schemes, the proposed ISAC scheme has the best overall
performance.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06383" title="Abstract">arXiv:2310.06383</a> [<a href="/pdf/2310.06383" title="Download PDF">pdf</a>, <a href="/format/2310.06383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Makes for Robust Multi-Modal Models in the Face of Missing  Modalities?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siting Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chenzhuang Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">With the growing success of multi-modal learning, research on the robustness
of multi-modal models, especially when facing situations with missing
modalities, is receiving increased attention. Nevertheless, previous studies in
this domain exhibit certain limitations, as they often lack theoretical
insights or their methodologies are tied to specific network architectures or
modalities. We model the scenarios of multi-modal models encountering missing
modalities from an information-theoretic perspective and illustrate that the
performance ceiling in such scenarios can be approached by efficiently
utilizing the information inherent in non-missing modalities. In practice,
there are two key aspects: (1) The encoder should be able to extract
sufficiently good features from the non-missing modality; (2) The extracted
features should be robust enough not to be influenced by noise during the
fusion process across modalities. To this end, we introduce Uni-Modal Ensemble
with Missing Modality Adaptation (UME-MMA). UME-MMA employs uni-modal
pre-trained weights for the multi-modal model to enhance feature extraction and
utilizes missing modality data augmentation techniques to better adapt to
situations with missing modalities. Apart from that, UME-MMA, built on a
late-fusion learning framework, allows for the plug-and-play use of various
encoders, making it suitable for a wide range of modalities and enabling
seamless integration of large-scale pre-trained encoders to further enhance
performance. And we demonstrate UME-MMA's effectiveness in audio-visual
datasets~(e.g., AV-MNIST, Kinetics-Sound, AVE) and vision-language
datasets~(e.g., MM-IMDB, UPMC Food101).
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06384" title="Abstract">arXiv:2310.06384</a> [<a href="/pdf/2310.06384" title="Download PDF">pdf</a>, <a href="/format/2310.06384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redundant and Loosely Coupled LiDAR-Wi-Fi Integration for Robust Global  Localization in Autonomous Mobile Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stathoulopoulos%2C+N">Nikolaos Stathoulopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Pagliari%2C+E">Emanuele Pagliari</a>, 
<a href="/search/cs?searchtype=author&query=Davoli%2C+L">Luca Davoli</a>, 
<a href="/search/cs?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures. Accepted for publication in the 21st International Conference on Advanced Robotics (ICAR 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a framework addressing the challenge of global
localization in autonomous mobile robotics by integrating LiDAR-based
descriptors and Wi-Fi fingerprinting in a pre-mapped environment. This is
motivated by the increasing demand for reliable localization in complex
scenarios, such as urban areas or underground mines, requiring robust systems
able to overcome limitations faced by traditional Global Navigation Satellite
System (GNSS)-based localization methods. By leveraging the complementary
strengths of LiDAR and Wi-Fi sensors used to generate predictions and evaluate
the confidence of each prediction as an indicator of potential degradation, we
propose a redundancy-based approach that enhances the system's overall
robustness and accuracy. The proposed framework allows independent operation of
the LiDAR and Wi-Fi sensors, ensuring system redundancy. By combining the
predictions while considering their confidence levels, we achieve enhanced and
consistent performance in localization tasks.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06385" title="Abstract">arXiv:2310.06385</a> [<a href="/pdf/2310.06385" title="Download PDF">pdf</a>, <a href="/format/2310.06385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic  Indoor Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishna%2C+G+S">Ghanta Sai Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Supriya%2C+K">Kundrapu Supriya</a>, 
<a href="/search/cs?searchtype=author&query=Baidya%2C+S">Sabur Baidya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The existence of variable factors within the environment can cause a decline
in camera localization accuracy, as it violates the fundamental assumption of a
static environment in Simultaneous Localization and Mapping (SLAM) algorithms.
Recent semantic SLAM systems towards dynamic environments either rely solely on
2D semantic information, or solely on geometric information, or combine their
results in a loosely integrated manner. In this research paper, we introduce
3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object
detection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic
and geometric constraints sequentially. We designed a 3D part-aware hybrid
transformer for point cloud-based object detection to identify dynamic objects.
Subsequently, we propose a dynamic feature filter based on HDBSCAN clustering
to extract objects with significant absolute depth differences. When compared
against ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across
the dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the
performance of the other four leading SLAM systems designed for dynamic
environments.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06387" title="Abstract">arXiv:2310.06387</a> [<a href="/pdf/2310.06387" title="Download PDF">pdf</a>, <a href="/format/2310.06387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jailbreak and Guard Aligned Language Models with Only Few In-Context  Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zeming Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Models (LLMs) have shown remarkable success in various tasks,
but concerns about their safety and the potential for generating malicious
content have emerged. In this paper, we explore the power of In-Context
Learning (ICL) in manipulating the alignment ability of LLMs. We find that by
providing just few in-context demonstrations without fine-tuning, LLMs can be
manipulated to increase or decrease the probability of jailbreaking, i.e.
answering malicious prompts. Based on these observations, we propose In-Context
Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding
aligned language model purposes. ICA crafts malicious contexts to guide models
in generating harmful outputs, while ICD enhances model robustness by
demonstrations of rejecting to answer harmful prompts. Our experiments show the
effectiveness of ICA and ICD in increasing or reducing the success rate of
adversarial jailbreaking attacks. Overall, we shed light on the potential of
ICL to influence LLM behavior and provide a new perspective for enhancing the
safety and alignment of LLMs.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06389" title="Abstract">arXiv:2310.06389</a> [<a href="/pdf/2310.06389" title="Download PDF">pdf</a>, <a href="/format/2310.06389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Stackable and Skippable LEGO Bricks for Efficient,  Reconfigurable, and Variable-Resolution Diffusion Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Huangjie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhendong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+G">Guanghan Ning</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Q">Quanzeng You</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models excel at generating photo-realistic images but come with
significant computational costs in both training and sampling. While various
techniques address these computational challenges, a less-explored issue is
designing an efficient and adaptable network backbone for iterative refinement.
Current options like U-Net and Vision Transformer often rely on
resource-intensive deep networks and lack the flexibility needed for generating
images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature
Enrichment and Global-content Orchestration. These bricks can be stacked to
create a test-time reconfigurable diffusion backbone, allowing selective
skipping of bricks to reduce sampling costs and generate higher-resolution
images than the training data. LEGO bricks enrich local regions with an MLP and
transform them using a Transformer block while maintaining a consistent
full-resolution image across all bricks. Experimental results demonstrate that
LEGO bricks enhance training efficiency, expedite convergence, and facilitate
variable-resolution image generation while maintaining strong generative
performance. Moreover, LEGO significantly reduces sampling time compared to
other methods, establishing it as a valuable enhancement for diffusion models.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06390" title="Abstract">arXiv:2310.06390</a> [<a href="/pdf/2310.06390" title="Download PDF">pdf</a>, <a href="/format/2310.06390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> P5: Plug-and-Play Persona Prompting for Personalized Response Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joosung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+M">Minsik Oh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donghun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The use of persona-grounded retrieval-based chatbots is crucial for
personalized conversations, but there are several challenges that need to be
addressed. 1) In general, collecting persona-grounded corpus is very expensive.
2) The chatbot system does not always respond in consideration of persona at
real applications. To address these challenges, we propose a plug-and-play
persona prompting method. Our system can function as a standard open-domain
chatbot if persona information is not available. We demonstrate that this
approach performs well in the zero-shot setting, which reduces the dependence
on persona-ground training data. This makes it easier to expand the system to
other languages without the need to build a persona-grounded corpus.
Additionally, our model can be fine-tuned for even better performance. In our
experiments, the zero-shot model improved the standard model by 7.71 and 1.04
points in the original persona and revised persona, respectively. The
fine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39
points in the original persona and revised persona, respectively. To the best
of our knowledge, this is the first attempt to solve the problem of
personalized response selection using prompt sequences. Our code is available
on github~\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06391" title="Abstract">arXiv:2310.06391</a> [<a href="/pdf/2310.06391" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved prompting and process for writing user personas with LLMs,  using qualitative interviews: Capturing behaviour and personality traits of  users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Paoli%2C+S">Stefano De Paoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">This draft paper presents a workflow for creating User Personas with Large
Language Models, using the results of a Thematic Analysis of qualitative
interviews. The proposed workflow uses improved prompting and a larger pool of
Themes, compared to previous work conducted by the author for the same task.
This is possible due to the capabilities of a recently released LLM which
allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to
the possibility to offer a refined prompting for the creation of Personas. The
paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then
discusses the improved workflow for creating Personas. The paper also offers
some reflections on the relationship between the proposed process and existing
approaches to Personas such as the data-driven and qualitative Personas.
Moreover, the paper offers reflections on the capacity of LLMs to capture user
behaviours and personality traits, from the underlying dataset of qualitative
interviews used for the analysis.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06393" title="Abstract">arXiv:2310.06393</a> [<a href="/pdf/2310.06393" title="Download PDF">pdf</a>, <a href="/format/2310.06393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing Administrative Data Inventories to Create a Reliable  Transnational Reference Database for Crop Type Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+M">Maja Schneider</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6rner%2C+M">Marco K&#xf6;rner</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Geoscience and Remote Sensing Symposium
  (IGARSS) 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">With leaps in machine learning techniques and their applicationon Earth
observation challenges has unlocked unprecedented performance across the
domain. While the further development of these methods was previously limited
by the availability and volume of sensor data and computing resources, the lack
of adequate reference data is now constituting new bottlenecks. Since creating
such ground-truth information is an expensive and error-prone task, new ways
must be devised to source reliable, high-quality reference data on large
scales. As an example, we showcase E URO C ROPS, a reference dataset for crop
type classification that aggregates and harmonizes administrative data surveyed
in different countries with the goal of transnational interoperability.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06396" title="Abstract">arXiv:2310.06396</a> [<a href="/pdf/2310.06396" title="Download PDF">pdf</a>, <a href="/format/2310.06396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Q">Qiyu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=She%2C+R">Rui She</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tay%2C+W+P">Wee Peng Tay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Advances in Neural Information Processing Systems (NeurIPS), New Orleans, USA, Dec. 2023, spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph neural networks (GNNs) are vulnerable to adversarial perturbations,
including those that affect both node features and graph topology. This paper
investigates GNNs derived from diverse neural flows, concentrating on their
connection to various stability notions such as BIBO stability, Lyapunov
stability, structural stability, and conservative stability. We argue that
Lyapunov stability, despite its common use, does not necessarily ensure
adversarial robustness. Inspired by physics principles, we advocate for the use
of conservative Hamiltonian neural flows to construct GNNs that are robust to
adversarial attacks. The adversarial robustness of different neural flow GNNs
is empirically compared on several benchmark datasets under a variety of
adversarial attacks. Extensive numerical experiments demonstrate that GNNs
leveraging conservative Hamiltonian flows with Lyapunov stability substantially
improve robustness against adversarial perturbations. The implementation code
of experiments is available at
https://github.com/zknus/NeurIPS-2023-HANG-Robustness.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06397" title="Abstract">arXiv:2310.06397</a> [<a href="/pdf/2310.06397" title="Download PDF">pdf</a>, <a href="/format/2310.06397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Top of the Heap: Efficient Memory Error Protection for Many Heap Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaiming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Payer%2C+M">Mathias Payer</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhiyun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Sampson%2C+J">Jack Sampson</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+G">Gang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Jaeger%2C+T">Trent Jaeger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Exploits against heap memory errors continue to be a major concern. Although
many defenses have been proposed, heap data are not protected from attacks that
exploit memory errors systematically. Research defenses focus on complete
coverage of heap objects, often giving up on comprehensive memory safety
protection and/or incurring high costs in performance overhead and memory
usage. In this paper, we propose a solution for heap memory safety enforcement
that aims to provide comprehensive protection from memory errors efficiently by
protecting those heap objects whose accesses are provably safe from memory
errors. Specifically, we present the Uriah system that statically validates
spatial and type memory safety for heap objects, isolating compliant objects on
a safe heap that enforces temporal type safety to prevent attacks on memory
reuse. Using Uriah, 71.9% of heap allocation sites can be shown to produce
objects (73% of allocations are found safe) that satisfy spatial and type
safety, which are then isolated using Uriah's heap allocator from memory
accesses via unsafe heap objects. Uriah only incurs 2.9% overhead and only uses
9.3% more memory on SPEC CPU2006 (C/C++) benchmarks, showing that many heap
objects can be protected from all classes of memory errors efficiently.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06399" title="Abstract">arXiv:2310.06399</a> [<a href="/pdf/2310.06399" title="Download PDF">pdf</a>, <a href="/format/2310.06399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lo-Hi: Practical ML Drug Discovery Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steshin%2C+S">Simon Steshin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Finding new drugs is getting harder and harder. One of the hopes of drug
discovery is to use machine learning models to predict molecular properties.
That is why models for molecular property prediction are being developed and
tested on benchmarks such as MoleculeNet. However, existing benchmarks are
unrealistic and are too different from applying the models in practice. We have
created a new practical \emph{Lo-Hi} benchmark consisting of two tasks: Lead
Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug
discovery process. For the Hi task, we designed a novel molecular splitting
algorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested
state-of-the-art and classic ML models, revealing which works better under
practical settings. We analyzed modern benchmarks and showed that they are
unrealistic and overoptimistic.
<br />Review: https://openreview.net/forum?id=H2Yb28qGLV
<br />Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023
<br />Lo-Hi splitter library: https://github.com/SteshinSS/lohi_splitter
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06403" title="Abstract">arXiv:2310.06403</a> [<a href="/pdf/2310.06403" title="Download PDF">pdf</a>, <a href="/format/2310.06403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boundary Discretization and Reliable Classification Network for Temporal  Action Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhenying Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Temporal action detection aims to recognize the action category and determine
the starting and ending time of each action instance in untrimmed videos. The
mixed methods have achieved remarkable performance by simply merging
anchor-based and anchor-free approaches. However, there are still two crucial
issues in the mixed framework: (1) Brute-force merging and handcrafted anchors
design affect the performance and practical application of the mixed methods.
(2) A large number of false positives in action category predictions further
impact the detection performance. In this paper, we propose a novel Boundary
Discretization and Reliable Classification Network (BDRC-Net) that addresses
the above issues by introducing boundary discretization and reliable
classification modules. Specifically, the boundary discretization module (BDM)
elegantly merges anchor-based and anchor-free approaches in the form of
boundary discretization, avoiding the handcrafted anchors design required by
traditional mixed methods. Furthermore, the reliable classification module
(RCM) predicts reliable action categories to reduce false positives in action
category predictions. Extensive experiments conducted on different benchmarks
demonstrate that our proposed method achieves favorable performance compared
with the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6%
on THUMOS'14, outperforming the previous best by 1.5%. The code will be
released at https://github.com/zhenyingfang/BDRC-Net.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06404" title="Abstract">arXiv:2310.06404</a> [<a href="/pdf/2310.06404" title="Download PDF">pdf</a>, <a href="/format/2310.06404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hexa: Self-Improving for Knowledge-Grounded Dialogue System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+D">Daejin Jo</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+D+W">Daniel Wontae Nam</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+G">Gunsoo Han</a>, 
<a href="/search/cs?searchtype=author&query=On%2C+K">Kyoung-Woon On</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taehwan Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Rho%2C+S">Seungeun Rho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungwoong Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">A common practice in knowledge-grounded dialogue generation is to explicitly
utilize intermediate steps (e.g., web-search, memory retrieval) with modular
approaches. However, data for such steps are often inaccessible compared to
those of dialogue responses as they are unobservable in an ordinary dialogue.
To fill in the absence of these data, we develop a self-improving method to
improve the generative performances of intermediate steps without the ground
truth data. In particular, we propose a novel bootstrapping scheme with a
guided prompt and a modified loss function to enhance the diversity of
appropriate self-generated responses. Through experiments on various benchmark
datasets, we empirically demonstrate that our method successfully leverages a
self-improving mechanism in generating intermediate and final responses and
improves the performances on the task of knowledge-grounded dialogue
generation.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06408" title="Abstract">arXiv:2310.06408</a> [<a href="/pdf/2310.06408" title="Download PDF">pdf</a>, <a href="/format/2310.06408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Humans and language models diverge when predicting repeating text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+A+R">Aditya R. Vaidya</a>, 
<a href="/search/cs?searchtype=author&query=Turek%2C+J">Javier Turek</a>, 
<a href="/search/cs?searchtype=author&query=Huth%2C+A+G">Alexander G. Huth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 26th Conference on Computational Natural Language Learning (CoNLL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language models that are trained on the next-word prediction task have been
shown to accurately model human behavior in word prediction and reading speed.
In contrast with these findings, we present a scenario in which the performance
of humans and LMs diverges. We collected a dataset of human next-word
predictions for five stimuli that are formed by repeating spans of text. Human
and GPT-2 LM predictions are strongly aligned in the first presentation of a
text span, but their performance quickly diverges when memory (or in-context
learning) begins to play a role. We traced the cause of this divergence to
specific attention heads in a middle layer. Adding a power-law recency bias to
these attention heads yielded a model that performs much more similarly to
humans. We hope that this scenario will spur future work in bringing LMs closer
to human behavior.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06412" title="Abstract">arXiv:2310.06412</a> [<a href="/pdf/2310.06412" title="Download PDF">pdf</a>, <a href="/format/2310.06412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoder-Decoder-Based Intra-Frame Block Partitioning Decision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yucheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Han Peng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yan Song</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jie Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+S">Songping Mai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">The recursive intra-frame block partitioning decision process, a crucial
component of the next-generation video coding standards, exerts significant
influence over the encoding time. In this paper, we propose an encoder-decoder
neural network (NN) to accelerate this process. Specifically, a CNN is utilized
to compress the pixel data of the largest coding unit (LCU) into a fixed-length
vector. Subsequently, a Transformer decoder is employed to transcribe the
fixed-length vector into a variable-length vector, which represents the block
partitioning outcomes of the encoding LCU. The vector transcription process
adheres to the constraints imposed by the block partitioning algorithm. By
fully parallelizing the NN prediction in the intra-mode decision, substantial
time savings can be attained during the decision phase. The experimental
results obtained from high-definition (HD) sequences coding demonstrate that
this framework achieves a remarkable 87.84\% reduction in encoding time, with a
relatively small loss (8.09\%) of coding performance compared to AVS3 HPM4.0.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06414" title="Abstract">arXiv:2310.06414</a> [<a href="/pdf/2310.06414" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plane Constraints Aided Multi-Vehicle Cooperative Positioning Using  Factor Graph Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+C">Chen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hongbo Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 16 figures, IEEE trans on ITS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">The development of vehicle-to-vehicle (V2V) communication facil-itates the
study of cooperative positioning (CP) techniques for vehicular applications.
The CP methods can improve the posi-tioning availability and accuracy by
inter-vehicle ranging and data exchange between vehicles. However, the
inter-vehicle rang-ing can be easily interrupted due to many factors such as
obsta-cles in-between two cars. Without inter-vehicle ranging, the other
cooperative data such as vehicle positions will be wasted, leading to
performance degradation of range-based CP methods. To fully utilize the
cooperative data and mitigate the impact of inter-vehicle ranging loss, a novel
cooperative positioning method aided by plane constraints is proposed in this
paper. The positioning results received from cooperative vehicles are used to
construct the road plane for each vehicle. The plane parameters are then
introduced into CP scheme to impose constraints on positioning solutions. The
state-of-art factor graph optimization (FGO) algo-rithm is employed to
integrate the plane constraints with raw data of Global Navigation Satellite
Systems (GNSS) as well as inter-vehicle ranging measurements. The proposed CP
method has the ability to resist the interruptions of inter-vehicle ranging
since the plane constraints are computed by just using position-related data. A
vehicle can still benefit from the position data of cooperative vehicles even
if the inter-vehicle ranging is unavaila-ble. The experimental results indicate
the superiority of the pro-posed CP method in positioning performance over the
existing methods, especially when the inter-ranging interruptions occur.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06415" title="Abstract">arXiv:2310.06415</a> [<a href="/pdf/2310.06415" title="Download PDF">pdf</a>, <a href="/format/2310.06415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep reinforcement learning uncovers processes for separating azeotropic  mixtures without prior knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B6ttl%2C+Q">Quirin G&#xf6;ttl</a>, 
<a href="/search/cs?searchtype=author&query=Pirnay%2C+J">Jonathan Pirnay</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+J">Jakob Burger</a>, 
<a href="/search/cs?searchtype=author&query=Grimm%2C+D+G">Dominik G. Grimm</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 7 figures, 4 tables. G\"ottl and Pirnay contributed equally as joint first authors. Burger and Grimm contributed equally as joint last authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Process synthesis in chemical engineering is a complex planning problem due
to vast search spaces, continuous parameters and the need for generalization.
Deep reinforcement learning agents, trained without prior knowledge, have shown
to outperform humans in various complex planning problems in recent years.
Existing work on reinforcement learning for flowsheet synthesis shows promising
concepts, but focuses on narrow problems in a single chemical system, limiting
its practicality. We present a general deep reinforcement learning approach for
flowsheet synthesis. We demonstrate the adaptability of a single agent to the
general task of separating binary azeotropic mixtures. Without prior knowledge,
it learns to craft near-optimal flowsheets for multiple chemical systems,
considering different feed compositions and conceptual approaches. On average,
the agent can separate more than 99% of the involved materials into pure
components, while autonomously learning fundamental process engineering
paradigms. This highlights the agent's planning flexibility, an encouraging
step toward true generality.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06417" title="Abstract">arXiv:2310.06417</a> [<a href="/pdf/2310.06417" title="Download PDF">pdf</a>, <a href="/format/2310.06417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advective Diffusion Transformers for Topological Generalization in Graph  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kaipeng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+F">Fan Nie</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph diffusion equations are intimately related to graph neural networks
(GNNs) and have recently attracted attention as a principled framework for
analyzing GNN dynamics, formalizing their expressive power, and justifying
architectural choices. One key open questions in graph learning is the
generalization capabilities of GNNs. A major limitation of current approaches
hinges on the assumption that the graph topologies in the training and test
sets come from the same distribution. In this paper, we make steps towards
understanding the generalization of GNNs by exploring how graph diffusion
equations extrapolate and generalize in the presence of varying graph
topologies. We first show deficiencies in the generalization capability of
existing models built upon local diffusion on graphs, stemming from the
exponential sensitivity to topology variation. Our subsequent analysis reveals
the promise of non-local diffusion, which advocates for feature propagation
over fully-connected latent graphs, under the assumption of a specific
data-generating condition. In addition to these findings, we propose a novel
graph encoder backbone, Advective Diffusion Transformer (ADiT), inspired by
advective graph diffusion equations that have a closed-form solution backed up
with theoretical guarantees of desired generalization under topological
distribution shifts. The new model, functioning as a versatile graph
Transformer, demonstrates superior performance across a wide range of graph
learning tasks.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06420" title="Abstract">arXiv:2310.06420</a> [<a href="/pdf/2310.06420" title="Download PDF">pdf</a>, <a href="/format/2310.06420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnoDODE: Anomaly Detection with Diffusion ODE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xianyao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Congming Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Anomaly detection is the process of identifying atypical data samples that
significantly deviate from the majority of the dataset. In the realm of
clinical screening and diagnosis, detecting abnormalities in medical images
holds great importance. Typically, clinical practice provides access to a vast
collection of normal images, while abnormal images are relatively scarce. We
hypothesize that abnormal images and their associated features tend to manifest
in low-density regions of the data distribution. Following this assumption, we
turn to diffusion ODEs for unsupervised anomaly detection, given their
tractability and superior performance in density estimation tasks. More
precisely, we propose a new anomaly detection method based on diffusion ODEs by
estimating the density of features extracted from multi-scale medical images.
Our anomaly scoring mechanism depends on computing the negative log-likelihood
of features extracted from medical images at different scales, quantified in
bits per dimension. Furthermore, we propose a reconstruction-based anomaly
localization suitable for our method. Our proposed method not only identifie
anomalies but also provides interpretability at both the image and pixel
levels. Through experiments on the BraTS2021 medical dataset, our proposed
method outperforms existing methods. These results confirm the effectiveness
and robustness of our method.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06422" title="Abstract">arXiv:2310.06422</a> [<a href="/pdf/2310.06422" title="Download PDF">pdf</a>, <a href="/format/2310.06422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Propaganda Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sprenkamp%2C+K">Kilian Sprenkamp</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+D+G">Daniel Gordon Jones</a>, 
<a href="/search/cs?searchtype=author&query=Zavolokina%2C+L">Liudmila Zavolokina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models' performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06424" title="Abstract">arXiv:2310.06424</a> [<a href="/pdf/2310.06424" title="Download PDF">pdf</a>, <a href="/format/2310.06424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feel the Tension: Manipulation of Deformable Linear Objects in  Environments with Fixtures using Force Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%BCberkr%C3%BCb%2C+F">Finn S&#xfc;berkr&#xfc;b</a>, 
<a href="/search/cs?searchtype=author&query=Laezza%2C+R">Rita Laezza</a>, 
<a href="/search/cs?searchtype=author&query=Karayiannidis%2C+Y">Yiannis Karayiannidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Humans are able to manipulate Deformable Linear Objects (DLOs) such as cables
and wires, with little or no visual information, relying mostly on force
sensing. In this work, we propose a reduced DLO model which enables such blind
manipulation by keeping the object under tension. Further, an online model
estimation procedure is also proposed. A set of elementary sliding and clipping
manipulation primitives are defined based on our model. The combination of
these primitives allows for more complex motions such as winding of a DLO. The
model estimation and manipulation primitives are tested individually but also
together in a real-world cable harness production task, using a dual-arm YuMi,
thus demonstrating that force-based perception can be sufficient even for such
a complex scenario.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06425" title="Abstract">arXiv:2310.06425</a> [<a href="/pdf/2310.06425" title="Download PDF">pdf</a>, <a href="/format/2310.06425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 6G Wireless Communications in 7-24 GHz Band: Opportunities, Techniques,  and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cui%2C+Z">Zhuangzhuang Cui</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+P">Peize Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Pollin%2C+S">Sofie Pollin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The sixth generation (6G) wireless communication nowadays is seeking a new
spectrum to inherit the pros and discard the cons of sub-6 GHz, millimeter-wave
(mmWave), and sub-terahertz (THz) bands. To this end, an upper mid-band,
Frequency Range (FR) spanning from 7 GHz to 24 GHz, also known as FR3, has
emerged as a focal point in 6G communications. Thus, as an inexorable
prerequisite, a comprehensive investigation encompassing spectrum utilization
and channel modeling is the first step to exploit potential applications and
future prospects of using this FR in the 6G ecosystem. In this article, we
provide FR3 deployment insights into emerging technologies including
non-terrestrial network (NTN), massive multi-input multi-output (mMIMO),
reconfigurable intelligent surface (RIS), and joint communications and sensing
(JCAS). Furthermore, leveraging ray-tracing simulations, our investigation
unveils the channel characteristics in FR3 are close to those in the sub-6 GHz
band. The analysis of RIS-aided communication shows a higher spectral
efficiency achieved in FR3 compared to other FRs when using the same RIS size.
Finally, challenges and promising directions are discussed for FR3-based
communication systems.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06427" title="Abstract">arXiv:2310.06427</a> [<a href="/pdf/2310.06427" title="Download PDF">pdf</a>, <a href="/format/2310.06427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zijie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wanjia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jingdong Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Ziniu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yadi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuanzhou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yizhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning complex multi-agent system dynamics from data is crucial across many
domains, such as in physical simulations and material modeling. Extended from
purely data-driven approaches, existing physics-informed approaches such as
Hamiltonian Neural Network strictly follow energy conservation law to introduce
inductive bias, making their learning more sample efficiently. However, many
real-world systems do not strictly conserve energy, such as spring systems with
frictions. Recognizing this, we turn our attention to a broader physical
principle: Time-Reversal Symmetry, which depicts that the dynamics of a system
shall remain invariant when traversed back over time. It still helps to
preserve energies for conservative systems and in the meanwhile, serves as a
strong inductive bias for non-conservative, reversible systems. To inject such
inductive bias, in this paper, we propose a simple-yet-effective
self-supervised regularization term as a soft constraint that aligns the
forward and backward trajectories predicted by a continuous graph neural
network-based ordinary differential equation (GraphODE). It effectively imposes
time-reversal symmetry to enable more accurate model predictions across a wider
range of dynamical systems under classical mechanics. In addition, we further
provide theoretical analysis to show that our regularization essentially
minimizes higher-order Taylor expansion terms during the ODE integration steps,
which enables our model to be more noise-tolerant and even applicable to
irreversible systems. Experimental results on a variety of physical systems
demonstrate the effectiveness of our proposed method. Particularly, it achieves
an MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06428" title="Abstract">arXiv:2310.06428</a> [<a href="/html/2310.06428" title="Download HTML">html</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proceedings of The first international workshop on eXplainable AI for  the Arts (XAIxArts)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bryan-Kinns%2C+N">Nick Bryan-Kinns</a>, 
<a href="/search/cs?searchtype=author&query=Ford%2C+C">Corey Ford</a>, 
<a href="/search/cs?searchtype=author&query=Chamberlain%2C+A">Alan Chamberlain</a>, 
<a href="/search/cs?searchtype=author&query=Benford%2C+S+D">Steven David Benford</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+H">Helen Kennedy</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijin Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiong%2C+W">Wu Qiong</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G+G">Gus G. Xia</a>, 
<a href="/search/cs?searchtype=author&query=Rezwana%2C+J">Jeba Rezwana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This first international workshop on explainable AI for the Arts (XAIxArts)
brought together a community of researchers in HCI, Interaction Design, AI,
explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.
<br />Workshop held at the 15th ACM Conference on Creativity and Cognition (C&amp;C
2023).
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06430" title="Abstract">arXiv:2310.06430</a> [<a href="/pdf/2310.06430" title="Download PDF">pdf</a>, <a href="/format/2310.06430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Prediction for Deep Classifier via Label Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianguo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+H">Huajun Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Linjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yue Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Statistics Theory (math.ST)

</div>
<p class="mathjax">Conformal prediction is a statistical framework that generates prediction
sets containing ground-truth labels with a desired coverage guarantee. The
predicted probabilities produced by machine learning models are generally
miscalibrated, leading to large prediction sets in conformal prediction. In
this paper, we empirically and theoretically show that disregarding the
probabilities' value will mitigate the undesirable effect of miscalibrated
probability values. Then, we propose a novel algorithm named $\textit{Sorted
Adaptive prediction sets}$ (SAPS), which discards all the probability values
except for the maximum softmax probability. The key idea behind SAPS is to
minimize the dependence of the non-conformity score on the probability values
while retaining the uncertainty information. In this manner, SAPS can produce
sets of small size and communicate instance-wise uncertainty. Theoretically, we
provide a finite-sample coverage guarantee of SAPS and show that the expected
value of set size from SAPS is always smaller than APS. Extensive experiments
validate that SAPS not only lessens the prediction sets but also broadly
enhances the conditional coverage rate and adaptation of prediction sets.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06433" title="Abstract">arXiv:2310.06433</a> [<a href="/pdf/2310.06433" title="Download PDF">pdf</a>, <a href="/format/2310.06433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retromorphic Testing: A New Approach to the Test Oracle Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Boxi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mang%2C+Q">Qiuyang Mang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qingshuo Guo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pinjia He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">A test oracle serves as a criterion or mechanism to assess the correspondence
between software output and the anticipated behavior for a given input set. In
automated testing, black-box techniques, known for their non-intrusive nature
in test oracle construction, are widely used, including notable methodologies
like differential testing and metamorphic testing. Inspired by the mathematical
concept of inverse function, we present Retromorphic Testing, a novel black-box
testing methodology. It leverages an auxiliary program in conjunction with the
program under test, which establishes a dual-program structure consisting of a
forward program and a backward program. The input data is first processed by
the forward program and then its program output is reversed to its original
input format using the backward program. In particular, the auxiliary program
can operate as either the forward or backward program, leading to different
testing modes. The process concludes by examining the relationship between the
initial input and the transformed output within the input domain. For example,
to test the implementation of the sine function $\sin(x)$, we can employ its
inverse function, $\arcsin(x)$, and validate the equation $x =
\sin(\arcsin(x)+2k\pi), \forall k \in \mathbb{Z}$. In addition to the
high-level concept of Retromorphic Testing, this paper presents its three
testing modes with illustrative use cases across diverse programs, including
algorithms, traditional software, and AI applications.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06434" title="Abstract">arXiv:2310.06434</a> [<a href="/pdf/2310.06434" title="Download PDF">pdf</a>, <a href="/format/2310.06434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whispering LLaMA: A Cross-Modal Generative Error Correction Framework  for Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+S">Srijith Radhakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S+A">Sumeer Ahmad Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Rohit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Kiani%2C+N+A">Narsis A. Kiani</a>, 
<a href="/search/cs?searchtype=author&query=Gomez-Cabrero%2C+D">David Gomez-Cabrero</a>, 
<a href="/search/cs?searchtype=author&query=Tegner%2C+J+N">Jesper N. Tegner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023. 10 pages. This work has been done in October 2022 and was submitted to EMNLP 23 once the draft was finalized. GitHub: <a href="https://github.com/Srijith-rkr/Whispering-LLaMA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We introduce a new cross-modal fusion technique designed for generative error
correction in automatic speech recognition (ASR). Our methodology leverages
both acoustic information and external linguistic representations to generate
accurate speech transcription contexts. This marks a step towards a fresh
paradigm in generative error correction within the realm of n-best hypotheses.
Unlike the existing ranking-based rescoring methods, our approach adeptly uses
distinct initialization techniques and parameter-efficient algorithms to boost
ASR performance derived from pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we evaluate the stability and
reproducibility of our fusion technique, demonstrating its improved word error
rate relative (WERR) performance in comparison to n-best hypotheses by
relatively 37.66%. To encourage future research, we have made our code and
pre-trained models open source at
https://github.com/Srijith-rkr/Whispering-LLaMA.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06435" title="Abstract">arXiv:2310.06435</a> [<a href="/pdf/2310.06435" title="Download PDF">pdf</a>, <a href="/format/2310.06435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DASICS: Enhancing Memory Protection with Dynamic Compartmentalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yue Jin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yibin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chengyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tianyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tianyue Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingyu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">In the existing software development ecosystem, security issues introduced by
third-party code cannot be overlooked. Among these security concerns, memory
access vulnerabilities stand out prominently, leading to risks such as the
theft or tampering of sensitive data. To address this issue, software-based
defense mechanisms have been established at the programming language, compiler,
and operating system levels. However, as a trade-off, these mechanisms
significantly reduce software execution efficiency. Hardware-software co-design
approaches have sought to either construct entirely isolated trusted execution
environments or attempt to partition security domains within the same address
space. While such approaches enhance efficiency compared to pure software
methods, they also encounter challenges related to granularity of protection,
performance overhead, and portability. In response to these challenges, we
present the DASICS (Dynamic in-Address-Space Isolation by Code Segments) secure
processor design, which offers dynamic and flexible security protection across
multiple privilege levels, addressing data flow protection, control flow
protection, and secure system calls. We have implemented hardware FPGA
prototypes and software QEMU simulator prototypes based on DASICS, along with
necessary modifications to system software for adaptability. We illustrate the
protective mechanisms and effectiveness of DASICS with two practical examples
and provide potential real-world use cases where DASICS could be applied.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06436" title="Abstract">arXiv:2310.06436</a> [<a href="/pdf/2310.06436" title="Download PDF">pdf</a>, <a href="/format/2310.06436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer  for Document Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+N">Nianlong Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yingqiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hahnloser%2C+R+H+R">Richard H. R. Hahnloser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is the technical research paper of CIKM 2023 DocIU challenges. The authors received the CIKM 2023 DocIU Winner Award, sponsored by Google, Microsoft, and the Centre for data-driven geoscience
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce MemSum-DQA, an efficient system for document question answering
(DQA) that leverages MemSum, a long document extractive summarizer. By
prefixing each text block in the parsed document with the provided question and
question type, MemSum-DQA selectively extracts text blocks as answers from
documents. On full-document answering tasks, this approach yields a 9%
improvement in exact match accuracy over prior state-of-the-art baselines.
Notably, MemSum-DQA excels in addressing questions related to
child-relationship understanding, underscoring the potential of extractive
summarization techniques for DQA tasks.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06437" title="Abstract">arXiv:2310.06437</a> [<a href="/pdf/2310.06437" title="Download PDF">pdf</a>, <a href="/format/2310.06437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skeleton Ground Truth Extraction: Methodology, Annotation Tool and  Benchmarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Indurkhya%2C+B">Bipin Indurkhya</a>, 
<a href="/search/cs?searchtype=author&query=See%2C+J">John See</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+B">Bo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">Yan Ke</a>, 
<a href="/search/cs?searchtype=author&query=Boukhers%2C+Z">Zeyd Boukhers</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhenyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Grzegorzek%2C+M">Marcin Grzegorzek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the International Journal of Computer Vision (IJCV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Skeleton Ground Truth (GT) is critical to the success of supervised skeleton
extraction methods, especially with the popularity of deep learning techniques.
Furthermore, we see skeleton GTs used not only for training skeleton detectors
with Convolutional Neural Networks (CNN) but also for evaluating
skeleton-related pruning and matching algorithms. However, most existing shape
and image datasets suffer from the lack of skeleton GT and inconsistency of GT
standards. As a result, it is difficult to evaluate and reproduce CNN-based
skeleton detectors and algorithms on a fair basis. In this paper, we present a
heuristic strategy for object skeleton GT extraction in binary shapes and
natural images. Our strategy is built on an extended theory of diagnosticity
hypothesis, which enables encoding human-in-the-loop GT extraction based on
clues from the target's context, simplicity, and completeness. Using this
strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing
shape and image datasets. The GTs are then structurally evaluated with
representative methods to build viable baselines for fair comparisons.
Experiments demonstrate that GTs generated by our strategy yield promising
quality with respect to standard consistency, and also provide a balance
between simplicity and completeness.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06440" title="Abstract">arXiv:2310.06440</a> [<a href="/pdf/2310.06440" title="Download PDF">pdf</a>, <a href="/format/2310.06440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic  Reasoning Task 2023
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiangyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shengdong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yifeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingguo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianfeng Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we present our solution to a Multi-modal Algorithmic Reasoning
Task: SMART-101 Challenge. Different from the traditional visual
question-answering datasets, this challenge evaluates the abstraction,
deduction, and generalization abilities of neural networks in solving
visuolinguistic puzzles designed specifically for children in the 6-8 age
group. We employed a divide-and-conquer approach. At the data level, inspired
by the challenge paper, we categorized the whole questions into eight types and
utilized the llama-2-chat model to directly generate the type for each question
in a zero-shot manner. Additionally, we trained a yolov7 model on the icon45
dataset for object detection and combined it with the OCR method to recognize
and locate objects and text within the images. At the model level, we utilized
the BLIP-2 model and added eight adapters to the image encoder VIT-G to
adaptively extract visual features for different question types. We fed the
pre-constructed question templates as input and generated answers using the
flan-t5-xxl decoder. Under the puzzle splits configuration, we achieved an
accuracy score of 26.5 on the validation set and 24.30 on the private test set.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06441" title="Abstract">arXiv:2310.06441</a> [<a href="/pdf/2310.06441" title="Download PDF">pdf</a>, <a href="/ps/2310.06441" title="Download PostScript">ps</a>, <a href="/format/2310.06441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stepwise functional refoundation of relational concept analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Euzenat%2C+J">J&#xe9;r&#xf4;me Euzenat</a> (MOEX)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> euzenat2023a
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Relational concept analysis (RCA) is an extension of formal concept analysis
allowing to deal with several related contexts simultaneously. It has been
designed for learning description logic theories from data and used within
various applications. A puzzling observation about RCA is that it returns a
single family of concept lattices although, when the data feature circular
dependencies, other solutions may be considered acceptable. The semantics of
RCA, provided in an operational way, does not shed light on this issue. In this
report, we define these acceptable solutions as those families of concept
lattices which belong to the space determined by the initial contexts
(well-formed), cannot scale new attributes (saturated), and refer only to
concepts of the family (self-supported). We adopt a functional view on the RCA
process by defining the space of well-formed solutions and two functions on
that space: one expansive and the other contractive. We show that the
acceptable solutions are the common fixed points of both functions. This is
achieved step-by-step by starting from a minimal version of RCA that considers
only one single context defined on a space of contexts and a space of lattices.
These spaces are then joined into a single space of context-lattice pairs,
which is further extended to a space of indexed families of context-lattice
pairs representing the objects manip
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06444" title="Abstract">arXiv:2310.06444</a> [<a href="/pdf/2310.06444" title="Download PDF">pdf</a>, <a href="/format/2310.06444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-dominant User Interest Network for Large-Scale Search Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuanping Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haitao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jingyou Hou</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+B">Bingqing Ke</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+j">junlin He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+E">Enyun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wenwu">Wenwu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Historical behaviors have shown great effect and potential in various
prediction tasks, including recommendation and information retrieval. The
overall historical behaviors are various but noisy while search behaviors are
always sparse. Most existing approaches in personalized search ranking adopt
the sparse search behaviors to learn representation with bottleneck, which do
not sufficiently exploit the crucial long-term interest. In fact, there is no
doubt that user long-term interest is various but noisy for instant search, and
how to exploit it well still remains an open problem.
<br />To tackle this problem, in this work, we propose a novel model named
Query-dominant user Interest Network (QIN), including two cascade units to
filter the raw user behaviors and reweigh the behavior subsequences.
Specifically, we propose a relevance search unit (RSU), which aims to search a
subsequence relevant to the query first and then search the sub-subsequences
relevant to the target item. These items are then fed into an attention unit
called Fused Attention Unit (FAU). It should be able to calculate attention
scores from the ID field and attribute field separately, and then adaptively
fuse the item embedding and content embedding based on the user engagement of
past period. Extensive experiments and ablation studies on real-world datasets
demonstrate the superiority of our model over state-of-the-art methods. The QIN
now has been successfully deployed on Kuaishou search, an online video search
platform, and obtained 7.6% improvement on CTR.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06445" title="Abstract">arXiv:2310.06445</a> [<a href="/pdf/2310.06445" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The DeMaDs Open Source Modeling Framework for Power System Malfunction  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fellner%2C+D">David Fellner</a>, 
<a href="/search/eess?searchtype=author&query=Strasser%2C+T+I">Thomas I Strasser</a>, 
<a href="/search/eess?searchtype=author&query=Kastner%2C+W">Wolfgang Kastner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 Open Source Modelling and Simulation of Energy Systems (OSMSES)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Modeling and simulation of electrical power systems are becoming increasingly
important approaches for the development and operation of novel smart grid
functionalities -- especially with regard to data-driven applications as data
of certain operational states or misconfigurations can be next to impossible to
obtain. The DeMaDs framework allows for the simulation and modeling of electric
power grids and malfunctions therein. Furthermore, it serves as a testbed to
assess the applicability of various data-driven malfunction detection methods.
These include data mining techniques, traditional machine learning approaches
as well as deep learning methods. The framework's capabilities and
functionality are laid out here, as well as explained by the means of an
illustrative example.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06446" title="Abstract">arXiv:2310.06446</a> [<a href="/pdf/2310.06446" title="Download PDF">pdf</a>, <a href="/format/2310.06446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule Mining for Correcting Classification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+H">Hirofumi Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Iwashita%2C+H">Hiroaki Iwashita</a>, 
<a href="/search/cs?searchtype=author&query=Takagi%2C+T">Takuya Takagi</a>, 
<a href="/search/cs?searchtype=author&query=Fujishige%2C+Y">Yuta Fujishige</a>, 
<a href="/search/cs?searchtype=author&query=Hara%2C+S">Satoshi Hara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning models need to be continually updated or corrected to ensure
that the prediction accuracy remains consistently high. In this study, we
consider scenarios where developers should be careful to change the prediction
results by the model correction, such as when the model is part of a complex
system or software. In such scenarios, the developers want to control the
specification of the corrections. To achieve this, the developers need to
understand which subpopulations of the inputs get inaccurate predictions by the
model. Therefore, we propose correction rule mining to acquire a comprehensive
list of rules that describe inaccurate subpopulations and how to correct them.
We also develop an efficient correction rule mining algorithm that is a
combination of frequent itemset mining and a unique pruning technique for
correction rules. We observed that the proposed algorithm found various rules
which help to collect data insufficiently learned, directly correct model
outputs, and analyze concept drift.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06448" title="Abstract">arXiv:2310.06448</a> [<a href="/pdf/2310.06448" title="Download PDF">pdf</a>, <a href="/format/2310.06448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Federated Learning with Incentive Mechanism Based on  Contract Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Danni Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yun Ji</a>, 
<a href="/search/cs?searchtype=author&query=Kou%2C+Z">Zhoubin Kou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xiaoxiong Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">To address the challenges posed by the heterogeneity inherent in federated
learning (FL) and to attract high-quality clients, various incentive mechanisms
have been employed. However, existing incentive mechanisms are typically
utilized in conventional synchronous aggregation, resulting in significant
straggler issues. In this study, we propose a novel asynchronous FL framework
that integrates an incentive mechanism based on contract theory. Within the
incentive mechanism, we strive to maximize the utility of the task publisher by
adaptively adjusting clients' local model training epochs, taking into account
factors such as time delay and test accuracy. In the asynchronous scheme,
considering client quality, we devise aggregation weights and an access control
algorithm to facilitate asynchronous aggregation. Through experiments conducted
on the MNIST dataset, the simulation results demonstrate that the test accuracy
achieved by our framework is 3.12% and 5.84% higher than that achieved by
FedAvg and FedProx without any attacks, respectively. The framework exhibits a
1.35% accuracy improvement over the ideal Local SGD under attacks. Furthermore,
aiming for the same target accuracy, our framework demands notably less
computation time than both FedAvg and FedProx.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06450" title="Abstract">arXiv:2310.06450</a> [<a href="/pdf/2310.06450" title="Download PDF">pdf</a>, <a href="/format/2310.06450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructive Large Language Models Alignment with Diverse Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianshu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Ting-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuchuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent research on large language models (LLMs), there has been a growing
emphasis on aligning these models with human values to reduce the impact of
harmful content. However, current alignment methods often rely solely on
singular forms of human feedback, such as preferences, annotated labels, or
natural language critiques, overlooking the potential advantages of combining
these feedback types. This limitation leads to suboptimal performance, even
when ample training data is available. In this paper, we introduce Constructive
and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired
by constructivist learning theory. Our approach involves collecting three
distinct types of feedback tailored to problems of varying difficulty levels
within the training dataset. Specifically, we exploit critique feedback for
easy problems, refinement feedback for medium problems, and preference feedback
for hard problems. By training our model with this diversified feedback, we
achieve enhanced alignment performance while using less training data. To
assess the effectiveness of CDF, we evaluate it against previous methods in
three downstream tasks: question answering, dialog generation, and text
summarization. Experimental results demonstrate that CDF achieves superior
performance even with a smaller training dataset.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06451" title="Abstract">arXiv:2310.06451</a> [<a href="/pdf/2310.06451" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Systems Test Case Discovery Enabled by Test Case Profile and  Repository
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raussi%2C+P">Petra Raussi</a>, 
<a href="/search/cs?searchtype=author&query=Kamsamrong%2C+J">Jirapa Kamsamrong</a>, 
<a href="/search/cs?searchtype=author&query=Paspatis%2C+A">Alexandros Paspatis</a>, 
<a href="/search/cs?searchtype=author&query=Heussen%2C+K">Kai Heussen</a>, 
<a href="/search/cs?searchtype=author&query=Zerihun%2C+T+A">Tesfaye Amare Zerihun</a>, 
<a href="/search/cs?searchtype=author&query=Widl%2C+E">Edmund Widl</a>, 
<a href="/search/cs?searchtype=author&query=Andr%C3%A9n%2C+F+P">Filip Pr&#xf6;stl Andr&#xe9;n</a>, 
<a href="/search/cs?searchtype=author&query=Kazmi%2C+J+H">Jawad H Kazmi</a>, 
<a href="/search/cs?searchtype=author&query=Strasser%2C+T+I">Thomas I. Strasser</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+F">Felipe Castro</a>, 
<a href="/search/cs?searchtype=author&query=Pellegrino%2C+L">Luigi Pellegrino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 Open Source Modelling and Simulation of Energy Systems (OSMSES)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Smart energy systems comprise multiple domains like power, thermal, control,
information, and communication technology, which increases the complexity of
research and development studies. This expansion also requires larger and ever
so complex experimental pilot environments driving the demand for
geographically distributed multi-research infrastructure tests. The Holistic
Test Description approach supports the design of multi-domain and
multi-research infrastructure tests by organizing the test cases into
comprehensive segments, ensuring all relevant items for testing are covered.
These test cases eventually form a pool, which to understand holistically would
require studying and reading all the descriptions. This work proposes therefore
the concept of Test Case Profiles to improve test case discovery and the
structured creation of them. Test Case Profiles add further structure to the
indexing in test case repositories. Along with the proposed indexing method,
four different use cases are introduced to motivate additional applications of
the proposed concept.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06452" title="Abstract">arXiv:2310.06452</a> [<a href="/pdf/2310.06452" title="Download PDF">pdf</a>, <a href="/format/2310.06452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Effects of RLHF on LLM Generalisation and Diversity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirk%2C+R">Robert Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Mediratta%2C+I">Ishita Mediratta</a>, 
<a href="/search/cs?searchtype=author&query=Nalmpantis%2C+C">Christoforos Nalmpantis</a>, 
<a href="/search/cs?searchtype=author&query=Luketina%2C+J">Jelena Luketina</a>, 
<a href="/search/cs?searchtype=author&query=Hambro%2C+E">Eric Hambro</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Raileanu%2C+R">Roberta Raileanu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model's ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the trade-off between generalisation
and diversity.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06457" title="Abstract">arXiv:2310.06457</a> [<a href="/pdf/2310.06457" title="Download PDF">pdf</a>, <a href="/format/2310.06457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small-Signal Stability and SCR Enhancement of Offshore WPPs with  Synchronous Condensers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghimire%2C+S">Sulav Ghimire</a>, 
<a href="/search/eess?searchtype=author&query=Kkuni%2C+K+V">Kanakesh V. Kkuni</a>, 
<a href="/search/eess?searchtype=author&query=Guest%2C+E+D">Emerson D. Guest</a>, 
<a href="/search/eess?searchtype=author&query=Jensen%2C+K+H">Kim H. Jensen</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guangya Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Synchronous condensers (SCs) have been reported to improve the overall
stability and short-circuit power of a power system. SCs are also being
integrated into offshore wind power plants (WPPs) for the same reason. This
paper, investigates the effect of synchronous condensers on an offshore wind
power plant with grid-following (GFL) and grid-forming (GFM) converter
controls. Primarily, the effect of synchronous condensers can be two-fold: (1)
overall stability enhancement of the WPP by providing reactive power support,
(2) contribution to the effective short circuit ratio (SCR) of the WPP by fault
current support. Therefore, this paper focuses on studies concerning these
effects on an aggregated model of a WPP connected to the grid. To that end, a
state-space model of the test system is developed for small-signal stability
assessment and the synchronous condenser's effect on its stability. In
addition, a mathematical explanation of SCR enhancement with synchronous
condenser is provided and is verified with time-domain electromagnetic
transient simulations.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06458" title="Abstract">arXiv:2310.06458</a> [<a href="/pdf/2310.06458" title="Download PDF">pdf</a>, <a href="/format/2310.06458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cultural Compass: Predicting Transfer Learning Success in Offensive  Language Detection with Cultural Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Karamolegkou%2C+A">Antonia Karamolegkou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hershcovich%2C+D">Daniel Hershcovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The increasing ubiquity of language technology necessitates a shift towards
considering cultural diversity in the machine learning realm, particularly for
subjective tasks that rely heavily on cultural nuances, such as Offensive
Language Detection (OLD). Current understanding underscores that these tasks
are substantially influenced by cultural values, however, a notable gap exists
in determining if cultural features can accurately predict the success of
cross-cultural transfer learning for such subjective tasks. Addressing this,
our study delves into the intersection of cultural features and transfer
learning effectiveness. The findings reveal that cultural value surveys indeed
possess a predictive power for cross-cultural transfer learning success in OLD
tasks and that it can be further improved using offensive word distance. Based
on these results, we advocate for the integration of cultural information into
datasets. Additionally, we recommend leveraging data sources rich in cultural
information, such as surveys, to enhance cultural adaptability. Our research
signifies a step forward in the quest for more inclusive, culturally sensitive
language technologies.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06468" title="Abstract">arXiv:2310.06468</a> [<a href="/pdf/2310.06468" title="Download PDF">pdf</a>, <a href="/format/2310.06468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Geometrical Approach to Evaluate the Adversarial Robustness of Deep  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>, 
<a href="/search/cs?searchtype=author&query=Piao%2C+H">Haiyin Piao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yufei Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baocai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM Transactions on Multimedia Computing, Communications, and Applications (ACM TOMM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep Neural Networks (DNNs) are widely used for computer vision tasks.
However, it has been shown that deep models are vulnerable to adversarial
attacks, i.e., their performances drop when imperceptible perturbations are
made to the original inputs, which may further degrade the following visual
tasks or introduce new problems such as data and privacy security. Hence,
metrics for evaluating the robustness of deep models against adversarial
attacks are desired. However, previous metrics are mainly proposed for
evaluating the adversarial robustness of shallow networks on the small-scale
datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness
(CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet
dataset), it is computationally expensive and its performance relies on a
tractable number of samples. In this paper, we propose the Adversarial
Converging Time Score (ACTS), an attack-dependent metric that quantifies the
adversarial robustness of a DNN on a specific input. Our key observation is
that local neighborhoods on a DNN's output surface would have different shapes
given different inputs. Hence, given different inputs, it requires different
time for converging to an adversarial sample. Based on this geometry meaning,
ACTS measures the converging time as an adversarial robustness metric. We
validate the effectiveness and generalization of the proposed ACTS metric
against different adversarial attacks on the large-scale ImageNet dataset using
state-of-the-art deep networks. Extensive experiments show that our ACTS metric
is an efficient and effective adversarial metric over the previous CLEVER
metric.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06469" title="Abstract">arXiv:2310.06469</a> [<a href="/pdf/2310.06469" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Circulating Current Induced Electromagnetic Torque Generation in  Electric Machines with Delta Windings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pramod%2C+P">Prerit Pramod</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper explains the phenomenon of current circulation and the resulting
electromagnetic torque generation in electric machines employing delta
windings. The description entails a systematic assessment of the electrical and
magnetic behavior of the machine to develop mathematical models, followed by
intuitive explanations of the derived analytical forms. The modeling is
thoroughly validated through simulation and experimental results on a prototype
machine.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06470" title="Abstract">arXiv:2310.06470</a> [<a href="/pdf/2310.06470" title="Download PDF">pdf</a>, <a href="/format/2310.06470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Focus on Local Regions for Query-based Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yamei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bo Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Query-based methods have garnered significant attention in object detection
since the advent of DETR, the pioneering end-to-end query-based detector.
However, these methods face challenges like slow convergence and suboptimal
performance. Notably, self-attention in object detection often hampers
convergence due to its global focus. To address these issues, we propose FoLR,
a transformer-like architecture with only decoders. We enhance the
self-attention mechanism by isolating connections between irrelevant objects
that makes it focus on local regions but not global regions. We also design the
adaptive sampling method to extract effective features based on queries' local
regions from feature maps. Additionally, we employ a look-back strategy for
decoders to retain prior information, followed by the Feature Mixer module to
fuse features and queries. Experimental results demonstrate FoLR's
state-of-the-art performance in query-based detectors, excelling in convergence
speed and computational efficiency.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06474" title="Abstract">arXiv:2310.06474</a> [<a href="/pdf/2310.06474" title="Download PDF">pdf</a>, <a href="/format/2310.06474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Jailbreak Challenges in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yue Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S+J">Sinno Jialin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LLMs) exhibit remarkable capabilities across a
wide range of tasks, they pose potential safety concerns, such as the
``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to
exhibit undesirable behavior. Although several preventive measures have been
developed to mitigate the potential risks associated with LLMs, they have
primarily focused on English data. In this study, we reveal the presence of
multilingual jailbreak challenges within LLMs and consider two potential risk
scenarios: unintentional and intentional. The unintentional scenario involves
users querying LLMs using non-English prompts and inadvertently bypassing the
safety mechanisms, while the intentional scenario concerns malicious users
combining malicious instructions with multilingual prompts to deliberately
attack LLMs. The experimental results reveal that in the unintentional
scenario, the rate of unsafe content increases as the availability of languages
decreases. Specifically, low-resource languages exhibit three times the
likelihood of encountering harmful content compared to high-resource languages,
with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts
can exacerbate the negative impact of malicious instructions, with
astonishingly high rates of unsafe output: 80.92\% for ChatGPT and 40.71\% for
GPT-4. To handle such a challenge in the multilingual context, we propose a
novel \textsc{Self-Defense} framework that automatically generates multilingual
training data for safety fine-tuning. Experimental results show that ChatGPT
fine-tuned with such data can achieve a substantial reduction in unsafe content
generation. Data is available at
https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This
paper contains examples with potentially harmful content.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06475" title="Abstract">arXiv:2310.06475</a> [<a href="/pdf/2310.06475" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approaches to the Algorithmic Allocation of Public Resources: A  Cross-disciplinary Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esnaashari%2C+S">Saba Esnaashari</a>, 
<a href="/search/cs?searchtype=author&query=Bright%2C+J">Jonathan Bright</a>, 
<a href="/search/cs?searchtype=author&query=Francis%2C+J">John Francis</a>, 
<a href="/search/cs?searchtype=author&query=Hashem%2C+Y">Youmna Hashem</a>, 
<a href="/search/cs?searchtype=author&query=Straub%2C+V">Vincent Straub</a>, 
<a href="/search/cs?searchtype=author&query=Morgan%2C+D">Deborah Morgan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Allocation of scarce resources is a recurring challenge for the public
sector: something that emerges in areas as diverse as healthcare, disaster
recovery, and social welfare. The complexity of these policy domains and the
need for meeting multiple and sometimes conflicting criteria has led to
increased focus on the use of algorithms in this type of decision. However,
little engagement between researchers across these domains has happened,
meaning a lack of understanding of common problems and techniques for
approaching them. Here, we performed a cross disciplinary literature review to
understand approaches taken for different areas of algorithmic allocation
including healthcare, organ transplantation, homelessness, disaster relief, and
welfare. We initially identified 1070 papers by searching the literature, then
six researchers went through them in two phases of screening resulting in 176
and 75 relevant papers respectively. We then analyzed the 75 papers from the
lenses of optimization goals, techniques, interpretability, flexibility, bias,
ethical considerations, and performance. We categorized approaches into
human-oriented versus resource-oriented perspective, and individual versus
aggregate and identified that 76% of the papers approached the problem from a
human perspective and 60% from an aggregate level using optimization
techniques. We found considerable potential for performance gains, with
optimization techniques often decreasing waiting times and increasing success
rate by as much as 50%. However, there was a lack of attention to responsible
innovation: only around one third of the papers considered ethical issues in
choosing the optimization goals while just a very few of them paid attention to
the bias issues. Our work can serve as a guide for policy makers and
researchers wanting to use an algorithm for addressing a resource allocation
problem.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06479" title="Abstract">arXiv:2310.06479</a> [<a href="/pdf/2310.06479" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Photovoltaic grid-forming control strategy investigation using  hardware-in-the-loop experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bhattacharya%2C+S">Somesh Bhattacharya</a>, 
<a href="/search/eess?searchtype=author&query=Charalambous%2C+C">Chrysanthos Charalambous</a>, 
<a href="/search/eess?searchtype=author&query=Banjac%2C+A">Anja Banjac</a>, 
<a href="/search/eess?searchtype=author&query=Miletic%2C+Z">Zoran Miletic</a>, 
<a href="/search/eess?searchtype=author&query=Strasser%2C+T">Thomas Strasser</a>, 
<a href="/search/eess?searchtype=author&query=Azzopardi%2C+B">Brian Azzopardi</a>, 
<a href="/search/eess?searchtype=author&query=Papadimitriou%2C+C">Christina Papadimitriou</a>, 
<a href="/search/eess?searchtype=author&query=Efthymiou%2C+V">Venizelos Efthymiou</a>, 
<a href="/search/eess?searchtype=author&query=Polycarpou%2C+A">Alexis Polycarpou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13th Mediterranean Conference on Power Generation, Transmission, Distribution and Energy Conversion (MEDPOWER 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The frequency stability of a power system is of paramount importance, as a
fast frequency swings in the system can lead to oscillatory instability, and
thereby blackouts. A grid-connected microgrid, that can operate in the islanded
mode can also possess such deteriorating effect due to the higher share of
converter-based sources. In this paper, a coordinated frequency control within
a distribution network is discussed, with a higher share of Photovoltaics (PV).
The main objective of this paper is to test the grid-forming capabilities of
PVs, without the requirement of an energy storage in the network. The tests
were carried out with the help of the Typhoon Hardware-in-the-loop (HIL)
platform using a real Cypriot network feeder. The real-time results confirm the
efficacy of the PV as a grid-forming inverter, provided it has sufficient input
(irradiance) to provide for the loads within the system of interest. The
grid-forming PV also possesses the capability of reconnection with the utility
grid through a synchronizer switch that requires minimal communication, makes
the overall control independent of any other power source, subject to certain
irradiance and loading conditions.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06481" title="Abstract">arXiv:2310.06481</a> [<a href="/pdf/2310.06481" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An improved CTGAN for data processing method of imbalanced disk failure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jingbo Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dawood%2C+H">Hussain Dawood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">To address the problem of insufficient failure data generated by disks and
the imbalance between the number of normal and failure data. The existing
Conditional Tabular Generative Adversarial Networks (CTGAN) deep learning
methods have been proven to be effective in solving imbalance disk failure
data. But CTGAN cannot learn the internal information of disk failure data very
well. In this paper, a fault diagnosis method based on improved CTGAN, a
classifier for specific category discrimination is added and a discriminator
generate adversarial network based on residual network is proposed. We named it
Residual Conditional Tabular Generative Adversarial Networks (RCTGAN). Firstly,
to enhance the stability of system a residual network is utilized. RCTGAN uses
a small amount of real failure data to synthesize fake fault data; Then, the
synthesized data is mixed with the real data to balance the amount of normal
and failure data; Finally, four classifier (multilayer perceptron, support
vector machine, decision tree, random forest) models are trained using the
balanced data set, and the performance of the models is evaluated using G-mean.
The experimental results show that the data synthesized by the RCTGAN can
further improve the fault diagnosis accuracy of the classifier.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06483" title="Abstract">arXiv:2310.06483</a> [<a href="/pdf/2310.06483" title="Download PDF">pdf</a>, <a href="/format/2310.06483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance Reduced Online Gradient Descent for Kernelized Pairwise  Learning with Limited Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlQuabeh%2C+H">Hilal AlQuabeh</a>, 
<a href="/search/cs?searchtype=author&query=Mukhoty%2C+B">Bhaskar Mukhoty</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+B">Bin Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ACML2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pairwise learning is essential in machine learning, especially for problems
involving loss functions defined on pairs of training examples. Online gradient
descent (OGD) algorithms have been proposed to handle online pairwise learning,
where data arrives sequentially. However, the pairwise nature of the problem
makes scalability challenging, as the gradient computation for a new sample
involves all past samples. Recent advancements in OGD algorithms have aimed to
reduce the complexity of calculating online gradients, achieving complexities
less than $O(T)$ and even as low as $O(1)$. However, these approaches are
primarily limited to linear models and have induced variance. In this study, we
propose a limited memory OGD algorithm that extends to kernel online pairwise
learning while improving the sublinear regret. Specifically, we establish a
clear connection between the variance of online gradients and the regret, and
construct online gradients using the most recent stratified samples with a
limited buffer of size of $s$ representing all past data, which have a
complexity of $O(sT)$ and employs $O(\sqrt{T}\log{T})$ random Fourier features
for kernel approximation. Importantly, our theoretical results demonstrate that
the variance-reduced online gradients lead to an improved sublinear regret
bound. The experiments on real-world datasets demonstrate the superiority of
our algorithm over both kernelized and linear online pairwise learning
algorithms.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06484" title="Abstract">arXiv:2310.06484</a> [<a href="/pdf/2310.06484" title="Download PDF">pdf</a>, <a href="/format/2310.06484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory efficient location recommendation through proximity-aware  representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xuan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+R">Rui Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hui Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Sequential location recommendation plays a huge role in modern life, which
can enhance user experience, bring more profit to businesses and assist in
government administration. Although methods for location recommendation have
evolved significantly thanks to the development of recommendation systems,
there is still limited utilization of geographic information, along with the
ongoing challenge of addressing data sparsity. In response, we introduce a
Proximity-aware based region representation for Sequential Recommendation (PASR
for short), built upon the Self-Attention Network architecture. We tackle the
sparsity issue through a novel loss function employing importance sampling,
which emphasizes informative negative samples during optimization. Moreover,
PASR enhances the integration of geographic information by employing a
self-attention-based geography encoder to the hierarchical grid and proximity
grid at each GPS point. To further leverage geographic information, we utilize
the proximity-aware negative samplers to enhance the quality of negative
samples. We conducted evaluations using three real-world Location-Based Social
Networking (LBSN) datasets, demonstrating that PASR surpasses state-of-the-art
sequential location recommendation methods
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06486" title="Abstract">arXiv:2310.06486</a> [<a href="/pdf/2310.06486" title="Download PDF">pdf</a>, <a href="/format/2310.06486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological RANSAC for instance verification and retrieval without  fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+G">Guoyuan An</a>, 
<a href="/search/cs?searchtype=author&query=Seon%2C+J">Juhyung Seon</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+I">Inkyu An</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yuchi Huo</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sung-Eui Yoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
<p class="mathjax">This paper presents an innovative approach to enhancing explainable image
retrieval, particularly in situations where a fine-tuning set is unavailable.
The widely-used SPatial verification (SP) method, despite its efficacy, relies
on a spatial model and the hypothesis-testing strategy for instance
recognition, leading to inherent limitations, including the assumption of
planar structures and neglect of topological relations among features. To
address these shortcomings, we introduce a pioneering technique that replaces
the spatial model with a topological one within the RANSAC process. We propose
bio-inspired saccade and fovea functions to verify the topological consistency
among features, effectively circumventing the issues associated with SP's
spatial model. Our experimental results demonstrate that our method
significantly outperforms SP, achieving state-of-the-art performance in
non-fine-tuning retrieval. Furthermore, our approach can enhance performance
when used in conjunction with fine-tuned features. Importantly, our method
retains high explainability and is lightweight, offering a practical and
adaptable solution for a variety of real-world applications.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06488" title="Abstract">arXiv:2310.06488</a> [<a href="/pdf/2310.06488" title="Download PDF">pdf</a>, <a href="/format/2310.06488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changze Lv</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianhan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Muling Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Spiking neural networks (SNNs) have demonstrated the capability to achieve
comparable performance to deep neural networks (DNNs) in both visual and
linguistic domains while offering the advantages of improved energy efficiency
and adherence to biological plausibility. However, the extension of such
single-modality SNNs into the realm of multimodal scenarios remains an
unexplored territory. Drawing inspiration from the concept of contrastive
language-image pre-training (CLIP), we introduce a novel framework, named
SpikeCLIP, to address the gap between two modalities within the context of
spike-based computing through a two-step recipe involving ``Alignment
Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that
SNNs achieve comparable results to their DNN counterparts while significantly
reducing energy consumption across a variety of datasets commonly used for
multimodal model evaluation. Furthermore, SpikeCLIP maintains robust
performance in image classification tasks that involve class labels not
predefined within specific categories.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06489" title="Abstract">arXiv:2310.06489</a> [<a href="/pdf/2310.06489" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Automatic Detection and Facial Recognition in Japanese  Macaques: Illuminating Social Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paulet%2C+J">Julien Paulet</a> (UJM), 
<a href="/search/cs?searchtype=author&query=Molina%2C+A">Axel Molina</a> (ENS-PSL), 
<a href="/search/cs?searchtype=author&query=Beltzung%2C+B">Benjamin Beltzung</a> (IPHC), 
<a href="/search/cs?searchtype=author&query=Suzumura%2C+T">Takafumi Suzumura</a>, 
<a href="/search/cs?searchtype=author&query=Yamamoto%2C+S">Shinya Yamamoto</a>, 
<a href="/search/cs?searchtype=author&query=Sueur%2C+C">C&#xe9;dric Sueur</a> (IPHC, IUF, ANTHROPO LAB)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Individual identification plays a pivotal role in ecology and ethology,
notably as a tool for complex social structures understanding. However,
traditional identification methods often involve invasive physical tags and can
prove both disruptive for animals and time-intensive for researchers. In recent
years, the integration of deep learning in research offered new methodological
perspectives through automatization of complex tasks. Harnessing object
detection and recognition technologies is increasingly used by researchers to
achieve identification on video footage. This study represents a preliminary
exploration into the development of a non-invasive tool for face detection and
individual identification of Japanese macaques (Macaca fuscata) through deep
learning. The ultimate goal of this research is, using identifications done on
the dataset, to automatically generate a social network representation of the
studied population. The current main results are promising: (i) the creation of
a Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2%
accuracy and (ii) the creation of an individual recognizer for K{\=o}jima
island macaques population (YOLOv8n model), reaching a 83% accuracy. We also
created a K{\=o}jima population social network by traditional methods, based on
co-occurrences on videos. Thus, we provide a benchmark against which the
automatically generated network will be assessed for reliability. These
preliminary results are a testament to the potential of this innovative
approach to provide the scientific community with a tool for tracking
individuals and social network studies in Japanese macaques.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06491" title="Abstract">arXiv:2310.06491</a> [<a href="/pdf/2310.06491" title="Download PDF">pdf</a>, <a href="/format/2310.06491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-facet Paradigm to Bridge Large Language Model and Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xinyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have garnered considerable attention in
recommender systems. To achieve LLM-based recommendation, item indexing and
generation grounding are two essential steps, bridging between recommendation
items and natural language. Item indexing assigns a unique identifier to
represent each item in natural language, and generation grounding grounds the
generated token sequences to in-corpus items. However, previous works suffer
from inherent limitations in the two steps. For item indexing, existing
ID-based identifiers (e.g., numeric IDs) and description-based identifiers
(e.g., titles) often compromise semantic richness or uniqueness. Moreover,
generation grounding might inadvertently produce out-of-corpus identifiers.
Worse still, autoregressive generation heavily relies on the initial token's
quality. To combat these issues, we propose a novel multi-facet paradigm,
namely TransRec, to bridge the LLMs to recommendation. Specifically, TransRec
employs multi-facet identifiers that incorporate ID, title, and attribute,
achieving both distinctiveness and semantics. Additionally, we introduce a
specialized data structure for TransRec to guarantee the in-corpus identifier
generation and adopt substring indexing to encourage LLMs to generate from any
position. We implement TransRec on two backbone LLMs, i.e., BART-large and
LLaMA-7B. Empirical results on three real-world datasets under diverse settings
(e.g., full training and few-shot training with warm- and cold-start testings)
attest to the superiority of TransRec.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06498" title="Abstract">arXiv:2310.06498</a> [<a href="/pdf/2310.06498" title="Download PDF">pdf</a>, <a href="/format/2310.06498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Benchmark and Reverse Validation Method for Passage-level  Hallucination Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shiping Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Renliang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023;Camera-ready version will be updated soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated their ability to collaborate
effectively with humans in real-world scenarios. However, LLMs are apt to
generate hallucinations, i.e., makeup incorrect text and unverified
information, which can cause significant damage when deployed for
mission-critical tasks. In this paper, we propose a self-check approach based
on reverse validation to detect factual errors automatically in a zero-resource
fashion. To facilitate future studies and assess different methods, we
construct a hallucination detection benchmark, which is generated by ChatGPT
and annotated by human annotators. Contrasting previous studies of
zero-resource hallucination detection, our method and benchmark concentrate on
passage-level detection instead of sentence-level. We empirically evaluate our
method and existing zero-resource detection methods on different domains of
benchmark to explore the implicit relation between hallucination and training
data. Furthermore, we manually analyze some hallucination cases that LLM failed
to capture, revealing the shared limitation of zero-resource methods.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06500" title="Abstract">arXiv:2310.06500</a> [<a href="/pdf/2310.06500" title="Download PDF">pdf</a>, <a href="/format/2310.06500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaAgents: Simulating Interactions of Human Behaviors for LLM-based  Task-oriented Coordination via Collaborative Generative Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Significant advancements have occurred in the application of Large Language
Models (LLMs) for various tasks and social simulations. Despite this, their
capacities to coordinate within task-oriented social contexts are
under-explored. Such capabilities are crucial if LLMs are to effectively mimic
human-like social behavior and produce meaningful results. To bridge this gap,
we introduce collaborative generative agents, endowing LLM-based Agents with
consistent behavior patterns and task-solving abilities. We situate these
agents in a simulated job fair environment as a case study to scrutinize their
coordination skills. We propose a novel framework that equips collaborative
generative agents with human-like reasoning abilities and specialized skills.
Our evaluation demonstrates that these agents show promising performance.
However, we also uncover limitations that hinder their effectiveness in more
complex coordination tasks. Our work provides valuable insights into the role
and evolution of LLMs in task-oriented social simulations.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06502" title="Abstract">arXiv:2310.06502</a> [<a href="/pdf/2310.06502" title="Download PDF">pdf</a>, <a href="/format/2310.06502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment  Quadruples: A Comparative Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiancai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jia-Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+R">Rongchang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+L">Lei Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, ChatGPT has attracted great attention from both industry and
academia due to its surprising abilities in natural language understanding and
generation. We are particularly curious about whether it can achieve promising
performance on one of the most complex tasks in aspect-based sentiment
analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from
texts. To this end, in this paper we develop a specialized prompt template that
enables ChatGPT to effectively tackle this complex quadruple extraction task.
Further, we propose a selection method on few-shot examples to fully exploit
the in-context learning ability of ChatGPT and uplift its effectiveness on this
complex task. Finally, we provide a comparative evaluation on ChatGPT against
existing state-of-the-art quadruple extraction models based on four public
datasets and highlight some important findings regarding the capability
boundaries of ChatGPT in the quadruple extraction.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06504" title="Abstract">arXiv:2310.06504</a> [<a href="/pdf/2310.06504" title="Download PDF">pdf</a>, <a href="/format/2310.06504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisit Input Perturbation Problems for LLMs: A Unified Robustness  Evaluation Framework for Noisy Slot Filling Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinxu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+T">Tingfeng Hui</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daichi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+W">Wenlong Wan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+B">Boqi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yueyan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Gongque%2C+Z">Zhuoma Gongque</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zechen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NLPCC 2023 (Oral Presentation)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the increasing capabilities of large language models (LLMs), these
high-performance models have achieved state-of-the-art results on a wide range
of natural language processing (NLP) tasks. However, the models' performance on
commonly-used benchmark datasets often fails to accurately reflect their
reliability and robustness when applied to real-world noisy data. To address
these challenges, we propose a unified robustness evaluation framework based on
the slot-filling task to systematically evaluate the dialogue understanding
capability of LLMs in diverse input perturbation scenarios. Specifically, we
construct a input perturbation evaluation dataset, Noise-LLM, which contains
five types of single perturbation and four types of mixed perturbation data.
Furthermore, we utilize a multi-level data augmentation method (character,
word, and sentence levels) to construct a candidate data pool, and carefully
design two ways of automatic task demonstration construction strategies
(instance-level and entity-level) with various prompt templates. Our aim is to
assess how well various robustness methods of LLMs perform in real-world noisy
scenarios. The experiments have demonstrated that the current open-source LLMs
generally achieve limited perturbation robustness performance. Based on these
experimental observations, we make some forward-looking suggestions to fuel the
research in this direction.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06505" title="Abstract">arXiv:2310.06505</a> [<a href="/pdf/2310.06505" title="Download PDF">pdf</a>, <a href="/format/2310.06505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of ChatGPT Feedback on ELL Writers&#x27; Coherence and Cohesion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Su-Youn Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Miszoglad%2C+E">Eva Miszoglad</a>, 
<a href="/search/cs?searchtype=author&query=Pierce%2C+L+R">Lisa R. Pierce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 1 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Since its launch in November 2022, ChatGPT has had a transformative effect on
education where students are using it to help with homework assignments and
teachers are actively employing it in their teaching practices. This includes
using ChatGPT as a tool for writing teachers to grade and generate feedback on
students' essays. In this study, we evaluated the quality of the feedback
generated by ChatGPT regarding the coherence and cohesion of the essays written
by English Language Learners (ELLs) students. We selected 50 argumentative
essays and generated feedback on coherence and cohesion using the ELLIPSE
rubric. During the feedback evaluation, we used a two-step approach: first,
each sentence in the feedback was classified into subtypes based on its
function (e.g., positive reinforcement, problem statement). Next, we evaluated
its accuracy and usability according to these types. Both the analysis of
feedback types and the evaluation of accuracy and usability revealed that most
feedback sentences were highly abstract and generic, failing to provide
concrete suggestions for improvement. The accuracy in detecting major problems,
such as repetitive ideas and the inaccurate use of cohesive devices, depended
on superficial linguistic features and was often incorrect. In conclusion,
ChatGPT, without specific training for the feedback generation task, does not
offer effective feedback on ELL students' coherence and cohesion.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06506" title="Abstract">arXiv:2310.06506</a> [<a href="/pdf/2310.06506" title="Download PDF">pdf</a>, <a href="/format/2310.06506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Runway Sign Classifier: A DAL C Certifiable Machine Learning System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dmitriev%2C+K">Konstantin Dmitriev</a>, 
<a href="/search/cs?searchtype=author&query=Schumann%2C+J">Johann Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Bostanov%2C+I">Islam Bostanov</a>, 
<a href="/search/cs?searchtype=author&query=Abdelhamid%2C+M">Mostafa Abdelhamid</a>, 
<a href="/search/cs?searchtype=author&query=Holzapfel%2C+F">Florian Holzapfel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years, the remarkable progress of Machine Learning (ML)
technologies within the domain of Artificial Intelligence (AI) systems has
presented unprecedented opportunities for the aviation industry, paving the way
for further advancements in automation, including the potential for single
pilot or fully autonomous operation of large commercial airplanes. However, ML
technology faces major incompatibilities with existing airborne certification
standards, such as ML model traceability and explainability issues or the
inadequacy of traditional coverage metrics. Certification of ML-based airborne
systems using current standards is problematic due to these challenges. This
paper presents a case study of an airborne system utilizing a Deep Neural
Network (DNN) for airport sign detection and classification. Building upon our
previous work, which demonstrates compliance with Design Assurance Level (DAL)
D, we upgrade the system to meet the more stringent requirements of Design
Assurance Level C. To achieve DAL C, we employ an established architectural
mitigation technique involving two redundant and dissimilar Deep Neural
Networks. The application of novel ML-specific data management techniques
further enhances this approach. This work is intended to illustrate how the
certification challenges of ML-based systems can be addressed for medium
criticality airborne applications.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06508" title="Abstract">arXiv:2310.06508</a> [<a href="/pdf/2310.06508" title="Download PDF">pdf</a>, <a href="/format/2310.06508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological data analysis of human vowels: Persistent homologies across  representation spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonafos%2C+G">Guillem Bonafos</a>, 
<a href="/search/cs?searchtype=author&query=Freyermuth%2C+J">Jean-Marc Freyermuth</a>, 
<a href="/search/cs?searchtype=author&query=Pudlo%2C+P">Pierre Pudlo</a>, 
<a href="/search/cs?searchtype=author&query=Tron%C3%A7on%2C+S">Samuel Tron&#xe7;on</a>, 
<a href="/search/cs?searchtype=author&query=Rey%2C+A">Arnaud Rey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Topological Data Analysis (TDA) has been successfully used for various tasks
in signal/image processing, from visualization to supervised/unsupervised
classification. Often, topological characteristics are obtained from persistent
homology theory. The standard TDA pipeline starts from the raw signal data or a
representation of it. Then, it consists in building a multiscale topological
structure on the top of the data using a pre-specified filtration, and finally
to compute the topological signature to be further exploited. The commonly used
topological signature is a persistent diagram (or transformations of it).
Current research discusses the consequences of the many ways to exploit
topological signatures, much less often the choice of the filtration, but to
the best of our knowledge, the choice of the representation of a signal has not
been the subject of any study yet. This paper attempts to provide some answers
on the latter problem. To this end, we collected real audio data and built a
comparative study to assess the quality of the discriminant information of the
topological signatures extracted from three different representation spaces.
Each audio signal is represented as i) an embedding of observed data in a
higher dimensional space using Taken's representation, ii) a spectrogram viewed
as a surface in a 3D ambient space, iii) the set of spectrogram's zeroes. From
vowel audio recordings, we use topological signature for three prediction
problems: speaker gender, vowel type, and individual. We show that
topologically-augmented random forest improves the Out-of-Bag Error (OOB) over
solely based Mel-Frequency Cepstral Coefficients (MFCC) for the last two
problems. Our results also suggest that the topological information extracted
from different signal representations is complementary, and that spectrogram's
zeros offers the best improvement for gender prediction.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06509" title="Abstract">arXiv:2310.06509</a> [<a href="/pdf/2310.06509" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language and Temporal Aspects: A Qualitative Study on Trigger  Interpretation in Trigger-Action Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrao%2C+M">Margherita Andrao</a>, 
<a href="/search/cs?searchtype=author&query=Treccani%2C+B">Barbara Treccani</a>, 
<a href="/search/cs?searchtype=author&query=Zancanaro%2C+M">Massimo Zancanaro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version did not undergo peer-review. A corrected version is published by Springer Nature in the Proceedings of 9th International Syposium on End-User Development (IS-EUD 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Spano, L.D., Schmidt, A., Santoro, C., Stumpf, S. (eds) End-User
  Development. IS-EUD 2023. Lecture Notes in Computer Science, vol 13917.
  Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper presents a qualitative study that investigates the effects of some
language choices in expressing the trigger part of a trigger-action rule on the
users' mental models. Specifically, we explored how 11 non-programmer
participants articulated the definition of trigger-action rules in different
contexts by choosing among alternative conjunctions, verbal structures, and
order of primitives. Our study shed some new light on how lexical choices
influence the users' mental models in End-User Development tasks. Specifically,
the conjunction "as soon as" clearly supports the idea of instantaneousness,
and the conjunction "while" the idea of protractedness of an event; the most
commonly used "if" and "when", instead, are prone to create ambiguity in the
mental representation of events. The order of rule elements helps participants
to construct accurate mental models. Usually, individuals are facilitated in
comprehension when the trigger is displayed at the beginning of the rule, even
though sometimes the reverse order (with the action first) is preferred as it
conveys the central element of the rule.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06511" title="Abstract">arXiv:2310.06511</a> [<a href="/pdf/2310.06511" title="Download PDF">pdf</a>, <a href="/format/2310.06511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Set Representation Learning for Unsupervised  Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D+B">Dong Bok Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seanie Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Joonho Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Dataset distillation methods have achieved remarkable success in distilling a
large dataset into a small set of representative samples. However, they are not
designed to produce a distilled dataset that can be effectively used for
facilitating self-supervised pre-training. To this end, we propose a novel
problem of distilling an unlabeled dataset into a set of small synthetic
samples for efficient self-supervised learning (SSL). We first prove that a
gradient of synthetic samples with respect to a SSL objective in naive bilevel
optimization is \textit{biased} due to the randomness originating from data
augmentations or masking. To address this issue, we propose to minimize the
mean squared error (MSE) between a model's representations of the synthetic
examples and their corresponding learnable target feature representations for
the inner objective, which does not introduce any randomness. Our primary
motivation is that the model obtained by the proposed inner optimization can
mimic the \textit{self-supervised target model}. To achieve this, we also
introduce the MSE between representations of the inner model and the
self-supervised target model on the original full dataset for outer
optimization. Lastly, assuming that a feature extractor is fixed, we only
optimize a linear head on top of the feature extractor, which allows us to
reduce the computational cost and obtain a closed-form solution of the head
with kernel ridge regression. We empirically validate the effectiveness of our
method on various applications involving transfer learning.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06513" title="Abstract">arXiv:2310.06513</a> [<a href="/pdf/2310.06513" title="Download PDF">pdf</a>, <a href="/format/2310.06513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Monte Carlo Tree Search with Probability Tree State  Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yangqing Fu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Ming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+B">Buqing Nie</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yue Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have
achieved superhuman performance in many challenging tasks. However, the
computational complexity of MCTS-based algorithms is influenced by the size of
the search space. To address this issue, we propose a novel probability tree
state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A
general tree state abstraction with path transitivity is defined. In addition,
the probability tree state abstraction is proposed for fewer mistakes during
the aggregation step. Furthermore, the theoretical guarantees of the
transitivity and aggregation error bound are justified. To evaluate the
effectiveness of the PTSA algorithm, we integrate it with state-of-the-art
MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental
results on different tasks demonstrate that our method can accelerate the
training process of state-of-the-art algorithms with 10%-45% search space
reduction.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06514" title="Abstract">arXiv:2310.06514</a> [<a href="/pdf/2310.06514" title="Download PDF">pdf</a>, <a href="/format/2310.06514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttributionLab: Faithfulness of Feature Attribution Under Controllable  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+H">Hannah Brown</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+M">Mina Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Bischl%2C+B">Bernd Bischl</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Khakzar%2C+A">Ashkan Khakzar</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages including Appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Feature attribution explains neural network outputs by identifying relevant
input features. How do we know if the identified features are indeed relevant
to the network? This notion is referred to as faithfulness, an essential
property that reflects the alignment between the identified (attributed)
features and the features used by the model. One recent trend to test
faithfulness is to design the data such that we know which input features are
relevant to the label and then train a model on the designed data.
Subsequently, the identified features are evaluated by comparing them with
these designed ground truth features. However, this idea has the underlying
assumption that the neural network learns to use all and only these designed
features, while there is no guarantee that the learning process trains the
network in this way. In this paper, we solve this missing link by explicitly
designing the neural network by manually setting its weights, along with
designing data, so we know precisely which input features in the dataset are
relevant to the designed network. Thus, we can test faithfulness in
AttributionLab, our designed synthetic environment, which serves as a sanity
check and is effective in filtering out attribution methods. If an attribution
method is not faithful in a simple controlled environment, it can be unreliable
in more complex scenarios. Furthermore, the AttributionLab environment serves
as a laboratory for controlled experiments through which we can study feature
attribution methods, identify issues, and suggest potential improvements.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06517" title="Abstract">arXiv:2310.06517</a> [<a href="/pdf/2310.06517" title="Download PDF">pdf</a>, <a href="/format/2310.06517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Semantic Publishing in Non-Invasive Brain Stimulation: A  Comprehensive Analysis of rTMS Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anil%2C+S">Swathi Anil</a>, 
<a href="/search/cs?searchtype=author&query=D%27Souza%2C+J">Jennifer D&#x27;Souza</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures. Accepted as a Practice Paper at The 25th International Conference on Asia-Pacific Digital Libraries (ICADL 2023) <a href="https://icadl.net/icadl2023/index.html#accepted">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL); Information Theory (cs.IT)

</div>
<p class="mathjax">Noninvasive brain stimulation (NIBS) encompasses transcranial stimulation
techniques that can influence brain excitability. These techniques have the
potential to treat conditions like depression, anxiety, and chronic pain, and
to provide insights into brain function. However, a lack of standardized
reporting practices limits its reproducibility and full clinical potential.
This paper aims to foster interinterdisciplinarity toward adopting Computer
Science Semantic reporting methods for the standardized documentation of
Neuroscience NIBS studies making them explicitly Findable, Accessible,
Interoperable, and Reusable (FAIR).
<br />In a large-scale systematic review of 600 repetitive transcranial magnetic
stimulation (rTMS), a subarea of NIBS, dosages, we describe key properties that
allow for structured descriptions and comparisons of the studies. This paper
showcases the semantic publishing of NIBS in the ecosphere of
knowledge-graph-based next-generation scholarly digital libraries.
Specifically, the FAIR Semantic Web resource(s)-based publishing paradigm is
implemented for the 600 reviewed rTMS studies in the Open Research Knowledge
Graph.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06522" title="Abstract">arXiv:2310.06522</a> [<a href="/pdf/2310.06522" title="Download PDF">pdf</a>, <a href="/format/2310.06522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watt For What: Rethinking Deep Learning&#x27;s Energy-Performance  Relationship
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gowda%2C+S+N">Shreyank N Gowda</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+X">Xinyue Hao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Gen Li</a>, 
<a href="/search/cs?searchtype=author&query=Sevilla-Lara%2C+L">Laura Sevilla-Lara</a>, 
<a href="/search/cs?searchtype=author&query=Gowda%2C+S+N">Shashank Narayana Gowda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep learning models have revolutionized various fields, from image
recognition to natural language processing, by achieving unprecedented levels
of accuracy. However, their increasing energy consumption has raised concerns
about their environmental impact, disadvantaging smaller entities in research
and exacerbating global energy consumption. In this paper, we explore the
trade-off between model accuracy and electricity consumption, proposing a
metric that penalizes large consumption of electricity. We conduct a
comprehensive study on the electricity consumption of various deep learning
models across different GPUs, presenting a detailed analysis of their
accuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity
consumed, we demonstrate how smaller, more energy-efficient models can
significantly expedite research while mitigating environmental concerns. Our
results highlight the potential for a more sustainable approach to deep
learning, emphasizing the importance of optimizing models for efficiency. This
research also contributes to a more equitable research landscape, where smaller
entities can compete effectively with larger counterparts. This advocates for
the adoption of efficient deep learning practices to reduce electricity
consumption, safeguarding the environment for future generations whilst also
helping ensure a fairer competitive landscape.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06525" title="Abstract">arXiv:2310.06525</a> [<a href="/pdf/2310.06525" title="Download PDF">pdf</a>, <a href="/format/2310.06525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual MAE for Image Manipulation Localization: A High-level Vision  Learner Focusing on Low-level Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaochen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jizhe Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhuohang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pun%2C+C">Chi-Man Pun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Nowadays, multimedia forensics faces unprecedented challenges due to the
rapid advancement of multimedia generation technology thereby making Image
Manipulation Localization (IML) crucial in the pursuit of truth. The key to IML
lies in revealing the artifacts or inconsistencies between the tampered and
authentic areas, which are evident under pixel-level features. Consequently,
existing studies treat IML as a low-level vision task, focusing on allocating
tampered masks by crafting pixel-level features such as image RGB noises, edge
signals, or high-frequency features. However, in practice, tampering commonly
occurs at the object level, and different classes of objects have varying
likelihoods of becoming targets of tampering. Therefore, object semantics are
also vital in identifying the tampered areas in addition to pixel-level
features. This necessitates IML models to carry out a semantic understanding of
the entire image. In this paper, we reformulate the IML task as a high-level
vision task that greatly benefits from low-level features. Based on such an
interpretation, we propose a method to enhance the Masked Autoencoder (MAE) by
incorporating high-resolution inputs and a perceptual loss supervision module,
which is termed Perceptual MAE (PMAE). While MAE has demonstrated an impressive
understanding of object semantics, PMAE can also compensate for low-level
semantics with our proposed enhancements. Evidenced by extensive experiments,
this paradigm effectively unites the low-level and high-level features of the
IML task and outperforms state-of-the-art tampering localization methods on all
five publicly available datasets.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06530" title="Abstract">arXiv:2310.06530</a> [<a href="/pdf/2310.06530" title="Download PDF">pdf</a>, <a href="/format/2310.06530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refining Decompiled C Code with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+W+K">Wai Kin Wong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huaijin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhibo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qiyi Tang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+S">Sen Nie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">A C decompiler converts an executable into source code. The recovered C
source code, once re-compiled, is expected to produce an executable with the
same functionality as the original executable. With over twenty years of
development, C decompilers have been widely used in production to support
reverse engineering applications. Despite the prosperous development of C
decompilers, it is widely acknowledged that decompiler outputs are mainly used
for human consumption, and are not suitable for automatic recompilation. Often,
a substantial amount of manual effort is required to fix the decompiler outputs
before they can be recompiled and executed properly.
<br />This paper is motived by the recent success of large language models (LLMs)
in comprehending dense corpus of natural language. To alleviate the tedious,
costly and often error-prone manual effort in fixing decompiler outputs, we
investigate the feasibility of using LLMs to augment decompiler outputs, thus
delivering recompilable decompilation. Note that different from previous
efforts that focus on augmenting decompiler outputs with higher readability
(e.g., recovering type/variable names), we focus on augmenting decompiler
outputs with recompilability, meaning to generate code that can be recompiled
into an executable with the same functionality as the original executable.
<br />We conduct a pilot study to characterize the obstacles in recompiling the
outputs of the de facto commercial C decompiler -- IDA-Pro. We then propose a
two-step, hybrid approach to augmenting decompiler outputs with LLMs. We
evaluate our approach on a set of popular C test cases, and show that our
approach can deliver a high recompilation success rate to over 75% with
moderate effort, whereas none of the IDA-Pro's original outputs can be
recompiled. We conclude with a discussion on the limitations of our approach
and promising future research directions.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06536" title="Abstract">arXiv:2310.06536</a> [<a href="/pdf/2310.06536" title="Download PDF">pdf</a>, <a href="/format/2310.06536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EmoTwiCS: A Corpus for Modelling Emotion Trajectories in Dutch Customer  Service Dialogues on Twitter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Labat%2C+S">Sofie Labat</a>, 
<a href="/search/cs?searchtype=author&query=Demeester%2C+T">Thomas Demeester</a>, 
<a href="/search/cs?searchtype=author&query=Hoste%2C+V">V&#xe9;ronique Hoste</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint to Language Resources and Evaluation Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Due to the rise of user-generated content, social media is increasingly
adopted as a channel to deliver customer service. Given the public character of
these online platforms, the automatic detection of emotions forms an important
application in monitoring customer satisfaction and preventing negative
word-of-mouth. This paper introduces EmoTwiCS, a corpus of 9,489 Dutch customer
service dialogues on Twitter that are annotated for emotion trajectories. In
our business-oriented corpus, we view emotions as dynamic attributes of the
customer that can change at each utterance of the conversation. The term
`emotion trajectory' refers therefore not only to the fine-grained emotions
experienced by customers (annotated with 28 labels and
valence-arousal-dominance scores), but also to the event happening prior to the
conversation and the responses made by the human operator (both annotated with
8 categories). Inter-annotator agreement (IAA) scores on the resulting dataset
are substantial and comparable with related research, underscoring its high
quality. Given the interplay between the different layers of annotated
information, we perform several in-depth analyses to investigate (i) static
emotions in isolated tweets, (ii) dynamic emotions and their shifts in
trajectory, and (iii) the role of causes and response strategies in emotion
trajectories. We conclude by listing the advantages and limitations of our
dataset, after which we give some suggestions on the different types of
predictive modelling tasks and open research questions to which EmoTwiCS can be
applied. The dataset is available upon request and will be made publicly
available upon acceptance of the paper.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06540" title="Abstract">arXiv:2310.06540</a> [<a href="/pdf/2310.06540" title="Download PDF">pdf</a>, <a href="/format/2310.06540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo:  A Romanian Clickbait Corpus of News Articles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Broscoteanu%2C+D">Daria-Mihaela Broscoteanu</a>, 
<a href="/search/cs?searchtype=author&query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">To increase revenue, news websites often resort to using deceptive news
titles, luring users into clicking on the title and reading the full news.
Clickbait detection is the task that aims to automatically detect this form of
false advertisement and avoid wasting the precious time of online users.
Despite the importance of the task, to the best of our knowledge, there is no
publicly available clickbait corpus for the Romanian language. To this end, we
introduce a novel Romanian Clickbait Corpus (RoCliCo) comprising 8,313 news
samples which are manually annotated with clickbait and non-clickbait labels.
Furthermore, we conduct experiments with four machine learning methods, ranging
from handcrafted models to recurrent and transformer-based neural networks, to
establish a line-up of competitive baselines. We also carry out experiments
with a weighted voting ensemble. Among the considered baselines, we propose a
novel BERT-based contrastive learning model that learns to encode news titles
and contents into a deep metric space such that titles and contents of
non-clickbait news have high cosine similarity, while titles and contents of
clickbait news have low cosine similarity. Our data set and code to reproduce
the baselines are publicly available for download at
https://github.com/dariabroscoteanu/RoCliCo.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06541" title="Abstract">arXiv:2310.06541</a> [<a href="/pdf/2310.06541" title="Download PDF">pdf</a>, <a href="/format/2310.06541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A  Quantum Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+G+S">Gyu Seon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+J">JaeHyun Chung</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Soohyun Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The advent of reusable rockets has heralded a new era in space exploration,
reducing the costs of launching satellites by a significant factor. Traditional
rockets were disposable, but the design of reusable rockets for repeated use
has revolutionized the financial dynamics of space missions. The most critical
phase of reusable rockets is the landing stage, which involves managing the
tremendous speed and attitude for safe recovery. The complexity of this task
presents new challenges for control systems, specifically in terms of precision
and adaptability. Classical control systems like the
proportional-integral-derivative (PID) controller lack the flexibility to adapt
to dynamic system changes, making them costly and time-consuming to redesign of
controller. This paper explores the integration of quantum reinforcement
learning into the control systems of reusable rockets as a promising
alternative. Unlike classical reinforcement learning, quantum reinforcement
learning uses quantum bits that can exist in superposition, allowing for more
efficient information encoding and reducing the number of parameters required.
This leads to increased computational efficiency, reduced memory requirements,
and more stable and predictable performance. Due to the nature of reusable
rockets, which must be light, heavy computers cannot fit into them. In the
reusable rocket scenario, quantum reinforcement learning, which has reduced
memory requirements due to fewer parameters, is a good solution.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06542" title="Abstract">arXiv:2310.06542</a> [<a href="/pdf/2310.06542" title="Download PDF">pdf</a>, <a href="/format/2310.06542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven mode shape selection and model-based vibration suppression  of 3-RRR parallel manipulator with flexible actuation links
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dingxu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The mode shape function is difficult to determine in modeling manipulators
with flexible links using the assumed mode method. In this paper, for a planar
3-RRR parallel manipulator with flexible actuation links, we provide a
data-driven method to identify the mode shape of the flexible links and propose
a model-based controller for the vibration suppression. By deriving the inverse
kinematics of the studied mechanism in analytical form, the dynamic model is
established by using the assumed mode method. To select the mode shape
function, the software of multi-body system dynamics is used to simulate the
dynamic behavior of the mechanism, and then the data-driven method which
combines the DMD and SINDy algorithms is employed to identify the reasonable
mode shape functions for the flexible links. To suppress the vibration of the
flexible links, a state observer for the end-effector is constructed by a
neural network, and the model-based control law is designed on this basis. In
comparison with the model-free controller, the proposed controller with
developed dynamic model has promising performance in terms of tracking accuracy
and vibration suppression.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06543" title="Abstract">arXiv:2310.06543</a> [<a href="/pdf/2310.06543" title="Download PDF">pdf</a>, <a href="/format/2310.06543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for  Travelling Salesman Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xueming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaochu Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent years have witnessed a surge in research on machine learning for
combinatorial optimization since learning-based approaches can outperform
traditional heuristics and approximate exact solvers at a lower computation
cost. However, most existing work on supervised neural combinatorial
optimization focuses on TSP instances with a fixed number of cities and
requires large amounts of training samples to achieve a good performance,
making them less practical to be applied to realistic optimization scenarios.
This work aims to develop a data-driven graph representation learning method
for solving travelling salesman problems (TSPs) with various numbers of cities.
To this end, we propose an edge-aware graph autoencoder (EdgeGAE) model that
can learn to solve TSPs after being trained on solution data of various sizes
with an imbalanced distribution. We formulate the TSP as a link prediction task
on sparse connected graphs. A residual gated encoder is trained to learn latent
edge embeddings, followed by an edge-centered decoder to output link
predictions in an end-to-end manner. To improve the model's generalization
capability of solving large-scale problems, we introduce an active sampling
strategy into the training process. In addition, we generate a benchmark
dataset containing 50,000 TSP instances with a size from 50 to 500 cities,
following an extremely scale-imbalanced distribution, making it ideal for
investigating the model's performance for practical applications. We conduct
experiments using different amounts of training data with various scales, and
the experimental results demonstrate that the proposed data-driven approach
achieves a highly competitive performance among state-of-the-art learning-based
methods for solving TSPs.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06546" title="Abstract">arXiv:2310.06546</a> [<a href="/pdf/2310.06546" title="Download PDF">pdf</a>, <a href="/format/2310.06546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual  Voice Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Haeyun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Gim%2C+J">Jio Gim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yuho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Suh%2C+Y">Young-Joo Suh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper proposes a simple and robust zero-shot voice conversion system
with a cycle structure and mel-spectrogram pre-processing. Previous works
suffer from information loss and poor synthesis quality due to their reliance
on a carefully designed bottleneck structure. Moreover, models relying solely
on self-reconstruction loss struggled with reproducing different speakers'
voices. To address these issues, we suggested a cycle-consistency loss that
considers conversion back and forth between target and source speakers.
Additionally, stacked random-shuffled mel-spectrograms and a label smoothing
method are utilized during speaker encoder training to extract a
time-independent global speaker representation from speech, which is the key to
a zero-shot conversion. Our model outperforms existing state-of-the-art results
in both subjective and objective evaluations. Furthermore, it facilitates
cross-lingual voice conversions and enhances the quality of synthesized speech.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06547" title="Abstract">arXiv:2310.06547</a> [<a href="/pdf/2310.06547" title="Download PDF">pdf</a>, <a href="/format/2310.06547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rationale-Enhanced Language Models are Better Continual Relation  Learners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Weimin Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sujian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Continual relation extraction (CRE) aims to solve the problem of catastrophic
forgetting when learning a sequence of newly emerging relations. Recent CRE
studies have found that catastrophic forgetting arises from the model's lack of
robustness against future analogous relations. To address the issue, we
introduce rationale, i.e., the explanations of relation classification results
generated by large language models (LLM), into CRE task. Specifically, we
design the multi-task rationale tuning strategy to help the model learn current
relations robustly. We also conduct contrastive rationale replay to further
distinguish analogous relations. Experimental results on two standard
benchmarks demonstrate that our method outperforms the state-of-the-art CRE
models.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06548" title="Abstract">arXiv:2310.06548</a> [<a href="/pdf/2310.06548" title="Download PDF">pdf</a>, <a href="/ps/2310.06548" title="Download PostScript">ps</a>, <a href="/format/2310.06548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterization of the Complexity of Computing the Capacity of Colored  Noise Gaussian Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boche%2C+H">Holger Boche</a>, 
<a href="/search/cs?searchtype=author&query=Grigorescu%2C+A">Andrea Grigorescu</a>, 
<a href="/search/cs?searchtype=author&query=Schaefer%2C+R+F">Rafael F. Schaefer</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper explores the computational complexity involved in determining the
capacity of the band-limited additive colored Gaussian noise (ACGN) channel and
its capacity-achieving power spectral density (p.s.d.). The study reveals that
when the noise p.s.d. is a strictly positive computable continuous function,
computing the capacity of the band-limited ACGN channel becomes a
$\#\mathrm{P}_1$-complete problem within the set of polynomial time computable
noise p.s.d.s. Meaning that it is even more complex than problems that are
$\mathrm{NP}_1$-complete. Additionally, it is shown that the capacity-achieving
distribution is also $\#\mathrm{P}_1$-complete. Furthermore, under the widely
accepted assumption that $\mathrm{FP}_1 \neq \#\mathrm{P}_1$, it has two
significant implications for the ACGN channel. The first implication is the
existence of a polynomial time computable noise p.s.d. for which the
computation of its capacity cannot be performed in polynomial time, i.e., the
number of computational steps on a Turing Machine grows faster than all
polynomials. The second one is the existence of a polynomial time computable
noise p.s.d. for which determining its capacity-achieving p.s.d. cannot be done
within polynomial time.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06549" title="Abstract">arXiv:2310.06549</a> [<a href="/pdf/2310.06549" title="Download PDF">pdf</a>, <a href="/format/2310.06549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 tables, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Label smoothing -- using softened labels instead of hard ones -- is a widely
adopted regularization method for deep learning, showing diverse benefits such
as enhanced generalization and calibration. Its implications for preserving
model privacy, however, have remained unexplored. To fill this gap, we
investigate the impact of label smoothing on model inversion attacks (MIAs),
which aim to generate class-representative samples by exploiting the knowledge
encoded in a classifier, thereby inferring sensitive information about its
training data. Through extensive analyses, we uncover that traditional label
smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even
more, we reveal that smoothing with negative factors counters this trend,
impeding the extraction of class-related information and leading to privacy
preservation, beating state-of-the-art defenses. This establishes a practical
and powerful novel way for enhancing model resilience against MIAs.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06552" title="Abstract">arXiv:2310.06552</a> [<a href="/pdf/2310.06552" title="Download PDF">pdf</a>, <a href="/format/2310.06552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated clinical coding using off-the-shelf large language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boyle%2C+J+S">Joseph S. Boyle</a>, 
<a href="/search/cs?searchtype=author&query=Kascenas%2C+A">Antanas Kascenas</a>, 
<a href="/search/cs?searchtype=author&query=Lok%2C+P">Pat Lok</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neil%2C+A+Q">Alison Q. O&#x27;Neil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment.
Unsupervised pre-training alone does not guarantee precise knowledge of the ICD
ontology and specialist clinical coding task, therefore we frame the task as
information extraction, providing a description of each coded concept and
asking the model to retrieve related mentions. For efficiency, rather than
iterating over all codes, we leverage the hierarchical nature of the ICD
ontology to sparsely search for relevant codes. Then, in a second stage, which
we term 'meta-refinement', we utilise GPT-4 to select a subset of the relevant
labels as predictions. We validate our method using Llama-2, GPT-3.5 and GPT-4
on the CodiEsp dataset of ICD-coded clinical case documents. Our tree-search
method achieves state-of-the-art performance on rarer classes, achieving the
best macro-F1 of 0.225, whilst achieving slightly lower micro-F1 of 0.157,
compared to 0.216 and 0.219 respectively from PLM-ICD. To the best of our
knowledge, this is the first method for automated ICD coding requiring no
task-specific learning.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06553" title="Abstract">arXiv:2310.06553</a> [<a href="/pdf/2310.06553" title="Download PDF">pdf</a>, <a href="/format/2310.06553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe-by-Construction Autonomous Vehicle Overtaking using Control Barrier  Functions and Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yuan%2C+D">Dingran Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+X">Xinyi Yu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shaoyuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+X">Xiang Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Ensuring safety for vehicle overtaking systems is one of the most fundamental
and challenging tasks in autonomous driving. This task is particularly
intricate when the vehicle must not only overtake its front vehicle safely but
also consider the presence of potential opposing vehicles in the opposite lane
that it will temporarily occupy. In order to tackle the overtaking task in such
challenging scenarios, we introduce a novel integrated framework tailored for
vehicle overtaking maneuvers. Our approach integrates the theories of
varying-level control barrier functions (CBF) and time-optimal model predictive
control (MPC). The main feature of our proposed overtaking strategy is that it
is safe-by-construction, which enables rigorous mathematical proof and
validation of the safety guarantees. We show that the proposed framework is
applicable when the opposing vehicle is either fully autonomous or driven by
human drivers. To demonstrate our framework, we perform a set of simulations
for overtaking scenarios under different settings. The simulation results show
the superiority of our framework in the sense that it ensures collision-free
and achieves better safety performance compared with the standard MPC-based
approach without safety guarantees.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06555" title="Abstract">arXiv:2310.06555</a> [<a href="/pdf/2310.06555" title="Download PDF">pdf</a>, <a href="/format/2310.06555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Temporal References in Emergent Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lipinski%2C+O">Olaf Lipinski</a>, 
<a href="/search/cs?searchtype=author&query=Sobey%2C+A+J">Adam J. Sobey</a>, 
<a href="/search/cs?searchtype=author&query=Cerutti%2C+F">Federico Cerutti</a>, 
<a href="/search/cs?searchtype=author&query=Norman%2C+T+J">Timothy J. Norman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 13 figures. Code available at <a href="https://anonymous.4open.science/r/TRG-E137/README.md">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">As humans, we use linguistic elements referencing time, such as before or
tomorrow, to easily share past experiences and future predictions. While
temporal aspects of the language have been considered in computational
linguistics, no such exploration has been done within the field of emergent
communication. We research this gap, providing the first reported temporal
vocabulary within emergent communication literature. Our experimental analysis
shows that a different agent architecture is sufficient for the natural
emergence of temporal references, and that no additional losses are necessary.
Our readily transferable architectural insights provide the basis for the
incorporation of temporal referencing into other emergent communication
environments.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06556" title="Abstract">arXiv:2310.06556</a> [<a href="/pdf/2310.06556" title="Download PDF">pdf</a>, <a href="/format/2310.06556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gender, Age, and Technology Education Influence the Adoption and  Appropriation of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Draxler%2C+F">Fiona Draxler</a>, 
<a href="/search/cs?searchtype=author&query=Buschek%2C+D">Daniel Buschek</a>, 
<a href="/search/cs?searchtype=author&query=Tavast%2C+M">Mikke Tavast</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A4m%C3%A4l%C3%A4inen%2C+P">Perttu H&#xe4;m&#xe4;l&#xe4;inen</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+A">Albrecht Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Kulshrestha%2C+J">Juhi Kulshrestha</a>, 
<a href="/search/cs?searchtype=author&query=Welsch%2C+R">Robin Welsch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large Language Models (LLMs) such as ChatGPT have become increasingly
integrated into critical activities of daily life, raising concerns about
equitable access and utilization across diverse demographics. This study
investigates the usage of LLMs among 1,500 representative US citizens.
Remarkably, 42% of participants reported utilizing an LLM. Our findings reveal
a gender gap in LLM technology adoption (more male users than female users)
with complex interaction patterns regarding age. Technology-related education
eliminates the gender gap in our sample. Moreover, expert users are more likely
than novices to list professional tasks as typical application scenarios,
suggesting discrepancies in effective usage at the workplace. These results
underscore the importance of providing education in artificial intelligence in
our technology-driven society to promote equitable access to and benefits from
LLMs. We urge for both international replication beyond the US and longitudinal
observation of adoption.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06562" title="Abstract">arXiv:2310.06562</a> [<a href="/pdf/2310.06562" title="Download PDF">pdf</a>, <a href="/format/2310.06562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Representation Learning for Brain Tumour Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kascenas%2C+A">Antanas Kascenas</a>, 
<a href="/search/cs?searchtype=author&query=Watson%2C+H">Hannah Watson</a>, 
<a href="/search/cs?searchtype=author&query=Tsaftaris%2C+S+A">Sotirios A. Tsaftaris</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neil%2C+A+Q">Alison Q. O&#x27;Neil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by DART workshop, MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">For brain tumour segmentation, deep learning models can achieve human
expert-level performance given a large amount of data and pixel-level
annotations. However, the expensive exercise of obtaining pixel-level
annotations for large amounts of data is not always feasible, and performance
is often heavily reduced in a low-annotated data regime. To tackle this
challenge, we adapt a mixed supervision framework, vMFNet, to learn robust
compositional representations using unsupervised learning and weak supervision
alongside non-exhaustive pixel-level pathology labels. In particular, we use
the BraTS dataset to simulate a collection of 2-point expert pathology
annotations indicating the top and bottom slice of the tumour (or tumour
sub-regions: peritumoural edema, GD-enhancing tumour, and the necrotic /
non-enhancing tumour) in each MRI volume, from which weak image-level labels
that indicate the presence or absence of the tumour (or the tumour sub-regions)
in the image are constructed. Then, vMFNet models the encoded image features
with von-Mises-Fisher (vMF) distributions, via learnable and compositional vMF
kernels which capture information about structures in the images. We show that
good tumour segmentation performance can be achieved with a large amount of
weakly labelled data but only a small amount of fully-annotated data.
Interestingly, emergent learning of anatomical structures occurs in the
compositional representation even given only supervision relating to pathology
(tumour).
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06566" title="Abstract">arXiv:2310.06566</a> [<a href="/pdf/2310.06566" title="Download PDF">pdf</a>, <a href="/format/2310.06566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Retrieval of Images with Irregular Patterns using  Morphological Image Analysis: Applications to Industrial and Healthcare  datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cosma%2C+G">Georgina Cosma</a>, 
<a href="/search/cs?searchtype=author&query=Bugby%2C+S">Sarah Bugby</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+J">Jason Watkins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 5 figures, 19 tables (17 tables in appendix), submitted to Special Issue: Advances and Challenges in Multimodal Machine Learning 2nd Edition, Journal of Imaging, MDPI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image retrieval is the process of searching and retrieving images from a
database based on their visual content and features. Recently, much attention
has been directed towards the retrieval of irregular patterns within industrial
or medical images by extracting features from the images, such as deep
features, colour-based features, shape-based features and local features. This
has applications across a spectrum of industries, including fault inspection,
disease diagnosis, and maintenance prediction. This paper proposes an image
retrieval framework to search for images containing similar irregular patterns
by extracting a set of morphological features (DefChars) from images; the
datasets employed in this paper contain wind turbine blade images with defects,
chest computerised tomography scans with COVID-19 infection, heatsink images
with defects, and lake ice images. The proposed framework was evaluated with
different feature extraction methods (DefChars, resized raw image, local binary
pattern, and scale-invariant feature transforms) and distance metrics to
determine the most efficient parameters in terms of retrieval performance
across datasets. The retrieval results show that the proposed framework using
the DefChars and the Manhattan distance metric achieves a mean average
precision of 80% and a low standard deviation of 0.09 across classes of
irregular patterns, outperforming alternative feature-metric combinations
across all datasets. Furthermore, the low standard deviation between each class
highlights DefChars' capability for a reliable image retrieval task, even in
the presence of class imbalances or small-sized datasets.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06573" title="Abstract">arXiv:2310.06573</a> [<a href="/pdf/2310.06573" title="Download PDF">pdf</a>, <a href="/format/2310.06573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order adaptive multi-domain time integration scheme for microscale  lithium-ion batteries simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=ASAD%2C+A">Ali ASAD</a> (CMAP), 
<a href="/search/math?searchtype=author&query=de+Loubens%2C+R">Romain de Loubens</a>, 
<a href="/search/math?searchtype=author&query=Fran%C3%A7ois%2C+L">Laurent Fran&#xe7;ois</a>, 
<a href="/search/math?searchtype=author&query=Massot%2C+M">Marc Massot</a> (CMAP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We investigate the modeling and simulation of ionic transport and charge
conservation in lithium-ion batteries (LIBs) at the microscale. It is a
multiphysics problem that involves a wide range of time scales. The associated
computational challenges motivate the investigation of numerical techniques
that can decouple the time integration of the governing equations in the liquid
electrolyte and the solid phase (active materials and current collectors).
First, it is shown that semi-discretization in space of the non-dimensionalized
governing equations leads to a system of index-1 semi-explicit differential
algebraic equations (DAEs). Then, a new generation of strategies for
multi-domain integration is presented, enabling high-order adaptive coupling of
both domains in time. A simple 1D LIB half-cell code is implemented as a
demonstrator of the new strategy for the simulation of different modes of cell
operation. The integration of the decoupled subsystems is performed with
high-order accurate implicit nonlinear solvers. The accuracy of the space
discretization is assessed by comparing the numerical results to the analytical
solutions. Then, temporal convergence studies demonstrate the accuracy of the
new multi-domain coupling approach. Finally, the accuracy and computational
efficiency of the adaptive coupling strategy are discussed in the light of the
conditioning of the decoupled subproblems compared to the one of the
fully-coupled problem. This new approach will constitute a key ingredient for
the full scale 3D LIB high-fidelity simulations based on actual electrode
microstructures.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06574" title="Abstract">arXiv:2310.06574</a> [<a href="/pdf/2310.06574" title="Download PDF">pdf</a>, <a href="/format/2310.06574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAI for Early Crop Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Ayshah Chan</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+M">Maja Schneider</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6rner%2C+M">Marco K&#xf6;rner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose an approach for early crop classification through identifying
important timesteps with eXplainable AI (XAI) methods. Our approach consists of
training a baseline crop classification model to carry out layer-wise relevance
propagation (LRP) so that the salient time step can be identified. We chose a
selected number of such important time indices to create the bounding region of
the shortest possible classification timeframe. We identified the period 21st
April 2019 to 9th August 2019 as having the best trade-off in terms of accuracy
and earliness. This timeframe only suffers a 0.75% loss in accuracy as compared
to using the full timeseries. We observed that the LRP-derived important
timesteps also highlight small details in input values that differentiates
between different classes and
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06577" title="Abstract">arXiv:2310.06577</a> [<a href="/pdf/2310.06577" title="Download PDF">pdf</a>, <a href="/format/2310.06577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SketchBodyNet: A Sketch-Driven Multi-faceted Decoder Network for 3D  Human Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Kongzhang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hefeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Baoquan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hao Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Teng Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, to appear in Pacific Graphics 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Reconstructing 3D human shapes from 2D images has received increasing
attention recently due to its fundamental support for many high-level 3D
applications. Compared with natural images, freehand sketches are much more
flexible to depict various shapes, providing a high potential and valuable way
for 3D human reconstruction. However, such a task is highly challenging. The
sparse abstract characteristics of sketches add severe difficulties, such as
arbitrariness, inaccuracy, and lacking image details, to the already badly
ill-posed problem of 2D-to-3D reconstruction. Although current methods have
achieved great success in reconstructing 3D human bodies from a single-view
image, they do not work well on freehand sketches. In this paper, we propose a
novel sketch-driven multi-faceted decoder network termed SketchBodyNet to
address this task. Specifically, the network consists of a backbone and three
separate attention decoder branches, where a multi-head self-attention module
is exploited in each decoder to obtain enhanced features, followed by a
multi-layer perceptron. The multi-faceted decoders aim to predict the camera,
shape, and pose parameters, respectively, which are then associated with the
SMPL model to reconstruct the corresponding 3D human mesh. In learning,
existing 3D meshes are projected via the camera parameters into 2D synthetic
sketches with joints, which are combined with the freehand sketches to optimize
the model. To verify our method, we collect a large-scale dataset of about 26k
freehand sketches and their corresponding 3D meshes containing various poses of
human bodies from 14 different angles. Extensive experimental results
demonstrate our SketchBodyNet achieves superior performance in reconstructing
3D human meshes from freehand sketches.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06578" title="Abstract">arXiv:2310.06578</a> [<a href="/pdf/2310.06578" title="Download PDF">pdf</a>, <a href="/format/2310.06578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Efficient Visual Search by Eye Movement and Low-Latency Spiking  Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dongqi Han</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yuguo Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Human vision incorporates non-uniform resolution retina, efficient eye
movement strategy, and spiking neural network (SNN) to balance the requirements
in visual field size, visual resolution, energy cost, and inference latency.
These properties have inspired interest in developing human-like computer
vision. However, existing models haven't fully incorporated the three features
of human vision, and their learned eye movement strategies haven't been
compared with human's strategy, making the models' behavior difficult to
interpret. Here, we carry out experiments to examine human visual search
behaviors and establish the first SNN-based visual search model. The model
combines an artificial retina with spiking feature extraction, memory, and
saccade decision modules, and it employs population coding for fast and
efficient saccade decisions. The model can learn either a human-like or a
near-optimal fixation strategy, outperform humans in search speed and accuracy,
and achieve high energy efficiency through short saccade decision latency and
sparse activation. It also suggests that the human search strategy is
suboptimal in terms of search speed. Our work connects modeling of vision in
neuroscience and machine learning and sheds light on developing more
energy-efficient computer vision algorithms.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06579" title="Abstract">arXiv:2310.06579</a> [<a href="/pdf/2310.06579" title="Download PDF">pdf</a>, <a href="/format/2310.06579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Non-Stationary Channel Measurement and Analysis for MaMIMO-UAV  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Colpaert%2C+A">Achiel Colpaert</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Z">Zhuangzhuang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Vinogradov%2C+E">Evgenii Vinogradov</a>, 
<a href="/search/cs?searchtype=author&query=Pollin%2C+S">Sofie Pollin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 12 figures, submitted to IEEE Transactions on Vehicular Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Unmanned aerial vehicles (UAVs) have gained popularity in the communications
research community because of their versatility in placement and potential to
extend the functions of communication networks. However, there remains still a
gap in existing works regarding detailed and measurement-verified air-to-ground
(A2G) Massive Multi-Input Multi-Output (MaMIMO) channel characteristics which
play an important role in realistic deployment. In this paper, we first design
a UAV MaMIMO communication platform for channel acquisition. We then use the
testbed to measure uplink Channel State Information (CSI) between a rotary-wing
drone and a 64-element MaMIMO base station (BS). For characterization, we focus
on multidimensional channel stationarity which is a fundamental metric in
communication systems. Afterward, we present measurement results and analyze
the channel statistics based on power delay profiles (PDPs) considering space,
time, and frequency domains. We propose the stationary angle (SA) as a
supplementary metric of stationary distance (SD) in the time domain. We analyze
the coherence bandwidth and RMS delay spread for frequency stationarity.
Finally, spatial correlations between elements are analyzed to indicate the
spatial stationarity of the array. The space-time-frequency channel stationary
characterization will benefit the physical layer design of MaMIMO-UAV
communications.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06582" title="Abstract">arXiv:2310.06582</a> [<a href="/pdf/2310.06582" title="Download PDF">pdf</a>, <a href="/format/2310.06582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Mask2Former: Panoptic Segmentation of Crops, Weeds and  Leaves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darbyshire%2C+M">Madeleine Darbyshire</a>, 
<a href="/search/cs?searchtype=author&query=Sklar%2C+E">Elizabeth Sklar</a>, 
<a href="/search/cs?searchtype=author&query=Parsons%2C+S">Simon Parsons</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 2 tables, for code, see <a href="https://github.com/madeleinedarbyshire/HierarchicalMask2Former">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Advancements in machine vision that enable detailed inferences to be made
from images have the potential to transform many sectors including agriculture.
Precision agriculture, where data analysis enables interventions to be
precisely targeted, has many possible applications. Precision spraying, for
example, can limit the application of herbicide only to weeds, or limit the
application of fertiliser only to undernourished crops, instead of spraying the
entire field. The approach promises to maximise yields, whilst minimising
resource use and harms to the surrounding environment. To this end, we propose
a hierarchical panoptic segmentation method to simultaneously identify
indicators of plant growth and locate weeds within an image. We adapt
Mask2Former, a state-of-the-art architecture for panoptic segmentation, to
predict crop, weed and leaf masks. We achieve a PQ{\dag} of 75.99.
Additionally, we explore approaches to make the architecture more compact and
therefore more suitable for time and compute constrained applications. With our
more compact architecture, inference is up to 60% faster and the reduction in
PQ{\dag} is less than 1%.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06585" title="Abstract">arXiv:2310.06585</a> [<a href="/pdf/2310.06585" title="Download PDF">pdf</a>, <a href="/format/2310.06585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Black-Box Physics-Informed Estimator based on Gaussian Process  Regression for Robot Inverse Dynamics Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giacomuzzo%2C+G">Giulio Giacomuzzo</a>, 
<a href="/search/cs?searchtype=author&query=Libera%2C+A+D">Alberto Dalla Libera</a>, 
<a href="/search/cs?searchtype=author&query=Romeres%2C+D">Diego Romeres</a>, 
<a href="/search/cs?searchtype=author&query=Carli%2C+R">Ruggero Carli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we propose a black-box model based on Gaussian process
regression for the identification of the inverse dynamics of robotic
manipulators. The proposed model relies on a novel multidimensional kernel,
called \textit{Lagrangian Inspired Polynomial} (\kernelInitials{}) kernel. The
\kernelInitials{} kernel is based on two main ideas. First, instead of directly
modeling the inverse dynamics components, we model as GPs the kinetic and
potential energy of the system. The GP prior on the inverse dynamics components
is derived from those on the energies by applying the properties of GPs under
linear operators. Second, as regards the energy prior definition, we prove a
polynomial structure of the kinetic and potential energy, and we derive a
polynomial kernel that encodes this property. As a consequence, the proposed
model allows also to estimate the kinetic and potential energy without
requiring any label on these quantities. Results on simulation and on two real
robotic manipulators, namely a 7 DOF Franka Emika Panda and a 6 DOF MELFA
RV4FL, show that the proposed model outperforms state-of-the-art black-box
estimators based both on Gaussian Processes and Neural Networks in terms of
accuracy, generality and data efficiency. The experiments on the MELFA robot
also demonstrate that our approach achieves performance comparable to
fine-tuned model-based estimators, despite requiring less prior information.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06588" title="Abstract">arXiv:2310.06588</a> [<a href="/pdf/2310.06588" title="Download PDF">pdf</a>, <a href="/format/2310.06588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yupei Du</a>, 
<a href="/search/cs?searchtype=author&query=Gatt%2C+A">Albert Gatt</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dong Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the massive success of fine-tuning large Pre-trained Language Models
(PLMs) on a wide range of Natural Language Processing (NLP) tasks, they remain
susceptible to out-of-distribution (OOD) and adversarial inputs. Data map (DM)
is a simple yet effective dual-model approach that enhances the robustness of
fine-tuned PLMs, which involves fine-tuning a model on the original training
set (i.e. reference model), selecting a specified fraction of important
training examples according to the training dynamics of the reference model,
and fine-tuning the same model on these selected examples (i.e. main model).
However, it suffers from the drawback of requiring fine-tuning the same model
twice, which is computationally expensive for large models. In this paper, we
first show that 1) training dynamics are highly transferable across different
model sizes and different pre-training methods, and that 2) main models
fine-tuned using DM learn faster than when using conventional Empirical Risk
Minimization (ERM). Building on these observations, we propose a novel
fine-tuning approach based on the DM method: Fine-Tuning by transFerring
Training dynamics (FTFT). Compared with DM, FTFT uses more efficient reference
models and then fine-tunes more capable main models for fewer steps. Our
experiments show that FTFT achieves better generalization robustness than ERM
while spending less than half of the training cost.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06590" title="Abstract">arXiv:2310.06590</a> [<a href="/pdf/2310.06590" title="Download PDF">pdf</a>, <a href="/ps/2310.06590" title="Download PostScript">ps</a>, <a href="/format/2310.06590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech  Recognition through Pitch Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fucci%2C+D">Dennis Fucci</a>, 
<a href="/search/cs?searchtype=author&query=Gaido%2C+M">Marco Gaido</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+M">Matteo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Cettolo%2C+M">Mauro Cettolo</a>, 
<a href="/search/cs?searchtype=author&query=Bentivogli%2C+L">Luisa Bentivogli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic speech recognition (ASR) systems are known to be sensitive to the
sociolinguistic variability of speech data, in which gender plays a crucial
role. This can result in disparities in recognition accuracy between male and
female speakers, primarily due to the under-representation of the latter group
in the training data. While in the context of hybrid ASR models several
solutions have been proposed, the gender bias issue has not been explicitly
addressed in end-to-end neural architectures. To fill this gap, we propose a
data augmentation technique that manipulates the fundamental frequency (f0) and
formants. This technique reduces the data unbalance among genders by simulating
voices of the under-represented female speakers and increases the variability
within each gender group. Experiments on spontaneous English speech show that
our technique yields a relative WER improvement up to 9.87% for utterances by
female speakers, with larger gains for the least-represented f0 ranges.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06594" title="Abstract">arXiv:2310.06594</a> [<a href="/pdf/2310.06594" title="Download PDF">pdf</a>, <a href="/format/2310.06594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning  Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+N">Ning Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Renqiu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Min Cao</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">There is an emerging line of research on multimodal instruction tuning, and a
line of benchmarks have been proposed for evaluating these models recently.
Instead of evaluating the models directly, in this paper we try to evaluate the
Vision-Language Instruction-Tuning (VLIT) datasets themselves and further seek
the way of building a dataset for developing an all-powerful VLIT model, which
we believe could also be of utility for establishing a grounded protocol for
benchmarking VLIT models. For effective analysis of VLIT datasets that remains
an open question, we propose a tune-cross-evaluation paradigm: tuning on one
dataset and evaluating on the others in turn. For each single tune-evaluation
experiment set, we define the Meta Quality (MQ) as the mean score measured by a
series of caption metrics including BLEU, METEOR, and ROUGE-L to quantify the
quality of a certain dataset or a sample. On this basis, to evaluate the
comprehensiveness of a dataset, we develop the Dataset Quality (DQ) covering
all tune-evaluation sets. To lay the foundation for building a comprehensive
dataset and developing an all-powerful model for practical applications, we
further define the Sample Quality (SQ) to quantify the all-sided quality of
each sample. Extensive experiments validate the rationality of the proposed
evaluation paradigm. Based on the holistic evaluation, we build a new dataset,
REVO-LION (REfining VisiOn-Language InstructiOn tuNing), by collecting samples
with higher SQ from each dataset. With only half of the full data, the model
trained on REVO-LION can achieve performance comparable to simply adding all
VLIT datasets up. In addition to developing an all-powerful model, REVO-LION
also includes an evaluation set, which is expected to serve as a convenient
evaluation benchmark for future research.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06599" title="Abstract">arXiv:2310.06599</a> [<a href="/pdf/2310.06599" title="Download PDF">pdf</a>, <a href="/format/2310.06599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Agile Scaling Approaches Make A Difference? An Empirical Comparison  of Team Effectiveness Across Popular Scaling Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verwijs%2C+C">Christiaan Verwijs</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+D">Daniel Russo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the era of Agile methodologies, organizations are exploring strategies to
scale development across teams. Various scaling strategies have emerged, from
"SAFe" to "LeSS", with some organizations creating their own methods. Despite
numerous studies on organizational challenges with these approaches, none have
empirically compared their impact on Agile team effectiveness. This study aims
to evaluate the effectiveness of Agile teams using different scaling methods,
focusing on factors like responsiveness, stakeholder satisfaction, and
management approach. We surveyed 15,078 Agile team members and 1,841
stakeholders, followed by statistical analyses. The results showed minor
differences in effectiveness across scaling strategies. In essence, the choice
of scaling strategy does not significantly impact team effectiveness, and
organizations should select based on their culture and management style.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06600" title="Abstract">arXiv:2310.06600</a> [<a href="/pdf/2310.06600" title="Download PDF">pdf</a>, <a href="/format/2310.06600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy  Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz-Jimenez%2C+G">Guillermo Ortiz-Jimenez</a>, 
<a href="/search/cs?searchtype=author&query=Jenatton%2C+R">Rodolphe Jenatton</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+M">Mark Collier</a>, 
<a href="/search/cs?searchtype=author&query=Kokiopoulou%2C+E">Efi Kokiopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Frossard%2C+P">Pascal Frossard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Label noise is a pervasive problem in deep learning that often compromises
the generalization performance of trained models. Recently, leveraging
privileged information (PI) -- information available only during training but
not at test time -- has emerged as an effective approach to mitigate this
issue. Yet, existing PI-based methods have failed to consistently outperform
their no-PI counterparts in terms of preventing overfitting to label noise. To
address this deficiency, we introduce Pi-DUAL, an architecture designed to
harness PI to distinguish clean from wrong labels. Pi-DUAL decomposes the
output logits into a prediction term, based on conventional input features, and
a noise-fitting term influenced solely by PI. A gating mechanism steered by PI
adaptively shifts focus between these terms, allowing the model to implicitly
separate the learning paths of clean and wrong labels. Empirically, Pi-DUAL
achieves significant performance improvements on key PI benchmarks (e.g., +6.8%
on ImageNet-PI), establishing a new state-of-the-art test set accuracy.
Additionally, Pi-DUAL is a potent method for identifying noisy samples
post-training, outperforming other strong methods at this task. Overall,
Pi-DUAL is a simple, scalable and practical approach for mitigating the effects
of label noise in a variety of real-world scenarios with PI.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06601" title="Abstract">arXiv:2310.06601</a> [<a href="/pdf/2310.06601" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control of Computer Peripherals using Human Eyes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Urumkar%2C+P">Pritish Urumkar</a>, 
<a href="/search/cs?searchtype=author&query=Gade%2C+A">Ashwini Gade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The essential activities such as communication via email, surfing the world
wide web, watching ones preferred Film or television series a large majority of
people impaired by neurolocomotor disorders including those paralyzed by
accident do not access machines. It was inferred from a previous research
review those eyeballs are really an exceptional contender for pervasive
computers because eyeballs switch involuntary through accordance via computer
equipment. It may be important to use this underlying information from eye
motions to carry the use of machines back to those patients. We suggest an
Eye-Gaze Cursor control device for this function that is fully controlled only
with the use of eyeballs. Goal of this project is to develop a easy to use
eye-gesture control device which will detect eye movements robustly and allow
the person to use a computer webcam in accordance to the behavior mirroring to
specific eye movements/gestures. It distinguishes the pupil from the face of
the user and then controls its gestures. In real-time, it has to be specific
enough that the user can use it with ease like most daily gadgets.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06603" title="Abstract">arXiv:2310.06603</a> [<a href="/pdf/2310.06603" title="Download PDF">pdf</a>, <a href="/format/2310.06603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric  Heterogenous Distillation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+C">Caizhen He</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yingfeng Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Object detection is the central issue of intelligent traffic systems, and
recent advancements in single-vehicle lidar-based 3D detection indicate that it
can provide accurate position information for intelligent agents to make
decisions and plan. Compared with single-vehicle perception, multi-view
vehicle-road cooperation perception has fundamental advantages, such as the
elimination of blind spots and a broader range of perception, and has become a
research hotspot. However, the current perception of cooperation focuses on
improving the complexity of fusion while ignoring the fundamental problems
caused by the absence of single-view outlines. We propose a multi-view
vehicle-road cooperation perception system, vehicle-to-everything cooperative
perception (V2X-AHD), in order to enhance the identification capability,
particularly for predicting the vehicle's shape. At first, we propose an
asymmetric heterogeneous distillation network fed with different training data
to improve the accuracy of contour recognition, with multi-view teacher
features transferring to single-view student features. While the point cloud
data are sparse, we propose Spara Pillar, a spare convolutional-based plug-in
feature extraction backbone, to reduce the number of parameters and improve and
enhance feature extraction capabilities. Moreover, we leverage the multi-head
self-attention (MSA) to fuse the single-view feature, and the lightweight
design makes the fusion feature a smooth expression. The results of applying
our algorithm to the massive open dataset V2Xset demonstrate that our method
achieves the state-of-the-art result. The V2X-AHD can effectively improve the
accuracy of 3D object detection and reduce the number of network parameters,
according to this study, which serves as a benchmark for cooperative
perception. The code for this article is available at
https://github.com/feeling0414-lab/V2X-AHD.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06606" title="Abstract">arXiv:2310.06606</a> [<a href="/pdf/2310.06606" title="Download PDF">pdf</a>, <a href="/format/2310.06606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SYNLOCO: Synthesizing Central Pattern Generator and Reinforcement  Learning for Quadruped Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhiyuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wei Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The Central Pattern Generator (CPG) is adept at generating rhythmic gait
patterns characterized by consistent timing and adequate foot clearance. Yet,
its open-loop configuration often compromises the system's control performance
in response to environmental variations. On the other hand, Reinforcement
Learning (RL), celebrated for its model-free properties, has gained significant
traction in robotics due to its inherent adaptability and robustness. However,
initiating traditional RL approaches from the ground up presents computational
challenges and a heightened risk of converging to suboptimal local minima. In
this paper, we propose an innovative quadruped locomotion framework, SYNLOCO,
by synthesizing CPG and RL that can ingeniously integrate the strengths of both
methods, enabling the development of a locomotion controller that is both
stable and natural. Furthermore, we introduce a set of performance-driven
reward metrics that augment the learning of locomotion control. To optimize the
learning trajectory of SYNLOCO, a two-phased training strategy is presented.
Our empirical evaluation, conducted on a Unitree GO1 robot under varied
conditions--including distinct velocities, terrains, and payload
capacities--showcases SYNLOCO's ability to produce consistent and clear-footed
gaits across diverse scenarios. The developed controller exhibits resilience
against substantial parameter variations, underscoring its potential for robust
real-world applications.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06609" title="Abstract">arXiv:2310.06609</a> [<a href="/pdf/2310.06609" title="Download PDF">pdf</a>, <a href="/format/2310.06609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Interpretable Physical Models Using Symbolic Regression and  Discrete Exterior Calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manti%2C+S">Simone Manti</a>, 
<a href="/search/cs?searchtype=author&query=Lucantonio%2C+A">Alessandro Lucantonio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Discrete Mathematics (cs.DM); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Computational modeling is a key resource to gather insight into physical
systems in modern scientific research and engineering. While access to large
amount of data has fueled the use of Machine Learning (ML) to recover physical
models from experiments and increase the accuracy of physical simulations,
purely data-driven models have limited generalization and interpretability. To
overcome these limitations, we propose a framework that combines Symbolic
Regression (SR) and Discrete Exterior Calculus (DEC) for the automated
discovery of physical models starting from experimental data. Since these
models consist of mathematical expressions, they are interpretable and amenable
to analysis, and the use of a natural, general-purpose discrete mathematical
language for physics favors generalization with limited input data.
Importantly, DEC provides building blocks for the discrete analogue of field
theories, which are beyond the state-of-the-art applications of SR to physical
problems. Further, we show that DEC allows to implement a strongly-typed SR
procedure that guarantees the mathematical consistency of the recovered models
and reduces the search space of symbolic expressions. Finally, we prove the
effectiveness of our methodology by re-discovering three models of Continuum
Physics from synthetic experimental data: Poisson equation, the Euler's
Elastica and the equations of Linear Elasticity. Thanks to their
general-purpose nature, the methods developed in this paper may be applied to
diverse contexts of physical modeling.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06613" title="Abstract">arXiv:2310.06613</a> [<a href="/pdf/2310.06613" title="Download PDF">pdf</a>, <a href="/format/2310.06613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BandMap: Application Mapping with Bandwidth Allocation forCoarse-Grained  Reconfigurable Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+X">Xiaobing Ni</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jiaheng Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+M">Mengke Ge</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wendi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Song Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yi Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">This paper proposes an application mapping algorithm, BandMap, for
coarse-grained reconfigurable array (CGRA), which allocates the bandwidth in PE
array according to the transferring demands of data, especially the data with
high spatial reuse, to reduce the routing PEs. To cover bandwidth allocation,
BandMap maps the data flow graphs (DFGs), abstracted from applications, by
solving the maximum independent set (MIS) on a mixture of tuple and quadruple
resource occupation conflict graph. Compared to a state-of-art BusMap work,
Bandmap can achieve reduced routing PEs with the same or even smaller
initiation interval (II).
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06622" title="Abstract">arXiv:2310.06622</a> [<a href="/pdf/2310.06622" title="Download PDF">pdf</a>, <a href="/format/2310.06622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness May be More Brittle than We Think under Different Degrees of  Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kaican Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N+L">Nevin L. Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Out-of-distribution (OOD) generalization is a complicated problem due to the
idiosyncrasies of possible distribution shifts between training and test
domains. Most benchmarks employ diverse datasets to address this issue;
however, the degree of the distribution shift between the training domains and
the test domains of each dataset remains largely fixed. This may lead to biased
conclusions that either underestimate or overestimate the actual OOD
performance of a model. Our study delves into a more nuanced evaluation setting
that covers a broad range of shift degrees. We show that the robustness of
models can be quite brittle and inconsistent under different degrees of
distribution shifts, and therefore one should be more cautious when drawing
conclusions from evaluations under a limited range of degrees. In addition, we
observe that large-scale pre-trained models, such as CLIP, are sensitive to
even minute distribution shifts of novel downstream tasks. This indicates that
while pre-trained representations may help improve downstream in-distribution
performance, they could have minimal or even adverse effects on generalization
in certain OOD scenarios of the downstream task if not used properly. In light
of these findings, we encourage future research to conduct evaluations across a
broader range of shift degrees whenever possible.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06624" title="Abstract">arXiv:2310.06624</a> [<a href="/pdf/2310.06624" title="Download PDF">pdf</a>, <a href="/format/2310.06624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BridgeHand2Vec Bridge Hand Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sztyber-Betley%2C+A">Anna Sztyber-Betley</a>, 
<a href="/search/cs?searchtype=author&query=Ko%C5%82odziej%2C+F">Filip Ko&#x142;odziej</a>, 
<a href="/search/cs?searchtype=author&query=Betley%2C+J">Jan Betley</a>, 
<a href="/search/cs?searchtype=author&query=Duszak%2C+P">Piotr Duszak</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Frontiers in Artificial Intelligence and Applications, Volume 372:
  ECAI 2023, Pages 2274 - 2281
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Contract bridge is a game characterized by incomplete information, posing an
exciting challenge for artificial intelligence methods. This paper proposes the
BridgeHand2Vec approach, which leverages a neural network to embed a bridge
player's hand (consisting of 13 cards) into a vector space. The resulting
representation reflects the strength of the hand in the game and enables
interpretable distances to be determined between different hands. This
representation is derived by training a neural network to estimate the number
of tricks that a pair of players can take. In the remainder of this paper, we
analyze the properties of the resulting vector space and provide examples of
its application in reinforcement learning, and opening bid classification.
Although this was not our main goal, the neural network used for the
vectorization achieves SOTA results on the DDBP2 problem (estimating the number
of tricks for two given hands).
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06625" title="Abstract">arXiv:2310.06625</a> [<a href="/pdf/2310.06625" title="Download PDF">pdf</a>, <a href="/format/2310.06625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> iTransformer: Inverted Transformers Are Effective for Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tengge Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haixu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lintao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The recent boom of linear forecasting models questions the ongoing passion
for architectural modifications of Transformer-based forecasters. These
forecasters leverage Transformers to model the global dependencies over
temporal tokens of time series, with each token formed by multiple variates of
the same timestamp. However, Transformer is challenged in forecasting series
with larger lookback windows due to performance degradation and computation
explosion. Besides, the unified embedding for each temporal token fuses
multiple variates with potentially unaligned timestamps and distinct physical
measurements, which may fail in learning variate-centric representations and
result in meaningless attention maps. In this work, we reflect on the competent
duties of Transformer components and repurpose the Transformer architecture
without any adaptation on the basic components. We propose iTransformer that
simply inverts the duties of the attention mechanism and the feed-forward
network. Specifically, the time points of individual series are embedded into
variate tokens which are utilized by the attention mechanism to capture
multivariate correlations; meanwhile, the feed-forward network is applied for
each variate token to learn nonlinear representations. The iTransformer model
achieves consistent state-of-the-art on several real-world datasets, which
further empowers the Transformer family with promoted performance,
generalization ability across different variates, and better utilization of
arbitrary lookback windows, making it a nice alternative as the fundamental
backbone of time series forecasting.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06626" title="Abstract">arXiv:2310.06626</a> [<a href="/pdf/2310.06626" title="Download PDF">pdf</a>, <a href="/format/2310.06626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic-DPR: Topic-based Prompts for Dense Passage Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Q">Qingfa Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuangyin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prompt-based learning's efficacy across numerous natural language processing
tasks has led to its integration into dense passage retrieval. Prior research
has mainly focused on enhancing the semantic understanding of pre-trained
language models by optimizing a single vector as a continuous prompt. This
approach, however, leads to a semantic space collapse; identical semantic
information seeps into all representations, causing their distributions to
converge in a restricted region. This hinders differentiation between relevant
and irrelevant passages during dense retrieval. To tackle this issue, we
present Topic-DPR, a dense passage retrieval model that uses topic-based
prompts. Unlike the single prompt method, multiple topic-based prompts are
established over a probabilistic simplex and optimized simultaneously through
contrastive learning. This encourages representations to align with their topic
distributions, improving space uniformity. Furthermore, we introduce a novel
positive and negative sampling strategy, leveraging semi-structured data to
boost dense retrieval efficiency. Experimental results from two datasets affirm
that our method surpasses previous state-of-the-art retrieval techniques.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06627" title="Abstract">arXiv:2310.06627</a> [<a href="/pdf/2310.06627" title="Download PDF">pdf</a>, <a href="/format/2310.06627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What If the TV Was Off? Examining Counterfactual Reasoning Abilities of  Multi-modal Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Letian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaotong Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhongkai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yongshuo Zong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bingchen Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short paper accepted at ICCV 2023 VLAR workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Counterfactual reasoning ability is one of the core abilities of human
intelligence. This reasoning process involves the processing of alternatives to
observed states or past events, and this process can improve our ability for
planning and decision-making. In this work, we focus on benchmarking the
counterfactual reasoning ability of multi-modal large language models. We take
the question and answer pairs from the VQAv2 dataset and add one counterfactual
presupposition to the questions, with the answer being modified accordingly.
After generating counterfactual questions and answers using ChatGPT, we
manually examine all generated questions and answers to ensure correctness.
Over 2k counterfactual question and answer pairs are collected this way. We
evaluate recent vision language models on our newly collected test dataset and
found that all models exhibit a large performance drop compared to the results
tested on questions without the counterfactual presupposition. This result
indicates that there still exists space for developing vision language models.
Apart from the vision language models, our proposed dataset can also serves as
a benchmark for evaluating the ability of code generation LLMs, results
demonstrate a large gap between GPT-4 and current open-source models. Our code
and dataset are available at \url{https://github.com/Letian2003/C-VQA}.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06629" title="Abstract">arXiv:2310.06629</a> [<a href="/pdf/2310.06629" title="Download PDF">pdf</a>, <a href="/format/2310.06629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yulong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongshuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zengqiang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Because of the advancement of deep learning technology, vision transformer
has demonstrated competitive performance in various computer vision tasks.
Unfortunately, vision transformer still faces some challenges such as high
computational complexity and absence of desirable inductive bias. To alleviate
these problems, this study proposes a novel Bi-Fovea Self-Attention (BFSA)
inspired by the physiological structure and characteristics of bi-fovea vision
in eagle eyes. This BFSA can simulate the shallow fovea and deep fovea
functions of eagle vision, enabling the network to extract feature
representations of targets from coarse to fine, facilitating the interaction of
multi-scale feature representations. Additionally, this study designs a Bionic
Eagle Vision (BEV) block based on BFSA and CNN. It combines CNN and Vision
Transformer, to enhance the network's local and global representation ability
for targets. Furthermore, this study develops a unified and efficient general
pyramid backbone network family, named Eagle Vision Transformers (EViTs) by
stacking the BEV blocks. Experimental results on various computer vision tasks
including image classification, object detection, instance segmentation and
other transfer learning tasks show that the proposed EViTs perform
significantly better than the baselines under similar model sizes, which
exhibits faster speed on graphics processing unit compared to other models.
Code will be released at https://github.com/nkusyl.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06630" title="Abstract">arXiv:2310.06630</a> [<a href="/pdf/2310.06630" title="Download PDF">pdf</a>, <a href="/format/2310.06630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Intelligent Iterative Methods for Solving Sparse Linear  Algebraic Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zou%2C+H">Haifeng Zou</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+X">Xiaowen Xu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Chen-Song Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Efficiently solving sparse linear algebraic equations is an important
research topic of numerical simulation. Commonly used approaches include direct
methods and iterative methods. Compared with the direct methods, the iterative
methods have lower computational complexity and memory consumption, and are
thus often used to solve large-scale sparse linear equations. However, there
are numerous iterative methods, parameters and components needed to be
carefully chosen, and an inappropriate combination may eventually lead to an
inefficient solution process in practice. With the development of deep
learning, intelligent iterative methods become popular in these years, which
can intelligently make a sufficiently good combination, optimize the parameters
and components in accordance with the properties of the input matrix. This
survey then reviews these intelligent iterative methods. To be clearer, we
shall divide our discussion into three aspects: a method aspect, a component
aspect and a parameter aspect. Moreover, we summarize the existing work and
propose potential research directions that may deserve a deep investigation.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06633" title="Abstract">arXiv:2310.06633</a> [<a href="/pdf/2310.06633" title="Download PDF">pdf</a>, <a href="/format/2310.06633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind Dates: Examining the Expression of Temporality in Historical  Photographs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barancov%C3%A1%2C+A">Alexandra Barancov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Wevers%2C+M">Melvin Wevers</a>, 
<a href="/search/cs?searchtype=author&query=van+Noord%2C+N">Nanne van Noord</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This paper explores the capacity of computer vision models to discern
temporal information in visual content, focusing specifically on historical
photographs. We investigate the dating of images using OpenCLIP, an open-source
implementation of CLIP, a multi-modal language and vision model. Our experiment
consists of three steps: zero-shot classification, fine-tuning, and analysis of
visual content. We use the \textit{De Boer Scene Detection} dataset, containing
39,866 gray-scale historical press photographs from 1950 to 1999. The results
show that zero-shot classification is relatively ineffective for image dating,
with a bias towards predicting dates in the past. Fine-tuning OpenCLIP with a
logistic classifier improves performance and eliminates the bias. Additionally,
our analysis reveals that images featuring buses, cars, cats, dogs, and people
are more accurately dated, suggesting the presence of temporal markers. The
study highlights the potential of machine learning models like OpenCLIP in
dating images and emphasizes the importance of fine-tuning for accurate
temporal analysis. Future research should explore the application of these
findings to color photographs and diverse datasets.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06639" title="Abstract">arXiv:2310.06639</a> [<a href="/pdf/2310.06639" title="Download PDF">pdf</a>, <a href="/format/2310.06639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Lattice Overparametrization Paradigm for the Machine Learning of  Lattice Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marcondes%2C+D">Diego Marcondes</a>, 
<a href="/search/cs?searchtype=author&query=Barrera%2C+J">Junior Barrera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The machine learning of lattice operators has three possible bottlenecks.
From a statistical standpoint, it is necessary to design a constrained class of
operators based on prior information with low bias, and low complexity relative
to the sample size. From a computational perspective, there should be an
efficient algorithm to minimize an empirical error over the class. From an
understanding point of view, the properties of the learned operator need to be
derived, so its behavior can be theoretically understood. The statistical
bottleneck can be overcome due to the rich literature about the representation
of lattice operators, but there is no general learning algorithm for them. In
this paper, we discuss a learning paradigm in which, by overparametrizing a
class via elements in a lattice, an algorithm for minimizing functions in a
lattice is applied to learn. We present the stochastic lattice gradient descent
algorithm as a general algorithm to learn on constrained classes of operators
as long as a lattice overparametrization of it is fixed, and we discuss
previous works which are proves of concept. Moreover, if there are algorithms
to compute the basis of an operator from its overparametrization, then its
properties can be deduced and the understanding bottleneck is also overcome.
This learning paradigm has three properties that modern methods based on neural
networks lack: control, transparency and interpretability. Nowadays, there is
an increasing demand for methods with these characteristics, and we believe
that mathematical morphology is in a unique position to supply them. The
lattice overparametrization paradigm could be a missing piece for it to achieve
its full potential within modern machine learning.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06641" title="Abstract">arXiv:2310.06641</a> [<a href="/pdf/2310.06641" title="Download PDF">pdf</a>, <a href="/format/2310.06641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How (not) to ensemble LVLMs for VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alazraki%2C+L">Lisa Alazraki</a>, 
<a href="/search/cs?searchtype=author&query=Castrejon%2C+L">Lluis Castrejon</a>, 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+M">Mostafa Dehghani</a>, 
<a href="/search/cs?searchtype=author&query=Huot%2C+F">Fantine Huot</a>, 
<a href="/search/cs?searchtype=author&query=Uijlings%2C+J">Jasper Uijlings</a>, 
<a href="/search/cs?searchtype=author&query=Mensink%2C+T">Thomas Mensink</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper studies ensembling in the era of Large Vision-Language Models
(LVLMs). Ensembling is a classical method to combine different models to get
increased performance. In the recent work on Encyclopedic-VQA the authors
examine a wide variety of models to solve their task: from vanilla LVLMs, to
models including the caption as extra context, to models augmented with
Lens-based retrieval of Wikipedia pages. Intuitively these models are highly
complementary, which should make them ideal for ensembling. Indeed, an oracle
experiment shows potential gains from 48.8% accuracy (the best single model)
all the way up to 67% (best possible ensemble). So it is a trivial exercise to
create an ensemble with substantial real gains. Or is it?
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06643" title="Abstract">arXiv:2310.06643</a> [<a href="/pdf/2310.06643" title="Download PDF">pdf</a>, <a href="/format/2310.06643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Variational Inference for High-Dimensional Posteriors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uppal%2C+A">Anshuk Uppal</a>, 
<a href="/search/cs?searchtype=author&query=Stensbo-Smidt%2C+K">Kristoffer Stensbo-Smidt</a>, 
<a href="/search/cs?searchtype=author&query=Boomsma%2C+W+K">Wouter K. Boomsma</a>, 
<a href="/search/cs?searchtype=author&query=Frellsen%2C+J">Jes Frellsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, and supplementary
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In variational inference, the benefits of Bayesian models rely on accurately
capturing the true posterior distribution. We propose using neural samplers
that specify implicit distributions, which are well-suited for approximating
complex multimodal and correlated posteriors in high-dimensional spaces. Our
approach advances inference using implicit distributions by introducing novel
bounds that come about by locally linearising the neural sampler. This is
distinct from existing methods that rely on additional discriminator networks
and unstable adversarial objectives. Furthermore, we present a new sampler
architecture that, for the first time, enables implicit distributions over
millions of latent variables, addressing computational concerns by using
differentiable numerical approximations. Our empirical analysis indicates our
method is capable of recovering correlations across layers in large Bayesian
neural networks, a property that is crucial for a network's performance but
notoriously challenging to achieve. To the best of our knowledge, no other
method has been shown to accomplish this task for such large models. Through
experiments in downstream tasks, we demonstrate that our expressive posteriors
outperform state-of-the-art uncertainty quantification methods, validating the
effectiveness of our training algorithm and the quality of the learned implicit
approximation.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06644" title="Abstract">arXiv:2310.06644</a> [<a href="/pdf/2310.06644" title="Download PDF">pdf</a>, <a href="/format/2310.06644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Level-Set Encoder for Neural Distance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeske%2C+S+R">Stefan Rhys Jeske</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jonathan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Michels%2C+D+L">Dominik L. Michels</a>, 
<a href="/search/cs?searchtype=author&query=Bender%2C+J">Jan Bender</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Neural shape representation generally refers to representing 3D geometry
using neural networks, e.g., to compute a signed distance or occupancy value at
a specific spatial position. Previous methods tend to rely on the auto-decoder
paradigm, which often requires densely-sampled and accurate signed distances to
be known during training and testing, as well as an additional optimization
loop during inference. This introduces a lot of computational overhead, in
addition to having to compute signed distances analytically, even during
testing. In this paper, we present a novel encoder-decoder neural network for
embedding 3D shapes in a single forward pass. Our architecture is based on a
multi-scale hybrid system incorporating graph-based and voxel-based components,
as well as a continuously differentiable decoder. Furthermore, the network is
trained to solve the Eikonal equation and only requires knowledge of the
zero-level set for training and inference. Additional volumetric samples can be
generated on-the-fly, and incorporated in an unsupervised manner. This means
that in contrast to most previous work, our network is able to output valid
signed distance fields without explicit prior knowledge of non-zero distance
values or shape occupancy. In other words, our network computes approximate
solutions to the boundary-valued Eikonal equation. It also requires only a
single forward pass during inference, instead of the common latent code
optimization. We further propose a modification of the loss function in case
that surface normals are not well defined, e.g., in the context of
non-watertight surface-meshes and non-manifold geometry. We finally demonstrate
the efficacy, generalizability and scalability of our method on datasets
consisting of deforming 3D shapes, single class encoding and multiclass
encoding, showcasing a wide range of possible applications.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06645" title="Abstract">arXiv:2310.06645</a> [<a href="/pdf/2310.06645" title="Download PDF">pdf</a>, <a href="/format/2310.06645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Representation Learning for Online Handwriting Text  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehralian%2C+P">Pouya Mehralian</a>, 
<a href="/search/cs?searchtype=author&query=BabaAli%2C+B">Bagher BabaAli</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+A+G">Ashena Gorgan Mohammadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Self-supervised learning offers an efficient way of extracting rich
representations from various types of unlabeled data while avoiding the cost of
annotating large-scale datasets. This is achievable by designing a pretext task
to form pseudo labels with respect to the modality and domain of the data.
Given the evolving applications of online handwritten texts, in this study, we
propose the novel Part of Stroke Masking (POSM) as a pretext task for
pretraining models to extract informative representations from the online
handwriting of individuals in English and Chinese languages, along with two
suggested pipelines for fine-tuning the pretrained models. To evaluate the
quality of the extracted representations, we use both intrinsic and extrinsic
evaluation methods. The pretrained models are fine-tuned to achieve
state-of-the-art results in tasks such as writer identification, gender
classification, and handedness classification, also highlighting the
superiority of utilizing the pretrained models over the models trained from
scratch.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06646" title="Abstract">arXiv:2310.06646</a> [<a href="/pdf/2310.06646" title="Download PDF">pdf</a>, <a href="/format/2310.06646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forgetful Large Language Models: Lessons Learned from Using LLMs in  Robot Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Juo-Tung Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chien-Ming Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages ,8 figures, accepted by the AAAI 2023 Fall Symposium Series
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Large language models offer new ways of empowering people to program robot
applications-namely, code generation via prompting. However, the code generated
by LLMs is susceptible to errors. This work reports a preliminary exploration
that empirically characterizes common errors produced by LLMs in robot
programming. We categorize these errors into two phases: interpretation and
execution. In this work, we focus on errors in execution and observe that they
are caused by LLMs being "forgetful" of key information provided in user
prompts. Based on this observation, we propose prompt engineering tactics
designed to reduce errors in execution. We then demonstrate the effectiveness
of these tactics with three language models: ChatGPT, Bard, and LLaMA-2.
Finally, we discuss lessons learned from using LLMs in robot programming and
call for the benchmarking of LLM-powered end-user development of robot
applications.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06648" title="Abstract">arXiv:2310.06648</a> [<a href="/pdf/2310.06648" title="Download PDF">pdf</a>, <a href="/format/2310.06648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity from Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ren-Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+K">Ke Xue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yutong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Peng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haobo Fu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chao Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Diversity plays a significant role in many problems, such as ensemble
learning, reinforcement learning, and combinatorial optimization. How to define
the diversity measure is a longstanding problem. Many methods rely on expert
experience to define a proper behavior space and then obtain the diversity
measure, which is, however, challenging in many scenarios. In this paper, we
propose the problem of learning a behavior space from human feedback and
present a general method called Diversity from Human Feedback (DivHF) to solve
it. DivHF learns a behavior descriptor consistent with human preference by
querying human feedback. The learned behavior descriptor can be combined with
any distance measure to define a diversity measure. We demonstrate the
effectiveness of DivHF by integrating it with the Quality-Diversity
optimization algorithm MAP-Elites and conducting experiments on the QDax suite.
The results show that DivHF learns a behavior space that aligns better with
human requirements compared to direct data-driven approaches and leads to more
diverse solutions under human preference. Our contributions include formulating
the problem, proposing the DivHF method, and demonstrating its effectiveness
through experiments.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06650" title="Abstract">arXiv:2310.06650</a> [<a href="/pdf/2310.06650" title="Download PDF">pdf</a>, <a href="/format/2310.06650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Parallelized, Adam-Based Solver for Reserve and Security Constrained  AC Unit Commitment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chevalier%2C+S">Samuel Chevalier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Power system optimization problems which include the nonlinear AC power flow
equations require powerful and robust numerical solution algorithms. Within
this sub-field of nonlinear optimization, interior point methods have come to
dominate the solver landscape. Over the last decade, however, a number of
efficient numerical optimizers have emerged from the field of Machine Learning
(ML). One algorithm in particular, Adam, has become the optimizer-of-choice for
a massive percentage of ML training problems (including, e.g., the training of
GPT-3), solving some of the largest unconstrained optimization problems ever
conceived of. Inspired by such progress, this paper designs a parallelized
Adam-based numerical solver to overcome one of the most challenging power
system optimization problems: security and reserve constrained AC Unit
Commitment. The resulting solver, termed quasiGrad, recently competed in the
third ARPA-E Grid Optimization (GO3) competition. In the day-ahead market
clearing category (with systems ranging from 3 to 23,643 buses over 48 time
periods), quasiGrad's aggregated market surplus scores were within 5% of the
winningest market surplus scores. The quasiGrad solver is now released as an
open-source Julia package: quasiGrad.jl. The internal gradient-based solver
(Adam) can easily be substituted for other ML-inspired solvers (e.g., AdaGrad,
AdaDelta, RMSProp, etc.). Test results from large experiments are provided.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06654" title="Abstract">arXiv:2310.06654</a> [<a href="/pdf/2310.06654" title="Download PDF">pdf</a>, <a href="/format/2310.06654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Explanation Methods for Vision-and-Language Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanhua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jia Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The ability to navigate robots with natural language instructions in an
unknown environment is a crucial step for achieving embodied artificial
intelligence (AI). With the improving performance of deep neural models
proposed in the field of vision-and-language navigation (VLN), it is equally
interesting to know what information the models utilize for their
decision-making in the navigation tasks. To understand the inner workings of
deep neural models, various explanation methods have been developed for
promoting explainable AI (XAI). But they are mostly applied to deep neural
models for image or text classification tasks and little work has been done in
explaining deep neural models for VLN tasks. In this paper, we address these
problems by building quantitative benchmarks to evaluate explanation methods
for VLN models in terms of faithfulness. We propose a new erasure-based
evaluation pipeline to measure the step-wise textual explanation in the
sequential decision-making setting. We evaluate several explanation methods for
two representative VLN models on two popular VLN datasets and reveal valuable
findings through our experiments.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06656" title="Abstract">arXiv:2310.06656</a> [<a href="/pdf/2310.06656" title="Download PDF">pdf</a>, <a href="/format/2310.06656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Impact of a Supervised Classification Filter on Flow-based  Hybrid Network Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Macko%2C+D">Dominik Macko</a>, 
<a href="/search/cs?searchtype=author&query=Goldschmidt%2C+P">Patrik Goldschmidt</a>, 
<a href="/search/cs?searchtype=author&query=Pi%C5%A1tek%2C+P">Peter Pi&#x161;tek</a>, 
<a href="/search/cs?searchtype=author&query=Chud%C3%A1%2C+D">Daniela Chud&#xe1;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Constant evolution and the emergence of new cyberattacks require the
development of advanced techniques for defense. This paper aims to measure the
impact of a supervised filter (classifier) in network anomaly detection. We
perform our experiments by employing a hybrid anomaly detection approach in
network flow data. For this purpose, we extended a state-of-the-art
autoencoder-based anomaly detection method by prepending a binary classifier
acting as a prefilter for the anomaly detector. The method was evaluated on the
publicly available real-world dataset UGR'16. Our empirical results indicate
that the hybrid approach does offer a higher detection rate of known attacks
than a standalone anomaly detector while still retaining the ability to detect
zero-day attacks. Employing a supervised binary prefilter has increased the AUC
metric by over 11%, detecting 30% more attacks while keeping the number of
false positives approximately the same.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06661" title="Abstract">arXiv:2310.06661</a> [<a href="/pdf/2310.06661" title="Download PDF">pdf</a>, <a href="/format/2310.06661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tertiary Lymphoid Structures Generation through Graph-based Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madeira%2C+M">Manuel Madeira</a>, 
<a href="/search/cs?searchtype=author&query=Thanou%2C+D">Dorina Thanou</a>, 
<a href="/search/cs?searchtype=author&query=Frossard%2C+P">Pascal Frossard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph-based representation approaches have been proven to be successful in
the analysis of biomedical data, due to their capability of capturing intricate
dependencies between biological entities, such as the spatial organization of
different cell types in a tumor tissue. However, to further enhance our
understanding of the underlying governing biological mechanisms, it is
important to accurately capture the actual distributions of such complex data.
Graph-based deep generative models are specifically tailored to accomplish
that. In this work, we leverage state-of-the-art graph-based diffusion models
to generate biologically meaningful cell-graphs. In particular, we show that
the adopted graph diffusion model is able to accurately learn the distribution
of cells in terms of their tertiary lymphoid structures (TLS) content, a
well-established biomarker for evaluating the cancer progression in oncology
research. Additionally, we further illustrate the utility of the learned
generative models for data augmentation in a TLS classification task. To the
best of our knowledge, this is the first work that leverages the power of graph
diffusion models in generating meaningful biological cell structures.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06663" title="Abstract">arXiv:2310.06663</a> [<a href="/pdf/2310.06663" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptive Cache-Friendly Priority Queue: Fine-Tuning Heap Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parvizi%2C+K">Kiarash Parvizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">A new approach to priority queues is presented, specifically designed to
enhance cache-friendliness. Our data structure incorporates three adjustable
parameters, allowing for a customized heap structure that can adapt to
different system conditions. A search method is used, determining the optimal
parameter values, eliminating the need for manual configuration and delivering
notable performance improvements, as demonstrated through rigorous testing on
the heap sort algorithm. Importantly, this designed data structure can
dynamically grow without the need for reconstructing the heap tree.The
adaptability of our cache-friendly priority queue makes it particularly
interesting in the context of modern computing, where system architectures can
vary widely.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06666" title="Abstract">arXiv:2310.06666</a> [<a href="/pdf/2310.06666" title="Download PDF">pdf</a>, <a href="/format/2310.06666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlock the Potential of Counterfactually-Augmented Data in  Out-Of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Caoyun Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jidong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yitian Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Hao He</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yaohui Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Expert Systems With Applications 2023. arXiv admin note: text overlap with <a href="/abs/2302.09345">arXiv:2302.09345</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Counterfactually-Augmented Data (CAD) -- minimal editing of sentences to flip
the corresponding labels -- has the potential to improve the
Out-Of-Distribution (OOD) generalization capability of language models, as CAD
induces language models to exploit domain-independent causal features and
exclude spurious correlations. However, the empirical results of CAD's OOD
generalization are not as efficient as anticipated. In this study, we attribute
the inefficiency to the myopia phenomenon caused by CAD: language models only
focus on causal features that are edited in the augmentation operation and
exclude other non-edited causal features. Therefore, the potential of CAD is
not fully exploited. To address this issue, we analyze the myopia phenomenon in
feature space from the perspective of Fisher's Linear Discriminant, then we
introduce two additional constraints based on CAD's structural properties
(dataset-level and sentence-level) to help language models extract more
complete causal features in CAD, thereby mitigating the myopia phenomenon and
improving OOD generalization capability. We evaluate our method on two tasks:
Sentiment Analysis and Natural Language Inference, and the experimental results
demonstrate that our method could unlock the potential of CAD and improve the
OOD generalization performance of language models by 1.0% to 5.9%.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06667" title="Abstract">arXiv:2310.06667</a> [<a href="/pdf/2310.06667" title="Download PDF">pdf</a>, <a href="/format/2310.06667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zikun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Aarabi%2C+P">Parham Aarabi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Ruowei Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Out Of Distribution Generalization in Computer Vision workshop at ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative Adversarial Networks (GANs) can synthesize realistic images, with
the learned latent space shown to encode rich semantic information with various
interpretable directions. However, due to the unstructured nature of the
learned latent space, it inherits the bias from the training data where
specific groups of visual attributes that are not causally related tend to
appear together, a phenomenon also known as spurious correlations, e.g., age
and eyeglasses or women and lipsticks. Consequently, the learned distribution
often lacks the proper modelling of the missing examples. The interpolation
following editing directions for one attribute could result in entangled
changes with other attributes. To address this problem, previous works
typically adjust the learned directions to minimize the changes in other
attributes, yet they still fail on strongly correlated features. In this work,
we study the entanglement issue in both the training data and the learned
latent space for the StyleGAN2-FFHQ model. We propose a novel framework
SC$^2$GAN that achieves disentanglement by re-projecting low-density latent
code samples in the original latent space and correcting the editing directions
based on both the high-density and low-density regions. By leveraging the
original meaningful directions and semantic region-specific layers, our
framework interpolates the original latent codes to generate images with
attribute combination that appears infrequently, then inverts these samples
back to the original latent space. We apply our framework to pre-existing
methods that learn meaningful latent directions and showcase its strong
capability to disentangle the attributes with small amounts of low-density
region samples added.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06668" title="Abstract">arXiv:2310.06668</a> [<a href="/pdf/2310.06668" title="Download PDF">pdf</a>, <a href="/format/2310.06668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Diffusion Counterfactual Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farid%2C+K">Karim Farid</a>, 
<a href="/search/cs?searchtype=author&query=Schrodi%2C+S">Simon Schrodi</a>, 
<a href="/search/cs?searchtype=author&query=Argus%2C+M">Max Argus</a>, 
<a href="/search/cs?searchtype=author&query=Brox%2C+T">Thomas Brox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Counterfactual explanations have emerged as a promising method for
elucidating the behavior of opaque black-box models. Recently, several works
leveraged pixel-space diffusion models for counterfactual generation. To handle
noisy, adversarial gradients during counterfactual generation -- causing
unrealistic artifacts or mere adversarial perturbations -- they required either
auxiliary adversarially robust models or computationally intensive guidance
schemes. However, such requirements limit their applicability, e.g., in
scenarios with restricted access to the model's training data. To address these
limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE).
LDCE harnesses the capabilities of recent class- or text-conditional foundation
latent diffusion models to expedite counterfactual generation and focus on the
important, semantic parts of the data. Furthermore, we propose a novel
consensus guidance mechanism to filter out noisy, adversarial gradients that
are misaligned with the diffusion model's implicit classifier. We demonstrate
the versatility of LDCE across a wide spectrum of models trained on diverse
datasets with different learning paradigms. Finally, we showcase how LDCE can
provide insights into model errors, enhancing our understanding of black-box
model behavior.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06670" title="Abstract">arXiv:2310.06670</a> [<a href="/pdf/2310.06670" title="Download PDF">pdf</a>, <a href="/format/2310.06670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Generalization by Rejecting Extreme Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aminbeidokhti%2C+M">Masih Aminbeidokhti</a>, 
<a href="/search/cs?searchtype=author&query=Pe%C3%B1a%2C+F+A+G">Fidel A. Guerrero Pe&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Medeiros%2C+H+R">Heitor Rapela Medeiros</a>, 
<a href="/search/cs?searchtype=author&query=Dubail%2C+T">Thomas Dubail</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>, 
<a href="/search/cs?searchtype=author&query=Pedersoli%2C+M">Marco Pedersoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Data augmentation is one of the most effective techniques for regularizing
deep learning models and improving their recognition performance in a variety
of tasks and domains. However, this holds for standard in-domain settings, in
which the training and test data follow the same distribution. For the
out-of-domain case, where the test data follow a different and unknown
distribution, the best recipe for data augmentation is unclear. In this paper,
we show that for out-of-domain and domain generalization settings, data
augmentation can provide a conspicuous and robust improvement in performance.
To do that, we propose a simple training procedure: (i) use uniform sampling on
standard data augmentation transformations; (ii) increase the strength
transformations to account for the higher data variance expected when working
out-of-domain, and (iii) devise a new reward function to reject extreme
transformations that can harm the training. With this procedure, our data
augmentation scheme achieves a level of accuracy that is comparable to or
better than state-of-the-art methods on benchmark domain generalization
datasets. Code: \url{https://github.com/Masseeh/DCAug}
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06671" title="Abstract">arXiv:2310.06671</a> [<a href="/pdf/2310.06671" title="Download PDF">pdf</a>, <a href="/format/2310.06671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Large Language Models Perform Better in Knowledge Graph  Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language model (LLM) based knowledge graph completion (KGC) aims to
predict the missing triples in the KGs with LLMs and enrich the KGs to become
better web infrastructure, which can benefit a lot of web-based automatic
services. However, research about LLM-based KGC is limited and lacks effective
utilization of LLM's inference capabilities, which ignores the important
structural information in KGs and prevents LLMs from acquiring accurate factual
knowledge. In this paper, we discuss how to incorporate the helpful KG
structural information into the LLMs, aiming to achieve structrual-aware
reasoning in the LLMs. We first transfer the existing LLM paradigms to
structural-aware settings and further propose a knowledge prefix adapter (KoPA)
to fulfill this stated goal. KoPA employs structural embedding pre-training to
capture the structural information of entities and relations in the KG. Then
KoPA informs the LLMs of the knowledge prefix adapter which projects the
structural embeddings into the textual space and obtains virtual knowledge
tokens as a prefix of the input prompt. We conduct comprehensive experiments on
these structural-aware LLM-based KGC methods and provide an in-depth analysis
comparing how the introduction of structural information would be better for
LLM's knowledge reasoning ability. Our code is released at
https://github.com/zjukg/KoPA.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06675" title="Abstract">arXiv:2310.06675</a> [<a href="/pdf/2310.06675" title="Download PDF">pdf</a>, <a href="/format/2310.06675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEER: A Knapsack approach to Exemplar Selection for In-Context HybridQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tonglet%2C+J">Jonathan Tonglet</a>, 
<a href="/search/cs?searchtype=author&query=Reusens%2C+M">Manon Reusens</a>, 
<a href="/search/cs?searchtype=author&query=Borchert%2C+P">Philipp Borchert</a>, 
<a href="/search/cs?searchtype=author&query=Baesens%2C+B">Bart Baesens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 main conference. Code available at github.com/jtonglet/SEER
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Question answering over hybrid contexts is a complex task, which requires the
combination of information extracted from unstructured texts and structured
tables in various ways. Recently, In-Context Learning demonstrated significant
performance advances for reasoning tasks. In this paradigm, a large language
model performs predictions based on a small set of supporting exemplars. The
performance of In-Context Learning depends heavily on the selection procedure
of the supporting exemplars, particularly in the case of HybridQA, where
considering the diversity of reasoning chains and the large size of the hybrid
contexts becomes crucial. In this work, we present Selection of ExEmplars for
hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that
is both representative and diverse. The key novelty of SEER is that it
formulates exemplar selection as a Knapsack Integer Linear Program. The
Knapsack framework provides the flexibility to incorporate diversity
constraints that prioritize exemplars with desirable attributes, and capacity
constraints that ensure that the prompt size respects the provided capacity
budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two
real-world benchmarks for HybridQA, where it outperforms previous exemplar
selection methods.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06678" title="Abstract">arXiv:2310.06678</a> [<a href="/pdf/2310.06678" title="Download PDF">pdf</a>, <a href="/format/2310.06678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling and Performance Analysis of the Over-the-Air Computing in  Cellular IoT Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Ying Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haonan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiaoshou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tingwei Lv</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qianbin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">Ultra-fast wireless data aggregation (WDA) of distributed data has emerged as
a critical design challenge in the ultra-densely deployed cellular internet of
things network (CITN) due to limited spectral resources. Over-the-air computing
(AirComp) has been proposed as an effective solution for ultra-fast WDA by
exploiting the superposition property of wireless channels. However, the effect
of access radius of access point (AP) on the AirComp performance has not been
investigated yet. Therefore, in this work, the mean square error (MSE)
performance of AirComp in the ultra-densely deployed CITN is analyzed with the
AP access radius. By modelling the spatial locations of internet of things
devices as a Poisson point process, the expression of MSE is derived in an
analytical form, which is validated by Monte Carlo simulations. Based on the
analytical MSE, we investigate the effect of AP access radius on the MSE of
AirComp numerically. The results show that there exists an optimal AP access
radius for AirComp, which can decrease the MSE by up to 12.7%. It indicates
that the AP access radius should be carefully chosen to improve the AirComp
performance in the ultra-densely deployed CITN.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06679" title="Abstract">arXiv:2310.06679</a> [<a href="/pdf/2310.06679" title="Download PDF">pdf</a>, <a href="/format/2310.06679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Quantum Systems with Magnetic p-bits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S">Shuvro Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Camsari%2C+K+Y">Kerem Y. Camsari</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Magnetic Conference - Short Papers
  (INTERMAG Short Papers), Sendai, Japan, 2023, pp. 1-2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Quantum Physics (quant-ph)

</div>
<p class="mathjax">The slowing down of Moore's Law has led to a crisis as the computing
workloads of Artificial Intelligence (AI) algorithms continue skyrocketing.
There is an urgent need for scalable and energy-efficient hardware catering to
the unique requirements of AI algorithms and applications. In this environment,
probabilistic computing with p-bits emerged as a scalable, domain-specific, and
energy-efficient computing paradigm, particularly useful for probabilistic
applications and algorithms. In particular, spintronic devices such as
stochastic magnetic tunnel junctions (sMTJ) show great promise in designing
integrated p-computers. Here, we examine how a scalable probabilistic computer
with such magnetic p-bits can be useful for an emerging field combining machine
learning and quantum physics.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06680" title="Abstract">arXiv:2310.06680</a> [<a href="/pdf/2310.06680" title="Download PDF">pdf</a>, <a href="/format/2310.06680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking and Explaining Large Language Model-based Code Generation:  A Causality-Centric Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zhenlan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Pingchuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While code generation has been widely used in various software development
scenarios, the quality of the generated code is not guaranteed. This has been a
particular concern in the era of large language models (LLMs)- based code
generation, where LLMs, deemed a complex and powerful black-box model, is
instructed by a high-level natural language specification, namely a prompt, to
generate code. Nevertheless, effectively evaluating and explaining the code
generation capability of LLMs is inherently challenging, given the complexity
of LLMs and the lack of transparency.
<br />Inspired by the recent progress in causality analysis and its application in
software engineering, this paper launches a causality analysis-based approach
to systematically analyze the causal relations between the LLM input prompts
and the generated code. To handle various technical challenges in this study,
we first propose a novel causal graph-based representation of the prompt and
the generated code, which is established over the fine-grained,
human-understandable concepts in the input prompts. The formed causal graph is
then used to identify the causal relations between the prompt and the derived
code. We illustrate the insights that our framework can provide by studying
over 3 popular LLMs with over 12 prompt adjustment strategies. The results of
these studies illustrate the potential of our technique to provide insights
into LLM effectiveness, and aid end-users in understanding predictions.
Additionally, we demonstrate that our approach provides actionable insights to
improve the quality of the LLM-generated code by properly calibrating the
prompt.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06682" title="Abstract">arXiv:2310.06682</a> [<a href="/pdf/2310.06682" title="Download PDF">pdf</a>, <a href="/format/2310.06682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the importance of catalyst-adsorbate 3D interactions for relaxed  energy predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carbonero%2C+A">Alvaro Carbonero</a>, 
<a href="/search/cs?searchtype=author&query=Duval%2C+A">Alexandre Duval</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+V">Victor Schmidt</a>, 
<a href="/search/cs?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez-Garcia%2C+A">Alex Hernandez-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Rolnick%2C+D">David Rolnick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The use of machine learning for material property prediction and discovery
has traditionally centered on graph neural networks that incorporate the
geometric configuration of all atoms. However, in practice not all this
information may be readily available, e.g.~when evaluating the potentially
unknown binding of adsorbates to catalyst. In this paper, we investigate
whether it is possible to predict a system's relaxed energy in the OC20 dataset
while ignoring the relative position of the adsorbate with respect to the
electro-catalyst. We consider SchNet, DimeNet++ and FAENet as base
architectures and measure the impact of four modifications on model
performance: removing edges in the input graph, pooling independent
representations, not sharing the backbone weights and using an attention
mechanism to propagate non-geometric relative information. We find that while
removing binding site information impairs accuracy as expected, modified models
are able to predict relaxed energies with remarkably decent MAE. Our work
suggests future research directions in accelerated materials discovery where
information on reactant configurations can be reduced or altogether omitted.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06684" title="Abstract">arXiv:2310.06684</a> [<a href="/pdf/2310.06684" title="Download PDF">pdf</a>, <a href="/ps/2310.06684" title="Download PostScript">ps</a>, <a href="/format/2310.06684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Multiplex Embeddings on Text-rich Networks with One Text  Encoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bowen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 11 appendix pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In real-world scenarios, texts in a network are often linked by multiple
semantic relations (e.g., papers in an academic network are referenced by other
publications, written by the same author, or published in the same venue),
where text documents and their relations form a multiplex text-rich network.
Mainstream text representation learning methods use pretrained language models
(PLMs) to generate one embedding for each text unit, expecting that all types
of relations between texts can be captured by these single-view embeddings.
However, this presumption does not hold particularly in multiplex text-rich
networks. Along another line of work, multiplex graph neural networks (GNNs)
directly initialize node attributes as a feature vector for node representation
learning, but they cannot fully capture the semantics of the nodes' associated
texts. To bridge these gaps, we propose METERN, a new framework for learning
Multiplex Embeddings on TExt-Rich Networks. In contrast to existing methods,
METERN uses one text encoder to model the shared knowledge across relations and
leverages a small number of parameters per relation to derive relation-specific
representations. This allows the encoder to effectively capture the multiplex
structures in the network while also preserving parameter efficiency. We
conduct experiments on nine downstream tasks in five networks from both
academic and e-commerce domains, where METERN outperforms baselines
significantly and consistently. The code is available at
https://github.com/PeterGriffinJin/METERN-submit.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06687" title="Abstract">arXiv:2310.06687</a> [<a href="/pdf/2310.06687" title="Download PDF">pdf</a>, <a href="/format/2310.06687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Divergence-Free and $H(div)$-Conforming Embedded-Hybridized DG Method  for the Incompressible Resistive MHD equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+J">Jau-Uei Chen</a>, 
<a href="/search/math?searchtype=author&query=Horv%C3%A1th%2C+T+L">Tam&#xe1;s L. Horv&#xe1;th</a>, 
<a href="/search/math?searchtype=author&query=Bui-Thanh%2C+T">Tan Bui-Thanh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We proposed a divergence-free and $H(div)$-conforming embedded-hybridized
discontinuous Galerkin (E-HDG) method for solving stationary incompressible
viso-resistive magnetohydrodynamic (MHD) equations. In particular, the E-HDG
method is computationally far more advantageous over the hybridized
discontinuous Galerkin (HDG) counterpart in general. The benefit is even
significant in the three-dimensional/high-order/fine mesh scenario. On a
simplicial mesh, our method with a specific choice of the approximation spaces
is proved to be well-posed for the linear case. Additionally, the velocity and
magnetic fields are divergence-free and $H(div)$-conforming for both linear and
nonlinear cases. Moreover, the results of well-posedness analysis,
divergence-free property, and $H(div)$-conformity can be directly applied to
the HDG version of the proposed approach. The HDG or E-HDG method for the
linearized MHD equations can be incorporated into the fixed point Picard
iteration to solve the nonlinear MHD equations in an iterative manner. We
examine the accuracy and convergence of our E-HDG method for both linear and
nonlinear cases through various numerical experiments including two- and
three-dimensional problems with smooth and singular solutions. For smooth
problems, the results indicate that convergence rates in the $L^2$ norm for the
velocity and magnetic fields are optimal in the regime of low Reynolds number
and magnetic Reynolds number. Furthermore, the divergence error is machine zero
for both smooth and singular problems. Finally, we numerically demonstrated
that our proposed method is pressure-robust.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06689" title="Abstract">arXiv:2310.06689</a> [<a href="/pdf/2310.06689" title="Download PDF">pdf</a>, <a href="/format/2310.06689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Nash Equilibria in Normal-Form Games via Stochastic  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gemp%2C+I">Ian Gemp</a>, 
<a href="/search/cs?searchtype=author&query=Marris%2C+L">Luke Marris</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We propose the first, to our knowledge, loss function for approximate Nash
equilibria of normal-form games that is amenable to unbiased Monte Carlo
estimation. This construction allows us to deploy standard non-convex
stochastic optimization techniques for approximating Nash equilibria, resulting
in novel algorithms with provable guarantees. We complement our theoretical
analysis with experiments demonstrating that stochastic gradient descent can
outperform previous state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06690" title="Abstract">arXiv:2310.06690</a> [<a href="/pdf/2310.06690" title="Download PDF">pdf</a>, <a href="/format/2310.06690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Coding-Modulation for Digital Semantic Communications via  Variational Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bo%2C+Y">Yufei Bo</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yiheng Duan</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+S">Shuo Shao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+M">Meixia Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Semantic communications have emerged as a new paradigm for improving
communication efficiency by transmitting the semantic information of a source
message that is most relevant to a desired task at the receiver. Most existing
approaches typically utilize neural networks (NNs) to design end-to-end
semantic communication systems, where NN-based semantic encoders output
continuously distributed signals to be sent directly to the channel in an
analog communication fashion. In this work, we propose a joint
coding-modulation framework for digital semantic communications by using
variational autoencoder (VAE). Our approach learns the transition probability
from source data to discrete constellation symbols, thereby avoiding the
non-differentiability problem of digital modulation. Meanwhile, by jointly
designing the coding and modulation process together, we can match the obtained
modulation strategy with the operating channel condition. We also derive a
matching loss function with information-theoretic meaning for end-to-end
training. Experiments conducted on image semantic communication validate that
our proposed joint coding-modulation framework outperforms separate design of
semantic coding and modulation under various channel conditions, transmission
rates, and modulation orders. Furthermore, its performance gap to analog
semantic communication reduces as the modulation order increases while enjoying
the hardware implementation convenience.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06692" title="Abstract">arXiv:2310.06692</a> [<a href="/pdf/2310.06692" title="Download PDF">pdf</a>, <a href="/format/2310.06692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task  Scenarios with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Anni Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures. arXiv admin note: text overlap with <a href="/abs/2210.03493">arXiv:2210.03493</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have unveiled remarkable reasoning capabilities
by exploiting chain-of-thought (CoT) prompting, which generates intermediate
reasoning chains to serve as the rationale for deriving the answer. However,
current CoT methods either simply employ general prompts such as Let's think
step by step, or heavily rely on handcrafted task-specific demonstrations to
attain preferable performances, thereby engendering an inescapable gap between
performance and generalization. To bridge this gap, we propose Meta-CoT, a
generalizable CoT prompting method in mixed-task scenarios where the type of
input questions is unknown. Meta-CoT firstly categorizes the scenario based on
the input question and subsequently constructs diverse demonstrations from the
corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys
remarkable performances on ten public benchmark reasoning tasks and superior
generalization capabilities. Notably, Meta-CoT achieves the state-of-the-art
result on SVAMP (93.7%) without any additional program-aided methods. Our
further experiments on five out-of-distribution datasets verify the stability
and generality of Meta-CoT.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06694" title="Abstract">arXiv:2310.06694</a> [<a href="/pdf/2310.06694" title="Download PDF">pdf</a>, <a href="/format/2310.06694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sheared LLaMA: Accelerating Language Model Pre-training via Structured  Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengzhou Xia</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Tianyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhiyuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code and models are available at <a href="https://github.com/princeton-nlp/LLM-Shearing">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged
moderate-sized large language models (LLMs) highlights the potential of
building smaller yet powerful LLMs. Regardless, the cost of training such
models from scratch on trillions of tokens remains high. In this work, we study
structured pruning as an effective means to develop smaller LLMs from
pre-trained, larger models. Our approach employs two key techniques: (1)
targeted structured pruning, which prunes a larger model to a specified target
shape by removing layers, heads, and intermediate and hidden dimensions in an
end-to-end manner, and (2) dynamic batch loading, which dynamically updates the
composition of sampled data in each training batch based on varying losses
across different domains. We demonstrate the efficacy of our approach by
presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B
and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art
open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA
models, on a wide range of downstream and instruction tuning evaluations, while
requiring only 3% of compute compared to training such models from scratch.
This work provides compelling evidence that leveraging existing LLMs with
structured pruning is a far more cost-effective approach for building smaller
LLMs.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06702" title="Abstract">arXiv:2310.06702</a> [<a href="/pdf/2310.06702" title="Download PDF">pdf</a>, <a href="/format/2310.06702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporally Aligning Long Audio Interviews with Questions: A Case Study  in Multimodal Data Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasi%2C+P+S">Piyush Singh Pasi</a>, 
<a href="/search/cs?searchtype=author&query=Battepati%2C+K">Karthikeya Battepati</a>, 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+P">Preethi Jyothi</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Mahapatra%2C+T">Tanmay Mahapatra</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Manoj Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work Accepted in IJCAI-23- AI and Social Good Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The problem of audio-to-text alignment has seen significant amount of
research using complete supervision during training. However, this is typically
not in the context of long audio recordings wherein the text being queried does
not appear verbatim within the audio file. This work is a collaboration with a
non-governmental organization called CARE India that collects long audio health
surveys from young mothers residing in rural parts of Bihar, India. Given a
question drawn from a questionnaire that is used to guide these surveys, we aim
to locate where the question is asked within a long audio recording. This is of
great value to African and Asian organizations that would otherwise have to
painstakingly go through long and noisy audio recordings to locate questions
(and answers) of interest. Our proposed framework, INDENT, uses a
cross-attention-based model and prior information on the temporal ordering of
sentences to learn speech embeddings that capture the semantics of the
underlying spoken text. These learnt embeddings are used to retrieve the
corresponding audio segment based on text queries at inference time. We
empirically demonstrate the significant effectiveness (improvement in R-avg of
about 3%) of our model over those obtained using text-based heuristics. We also
show how noisy ASR, generated using state-of-the-art ASR models for Indian
languages, yields better results when used in place of speech. INDENT, trained
only on Hindi data is able to cater to all languages supported by the
(semantically) shared text space. We illustrate this empirically on 11 Indic
languages.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06703" title="Abstract">arXiv:2310.06703</a> [<a href="/pdf/2310.06703" title="Download PDF">pdf</a>, <a href="/format/2310.06703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepLSH: Deep Locality-Sensitive Hash Learning for Fast and Efficient  Near-Duplicate Crash Report Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Remil%2C+Y">Youcef Remil</a>, 
<a href="/search/cs?searchtype=author&query=Bendimerad%2C+A">Anes Bendimerad</a>, 
<a href="/search/cs?searchtype=author&query=Mathonat%2C+R">Romain Mathonat</a>, 
<a href="/search/cs?searchtype=author&query=Raissi%2C+C">Chedy Raissi</a>, 
<a href="/search/cs?searchtype=author&query=Kaytoue%2C+M">Mehdi Kaytoue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automatic crash bucketing is a crucial phase in the software development
process for efficiently triaging bug reports. It generally consists in grouping
similar reports through clustering techniques. However, with real-time
streaming bug collection, systems are needed to quickly answer the question:
What are the most similar bugs to a new one?, that is, efficiently find
near-duplicates. It is thus natural to consider nearest neighbors search to
tackle this problem and especially the well-known locality-sensitive hashing
(LSH) to deal with large datasets due to its sublinear performance and
theoretical guarantees on the similarity search accuracy. Surprisingly, LSH has
not been considered in the crash bucketing literature. It is indeed not trivial
to derive hash functions that satisfy the so-called locality-sensitive property
for the most advanced crash bucketing metrics. Consequently, we study in this
paper how to leverage LSH for this task. To be able to consider the most
relevant metrics used in the literature, we introduce DeepLSH, a Siamese DNN
architecture with an original loss function, that perfectly approximates the
locality-sensitivity property even for Jaccard and Cosine metrics for which
exact LSH solutions exist. We support this claim with a series of experiments
on an original dataset, which we make available.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06707" title="Abstract">arXiv:2310.06707</a> [<a href="/pdf/2310.06707" title="Download PDF">pdf</a>, <a href="/format/2310.06707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Control at Your Fingertips: Quality-Aware Translation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tomani%2C+C">Christian Tomani</a>, 
<a href="/search/cs?searchtype=author&query=Vilar%2C+D">David Vilar</a>, 
<a href="/search/cs?searchtype=author&query=Freitag%2C+M">Markus Freitag</a>, 
<a href="/search/cs?searchtype=author&query=Cherry%2C+C">Colin Cherry</a>, 
<a href="/search/cs?searchtype=author&query=Naskar%2C+S">Subhajit Naskar</a>, 
<a href="/search/cs?searchtype=author&query=Finkelstein%2C+M">Mara Finkelstein</a>, 
<a href="/search/cs?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy
for neural machine translation (NMT) models. The underlying assumption is that
model probability correlates well with human judgment, with better translations
being more likely. However, research has shown that this assumption does not
always hold, and decoding strategies which directly optimize a utility
function, like Minimum Bayes Risk (MBR) or Quality-Aware decoding can
significantly improve translation quality over standard MAP decoding. The main
disadvantage of these methods is that they require an additional model to
predict the utility, and additional steps during decoding, which makes the
entire process computationally demanding. In this paper, we propose to make the
NMT models themselves quality-aware by training them to estimate the quality of
their own output. During decoding, we can use the model's own quality estimates
to guide the generation process and produce the highest-quality translations
possible. We demonstrate that the model can self-evaluate its own output during
translation, eliminating the need for a separate quality estimation model.
Moreover, we show that using this quality signal as a prompt during MAP
decoding can significantly improve translation quality. When using the internal
quality estimate to prune the hypothesis space during MBR decoding, we can not
only further improve translation quality, but also reduce inference speed by
two orders of magnitude.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06710" title="Abstract">arXiv:2310.06710</a> [<a href="/pdf/2310.06710" title="Download PDF">pdf</a>, <a href="/format/2310.06710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Transfer in Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cauderan%2C+A">Alvaro Cauderan</a>, 
<a href="/search/cs?searchtype=author&query=Boeshertz%2C+G">Gauthier Boeshertz</a>, 
<a href="/search/cs?searchtype=author&query=Schwarb%2C+F">Florian Schwarb</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Calvin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present an algorithm that learns to imitate expert behavior and can
transfer to previously unseen domains without retraining. Such an algorithm is
extremely relevant in real-world applications such as robotic learning because
1) reward functions are difficult to design, 2) learned policies from one
domain are difficult to deploy in another domain and 3) learning directly in
the real world is either expensive or unfeasible due to security concerns. To
overcome these constraints, we combine recent advances in Deep RL by using an
AnnealedVAE to learn a disentangled state representation and imitate an expert
by learning a single Q-function which avoids adversarial training. We
demonstrate the effectiveness of our method in 3 environments ranging in
difficulty and the type of transfer knowledge required.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06712" title="Abstract">arXiv:2310.06712</a> [<a href="/pdf/2310.06712" title="Download PDF">pdf</a>, <a href="/format/2310.06712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disappearing repositories -- taking an infrastructure perspective on the  long-term availability of research data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Strecker%2C+D">Dorothea Strecker</a>, 
<a href="/search/cs?searchtype=author&query=Pampel%2C+H">Heinz Pampel</a>, 
<a href="/search/cs?searchtype=author&query=Schabinger%2C+R">Rouven Schabinger</a>, 
<a href="/search/cs?searchtype=author&query=Weisweiler%2C+N+L">Nina Leonie Weisweiler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">Currently, there is limited research investigating the phenomenon of research
data repositories being shut down, and the impact this has on the long-term
availability of data. This paper takes an infrastructure perspective on the
preservation of research data by using a registry to identify 191 research data
repositories that have been closed and presenting information on the shutdown
process. The results show that 6.2 % of research data repositories indexed in
the registry were shut down. The risks resulting in repository shutdown are
varied. The median age of a repository when shutting down is 12 years.
Strategies to prevent data loss at the infrastructure level are pursued to
varying extent. 44 % of the repositories in the sample migrated data to another
repository, and 12 % maintain limited access to their data collection. However,
both strategies are not permanent solutions. Finally, the general lack of
information on repository shutdown events as well as the effect on the
findability of data and the permanence of the scholarly record are discussed.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06713" title="Abstract">arXiv:2310.06713</a> [<a href="/pdf/2310.06713" title="Download PDF">pdf</a>, <a href="/format/2310.06713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Traffic Event Analysis with Bayesian Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zeyi Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Although existing machine learning-based methods for traffic accident
analysis can provide good quality results to downstream tasks, they lack
interpretability which is crucial for this critical problem. This paper
proposes an interpretable framework based on Bayesian Networks for traffic
accident prediction. To enable the ease of interpretability, we design a
dataset construction pipeline to feed the traffic data into the framework while
retaining the essential traffic data information. With a concrete case study,
our framework can derive a Bayesian Network from a dataset based on the causal
relationships between weather and traffic events across the United States.
Consequently, our framework enables the prediction of traffic accidents with
competitive accuracy while examining how the probability of these events
changes under different conditions, thus illustrating transparent relationships
between traffic and weather events. Additionally, the visualization of the
network simplifies the analysis of relationships between different variables,
revealing the primary causes of traffic accidents and ultimately providing a
valuable reference for reducing traffic accidents.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06714" title="Abstract">arXiv:2310.06714</a> [<a href="/pdf/2310.06714" title="Download PDF">pdf</a>, <a href="/format/2310.06714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Memorization in Fine-tuned Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shenglai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">LLMs have shown great capabilities in various tasks but also exhibited
memorization of training data, thus raising tremendous privacy and copyright
concerns. While prior work has studied memorization during pre-training, the
exploration of memorization during fine-tuning is rather limited. Compared with
pre-training, fine-tuning typically involves sensitive data and diverse
objectives, thus may bring unique memorization behaviors and distinct privacy
risks. In this work, we conduct the first comprehensive analysis to explore
LMs' memorization during fine-tuning across tasks. Our studies with
open-sourced and our own fine-tuned LMs across various tasks indicate that
fine-tuned memorization presents a strong disparity among tasks. We provide an
understanding of this task disparity via sparse coding theory and unveil a
strong correlation between memorization and attention score distribution. By
investigating its memorization behavior, multi-task fine-tuning paves a
potential strategy to mitigate fine-tuned memorization.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06715" title="Abstract">arXiv:2310.06715</a> [<a href="/pdf/2310.06715" title="Download PDF">pdf</a>, <a href="/format/2310.06715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S4Sleep: Elucidating the design space of deep-learning-based sleep stage  classification models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tiezhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Strodthoff%2C+N">Nils Strodthoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure, code available at <a href="https://github.com/AI4HealthUOL/s4sleep">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Scoring sleep stages in polysomnography recordings is a time-consuming task
plagued by significant inter-rater variability. Therefore, it stands to benefit
from the application of machine learning algorithms. While many algorithms have
been proposed for this purpose, certain critical architectural decisions have
not received systematic exploration. In this study, we meticulously investigate
these design choices within the broad category of encoder-predictor
architectures. We identify robust architectures applicable to both time series
and spectrogram input representations. These architectures incorporate
structured state space models as integral components, leading to statistically
significant advancements in performance on the extensive SHHS dataset. These
improvements are assessed through both statistical and systematic error
estimations. We anticipate that the architectural insights gained from this
study will not only prove valuable for future research in sleep staging but
also hold relevance for other time series annotation tasks.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06717" title="Abstract">arXiv:2310.06717</a> [<a href="/pdf/2310.06717" title="Download PDF">pdf</a>, <a href="/format/2310.06717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Pseudo-Time Stepping Convergence for CFD Simulations With  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zandbergen%2C+A">Anouk Zandbergen</a>, 
<a href="/search/math?searchtype=author&query=van+Noorden%2C+T">Tycho van Noorden</a>, 
<a href="/search/math?searchtype=author&query=Heinlein%2C+A">Alexander Heinlein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Computational fluid dynamics (CFD) simulations of viscous fluids described by
the Navier-Stokes equations are considered. Depending on the Reynolds number of
the flow, the Navier-Stokes equations may exhibit a highly nonlinear behavior.
The system of nonlinear equations resulting from the discretization of the
Navier-Stokes equations can be solved using nonlinear iteration methods, such
as Newton's method. However, fast quadratic convergence is typically only
obtained in a local neighborhood of the solution, and for many configurations,
the classical Newton iteration does not converge at all. In such cases,
so-called globalization techniques may help to improve convergence.
<br />In this paper, pseudo-transient continuation is employed in order to improve
nonlinear convergence. The classical algorithm is enhanced by a neural network
model that is trained to predict a local pseudo-time step. Generalization of
the novel approach is facilitated by predicting the local pseudo-time step
separately on each element using only local information on a patch of adjacent
elements as input. Numerical results for standard benchmark problems, including
flow through a backward facing step geometry and Couette flow, show the
performance of the machine learning-enhanced globalization approach; as the
software for the simulations, the CFD module of COMSOL Multiphysics is
employed.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06724" title="Abstract">arXiv:2310.06724</a> [<a href="/pdf/2310.06724" title="Download PDF">pdf</a>, <a href="/format/2310.06724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A tiny public key scheme based on Niederreiter Cryptosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalvan%2C+A">Arash Khalvan</a>, 
<a href="/search/cs?searchtype=author&query=Zali%2C+A">Amirhossein Zali</a>, 
<a href="/search/cs?searchtype=author&query=Attari%2C+M+A">Mahmoud Ahmadian Attari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Due to the weakness of public key cryptosystems encounter of quantum
computers, the need to provide a solution was emerged. The McEliece
cryptosystem and its security equivalent, the Niederreiter cryptosystem, which
are based on Goppa codes, are one of the solutions, but they are not practical
due to their long key length. Several prior attempts to decrease the length of
the public key in code-based cryptosystems involved substituting the Goppa code
family with other code families. However, these efforts ultimately proved to be
insecure. In 2016, the National Institute of Standards and Technology (NIST)
called for proposals from around the world to standardize post-quantum
cryptography (PQC) schemes to solve this issue. After receiving of various
proposals in this field, the Classic McEliece cryptosystem, as well as the
Hamming Quasi-Cyclic (HQC) and Bit Flipping Key Encapsulation (BIKE), chosen as
code-based encryption category cryptosystems that successfully progressed to
the final stage. This article proposes a method for developing a code-based
public key cryptography scheme that is both simple and implementable. The
proposed scheme has a much shorter public key length compared to the NIST
finalist cryptosystems. The key length for the primary parameters of the
McEliece cryptosystem (n=1024, k=524, t=50) ranges from 18 to 500 bits. The
security of this system is at least as strong as the security of the
Niederreiter cryptosystem. The proposed structure is based on the Niederreiter
cryptosystem which exhibits a set of highly advantageous properties that make
it a suitable candidate for implementation in all extant systems.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06726" title="Abstract">arXiv:2310.06726</a> [<a href="/pdf/2310.06726" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Status Quo and Problems of Requirements Engineering for Machine  Learning: Results from an International Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alves%2C+A+P+S">Antonio Pedro Santos Alves</a>, 
<a href="/search/cs?searchtype=author&query=Kalinowski%2C+M">Marcos Kalinowski</a>, 
<a href="/search/cs?searchtype=author&query=Giray%2C+G">G&#xf6;rkem Giray</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+D">Daniel Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Lavesson%2C+N">Niklas Lavesson</a>, 
<a href="/search/cs?searchtype=author&query=Azevedo%2C+K">Kelly Azevedo</a>, 
<a href="/search/cs?searchtype=author&query=Villamizar%2C+H">Hugo Villamizar</a>, 
<a href="/search/cs?searchtype=author&query=Escovedo%2C+T">Tatiana Escovedo</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+H">Helio Lopes</a>, 
<a href="/search/cs?searchtype=author&query=Biffl%2C+S">Stefan Biffl</a>, 
<a href="/search/cs?searchtype=author&query=Musil%2C+J">J&#xfc;rgen Musil</a>, 
<a href="/search/cs?searchtype=author&query=Felderer%2C+M">Michael Felderer</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+S">Stefan Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+T">Teresa Baldassarre</a>, 
<a href="/search/cs?searchtype=author&query=Gorschek%2C+T">Tony Gorschek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Publication at PROFES 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Systems that use Machine Learning (ML) have become commonplace for companies
that want to improve their products and processes. Literature suggests that
Requirements Engineering (RE) can help address many problems when engineering
ML-enabled systems. However, the state of empirical evidence on how RE is
applied in practice in the context of ML-enabled systems is mainly dominated by
isolated case studies with limited generalizability. We conducted an
international survey to gather practitioner insights into the status quo and
problems of RE in ML-enabled systems. We gathered 188 complete responses from
25 countries. We conducted quantitative statistical analyses on contemporary
practices using bootstrapping with confidence intervals and qualitative
analyses on the reported problems involving open and axial coding procedures.
We found significant differences in RE practices within ML projects. For
instance, (i) RE-related activities are mostly conducted by project leaders and
data scientists, (ii) the prevalent requirements documentation format concerns
interactive Notebooks, (iii) the main focus of non-functional requirements
includes data quality, model reliability, and model explainability, and (iv)
main challenges include managing customer expectations and aligning
requirements with data. The qualitative analyses revealed that practitioners
face problems related to lack of business domain understanding, unclear goals
and requirements, low customer engagement, and communication issues. These
results help to provide a better understanding of the adopted practices and of
which problems exist in practical environments. We put forward the need to
adapt further and disseminate RE-related practices for engineering ML-enabled
systems.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06732" title="Abstract">arXiv:2310.06732</a> [<a href="/pdf/2310.06732" title="Download PDF">pdf</a>, <a href="/format/2310.06732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-Based Analysis and Visualisation of Mobility Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%A1rquez%2C+R+M">Rafael Mart&#xed;nez M&#xe1;rquez</a>, 
<a href="/search/cs?searchtype=author&query=Patan%C3%A8%2C+G">Giuseppe Patan&#xe8;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">Urban mobility forecast and analysis can be addressed through grid-based and
graph-based models. However, graph-based representations have the advantage of
more realistically depicting the mobility networks and being more robust since
they allow the implementation of Graph Theory machinery, enhancing the analysis
and visualisation of mobility flows. We define two types of mobility graphs:
Region Adjacency graphs and Origin-Destination graphs. Several node centrality
metrics of graphs are applied to identify the most relevant nodes of the
network in terms of graph connectivity. Additionally, the Perron vector
associated with a strongly connected graph is applied to define a circulation
function on the mobility graph. Such node values are visualised in the
geographically embedded graphs, showing clustering patterns within the network.
Since mobility graphs can be directed or undirected, we define several Graph
Laplacian for both cases and show that these matrices and their spectral
properties provide insightful information for network analysis. The computation
of node centrality metrics and Perron-induced circulation functions for three
different geographical regions demonstrate that basic elements from Graph
Theory applied to mobility networks can lead to structure analysis for graphs
of different connectivity, size, and orientation properties.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06743" title="Abstract">arXiv:2310.06743</a> [<a href="/pdf/2310.06743" title="Download PDF">pdf</a>, <a href="/format/2310.06743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geographic Location Encoding with Spherical Harmonics and Sinusoidal  Representation Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ru%C3%9Fwurm%2C+M">Marc Ru&#xdf;wurm</a>, 
<a href="/search/cs?searchtype=author&query=Klemmer%2C+K">Konstantin Klemmer</a>, 
<a href="/search/cs?searchtype=author&query=Rolf%2C+E">Esther Rolf</a>, 
<a href="/search/cs?searchtype=author&query=Zbinden%2C+R">Robin Zbinden</a>, 
<a href="/search/cs?searchtype=author&query=Tuia%2C+D">Devis Tuia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning feature representations of geographical space is vital for any
machine learning model that integrates geolocated data, spanning application
domains such as remote sensing, ecology, or epidemiology. Recent work mostly
embeds coordinates using sine and cosine projections based on Double Fourier
Sphere (DFS) features -- these embeddings assume a rectangular data domain even
on global data, which can lead to artifacts, especially at the poles. At the
same time, relatively little attention has been paid to the exact design of the
neural network architectures these functional embeddings are combined with.
This work proposes a novel location encoder for globally distributed geographic
data that combines spherical harmonic basis functions, natively defined on
spherical surfaces, with sinusoidal representation networks (SirenNets) that
can be interpreted as learned Double Fourier Sphere embedding. We
systematically evaluate the cross-product of positional embeddings and neural
network architectures across various classification and regression benchmarks
and synthetic evaluation datasets. In contrast to previous approaches that
require the combination of both positional encoding and neural networks to
learn meaningful representations, we show that both spherical harmonics and
sinusoidal representation networks are competitive on their own but set
state-of-the-art performances across tasks when combined. We provide source
code at www.github.com/marccoru/locationencoder
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06744" title="Abstract">arXiv:2310.06744</a> [<a href="/pdf/2310.06744" title="Download PDF">pdf</a>, <a href="/format/2310.06744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiFi-123: Towards High-fidelity One Image to 3D Content Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wangbo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan-Pei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiangjun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+L">Long Quan</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonghong Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in text-to-image diffusion models have enabled 3D generation
from a single image. However, current image-to-3D methods often produce
suboptimal results for novel views, with blurred textures and deviations from
the reference image, limiting their practical applications. In this paper, we
introduce HiFi-123, a method designed for high-fidelity and multi-view
consistent 3D generation. Our contributions are twofold: First, we propose a
reference-guided novel view enhancement technique that substantially reduces
the quality gap between synthesized and reference views. Second, capitalizing
on the novel view enhancement, we present a novel reference-guided state
distillation loss. When incorporated into the optimization-based image-to-3D
pipeline, our method significantly improves 3D generation quality, achieving
state-of-the-art performance. Comprehensive evaluations demonstrate the
effectiveness of our approach over existing methods, both qualitatively and
quantitatively.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06746" title="Abstract">arXiv:2310.06746</a> [<a href="/pdf/2310.06746" title="Download PDF">pdf</a>, <a href="/format/2310.06746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Rule Learning: Enhancing the Understanding of Heterogeneous  Treatment Effect via Weighted Causal Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Ying Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kai Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+X">Xiangyu Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Interpretability is a key concern in estimating heterogeneous treatment
effects using machine learning methods, especially for healthcare applications
where high-stake decisions are often made. Inspired by the Predictive,
Descriptive, Relevant framework of interpretability, we propose causal rule
learning which finds a refined set of causal rules characterizing potential
subgroups to estimate and enhance our understanding of heterogeneous treatment
effects. Causal rule learning involves three phases: rule discovery, rule
selection, and rule analysis. In the rule discovery phase, we utilize a causal
forest to generate a pool of causal rules with corresponding subgroup average
treatment effects. The selection phase then employs a D-learning method to
select a subset of these rules to deconstruct individual-level treatment
effects as a linear combination of the subgroup-level effects. This helps to
answer an ignored question by previous literature: what if an individual
simultaneously belongs to multiple groups with different average treatment
effects? The rule analysis phase outlines a detailed procedure to further
analyze each rule in the subset from multiple perspectives, revealing the most
promising rules for further validation. The rules themselves, their
corresponding subgroup treatment effects, and their weights in the linear
combination give us more insights into heterogeneous treatment effects.
Simulation and real-world data analysis demonstrate the superior performance of
causal rule learning on the interpretable estimation of heterogeneous treatment
effect when the ground truth is complex and the sample size is sufficient.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06751" title="Abstract">arXiv:2310.06751</a> [<a href="/pdf/2310.06751" title="Download PDF">pdf</a>, <a href="/format/2310.06751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EARL: Eye-on-Hand Reinforcement Learner for Dynamic Grasping with Active  Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baichuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Siddarth Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented on IROS 2023 Corresponding author Siddarth Jain
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we explore the dynamic grasping of moving objects through
active pose tracking and reinforcement learning for hand-eye coordination
systems. Most existing vision-based robotic grasping methods implicitly assume
target objects are stationary or moving predictably. Performing grasping of
unpredictably moving objects presents a unique set of challenges. For example,
a pre-computed robust grasp can become unreachable or unstable as the target
object moves, and motion planning must also be adaptive. In this work, we
present a new approach, Eye-on-hAnd Reinforcement Learner (EARL), for enabling
coupled Eye-on-Hand (EoH) robotic manipulation systems to perform real-time
active pose tracking and dynamic grasping of novel objects without explicit
motion prediction. EARL readily addresses many thorny issues in automated
hand-eye coordination, including fast-tracking of 6D object pose from vision,
learning control policy for a robotic arm to track a moving object while
keeping the object in the camera's field of view, and performing dynamic
grasping. We demonstrate the effectiveness of our approach in extensive
experiments validated on multiple commercial robotic arms in both simulations
and complex real-world tasks.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06752" title="Abstract">arXiv:2310.06752</a> [<a href="/pdf/2310.06752" title="Download PDF">pdf</a>, <a href="/format/2310.06752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography  Parameters in Third-Party E-Commerce Integrations: A Pre-Quantum Era Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tellez%2C+F">Felipe Tellez</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz%2C+J">Jorge Ortiz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a comparative analysis between the Genetic Algorithm (GA)
and Particle Swarm Optimization (PSO), two vital artificial intelligence
algorithms, focusing on optimizing Elliptic Curve Cryptography (ECC)
parameters. These encompass the elliptic curve coefficients, prime number,
generator point, group order, and cofactor. The study provides insights into
which of the bio-inspired algorithms yields better optimization results for ECC
configurations, examining performances under the same fitness function. This
function incorporates methods to ensure robust ECC parameters, including
assessing for singular or anomalous curves and applying Pollard's rho attack
and Hasse's theorem for optimization precision. The optimized parameters
generated by GA and PSO are tested in a simulated e-commerce environment,
contrasting with well-known curves like secp256k1 during the transmission of
order messages using Elliptic Curve-Diffie Hellman (ECDH) and Hash-based
Message Authentication Code (HMAC). Focusing on traditional computing in the
pre-quantum era, this research highlights the efficacy of GA and PSO in ECC
optimization, with implications for enhancing cybersecurity in third-party
e-commerce integrations. We recommend the immediate consideration of these
findings before quantum computing's widespread adoption.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06753" title="Abstract">arXiv:2310.06753</a> [<a href="/pdf/2310.06753" title="Download PDF">pdf</a>, <a href="/format/2310.06753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dongming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jiahao Chang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Fan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yingfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tiancai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianbing Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 1st solution for 1st OpenLane Topology in Autonomous Driving Challenge. Code is at <a href="https://github.com/wudongming97/TopoMLP">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Topology reasoning aims to comprehensively understand road scenes and present
drivable routes in autonomous driving. It requires detecting road centerlines
(lane) and traffic elements, further reasoning their topology relationship,
i.e., lane-lane topology, and lane-traffic topology. In this work, we first
present that the topology score relies heavily on detection performance on lane
and traffic elements. Therefore, we introduce a powerful 3D lane detector and
an improved 2D traffic element detector to extend the upper limit of topology
performance. Further, we propose TopoMLP, a simple yet high-performance
pipeline for driving topology reasoning. Based on the impressive detection
performance, we develop two simple MLP-based heads for topology generation.
TopoMLP achieves state-of-the-art performance on OpenLane-V2 benchmark, i.e.,
41.2% OLS with ResNet-50 backbone. It is also the 1st solution for 1st OpenLane
Topology in Autonomous Driving Challenge. We hope such simple and strong
pipeline can provide some new insights to the community. Code is at
https://github.com/wudongming97/TopoMLP.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06754" title="Abstract">arXiv:2310.06754</a> [<a href="/pdf/2310.06754" title="Download PDF">pdf</a>, <a href="/format/2310.06754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of RIS-assisted MIMO-OFDM Cellular Networks Based  on Matern Cluster Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guodong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Baccelli%2C+F">Francois Baccelli</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Ke Feng</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+L+U">Luis Uzeda Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Paris%2C+S">Stefano Paris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Performance (cs.PF)

</div>
<p class="mathjax">The Reconfigurable Intelligent Surface (RIS) technology is a promising
physical-layer candidate for sixth-generation (6G) cellular networks. In this
work, we provide a system-level performance assessment of RIS-assisted
multi-input multi-output (MIMO) cellular networks in the downlink in terms of
both the coverage probability and the ergodic rate. To accurately capture the
random layouts of the spatial deployments of both Base Stations (BSs) and RISs,
we propose a new stochastic geometry model for the RIS-assisted radio cellular
system based on the Matern Cluster Process (MCP). This MCP model consists in
adding randomly distributed RISs around BSs, whose placement is modeled as a
Poisson Point Process (PPP). Two types of diversity are available in this
model, namely, the multipath diversity provided by the multiple RISs and the
antenna diversity provided by the multiple antenna receiver. The system employs
the orthogonal frequency division multiplexing (OFDM) technique to modulate the
former and employ the maximal ratio combining (MRC) technique at the receiver
to exploit the latter. The coverage probability is then evaluated when
considering RISs operating as batched powerless beamformers. Based on the
analysis of the coverage probability, we further derive expressions for the
ergodic rate. These analytical expressions provide a new methodology to
evaluate the impact of randomly located RISs around the BSs, given the
RIS-related parameters, such as the density, size, and cluster radius.
Numerical evaluations of the analytical expressions and Monte-Carlo simulations
jointly validate the proposed analytical approach and provide valuable insights
into deploying RIS-assisted radio cellular networks.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06756" title="Abstract">arXiv:2310.06756</a> [<a href="/pdf/2310.06756" title="Download PDF">pdf</a>, <a href="/format/2310.06756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Beyond Neural Network Feature Similarity: The Network Feature  Complexity and Its Interpretation Using Category Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanpeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The behavior of neural networks still remains opaque, and a recently widely
noted phenomenon is that networks often achieve similar performance when
initialized with different random parameters. This phenomenon has attracted
significant attention in measuring the similarity between features learned by
distinct networks. However, feature similarity could be vague in describing the
same feature since equivalent features hardly exist. In this paper, we expand
the concept of equivalent feature and provide the definition of what we call
functionally equivalent features. These features produce equivalent output
under certain transformations. Using this definition, we aim to derive a more
intrinsic metric for the so-called feature complexity regarding the redundancy
of features learned by a neural network at each layer. We offer a formal
interpretation of our approach through the lens of category theory, a
well-developed area in mathematics. To quantify the feature complexity, we
further propose an efficient algorithm named Iterative Feature Merging. Our
experimental results validate our ideas and theories from various perspectives.
We empirically demonstrate that the functionally equivalence widely exists
among different features learned by the same neural network and we could reduce
the number of parameters of the network without affecting the performance.The
IFM shows great potential as a data-agnostic model prune method. We have also
drawn several interesting empirical findings regarding the defined feature
complexity.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06758" title="Abstract">arXiv:2310.06758</a> [<a href="/pdf/2310.06758" title="Download PDF">pdf</a>, <a href="/format/2310.06758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> slash: A Technique for Static Configuration-Logic Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhanahnah%2C+M">Mohannad Alhanahnah</a>, 
<a href="/search/cs?searchtype=author&query=Schubert%2C+P">Philipp Schubert</a>, 
<a href="/search/cs?searchtype=author&query=Reps%2C+T">Thomas Reps</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Somesh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Bodden%2C+E">Eric Bodden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Researchers have recently devised tools for debloating software and detecting
configuration errors. Several of these tools rely on the observation that
programs are composed of an initialization phase followed by a main-computation
phase. Users of these tools are required to manually annotate the boundary that
separates these phases, a task that can be time-consuming and error-prone
(typically, the user has to read and understand the source code or trace
executions with a debugger). Because errors can impair the tool's accuracy and
functionality, the manual-annotation requirement hinders the ability to apply
the tools on a large scale.
<br />In this paper, we present a field study of 24 widely-used C/C++ programs,
identifying common boundary properties in 96\% of them. We then introduce
\textit{slash}, an automated tool that locates the boundary based on the
identified properties. \textit{slash} successfully identifies the boundary in
87.5\% of the studied programs within 8.5\ minutes, using up to 4.4\ GB memory.
In an independent test, carried out after \textit{slash} was developed,
\textit{slash} identified the boundary in 85.7\% of a dataset of 21 popular
C/C++ GitHub repositories. Finally, we demonstrate \textit{slash}'s potential
to streamline the boundary-identification process of software-debloating and
error-detection tools.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06762" title="Abstract">arXiv:2310.06762</a> [<a href="/pdf/2310.06762" title="Download PDF">pdf</a>, <a href="/format/2310.06762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRACE: A Comprehensive Benchmark for Continual Learning in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuansen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Songyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Senjie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Z">Zhiheng Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yicheng Zou</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Aligned large language models (LLMs) demonstrate exceptional capabilities in
task-solving, following instructions, and ensuring safety. However, the
continual learning aspect of these aligned LLMs has been largely overlooked.
Existing continual learning benchmarks lack sufficient challenge for leading
aligned LLMs, owing to both their simplicity and the models' potential exposure
during instruction tuning. In this paper, we introduce TRACE, a novel benchmark
designed to evaluate continual learning in LLMs. TRACE consists of 8 distinct
datasets spanning challenging tasks including domain-specific tasks,
multilingual capabilities, code generation, and mathematical reasoning. All
datasets are standardized into a unified format, allowing for effortless
automatic evaluation of LLMs. Our experiments show that after training on
TRACE, aligned LLMs exhibit significant declines in both general ability and
instruction-following capabilities. For example, the accuracy of llama2-chat
13B on gsm8k dataset declined precipitously from 28.8\% to 2\% after training
on our datasets. This highlights the challenge of finding a suitable tradeoff
between achieving performance on specific tasks while preserving the original
prowess of LLMs. Empirical findings suggest that tasks inherently equipped with
reasoning paths contribute significantly to preserving certain capabilities of
LLMs against potential declines. Motivated by this, we introduce the
Reasoning-augmented Continual Learning (RCL) approach. RCL integrates
task-specific cues with meta-rationales, effectively reducing catastrophic
forgetting in LLMs while expediting convergence on novel tasks.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06763" title="Abstract">arXiv:2310.06763</a> [<a href="/pdf/2310.06763" title="Download PDF">pdf</a>, <a href="/format/2310.06763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FABind: Fast and Accurate Protein-Ligand Binding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+Q">Qizhi Pei</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+K">Kaiyuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lijun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinhua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yingce Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shufang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+T">Tao Qin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kun He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tie-Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neural Information Processing Systems (NIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">Modeling the interaction between proteins and ligands and accurately
predicting their binding structures is a critical yet challenging task in drug
discovery. Recent advancements in deep learning have shown promise in
addressing this challenge, with sampling-based and regression-based methods
emerging as two prominent approaches. However, these methods have notable
limitations. Sampling-based methods often suffer from low efficiency due to the
need for generating multiple candidate structures for selection. On the other
hand, regression-based methods offer fast predictions but may experience
decreased accuracy. Additionally, the variation in protein sizes often requires
external modules for selecting suitable binding pockets, further impacting
efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model
that combines pocket prediction and docking to achieve accurate and fast
protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed
pocket prediction module, which is also leveraged for docking pose estimation.
The model further enhances the docking process by incrementally integrating the
predicted pocket to optimize protein-ligand binding, reducing discrepancies
between training and inference. Through extensive experiments on benchmark
datasets, our proposed $\mathbf{FABind}$ demonstrates strong advantages in
terms of effectiveness and efficiency compared to existing methods. Our code is
available at $\href{https://github.com/QizhiPei/FABind}{Github}$.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06764" title="Abstract">arXiv:2310.06764</a> [<a href="/pdf/2310.06764" title="Download PDF">pdf</a>, <a href="/format/2310.06764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OmniLingo: Listening- and speaking-based language learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tyers%2C+F+M">Francis M. Tyers</a>, 
<a href="/search/cs?searchtype=author&query=Howell%2C+N">Nicholas Howell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this demo paper we present OmniLingo, an architecture for distributing
data for listening- and speaking-based language learning applications and a
demonstration client built using the architecture. The architecture is based on
the Interplanetary Filesystem (IPFS) and puts at the forefront user sovereignty
over data.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06765" title="Abstract">arXiv:2310.06765</a> [<a href="/pdf/2310.06765" title="Download PDF">pdf</a>, <a href="/format/2310.06765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Graduated Non-Convexity for Pose Graph Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wonseok Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaehyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+J">Jiseong Chung</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Seungwon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Tae-wan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We propose a novel approach to Graduated Non-Convexity (GNC) and demonstrate
its efficacy through its application in robust pose graph optimization, a key
component in SLAM backends. Traditional GNC methods often rely on heuristic
methods for GNC schedule, updating control parameter {\mu} for escalating the
non-convexity. In contrast, our approach leverages the properties of convex
functions and convex optimization to identify the boundary points beyond which
convexity is no longer guaranteed, thereby eliminating redundant optimization
steps in existing methodologies and enhancing both speed and robustness. We
show that our method outperforms the state-of-the-art method in terms of speed
and accuracy when used for robust back-end pose graph optimization via GNC. Our
work builds upon and enhances the open-source riSAM framework. Our
implementation can be accessed from: https://github.com/SNU-DLLAB/EGNC-PGO
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06770" title="Abstract">arXiv:2310.06770</a> [<a href="/pdf/2310.06770" title="Download PDF">pdf</a>, <a href="/format/2310.06770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SWE-bench: Can Language Models Resolve Real-World GitHub Issues?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jimenez%2C+C+E">Carlos E. Jimenez</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">John Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wettig%2C+A">Alexander Wettig</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+K">Kexin Pei</a>, 
<a href="/search/cs?searchtype=author&query=Press%2C+O">Ofir Press</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K">Karthik Narasimhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Data, code, and leaderboard are available at <a href="https://www.swebench.com">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Language models have outpaced our ability to evaluate them effectively, but
for their future development it is essential to study the frontier of their
capabilities. We consider real-world software engineering to be a rich,
sustainable, and challenging testbed for evaluating the next generation of
language models. We therefore introduce SWE-bench, an evaluation framework
including $2,294$ software engineering problems drawn from real GitHub issues
and corresponding pull requests across $12$ popular Python repositories. Given
a codebase along with a description of an issue to be resolved, a language
model is tasked with editing the codebase to address the issue. Resolving
issues in SWE-bench frequently requires understanding and coordinating changes
across multiple functions, classes, and even files simultaneously, calling for
models to interact with execution environments, process extremely long contexts
and perform complex reasoning that goes far beyond traditional code generation.
Our evaluations show that both state-of-the-art proprietary models and our
fine-tuned model SWE-Llama can resolve only the simplest issues. Claude 2 and
GPT-4 solve a mere $4.8$% and $1.7$% of instances respectively, even when
provided with an oracle retriever. Advances on SWE-bench represent steps
towards LMs that are more practical, intelligent, and autonomous.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06771" title="Abstract">arXiv:2310.06771</a> [<a href="/pdf/2310.06771" title="Download PDF">pdf</a>, <a href="/format/2310.06771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlated Noise Provably Beats Independent Noise for Differentially  Private Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choquette-Choo%2C+C+A">Christopher A. Choquette-Choo</a>, 
<a href="/search/cs?searchtype=author&query=Dvijotham%2C+K">Krishnamurthy Dvijotham</a>, 
<a href="/search/cs?searchtype=author&query=Pillutla%2C+K">Krishna Pillutla</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+A">Arun Ganesh</a>, 
<a href="/search/cs?searchtype=author&query=Steinke%2C+T">Thomas Steinke</a>, 
<a href="/search/cs?searchtype=author&query=Thakurta%2C+A">Abhradeep Thakurta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, and Krishna Pillutla contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Optimization and Control (math.OC)

</div>
<p class="mathjax">Differentially private learning algorithms inject noise into the learning
process. While the most common private learning algorithm, DP-SGD, adds
independent Gaussian noise in each iteration, recent work on matrix
factorization mechanisms has shown empirically that introducing correlations in
the noise can greatly improve their utility. We characterize the asymptotic
learning utility for any choice of the correlation function, giving precise
analytical bounds for linear regression and as the solution to a convex program
for general convex functions. We show, using these bounds, how correlated noise
provably improves upon vanilla DP-SGD as a function of problem parameters such
as the effective dimension and condition number. Moreover, our analytical
expression for the near-optimal correlation function circumvents the cubic
complexity of the semi-definite program used to optimize the noise correlation
matrix in previous work. We validate our theory with experiments on private
deep learning. Our work matches or outperforms prior work while being efficient
both in terms of compute and memory.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06773" title="Abstract">arXiv:2310.06773</a> [<a href="/pdf/2310.06773" title="Download PDF">pdf</a>, <a href="/format/2310.06773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uni3D: Exploring Unified 3D Representation at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Junsheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinsheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+B">Baorui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu-Shen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiejun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinlong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and Demo: <a href="https://github.com/baaivision/Uni3D">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Scaling up representations for images or text has been extensively
investigated in the past few years and has led to revolutions in learning
vision and language. However, scalable representation for 3D objects and scenes
is relatively unexplored. In this work, we present Uni3D, a 3D foundation model
to explore the unified 3D representation at scale. Uni3D uses a 2D initialized
ViT end-to-end pretrained to align the 3D point cloud features with the
image-text aligned features. Via the simple architecture and pretext task,
Uni3D can leverage abundant 2D pretrained models as initialization and
image-text aligned models as the target, unlocking the great potential of 2D
models and scaling-up strategies to the 3D world. We efficiently scale up Uni3D
to one billion parameters, and set new records on a broad range of 3D tasks,
such as zero-shot classification, few-shot classification, open-world
understanding and part segmentation. We show that the strong Uni3D
representation also enables applications such as 3D painting and retrieval in
the wild. We believe that Uni3D provides a new direction for exploring both
scaling up and efficiency of the representation in 3D domain.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06775" title="Abstract">arXiv:2310.06775</a> [<a href="/pdf/2310.06775" title="Download PDF">pdf</a>, <a href="/format/2310.06775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conceptual Framework for Autonomous Cognitive Entities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+D">David Shapiro</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wangfan Li</a>, 
<a href="/search/cs?searchtype=author&query=Delaflor%2C+M">Manuel Delaflor</a>, 
<a href="/search/cs?searchtype=author&query=Toxtli%2C+C">Carlos Toxtli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid development and adoption of Generative AI (GAI) technology in the
form of chatbots such as ChatGPT and Claude has greatly increased interest in
agentic machines. This paper introduces the Autonomous Cognitive Entity (ACE)
model, a novel framework for a cognitive architecture, enabling machines and
software agents to operate more independently. Drawing inspiration from the OSI
model, the ACE framework presents layers of abstraction to conceptualize
artificial cognitive architectures. The model is designed to harness the
capabilities of the latest generative AI technologies, including large language
models (LLMs) and multimodal generative models (MMMs), to build autonomous,
agentic systems. The ACE framework comprises six layers: the Aspirational
Layer, Global Strategy, Agent Model, Executive Function, Cognitive Control, and
Task Prosecution. Each layer plays a distinct role, ranging from setting the
moral compass and strategic thinking to task selection and execution. The ACE
framework also incorporates mechanisms for handling failures and adapting
actions, thereby enhancing the robustness and flexibility of autonomous agents.
This paper introduces the conceptual framework and proposes implementation
strategies that have been tested and observed in industry. The goal of this
paper is to formalize this framework so as to be more accessible.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06777" title="Abstract">arXiv:2310.06777</a> [<a href="/pdf/2310.06777" title="Download PDF">pdf</a>, <a href="/format/2310.06777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Content Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chmura%2C+J">Jacob Chmura</a>, 
<a href="/search/cs?searchtype=author&query=Burhani%2C+H">Hasham Burhani</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X+Q">Xiao Qi Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sparse reward environments are known to be challenging for reinforcement
learning agents. In such environments, efficient and scalable exploration is
crucial. Exploration is a means by which an agent gains information about the
environment. We expand on this topic and propose a new intrinsic reward that
systemically quantifies exploratory behavior and promotes state coverage by
maximizing the information content of a trajectory taken by an agent. We
compare our method to alternative exploration based intrinsic reward
techniques, namely Curiosity Driven Learning and Random Network Distillation.
We show that our information theoretic reward induces efficient exploration and
outperforms in various games, including Montezuma Revenge, a known difficult
task for reinforcement learning. Finally, we propose an extension that
maximizes information content in a discretely compressed latent space which
boosts sample efficiency and generalizes to continuous state spaces.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06778" title="Abstract">arXiv:2310.06778</a> [<a href="/pdf/2310.06778" title="Download PDF">pdf</a>, <a href="/format/2310.06778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Knowledge Workers Think Generative AI Will (Not) Transform Their  Industries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+A">Allison Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Shelby%2C+R">Renee Shelby</a>, 
<a href="/search/cs?searchtype=author&query=Kelley%2C+P+G">Patrick Gage Kelley</a>, 
<a href="/search/cs?searchtype=author&query=Rousso-Schindler%2C+S">Steven Rousso-Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Smith-Loud%2C+J">Jamila Smith-Loud</a>, 
<a href="/search/cs?searchtype=author&query=Wilcox%2C+L">Lauren Wilcox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 5 tables, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Generative AI is expected to have transformative effects in multiple
knowledge industries. To better understand how knowledge workers expect
generative AI may affect their industries in the future, we conducted
participatory research workshops for seven different industries, with a total
of 54 participants across three US cities. We describe participants'
expectations of generative AI's impact, including a dominant narrative that cut
across the groups' discourse: participants largely envision generative AI as a
tool to perform menial work, under human review. Participants do not generally
anticipate the disruptive changes to knowledge industries currently projected
in common media and academic narratives. Participants do however envision
generative AI may amplify four social forces currently shaping their
industries: deskilling, dehumanization, disconnection, and disinformation. We
describe these forces, and then we provide additional detail regarding
attitudes in specific knowledge industries. We conclude with a discussion of
implications and research challenges for the HCI community.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06779" title="Abstract">arXiv:2310.06779</a> [<a href="/pdf/2310.06779" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Supervised Embedding and Clustering Anomaly Detection method for  classification of Mobile Network Faults
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mosayebi%2C+R">R. Mosayebi</a>, 
<a href="/search/cs?searchtype=author&query=Kia%2C+H">H. Kia</a>, 
<a href="/search/cs?searchtype=author&query=Raki%2C+A+K">A. Kianpour Raki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The paper introduces Supervised Embedding and Clustering Anomaly Detection
(SEMC-AD), a method designed to efficiently identify faulty alarm logs in a
mobile network and alleviate the challenges of manual monitoring caused by the
growing volume of alarm logs. SEMC-AD employs a supervised embedding approach
based on deep neural networks, utilizing historical alarm logs and their labels
to extract numerical representations for each log, effectively addressing the
issue of imbalanced classification due to a small proportion of anomalies in
the dataset without employing one-hot encoding. The robustness of the embedding
is evaluated by plotting the two most significant principle components of the
embedded alarm logs, revealing that anomalies form distinct clusters with
similar embeddings. Multivariate normal Gaussian clustering is then applied to
these components, identifying clusters with a high ratio of anomalies to normal
alarms (above 90%) and labeling them as the anomaly group. To classify new
alarm logs, we check if their embedded vectors' two most significant principle
components fall within the anomaly-labeled clusters. If so, the log is
classified as an anomaly. Performance evaluation demonstrates that SEMC-AD
outperforms conventional random forest and gradient boosting methods without
embedding. SEMC-AD achieves 99% anomaly detection, whereas random forest and
XGBoost only detect 86% and 81% of anomalies, respectively. While supervised
classification methods may excel in labeled datasets, the results demonstrate
that SEMC-AD is more efficient in classifying anomalies in datasets with
numerous categorical features, significantly enhancing anomaly detection,
reducing operator burden, and improving network maintenance.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06786" title="Abstract">arXiv:2310.06786</a> [<a href="/pdf/2310.06786" title="Download PDF">pdf</a>, <a href="/format/2310.06786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paster%2C+K">Keiran Paster</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+M+D">Marco Dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Azerbayev%2C+Z">Zhangir Azerbayev</a>, 
<a href="/search/cs?searchtype=author&query=Ba%2C+J">Jimmy Ba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">There is growing evidence that pretraining on high quality, carefully
thought-out tokens such as code or mathematics plays an important role in
improving the reasoning abilities of large language models. For example,
Minerva, a PaLM model finetuned on billions of tokens of mathematical documents
from arXiv and the web, reported dramatically improved performance on problems
that require quantitative reasoning. However, because all known open source web
datasets employ preprocessing that does not faithfully preserve mathematical
notation, the benefits of large scale training on quantitive web documents are
unavailable to the research community. We introduce OpenWebMath, an open
dataset inspired by these works containing 14.7B tokens of mathematical
webpages from Common Crawl. We describe in detail our method for extracting
text and LaTeX content and removing boilerplate from HTML documents, as well as
our methods for quality filtering and deduplication. Additionally, we run
small-scale experiments by training 1.4B parameter language models on
OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass
the performance of models trained on over 20x the amount of general language
data. We hope that our dataset, openly released on the Hugging Face Hub, will
help spur advances in the reasoning abilities of large language models.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06790" title="Abstract">arXiv:2310.06790</a> [<a href="/pdf/2310.06790" title="Download PDF">pdf</a>, <a href="/format/2310.06790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with  Automatic Differentiation: Koopman and Neural ODE Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Constante-Amores%2C+C+R">C. Ricardo Constante-Amores</a>, 
<a href="/search/cs?searchtype=author&query=Linot%2C+A+J">Alec J. Linot</a>, 
<a href="/search/cs?searchtype=author&query=Graham%2C+M+D">Michael D. Graham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Data-driven approximations of the Koopman operator are promising for
predicting the time evolution of systems characterized by complex dynamics.
Among these methods, the approach known as extended dynamic mode decomposition
with dictionary learning (EDMD-DL) has garnered significant attention. Here we
present a modification of EDMD-DL that concurrently determines both the
dictionary of observables and the corresponding approximation of the Koopman
operator. This innovation leverages automatic differentiation to facilitate
gradient descent computations through the pseudoinverse. We also address the
performance of several alternative methodologies. We assess a 'pure' Koopman
approach, which involves the direct time-integration of a linear,
high-dimensional system governing the dynamics within the space of observables.
Additionally, we explore a modified approach where the system alternates
between spaces of states and observables at each time step -- this approach no
longer satisfies the linearity of the true Koopman operator representation. For
further comparisons, we also apply a state space approach (neural ODEs). We
consider systems encompassing two and three-dimensional ordinary differential
equation systems featuring steady, oscillatory, and chaotic attractors, as well
as partial differential equations exhibiting increasingly complex and intricate
behaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the
state space approach offers superior performance compared to the 'pure' Koopman
approach where the entire time evolution occurs in the space of observables.
When the temporal evolution of the Koopman approach alternates between states
and observables at each time step, however, its predictions become comparable
to those of the state space approach.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06793" title="Abstract">arXiv:2310.06793</a> [<a href="/pdf/2310.06793" title="Download PDF">pdf</a>, <a href="/ps/2310.06793" title="Download PostScript">ps</a>, <a href="/format/2310.06793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stojanovic%2C+S">Stefan Stojanovic</a>, 
<a href="/search/cs?searchtype=author&query=Jedra%2C+Y">Yassir Jedra</a>, 
<a href="/search/cs?searchtype=author&query=Proutiere%2C+A">Alexandre Proutiere</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study matrix estimation problems arising in reinforcement learning (RL)
with low-rank structure. In low-rank bandits, the matrix to be recovered
specifies the expected arm rewards, and for low-rank Markov Decision Processes
(MDPs), it may for example characterize the transition kernel of the MDP. In
both cases, each entry of the matrix carries important information, and we seek
estimation methods with low entry-wise error. Importantly, these methods
further need to accommodate for inherent correlations in the available data
(e.g. for MDPs, the data consists of system trajectories). We investigate the
performance of simple spectral-based matrix estimation approaches: we show that
they efficiently recover the singular subspaces of the matrix and exhibit
nearly-minimal entry-wise error. These new results on low-rank matrix
estimation make it possible to devise reinforcement learning algorithms that
fully exploit the underlying low-rank structure. We provide two examples of
such algorithms: a regret minimization algorithm for low-rank bandit problems,
and a best policy identification algorithm for reward-free RL in low-rank MDPs.
Both algorithms yield state-of-the-art performance guarantees.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06794" title="Abstract">arXiv:2310.06794</a> [<a href="/pdf/2310.06794" title="Download PDF">pdf</a>, <a href="/format/2310.06794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $f$-Policy Gradients: A General Framework for Goal Conditioned RL using  $f$-Divergences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Siddhant Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Durugkar%2C+I">Ishan Durugkar</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Goal-Conditioned Reinforcement Learning (RL) problems often have access to
sparse rewards where the agent receives a reward signal only when it has
achieved the goal, making policy optimization a difficult problem. Several
works augment this sparse reward with a learned dense reward function, but this
can lead to sub-optimal policies if the reward is misaligned. Moreover, recent
works have demonstrated that effective shaping rewards for a particular problem
can depend on the underlying learning algorithm. This paper introduces a novel
way to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG
minimizes the f-divergence between the agent's state visitation distribution
and the goal, which we show can lead to an optimal policy. We derive gradients
for various f-divergences to optimize this objective. Our learning paradigm
provides dense learning signals for exploration in sparse reward settings. We
further introduce an entropy-regularized policy optimization objective, that we
call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective.
We show that several metric-based shaping rewards like L2 can be used with
$s$-MaxEnt RL, providing a common ground to study such metric-based shaping
rewards with efficient exploration. We find that $f$-PG has better performance
compared to standard policy gradient methods on a challenging gridworld as well
as the Point Maze and FetchReach environments. More information on our website
https://agarwalsiddhant10.github.io/projects/fpg.html.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06800" title="Abstract">arXiv:2310.06800</a> [<a href="/pdf/2310.06800" title="Download PDF">pdf</a>, <a href="/format/2310.06800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test &amp; Evaluation Best Practices for Machine Learning-Enabled Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+J">Jaganmohan Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Cody%2C+T">Tyler Cody</a>, 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+N">Nicola McCarthy</a>, 
<a href="/search/cs?searchtype=author&query=Lanus%2C+E">Erin Lanus</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+L">Laura Freeman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning (ML) - based software systems are rapidly gaining adoption
across various domains, making it increasingly essential to ensure they perform
as intended. This report presents best practices for the Test and Evaluation
(T&amp;E) of ML-enabled software systems across its lifecycle. We categorize the
lifecycle of ML-enabled software systems into three stages: component,
integration and deployment, and post-deployment. At the component level, the
primary objective is to test and evaluate the ML model as a standalone
component. Next, in the integration and deployment stage, the goal is to
evaluate an integrated ML-enabled system consisting of both ML and non-ML
components. Finally, once the ML-enabled software system is deployed and
operationalized, the T&amp;E objective is to ensure the system performs as
intended. Maintenance activities for ML-enabled software systems span the
lifecycle and involve maintaining various assets of ML-enabled software
systems.
<br />Given its unique characteristics, the T&amp;E of ML-enabled software systems is
challenging. While significant research has been reported on T&amp;E at the
component level, limited work is reported on T&amp;E in the remaining two stages.
Furthermore, in many cases, there is a lack of systematic T&amp;E strategies
throughout the ML-enabled system's lifecycle. This leads practitioners to
resort to ad-hoc T&amp;E practices, which can undermine user confidence in the
reliability of ML-enabled software systems. New systematic testing approaches,
adequacy measurements, and metrics are required to address the T&amp;E challenges
across all stages of the ML-enabled system lifecycle.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06801" title="Abstract">arXiv:2310.06801</a> [<a href="/pdf/2310.06801" title="Download PDF">pdf</a>, <a href="/format/2310.06801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Factorized Q-Learning for Cooperative Multi-agent Imitation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
The <a href="/search/cs?searchtype=author&query=Bui%2C+V">Viet Bui</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+T">Tien Mai</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+H">Thanh Hong Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This paper concerns imitation learning (IL) (i.e, the problem of learning to
mimic expert behaviors from demonstrations) in cooperative multi-agent systems.
The learning problem under consideration poses several challenges,
characterized by high-dimensional state and action spaces and intricate
inter-agent dependencies. In a single-agent setting, IL has proven to be done
efficiently through an inverse soft-Q learning process given expert
demonstrations. However, extending this framework to a multi-agent context
introduces the need to simultaneously learn both local value functions to
capture local observations and individual actions, and a joint value function
for exploiting centralized learning. In this work, we introduce a novel
multi-agent IL algorithm designed to address these challenges. Our approach
enables the centralized learning by leveraging mixing networks to aggregate
decentralized Q functions. A main advantage of this approach is that the
weights of the mixing networks can be trained using information derived from
global states. We further establish conditions for the mixing networks under
which the multi-agent objective function exhibits convexity within the Q
function space. We present extensive experiments conducted on some challenging
competitive and cooperative multi-agent game environments, including an
advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2), which
demonstrates the effectiveness of our proposed algorithm compared to existing
state-of-the-art multi-agent IL algorithms.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06803" title="Abstract">arXiv:2310.06803</a> [<a href="/pdf/2310.06803" title="Download PDF">pdf</a>, <a href="/format/2310.06803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Transformer&#x27;s Capabilities in Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yunqiu Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hanyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yulun Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in general purpose pre-trained language models have shown
great potential in commonsense reasoning. However, current works still perform
poorly on standard commonsense reasoning benchmarks including the Com2Sense
Dataset. We argue that this is due to a disconnect with current cutting-edge
machine learning methods. In this work, we aim to bridge the gap by introducing
current ML-based methods to improve general purpose pre-trained language models
in the task of commonsense reasoning. Specifically, we experiment with and
systematically evaluate methods including knowledge transfer, model ensemble,
and introducing an additional pairwise contrastive objective. Our best model
outperforms the strongest previous works by ~15\% absolute gains in Pairwise
Accuracy and ~8.7\% absolute gains in Standard Accuracy.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06816" title="Abstract">arXiv:2310.06816</a> [<a href="/pdf/2310.06816" title="Download PDF">pdf</a>, <a href="/format/2310.06816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Embeddings Reveal (Almost) As Much As Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morris%2C+J+X">John X. Morris</a>, 
<a href="/search/cs?searchtype=author&query=Kuleshov%2C+V">Volodymyr Kuleshov</a>, 
<a href="/search/cs?searchtype=author&query=Shmatikov%2C+V">Vitaly Shmatikov</a>, 
<a href="/search/cs?searchtype=author&query=Rush%2C+A+M">Alexander M. Rush</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">How much private information do text embeddings reveal about the original
text? We investigate the problem of embedding \textit{inversion},
reconstructing the full text represented in dense text embeddings. We frame the
problem as controlled generation: generating text that, when reembedded, is
close to a fixed point in latent space. We find that although a na\"ive model
conditioned on the embedding performs poorly, a multi-step method that
iteratively corrects and re-embeds text is able to recover $92\%$ of
$32\text{-token}$ text inputs exactly. We train our model to decode text
embeddings from two state-of-the-art embedding models, and also show that our
model can recover important personal information (full names) from a dataset of
clinical notes. Our code is available on Github:
\href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06822" title="Abstract">arXiv:2310.06822</a> [<a href="/pdf/2310.06822" title="Download PDF">pdf</a>, <a href="/format/2310.06822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Bounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+M">Michael Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+P+D">Paul D. Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Ritschel%2C+T">Tobias Ritschel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Bounding volumes are an established concept in computer graphics and vision
tasks but have seen little change since their early inception. In this work, we
study the use of neural networks as bounding volumes. Our key observation is
that bounding, which so far has primarily been considered a problem of
computational geometry, can be redefined as a problem of learning to classify
space into free and empty. This learning-based approach is particularly
advantageous in high-dimensional spaces, such as animated scenes with complex
queries, where neural networks are known to excel. However, unlocking neural
bounding requires a twist: allowing -- but also limiting -- false positives,
while ensuring that the number of false negatives is strictly zero. We enable
such tight and conservative results using a dynamically-weighted asymmetric
loss function. Our results show that our neural bounding produces up to an
order of magnitude fewer false positives than traditional methods.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06824" title="Abstract">arXiv:2310.06824</a> [<a href="/pdf/2310.06824" title="Download PDF">pdf</a>, <a href="/format/2310.06824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Geometry of Truth: Emergent Linear Structure in Large Language Model  Representations of True/False Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marks%2C+S">Samuel Marks</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have impressive capabilities, but are also prone
to outputting falsehoods. Recent work has developed techniques for inferring
whether a LLM is telling the truth by training probes on the LLM's internal
activations. However, this line of work is controversial, with some authors
pointing out failures of these probes to generalize in basic ways, among other
conceptual issues. In this work, we curate high-quality datasets of true/false
statements and use them to study in detail the structure of LLM representations
of truth, drawing on three lines of evidence: 1. Visualizations of LLM
true/false statement representations, which reveal clear linear structure. 2.
Transfer experiments in which probes trained on one dataset generalize to
different datasets. 3. Causal evidence obtained by surgically intervening in a
LLM's forward pass, causing it to treat false statements as true and vice
versa. Overall, we present evidence that language models linearly represent the
truth or falsehood of factual statements. We also introduce a novel technique,
mass-mean probing, which generalizes better and is more causally implicated in
model outputs than other probing techniques.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06825" title="Abstract">arXiv:2310.06825</a> [<a href="/pdf/2310.06825" title="Download PDF">pdf</a>, <a href="/format/2310.06825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mistral 7B
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+A+Q">Albert Q. Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sablayrolles%2C+A">Alexandre Sablayrolles</a>, 
<a href="/search/cs?searchtype=author&query=Mensch%2C+A">Arthur Mensch</a>, 
<a href="/search/cs?searchtype=author&query=Bamford%2C+C">Chris Bamford</a>, 
<a href="/search/cs?searchtype=author&query=Chaplot%2C+D+S">Devendra Singh Chaplot</a>, 
<a href="/search/cs?searchtype=author&query=de+las+Casas%2C+D">Diego de las Casas</a>, 
<a href="/search/cs?searchtype=author&query=Bressand%2C+F">Florian Bressand</a>, 
<a href="/search/cs?searchtype=author&query=Lengyel%2C+G">Gianna Lengyel</a>, 
<a href="/search/cs?searchtype=author&query=Lample%2C+G">Guillaume Lample</a>, 
<a href="/search/cs?searchtype=author&query=Saulnier%2C+L">Lucile Saulnier</a>, 
<a href="/search/cs?searchtype=author&query=Lavaud%2C+L+R">L&#xe9;lio Renard Lavaud</a>, 
<a href="/search/cs?searchtype=author&query=Lachaux%2C+M">Marie-Anne Lachaux</a>, 
<a href="/search/cs?searchtype=author&query=Stock%2C+P">Pierre Stock</a>, 
<a href="/search/cs?searchtype=author&query=Scao%2C+T+L">Teven Le Scao</a>, 
<a href="/search/cs?searchtype=author&query=Lavril%2C+T">Thibaut Lavril</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Thomas Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lacroix%2C+T">Timoth&#xe9;e Lacroix</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+W+E">William El Sayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Models and code are available at <a href="https://mistral.ai/news/announcing-mistral-7b/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered
for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B
across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and
code generation. Our model leverages grouped-query attention (GQA) for faster
inference, coupled with sliding window attention (SWA) to effectively handle
sequences of arbitrary length with a reduced inference cost. We also provide a
model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses
the Llama 2 13B -- Chat model both on human and automated benchmarks. Our
models are released under the Apache 2.0 license.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06827" title="Abstract">arXiv:2310.06827</a> [<a href="/pdf/2310.06827" title="Download PDF">pdf</a>, <a href="/format/2310.06827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teaching Language Models to Hallucinate Less with Synthetic Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jones%2C+E">Erik Jones</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B5es%2C+C">Clarisse Sim&#xf5;es</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+V">Varun Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhabrata Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+A">Arindam Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+A">Ahmed Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Kamar%2C+E">Ece Kamar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) frequently hallucinate on abstractive
summarization tasks such as document-based question-answering, meeting
summarization, and clinical report generation, even though all necessary
information is included in context. However, optimizing LLMs to hallucinate
less on these tasks is challenging, as hallucination is hard to efficiently
evaluate at each optimization step. In this work, we show that reducing
hallucination on a synthetic task can also reduce hallucination on real-world
downstream tasks. Our method, SynTra, first designs a synthetic task where
hallucinations are easy to elicit and measure. It next optimizes the LLM's
system message via prefix-tuning on the synthetic task, and finally transfers
the system message to realistic, hard-to-optimize tasks. Across three realistic
abstractive summarization tasks, SynTra reduces hallucination for two
13B-parameter LLMs using only a synthetic retrieval task for supervision. We
also find that optimizing the system message rather than the model weights can
be critical; fine-tuning the entire model on the synthetic task can
counterintuitively increase hallucination. Overall, SynTra demonstrates that
the extra flexibility of working with synthetic data can help mitigate
undesired behaviors in practice.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06828" title="Abstract">arXiv:2310.06828</a> [<a href="/pdf/2310.06828" title="Download PDF">pdf</a>, <a href="/format/2310.06828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoboHive: A Unified Framework for Robot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vikash Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R">Rutav Shah</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Gaoyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+V">Vincent Moens</a>, 
<a href="/search/cs?searchtype=author&query=Caggiano%2C+V">Vittorio Caggiano</a>, 
<a href="/search/cs?searchtype=author&query=Vakil%2C+J">Jay Vakil</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Rajeswaran%2C+A">Aravind Rajeswaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present RoboHive, a comprehensive software platform and ecosystem for
research in the field of Robot Learning and Embodied Artificial Intelligence.
Our platform encompasses a diverse range of pre-existing and novel
environments, including dexterous manipulation with the Shadow Hand, whole-arm
manipulation tasks with Franka and Fetch robots, quadruped locomotion, among
others. Included environments are organized within and cover multiple domains
such as hand manipulation, locomotion, multi-task, multi-agent, muscles, etc.
In comparison to prior works, RoboHive offers a streamlined and unified task
interface taking dependency on only a minimal set of well-maintained packages,
features tasks with high physics fidelity and rich visual diversity, and
supports common hardware drivers for real-world deployment. The unified
interface of RoboHive offers a convenient and accessible abstraction for
algorithmic research in imitation, reinforcement, multi-task, and hierarchical
learning. Furthermore, RoboHive includes expert demonstrations and baseline
results for most environments, providing a standard for benchmarking and
comparisons. Details: https://sites.google.com/view/robohive
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06830" title="Abstract">arXiv:2310.06830</a> [<a href="/pdf/2310.06830" title="Download PDF">pdf</a>, <a href="/format/2310.06830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lemur: Harmonizing Natural Language and Code for Language Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yiheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hongjin Su</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+C">Chen Xing</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+B">Boyu Mi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Binyuan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yitao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tianbao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhoujun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Siheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce Lemur and Lemur-Chat, openly accessible language models
optimized for both natural language and coding capabilities to serve as the
backbone of versatile language agents. The evolution from language chat models
to functional language agents demands that models not only master human
interaction, reasoning, and planning but also ensure grounding in the relevant
environments. This calls for a harmonious blend of language and coding
capabilities in the models. Lemur and Lemur-Chat are proposed to address this
necessity, demonstrating balanced proficiencies in both domains, unlike
existing open-source models that tend to specialize in either. Through
meticulous pre-training using a code-intensive corpus and instruction
fine-tuning on text and code data, our models achieve state-of-the-art averaged
performance across diverse text and coding benchmarks among open-source models.
Comprehensive experiments demonstrate Lemur's superiority over existing
open-source models and its proficiency across various agent tasks involving
human communication, tool usage, and interaction under fully- and partially-
observable environments. The harmonization between natural and programming
languages enables Lemur-Chat to significantly narrow the gap with proprietary
models on agent abilities, providing key insights into developing advanced
open-source agents adept at reasoning, planning, and operating seamlessly
across environments. https://github.com/OpenLemur/Lemur
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06835" title="Abstract">arXiv:2310.06835</a> [<a href="/pdf/2310.06835" title="Download PDF">pdf</a>, <a href="/format/2310.06835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherji%2C+K">Kaustuv Mukherji</a>, 
<a href="/search/cs?searchtype=author&query=Parkar%2C+D">Devendra Parkar</a>, 
<a href="/search/cs?searchtype=author&query=Pokala%2C+L">Lahari Pokala</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+D">Dyuman Aditya</a>, 
<a href="/search/cs?searchtype=author&query=Shakarian%2C+P">Paulo Shakarian</a>, 
<a href="/search/cs?searchtype=author&query=Dorman%2C+C">Clark Dorman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE International Conference on Semantic Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Recent advances in reinforcement learning (RL) have shown much promise across
a variety of applications. However, issues such as scalability, explainability,
and Markovian assumptions limit its applicability in certain domains. We
observe that many of these shortcomings emanate from the simulator as opposed
to the RL training algorithms themselves. As such, we propose a semantic proxy
for simulation based on a temporal extension to annotated logic. In comparison
with two high-fidelity simulators, we show up to three orders of magnitude
speed-up while preserving the quality of policy learned in addition to showing
the ability to model and leverage non-Markovian dynamics and instantaneous
actions while providing an explainable trace describing the outcomes of the
agent actions.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06836" title="Abstract">arXiv:2310.06836</a> [<a href="/pdf/2310.06836" title="Download PDF">pdf</a>, <a href="/format/2310.06836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Does Stable Diffusion Know about the 3D Scene?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+G">Guanqi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanxia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zisserman%2C+A">Andrew Zisserman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advances in generative models like Stable Diffusion enable the
generation of highly photo-realistic images. Our objective in this paper is to
probe the diffusion network to determine to what extent it 'understands'
different properties of the 3D scene depicted in an image. To this end, we make
the following contributions: (i) We introduce a protocol to evaluate whether a
network models a number of physical 'properties' of the 3D scene by probing for
explicit features that represent these properties. The probes are applied on
datasets of real images with annotations for the property. (ii) We apply this
protocol to properties covering scene geometry, scene material, support
relations, lighting, and view dependent measures. (iii) We find that Stable
Diffusion is good at a number of properties including scene geometry, support
relations, shadows and depth, but less performant for occlusion. (iv) We also
apply the probes to other models trained at large-scale, including DINO and
CLIP, and find their performance inferior to that of Stable Diffusion.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06837" title="Abstract">arXiv:2310.06837</a> [<a href="/pdf/2310.06837" title="Download PDF">pdf</a>, <a href="/format/2310.06837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating and Evaluating Tests for K-12 Students with Language Model  Simulations: A Case Study on Sentence Reading Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zelikman%2C+E">Eric Zelikman</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W+A">Wanjing Anya Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+J+E">Jasmine E. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yeatman%2C+J+D">Jason D. Yeatman</a>, 
<a href="/search/cs?searchtype=author&query=Haber%2C+N">Nick Haber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Developing an educational test can be expensive and time-consuming, as each
item must be written by experts and then evaluated by collecting hundreds of
student responses. Moreover, many tests require multiple distinct sets of
questions administered throughout the school year to closely monitor students'
progress, known as parallel tests. In this study, we focus on tests of silent
sentence reading efficiency, used to assess students' reading ability over
time. To generate high-quality parallel tests, we propose to fine-tune large
language models (LLMs) to simulate how previous students would have responded
to unseen items. With these simulated responses, we can estimate each item's
difficulty and ambiguity. We first use GPT-4 to generate new test items
following a list of expert-developed rules and then apply a fine-tuned LLM to
filter the items based on criteria from psychological measurements. We also
propose an optimal-transport-inspired technique for generating parallel tests
and show the generated tests closely correspond to the original test's
difficulty and reliability based on crowdworker responses. Our evaluation of a
generated test with 234 students from grades 2 to 8 produces test scores highly
correlated (r=0.93) to those of a standard test form written by human experts
and evaluated across thousands of K-12 students.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06838" title="Abstract">arXiv:2310.06838</a> [<a href="/pdf/2310.06838" title="Download PDF">pdf</a>, <a href="/format/2310.06838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tengda Han</a>, 
<a href="/search/cs?searchtype=author&query=Bain%2C+M">Max Bain</a>, 
<a href="/search/cs?searchtype=author&query=Nagrani%2C+A">Arsha Nagrani</a>, 
<a href="/search/cs?searchtype=author&query=Varol%2C+G">G&#xfc;l Varol</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zisserman%2C+A">Andrew Zisserman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICCV2023. Project page: <a href="https://www.robots.ox.ac.uk/vgg/research/autoad/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Audio Description (AD) is the task of generating descriptions of visual
content, at suitable time intervals, for the benefit of visually impaired
audiences. For movies, this presents notable challenges -- AD must occur only
during existing pauses in dialogue, should refer to characters by name, and
ought to aid understanding of the storyline as a whole. To this end, we develop
a new model for automatically generating movie AD, given CLIP visual features
of the frames, the cast list, and the temporal locations of the speech;
addressing all three of the 'who', 'when', and 'what' questions: (i) who -- we
introduce a character bank consisting of the character's name, the actor that
played the part, and a CLIP feature of their face, for the principal cast of
each movie, and demonstrate how this can be used to improve naming in the
generated AD; (ii) when -- we investigate several models for determining
whether an AD should be generated for a time interval or not, based on the
visual content of the interval and its neighbours; and (iii) what -- we
implement a new vision-language model for this task, that can ingest the
proposals from the character bank, whilst conditioning on the visual features
using cross-attention, and demonstrate how this improves over previous
architectures for AD text generation in an apples-to-apples comparison.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06839" title="Abstract">arXiv:2310.06839</a> [<a href="/pdf/2310.06839" title="Download PDF">pdf</a>, <a href="/format/2310.06839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios  via Prompt Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Huiqiang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qianhui Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xufang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chin-Yew Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lili Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In long context scenarios, large language models (LLMs) face three main
challenges: higher computational/financial cost, longer latency, and inferior
performance. Some studies reveal that the performance of LLMs depends on both
the density and the position of the key information (question relevant) in the
input prompt. Inspired by these findings, we propose LongLLMLingua for prompt
compression towards improving LLMs' perception of the key information to
simultaneously address the three challenges. We conduct evaluation on a wide
range of long context scenarios including single-/multi-document QA, few-shot
learning, summarization, synthetic tasks, and code completion. The experimental
results show that LongLLMLingua compressed prompt can derive higher performance
with much less cost. The latency of the end-to-end system is also reduced. For
example, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost
of up to 17.1% over the original prompt with ~4x fewer tokens as input to
GPT-3.5-Turbo. It can derive cost savings of \$28.5 and \$27.4 per 1,000
samples from the LongBench and ZeroScrolls benchmark, respectively.
Additionally, when compressing prompts of ~10k tokens at a compression rate of
2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our
code is available at https://aka.ms/LLMLingua.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed, 11 Oct 23</h3>
<dl>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.14240" title="Abstract">arXiv:2204.14240</a> (cross-list from eess.IV) [<a href="/pdf/2204.14240" title="Download PDF">pdf</a>, <a href="/format/2204.14240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EndoMapper dataset of complete calibrated endoscopy procedures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Azagra%2C+P">Pablo Azagra</a>, 
<a href="/search/eess?searchtype=author&query=Sostres%2C+C">Carlos Sostres</a>, 
<a href="/search/eess?searchtype=author&query=Ferrandez%2C+%C3%81">&#xc1;ngel Ferrandez</a>, 
<a href="/search/eess?searchtype=author&query=Riazuelo%2C+L">Luis Riazuelo</a>, 
<a href="/search/eess?searchtype=author&query=Tomasini%2C+C">Clara Tomasini</a>, 
<a href="/search/eess?searchtype=author&query=Barbed%2C+O+L">Oscar Le&#xf3;n Barbed</a>, 
<a href="/search/eess?searchtype=author&query=Morlana%2C+J">Javier Morlana</a>, 
<a href="/search/eess?searchtype=author&query=Recasens%2C+D">David Recasens</a>, 
<a href="/search/eess?searchtype=author&query=Batlle%2C+V+M">Victor M. Batlle</a>, 
<a href="/search/eess?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+J+J">Juan J. G&#xf3;mez-Rodr&#xed;guez</a>, 
<a href="/search/eess?searchtype=author&query=Elvira%2C+R">Richard Elvira</a>, 
<a href="/search/eess?searchtype=author&query=L%C3%B3pez%2C+J">Julia L&#xf3;pez</a>, 
<a href="/search/eess?searchtype=author&query=Oriol%2C+C">Cristina Oriol</a>, 
<a href="/search/eess?searchtype=author&query=Civera%2C+J">Javier Civera</a>, 
<a href="/search/eess?searchtype=author&query=Tard%C3%B3s%2C+J+D">Juan D. Tard&#xf3;s</a>, 
<a href="/search/eess?searchtype=author&query=Murillo%2C+A+C">Ana Cristina Murillo</a>, 
<a href="/search/eess?searchtype=author&query=Lanas%2C+A">Angel Lanas</a>, 
<a href="/search/eess?searchtype=author&query=Montiel%2C+J+M+M">Jos&#xe9; M.M. Montiel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 14 figures, 8 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sci Data 10, 671 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Computer-assisted systems are becoming broadly used in medicine. In
endoscopy, most research focuses on the automatic detection of polyps or other
pathologies, but localization and navigation of the endoscope are completely
performed manually by physicians. To broaden this research and bring spatial
Artificial Intelligence to endoscopies, data from complete procedures is
needed. This paper introduces the Endomapper dataset, the first collection of
complete endoscopy sequences acquired during regular medical practice, making
secondary use of medical data. Its main purpose is to facilitate the
development and evaluation of Visual Simultaneous Localization and Mapping
(VSLAM) methods in real endoscopy data. The dataset contains more than 24 hours
of video. It is the first endoscopic dataset that includes endoscope
calibration as well as the original calibration videos. Meta-data and
annotations associated with the dataset vary from the anatomical landmarks,
procedure labeling, segmentations, reconstructions, simulated sequences with
ground truth and same patient procedures. The software used in this paper is
publicly available.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10243" title="Abstract">arXiv:2302.10243</a> (cross-list from physics.plasm-ph) [<a href="/pdf/2302.10243" title="Download PDF">pdf</a>, <a href="/format/2302.10243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mallat Scattering Transformation based surrogate for  MagnetoHydroDynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Glinsky%2C+M+E">Michael E. Glinsky</a>, 
<a href="/search/physics?searchtype=author&query=Maupin%2C+K">Kathryn Maupin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 20 figures, 3 animations, accepted for publication in Computational Mechanics
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computational Mechanics 72, 291 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">A Machine and Deep Learning methodology is developed and applied to give a
high fidelity, fast surrogate for 2D resistive MHD simulations of MagLIF
implosions. The resistive MHD code GORGON is used to generate an ensemble of
implosions with different liner aspect ratios, initial gas preheat temperatures
(that is, different adiabats), and different liner perturbations. The liner
density and magnetic field as functions of $x$, $y$, and $t$ were generated.
The Mallat Scattering Transformation (MST) is taken of the logarithm of both
fields and a Principal Components Analysis is done on the logarithm of the MST
of both fields. The fields are projected onto the PCA vectors and a small
number of these PCA vector components are kept. Singular Value Decompositions
of the cross correlation of the input parameters to the output logarithm of the
MST of the fields, and of the cross correlation of the SVD vector components to
the PCA vector components are done. This allows the identification of the PCA
vectors vis-a-vis the input parameters. Finally, a Multi Layer Perceptron
neural network with ReLU activation and a simple three layer encoder/decoder
architecture is trained on this dataset to predict the PCA vector components of
the fields as a function of time. Details of the implosion, stagnation, and the
disassembly are well captured. Examination of the PCA vectors and a permutation
importance analysis of the MLP show definitive evidence of an inverse turbulent
cascade into a dipole emergent behavior. The orientation of the dipole is set
by the initial liner perturbation. The analysis is repeated with a version of
the MST which includes phase, called Wavelet Phase Harmonics (WPH). While WPH
do not give the physical insight of the MST, they can and are inverted to give
field configurations as a function of time, including field-to-field
correlations.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04986" title="Abstract">arXiv:2310.04986</a> (cross-list from econ.TH) [<a href="/pdf/2310.04986" title="Download PDF">pdf</a>, <a href="/format/2310.04986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new economic and financial theory of money
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Glinsky%2C+M+E">Michael E. Glinsky</a>, 
<a href="/search/econ?searchtype=author&query=Sievert%2C+S">Sharon Sievert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 31 figures, 157 equations, to be submitted to Journal of Economic Affairs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Artificial Intelligence (cs.AI); Classical Physics (physics.class-ph)

</div>
<p class="mathjax">This paper fundamentally reformulates economic and financial theory to
include electronic currencies. The valuation of the electronic currencies will
be based on macroeconomic theory and the fundamental equation of monetary
policy, not the microeconomic theory of discounted cash flows. The view of
electronic currency as a transactional equity associated with tangible assets
of a sub-economy will be developed, in contrast to the view of stock as an
equity associated mostly with intangible assets of a sub-economy. The view will
be developed of the electronic currency management firm as an entity
responsible for coordinated monetary (electronic currency supply and value
stabilization) and fiscal (investment and operational) policies of a
substantial (for liquidity of the electronic currency) sub-economy. The risk
model used in the valuations and the decision-making will not be the
ubiquitous, yet inappropriate, exponential risk model that leads to discount
rates, but will be multi time scale models that capture the true risk. The
decision-making will be approached from the perspective of true systems control
based on a system response function given by the multi scale risk model and
system controllers that utilize the Deep Reinforcement Learning, Generative
Pretrained Transformers, and other methods of Artificial Intelligence
(DRL/GPT/AI). Finally, the sub-economy will be viewed as a nonlinear complex
physical system with both stable equilibriums that are associated with
short-term exploitation, and unstable equilibriums that need to be stabilized
with active nonlinear control based on the multi scale system response
functions and DRL/GPT/AI.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05926" title="Abstract">arXiv:2310.05926</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.05926" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The using of bibliometric analysis to classify trends and future  directions on &#x27;&#x27;Smart Farm&#x27;&#x27;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Suebsombut%2C+P">Paweena Suebsombut</a> (CMU, UL2), 
<a href="/search/physics?searchtype=author&query=Sekhari%2C+A">Aicha Sekhari</a> (UL2, DISP, IUT Lumi&#xe8;re), 
<a href="/search/physics?searchtype=author&query=Sureepong%2C+P">Pradorn Sureepong</a> (CMU), 
<a href="/search/physics?searchtype=author&query=Ueasangkomsate%2C+P">Pittawat Ueasangkomsate</a> (KU), 
<a href="/search/physics?searchtype=author&query=Bouras%2C+A">Abdelaziz Bouras</a> (UL2)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2017 International Conference on Digital Arts, Media and
  Technology (ICDAMT), Chiang Mai University, Mar 2017, Chiang Mai, Thailand.
  pp.136-141
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Climate change has affected the cultivation in all countries with extreme
drought, flooding, higher temperature, and changes in the season thus leaving
behind the uncontrolled production. Consequently, the smart farm has become
part of the crucial trend that is needed for application in certain farm areas.
The aims of smart farm are to control and to enhance food production and
productivity, and to increase farmers' profits. The advantages in applying
smart farm will improve the quality of production, supporting the farm workers,
and better utilization of resources. This study aims to explore the research
trends and identify research clusters on smart farm using bibliometric analysis
that has supported farming to improve the quality of farm production. The
bibliometric analysis is the method to explore the relationship of the articles
from a co-citation network of the articles and then science mapping is used to
identify clusters in the relationship. This study examines the selected
research articles in the smart farm field. The area of re search in smart farm
is categorized into two clusters that are soil carbon e mission from farming
activity, food security and farm management by using a VO Sviewer tool with
keywords related to re search articles on smart farm, agriculture, supply
chain, knowledge management, traceability, and product lifecycle management
from Web of Science (WOS) and Scopus online database. The major cluster of
smart farm research is the soil carbon emission from farming activity which
impacts on climate change that affects food production and productivity. The
contribution is to identify the trends on smart farm to develop research in the
future by means of bibliometric analysis.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05927" title="Abstract">arXiv:2310.05927</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.05927" title="Download PDF">pdf</a>, <a href="/format/2310.05927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The evolution of cooperation in a mobile population on random networks:  Network topology matters only for low-degree networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Erovenko%2C+I+V">Igor V. Erovenko</a>, 
<a href="/search/physics?searchtype=author&query=Broom%2C+M">Mark Broom</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We consider a finite structured population of mobile individuals that
strategically explore a network using a Markov movement model and interact with
each other via a public goods game. We extend the model of Erovenko et al.
(2019) from complete, circle, and star graphs to various random networks to
further investigate the effect of network topology on the evolution of
cooperation. We discover that the network topology affects the outcomes of the
evolutionary process only for networks of small average degree. Once the degree
becomes sufficiently high, the outcomes match those for the complete graph. The
actual value of the degree when this happens is much smaller than that of the
complete graph, and the threshold value depends on other network
characteristics.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05928" title="Abstract">arXiv:2310.05928</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.05928" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Ecology of Marriage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=David-Barrett%2C+T">Tamas David-Barrett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI); Theoretical Economics (econ.TH); Populations and Evolution (q-bio.PE); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The practice of marriage is an understudied phenomenon in behavioural
sciences despite being ubiquitous across human cultures. This modelling paper
shows that replacing distant direct kin with in-laws increases the
interconnectedness of the family social network graph, which allows more
cooperative and larger groups. In this framing, marriage can be seen as a
social technology that reduces free-riding within collaborative group. This
approach offers a solution to the puzzle of why our species has this particular
form of regulating mating behaviour, uniquely among pair-bonded animals.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05954" title="Abstract">arXiv:2310.05954</a> (cross-list from physics.app-ph) [<a href="/pdf/2310.05954" title="Download PDF">pdf</a>, <a href="/format/2310.05954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization of Raman amplifiers: a comparison between black-, grey- and  white-box modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Yankov%2C+M+P">Metodi P. Yankov</a>, 
<a href="/search/physics?searchtype=author&query=Soltani%2C+M">Mehran Soltani</a>, 
<a href="/search/physics?searchtype=author&query=Carena%2C+A">Andrea Carena</a>, 
<a href="/search/physics?searchtype=author&query=Zibar%2C+D">Darko Zibar</a>, 
<a href="/search/physics?searchtype=author&query=Da+Ros%2C+F">Francesco Da Ros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Optics (physics.optics)

</div>
<p class="mathjax">Designing and optimizing optical amplifiers to maximize system performance is
becoming increasingly important as optical communication systems strive to
increase throughput. Offline optimization of optical amplifiers relies on
models ranging from white-box models deeply rooted in physics to black-box
data-driven physics-agnostic models. Here, we compare the capabilities of
white-, grey- and black-box models to achieve a target frequency-distance
amplification in a bidirectional Raman amplifier. We show that any of the
studied methods can achieve down to 1 dB of frequency-distance flatness over
the C-band in a 100-km span. Then, we discuss the models' applicability,
advantages, and drawbacks based on the target application scenario, in
particular in terms of optimization speed and access to training data.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05955" title="Abstract">arXiv:2310.05955</a> (cross-list from math.OC) [<a href="/pdf/2310.05955" title="Download PDF">pdf</a>, <a href="/format/2310.05955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Quality-Diversity approaches for constrained optimization  problems with mixed continuous, discrete and categorical variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brevault%2C+L">Loic Brevault</a>, 
<a href="/search/math?searchtype=author&query=Balesdent%2C+M">Mathieu Balesdent</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Complex engineering design problems, such as those involved in aerospace,
civil, or energy engineering, require the use of numerically costly simulation
codes in order to predict the behavior and performance of the system to be
designed. To perform the design of the systems, these codes are often embedded
into an optimization process to provide the best design while satisfying the
design constraints. Recently, new approaches, called Quality-Diversity, have
been proposed in order to enhance the exploration of the design space and to
provide a set of optimal diversified solutions with respect to some feature
functions. These functions are interesting to assess trade-offs. Furthermore,
complex engineering design problems often involve mixed continuous, discrete,
and categorical design variables allowing to take into account technological
choices in the optimization problem. In this paper, a new Quality-Diversity
methodology based on mixed continuous, discrete and categorical Bayesian
optimization strategy is proposed. This approach allows to reduce the
computational cost with respect to classical Quality - Diversity approaches
while dealing with discrete choices and constraints. The performance of the
proposed method is assessed on a benchmark of analytical problems as well as on
an industrial design optimization problem dealing with aerospace systems.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05958" title="Abstract">arXiv:2310.05958</a> (cross-list from quant-ph) [<a href="/pdf/2310.05958" title="Download PDF">pdf</a>, <a href="/format/2310.05958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimising T-count is NP-hard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=van+de+Wetering%2C+J">John van de Wetering</a>, 
<a href="/search/quant-ph?searchtype=author&query=Amy%2C+M">Matt Amy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">In this short note we show that Boolean satisfiability reduces to finding the
optimal number of T gates of a quantum circuit, and hence that optimising
T-count is NP-hard.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05969" title="Abstract">arXiv:2310.05969</a> (cross-list from eess.IV) [<a href="/pdf/2310.05969" title="Download PDF">pdf</a>, <a href="/format/2310.05969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Muharram%2C+A+P">Arief Purnama Muharram</a>, 
<a href="/search/eess?searchtype=author&query=Haryono%2C+H+P">Hollyana Puteri Haryono</a>, 
<a href="/search/eess?searchtype=author&query=Juma%2C+A+H">Abassi Haji Juma</a>, 
<a href="/search/eess?searchtype=author&query=Puspasari%2C+I">Ira Puspasari</a>, 
<a href="/search/eess?searchtype=author&query=Utama%2C+N+P">Nugraha Priya Utama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented in the 2023 IEEE International Conference on Data and Software Engineering (ICoDSE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Reading and interpreting chest X-ray images is one of the most radiologist's
routines. However, it still can be challenging, even for the most experienced
ones. Therefore, we proposed a multi-model deep learning-based automated chest
X-ray report generator system designed to assist radiologists in their work.
The basic idea of the proposed system is by utilizing multi
binary-classification models for detecting multi abnormalities, with each model
responsible for detecting one abnormality, in a single image. In this study, we
limited the radiology abnormalities detection to only cardiomegaly, lung
effusion, and consolidation. The system generates a radiology report by
performing the following three steps: image pre-processing, utilizing deep
learning models to detect abnormalities, and producing a report. The aim of the
image pre-processing step is to standardize the input by scaling it to 128x128
pixels and slicing it into three segments, which covers the upper, lower, and
middle parts of the lung. After pre-processing, each corresponding model
classifies the image, resulting in a 0 (zero) for no abnormality detected and a
1 (one) for the presence of an abnormality. The prediction outputs of each
model are then concatenated to form a 'result code'. The 'result code' is used
to construct a report by selecting the appropriate pre-determined sentence for
each detected abnormality in the report generation step. The proposed system is
expected to reduce the workload of radiologists and increase the accuracy of
chest X-ray diagnosis.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05976" title="Abstract">arXiv:2310.05976</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.05976" title="Download PDF">pdf</a>, <a href="/format/2310.05976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An evolutionary model of personality traits related to cooperative  behavior using a large language model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Suzuki%2C+R">Reiji Suzuki</a>, 
<a href="/search/physics?searchtype=author&query=Arita%2C+T">Takaya Arita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA); Neural and Evolutionary Computing (cs.NE); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">This paper aims to shed light on the evolutionary dynamics of diverse and
social populations by introducing the rich expressiveness of generative models
into the trait expression of social agent-based evolutionary models.
Specifically, we focus on the evolution of personality traits in the context of
a game-theoretic relationship as a situation in which inter-individual
interests exert strong selection pressures. We construct an agent model in
which linguistic descriptions of personality traits related to cooperative
behavior are used as genes. The deterministic strategies extracted from Large
Language Model (LLM) that make behavioral decisions based on these personality
traits are used as behavioral traits. The population is evolved according to
selection based on average payoff and mutation of genes by asking LLM to
slightly modify the parent gene toward cooperative or selfish. Through
preliminary experiments and analyses, we clarify that such a model can indeed
exhibit the evolution of cooperative behavior based on the diverse and
higher-order representation of personality traits. We also observed the
repeated intrusion of cooperative and selfish personality traits through
changes in the expression of personality traits, and found that the emerging
words in the evolved gene well reflected the behavioral tendency of its
personality in terms of their semantics.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05979" title="Abstract">arXiv:2310.05979</a> (cross-list from nlin.AO) [<a href="/pdf/2310.05979" title="Download PDF">pdf</a>, <a href="/format/2310.05979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule switching mechanisms in the Game of Life with synchronous and  asynchronous updating policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Miszczak%2C+J+A">Jaros&#x142;aw Adam Miszczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 5+1 figures, code available at <a href="https://doi.org/10.5281/zenodo.8099606">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Physica Scripta, 98, 115210 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Adaptation and Self-Organizing Systems (nlin.AO)</span>; Statistical Mechanics (cond-mat.stat-mech); Multiagent Systems (cs.MA); Cellular Automata and Lattice Gases (nlin.CG)

</div>
<p class="mathjax">The emergence of complex structures in the systems governed by a simple set
of rules is among the most fascinating aspects of Nature. The particularly
powerful and versatile model suitable for investigating this phenomenon is
provided by cellular automata, with the Game of Life being one of the most
prominent examples. However, this simplified model can be too limiting in
providing a tool for modelling real systems. To address this, we introduce and
study an extended version of the Game of Life, with the dynamical process
governing the rule selection at each step. We show that the introduced
modification significantly alters the behaviour of the game. We also
demonstrate that the choice of the synchronization policy can be used to
control the trade-off between the stability and the growth in the system.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05990" title="Abstract">arXiv:2310.05990</a> (cross-list from eess.IV) [<a href="/pdf/2310.05990" title="Download PDF">pdf</a>, <a href="/format/2310.05990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation through Pseudolabels in Automatic Region Based  Coronary Artery Segmentation for Disease Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pokhrel%2C+S">Sandesh Pokhrel</a>, 
<a href="/search/eess?searchtype=author&query=Bhandari%2C+S">Sanjay Bhandari</a>, 
<a href="/search/eess?searchtype=author&query=Vazquez%2C+E">Eduard Vazquez</a>, 
<a href="/search/eess?searchtype=author&query=Shrestha%2C+Y+R">Yash Raj Shrestha</a>, 
<a href="/search/eess?searchtype=author&query=Bhattarai%2C+B">Binod Bhattarai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2310.04749">arXiv:2310.04749</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Coronary Artery Diseases(CADs) though preventable are one of the leading
causes of death and disability. Diagnosis of these diseases is often difficult
and resource intensive. Segmentation of arteries in angiographic images has
evolved as a tool for assistance, helping clinicians in making accurate
diagnosis. However, due to the limited amount of data and the difficulty in
curating a dataset, the task of segmentation has proven challenging. In this
study, we introduce the idea of using pseudolabels as a data augmentation
technique to improve the performance of the baseline Yolo model. This method
increases the F1 score of the baseline by 9% in the validation dataset and by
3% in the test dataset.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05997" title="Abstract">arXiv:2310.05997</a> (cross-list from stat.ML) [<a href="/pdf/2310.05997" title="Download PDF">pdf</a>, <a href="/format/2310.05997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-sensitive probabilistic predictions for support vector machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ben%C3%ADtez-Pe%C3%B1a%2C+S">Sandra Ben&#xed;tez-Pe&#xf1;a</a>, 
<a href="/search/stat?searchtype=author&query=Blanquero%2C+R">Rafael Blanquero</a>, 
<a href="/search/stat?searchtype=author&query=Carrizosa%2C+E">Emilio Carrizosa</a>, 
<a href="/search/stat?searchtype=author&query=Ram%C3%ADrez-Cobo%2C+P">Pepa Ram&#xed;rez-Cobo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> European Journal of Operational Research (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Support vector machines (SVMs) are widely used and constitute one of the best
examined and used machine learning models for two-class classification.
Classification in SVM is based on a score procedure, yielding a deterministic
classification rule, which can be transformed into a probabilistic rule (as
implemented in off-the-shelf SVM libraries), but is not probabilistic in
nature. On the other hand, the tuning of the regularization parameters in SVM
is known to imply a high computational effort and generates pieces of
information that are not fully exploited, not being used to build a
probabilistic classification rule. In this paper we propose a novel approach to
generate probabilistic outputs for the SVM. The new method has the following
three properties. First, it is designed to be cost-sensitive, and thus the
different importance of sensitivity (or true positive rate, TPR) and
specificity (true negative rate, TNR) is readily accommodated in the model. As
a result, the model can deal with imbalanced datasets which are common in
operational business problems as churn prediction or credit scoring. Second,
the SVM is embedded in an ensemble method to improve its performance, making
use of the valuable information generated in the parameters tuning process.
Finally, the probabilities estimation is done via bootstrap estimates, avoiding
the use of parametric models as competing approaches. Numerical tests on a wide
range of datasets show the advantages of our approach over benchmark
procedures.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06000" title="Abstract">arXiv:2310.06000</a> (cross-list from econ.GN) [<a href="/pdf/2310.06000" title="Download PDF">pdf</a>, <a href="/format/2310.06000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentivizing Data Sharing for Energy Forecasting: Analytics Markets  with Correlated Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Falconer%2C+T">Thomas Falconer</a>, 
<a href="/search/econ?searchtype=author&query=Kazempour%2C+J">Jalal Kazempour</a>, 
<a href="/search/econ?searchtype=author&query=Pinson%2C+P">Pierre Pinson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Reliably forecasting uncertain power production is beneficial for the social
welfare of electricity markets by reducing the need for balancing resources.
Describing such forecasting as an analytics task, the current literature
proposes analytics markets as an incentive for data sharing to improve
accuracy, for instance by leveraging spatio-temporal correlations. The
challenge is that, when used as input features for forecasting, correlated data
complicates the market design with respect to the revenue allocation, as the
value of overlapping information is inherently combinatorial. We develop a
correlation-aware analytics market for a wind power forecasting application. To
allocate revenue, we adopt a Shapley value-based attribution policy, framing
the features of agents as players and their interactions as a characteristic
function game. We illustrate that there are multiple options to describe such a
game, each having causal nuances that influence market behavior when features
are correlated. We argue that no option is correct in a general sense, but that
the decision hinges on whether the market should address correlations from a
data-centric or model-centric perspective, a choice that can yield
counter-intuitive allocations if not considered carefully by the market
designer.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06001" title="Abstract">arXiv:2310.06001</a> (cross-list from physics.ed-ph) [<a href="/pdf/2310.06001" title="Download PDF">pdf</a>, <a href="/format/2310.06001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teaching control with Basic Maths: Introduction to Process Control  course as a novel educational approach for undergraduate engineering programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Normey-Rico%2C+J+E">Julio Elias Normey-Rico</a>, 
<a href="/search/physics?searchtype=author&query=Morato%2C+M+M">Marcelo Menezes Morato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, 13 figures, Screening at the Journal of Control, Automation and Electrical Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics Education (physics.ed-ph)</span>; Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this article, we discuss a novel education approach to control theory in
undergraduate engineering programs. In particular, we elaborate on the
inclusion of an introductory course on process control during the first years
of the program, to appear right after the students undergo basic calculus and
physics courses. Our novel teaching proposal comprises debating the basic
elements of control theory without requiring any background on advanced
mathematical frameworks from the part of the students. The methodology
addresses, conceptually, the majority of the steps required for the analysis
and design of simple control systems. Herein, we thoroughly detail this
educational guideline, as well as tools that can be used in the classroom.
Furthermore, we propose a cheap test-bench kit and an open-source numerical
simulator that can be used to carry out experiments during the proposed course.
Most importantly, we also assess on how the Introduction to process control
course has affected the undergraduate program on Control and Automation
Engineering at Universidade Federal de Santa Catarina (UFSC, Brazil).
Specifically, we debate the outcomes of implementing our education approach at
UFSC from 2016 to 2023, considering students' rates of success in other control
courses and perspectives on how the chair helped them throughout the course of
their program. Based on randomised interviews, we indicate that our educational
approach has had good teaching-learning results: students tend to be more
motivated for other control-related subjects, while exhibiting higher rates of
success.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06069" title="Abstract">arXiv:2310.06069</a> (cross-list from stat.ML) [<a href="/pdf/2310.06069" title="Download PDF">pdf</a>, <a href="/format/2310.06069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Exploration is no harder than Thompson Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Z">Zhaoqi Li</a>, 
<a href="/search/stat?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>, 
<a href="/search/stat?searchtype=author&query=Jain%2C+L">Lalit Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown
parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear
bandit problem aims to return $\arg\max_{z\in \mathcal{Z}}
z^{\top}\theta_{\ast}$, with high probability through noisy measurements of
$x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing
(asymptotically) optimal methods require either a) potentially costly
projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a
subset of $\mathcal{Z}$ under consideration at each time. This complexity is at
odds with the popular and simple Thompson Sampling algorithm for regret
minimization, which just requires access to a posterior sampling and argmax
oracle, and does not need to enumerate $\mathcal{Z}$ at any point.
Unfortunately, Thompson sampling is known to be sub-optimal for pure
exploration. In this work, we pose a natural question: is there an algorithm
that can explore optimally and only needs the same computational primitives as
Thompson Sampling? We answer the question in the affirmative. We provide an
algorithm that leverages only sampling and argmax oracles and achieves an
exponential convergence rate, with the exponent being the optimal among all
possible allocations asymptotically. In addition, we show that our algorithm
can be easily implemented and performs as well empirically as existing
asymptotically optimal methods.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06079" title="Abstract">arXiv:2310.06079</a> (cross-list from q-fin.CP) [<a href="/pdf/2310.06079" title="Download PDF">pdf</a>, <a href="/format/2310.06079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomalous diffusion and price impact in the fluid-limit of an order book
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Diana%2C+D">Derick Diana</a>, 
<a href="/search/q-fin?searchtype=author&query=Gebbie%2C+T">Tim Gebbie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 26 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Computational Engineering, Finance, and Science (cs.CE); Adaptation and Self-Organizing Systems (nlin.AO); Trading and Market Microstructure (q-fin.TR)

</div>
<p class="mathjax">We extend a Discrete Time Random Walk (DTRW) numerical scheme to simulate the
anomalous diffusion of financial market orders in a simulated order book. Here
using random walks with Sibuya waiting times to include a time-dependent
stochastic forcing function with non-uniformly sampled times between order book
events in the setting of fractional diffusion. This models the fluid limit of
an order book by modelling the continuous arrival, cancellation and diffusion
of orders in the presence of information shocks. We study the impulse response
and stylised facts of orders undergoing anomalous diffusion for different
forcing functions and model parameters. Concretely, we demonstrate the price
impact for flash limit-orders and market orders and show how the numerical
method generate kinks in the price impact. We use cubic spline interpolation to
generate smoothed price impact curves. The work promotes the use of non-uniform
sampling in the presence of diffusive dynamics as the preferred simulation
method.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06080" title="Abstract">arXiv:2310.06080</a> (cross-list from eess.IV) [<a href="/pdf/2310.06080" title="Download PDF">pdf</a>, <a href="/format/2310.06080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Diagnostic Precision: Leveraging Machine Learning Techniques  for Accurate Detection of Covid-19, Pneumonia, and Tuberculosis in Chest  X-Ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kulkarni%2C+A">Aditya Kulkarni</a>, 
<a href="/search/eess?searchtype=author&query=Parasnis%2C+G">Guruprasad Parasnis</a>, 
<a href="/search/eess?searchtype=author&query=Balasubramanian%2C+H">Harish Balasubramanian</a>, 
<a href="/search/eess?searchtype=author&query=Jain%2C+V">Vansh Jain</a>, 
<a href="/search/eess?searchtype=author&query=Chokshi%2C+A">Anmol Chokshi</a>, 
<a href="/search/eess?searchtype=author&query=Sonkusare%2C+R">Reena Sonkusare</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 18 figures, Under review in Discover Artificial Intelligence Journal by Springer Nature
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Lung diseases such as COVID-19, tuberculosis (TB), and pneumonia continue to
be serious global health concerns that affect millions of people worldwide. In
medical practice, chest X-ray examinations have emerged as the norm for
diagnosing diseases, particularly chest infections such as COVID-19. Paramedics
and scientists are working intensively to create a reliable and precise
approach for early-stage COVID-19 diagnosis in order to save lives. But with a
variety of symptoms, medical diagnosis of these disorders poses special
difficulties. It is essential to address their identification and timely
diagnosis in order to successfully treat and prevent these illnesses. In this
research, a multiclass classification approach using state-of-the-art methods
for deep learning and image processing is proposed. This method takes into
account the robustness and efficiency of the system in order to increase
diagnostic precision of chest diseases. A comparison between a brand-new
convolution neural network (CNN) and several transfer learning pre-trained
models including VGG19, ResNet, DenseNet, EfficientNet, and InceptionNet is
recommended. Publicly available and widely used research datasets like Shenzen,
Montogomery, the multiclass Kaggle dataset and the NIH dataset were used to
rigorously test the model. Recall, precision, F1-score, and Area Under Curve
(AUC) score are used to evaluate and compare the performance of the proposed
model. An AUC value of 0.95 for COVID-19, 0.99 for TB, and 0.98 for pneumonia
is obtained using the proposed network. Recall and precision ratings of 0.95,
0.98, and 0.97, respectively, likewise met high standards.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06081" title="Abstract">arXiv:2310.06081</a> (cross-list from math.OC) [<a href="/pdf/2310.06081" title="Download PDF">pdf</a>, <a href="/ps/2310.06081" title="Download PostScript">ps</a>, <a href="/format/2310.06081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ito Diffusion Approximation of Universal Ito Chains for Sampling,  Optimization and Boosting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ustimenko%2C+A">Aleksei Ustimenko</a>, 
<a href="/search/math?searchtype=author&query=Beznosikov%2C+A">Aleksandr Beznosikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">This work considers a rather general and broad class of Markov chains, Ito
chains that look like Euler-Maryama discretization of some Stochastic
Differential Equation. The chain we study is a unified framework for
theoretical analysis. It comes with almost arbitrary isotropic and
state-dependent noise instead of normal and state-independent one, as in most
related papers. Moreover, our chain's drift and diffusion coefficient can be
inexact to cover a wide range of applications such as Stochastic Gradient
Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic
Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of
the Ito chain and the corresponding Stochastic Differential Equation. These
results improve or cover most of the known estimates. Moreover, for some
particular cases, our analysis is the first.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06105" title="Abstract">arXiv:2310.06105</a> (cross-list from stat.ML) [<a href="/pdf/2310.06105" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Uncertainty in Deep Learning Classification with Noise in  Discrete Inputs for Risk-Based Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kheirandish%2C+M">Maryam Kheirandish</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+S">Shengfan Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Catanzaro%2C+D+G">Donald G. Catanzaro</a>, 
<a href="/search/stat?searchtype=author&query=Crudu%2C+V">Valeriu Crudu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">The use of Deep Neural Network (DNN) models in risk-based decision-making has
attracted extensive attention with broad applications in medical, finance,
manufacturing, and quality control. To mitigate prediction-related risks in
decision making, prediction confidence or uncertainty should be assessed
alongside the overall performance of algorithms. Recent studies on Bayesian
deep learning helps quantify prediction uncertainty arises from input noises
and model parameters. However, the normality assumption of input noise in these
models limits their applicability to problems involving categorical and
discrete feature variables in tabular datasets. In this paper, we propose a
mathematical framework to quantify prediction uncertainty for DNN models. The
prediction uncertainty arises from errors in predictors that follow some known
finite discrete distribution. We then conducted a case study using the
framework to predict treatment outcome for tuberculosis patients during their
course of treatment. The results demonstrate under a certain level of risk, we
can identify risk-sensitive cases, which are prone to be misclassified due to
error in predictors. Comparing to the Monte Carlo dropout method, our proposed
framework is more aware of misclassification cases. Our proposed framework for
uncertainty quantification in deep learning can support risk-based decision
making in applications when discrete errors in predictors are present.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06110" title="Abstract">arXiv:2310.06110</a> (cross-list from stat.ML) [<a href="/pdf/2310.06110" title="Download PDF">pdf</a>, <a href="/format/2310.06110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grokking as the Transition from Lazy to Rich Training Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kumar%2C+T">Tanishq Kumar</a>, 
<a href="/search/stat?searchtype=author&query=Bordelon%2C+B">Blake Bordelon</a>, 
<a href="/search/stat?searchtype=author&query=Gershman%2C+S+J">Samuel J. Gershman</a>, 
<a href="/search/stat?searchtype=author&query=Pehlevan%2C+C">Cengiz Pehlevan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose that the grokking phenomenon, where the train loss of a neural
network decreases much earlier than its test loss, can arise due to a neural
network transitioning from lazy training dynamics to a rich, feature learning
regime. To illustrate this mechanism, we study the simple setting of vanilla
gradient descent on a polynomial regression problem with a two layer neural
network which exhibits grokking without regularization in a way that cannot be
explained by existing theories. We identify sufficient statistics for the test
loss of such a network, and tracking these over training reveals that grokking
arises in this setting when the network first attempts to fit a kernel
regression solution with its initial features, followed by late-time feature
learning where a generalizing solution is identified after train loss is
already low. We find that the key determinants of grokking are the rate of
feature learning -- which can be controlled precisely by parameters that scale
the network output -- and the alignment of the initial features with the target
function $y(x)$. We argue this delayed generalization arises when (1) the top
eigenvectors of the initial neural tangent kernel and the task labels $y(x)$
are misaligned, but (2) the dataset size is large enough so that it is possible
for the network to generalize eventually, but not so large that train loss
perfectly tracks test loss at all epochs, and (3) the network begins training
in the lazy regime so does not learn features immediately. We conclude with
evidence that this transition from lazy (linear model) to rich training
(feature learning) can control grokking in more general settings, like on
MNIST, one-layer Transformers, and student-teacher networks.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06143" title="Abstract">arXiv:2310.06143</a> (cross-list from eess.IV) [<a href="/pdf/2310.06143" title="Download PDF">pdf</a>, <a href="/format/2310.06143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease  Classification from Chest X-ray Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=%C3%96zt%C3%BCrk%2C+%C5%9E">&#x15e;aban &#xd6;zt&#xfc;rk</a>, 
<a href="/search/eess?searchtype=author&query=Tural%C4%B1%2C+M+Y">M. Yi&#x11f;it Tural&#x131;</a>, 
<a href="/search/eess?searchtype=author&query=%C3%87ukur%2C+T">Tolga &#xc7;ukur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Chest X-ray is an essential diagnostic tool in the identification of chest
diseases given its high sensitivity to pathological abnormalities in the lungs.
However, image-driven diagnosis is still challenging due to heterogeneity in
size and location of pathology, as well as visual similarities and
co-occurrence of separate pathology. Since disease-related regions often occupy
a relatively small portion of diagnostic images, classification models based on
traditional convolutional neural networks (CNNs) are adversely affected given
their locality bias. While CNNs were previously augmented with attention maps
or spatial masks to guide focus on potentially critical regions, learning
localization guidance under heterogeneity in the spatial distribution of
pathology is challenging. To improve multi-label classification performance,
here we propose a novel method, HydraViT, that synergistically combines a
transformer backbone with a multi-branch output module with learned weighting.
The transformer backbone enhances sensitivity to long-range context in X-ray
images, while using the self-attention mechanism to adaptively focus on
task-critical regions. The multi-branch output module dedicates an independent
branch to each disease label to attain robust learning across separate disease
classes, along with an aggregated branch across labels to maintain sensitivity
to co-occurrence relationships among pathology. Experiments demonstrate that,
on average, HydraViT outperforms competing attention-guided methods by 1.2%,
region-guided methods by 1.4%, and semantic-guided methods by 1.0% in
multi-label classification performance.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06185" title="Abstract">arXiv:2310.06185</a> (cross-list from math.OC) [<a href="/pdf/2310.06185" title="Download PDF">pdf</a>, <a href="/format/2310.06185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Bounds for the Maximum Distance Over a Polytope to a Given Point
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Costandin%2C+M">Marius Costandin</a>, 
<a href="/search/math?searchtype=author&query=Costandin%2C+B">Beniamin Costandin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.15054">arXiv:2308.15054</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">In this paper we study the problem of maximizing the distance to a given
point $C_0$ over a polytope $\mathcal{P}$. Assuming that the polytope is
circumscribed by a known ball we construct an intersection of balls which
preserves the vertices of the polytope on the boundary of this ball, and show
that the intersection of balls approximates the polytope arbitrarily well.
Then, we use some known results regarding the maximization of distances to a
given point over an intersection of balls to create a new polytope which
preserves the maximizers to the original problem. Next, a new intersection of
balls is obtained in a similar fashion, and as such, after a finite number of
iterations, we conjecture, we end up with an intersection of balls over which
we can maximize the distance to the given point. The obtained distance is shown
to be a non trivial upper bound to the original distance. Tests are made with
maximizing the distance to a random point over the unit hypercube up to
dimension $n = 100$. Several detailed 2-d examples are also shown.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06194" title="Abstract">arXiv:2310.06194</a> (cross-list from math.OC) [<a href="/pdf/2310.06194" title="Download PDF">pdf</a>, <a href="/format/2310.06194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability and Regret bounds on Distributed Truncated Predictive Control  for Networked Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xu%2C+E">Eric Xu</a>, 
<a href="/search/math?searchtype=author&query=Qu%2C+G">Guannan Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 2 figures, submitted to ACC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This work is primarily concerned about the distributed control of networked
linear timeinvariant (LTI) systems. In particular, we propose a truncated
predictive control algorithm based on $\kappa$-hop neighbourhoods of the agents
of the networked system. We establish stability and regret bounds for the
proposed algorithm, which shows that the regret decays exponentially when the
temporal prediction horizon $k$ and the spatial radius $\kappa$ increases.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06203" title="Abstract">arXiv:2310.06203</a> (cross-list from math.CO) [<a href="/pdf/2310.06203" title="Download PDF">pdf</a>, <a href="/ps/2310.06203" title="Download PostScript">ps</a>, <a href="/format/2310.06203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graphs with three and four distinct eigenvalues based on circulants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ba%C5%A1i%C4%87%2C+M">Milan Ba&#x161;i&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">In this paper, we aim to address the open questions raised in various recent
papers regarding characterization of circulant graphs with three or four
distinct eigenvalues in their spectra. Our focus is on providing
characterizations and constructing classes of graphs falling under this
specific category. We present a characterization of circulant graphs with prime
number order and unitary Cayley graphs with arbitrary order, both of which
possess spectra displaying three or four distinct eigenvalues. Various
constructions of circulant graphs with composite orders are provided whose
spectra consist of four distinct eigenvalues. These constructions primarily
utilize specific subgraphs of circulant graphs that already possess two or
three eigenvalues in their spectra, employing graph operations like the tensor
product, the union, and the complement. Finally, we characterize the iterated
line graphs of unitary Cayley graphs whose spectra contain three or four
distinct eigenvalues, and we show their non-circulant nature.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06211" title="Abstract">arXiv:2310.06211</a> (cross-list from math.OC) [<a href="/pdf/2310.06211" title="Download PDF">pdf</a>, <a href="/format/2310.06211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On convergence rates of proximal alternating direction method of  multipliers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jin%2C+Q">Qinian Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper we consider from two different aspects the proximal alternating
direction method of multipliers (ADMM) in Hilbert spaces. We first consider the
application of the proximal ADMM to solve well-posed linearly constrained
two-block separable convex minimization problems in Hilbert spaces and obtain
new and improved non-ergodic convergence rate results, including linear and
sublinear rates under certain regularity conditions. We next consider proximal
ADMM as a regularization method for solving linear ill-posed inverse problems
in Hilbert spaces. When the data is corrupted by additive noise, we establish,
under a benchmark source condition, a convergence rate result in terms of the
noise level when the number of iteration is properly chosen.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06223" title="Abstract">arXiv:2310.06223</a> (cross-list from math.OC) [<a href="/pdf/2310.06223" title="Download PDF">pdf</a>, <a href="/format/2310.06223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projected Push-Pull For Distributed Constrained Optimization Over  Time-Varying Directed Graphs (extended version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Akg%C3%BCn%2C+O+E">Orhan Eren Akg&#xfc;n</a>, 
<a href="/search/math?searchtype=author&query=Day%C4%B1%2C+A+K">Arif Kerem Day&#x131;</a>, 
<a href="/search/math?searchtype=author&query=Gil%2C+S">Stephanie Gil</a>, 
<a href="/search/math?searchtype=author&query=Nedi%C4%87%2C+A">Angelia Nedi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We introduce the Projected Push-Pull algorithm that enables multiple agents
to solve a distributed constrained optimization problem with private cost
functions and global constraints, in a collaborative manner. Our algorithm
employs projected gradient descent to deal with constraints and a lazy update
rule to control the trade-off between the consensus and optimization steps in
the protocol. We prove that our algorithm achieves geometric convergence over
time-varying directed graphs while ensuring that the decision variable always
stays within the constraint set. We derive explicit bounds for step sizes that
guarantee geometric convergence based on the strong-convexity and smoothness of
cost functions, and graph properties. Moreover, we provide additional
theoretical results on the usefulness of lazy updates, revealing the challenges
in the analysis of any gradient tracking method that uses projection operators
in a distributed constrained optimization setting. We validate our theoretical
results with numerical studies over different graph types, showing that our
algorithm achieves geometric convergence empirically.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06241" title="Abstract">arXiv:2310.06241</a> (cross-list from stat.ML) [<a href="/pdf/2310.06241" title="Download PDF">pdf</a>, <a href="/format/2310.06241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bayesian framework for discovering interpretable Lagrangian of  dynamical systems from data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tripura%2C+T">Tapas Tripura</a>, 
<a href="/search/stat?searchtype=author&query=Chakraborty%2C+S">Souvik Chakraborty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning and predicting the dynamics of physical systems requires a profound
understanding of the underlying physical laws. Recent works on learning
physical laws involve generalizing the equation discovery frameworks to the
discovery of Hamiltonian and Lagrangian of physical systems. While the existing
methods parameterize the Lagrangian using neural networks, we propose an
alternate framework for learning interpretable Lagrangian descriptions of
physical systems from limited data using the sparse Bayesian approach. Unlike
existing neural network-based approaches, the proposed approach (a) yields an
interpretable description of Lagrangian, (b) exploits Bayesian learning to
quantify the epistemic uncertainty due to limited data, (c) automates the
distillation of Hamiltonian from the learned Lagrangian using Legendre
transformation, and (d) provides ordinary (ODE) and partial differential
equation (PDE) based descriptions of the observed systems. Six different
examples involving both discrete and continuous system illustrates the efficacy
of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06251" title="Abstract">arXiv:2310.06251</a> (cross-list from stat.ML) [<a href="/pdf/2310.06251" title="Download PDF">pdf</a>, <a href="/format/2310.06251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning: A Tutorial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Polson%2C+N">Nick Polson</a>, 
<a href="/search/stat?searchtype=author&query=Sokolov%2C+V">Vadim Sokolov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/1808.08618">arXiv:1808.08618</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Our goal is to provide a review of deep learning methods which provide
insight into structured high-dimensional data. Rather than using shallow
additive architectures common to most statistical models, deep learning uses
layers of semi-affine input transformations to provide a predictive rule.
Applying these layers of transformations leads to a set of attributes (or,
features) to which probabilistic statistical methods can be applied. Thus, the
best of both worlds can be achieved: scalable prediction rules fortified with
uncertainty quantification, where sparse regularization finds the features.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06259" title="Abstract">arXiv:2310.06259</a> (cross-list from eess.IV) [<a href="/pdf/2310.06259" title="Download PDF">pdf</a>, <a href="/format/2310.06259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-modal Cognitive Consensus guided Audio-Visual Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+Z">Zhaofeng Shi</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Q">Qingbo Wu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hongliang Li</a>, 
<a href="/search/eess?searchtype=author&query=Meng%2C+F">Fanman Meng</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+L">Linfeng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio-Visual Segmentation (AVS) aims to extract the sounding object from a
video frame, which is represented by a pixel-wise segmentation mask. The
pioneering work conducts this task through dense feature-level audio-visual
interaction, which ignores the dimension gap between different modalities. More
specifically, the audio clip could only provide a \textit{Global} semantic
label in each sequence, but the video frame covers multiple semantic objects
across different \textit{Local} regions. In this paper, we propose a
Cross-modal Cognitive Consensus guided Network (C3N) to align the audio-visual
semantics from the global dimension and progressively inject them into the
local regions via an attention mechanism. Firstly, a Cross-modal Cognitive
Consensus Inference Module (C3IM) is developed to extract a unified-modal label
by integrating audio/visual classification confidence and similarities of
modality-specific label embeddings. Then, we feed the unified-modal label back
to the visual backbone as the explicit semantic-level guidance via a Cognitive
Consensus guided Attention Module (CCAM), which highlights the local features
corresponding to the interested object. Extensive experiments on the Single
Sound Source Segmentation (S4) setting and Multiple Sound Source Segmentation
(MS3) setting of the AVSBench dataset demonstrate the effectiveness of the
proposed method, which achieves state-of-the-art performance.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06289" title="Abstract">arXiv:2310.06289</a> (cross-list from math.ST) [<a href="/pdf/2310.06289" title="Download PDF">pdf</a>, <a href="/ps/2310.06289" title="Download PostScript">ps</a>, <a href="/format/2310.06289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better and Simpler Lower Bounds for Differentially Private Statistical  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Narayanan%2C+S">Shyam Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">We provide improved lower bounds for two well-known high-dimensional private
estimation tasks. First, we prove that for estimating the covariance of a
Gaussian up to spectral error $\alpha$ with approximate differential privacy,
one needs $\tilde{\Omega}\left(\frac{d^{3/2}}{\alpha \varepsilon} +
\frac{d}{\alpha^2}\right)$ samples for any $\alpha \le O(1)$, which is tight up
to logarithmic factors. This improves over previous work which established this
for $\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$, and is also simpler than
previous work. Next, we prove that for estimating the mean of a heavy-tailed
distribution with bounded $k$th moments with approximate differential privacy,
one needs $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} +
\frac{d}{\alpha^2}\right)$ samples. This matches known upper bounds and
improves over the best known lower bound for this problem, which only hold for
pure differential privacy, or when $k = 2$. Our techniques follow the method of
fingerprinting and are generally quite simple. Our lower bound for heavy-tailed
estimation is based on a black-box reduction from privately estimating
identity-covariance Gaussians. Our lower bound for covariance estimation
utilizes a Bayesian approach to show that, under an Inverse Wishart prior
distribution for the covariance matrix, no private estimator can be accurate
even in expectation, without sufficiently many samples.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06291" title="Abstract">arXiv:2310.06291</a> (cross-list from eess.IV) [<a href="/pdf/2310.06291" title="Download PDF">pdf</a>, <a href="/format/2310.06291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three-Dimensional Medical Image Fusion with Deformable Cross-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+L">Lin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+X">Xinxin Fan</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chulong Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+J">Jingjing Dai</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+Y">Yaoqin Xie</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+X">Xiaokun Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Multimodal medical image fusion plays an instrumental role in several areas
of medical image processing, particularly in disease recognition and tumor
detection. Traditional fusion methods tend to process each modality
independently before combining the features and reconstructing the fusion
image. However, this approach often neglects the fundamental commonalities and
disparities between multimodal information. Furthermore, the prevailing
methodologies are largely confined to fusing two-dimensional (2D) medical image
slices, leading to a lack of contextual supervision in the fusion images and
subsequently, a decreased information yield for physicians relative to
three-dimensional (3D) images. In this study, we introduce an innovative
unsupervised feature mutual learning fusion network designed to rectify these
limitations. Our approach incorporates a Deformable Cross Feature Blend (DCFB)
module that facilitates the dual modalities in discerning their respective
similarities and differences. We have applied our model to the fusion of 3D MRI
and PET images obtained from 660 patients in the Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset. Through the application of the DCFB
module, our network generates high-quality MRI-PET fusion images. Experimental
results demonstrate that our method surpasses traditional 2D image fusion
methods in performance metrics such as Peak Signal to Noise Ratio (PSNR) and
Structural Similarity Index Measure (SSIM). Importantly, the capacity of our
method to fuse 3D images enhances the information available to physicians and
researchers, thus marking a significant step forward in the field. The code
will soon be available online.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06318" title="Abstract">arXiv:2310.06318</a> (cross-list from eess.IV) [<a href="/pdf/2310.06318" title="Download PDF">pdf</a>, <a href="/format/2310.06318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Masked Image Inpainting for Robust Detection of Mpox and  Non-Mpox
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yue%2C+Y">Yubiao Yue</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhenzhang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Due to the lack of efficient mpox diagnostic technology, mpox cases continue
to increase. Recently, the great potential of deep learning models in detecting
mpox and non-mpox has been proven. However, existing models learn image
representations via image classification, which results in they may be easily
susceptible to interference from real-world noise, require diverse non-mpox
images, and fail to detect abnormal input. These drawbacks make classification
models inapplicable in real-world settings. To address these challenges, we
propose "Mask, Inpainting, and Measure" (MIM). In MIM's pipeline, a generative
adversarial network only learns mpox image representations by inpainting the
masked mpox images. Then, MIM determines whether the input belongs to mpox by
measuring the similarity between the inpainted image and the original image.
The underlying intuition is that since MIM solely models mpox images, it
struggles to accurately inpaint non-mpox images in real-world settings. Without
utilizing any non-mpox images, MIM cleverly detects mpox and non-mpox and can
handle abnormal inputs. We used the recognized mpox dataset (MSLD) and images
of eighteen non-mpox skin diseases to verify the effectiveness and robustness
of MIM. Experimental results show that the average AUROC of MIM achieves
0.8237. In addition, we demonstrated the drawbacks of classification models and
buttressed the potential of MIM through clinical validation. Finally, we
developed an online smartphone app to provide free testing to the public in
affected areas. This work first employs generative models to improve mpox
detection and provides new insights into binary decision-making tasks in
medical images.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06336" title="Abstract">arXiv:2310.06336</a> (cross-list from eess.SP) [<a href="/pdf/2310.06336" title="Download PDF">pdf</a>, <a href="/format/2310.06336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HoloFed: Environment-Adaptive Positioning via Multi-band Reconfigurable  Holographic Surfaces and Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+J">Jingzhi Hu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhe Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+T">Tianyue Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Schober%2C+R">Robert Schober</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+J">Jun Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Positioning is an essential service for various applications and is expected
to be integrated with existing communication infrastructures in 5G and 6G.
Though current Wi-Fi and cellular base stations (BSs) can be used to support
this integration, the resulting precision is unsatisfactory due to the lack of
precise control of the wireless signals. Recently, BSs adopting reconfigurable
holographic surfaces (RHSs) have been advocated for positioning as RHSs' large
number of antenna elements enable generation of arbitrary and highly-focused
signal beam patterns. However, existing designs face two major challenges: i)
RHSs only have limited operating bandwidth, and ii) the positioning methods
cannot adapt to the diverse environments encountered in practice. To overcome
these challenges, we present HoloFed, a system providing high-precision
environment-adaptive user positioning services by exploiting multi-band(MB)-RHS
and federated learning (FL). For improving the positioning performance, a lower
bound on the error variance is obtained and utilized for guiding MB-RHS's
digital and analog beamforming design. For better adaptability while preserving
privacy, an FL framework is proposed for users to collaboratively train a
position estimator, where we exploit the transfer learning technique to handle
the lack of position labels of the users. Moreover, a scheduling algorithm for
the BS to select which users train the position estimator is designed, jointly
considering the convergence and efficiency of FL. Our simulation results
confirm that HoloFed achieves a 57% lower positioning error variance compared
to a beam-scanning baseline and can effectively adapt to diverse environments.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06339" title="Abstract">arXiv:2310.06339</a> (cross-list from eess.IV) [<a href="/pdf/2310.06339" title="Download PDF">pdf</a>, <a href="/format/2310.06339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic nodule identification and differentiation in ultrasound videos  to facilitate per-nodule examination
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+S">Siyuan Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+Y">Yan Ding</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yuling Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+L">Lei Xu</a>, 
<a href="/search/eess?searchtype=author&query=Dai%2C+W">Wenli Dai</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+W">Wanru Chang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jianfeng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jie Yu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jianqiao Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chunquan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+P">Ping Liang</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+D">Dexing Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Ultrasound is a vital diagnostic technique in health screening, with the
advantages of non-invasive, cost-effective, and radiation free, and therefore
is widely applied in the diagnosis of nodules. However, it relies heavily on
the expertise and clinical experience of the sonographer. In ultrasound images,
a single nodule might present heterogeneous appearances in different
cross-sectional views which makes it hard to perform per-nodule examination.
Sonographers usually discriminate different nodules by examining the nodule
features and the surrounding structures like gland and duct, which is
cumbersome and time-consuming. To address this problem, we collected hundreds
of breast ultrasound videos and built a nodule reidentification system that
consists of two parts: an extractor based on the deep learning model that can
extract feature vectors from the input video clips and a real-time clustering
algorithm that automatically groups feature vectors by nodules. The system
obtains satisfactory results and exhibits the capability to differentiate
ultrasound videos. As far as we know, it's the first attempt to apply
re-identification technique in the ultrasonic field.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06394" title="Abstract">arXiv:2310.06394</a> (cross-list from physics.optics) [<a href="/pdf/2310.06394" title="Download PDF">pdf</a>, <a href="/format/2310.06394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatial-photonic Ising machine by space-division multiplexing with  physically tunable coefficients of a multi-component model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sakabe%2C+T">Takumi Sakabe</a>, 
<a href="/search/physics?searchtype=author&query=Shimomura%2C+S">Suguru Shimomura</a>, 
<a href="/search/physics?searchtype=author&query=Ogura%2C+Y">Yusuke Ogura</a>, 
<a href="/search/physics?searchtype=author&query=Okubo%2C+K">Ken-ichi Okubo</a>, 
<a href="/search/physics?searchtype=author&query=Yamashita%2C+H">Hiroshi Yamashita</a>, 
<a href="/search/physics?searchtype=author&query=Suzuki%2C+H">Hideyuki Suzuki</a>, 
<a href="/search/physics?searchtype=author&query=Tanida%2C+J">Jun Tanida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Emerging Technologies (cs.ET); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">This paper proposes a space-division multiplexed spatial-photonic Ising
machine (SDM-SPIM) that physically calculates the weighted sum of the Ising
Hamiltonians for individual components in a multi-component model.
Space-division multiplexing enables tuning a set of weight coefficients as an
optical parameter and obtaining the desired Ising Hamiltonian at a time. We
solved knapsack problems to verify the system's validity, demonstrating that
optical parameters impact the search property. We also investigated a new
dynamic coefficient search algorithm to enhance search performance. The
SDM-SPIM would physically calculate the Hamiltonian and a part of the
optimization with an electronics process.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06532" title="Abstract">arXiv:2310.06532</a> (cross-list from eess.SP) [<a href="/pdf/2310.06532" title="Download PDF">pdf</a>, <a href="/ps/2310.06532" title="Download PostScript">ps</a>, <a href="/format/2310.06532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChannelComp: A General Method for Computation by Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Razavikia%2C+S">Saeed Razavikia</a>, 
<a href="/search/eess?searchtype=author&query=Da+Silva+J%C3%BAnior%2C+J+M+B">Jos&#xe9; Mairton Barros Da Silva J&#xfa;nior</a>, 
<a href="/search/eess?searchtype=author&query=Fischione%2C+C">Carlo Fischione</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Information Theory (cs.IT)

</div>
<p class="mathjax">Over-the-air computation (AirComp) is a well-known technique by which several
wireless devices transmit by analog amplitude modulation to achieve a sum of
their transmit signals at a common receiver. The underlying physical principle
is the superposition property of the radio waves. Since such superposition is
analog and in amplitude, it is natural that AirComp uses analog amplitude
modulations. Unfortunately, this is impractical because most wireless devices
today use digital modulations. It would be highly desirable to use digital
communications because of their numerous benefits, such as error correction,
synchronization, acquisition of channel state information, and widespread use.
However, when we use digital modulations for AirComp, a general belief is that
the superposition property of the radio waves returns a meaningless overlapping
of the digital signals. In this paper, we break through such beliefs and
propose an entirely new digital channel computing method named ChannelComp,
which can use digital as well as analog modulations. We propose a feasibility
optimization problem that ascertains the optimal modulation for computing
arbitrary functions over-the-air. Additionally, we propose pre-coders to adapt
existing digital modulation schemes for computing the function over the
multiple access channel. The simulation results verify the superior performance
of ChannelComp compared to AirComp, particularly for the product functions,
with more than 10 dB improvement of the computation error.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06534" title="Abstract">arXiv:2310.06534</a> (cross-list from stat.ML) [<a href="/pdf/2310.06534" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disk failure prediction based on multi-layer domain adaptive learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Gao%2C+G">Guangfu Gao</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+P">Peng Wu</a>, 
<a href="/search/stat?searchtype=author&query=Dawood%2C+H">Hussain Dawood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large scale data storage is susceptible to failure. As disks are damaged and
replaced, traditional machine learning models, which rely on historical data to
make predictions, struggle to accurately predict disk failures. This paper
presents a novel method for predicting disk failures by leveraging multi-layer
domain adaptive learning techniques. First, disk data with numerous faults is
selected as the source domain, and disk data with fewer faults is selected as
the target domain. A training of the feature extraction network is performed
with the selected origin and destination domains. The contrast between the two
domains facilitates the transfer of diagnostic knowledge from the domain of
source and target. According to the experimental findings, it has been
demonstrated that the proposed technique can generate a reliable prediction
model and improve the ability to predict failures on disk data with few failure
samples.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06537" title="Abstract">arXiv:2310.06537</a> (cross-list from stat.ML) [<a href="/pdf/2310.06537" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-level hybrid strategy selection for disk fault prediction model  based on multivariate GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yuan%2C+S">Shuangshuang Yuan</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+P">Peng Wu</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+Y">Yuehui Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Data class imbalance is a common problem in classification problems, where
minority class samples are often more important and more costly to misclassify
in a classification task. Therefore, it is very important to solve the data
class imbalance classification problem. The SMART dataset exhibits an evident
class imbalance, comprising a substantial quantity of healthy samples and a
comparatively limited number of defective samples. This dataset serves as a
reliable indicator of the disc's health status. In this paper, we obtain the
best balanced disk SMART dataset for a specific classification model by mixing
and integrating the data synthesised by multivariate generative adversarial
networks (GAN) to balance the disk SMART dataset at the data level; and combine
it with genetic algorithms to obtain higher disk fault classification
prediction accuracy on a specific classification model.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06554" title="Abstract">arXiv:2310.06554</a> (cross-list from eess.AS) [<a href="/pdf/2310.06554" title="Download PDF">pdf</a>, <a href="/format/2310.06554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling of Speech-dependent Own Voice Transfer Characteristics for  Hearables with In-ear Microphones
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ohlenbusch%2C+M">Mattes Ohlenbusch</a>, 
<a href="/search/eess?searchtype=author&query=Rollwage%2C+C">Christian Rollwage</a>, 
<a href="/search/eess?searchtype=author&query=Doclo%2C+S">Simon Doclo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures; Extended version of <a href="/abs/2309.08294">arXiv:2309.08294</a> (more detailed description of the problem, additional models considered, more systematic evaluation conducted on a different, larger dataset)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Hearables often contain an in-ear microphone, which may be used to capture
the own voice of its user. However, due to ear canal occlusion the in-ear
microphone mostly records body-conducted speech, which suffers from
band-limitation effects and is subject to amplification of low frequency
content. These transfer characteristics are assumed to vary both based on
speech content and between individual talkers. It is desirable to have an
accurate model of the own voice transfer characteristics between hearable
microphones. Such a model can be used, e.g., to simulate a large amount of
in-ear recordings to train supervised learning-based algorithms aiming at
compensating own voice transfer characteristics. In this paper we propose a
speech-dependent system identification model based on phoneme recognition.
Using recordings from a prototype hearable, the modeling accuracy is evaluated
in terms of technical measures. We investigate robustness of transfer
characteristic models to utterance or talker mismatch. Simulation results show
that using the proposed speech-dependent model is preferable for simulating
in-ear recordings compared to a speech-independent model. The proposed model is
able to generalize better to new utterances than an adaptive filtering-based
model. Additionally, we find that talker-averaged models generalize better to
different talkers than individual models.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06557" title="Abstract">arXiv:2310.06557</a> (cross-list from eess.IV) [<a href="/pdf/2310.06557" title="Download PDF">pdf</a>, <a href="/format/2310.06557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data efficient deep learning for medical image analysis: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kumari%2C+S">Suruchi Kumari</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+P">Pravendra Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid evolution of deep learning has significantly advanced the field of
medical image analysis. However, despite these achievements, the further
enhancement of deep learning models for medical image analysis faces a
significant challenge due to the scarcity of large, well-annotated datasets. To
address this issue, recent years have witnessed a growing emphasis on the
development of data-efficient deep learning methods. This paper conducts a
thorough review of data-efficient deep learning methods for medical image
analysis. To this end, we categorize these methods based on the level of
supervision they rely on, encompassing categories such as no supervision,
inexact supervision, incomplete supervision, inaccurate supervision, and only
limited supervision. We further divide these categories into finer
subcategories. For example, we categorize inexact supervision into multiple
instance learning and learning with weak annotations. Similarly, we categorize
incomplete supervision into semi-supervised learning, active learning, and
domain-adaptive learning and so on. Furthermore, we systematically summarize
commonly used datasets for data efficient deep learning in medical image
analysis and investigate future research directions to conclude this survey.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06571" title="Abstract">arXiv:2310.06571</a> (cross-list from stat.ML) [<a href="/pdf/2310.06571" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical properties and privacy guarantees of an original  distance-based fully synthetic data generation method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chapelle%2C+R">R&#xe9;my Chapelle</a> (CESP, EVDG), 
<a href="/search/stat?searchtype=author&query=Falissard%2C+B">Bruno Falissard</a> (CESP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Introduction: The amount of data generated by original research is growing
exponentially. Publicly releasing them is recommended to comply with the Open
Science principles. However, data collected from human participants cannot be
released as-is without raising privacy concerns. Fully synthetic data represent
a promising answer to this challenge. This approach is explored by the French
Centre de Recherche en {\'E}pid{\'e}miologie et Sant{\'e} des Populations in
the form of a synthetic data generation framework based on Classification and
Regression Trees and an original distance-based filtering. The goal of this
work was to develop a refined version of this framework and to assess its
risk-utility profile with empirical and formal tools, including novel ones
developed for the purpose of this evaluation.Materials and Methods: Our
synthesis framework consists of four successive steps, each of which is
designed to prevent specific risks of disclosure. We assessed its performance
by applying two or more of these steps to a rich epidemiological dataset.
Privacy and utility metrics were computed for each of the resulting synthetic
datasets, which were further assessed using machine learning
approaches.Results: Computed metrics showed a satisfactory level of protection
against attribute disclosure attacks for each synthetic dataset, especially
when the full framework was used. Membership disclosure attacks were formally
prevented without significantly altering the data. Machine learning approaches
showed a low risk of success for simulated singling out and linkability
attacks. Distributional and inferential similarity with the original data were
high with all datasets.Discussion: This work showed the technical feasibility
of generating publicly releasable synthetic data using a multi-step framework.
Formal and empirical tools specifically developed for this demonstration are a
valuable contribution to this field. Further research should focus on the
extension and validation of these tools, in an effort to specify the intrinsic
qualities of alternative data synthesis methods.Conclusion: By successfully
assessing the quality of data produced using a novel multi-step synthetic data
generation framework, we showed the technical and conceptual soundness of the
Open-CESP initiative, which seems ripe for full-scale implementation.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06572" title="Abstract">arXiv:2310.06572</a> (cross-list from physics.ins-det) [<a href="/pdf/2310.06572" title="Download PDF">pdf</a>, <a href="/format/2310.06572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning reconstruction with uncertainty estimation for $&#x3b3;$  photon interaction in fast scintillator detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Daniel%2C+G">Geoffrey Daniel</a>, 
<a href="/search/physics?searchtype=author&query=Yahiaoui%2C+M+B">Mohamed Bahi Yahiaoui</a>, 
<a href="/search/physics?searchtype=author&query=Comtat%2C+C">Claude Comtat</a>, 
<a href="/search/physics?searchtype=author&query=Jan%2C+S">Sebastien Jan</a>, 
<a href="/search/physics?searchtype=author&query=Kochebina%2C+O">Olga Kochebina</a>, 
<a href="/search/physics?searchtype=author&query=Martinez%2C+J">Jean-Marc Martinez</a>, 
<a href="/search/physics?searchtype=author&query=Sergeyeva%2C+V">Viktoriya Sergeyeva</a>, 
<a href="/search/physics?searchtype=author&query=Sharyy%2C+V">Viatcheslav Sharyy</a>, 
<a href="/search/physics?searchtype=author&query=Sung%2C+C">Chi-Hsun Sung</a>, 
<a href="/search/physics?searchtype=author&query=Yvon%2C+D">Dominique Yvon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Artificial Intelligence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">This article presents a physics-informed deep learning method for the
quantitative estimation of the spatial coordinates of gamma interactions within
a monolithic scintillator, with a focus on Positron Emission Tomography (PET)
imaging. A Density Neural Network approach is designed to estimate the
2-dimensional gamma photon interaction coordinates in a fast lead tungstate
(PbWO4) monolithic scintillator detector. We introduce a custom loss function
to estimate the inherent uncertainties associated with the reconstruction
process and to incorporate the physical constraints of the detector.
<br />This unique combination allows for more robust and reliable position
estimations and the obtained results demonstrate the effectiveness of the
proposed approach and highlights the significant benefits of the uncertainties
estimation. We discuss its potential impact on improving PET imaging quality
and show how the results can be used to improve the exploitation of the model,
to bring benefits to the application and how to evaluate the validity of the
given prediction and the associated uncertainties. Importantly, our proposed
methodology extends beyond this specific use case, as it can be generalized to
other applications beyond PET imaging.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06604" title="Abstract">arXiv:2310.06604</a> (cross-list from eess.SP) [<a href="/pdf/2310.06604" title="Download PDF">pdf</a>, <a href="/format/2310.06604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near and Far Field Model Mismatch: Implications on 6G Communications,  Localization, and Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Elzanaty%2C+A">Ahmed Elzanaty</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jiuyu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Guerra%2C+A">Anna Guerra</a>, 
<a href="/search/eess?searchtype=author&query=Guidi%2C+F">Francesco Guidi</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+Y">Yi Ma</a>, 
<a href="/search/eess?searchtype=author&query=Tafazolli%2C+R">Rahim Tafazolli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The upcoming 6G technology is expected to operate in near-field (NF)
radiating conditions thanks to high-frequency and electrically large antenna
arrays. While several studies have already addressed this possibility, it is
worth noting that NF models introduce heightened complexity, the justification
for which is not always evident in terms of performance improvements.
Therefore, this paper delves into the implications of the disparity between NF
and far-field (FF) models concerning communication, localization, and sensing
systems. Such disparity might lead to a degradation of performance metrics like
localization accuracy, sensing reliability, and communication efficiency.
Through an exploration of the effects arising from the mismatches between NF
and FF models, this study seeks to illuminate the challenges confronting system
designers and offer valuable insights into the balance between model accuracy,
which typically requires a high complexity and achievable performance. To
substantiate our perspective, we also incorporate a numerical performance
assessment confirming the repercussions of the mismatch between NF and FF
models.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06628" title="Abstract">arXiv:2310.06628</a> (cross-list from eess.IV) [<a href="/pdf/2310.06628" title="Download PDF">pdf</a>, <a href="/format/2310.06628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Cardiac MRI Reconstruction with ADMM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yiasemis%2C+G">George Yiasemis</a>, 
<a href="/search/eess?searchtype=author&query=Moriakov%2C+N">Nikita Moriakov</a>, 
<a href="/search/eess?searchtype=author&query=Sonke%2C+J">Jan-Jakob Sonke</a>, 
<a href="/search/eess?searchtype=author&query=Teuwen%2C+J">Jonas Teuwen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures, 2 tables. CMRxRecon Challenge, MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Cardiac magnetic resonance imaging is a valuable non-invasive tool for
identifying cardiovascular diseases. For instance, Cine MRI is the benchmark
modality for assessing the cardiac function and anatomy. On the other hand,
multi-contrast (T1 and T2) mapping has the potential to assess pathologies and
abnormalities in the myocardium and interstitium. However, voluntary
breath-holding and often arrhythmia, in combination with MRI's slow imaging
speed, can lead to motion artifacts, hindering real-time acquisition image
quality. Although performing accelerated acquisitions can facilitate dynamic
imaging, it induces aliasing, causing low reconstructed image quality in Cine
MRI and inaccurate T1 and T2 mapping estimation. In this work, inspired by
related work in accelerated MRI reconstruction, we present a deep learning
(DL)-based method for accelerated cine and multi-contrast reconstruction in the
context of dynamic cardiac imaging. We formulate the reconstruction problem as
a least squares regularized optimization task, and employ vSHARP, a
state-of-the-art DL-based inverse problem solver, which incorporates
half-quadratic variable splitting and the alternating direction method of
multipliers with neural networks. We treat the problem in two setups; a 2D
reconstruction and a 2D dynamic reconstruction task, and employ 2D and 3D deep
learning networks, respectively. Our method optimizes in both the image and
k-space domains, allowing for high reconstruction fidelity. Although the target
data is undersampled with a Cartesian equispaced scheme, we train our model
using both Cartesian and simulated non-Cartesian undersampling schemes to
enhance generalization of the model to unseen data. Furthermore, our model
adopts a deep neural network to learn and refine the sensitivity maps of
multi-coil k-space data. Lastly, our method is jointly trained on both,
undersampled cine and multi-contrast data.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06665" title="Abstract">arXiv:2310.06665</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2310.06665" title="Download PDF">pdf</a>, <a href="/format/2310.06665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite difference method in prolate spheroidal coordinates for freely  suspended spheroidal particles in linear flows of viscous and viscoelastic  fluids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sharma%2C+A">Arjun Sharma</a>, 
<a href="/search/physics?searchtype=author&query=Koch%2C+D+L">Donald L. Koch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 12 figures. Accepted at Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Soft Condensed Matter (cond-mat.soft); Computational Engineering, Finance, and Science (cs.CE); Numerical Analysis (math.NA)

</div>
<p class="mathjax">A finite difference scheme is used to develop a numerical method to solve the
flow of an unbounded viscoelastic fluid with zero to moderate inertia around a
prolate spheroidal particle. The equations are written in prolate spheroidal
coordinates, and the shape of the particle is exactly resolved as one of the
coordinate surfaces representing the inner boundary of the computational
domain. As the prolate spheroidal grid is naturally clustered near the particle
surface, good resolution is obtained in the regions where the gradients of
relevant flow variables are most significant. This coordinate system also
allows large domain sizes with a reasonable number of mesh points to simulate
unbounded fluid around a particle. Changing the aspect ratio of the inner
computational boundary enables simulations of different particle shapes ranging
from a sphere to a slender fiber. Numerical studies of the latter particle
shape allow testing of slender body theories. The mass and momentum equations
are solved with a Schur complement approach allowing us to solve the zero
inertia case necessary to isolate the viscoelastic effects. The singularities
associated with the coordinate system are overcome using L'Hopital's rule. A
straightforward imposition of conditions representing a time-varying
combination of linear flows on the outer boundary allows us to study various
flows with the same computational domain geometry. {For the special but
important case of zero fluid and particle inertia we obtain a novel formulation
that satisfies the force- and torque-free constraint in an iteration-free
manner.} The numerical method is demonstrated for various flows of Newtonian
and viscoelastic fluids around spheres and spheroids (including those with
large aspect ratio). Good agreement is demonstrated with existing theoretical
and numerical results.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06686" title="Abstract">arXiv:2310.06686</a> (cross-list from math.PR) [<a href="/pdf/2310.06686" title="Download PDF">pdf</a>, <a href="/ps/2310.06686" title="Download PostScript">ps</a>, <a href="/format/2310.06686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Wick Decompositions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=MacLeod%2C+C">Chris MacLeod</a>, 
<a href="/search/math?searchtype=author&query=Nitishinskaya%2C+E">Evgenia Nitishinskaya</a>, 
<a href="/search/math?searchtype=author&query=Shlegeris%2C+B">Buck Shlegeris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We review the cumulant decomposition (a way of decomposing the expectation of
a product of random variables (e.g. $\mathbb{E}[XYZ]$) into a sum of terms
corresponding to partitions of these variables.) and the Wick decomposition (a
way of decomposing a product of (not necessarily random) variables into a sum
of terms corresponding to subsets of the variables). Then we generalize each
one to a new decomposition where the product function is generalized to an
arbitrary function.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06725" title="Abstract">arXiv:2310.06725</a> (cross-list from q-bio.BM) [<a href="/pdf/2310.06725" title="Download PDF">pdf</a>, <a href="/format/2310.06725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Growing ecosystem of deep learning methods for modeling  protein$\unicode{x2013}$protein interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Rogers%2C+J+R">Julia R. Rogers</a>, 
<a href="/search/q-bio?searchtype=author&query=Nikol%C3%A9nyi%2C+G">Gerg&#x151; Nikol&#xe9;nyi</a>, 
<a href="/search/q-bio?searchtype=author&query=AlQuraishi%2C+M">Mohammed AlQuraishi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Numerous cellular functions rely on protein$\unicode{x2013}$protein
interactions. Efforts to comprehensively characterize them remain challenged
however by the diversity of molecular recognition mechanisms employed within
the proteome. Deep learning has emerged as a promising approach for tackling
this problem by exploiting both experimental data and basic biophysical
knowledge about protein interactions. Here, we review the growing ecosystem of
deep learning methods for modeling protein interactions, highlighting the
diversity of these biophysically-informed models and their respective
trade-offs. We discuss recent successes in using representation learning to
capture complex features pertinent to predicting protein interactions and
interaction sites, geometric deep learning to reason over protein structures
and predict complex structures, and generative modeling to design de novo
protein assemblies. We also outline some of the outstanding challenges and
promising new directions. Opportunities abound to discover novel interactions,
elucidate their physical mechanisms, and engineer binders to modulate their
functions using deep learning and, ultimately, unravel how protein interactions
orchestrate complex cellular behaviors.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06733" title="Abstract">arXiv:2310.06733</a> (cross-list from math.OC) [<a href="/pdf/2310.06733" title="Download PDF">pdf</a>, <a href="/format/2310.06733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Preconditioned Gradient Descent with Energy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+H">Hailiang Liu</a>, 
<a href="/search/math?searchtype=author&query=Nurbekyan%2C+L">Levon Nurbekyan</a>, 
<a href="/search/math?searchtype=author&query=Tian%2C+X">Xuping Tian</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+Y">Yunan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We propose an adaptive time step with energy for a large class of
preconditioned gradient descent methods, mainly applied to constrained
optimization problems. Our strategy relies on representing the usual descent
direction by the product of an energy variable and a transformed gradient, with
a preconditioning matrix, for example, to reflect the natural gradient induced
by the underlying metric in parameter space or to endow a projection operator
when linear equality constraints are present. We present theoretical results on
both unconditional stability and convergence rates for three respective classes
of objective functions. In addition, our numerical results shed light on the
excellent performance of the proposed method on several benchmark optimization
problems.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06737" title="Abstract">arXiv:2310.06737</a> (cross-list from eess.IV) [<a href="/pdf/2310.06737" title="Download PDF">pdf</a>, <a href="/format/2310.06737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-domain improves out-of-distribution and data-limited scenarios for  medical image analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ozkan%2C+E">Ece Ozkan</a>, 
<a href="/search/eess?searchtype=author&query=Boix%2C+X">Xavier Boix</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Current machine learning methods for medical image analysis primarily focus
on developing models tailored for their specific tasks, utilizing data within
their target domain. These specialized models tend to be data-hungry and often
exhibit limitations in generalizing to out-of-distribution samples. Recently,
foundation models have been proposed, which combine data from various domains
and demonstrate excellent generalization capabilities. Building upon this, this
work introduces the incorporation of diverse medical image domains, including
different imaging modalities like X-ray, MRI, CT, and ultrasound images, as
well as various viewpoints such as axial, coronal, and sagittal views. We refer
to this approach as multi-domain model and compare its performance to that of
specialized models. Our findings underscore the superior generalization
capabilities of multi-domain models, particularly in scenarios characterized by
limited data availability and out-of-distribution, frequently encountered in
healthcare applications. The integration of diverse data allows multi-domain
models to utilize shared information across domains, enhancing the overall
outcomes significantly. To illustrate, for organ recognition, multi-domain
model can enhance accuracy by up to 10% compared to conventional specialized
models.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06742" title="Abstract">arXiv:2310.06742</a> (cross-list from math.OC) [<a href="/pdf/2310.06742" title="Download PDF">pdf</a>, <a href="/format/2310.06742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimality of Finite-Memory Codes and Reinforcement Learning for  Zero-Delay Coding of Markov Sources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cregg%2C+L">Liam Cregg</a>, 
<a href="/search/math?searchtype=author&query=Alajaji%2C+F">Fady Alajaji</a>, 
<a href="/search/math?searchtype=author&query=Yuksel%2C+S">Serdar Yuksel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 American Control Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We study the problem of zero-delay coding of a Markov source over a noisy
channel with feedback. We first formulate the problem as a Markov decision
process (MDP) where the state is a previous belief term along with a finite
memory of channel outputs and quantizers. We then approximate this state by
marginalizing over all possible beliefs, so that our policies only use the
finite-memory term to encode the source. Under an appropriate notion of
predictor stability, we show that such policies are near-optimal for the
zero-delay coding problem as the memory length increases. We also give
sufficient conditions for predictor stability to hold, and propose a
reinforcement learning algorithm to compute near-optimal finite-memory
policies. These theoretical results are supported by simulations.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06823" title="Abstract">arXiv:2310.06823</a> (cross-list from stat.ML) [<a href="/pdf/2310.06823" title="Download PDF">pdf</a>, <a href="/format/2310.06823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NECO: NEural Collapse Based Out-of-distribution detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ammar%2C+M+B">Mou&#xef;n Ben Ammar</a>, 
<a href="/search/stat?searchtype=author&query=Belkhir%2C+N">Nacim Belkhir</a>, 
<a href="/search/stat?searchtype=author&query=Popescu%2C+S">Sebastian Popescu</a>, 
<a href="/search/stat?searchtype=author&query=Manzanera%2C+A">Antoine Manzanera</a>, 
<a href="/search/stat?searchtype=author&query=Franchi%2C+G">Gianni Franchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse'', a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse'' and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. We plan to release the code after the anonymity
period.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06826" title="Abstract">arXiv:2310.06826</a> (cross-list from math.CO) [<a href="/pdf/2310.06826" title="Download PDF">pdf</a>, <a href="/format/2310.06826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding cliques and dense subgraphs using edge queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cs%C3%B3ka%2C+E">Endre Cs&#xf3;ka</a>, 
<a href="/search/math?searchtype=author&query=Pongr%C3%A1cz%2C+A">Andr&#xe1;s Pongr&#xe1;cz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pp, 5 figures, Focused Workshop on Networks and Their Limits held at the Erd\H{o}s Center, Budapest, Hungary in July 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We consider the problem of finding a large clique in an Erd\H{o}s--R\'enyi
random graph where we are allowed unbounded computational time but can only
query a limited number of edges. Recall that the largest clique in $G \sim
G(n,1/2)$ has size roughly $2\log_{2} n$. Let $\alpha_{\star}(\delta,\ell)$ be
the supremum over $\alpha$ such that there exists an algorithm that makes
$n^{\delta}$ queries in total to the adjacency matrix of $G$, in a constant
$\ell$ number of rounds, and outputs a clique of size $\alpha \log_{2} n$ with
high probability. We give improved upper bounds on
$\alpha_{\star}(\delta,\ell)$ for every $\delta \in [1,2)$ and $\ell \geq 3$.
We also study analogous questions for finding subgraphs with density at least
$\eta$ for a given $\eta$, and prove corresponding impossibility results.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed, 11 Oct 23</h3>
<dl>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.09143" title="Abstract">arXiv:1910.09143</a> (replaced) [<a href="/pdf/1910.09143" title="Download PDF">pdf</a>, <a href="/format/1910.09143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Subgoal-based Exploration via Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yijia Wang</a>, 
<a href="/search/math?searchtype=author&query=Poloczek%2C+M">Matthias Poloczek</a>, 
<a href="/search/math?searchtype=author&query=Jiang%2C+D+R">Daniel R. Jiang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (09/2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2005.00797" title="Abstract">arXiv:2005.00797</a> (replaced) [<a href="/pdf/2005.00797" title="Download PDF">pdf</a>, <a href="/ps/2005.00797" title="Download PostScript">ps</a>, <a href="/format/2005.00797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-consensus Decentralized Accelerated Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haishan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Luo Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Ziang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.10349" title="Abstract">arXiv:2107.10349</a> (replaced) [<a href="/pdf/2107.10349" title="Download PDF">pdf</a>, <a href="/format/2107.10349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Cantor Derivative Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fern%C3%A1ndez-Duque%2C+D">David Fern&#xe1;ndez-Duque</a>, 
<a href="/search/math?searchtype=author&query=Montacute%2C+Y">Yo&#xe0;v Montacute</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in Computer Science Logic (CSL) 2022 Proceedings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06095" title="Abstract">arXiv:2202.06095</a> (replaced) [<a href="/pdf/2202.06095" title="Download PDF">pdf</a>, <a href="/format/2202.06095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Deep Learning-based Approaches for Deepfake Content  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Passos%2C+L+A">Leandro A. Passos</a>, 
<a href="/search/cs?searchtype=author&query=Jodas%2C+D">Danilo Jodas</a>, 
<a href="/search/cs?searchtype=author&query=da+Costa%2C+K+A+P">Kelton A. P. da Costa</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%BAnior%2C+L+A+S">Luis A. Souza J&#xfa;nior</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+D">Douglas Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Del+Ser%2C+J">Javier Del Ser</a>, 
<a href="/search/cs?searchtype=author&query=Camacho%2C+D">David Camacho</a>, 
<a href="/search/cs?searchtype=author&query=Papa%2C+J+P">Jo&#xe3;o Paulo Papa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.07868" title="Abstract">arXiv:2202.07868</a> (replaced) [<a href="/pdf/2202.07868" title="Download PDF">pdf</a>, <a href="/ps/2202.07868" title="Download PostScript">ps</a>, <a href="/format/2202.07868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Minimax Optimization with Expectation Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuoguang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xudong Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+G">Guanghui Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.10115" title="Abstract">arXiv:2202.10115</a> (replaced) [<a href="/pdf/2202.10115" title="Download PDF">pdf</a>, <a href="/format/2202.10115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Smoothing and Thresholding Image Segmentation Framework  with Weighted Anisotropic-Isotropic Total Variation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bui%2C+K">Kevin Bui</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Y">Yifei Lou</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+F">Fredrick Park</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jack Xin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to Springer CAMC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.11219" title="Abstract">arXiv:2202.11219</a> (replaced) [<a href="/pdf/2202.11219" title="Download PDF">pdf</a>, <a href="/format/2202.11219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No-Regret Learning with Unbounded Losses: The Case of Logarithmic  Pooling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neyman%2C+E">Eric Neyman</a>, 
<a href="/search/cs?searchtype=author&query=Roughgarden%2C+T">Tim Roughgarden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.12361" title="Abstract">arXiv:2202.12361</a> (replaced) [<a href="/pdf/2202.12361" title="Download PDF">pdf</a>, <a href="/format/2202.12361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset  for Natural Disaster Damage Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahnemoonfar%2C+M">Maryam Rahnemoonfar</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+T">Tashnim Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+R">Robin Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.09314" title="Abstract">arXiv:2203.09314</a> (replaced) [<a href="/pdf/2203.09314" title="Download PDF">pdf</a>, <a href="/format/2203.09314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sparse Grids Matlab kit -- a Matlab implementation of sparse grids  for high-dimensional function approximation and uncertainty quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piazzola%2C+C">Chiara Piazzola</a>, 
<a href="/search/cs?searchtype=author&query=Tamellini%2C+L">Lorenzo Tamellini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.01382" title="Abstract">arXiv:2204.01382</a> (replaced) [<a href="/pdf/2204.01382" title="Download PDF">pdf</a>, <a href="/format/2204.01382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Global Convergence of Stochastic Fictitious Play in Stochastic  Games with Turn-based Controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sayin%2C+M+O">Muhammed O. Sayin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a corrected version of the conference paper: "M. O. Sayin. On the global convergence of stochastic fictitious play in stochastic games with turn-based controllers. in IEEE Conference on Decision and Control (CDC), pages 2851-2856, 2022."
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.04297" title="Abstract">arXiv:2204.04297</a> (replaced) [<a href="/pdf/2204.04297" title="Download PDF">pdf</a>, <a href="/format/2204.04297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Modulate Random Weights: Neuromodulation-inspired Neural  Networks For Efficient Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jinyung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Pavlic%2C+T+P">Theodore P. Pavlic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 13 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.15449" title="Abstract">arXiv:2205.15449</a> (replaced) [<a href="/pdf/2205.15449" title="Download PDF">pdf</a>, <a href="/format/2205.15449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior and Computational Uncertainty in Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wenger%2C+J">Jonathan Wenger</a>, 
<a href="/search/cs?searchtype=author&query=Pleiss%2C+G">Geoff Pleiss</a>, 
<a href="/search/cs?searchtype=author&query=Pf%C3%B6rtner%2C+M">Marvin Pf&#xf6;rtner</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>, 
<a href="/search/cs?searchtype=author&query=Cunningham%2C+J+P">John P. Cunningham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Advances in Neural Information Processing Systems (NeurIPS 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00311" title="Abstract">arXiv:2206.00311</a> (replaced) [<a href="/pdf/2206.00311" title="Download PDF">pdf</a>, <a href="/format/2206.00311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+P">Pengyuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengquan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shanshan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+M">Meina Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yangliu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Kun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Junyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+E">Errui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05330" title="Abstract">arXiv:2206.05330</a> (replaced) [<a href="/pdf/2206.05330" title="Download PDF">pdf</a>, <a href="/format/2206.05330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Gender Gap in Scholarly Self-Promotion on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Teplitskiy%2C+M">Misha Teplitskiy</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+D+M">Daniel M. Romero</a>, 
<a href="/search/cs?searchtype=author&query=Horv%C3%A1t%2C+E">Em&#x151;ke-&#xc1;gnes Horv&#xe1;t</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05897" title="Abstract">arXiv:2206.05897</a> (replaced) [<a href="/pdf/2206.05897" title="Download PDF">pdf</a>, <a href="/format/2206.05897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\texttt{GradICON}$: Approximate Diffeomorphisms via Gradient Inverse  Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+L">Lin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Greer%2C+H">Hastings Greer</a>, 
<a href="/search/cs?searchtype=author&query=Vialard%2C+F">Fran&#xe7;ois-Xavier Vialard</a>, 
<a href="/search/cs?searchtype=author&query=Kwitt%2C+R">Roland Kwitt</a>, 
<a href="/search/cs?searchtype=author&query=Est%C3%A9par%2C+R+S+J">Ra&#xfa;l San Jos&#xe9; Est&#xe9;par</a>, 
<a href="/search/cs?searchtype=author&query=Rushmore%2C+R+J">Richard Jarrett Rushmore</a>, 
<a href="/search/cs?searchtype=author&query=Makris%2C+N">Nikolaos Makris</a>, 
<a href="/search/cs?searchtype=author&query=Bouix%2C+S">Sylvain Bouix</a>, 
<a href="/search/cs?searchtype=author&query=Niethammer%2C+M">Marc Niethammer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 16 figures, CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12993" title="Abstract">arXiv:2207.12993</a> (replaced) [<a href="/pdf/2207.12993" title="Download PDF">pdf</a>, <a href="/ps/2207.12993" title="Download PostScript">ps</a>, <a href="/format/2207.12993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of Electromechanical Switching Devices: A Study of  Hysteretic Switching Behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ramirez-Laboreo%2C+E">Edgar Ramirez-Laboreo</a> (1), 
<a href="/search/eess?searchtype=author&query=Sagues%2C+C">Carlos Sagues</a> (1), 
<a href="/search/eess?searchtype=author&query=Moya-Lasheras%2C+E">Eduardo Moya-Lasheras</a> (1), 
<a href="/search/eess?searchtype=author&query=Serrano-Seco%2C+E">Eloy Serrano-Seco</a> (1) ((1) Universidad de Zaragoza)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13126" title="Abstract">arXiv:2207.13126</a> (replaced) [<a href="/pdf/2207.13126" title="Download PDF">pdf</a>, <a href="/ps/2207.13126" title="Download PostScript">ps</a>, <a href="/format/2207.13126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample Complexity of Forecast Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09897" title="Abstract">arXiv:2208.09897</a> (replaced) [<a href="/pdf/2208.09897" title="Download PDF">pdf</a>, <a href="/format/2208.09897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Descent in the Multiple Random Feature Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Meng%2C+X">Xuran Meng</a>, 
<a href="/search/math?searchtype=author&query=Yao%2C+J">Jianfeng Yao</a>, 
<a href="/search/math?searchtype=author&query=Cao%2C+Y">Yuan Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 89 pages, 9 figures. Version 3 adds new description of triple descent in certain double random feature model, deletes the discussion of NTK regimes, and adds more literature references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.13305" title="Abstract">arXiv:2208.13305</a> (replaced) [<a href="/pdf/2208.13305" title="Download PDF">pdf</a>, <a href="/format/2208.13305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network Approximation of Continuous Functions in High Dimensions  with Applications to Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Karnik%2C+S">Santhosh Karnik</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>, 
<a href="/search/stat?searchtype=author&query=Iwen%2C+M">Mark Iwen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.13876" title="Abstract">arXiv:2208.13876</a> (replaced) [<a href="/pdf/2208.13876" title="Download PDF">pdf</a>, <a href="/format/2208.13876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Barnes double gamma function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alexanian%2C+S">Shahen Alexanian</a>, 
<a href="/search/math?searchtype=author&query=Kuznetsov%2C+A">Alexey Kuznetsov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 2 figures. In this version we added Remark 1 on page 14
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.00303" title="Abstract">arXiv:2209.00303</a> (replaced) [<a href="/pdf/2209.00303" title="Download PDF">pdf</a>, <a href="/format/2209.00303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis and Numerical Approximation of Stationary Second-Order Mean  Field Game Partial Differential Inclusions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Osborne%2C+Y+A+P">Yohance A. P. Osborne</a>, 
<a href="/search/math?searchtype=author&query=Smears%2C+I">Iain Smears</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05598" title="Abstract">arXiv:2209.05598</a> (replaced) [<a href="/pdf/2209.05598" title="Download PDF">pdf</a>, <a href="/format/2209.05598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning domain-specific causal discovery from time series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kording%2C+K+P">Konrad Paul Kording</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 main pages, 7 figures. Accepted by TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.06948" title="Abstract">arXiv:2209.06948</a> (replaced) [<a href="/pdf/2209.06948" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated 2D and 3D Finite Element Overclosure Adjustment and Mesh  Morphing Using Generalized Regression Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Andreassen%2C+T+E">Thor E. Andreassen</a>, 
<a href="/search/q-bio?searchtype=author&query=Hume%2C+D+R">Donald R. Hume</a>, 
<a href="/search/q-bio?searchtype=author&query=Hamilton%2C+L+D">Landon D. Hamilton</a>, 
<a href="/search/q-bio?searchtype=author&query=Higinbotham%2C+S+E">Sean E. Higinbotham</a>, 
<a href="/search/q-bio?searchtype=author&query=Shelburne%2C+K+B">Kevin B. Shelburne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updates were made to the original article to include a new algorithm capable of rapid morphing of generated meshes, and the validation included. New Paper total is 40 Pages, 10 Figures, 4 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07825" title="Abstract">arXiv:2209.07825</a> (replaced) [<a href="/pdf/2209.07825" title="Download PDF">pdf</a>, <a href="/format/2209.07825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A System of Interaction and Structure III: The Complexity of BV and  Pomset Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguy%C3%AAn%2C+L+T+D">L&#xea; Th&#xe0;nh D&#x169;ng Nguy&#xea;n</a>, 
<a href="/search/cs?searchtype=author&query=Stra%C3%9Fburger%2C+L">Lutz Stra&#xdf;burger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.13808" title="Abstract">arXiv:2209.13808</a> (replaced) [<a href="/pdf/2209.13808" title="Download PDF">pdf</a>, <a href="/format/2209.13808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Video Temporal Action Segmentation In Real Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wujun Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhuben Dong</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wanxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenlan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ISKE2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.14699" title="Abstract">arXiv:2209.14699</a> (replaced) [<a href="/pdf/2209.14699" title="Download PDF">pdf</a>, <a href="/format/2209.14699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARQ-based Average Consensus over Unreliable Directed Network Topologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Makridis%2C+E">Evagoras Makridis</a>, 
<a href="/search/eess?searchtype=author&query=Charalambous%2C+T">Themistoklis Charalambous</a>, 
<a href="/search/eess?searchtype=author&query=Hadjicostis%2C+C+N">Christoforos N. Hadjicostis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00848" title="Abstract">arXiv:2210.00848</a> (replaced) [<a href="/pdf/2210.00848" title="Download PDF">pdf</a>, <a href="/format/2210.00848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Trustworthy Neural Program Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Key%2C+D">Darren Key</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wen-Ding Li</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+K">Kevin Ellis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01189" title="Abstract">arXiv:2210.01189</a> (replaced) [<a href="/pdf/2210.01189" title="Download PDF">pdf</a>, <a href="/format/2210.01189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rank-N-Contrast: Learning Continuous Representations for Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zha%2C+K">Kaiwen Zha</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Peng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Son%2C+J">Jeany Son</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuzhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Katabi%2C+D">Dina Katabi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Spotlight. The first two authors contributed equally to this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03729" title="Abstract">arXiv:2210.03729</a> (replaced) [<a href="/pdf/2210.03729" title="Download PDF">pdf</a>, <a href="/format/2210.03729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Attention-Based Multi-Policy Fusion for Efficient Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiu%2C+Z">Zih-Yun Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+Y">Yi-Lin Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yip%2C+M+C">Michael C. Yip</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.13869" title="Abstract">arXiv:2210.13869</a> (replaced) [<a href="/pdf/2210.13869" title="Download PDF">pdf</a>, <a href="/format/2210.13869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jet tagging algorithm of graph network with HaarPooling message passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Ma%2C+F">Fei Ma</a>, 
<a href="/search/hep-ex?searchtype=author&query=Liu%2C+F">Feiyi Liu</a>, 
<a href="/search/hep-ex?searchtype=author&query=Li%2C+W">Wei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); High Energy Physics - Phenomenology (hep-ph)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15148" title="Abstract">arXiv:2210.15148</a> (replaced) [<a href="/pdf/2210.15148" title="Download PDF">pdf</a>, <a href="/format/2210.15148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ranking Edges by their Impact on the Spectral Complexity of Information  Diffusion over Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kazimer%2C+J">Jeremy Kazimer</a>, 
<a href="/search/physics?searchtype=author&query=de+Domenico%2C+M">Manlio de Domenico</a>, 
<a href="/search/physics?searchtype=author&query=Mucha%2C+P+J">Peter J. Mucha</a>, 
<a href="/search/physics?searchtype=author&query=Taylor%2C+D">Dane Taylor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 9 figures. Revision based on submission to SIAM MMS journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15724" title="Abstract">arXiv:2210.15724</a> (replaced) [<a href="/pdf/2210.15724" title="Download PDF">pdf</a>, <a href="/ps/2210.15724" title="Download PostScript">ps</a>, <a href="/format/2210.15724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong formulations of the generalised Navier-Stokes momentum equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Basic%2C+J">Josip Basic</a>, 
<a href="/search/physics?searchtype=author&query=Basic%2C+M">Martina Basic</a>, 
<a href="/search/physics?searchtype=author&query=Blagojevic%2C+B">Branko Blagojevic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.16075" title="Abstract">arXiv:2210.16075</a> (replaced) [<a href="/pdf/2210.16075" title="Download PDF">pdf</a>, <a href="/ps/2210.16075" title="Download PostScript">ps</a>, <a href="/format/2210.16075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Hamiltonian Particle methods for Vlasov--Poisson  equations with a nonhomogeneous magnetic field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gu%2C+A">Anjiao Gu</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+Y">Yajuan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.16947" title="Abstract">arXiv:2210.16947</a> (replaced) [<a href="/pdf/2210.16947" title="Download PDF">pdf</a>, <a href="/format/2210.16947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Models are Better than One: Federated Learning Is Not Private For  Google GBoard Next Word Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suliman%2C+M">Mohamed Suliman</a>, 
<a href="/search/cs?searchtype=author&query=Leith%2C+D">Douglas Leith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ESORICS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01126" title="Abstract">arXiv:2211.01126</a> (replaced) [<a href="/pdf/2211.01126" title="Download PDF">pdf</a>, <a href="/format/2211.01126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Likelihood-free hypothesis testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gerber%2C+P+R">Patrik R&#xf3;bert Gerber</a>, 
<a href="/search/math?searchtype=author&query=Polyanskiy%2C+Y">Yury Polyanskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06321" title="Abstract">arXiv:2211.06321</a> (replaced) [<a href="/pdf/2211.06321" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sociodemographic inequalities in student achievement: An intersectional  multilevel analysis of individual heterogeneity and discriminatory accuracy  (MAIHDA) with application to students in London, England
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Prior%2C+L">Lucy Prior</a>, 
<a href="/search/stat?searchtype=author&query=Evans%2C+C">Clare Evans</a>, 
<a href="/search/stat?searchtype=author&query=Merlo%2C+J">Juan Merlo</a>, 
<a href="/search/stat?searchtype=author&query=Leckie%2C+G">George Leckie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 63 pages (main text 39 pages), 10 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.08559" title="Abstract">arXiv:2211.08559</a> (replaced) [<a href="/pdf/2211.08559" title="Download PDF">pdf</a>, <a href="/format/2211.08559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Alzheimer&#x27;s Progression Modeling using Cross-Domain  Self-Supervised Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dadsetan%2C+S">Saba Dadsetan</a>, 
<a href="/search/cs?searchtype=author&query=Hejrati%2C+M">Mohsen Hejrati</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shandong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hashemifar%2C+S">Somaye Hashemifar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been published at the Transactions on Machine Learning Research (TMLR) journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09359" title="Abstract">arXiv:2211.09359</a> (replaced) [<a href="/pdf/2211.09359" title="Download PDF">pdf</a>, <a href="/format/2211.09359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Fine-Tune Vision Models with SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Ananya Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+R">Ruoqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Bubeck%2C+S">Sebastien Bubeck</a>, 
<a href="/search/cs?searchtype=author&query=Gunasekar%2C+S">Suriya Gunasekar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12929" title="Abstract">arXiv:2211.12929</a> (replaced) [<a href="/pdf/2211.12929" title="Download PDF">pdf</a>, <a href="/format/2211.12929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can the PageRank centrality be manipulated to obtain any desired  ranking?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Contreras-Aso%2C+G">Gonzalo Contreras-Aso</a>, 
<a href="/search/physics?searchtype=author&query=Criado%2C+R">Regino Criado</a>, 
<a href="/search/physics?searchtype=author&query=Romance%2C+M">Miguel Romance</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Chaos 33, 083152 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02014" title="Abstract">arXiv:2212.02014</a> (replaced) [<a href="/pdf/2212.02014" title="Download PDF">pdf</a>, <a href="/format/2212.02014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query  Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Heng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Ke Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Le Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minfeng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02327" title="Abstract">arXiv:2212.02327</a> (replaced) [<a href="/pdf/2212.02327" title="Download PDF">pdf</a>, <a href="/ps/2212.02327" title="Download PostScript">ps</a>, <a href="/format/2212.02327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space-efficient conversions from SLPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gagie%2C+T">Travis Gagie</a>, 
<a href="/search/cs?searchtype=author&query=Goga%2C+A">Adri&#xe1;n Goga</a>, 
<a href="/search/cs?searchtype=author&query=Je%C5%BC%2C+A">Artur Je&#x17c;</a>, 
<a href="/search/cs?searchtype=author&query=Navarro%2C+G">Gonzalo Navarro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02495" title="Abstract">arXiv:2212.02495</a> (replaced) [<a href="/pdf/2212.02495" title="Download PDF">pdf</a>, <a href="/format/2212.02495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Binary Tree Schemes for Computing Zernike Radial Polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+H">Hong-Yan Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+Z">Zhi-Qiang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, 11(11):106567-106579, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02771" title="Abstract">arXiv:2212.02771</a> (replaced) [<a href="/pdf/2212.02771" title="Download PDF">pdf</a>, <a href="/format/2212.02771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of large exact subgraph isomorphisms with a topology-only  graphlet index built using deterministic walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Patrick Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Henry Ye</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+W">Wayne Hayes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 11 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03749" title="Abstract">arXiv:2212.03749</a> (replaced) [<a href="/pdf/2212.03749" title="Download PDF">pdf</a>, <a href="/format/2212.03749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memorization of Named Entities in Fine-tuned BERT Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diera%2C+A">Andor Diera</a>, 
<a href="/search/cs?searchtype=author&query=Lell%2C+N">Nicolas Lell</a>, 
<a href="/search/cs?searchtype=author&query=Garifullina%2C+A">Aygul Garifullina</a>, 
<a href="/search/cs?searchtype=author&query=Scherp%2C+A">Ansgar Scherp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at CD-MAKE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04064" title="Abstract">arXiv:2212.04064</a> (replaced) [<a href="/pdf/2212.04064" title="Download PDF">pdf</a>, <a href="/ps/2212.04064" title="Download PostScript">ps</a>, <a href="/format/2212.04064" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRC-Aided High-Rate Convolutional Codes With Short Blocklengths for List  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+W">Wenhui Sui</a>, 
<a href="/search/cs?searchtype=author&query=Towell%2C+B">Brendan Towell</a>, 
<a href="/search/cs?searchtype=author&query=Asmani%2C+A">Ava Asmani</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hengjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Grissett%2C+H">Holden Grissett</a>, 
<a href="/search/cs?searchtype=author&query=Wesel%2C+R+D">Richard D. Wesel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2111.07929">arXiv:2111.07929</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07220" title="Abstract">arXiv:2212.07220</a> (replaced) [<a href="/pdf/2212.07220" title="Download PDF">pdf</a>, <a href="/format/2212.07220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Translationese in Cross-Lingual Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yunlong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiarong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhixu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09289" title="Abstract">arXiv:2212.09289</a> (replaced) [<a href="/pdf/2212.09289" title="Download PDF">pdf</a>, <a href="/format/2212.09289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mining User Privacy Concern Topics from App Reviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianzhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+J">Jinping Hua</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+N">Nan Niu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chuang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.13965" title="Abstract">arXiv:2212.13965</a> (replaced) [<a href="/pdf/2212.13965" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FoldingNet Autoencoder model to create a geospatial grouping of CityGML  building dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+D">Deepank Verma</a>, 
<a href="/search/cs?searchtype=author&query=Mumm%2C+O">Olaf Mumm</a>, 
<a href="/search/cs?searchtype=author&query=Carlow%2C+V+M">Vanessa Miriam Carlow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03288" title="Abstract">arXiv:2301.03288</a> (replaced) [<a href="/pdf/2301.03288" title="Download PDF">pdf</a>, <a href="/format/2301.03288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surfaces 2.0: Beyond Diagonal Phase Shift  Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hongyu Li</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+S">Shanpu Shen</a>, 
<a href="/search/eess?searchtype=author&query=Nerini%2C+M">Matteo Nerini</a>, 
<a href="/search/eess?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, submitted to IEEE journal for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04235" title="Abstract">arXiv:2301.04235</a> (replaced) [<a href="/pdf/2301.04235" title="Download PDF">pdf</a>, <a href="/format/2301.04235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyper-cores promote localization and efficient seeding in higher-order  processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Mancastroppa%2C+M">Marco Mancastroppa</a>, 
<a href="/search/physics?searchtype=author&query=Iacopini%2C+I">Iacopo Iacopini</a>, 
<a href="/search/physics?searchtype=author&query=Petri%2C+G">Giovanni Petri</a>, 
<a href="/search/physics?searchtype=author&query=Barrat%2C+A">Alain Barrat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main document: 13 pages, 6 figures; Supplementary Material: 47 pages, 40 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nat. Commun. 14, 6223 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04494" title="Abstract">arXiv:2301.04494</a> (replaced) [<a href="/pdf/2301.04494" title="Download PDF">pdf</a>, <a href="/format/2301.04494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-label Image Classification using Adaptive Graph Convolutional  Networks: from a Single Domain to Multiple Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+I+P">Indel Pal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbel%2C+E">Enjie Ghorbel</a>, 
<a href="/search/cs?searchtype=author&query=Oyedotun%2C+O">Oyebade Oyedotun</a>, 
<a href="/search/cs?searchtype=author&query=Aouada%2C+D">Djamila Aouada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08707" title="Abstract">arXiv:2301.08707</a> (replaced) [<a href="/pdf/2301.08707" title="Download PDF">pdf</a>, <a href="/format/2301.08707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separating the edges of a graph by a linear number of paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonamy%2C+M">Marthe Bonamy</a>, 
<a href="/search/math?searchtype=author&query=Botler%2C+F">F&#xe1;bio Botler</a>, 
<a href="/search/math?searchtype=author&query=Dross%2C+F">Fran&#xe7;ois Dross</a>, 
<a href="/search/math?searchtype=author&query=Naia%2C+T">T&#xe1;ssio Naia</a>, 
<a href="/search/math?searchtype=author&query=Skokan%2C+J">Jozef Skokan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Combinatorics 2023:6, 7pp
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01961" title="Abstract">arXiv:2302.01961</a> (replaced) [<a href="/pdf/2302.01961" title="Download PDF">pdf</a>, <a href="/format/2302.01961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymmetric Certified Robustness via Feature-Convex Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pfrommer%2C+S">Samuel Pfrommer</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+B+G">Brendon G. Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Piet%2C+J">Julien Piet</a>, 
<a href="/search/cs?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05534" title="Abstract">arXiv:2302.05534</a> (replaced) [<a href="/pdf/2302.05534" title="Download PDF">pdf</a>, <a href="/format/2302.05534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Knowledge Transfer in Tiered Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiawei Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 Pages; 1 Figure; NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07409" title="Abstract">arXiv:2302.07409</a> (replaced) [<a href="/pdf/2302.07409" title="Download PDF">pdf</a>, <a href="/format/2302.07409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Learning Theory Beyond Batch Binary Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohan%2C+P">Preetham Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures, 2 tables; v3: tightens expected regret bounds in various settings under quantum online learning (Section 5); adds a figure to illustrate the quantum circuit used for reduction in Appendix A; incorporates more recent work (as compared to v2); provides explicit open problems in Sections 5.4 and 6;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Quantum Physics (quant-ph); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08861" title="Abstract">arXiv:2302.08861</a> (replaced) [<a href="/pdf/2302.08861" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AliasNet: Alias Artefact Suppression Network for Accelerated  Phase-Encode MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lorenzana%2C+M+E+B">Marlon E. Bran Lorenzana</a>, 
<a href="/search/eess?searchtype=author&query=Chandra%2C+S+S">Shekhar S. Chandra</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+F">Feng Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09907" title="Abstract">arXiv:2302.09907</a> (replaced) [<a href="/pdf/2302.09907" title="Download PDF">pdf</a>, <a href="/format/2302.09907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Rotation Invariance Learning for Point Clouds via Weight-Feature  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Liang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yibo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Binbin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaofei He</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+R">Ronghua Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10034" title="Abstract">arXiv:2302.10034</a> (replaced) [<a href="/pdf/2302.10034" title="Download PDF">pdf</a>, <a href="/format/2302.10034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Over-Parameterization Exponentially Slows Down Gradient Descent for  Learning a Single Neuron
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weihang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, LaTeX; typos corrected; references added;
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of Thirty Sixth Conference on Learning Theory, PMLR
  195:1155-1198, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11520" title="Abstract">arXiv:2302.11520</a> (replaced) [<a href="/pdf/2302.11520" title="Download PDF">pdf</a>, <a href="/format/2302.11520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding Large Language Models via Directional Stimulus Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Baolin Peng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengcheng He</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xifeng Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS2023. The code and data are available at <a href="https://github.com/Leezekun/Directional-Stimulus-Prompting">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14030" title="Abstract">arXiv:2302.14030</a> (replaced) [<a href="/pdf/2302.14030" title="Download PDF">pdf</a>, <a href="/format/2302.14030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Speech Recognition for Language-Guided Embodied Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+A">Allen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaoyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Monga%2C+A">Aarav Monga</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Seoho Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+T">Tejas Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Thomason%2C+J">Jesse Thomason</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of Interspeech 2023, 1608-1612
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00456" title="Abstract">arXiv:2303.00456</a> (replaced) [<a href="/pdf/2303.00456" title="Download PDF">pdf</a>, <a href="/format/2303.00456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses  and Constrained Decoding Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Rao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M+J+F">Mark J. F. Gales</a>, 
<a href="/search/cs?searchtype=author&query=Knill%2C+K+M">Kate M. Knill</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+M">Mengjie Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of INTERSPEECH
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02542" title="Abstract">arXiv:2303.02542</a> (replaced) [<a href="/pdf/2303.02542" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed neural network for friction-involved nonsmooth dynamics  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zilin Li</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jinshuai Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+H">Huajing Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Martelli%2C+S">Saulo Martelli</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Ming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongtao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wei-Ron Han</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuantong Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 Pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04475" title="Abstract">arXiv:2303.04475</a> (replaced) [<a href="/pdf/2303.04475" title="Download PDF">pdf</a>, <a href="/format/2303.04475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RACCER: Towards Reachable and Certain Counterfactual Explanations for  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gajcin%2C+J">Jasmina Gajcin</a>, 
<a href="/search/cs?searchtype=author&query=Dusparic%2C+I">Ivana Dusparic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06222" title="Abstract">arXiv:2303.06222</a> (replaced) [<a href="/pdf/2303.06222" title="Download PDF">pdf</a>, <a href="/format/2303.06222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust MADER: Decentralized Multiagent Trajectory Planner Robust to  Communication Delay in Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kondo%2C+K">Kota Kondo</a>, 
<a href="/search/cs?searchtype=author&query=Figueroa%2C+R">Reinaldo Figueroa</a>, 
<a href="/search/cs?searchtype=author&query=Rached%2C+J">Juan Rached</a>, 
<a href="/search/cs?searchtype=author&query=Tordesillas%2C+J">Jesus Tordesillas</a>, 
<a href="/search/cs?searchtype=author&query=Lusk%2C+P+C">Parker C. Lusk</a>, 
<a href="/search/cs?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pagers, 10 figures,. arXiv admin note: substantial text overlap with <a href="/abs/2209.13667">arXiv:2209.13667</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08423" title="Abstract">arXiv:2303.08423</a> (replaced) [<a href="/pdf/2303.08423" title="Download PDF">pdf</a>, <a href="/format/2303.08423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Design for Quantized Decentralized Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weidong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08676" title="Abstract">arXiv:2303.08676</a> (replaced) [<a href="/pdf/2303.08676" title="Download PDF">pdf</a>, <a href="/ps/2303.08676" title="Download PostScript">ps</a>, <a href="/format/2303.08676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Publicly-Verifiable Deletion via Target-Collapsing Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bartusek%2C+J">James Bartusek</a>, 
<a href="/search/quant-ph?searchtype=author&query=Khurana%2C+D">Dakshita Khurana</a>, 
<a href="/search/quant-ph?searchtype=author&query=Poremba%2C+A">Alexander Poremba</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09609" title="Abstract">arXiv:2303.09609</a> (replaced) [<a href="/pdf/2303.09609" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Nyquist-Like Impedance-Based Criteria for Converter-Based AC  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+C">Chongbin Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Q">Qirong Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Y">Yixin Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CSEE JPES
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09756" title="Abstract">arXiv:2303.09756</a> (replaced) [<a href="/pdf/2303.09756" title="Download PDF">pdf</a>, <a href="/format/2303.09756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Action Recognition with Attentive Semantic Units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dapeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruijin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wei Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICCV 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2023, pp. 10170-10180
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11366" title="Abstract">arXiv:2303.11366</a> (replaced) [<a href="/pdf/2303.11366" title="Download PDF">pdf</a>, <a href="/format/2303.11366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reflexion: Language Agents with Verbal Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shinn%2C+N">Noah Shinn</a>, 
<a href="/search/cs?searchtype=author&query=Cassano%2C+F">Federico Cassano</a>, 
<a href="/search/cs?searchtype=author&query=Berman%2C+E">Edward Berman</a>, 
<a href="/search/cs?searchtype=author&query=Gopinath%2C+A">Ashwin Gopinath</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K">Karthik Narasimhan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v4 contains a few additional experiments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17063" title="Abstract">arXiv:2303.17063</a> (replaced) [<a href="/pdf/2303.17063" title="Download PDF">pdf</a>, <a href="/format/2303.17063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Colosseum as a Digital Twin: Bridging Real-World Experimentation and  Wireless Network Emulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Villa%2C+D">Davide Villa</a>, 
<a href="/search/cs?searchtype=author&query=Tehrani-Moayyed%2C+M">Miead Tehrani-Moayyed</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C+P">Clifton Paul Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Bonati%2C+L">Leonardo Bonati</a>, 
<a href="/search/cs?searchtype=author&query=Johari%2C+P">Pedram Johari</a>, 
<a href="/search/cs?searchtype=author&query=Polese%2C+M">Michele Polese</a>, 
<a href="/search/cs?searchtype=author&query=Basagni%2C+S">Stefano Basagni</a>, 
<a href="/search/cs?searchtype=author&query=Melodia%2C+T">Tommaso Melodia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 23 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00818" title="Abstract">arXiv:2304.00818</a> (replaced) [<a href="/pdf/2304.00818" title="Download PDF">pdf</a>, <a href="/format/2304.00818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swarm Reinforcement Learning For Adaptive Mesh Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freymuth%2C+N">Niklas Freymuth</a>, 
<a href="/search/cs?searchtype=author&query=Dahlinger%2C+P">Philipp Dahlinger</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%BCrth%2C+T">Tobias W&#xfc;rth</a>, 
<a href="/search/cs?searchtype=author&query=Reisch%2C+S">Simon Reisch</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4rger%2C+L">Luise K&#xe4;rger</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Neural Information Processing Systems (NeurIPS) 2023. Version 1 of this paper is a preliminary version that was accepted as a workshop paper in the International Conference on Learning Representations (ICLR) 2023 Workshop on Physics for Machine Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01002" title="Abstract">arXiv:2304.01002</a> (replaced) [<a href="/pdf/2304.01002" title="Download PDF">pdf</a>, <a href="/format/2304.01002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Human Collaboration Enhance the Accuracy of Identifying  LLM-Generated Deepfake Texts?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uchendu%2C+A">Adaku Uchendu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jooyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Hua Shen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T+%27">Ting-Hao &#x27;Kenneth&#x27; Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at The 11th AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03347" title="Abstract">arXiv:2304.03347</a> (replaced) [<a href="/pdf/2304.03347" title="Download PDF">pdf</a>, <a href="/format/2304.03347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Interpretable Mental Health Analysis with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shaoxiong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Z">Ziyan Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference as a long paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03853" title="Abstract">arXiv:2304.03853</a> (replaced) [<a href="/pdf/2304.03853" title="Download PDF">pdf</a>, <a href="/format/2304.03853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StepMix: A Python Package for Pseudo-Likelihood Estimation of  Generalized Mixture Models with External Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Morin%2C+S">Sacha Morin</a>, 
<a href="/search/stat?searchtype=author&query=Legault%2C+R">Robin Legault</a>, 
<a href="/search/stat?searchtype=author&query=Lalibert%C3%A9%2C+F">F&#xe9;lix Lalibert&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Bakk%2C+Z">Zsuzsa Bakk</a>, 
<a href="/search/stat?searchtype=author&query=Gigu%C3%A8re%2C+C">Charles-&#xc9;douard Gigu&#xe8;re</a>, 
<a href="/search/stat?searchtype=author&query=de+la+Sablonni%C3%A8re%2C+R">Roxane de la Sablonni&#xe8;re</a>, 
<a href="/search/stat?searchtype=author&query=Lacourse%2C+%C3%89">&#xc9;ric Lacourse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Sacha Morin and Robin Legault contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04193" title="Abstract">arXiv:2304.04193</a> (replaced) [<a href="/pdf/2304.04193" title="Download PDF">pdf</a>, <a href="/format/2304.04193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extractive Summarization via ChatGPT for Faithful Summary Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04441" title="Abstract">arXiv:2304.04441</a> (replaced) [<a href="/pdf/2304.04441" title="Download PDF">pdf</a>, <a href="/format/2304.04441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-training with dual uncertainty for semi-supervised medical image  segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zhanhong Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+H">Haitao Gan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Ming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhongwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04531" title="Abstract">arXiv:2304.04531</a> (replaced) [<a href="/pdf/2304.04531" title="Download PDF">pdf</a>, <a href="/ps/2304.04531" title="Download PostScript">ps</a>, <a href="/format/2304.04531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alon-Tarsi Number of Some Regular Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=S%2C+P">Prajnanaswaroopa S</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05292" title="Abstract">arXiv:2304.05292</a> (replaced) [<a href="/pdf/2304.05292" title="Download PDF">pdf</a>, <a href="/format/2304.05292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive  Impairment in older adults using facial videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Dodge%2C+H+H">Hiroko H. Dodge</a>, 
<a href="/search/cs?searchtype=author&query=Mahoor%2C+M+H">Mohammad H. Mahoor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 tables, 7 figures, 9 equations
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications, 238, 121929 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05708" title="Abstract">arXiv:2304.05708</a> (replaced) [<a href="/pdf/2304.05708" title="Download PDF">pdf</a>, <a href="/format/2304.05708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Domain Decomposition Based on Variable-Separation Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+Y">Yaru Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Q">Qiuqi Li</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhiwen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06479" title="Abstract">arXiv:2304.06479</a> (replaced) [<a href="/pdf/2304.06479" title="Download PDF">pdf</a>, <a href="/format/2304.06479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Search Approaches to Sampling-Based Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lathrop%2C+P">Paul Lathrop</a>, 
<a href="/search/quant-ph?searchtype=author&query=Boardman%2C+B">Beth Boardman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mart%C3%ADnez%2C+S">Sonia Mart&#xed;nez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, vol. 11, pp. 89506-89519, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07013" title="Abstract">arXiv:2304.07013</a> (replaced) [<a href="/pdf/2304.07013" title="Download PDF">pdf</a>, <a href="/format/2304.07013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Dimming Sunglasses for Photophobia Using Spatial Light Modulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaodan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Uchiyama%2C+H">Hideaki Uchiyama</a>, 
<a href="/search/cs?searchtype=author&query=Isoyama%2C+N">Naoya Isoyama</a>, 
<a href="/search/cs?searchtype=author&query=Sakata%2C+N">Nobuchika Sakata</a>, 
<a href="/search/cs?searchtype=author&query=Kiyokawa%2C+K">Kiyoshi Kiyokawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08072" title="Abstract">arXiv:2304.08072</a> (replaced) [<a href="/e-print/2304.08072" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-stage MR Image Segmentation Method for Brain Tumors based on  Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Li Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+J">Jiawei Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+L">Lin Lu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Some contributing authors are not signed
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09444" title="Abstract">arXiv:2304.09444</a> (replaced) [<a href="/e-print/2304.09444" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rank-Based Learning and Local Model Based Evolutionary Algorithm for  High-Dimensional Expensive Multi-Objective Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guodong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J+J">Jiu Jimmy Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+X">Xiaoming Xue</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongzheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are some key errors inside the paper. We have to withdraw the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09654" title="Abstract">arXiv:2304.09654</a> (replaced) [<a href="/pdf/2304.09654" title="Download PDF">pdf</a>, <a href="/format/2304.09654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform Generation of Temporal Graphs with Given Degrees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allendorf%2C+D">Daniel Allendorf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09846" title="Abstract">arXiv:2304.09846</a> (replaced) [<a href="/pdf/2304.09846" title="Download PDF">pdf</a>, <a href="/ps/2304.09846" title="Download PostScript">ps</a>, <a href="/format/2304.09846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakening Assumptions for Publicly-Verifiable Deletion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Bartusek%2C+J">James Bartusek</a>, 
<a href="/search/quant-ph?searchtype=author&query=Khurana%2C+D">Dakshita Khurana</a>, 
<a href="/search/quant-ph?searchtype=author&query=Malavolta%2C+G">Giulio Malavolta</a>, 
<a href="/search/quant-ph?searchtype=author&query=Poremba%2C+A">Alexander Poremba</a>, 
<a href="/search/quant-ph?searchtype=author&query=Walter%2C+M">Michael Walter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11005" title="Abstract">arXiv:2304.11005</a> (replaced) [<a href="/pdf/2304.11005" title="Download PDF">pdf</a>, <a href="/format/2304.11005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Correcting Bayesian Optimization through Bayesian Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Hellsten%2C+E">Erik Hellsten</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 37th International Conference on Neural Information Processing
  Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13828" title="Abstract">arXiv:2304.13828</a> (replaced) [<a href="/pdf/2304.13828" title="Download PDF">pdf</a>, <a href="/format/2304.13828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Interleaved C-band Co-Propagation of Quantum and Classical Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rollick%2C+B+J">Brian J. Rollick</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huberman%2C+B+A">Bernardo A. Huberman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00067" title="Abstract">arXiv:2305.00067</a> (replaced) [<a href="/pdf/2305.00067" title="Download PDF">pdf</a>, <a href="/format/2305.00067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Discovery of 3D Hierarchical Structure with Generative  Diffusion Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tursynbek%2C+N">Nurislam Tursynbek</a>, 
<a href="/search/cs?searchtype=author&query=Niethammer%2C+M">Marc Niethammer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00087" title="Abstract">arXiv:2305.00087</a> (replaced) [<a href="/pdf/2305.00087" title="Download PDF">pdf</a>, <a href="/format/2305.00087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Consistency by Construction for Multistep Deep Registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greer%2C+H">Hastings Greer</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+L">Lin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Vialard%2C+F">Francois-Xavier Vialard</a>, 
<a href="/search/cs?searchtype=author&query=Kwitt%2C+R">Roland Kwitt</a>, 
<a href="/search/cs?searchtype=author&query=Bouix%2C+S">Sylvain Bouix</a>, 
<a href="/search/cs?searchtype=author&query=Estepar%2C+R+S+J">Raul San Jose Estepar</a>, 
<a href="/search/cs?searchtype=author&query=Rushmore%2C+R">Richard Rushmore</a>, 
<a href="/search/cs?searchtype=author&query=Niethammer%2C+M">Marc Niethammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01498" title="Abstract">arXiv:2305.01498</a> (replaced) [<a href="/pdf/2305.01498" title="Download PDF">pdf</a>, <a href="/format/2305.01498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Summarizing Multiple Documents with Conversational Structure for  Meta-Review Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Miao Li</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+E">Eduard Hovy</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+J+H">Jey Han Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long paper; Accepted to EMNLP 2023; Soundness: 3, 3, 4; Excitement: 3, 4, 4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03308" title="Abstract">arXiv:2305.03308</a> (replaced) [<a href="/pdf/2305.03308" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of  Motion Artifacts in Photoplethysmogram Signals on Edge Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Y">Yali Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+C">Chen Wu</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+P">Peizheng Cai</a>, 
<a href="/search/eess?searchtype=author&query=Zhong%2C+Z">Zhiqiang Zhong</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+H">Hongda Huang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Y">Yuqi Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05314" title="Abstract">arXiv:2305.05314</a> (replaced) [<a href="/pdf/2305.05314" title="Download PDF">pdf</a>, <a href="/format/2305.05314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and  Subtyping in Whole Slide Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fourkioti%2C+O">Olga Fourkioti</a>, 
<a href="/search/cs?searchtype=author&query=De+Vries%2C+M">Matt De Vries</a>, 
<a href="/search/cs?searchtype=author&query=Bakal%2C+C">Chris Bakal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05456" title="Abstract">arXiv:2305.05456</a> (replaced) [<a href="/pdf/2305.05456" title="Download PDF">pdf</a>, <a href="/format/2305.05456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Grounded Control for Coordinated Robot Motion and Speech
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tejwani%2C+R">Ravi Tejwani</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chengyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gomez-Paz%2C+P">Paco Gomez-Paz</a>, 
<a href="/search/cs?searchtype=author&query=Bonato%2C+P">Paolo Bonato</a>, 
<a href="/search/cs?searchtype=author&query=Asada%2C+H+H">H. Harry Asada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review in ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05506" title="Abstract">arXiv:2305.05506</a> (replaced) [<a href="/pdf/2305.05506" title="Download PDF">pdf</a>, <a href="/format/2305.05506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedGT: Identification of Malicious Clients in Federated Learning with  Secure Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xhemrishi%2C+M">Marvin Xhemrishi</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96stman%2C+J">Johan &#xd6;stman</a>, 
<a href="/search/cs?searchtype=author&query=Wachter-Zeh%2C+A">Antonia Wachter-Zeh</a>, 
<a href="/search/cs?searchtype=author&query=Amat%2C+A+G+i">Alexandre Graell i Amat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05803" title="Abstract">arXiv:2305.05803</a> (replaced) [<a href="/pdf/2305.05803" title="Download PDF">pdf</a>, <a href="/format/2305.05803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly  Supervised Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianle Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+Z">Zheda Mai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruiwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-lun Chao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tianle Chen and Zheda Mai contributed equally to this work. Our code is available at \url{<a href="https://github.com/cskyl/SAM_WSSS">this https URL</a>}
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07336" title="Abstract">arXiv:2305.07336</a> (replaced) [<a href="/pdf/2305.07336" title="Download PDF">pdf</a>, <a href="/format/2305.07336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with  Bird&#x27;s Eye View based Appearance and Motion Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jiapeng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chuanzhao Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07583" title="Abstract">arXiv:2305.07583</a> (replaced) [<a href="/pdf/2305.07583" title="Download PDF">pdf</a>, <a href="/format/2305.07583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoMo: Momentum Models for Adaptive Learning Rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaipp%2C+F">Fabian Schaipp</a>, 
<a href="/search/cs?searchtype=author&query=Ohana%2C+R">Ruben Ohana</a>, 
<a href="/search/cs?searchtype=author&query=Eickenberg%2C+M">Michael Eickenberg</a>, 
<a href="/search/cs?searchtype=author&query=Defazio%2C+A">Aaron Defazio</a>, 
<a href="/search/cs?searchtype=author&query=Gower%2C+R+M">Robert M. Gower</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10491" title="Abstract">arXiv:2305.10491</a> (replaced) [<a href="/pdf/2305.10491" title="Download PDF">pdf</a>, <a href="/format/2305.10491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Renormalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-th?searchtype=author&query=Berman%2C+D+S">David S. Berman</a>, 
<a href="/search/hep-th?searchtype=author&query=Klinger%2C+M+S">Marc S. Klinger</a>, 
<a href="/search/hep-th?searchtype=author&query=Stapleton%2C+A+G">Alexander G. Stapleton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, no figures. V2: Citation format fixed, references added. V3: Journal accepted version, new Section 4 includes fully worked implementation of Bayesian Renormalization to a Neural Network, 30 pages, 2 tables, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Theory (hep-th)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10983" title="Abstract">arXiv:2305.10983</a> (replaced) [<a href="/pdf/2305.10983" title="Download PDF">pdf</a>, <a href="/format/2305.10983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessor360: Multi-sequence Network for Blind Omnidirectional Image  Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Haoming Cai</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Mingdeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jing Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yinqiang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11141" title="Abstract">arXiv:2305.11141</a> (replaced) [<a href="/pdf/2305.11141" title="Download PDF">pdf</a>, <a href="/format/2305.11141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clifford Group Equivariant Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruhe%2C+D">David Ruhe</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>, 
<a href="/search/cs?searchtype=author&query=Forr%C3%A9%2C+P">Patrick Forr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11186" title="Abstract">arXiv:2305.11186</a> (replaced) [<a href="/pdf/2305.11186" title="Download PDF">pdf</a>, <a href="/format/2305.11186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM  Inference with Transferable Prompt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaozhuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zirui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuxin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaixiong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+A">Anshumali Shrivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12660" title="Abstract">arXiv:2305.12660</a> (replaced) [<a href="/pdf/2305.12660" title="Download PDF">pdf</a>, <a href="/format/2305.12660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beneath Surface Similarity: Large Language Models Make Reasonable  Scientific Analogies after Structure Abduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Siyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+X">Xuyang Ge</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Deqing Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12696" title="Abstract">arXiv:2305.12696</a> (replaced) [<a href="/pdf/2305.12696" title="Download PDF">pdf</a>, <a href="/format/2305.12696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interpretable Style Embeddings via Prompting LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Ajay Patel</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+D">Delip Rao</a>, 
<a href="/search/cs?searchtype=author&query=Kothary%2C+A">Ansh Kothary</a>, 
<a href="/search/cs?searchtype=author&query=McKeown%2C+K">Kathleen McKeown</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13160" title="Abstract">arXiv:2305.13160</a> (replaced) [<a href="/pdf/2305.13160" title="Download PDF">pdf</a>, <a href="/format/2305.13160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via  Debate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiang Yue</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP-23 (findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13257" title="Abstract">arXiv:2305.13257</a> (replaced) [<a href="/pdf/2305.13257" title="Download PDF">pdf</a>, <a href="/format/2305.13257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watermarking Classification Dataset for Copyright Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hongsheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13655" title="Abstract">arXiv:2305.13655</a> (replaced) [<a href="/pdf/2305.13655" title="Download PDF">pdf</a>, <a href="/format/2305.13655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image  Diffusion Models with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lian%2C+L">Long Lian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Yala%2C+A">Adam Yala</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://llm-grounded-diffusion.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14189" title="Abstract">arXiv:2305.14189</a> (replaced) [<a href="/pdf/2305.14189" title="Download PDF">pdf</a>, <a href="/format/2305.14189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Shared Vocabulary: Increasing Representational Word Similarities  across Languages for Multilingual Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14210" title="Abstract">arXiv:2305.14210</a> (replaced) [<a href="/pdf/2305.14210" title="Download PDF">pdf</a>, <a href="/format/2305.14210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skill-Based Few-Shot Selection for In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Shengnan An</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zeqi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14342" title="Abstract">arXiv:2305.14342</a> (replaced) [<a href="/pdf/2305.14342" title="Download PDF">pdf</a>, <a href="/format/2305.14342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sophia: A Scalable Stochastic Second-order Optimizer for Language Model  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+D">David Hall</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengyu Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14585" title="Abstract">arXiv:2305.14585</a> (replaced) [<a href="/pdf/2305.14585" title="Download PDF">pdf</a>, <a href="/format/2305.14585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful and Efficient Explanations for Neural Networks via Neural  Tangent Kernel Surrogate Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engel%2C+A">Andrew Engel</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Frank%2C+N+S">Natalie S. Frank</a>, 
<a href="/search/cs?searchtype=author&query=Dumitriu%2C+I">Ioana Dumitriu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Sarwate%2C+A">Anand Sarwate</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Tony Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 3 tables Updated 10/9/2023: Errata re. trNTK implementation for poisoned data experiments. Changes do not change our conclusions. Github repository will be posted soon. Updated 10/4/2023: significant changes for ICLR2024 submission. Github repository will be live soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14782" title="Abstract">arXiv:2305.14782</a> (replaced) [<a href="/pdf/2305.14782" title="Download PDF">pdf</a>, <a href="/format/2305.14782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IBCL: Zero-shot Model Generation for Task Trade-offs in Continual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pengyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Caprio%2C+M">Michele Caprio</a>, 
<a href="/search/cs?searchtype=author&query=Eaton%2C+E">Eric Eaton</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Insup Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We falsely submitted this as a new article at <a href="/abs/2310.02995">2310.02995</a>. That article is withdrawn. This is the correct submission (to replace a previous version)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14829" title="Abstract">arXiv:2305.14829</a> (replaced) [<a href="/pdf/2305.14829" title="Download PDF">pdf</a>, <a href="/format/2305.14829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deakin RF-Sensing: Experiments on Correlated Knowledge Distillation for  Monitoring Human Postures with Radios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pokhrel%2C+S+R">Shiva Raj Pokhrel</a>, 
<a href="/search/cs?searchtype=author&query=Kua%2C+J">Jonathan Kua</a>, 
<a href="/search/cs?searchtype=author&query=Satish%2C+D">Deol Satish</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+P">Philip Williams</a>, 
<a href="/search/cs?searchtype=author&query=Zaslavsky%2C+A">Arkady Zaslavsky</a>, 
<a href="/search/cs?searchtype=author&query=Loke%2C+S+W">Seng W. Loke</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinho Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14956" title="Abstract">arXiv:2305.14956</a> (replaced) [<a href="/pdf/2305.14956" title="Download PDF">pdf</a>, <a href="/format/2305.14956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Editing Common Sense in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anshita Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+D">Debanjan Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Sheshadri%2C+A+K">Akshay Krishna Sheshadri</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenlong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X+L">Xiang Lorraine Li</a>, 
<a href="/search/cs?searchtype=author&query=Wiegreffe%2C+S">Sarah Wiegreffe</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023. Anshita, Debanjan, Akshay are co-first authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15004" title="Abstract">arXiv:2305.15004</a> (replaced) [<a href="/pdf/2305.15004" title="Download PDF">pdf</a>, <a href="/format/2305.15004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMDet: A Third Party Large Language Models Generated Text Detection  Tool
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kangxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15011" title="Abstract">arXiv:2305.15011</a> (replaced) [<a href="/pdf/2305.15011" title="Download PDF">pdf</a>, <a href="/format/2305.15011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bactrian-X: Multilingual Replicable Instruction-Following Models with  Low-Rank Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Koto%2C+F">Fajri Koto</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Minghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18459" title="Abstract">arXiv:2305.18459</a> (replaced) [<a href="/pdf/2305.18459" title="Download PDF">pdf</a>, <a href="/format/2305.18459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Model is an Effective Planner and Data Synthesizer for  Multi-Task Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haoran He</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+C">Chenjia Bai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19011" title="Abstract">arXiv:2305.19011</a> (replaced) [<a href="/pdf/2305.19011" title="Download PDF">pdf</a>, <a href="/format/2305.19011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yu-Hsiang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Huang-Yu Chen</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+W">Winston Hsu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19420" title="Abstract">arXiv:2305.19420</a> (replaced) [<a href="/pdf/2305.19420" title="Download PDF">pdf</a>, <a href="/ps/2305.19420" title="Download PostScript">ps</a>, <a href="/format/2305.19420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What and How does In-Context Learning Learn? Bayesian Model Averaging,  Parameterization, and Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Y">Yufeng Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+F">Fengzhuo Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00042" title="Abstract">arXiv:2306.00042</a> (replaced) [<a href="/pdf/2306.00042" title="Download PDF">pdf</a>, <a href="/format/2306.00042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based methods coupled with specific distributional distances for  adversarial attack detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nwaigwe%2C+D">Dwight Nwaigwe</a>, 
<a href="/search/cs?searchtype=author&query=Carboni%2C+L">Lucrezia Carboni</a>, 
<a href="/search/cs?searchtype=author&query=Mermillod%2C+M">Martial Mermillod</a>, 
<a href="/search/cs?searchtype=author&query=Achard%2C+S">Sophie Achard</a>, 
<a href="/search/cs?searchtype=author&query=Dojat%2C+M">Michel Dojat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> published in Neural Networks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00398" title="Abstract">arXiv:2306.00398</a> (replaced) [<a href="/pdf/2306.00398" title="Download PDF">pdf</a>, <a href="/format/2306.00398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preference-grounded Token-level Guidance for Language Model Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shentao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shujian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C">Congying Xia</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyuan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00432" title="Abstract">arXiv:2306.00432</a> (replaced) [<a href="/pdf/2306.00432" title="Download PDF">pdf</a>, <a href="/ps/2306.00432" title="Download PostScript">ps</a>, <a href="/format/2306.00432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time and Space Optimal Massively Parallel Algorithm for the 2-Ruling Set  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cambus%2C+M">M&#xe9;lanie Cambus</a>, 
<a href="/search/cs?searchtype=author&query=Kuhn%2C+F">Fabian Kuhn</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+S">Shreyas Pai</a>, 
<a href="/search/cs?searchtype=author&query=Uitto%2C+J">Jara Uitto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01208" title="Abstract">arXiv:2306.01208</a> (replaced) [<a href="/pdf/2306.01208" title="Download PDF">pdf</a>, <a href="/format/2306.01208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting an Unadaptable ASR System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ma%2C+R">Rao Ma</a>, 
<a href="/search/eess?searchtype=author&query=Qian%2C+M">Mengjie Qian</a>, 
<a href="/search/eess?searchtype=author&query=Gales%2C+M+J+F">Mark J. F. Gales</a>, 
<a href="/search/eess?searchtype=author&query=Knill%2C+K+M">Kate M. Knill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of INTERSPEECH
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03580" title="Abstract">arXiv:2306.03580</a> (replaced) [<a href="/pdf/2306.03580" title="Download PDF">pdf</a>, <a href="/format/2306.03580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L-C2ST: Local Diagnostics for Posterior Approximations in  Simulation-Based Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Linhart%2C+J">Julia Linhart</a>, 
<a href="/search/stat?searchtype=author&query=Gramfort%2C+A">Alexandre Gramfort</a>, 
<a href="/search/stat?searchtype=author&query=Rodrigues%2C+P+L+C">Pedro L. C. Rodrigues</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 6 figures, 3 tables, 6 appendices, NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03740" title="Abstract">arXiv:2306.03740</a> (replaced) [<a href="/pdf/2306.03740" title="Download PDF">pdf</a>, <a href="/format/2306.03740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GMMap: Memory-Efficient Continuous Occupancy Map Using Gaussian Mixture  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P+Z+X">Peter Zhi Xuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Karaman%2C+S">Sertac Karaman</a>, 
<a href="/search/cs?searchtype=author&query=Sze%2C+V">Vivienne Sze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04642" title="Abstract">arXiv:2306.04642</a> (replaced) [<a href="/pdf/2306.04642" title="Download PDF">pdf</a>, <a href="/format/2306.04642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffusionShield: A Watermark for Copyright Protection against Generative  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yingqian Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04893" title="Abstract">arXiv:2306.04893</a> (replaced) [<a href="/pdf/2306.04893" title="Download PDF">pdf</a>, <a href="/format/2306.04893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coping with Change: Learning Invariant and Minimum Sufficient  Representations for Fine-Grained Visual Categorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Shuo Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shujian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+W">Wenjin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+X">Xinge You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript accepted by CVIU, code is available at Github
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05534" title="Abstract">arXiv:2306.05534</a> (replaced) [<a href="/pdf/2306.05534" title="Download PDF">pdf</a>, <a href="/format/2306.05534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Security Blind Spots of Software Composition Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dietrich%2C+J">Jens Dietrich</a>, 
<a href="/search/cs?searchtype=author&query=Rasheed%2C+S">Shawn Rasheed</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+A">Alexander Jordan</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+T">Tim White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05718" title="Abstract">arXiv:2306.05718</a> (replaced) [<a href="/pdf/2306.05718" title="Download PDF">pdf</a>, <a href="/format/2306.05718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Domain-Aware Detection Head with Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haochen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Hantao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xinkai Song</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yifan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yongwei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Ling Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunji Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06304" title="Abstract">arXiv:2306.06304</a> (replaced) [<a href="/pdf/2306.06304" title="Download PDF">pdf</a>, <a href="/format/2306.06304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element interpolated neural networks for solving forward and  inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Badia%2C+S">Santiago Badia</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/math?searchtype=author&query=Mart%C3%ADn%2C+A+F">Alberto F. Mart&#xed;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07220" title="Abstract">arXiv:2306.07220</a> (replaced) [<a href="/pdf/2306.07220" title="Download PDF">pdf</a>, <a href="/format/2306.07220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strokes2Surface: Recovering Curve Networks From 4D Architectural Design  Sketches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasoulzadeh%2C+S">S. Rasoulzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Wimmer%2C+M">M. Wimmer</a>, 
<a href="/search/cs?searchtype=author&query=Kovacic%2C+I">I. Kovacic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08749" title="Abstract">arXiv:2306.08749</a> (replaced) [<a href="/pdf/2306.08749" title="Download PDF">pdf</a>, <a href="/format/2306.08749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology  Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Mathai%2C+T+S">Tejas Sudharshan Mathai</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+P">Pritam Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Summers%2C+R+M">Ronald M. Summers</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08937" title="Abstract">arXiv:2306.08937</a> (replaced) [<a href="/pdf/2306.08937" title="Download PDF">pdf</a>, <a href="/format/2306.08937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocumentNet: Bridging the Data Gap in Document Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lijun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+J">Jin Miao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoyu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hauptmann%2C+A+G">Alexander G. Hauptmann</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09681" title="Abstract">arXiv:2306.09681</a> (replaced) [<a href="/pdf/2306.09681" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations  of Imperfection Factors and Macromolecular Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chen%2C+D">Dicheng Chen</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+M">Meijin Lin</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+H">Huiting Liu</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+J">Jiayu Li</a>, 
<a href="/search/physics?searchtype=author&query=Zhou%2C+Y">Yirong Zhou</a>, 
<a href="/search/physics?searchtype=author&query=Kang%2C+T">Taishan Kang</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+L">Liangjie Lin</a>, 
<a href="/search/physics?searchtype=author&query=Wu%2C+Z">Zhigang Wu</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+J">Jiazheng Wang</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/physics?searchtype=author&query=Lin%2C+J">Jianzhong Lin</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/physics?searchtype=author&query=Guo%2C+D">Di Guo</a>, 
<a href="/search/physics?searchtype=author&query=Qu%2C+X">Xiaobo Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10502" title="Abstract">arXiv:2306.10502</a> (replaced) [<a href="/pdf/2306.10502" title="Download PDF">pdf</a>, <a href="/format/2306.10502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Map Vectorization for Autonomous Driving: A Rasterization  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gongjie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiahao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yilin Song</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhipeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zuoguan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> [NeurIPS 2023]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11375" title="Abstract">arXiv:2306.11375</a> (replaced) [<a href="/pdf/2306.11375" title="Download PDF">pdf</a>, <a href="/format/2306.11375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Top-down machine learning of coarse-grained protein force-fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Navarro%2C+C">Carles Navarro</a>, 
<a href="/search/q-bio?searchtype=author&query=Majewski%2C+M">Maciej Majewski</a>, 
<a href="/search/q-bio?searchtype=author&query=de+Fabritiis%2C+G">Gianni de Fabritiis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11526" title="Abstract">arXiv:2306.11526</a> (replaced) [<a href="/pdf/2306.11526" title="Download PDF">pdf</a>, <a href="/format/2306.11526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Contrastive Learning Through the Lens of Margins
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rho%2C+D">Daniel Rho</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">TaeSoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sooill Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaehyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">JaeHan Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12369" title="Abstract">arXiv:2306.12369</a> (replaced) [<a href="/pdf/2306.12369" title="Download PDF">pdf</a>, <a href="/format/2306.12369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Efficient MPPI Trajectory Generation with Unscented Guidance:  U-MPPI Control Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+I+S">Ihab S. Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Junhong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sukhatme%2C+G+S">Gaurav S Sukhatme</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lantao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has 15 pages, 10 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13826" title="Abstract">arXiv:2306.13826</a> (replaced) [<a href="/pdf/2306.13826" title="Download PDF">pdf</a>, <a href="/format/2306.13826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalised f-Mean Aggregation for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kortvelesy%2C+R">Ryan Kortvelesy</a>, 
<a href="/search/cs?searchtype=author&query=Morad%2C+S">Steven Morad</a>, 
<a href="/search/cs?searchtype=author&query=Prorok%2C+A">Amanda Prorok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15909" title="Abstract">arXiv:2306.15909</a> (replaced) [<a href="/pdf/2306.15909" title="Download PDF">pdf</a>, <a href="/format/2306.15909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+A">Abhinav Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Nashed%2C+S+B">Samer B. Nashed</a>, 
<a href="/search/cs?searchtype=author&query=Zilberstein%2C+S">Shlomo Zilberstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15989" title="Abstract">arXiv:2306.15989</a> (replaced) [<a href="/pdf/2306.15989" title="Download PDF">pdf</a>, <a href="/format/2306.15989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensorformer: Normalized Matrix Attention Transformer for High-quality  Point Cloud Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hui Tian</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zheng Qin</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+R">Renjiao Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenyang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16788" title="Abstract">arXiv:2306.16788</a> (replaced) [<a href="/pdf/2306.16788" title="Download PDF">pdf</a>, <a href="/format/2306.16788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+M">Max Zimmer</a>, 
<a href="/search/cs?searchtype=author&query=Spiegel%2C+C">Christoph Spiegel</a>, 
<a href="/search/cs?searchtype=author&query=Pokutta%2C+S">Sebastian Pokutta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 pages references, 15 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16922" title="Abstract">arXiv:2306.16922</a> (replaced) [<a href="/pdf/2306.16922" title="Download PDF">pdf</a>, <a href="/format/2306.16922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Expressive Leaky Memory Neuron: an Efficient and Expressive  Phenomenological Neuron Model Can Solve Long-Horizon Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spieler%2C+A">Aaron Spieler</a>, 
<a href="/search/cs?searchtype=author&query=Rahaman%2C+N">Nasim Rahaman</a>, 
<a href="/search/cs?searchtype=author&query=Martius%2C+G">Georg Martius</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Levina%2C+A">Anna Levina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 12 figures, 10 tables, additional experiments and clarifications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17560" title="Abstract">arXiv:2306.17560</a> (replaced) [<a href="/pdf/2306.17560" title="Download PDF">pdf</a>, <a href="/format/2306.17560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class-Incremental Learning using Diffusion Model for Distillation and  Replay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jodelet%2C+Q">Quentin Jodelet</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Phua%2C+Y+J">Yin Jun Phua</a>, 
<a href="/search/cs?searchtype=author&query=Murata%2C+T">Tsuyoshi Murata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Best paper award at 1st Workshop on Visual Continual Learning, ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00847" title="Abstract">arXiv:2307.00847</a> (replaced) [<a href="/pdf/2307.00847" title="Download PDF">pdf</a>, <a href="/format/2307.00847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An analysis on stochastic Lanczos quadrature with asymmetric quadrature  nodes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/math?searchtype=author&query=Han%2C+Z">Zongyuan Han</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yixuan Huang</a>, 
<a href="/search/math?searchtype=author&query=Wathen%2C+A">Andy Wathen</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+S">Shengxin Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02736" title="Abstract">arXiv:2307.02736</a> (replaced) [<a href="/pdf/2307.02736" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Uncertainty Aided Framework for Learning based Liver $T_1&#x3c1;$  Mapping and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Huang%2C+C">Chaoxing Huang</a>, 
<a href="/search/physics?searchtype=author&query=Wong%2C+V+W+S">Vincent Wai Sun Wong</a>, 
<a href="/search/physics?searchtype=author&query=Chan%2C+Q">Queenie Chan</a>, 
<a href="/search/physics?searchtype=author&query=Chu%2C+W+C+W">Winnie Chiu Wing Chu</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+W">Weitian Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05330" title="Abstract">arXiv:2307.05330</a> (replaced) [<a href="/pdf/2307.05330" title="Download PDF">pdf</a>, <a href="/format/2307.05330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Value of Chess Squares
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aditya Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Maharaj%2C+S">Shiva Maharaj</a>, 
<a href="/search/cs?searchtype=author&query=Polson%2C+N">Nicholas Polson</a>, 
<a href="/search/cs?searchtype=author&query=Sokolov%2C+V">Vadim Sokolov</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Entropy 25, no. 10: 1374
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06077" title="Abstract">arXiv:2307.06077</a> (replaced) [<a href="/pdf/2307.06077" title="Download PDF">pdf</a>, <a href="/format/2307.06077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalised Theory of Proportionality in Collective Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masa%C5%99%C3%ADk%2C+T">Tom&#xe1;&#x161; Masa&#x159;&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Pierczy%C5%84ski%2C+G">Grzegorz Pierczy&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Skowron%2C+P">Piotr Skowron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08966" title="Abstract">arXiv:2307.08966</a> (replaced) [<a href="/pdf/2307.08966" title="Download PDF">pdf</a>, <a href="/ps/2307.08966" title="Download PostScript">ps</a>, <a href="/format/2307.08966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Robot Patrol Algorithm with Distributed Coordination and  Consciousness of the Base Station&#x27;s Situation Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+K">Kazuho Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Ueno%2C+S">Seiya Ueno</a>, 
<a href="/search/cs?searchtype=author&query=Higuchi%2C+T">Takehiro Higuchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09378" title="Abstract">arXiv:2307.09378</a> (replaced) [<a href="/pdf/2307.09378" title="Download PDF">pdf</a>, <a href="/format/2307.09378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting an ASR Foundation Model for Spoken Language Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Rao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+M">Mengjie Qian</a>, 
<a href="/search/cs?searchtype=author&query=Gales%2C+M+J+F">Mark J. F. Gales</a>, 
<a href="/search/cs?searchtype=author&query=Knill%2C+K+M">Kate M. Knill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of SLaTE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10236" title="Abstract">arXiv:2307.10236</a> (replaced) [<a href="/pdf/2307.10236" title="Download PDF">pdf</a>, <a href="/format/2307.10236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Look Before You Leap: An Exploratory Study of Uncertainty Measurement  for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiayang Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Juefei-Xu%2C+F">Felix Juefei-Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11643" title="Abstract">arXiv:2307.11643</a> (replaced) [<a href="/pdf/2307.11643" title="Download PDF">pdf</a>, <a href="/format/2307.11643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Morphological Image Analysis and Feature Extraction for Reasoning with  AI-based Defect Detection and Classification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cosma%2C+G">Georgina Cosma</a>, 
<a href="/search/cs?searchtype=author&query=Bugby%2C+S">Sarah Bugby</a>, 
<a href="/search/cs?searchtype=author&query=Finke%2C+A">Axel Finke</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+J">Jason Watkins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, 5 tables; accepted in 2023 IEEE symposium series on computational intelligence (SSCI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12493" title="Abstract">arXiv:2307.12493</a> (replaced) [<a href="/pdf/2307.12493" title="Download PDF">pdf</a>, <a href="/format/2307.12493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shilin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanzhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+A+W">Adams Wai-Kin Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13421" title="Abstract">arXiv:2307.13421</a> (replaced) [<a href="/pdf/2307.13421" title="Download PDF">pdf</a>, <a href="/format/2307.13421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Learning Dynamics of Attention Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vashisht%2C+R">Rahul Vashisht</a>, 
<a href="/search/cs?searchtype=author&query=Ramaswamy%2C+H+G">Harish G. Ramaswamy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings at ECAI-2023 IOS Press
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13815" title="Abstract">arXiv:2307.13815</a> (replaced) [<a href="/pdf/2307.13815" title="Download PDF">pdf</a>, <a href="/format/2307.13815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and  Classification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cosma%2C+G">Georgina Cosma</a>, 
<a href="/search/cs?searchtype=author&query=Bugby%2C+S">Sarah Bugby</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+J">Jason Watkins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, accepted in 2023 IEEE symposium series on computational intelligence (SSCI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14439" title="Abstract">arXiv:2307.14439</a> (replaced) [<a href="/pdf/2307.14439" title="Download PDF">pdf</a>, <a href="/format/2307.14439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed Integral Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kortvelesy%2C+R">Ryan Kortvelesy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14839" title="Abstract">arXiv:2307.14839</a> (replaced) [<a href="/pdf/2307.14839" title="Download PDF">pdf</a>, <a href="/format/2307.14839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernelised Normalising Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=English%2C+E">Eshant English</a>, 
<a href="/search/stat?searchtype=author&query=Kirchler%2C+M">Matthias Kirchler</a>, 
<a href="/search/stat?searchtype=author&query=Lippert%2C+C">Christoph Lippert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Alternate title: Kernelized Normalizing Flows
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16051" title="Abstract">arXiv:2307.16051</a> (replaced) [<a href="/pdf/2307.16051" title="Download PDF">pdf</a>, <a href="/ps/2307.16051" title="Download PostScript">ps</a>, <a href="/format/2307.16051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Newton&#x27;s Method in Three Precisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kelley%2C+C+T">C. T. Kelley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16368" title="Abstract">arXiv:2307.16368</a> (replaced) [<a href="/pdf/2307.16368" title="Download PDF">pdf</a>, <a href="/format/2307.16368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AntGPT: Can Large Language Models Help Long-term Action Anticipation  from Videos?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Changcheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+M+Q">Minh Quan Do</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+N">Nakul Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kwonjoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01781" title="Abstract">arXiv:2308.01781</a> (replaced) [<a href="/pdf/2308.01781" title="Download PDF">pdf</a>, <a href="/ps/2308.01781" title="Download PostScript">ps</a>, <a href="/format/2308.01781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influences of some families of error-correcting codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Egan%2C+H">Hailey Egan</a>, 
<a href="/search/cs?searchtype=author&query=LeGrow%2C+J+T">Jason T. LeGrow</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+G+L">Gretchen L. Matthews</a>, 
<a href="/search/cs?searchtype=author&query=Suliga%2C+J">Jeff Suliga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01853" title="Abstract">arXiv:2308.01853</a> (replaced) [<a href="/pdf/2308.01853" title="Download PDF">pdf</a>, <a href="/format/2308.01853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Estimation Under Distribution Shift: Wasserstein  Perturbations and Minimax Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chao%2C+P">Patrick Chao</a>, 
<a href="/search/stat?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02599" title="Abstract">arXiv:2308.02599</a> (replaced) [<a href="/pdf/2308.02599" title="Download PDF">pdf</a>, <a href="/format/2308.02599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Branched Latent Neural Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salvador%2C+M">Matteo Salvador</a>, 
<a href="/search/cs?searchtype=author&query=Marsden%2C+A+L">Alison Lesley Marsden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04313" title="Abstract">arXiv:2308.04313</a> (replaced) [<a href="/pdf/2308.04313" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Apple Vision Pro for Healthcare: &quot;The Ultimate Display&quot;? -- Entering the  Wonderland of Precision Medicine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Egger%2C+J">Jan Egger</a>, 
<a href="/search/cs?searchtype=author&query=Gsaxner%2C+C">Christina Gsaxner</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/cs?searchtype=author&query=Puladi%2C+B">Behrus Puladi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a Preprint under CC BY. This work was supported by NIH/NIAID R01AI172875, NIH/NCATS UL1 TR001427, the REACT-EU project KITE and enFaced 2.0 (FWF KLI 1044). B. Puladi was funded by the Medical Faculty of the RWTH Aachen University as part of the Clinician Scientist Program. C. Gsaxner was funded by the Advanced Research Opportunities Program from the RWTH Aachen University
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Graphics (cs.GR); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05370" title="Abstract">arXiv:2308.05370</a> (replaced) [<a href="/pdf/2308.05370" title="Download PDF">pdf</a>, <a href="/format/2308.05370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Co-movement Pattern Mining from Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Teng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junnan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Bei%2C+Y">Yijun Bei</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K">Kian-Lee Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Gang Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06160" title="Abstract">arXiv:2308.06160</a> (replaced) [<a href="/pdf/2308.06160" title="Download PDF">pdf</a>, <a href="/format/2308.06160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuzhong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuchao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chunhua Shen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. Advances In Neural Information Processing Systems (NeurIPS
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06961" title="Abstract">arXiv:2308.06961</a> (replaced) [<a href="/pdf/2308.06961" title="Download PDF">pdf</a>, <a href="/format/2308.06961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Structural Residuals: A Learning Approach to Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Augustin%2C+J+L">Jan Lukas Augustin</a>, 
<a href="/search/cs?searchtype=author&query=Niggemann%2C+O">Oliver Niggemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, added missing section heading
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07828" title="Abstract">arXiv:2308.07828</a> (replaced) [<a href="/pdf/2308.07828" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Genetic Algorithm Meta-Heuristic for a Generalized Quadratic  Assignment Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farahani%2C+M+A">Mojtaba A. Farahani</a>, 
<a href="/search/cs?searchtype=author&query=McKendall%2C+A">Alan McKendall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08231" title="Abstract">arXiv:2308.08231</a> (replaced) [<a href="/pdf/2308.08231" title="Download PDF">pdf</a>, <a href="/format/2308.08231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDF-HO: Hand-Held Object Reconstruction via Conditional Directed  Distance Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyangguang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+Y">Yan Di</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruida Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangyao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Manhardt%2C+F">Fabian Manhardt</a>, 
<a href="/search/cs?searchtype=author&query=Tombari%2C+F">Federico Tombari</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09858" title="Abstract">arXiv:2308.09858</a> (replaced) [<a href="/pdf/2308.09858" title="Download PDF">pdf</a>, <a href="/format/2308.09858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed)  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yequan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xinling Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhixiong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11053" title="Abstract">arXiv:2308.11053</a> (replaced) [<a href="/pdf/2308.11053" title="Download PDF">pdf</a>, <a href="/format/2308.11053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ultra Dual-Path Compression For Joint Echo Cancellation And Noise  Suppression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Hangting Chen</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+J">Jianwei Yu</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+Y">Yi Luo</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+R">Rongzhi Gu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Weihua Li</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+Z">Zhuocheng Lu</a>, 
<a href="/search/eess?searchtype=author&query=Weng%2C+C">Chao Weng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of INTERSPEECH
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11940" title="Abstract">arXiv:2308.11940</a> (replaced) [<a href="/pdf/2308.11940" title="Download PDF">pdf</a>, <a href="/format/2308.11940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio Generation with Multiple Conditional Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhifang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jianguo Mao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Rui Tao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Long Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ouchi%2C+K">Kazushige Ouchi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiangdong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12593" title="Abstract">arXiv:2308.12593</a> (replaced) [<a href="/pdf/2308.12593" title="Download PDF">pdf</a>, <a href="/ps/2308.12593" title="Download PostScript">ps</a>, <a href="/format/2308.12593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Polynomial Kernels for Knapsack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heeger%2C+K">Klaus Heeger</a>, 
<a href="/search/cs?searchtype=author&query=Hermelin%2C+D">Danny Hermelin</a>, 
<a href="/search/cs?searchtype=author&query=Mnich%2C+M">Matthias Mnich</a>, 
<a href="/search/cs?searchtype=author&query=Shabtay%2C+D">Dvir Shabtay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13150" title="Abstract">arXiv:2308.13150</a> (replaced) [<a href="/pdf/2308.13150" title="Download PDF">pdf</a>, <a href="/format/2308.13150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Breast Cancer Classification Using Transfer ResNet with  Lightweight Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Suxing Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures,6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14120" title="Abstract">arXiv:2308.14120</a> (replaced) [<a href="/pdf/2308.14120" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Streamline Automated Machine Learning for Clinical  Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tianyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lotfinia%2C+M">Mahshad Lotfinia</a>, 
<a href="/search/cs?searchtype=author&query=Kuhl%2C+C">Christiane Kuhl</a>, 
<a href="/search/cs?searchtype=author&query=Kather%2C+J+N">Jakob Nikolas Kather</a>, 
<a href="/search/cs?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/cs?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14311" title="Abstract">arXiv:2308.14311</a> (replaced) [<a href="/pdf/2308.14311" title="Download PDF">pdf</a>, <a href="/format/2308.14311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spread Control Method on Unknown Networks Based on Hierarchical  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wenxiang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhanjiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H+V">H.Vicky Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15009" title="Abstract">arXiv:2308.15009</a> (replaced) [<a href="/pdf/2308.15009" title="Download PDF">pdf</a>, <a href="/ps/2308.15009" title="Download PostScript">ps</a>, <a href="/format/2308.15009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double Public Key Signing Function Oracle Attack on EdDSA Software  Implementations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grierson%2C+S">Sam Grierson</a>, 
<a href="/search/cs?searchtype=author&query=Chalkias%2C+K">Konstantinos Chalkias</a>, 
<a href="/search/cs?searchtype=author&query=Buchanan%2C+W+J">William J Buchanan</a>, 
<a href="/search/cs?searchtype=author&query=Maglaras%2C+L">Leandros Maglaras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15116" title="Abstract">arXiv:2308.15116</a> (replaced) [<a href="/pdf/2308.15116" title="Download PDF">pdf</a>, <a href="/format/2308.15116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of  Protein Simulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingbang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xingwei Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuangjia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15126" title="Abstract">arXiv:2308.15126</a> (replaced) [<a href="/pdf/2308.15126" title="Download PDF">pdf</a>, <a href="/format/2308.15126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation and Analysis of Hallucination in Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guohai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+P">Pengcheng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chenlin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qinghao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jihua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+J">Jitao Sang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Haoyu Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00976" title="Abstract">arXiv:2309.00976</a> (replaced) [<a href="/pdf/2309.00976" title="Download PDF">pdf</a>, <a href="/format/2309.00976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pure Message Passing Can Estimate Common Neighbor for Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kaiwen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhichun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01628" title="Abstract">arXiv:2309.01628</a> (replaced) [<a href="/pdf/2309.01628" title="Download PDF">pdf</a>, <a href="/ps/2309.01628" title="Download PostScript">ps</a>, <a href="/format/2309.01628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bowen&#x27;s equations for invariance pressure of control systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+E">Ercai Chen</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+J">Jiao Yang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+X">Xiaoyao Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Information Theory (cs.IT); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03063" title="Abstract">arXiv:2309.03063</a> (replaced) [<a href="/pdf/2309.03063" title="Download PDF">pdf</a>, <a href="/format/2309.03063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-based Ingredient-Oriented All-in-One Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+D">Depeng Dang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03504" title="Abstract">arXiv:2309.03504</a> (replaced) [<a href="/pdf/2309.03504" title="Download PDF">pdf</a>, <a href="/format/2309.03504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stroke-based Neural Painting and Stylization with Dynamically Predicted  Painting Region
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Teng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+R">Ran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haokun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Jinlong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yabiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lizhuang Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05249" title="Abstract">arXiv:2309.05249</a> (replaced) [<a href="/pdf/2309.05249" title="Download PDF">pdf</a>, <a href="/format/2309.05249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Visual Odometry Methods for Autonomous Driving in Rain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y+X">Yu Xiang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Prasetyo%2C+M+B">Marcel Bartholomeus Prasetyo</a>, 
<a href="/search/cs?searchtype=author&query=Daffa%2C+M+A">Mohammad Alif Daffa</a>, 
<a href="/search/cs?searchtype=author&query=Nitin%2C+D+S">Deshpande Sunny Nitin</a>, 
<a href="/search/cs?searchtype=author&query=Meghjani%2C+M">Malika Meghjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, Accepted at IEEE International Conference on Automation Science and Engineering (CASE) 2023. Fixed grammar and phrasing to improve clarity of the statements made. Emphasized on the need for a more robust sensor fusion based approach for localization in rain for autonomous driving
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07038" title="Abstract">arXiv:2309.07038</a> (replaced) [<a href="/pdf/2309.07038" title="Download PDF">pdf</a>, <a href="/format/2309.07038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Reinforcement Learning for Jumping Monopods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bussola%2C+R">Riccardo Bussola</a>, 
<a href="/search/cs?searchtype=author&query=Focchi%2C+M">Michele Focchi</a>, 
<a href="/search/cs?searchtype=author&query=Del+Prete%2C+A">Andrea Del Prete</a>, 
<a href="/search/cs?searchtype=author&query=Fontanelli%2C+D">Daniele Fontanelli</a>, 
<a href="/search/cs?searchtype=author&query=Palopoli%2C+L">Luigi Palopoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07270" title="Abstract">arXiv:2309.07270</a> (replaced) [<a href="/pdf/2309.07270" title="Download PDF">pdf</a>, <a href="/format/2309.07270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPU Scheduler for De Novo Genome Assembly with Multiple MPI Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Guanghao Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08021" title="Abstract">arXiv:2309.08021</a> (replaced) [<a href="/e-print/2309.08021" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-based Analysis of Driver Activity and Driving Performance Under  the Influence of Alcohol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greer%2C+R">Ross Greer</a>, 
<a href="/search/cs?searchtype=author&query=Gopalkrishnan%2C+A">Akshay Gopalkrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Mandadi%2C+S">Sumega Mandadi</a>, 
<a href="/search/cs?searchtype=author&query=Gunaratne%2C+P">Pujitha Gunaratne</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+M+M">Mohan M. Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Marcotte%2C+T+D">Thomas D. Marcotte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Withdrawn at the request of industry research collaborators, per contract agreement
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08066" title="Abstract">arXiv:2309.08066</a> (replaced) [<a href="/pdf/2309.08066" title="Download PDF">pdf</a>, <a href="/format/2309.08066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Morphologically-Aware Consensus Computation via Heuristics-based  IterATive Optimization (MACCHIatO)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamzaoui%2C+D">Dimitri Hamzaoui</a>, 
<a href="/search/cs?searchtype=author&query=Montagne%2C+S">Sarah Montagne</a>, 
<a href="/search/cs?searchtype=author&query=Renard-Penna%2C+R">Rapha&#xeb;le Renard-Penna</a>, 
<a href="/search/cs?searchtype=author&query=Ayache%2C+N">Nicholas Ayache</a>, 
<a href="/search/cs?searchtype=author&query=Delingette%2C+H">Herv&#xe9; Delingette</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) <a href="https://melba-journal.org/2023:013">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine.Learning.for.Biomedical.Imaging. 2 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08774" title="Abstract">arXiv:2309.08774</a> (replaced) [<a href="/pdf/2309.08774" title="Download PDF">pdf</a>, <a href="/format/2309.08774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of Novel Analog Compute Paradigms with Ark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Neng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cowan%2C+G">Glenn Cowan</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BChrmair%2C+U">Ulrich R&#xfc;hrmair</a>, 
<a href="/search/cs?searchtype=author&query=Achour%2C+S">Sara Achour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09464" title="Abstract">arXiv:2309.09464</a> (replaced) [<a href="/pdf/2309.09464" title="Download PDF">pdf</a>, <a href="/ps/2309.09464" title="Download PostScript">ps</a>, <a href="/format/2309.09464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Adversarial Training Cost with Gradient Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Huihui Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The experiments are insufficient, later will be updated. Withraw this manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09507" title="Abstract">arXiv:2309.09507</a> (replaced) [<a href="/pdf/2309.09507" title="Download PDF">pdf</a>, <a href="/format/2309.09507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pruning Large Language Models via Accuracy Predictor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yupeng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yibo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiucai Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figs, submitted to IEEE ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10400" title="Abstract">arXiv:2309.10400</a> (replaced) [<a href="/pdf/2309.10400" title="Download PDF">pdf</a>, <a href="/format/2309.10400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoSE: Efficient Context Window Extension of LLMs via Positional  Skip-wise Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dawei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yifan Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sujian Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10791" title="Abstract">arXiv:2309.10791</a> (replaced) [<a href="/pdf/2309.10791" title="Download PDF">pdf</a>, <a href="/format/2309.10791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-spectral Entropy Constrained Neural Compression of Solar Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zafari%2C+A">Ali Zafari</a>, 
<a href="/search/eess?searchtype=author&query=Khoshkhahtinat%2C+A">Atefeh Khoshkhahtinat</a>, 
<a href="/search/eess?searchtype=author&query=Mehta%2C+P+M">Piyush M. Mehta</a>, 
<a href="/search/eess?searchtype=author&query=Nasrabadi%2C+N+M">Nasser M. Nasrabadi</a>, 
<a href="/search/eess?searchtype=author&query=Thompson%2C+B+J">Barbara J. Thompson</a>, 
<a href="/search/eess?searchtype=author&query=Kirk%2C+M+S+F">Michael S. F. Kirk</a>, 
<a href="/search/eess?searchtype=author&query=da+Silva%2C+D">Daniel da Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE 22$^{nd}$ International Conference on Machine Learning and Applications 2023 (ICMLA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10818" title="Abstract">arXiv:2309.10818</a> (replaced) [<a href="/pdf/2309.10818" title="Download PDF">pdf</a>, <a href="/format/2309.10818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SlimPajama-DC: Understanding Data Combinations for LLM Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+T">Tianhua Tao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Liqun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Neiswanger%2C+W">Willie Neiswanger</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+B">Bowen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Hestness%2C+J">Joel Hestness</a>, 
<a href="/search/cs?searchtype=author&query=Vassilieva%2C+N">Natalia Vassilieva</a>, 
<a href="/search/cs?searchtype=author&query=Soboleva%2C+D">Daria Soboleva</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report. Huggingface: <a href="https://huggingface.co/MBZUAI-LLM">this https URL</a> and <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10916" title="Abstract">arXiv:2309.10916</a> (replaced) [<a href="/pdf/2309.10916" title="Download PDF">pdf</a>, <a href="/format/2309.10916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Learned Representations and Influence Functions Can Tell Us About  Adversarial Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tonni%2C+S+M">Shakila Mahjabin Tonni</a>, 
<a href="/search/cs?searchtype=author&query=Dras%2C+M">Mark Dras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, Accepted in IJCNLP_AACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11592" title="Abstract">arXiv:2309.11592</a> (replaced) [<a href="/pdf/2309.11592" title="Download PDF">pdf</a>, <a href="/format/2309.11592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel-mentoring for Offline Model-based Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Can Chen</a>, 
<a href="/search/cs?searchtype=author&query=Beckham%2C+C">Christopher Beckham</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+C">Christopher Pal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11600" title="Abstract">arXiv:2309.11600</a> (replaced) [<a href="/pdf/2309.11600" title="Download PDF">pdf</a>, <a href="/format/2309.11600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Importance-aware Co-teaching for Offline Model-based Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Can Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Neiswanger%2C+W">Willie Neiswanger</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12301" title="Abstract">arXiv:2309.12301</a> (replaced) [<a href="/e-print/2309.12301" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Environment-biased Feature Ranking for Novelty Detection Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smeu%2C+S">Stefan Smeu</a>, 
<a href="/search/cs?searchtype=author&query=Burceanu%2C+E">Elena Burceanu</a>, 
<a href="/search/cs?searchtype=author&query=Haller%2C+E">Emanuela Haller</a>, 
<a href="/search/cs?searchtype=author&query=Nicolicioiu%2C+A+L">Andrei Liviu Nicolicioiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The updated, long version of the paper is available at <a href="/abs/2310.03738">arXiv:2310.03738</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12559" title="Abstract">arXiv:2309.12559</a> (replaced) [<a href="/pdf/2309.12559" title="Download PDF">pdf</a>, <a href="/format/2309.12559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariant Learning via Probability of Sufficient and Necessary Causes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Furui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ton%2C+J">Jean-Francois Ton</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13570" title="Abstract">arXiv:2309.13570</a> (replaced) [<a href="/pdf/2309.13570" title="Download PDF">pdf</a>, <a href="/format/2309.13570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Digital-Twin Localization via An RGBD-based Transformer Network  and A Comprehensive Evaluation on a Mobile Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zixun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Keling Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S+Z">Seth Z. Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chuanyu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianjian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Weiyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+A+Y">Allen Y. Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13679" title="Abstract">arXiv:2309.13679</a> (replaced) [<a href="/pdf/2309.13679" title="Download PDF">pdf</a>, <a href="/format/2309.13679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network-PSO-based Velocity Control Algorithm for Landing UAVs on  a Boat
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Li-Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rastgaar%2C+M">Mo Rastgaar</a>, 
<a href="/search/cs?searchtype=author&query=Mahmoudian%2C+N">Nina Mahmoudian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14970" title="Abstract">arXiv:2309.14970</a> (replaced) [<a href="/pdf/2309.14970" title="Download PDF">pdf</a>, <a href="/format/2309.14970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Hypernetworks are Surprisingly Strong in Meta-RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+J">Jacob Beck</a>, 
<a href="/search/cs?searchtype=author&query=Vuorio%2C+R">Risto Vuorio</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zheng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Whiteson%2C+S">Shimon Whiteson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14992" title="Abstract">arXiv:2309.14992</a> (replaced) [<a href="/pdf/2309.14992" title="Download PDF">pdf</a>, <a href="/format/2309.14992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the ChatGPT Approach for Bidirectional Traceability Problem  between Design Models and Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kanuka%2C+H">Hideyuki Kanuka</a>, 
<a href="/search/cs?searchtype=author&query=Koreki%2C+G">Genta Koreki</a>, 
<a href="/search/cs?searchtype=author&query=Soga%2C+R">Ryo Soga</a>, 
<a href="/search/cs?searchtype=author&query=Nishikawa%2C+K">Kazu Nishikawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages including references and appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15223" title="Abstract">arXiv:2309.15223</a> (replaced) [<a href="/pdf/2309.15223" title="Download PDF">pdf</a>, <a href="/format/2309.15223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-rank Adaptation of Large Language Model Rescoring for  Parameter-Efficient Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kolehmainen%2C+J">Jari Kolehmainen</a>, 
<a href="/search/cs?searchtype=author&query=Shivakumar%2C+P+G">Prashanth G. Shivakumar</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yile Gu</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+S">Sungho Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Roger Ren</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Q">Qi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gourav%2C+A">Aditya Gourav</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+I">I-Fan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi-Chieh Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dinh%2C+T">Tuan Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Gandhe%2C+A">Ankur Gandhe</a>, 
<a href="/search/cs?searchtype=author&query=Filimonov%2C+D">Denis Filimonov</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Shalini Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Stolcke%2C+A">Andreas Stolcke</a>, 
<a href="/search/cs?searchtype=author&query=Rastow%2C+A">Ariya Rastow</a>, 
<a href="/search/cs?searchtype=author&query=Bulyko%2C+I">Ivan Bulyko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023. Internal Review Approved. Revised 2nd version with Andreas and Huck. The first version is in Sep 29th. 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15312" title="Abstract">arXiv:2309.15312</a> (replaced) [<a href="/pdf/2309.15312" title="Download PDF">pdf</a>, <a href="/format/2309.15312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAPTree: Beating &quot;Optimal&quot; Decision Trees with Bayesian Decision Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+C">Colin Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mo Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Thrun%2C+S">Sebastian Thrun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15649" title="Abstract">arXiv:2309.15649</a> (replaced) [<a href="/pdf/2309.15649" title="Download PDF">pdf</a>, <a href="/format/2309.15649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Speech Recognition Error Correction with Large Language  Models and Task-Activating Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yile Gu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi-Chieh Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Shalini Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Bulyko%2C+I">Ivan Bulyko</a>, 
<a href="/search/cs?searchtype=author&query=Stolcke%2C+A">Andreas Stolcke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Automatic Speech Recognition and Understanding (ASRU) 2023. 8 pages. 2nd version revised from Sep 29th's version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16583" title="Abstract">arXiv:2309.16583</a> (replaced) [<a href="/pdf/2309.16583" title="Download PDF">pdf</a>, <a href="/format/2309.16583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-Fathom: Benchmarking Large Language Models to Decipher the  Evolutionary Path towards GPT-4 and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yijie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+C">Chenguang Xi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Pengyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00117" title="Abstract">arXiv:2310.00117</a> (replaced) [<a href="/pdf/2310.00117" title="Download PDF">pdf</a>, <a href="/format/2310.00117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI  Co-Writing Tasks using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reza%2C+M">Mohi Reza</a>, 
<a href="/search/cs?searchtype=author&query=Laundry%2C+N">Nathan Laundry</a>, 
<a href="/search/cs?searchtype=author&query=Musabirov%2C+I">Ilya Musabirov</a>, 
<a href="/search/cs?searchtype=author&query=Dushniku%2C+P">Peter Dushniku</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z+Y+%22">Zhi Yuan &quot;Michael&quot; Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+K">Kashish Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Grossman%2C+T">Tovi Grossman</a>, 
<a href="/search/cs?searchtype=author&query=Liut%2C+M">Michael Liut</a>, 
<a href="/search/cs?searchtype=author&query=Kuzminykh%2C+A">Anastasia Kuzminykh</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J+J">Joseph Jay Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00212" title="Abstract">arXiv:2310.00212</a> (replaced) [<a href="/pdf/2310.00212" title="Download PDF">pdf</a>, <a href="/format/2310.00212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for  LLM Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhaojin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ramchandran%2C+K">Kannan Ramchandran</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00270" title="Abstract">arXiv:2310.00270</a> (replaced) [<a href="/pdf/2310.00270" title="Download PDF">pdf</a>, <a href="/format/2310.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpatialRank: Urban Event Ranking with NDCG Optimization on  Spatiotemporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bang An</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yongjian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianbao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00297" title="Abstract">arXiv:2310.00297</a> (replaced) [<a href="/pdf/2310.00297" title="Download PDF">pdf</a>, <a href="/format/2310.00297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding In-Context Learning from Repetitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jianhao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chiyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yafu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00322" title="Abstract">arXiv:2310.00322</a> (replaced) [<a href="/pdf/2310.00322" title="Download PDF">pdf</a>, <a href="/format/2310.00322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chengdong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Minquan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ci%2C+H">Hai Ci</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00369" title="Abstract">arXiv:2310.00369</a> (replaced) [<a href="/pdf/2310.00369" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Inductive Bias: Knowledge Distillation Beyond Model  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habib%2C+G">Gousia Habib</a>, 
<a href="/search/cs?searchtype=author&query=Saleem%2C+T+J">Tausifa Jan Saleem</a>, 
<a href="/search/cs?searchtype=author&query=Lall%2C+B">Brejesh Lall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00839" title="Abstract">arXiv:2310.00839</a> (replaced) [<a href="/pdf/2310.00839" title="Download PDF">pdf</a>, <a href="/format/2310.00839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subsurface Characterization using Ensemble-based Approaches with Deep  Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+J">Jichao Bao</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+H">Hongkyu Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonghyun Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00977" title="Abstract">arXiv:2310.00977</a> (replaced) [<a href="/pdf/2310.00977" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Sensing Errors in Synchronous Motor Drives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pramod%2C+P">Prerit Pramod</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01271" title="Abstract">arXiv:2310.01271</a> (replaced) [<a href="/pdf/2310.01271" title="Download PDF">pdf</a>, <a href="/format/2310.01271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEEC: A Legal Element Extraction Dataset with an Extensive  Domain-Specific Label System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zongyue%2C+X">Xue Zongyue</a>, 
<a href="/search/cs?searchtype=author&query=Huanghai%2C+L">Liu Huanghai</a>, 
<a href="/search/cs?searchtype=author&query=Yiran%2C+H">Hu Yiran</a>, 
<a href="/search/cs?searchtype=author&query=Kangle%2C+K">Kong Kangle</a>, 
<a href="/search/cs?searchtype=author&query=Chenlu%2C+W">Wang Chenlu</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+L">Liu Yun</a>, 
<a href="/search/cs?searchtype=author&query=Weixing%2C+S">Shen Weixing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01366" title="Abstract">arXiv:2310.01366</a> (replaced) [<a href="/pdf/2310.01366" title="Download PDF">pdf</a>, <a href="/format/2310.01366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Window-based Model Averaging Improves Generalization in Heterogeneous  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caldarola%2C+D">Debora Caldarola</a>, 
<a href="/search/cs?searchtype=author&query=Caputo%2C+B">Barbara Caputo</a>, 
<a href="/search/cs?searchtype=author&query=Ciccone%2C+M">Marco Ciccone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Computer Vision Workshop (ICCVW)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01405" title="Abstract">arXiv:2310.01405</a> (replaced) [<a href="/pdf/2310.01405" title="Download PDF">pdf</a>, <a href="/format/2310.01405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Engineering: A Top-Down Approach to AI Transparency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Andy Zou</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+L">Long Phan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sarah Chen</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+J">James Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Phillip Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Richard Ren</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+A">Alexander Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xuwang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mazeika%2C+M">Mantas Mazeika</a>, 
<a href="/search/cs?searchtype=author&query=Dombrowski%2C+A">Ann-Kathrin Dombrowski</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Shashwat Goel</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nathaniel Li</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+M+J">Michael J. Byun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mallen%2C+A">Alex Mallen</a>, 
<a href="/search/cs?searchtype=author&query=Basart%2C+S">Steven Basart</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Fredrikson%2C+M">Matt Fredrikson</a>, 
<a href="/search/cs?searchtype=author&query=Kolter%2C+J+Z">J. Zico Kolter</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/andyzoujm/representation-engineering">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01444" title="Abstract">arXiv:2310.01444</a> (replaced) [<a href="/pdf/2310.01444" title="Download PDF">pdf</a>, <a href="/format/2310.01444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting LLM Agents Through Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yadong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Santacroce%2C+M">Michael Santacroce</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01641" title="Abstract">arXiv:2310.01641</a> (replaced) [<a href="/pdf/2310.01641" title="Download PDF">pdf</a>, <a href="/format/2310.01641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Only Look at Once for Real-time and Generic Multi-Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiayuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q+M+J">Q. M. Jonathan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01867" title="Abstract">arXiv:2310.01867</a> (replaced) [<a href="/pdf/2310.01867" title="Download PDF">pdf</a>, <a href="/format/2310.01867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio-visual child-adult speaker classification in dyadic interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+A">Anfeng Xu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+K">Kevin Huang</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+T">Tiantian Feng</a>, 
<a href="/search/eess?searchtype=author&query=Tager-Flusberg%2C+H">Helen Tager-Flusberg</a>, 
<a href="/search/eess?searchtype=author&query=Narayanan%2C+S">Shrikanth Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In review for ICASSP 2024, 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02402" title="Abstract">arXiv:2310.02402</a> (replaced) [<a href="/pdf/2310.02402" title="Download PDF">pdf</a>, <a href="/format/2310.02402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Parallel Complexity of Multilevel Monte Carlo in Stochastic  Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+K">Kei Ishikawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fixed a typo in the title and added acknowledgement
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02635" title="Abstract">arXiv:2310.02635</a> (replaced) [<a href="/pdf/2310.02635" title="Download PDF">pdf</a>, <a href="/format/2310.02635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Reinforcement Learning: towards Embodied Generalist Agents  with Foundation Prior Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Weirui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunsheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xianfan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02717" title="Abstract">arXiv:2310.02717</a> (replaced) [<a href="/pdf/2310.02717" title="Download PDF">pdf</a>, <a href="/format/2310.02717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Clustering of Bandits with Misspecified User Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jize Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xutong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+J+C+S">John C.S. Lui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02995" title="Abstract">arXiv:2310.02995</a> (replaced) [<a href="/e-print/2310.02995" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IBCL: Zero-shot Model Generation for Task Trade-offs in Continual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pengyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Caprio%2C+M">Michele Caprio</a>, 
<a href="/search/cs?searchtype=author&query=Eaton%2C+E">Eric Eaton</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Insup Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This should be a replacement for <a href="/abs/2305.14782">arXiv:2305.14782</a>. I falsely submitted a new paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02996" title="Abstract">arXiv:2310.02996</a> (replaced) [<a href="/pdf/2310.02996" title="Download PDF">pdf</a>, <a href="/format/2310.02996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Stochastic Dynamic Aggregative Game for Demand-Side  Management in Microgrids with Shared Battery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yadollahi%2C+S">Shahram Yadollahi</a>, 
<a href="/search/eess?searchtype=author&query=Kebriaei%2C+H">Hamed Kebriaei</a>, 
<a href="/search/eess?searchtype=author&query=Soudjani%2C+S">Sadegh Soudjani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03030" title="Abstract">arXiv:2310.03030</a> (replaced) [<a href="/pdf/2310.03030" title="Download PDF">pdf</a>, <a href="/format/2310.03030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-MolBERTa: GPT Molecular Features Language Model for molecular  property prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Balaji%2C+S">Suryanarayanan Balaji</a>, 
<a href="/search/physics?searchtype=author&query=Magar%2C+R">Rishikesh Magar</a>, 
<a href="/search/physics?searchtype=author&query=Jadhav%2C+Y">Yayati Jadhav</a>, 
<a href="/search/physics?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper has 17 pages, 4 figures and 4 tables, along with 71 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03304" title="Abstract">arXiv:2310.03304</a> (replaced) [<a href="/pdf/2310.03304" title="Download PDF">pdf</a>, <a href="/format/2310.03304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Personalized Story Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kevin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanlin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaomeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Andrew Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03742" title="Abstract">arXiv:2310.03742</a> (replaced) [<a href="/pdf/2310.03742" title="Download PDF">pdf</a>, <a href="/format/2310.03742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A High-Performance Design, Implementation, Deployment, and Evaluation of  The Slim Fly Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blach%2C+N">Nils Blach</a>, 
<a href="/search/cs?searchtype=author&query=Besta%2C+M">Maciej Besta</a>, 
<a href="/search/cs?searchtype=author&query=De+Sensi%2C+D">Daniele De Sensi</a>, 
<a href="/search/cs?searchtype=author&query=Domke%2C+J">Jens Domke</a>, 
<a href="/search/cs?searchtype=author&query=Harake%2C+H">Hussein Harake</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shigang Li</a>, 
<a href="/search/cs?searchtype=author&query=Iff%2C+P">Patrick Iff</a>, 
<a href="/search/cs?searchtype=author&query=Konieczny%2C+M">Marek Konieczny</a>, 
<a href="/search/cs?searchtype=author&query=Lakhotia%2C+K">Kartik Lakhotia</a>, 
<a href="/search/cs?searchtype=author&query=Kubicek%2C+A">Ales Kubicek</a>, 
<a href="/search/cs?searchtype=author&query=Ferrari%2C+M">Marcel Ferrari</a>, 
<a href="/search/cs?searchtype=author&query=Petrini%2C+F">Fabrizio Petrini</a>, 
<a href="/search/cs?searchtype=author&query=Hoefler%2C+T">Torsten Hoefler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03748" title="Abstract">arXiv:2310.03748</a> (replaced) [<a href="/pdf/2310.03748" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase Synchrony Component Self-Organization in Brain Computer Interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Niu%2C+X">Xu Niu</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+N">Na Lu</a>, 
<a href="/search/eess?searchtype=author&query=Luo%2C+H">Huan Luo</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+R">Ruofan Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03925" title="Abstract">arXiv:2310.03925</a> (replaced) [<a href="/pdf/2310.03925" title="Download PDF">pdf</a>, <a href="/format/2310.03925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multitask Learning for Time Series Data with 2D Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C+M">Chin-Chia Michael Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xin Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yujie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Der%2C+A">Audrey Der</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zhongfang Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03951" title="Abstract">arXiv:2310.03951</a> (replaced) [<a href="/pdf/2310.03951" title="Download PDF">pdf</a>, <a href="/format/2310.03951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain of Natural Language Inference for Reducing Large Language Model  Ungrounded Hallucinations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+D">Deren Lei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengya Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+V">Vincent Yun</a>, 
<a href="/search/cs?searchtype=author&query=Ching%2C+E">Emily Ching</a>, 
<a href="/search/cs?searchtype=author&query=Kamal%2C+E">Eslam Kamal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The source code is available at <a href="https://github.com/microsoft/CoNLI_hallucination">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04006" title="Abstract">arXiv:2310.04006</a> (replaced) [<a href="/pdf/2310.04006" title="Download PDF">pdf</a>, <a href="/format/2310.04006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating optimization over the space of probability measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+S">Shi Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Q">Qin Li</a>, 
<a href="/search/math?searchtype=author&query=Tse%2C+O">Oliver Tse</a>, 
<a href="/search/math?searchtype=author&query=Wright%2C+S+J">Stephen J. Wright</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04086" title="Abstract">arXiv:2310.04086</a> (replaced) [<a href="/pdf/2310.04086" title="Download PDF">pdf</a>, <a href="/format/2310.04086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Chess Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masouris%2C+A">Athanasios Masouris</a>, 
<a href="/search/cs?searchtype=author&query=van+Gemert%2C+J">Jan van Gemert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04270" title="Abstract">arXiv:2310.04270</a> (replaced) [<a href="/pdf/2310.04270" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Evaluation of Large Language Models on Benchmark  Biomedical Text Processing Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahan%2C+I">Israt Jahan</a>, 
<a href="/search/cs?searchtype=author&query=Laskar%2C+M+T+R">Md Tahmid Rahman Laskar</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Chun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimmy Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of the following BioNLP paper: <a href="https://aclanthology.org/2023.bionlp-1.30/">this https URL</a> (<a href="/abs/2306.04504">arXiv:2306.04504</a>). arXiv admin note: substantial text overlap with <a href="/abs/2306.04504">arXiv:2306.04504</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04373" title="Abstract">arXiv:2310.04373</a> (replaced) [<a href="/pdf/2310.04373" title="Download PDF">pdf</a>, <a href="/format/2310.04373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confronting Reward Model Overoptimization with Constrained RLHF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moskovitz%2C+T">Ted Moskovitz</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A+K">Aaditya K. Singh</a>, 
<a href="/search/cs?searchtype=author&query=Strouse%2C+D">DJ Strouse</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>, 
<a href="/search/cs?searchtype=author&query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A+D">Anca D. Dragan</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S">Stephen McAleer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04444" title="Abstract">arXiv:2310.04444</a> (replaced) [<a href="/pdf/2310.04444" title="Download PDF">pdf</a>, <a href="/format/2310.04444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s the Magic Word? A Control Theory of LLM Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhargava%2C+A">Aman Bhargava</a>, 
<a href="/search/cs?searchtype=author&query=Witkowski%2C+C">Cameron Witkowski</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Manav Shah</a>, 
<a href="/search/cs?searchtype=author&query=Thomson%2C+M">Matt Thomson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures. Under review for ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04474" title="Abstract">arXiv:2310.04474</a> (replaced) [<a href="/pdf/2310.04474" title="Download PDF">pdf</a>, <a href="/format/2310.04474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinger Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hui Cai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yicheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Rui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jing Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04480" title="Abstract">arXiv:2310.04480</a> (replaced) [<a href="/pdf/2310.04480" title="Download PDF">pdf</a>, <a href="/format/2310.04480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-survey Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khuong%2C+T+G+H">Thanh Gia Hieu Khuong</a> (TAU, LISN), 
<a href="/search/cs?searchtype=author&query=Rachmat%2C+B+K">Benedictus Kent Rachmat</a> (TAU, LISN)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Junior Conference on Data Science and Engineering 2023, Sep 2023,
  Orsay, France
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04484" title="Abstract">arXiv:2310.04484</a> (replaced) [<a href="/pdf/2310.04484" title="Download PDF">pdf</a>, <a href="/format/2310.04484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ada-Instruct: Adapting Instruction Generators for Complex Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wanyun Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianle Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04567" title="Abstract">arXiv:2310.04567</a> (replaced) [<a href="/pdf/2310.04567" title="Download PDF">pdf</a>, <a href="/format/2310.04567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPM-TSE: A Diffusion Probabilistic Model for Target Sound Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hai%2C+J">Jiarui Hai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Helin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+D">Dongchao Yang</a>, 
<a href="/search/eess?searchtype=author&query=Thakkar%2C+K">Karan Thakkar</a>, 
<a href="/search/eess?searchtype=author&query=Dehak%2C+N">Najim Dehak</a>, 
<a href="/search/eess?searchtype=author&query=Elhilali%2C+M">Mounya Elhilali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04570" title="Abstract">arXiv:2310.04570</a> (replaced) [<a href="/pdf/2310.04570" title="Download PDF">pdf</a>, <a href="/format/2310.04570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction  from Variable-Sized Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hehn%2C+T+M">Thomas M. Hehn</a>, 
<a href="/search/cs?searchtype=author&query=Orekondy%2C+T">Tribhuvanesh Orekondy</a>, 
<a href="/search/cs?searchtype=author&query=Shental%2C+O">Ori Shental</a>, 
<a href="/search/cs?searchtype=author&query=Behboodi%2C+A">Arash Behboodi</a>, 
<a href="/search/cs?searchtype=author&query=Bucheli%2C+J">Juan Bucheli</a>, 
<a href="/search/cs?searchtype=author&query=Doshi%2C+A">Akash Doshi</a>, 
<a href="/search/cs?searchtype=author&query=Namgoong%2C+J">June Namgoong</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+T">Taesang Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Sampath%2C+A">Ashwin Sampath</a>, 
<a href="/search/cs?searchtype=author&query=Soriaga%2C+J+B">Joseph B. Soriaga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE GLOBECOM 2023, v2: Changed license on arxiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04633" title="Abstract">arXiv:2310.04633</a> (replaced) [<a href="/pdf/2310.04633" title="Download PDF">pdf</a>, <a href="/format/2310.04633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unbiased and Robust: External Attention-enhanced Graph Contrastive  Learning for Cross-domain Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinhua Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+H">Houping Yue</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zizheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liancheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, accepted by ICDM 2023 (workshop-GML4Rec)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04671" title="Abstract">arXiv:2310.04671</a> (replaced) [<a href="/pdf/2310.04671" title="Download PDF">pdf</a>, <a href="/format/2310.04671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem  Formulation and Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charoenpitaks%2C+K">Korawat Charoenpitaks</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Van-Quang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Suganuma%2C+M">Masanori Suganuma</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+M">Masahiro Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Niihara%2C+R">Ryoma Niihara</a>, 
<a href="/search/cs?searchtype=author&query=Okatani%2C+T">Takayuki Okatani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main Paper: 10 pages, Supplementary Materials: 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04673" title="Abstract">arXiv:2310.04673</a> (replaced) [<a href="/pdf/2310.04673" title="Download PDF">pdf</a>, <a href="/format/2310.04673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhihao Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Y">Yunfei Chu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhifu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zerui Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kai Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaohuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Siqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhijie Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04678" title="Abstract">arXiv:2310.04678</a> (replaced) [<a href="/pdf/2310.04678" title="Download PDF">pdf</a>, <a href="/format/2310.04678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaicheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Naidu%2C+P">Prudhviraj Naidu</a>, 
<a href="/search/cs?searchtype=author&query=Bergen%2C+L">Leon Bergen</a>, 
<a href="/search/cs?searchtype=author&query=Paturi%2C+R">Ramamohan Paturi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in NeurIPS 2023 Datasets and Benchmarks Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04693" title="Abstract">arXiv:2310.04693</a> (replaced) [<a href="/pdf/2310.04693" title="Download PDF">pdf</a>, <a href="/format/2310.04693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness-enhanced Uplift Modeling with Adversarial Feature  Desensitization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zexu Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bowei He</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Ming Ma</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiakai Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dugang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04696" title="Abstract">arXiv:2310.04696</a> (replaced) [<a href="/pdf/2310.04696" title="Download PDF">pdf</a>, <a href="/format/2310.04696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Serving Deep Learning Model in Relational Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eichenberger%2C+A">Alexandre Eichenberger</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Masood%2C+S">Saif Masood</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+H">Hong Min</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+A">Alexander Sim</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kesheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Binhang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lixi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Authors are ordered alphabetically; Jia Zou is the corresponding author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04706" title="Abstract">arXiv:2310.04706</a> (replaced) [<a href="/pdf/2310.04706" title="Download PDF">pdf</a>, <a href="/format/2310.04706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Imitation Learning with Variational Counterfactual Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bowei He</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zexu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04711" title="Abstract">arXiv:2310.04711</a> (replaced) [<a href="/pdf/2310.04711" title="Download PDF">pdf</a>, <a href="/format/2310.04711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DP-starJ: A Differential Private Scheme towards Analytical Star-Join  Queries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Congcong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian Lou</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiangtao Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04741" title="Abstract">arXiv:2310.04741</a> (replaced) [<a href="/pdf/2310.04741" title="Download PDF">pdf</a>, <a href="/format/2310.04741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing stability and plasticity in continual learning: the  readout-decomposition of activation change (RDAC) framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anthes%2C+D">Daniel Anthes</a>, 
<a href="/search/cs?searchtype=author&query=Thorat%2C+S">Sushrut Thorat</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nig%2C+P">Peter K&#xf6;nig</a>, 
<a href="/search/cs?searchtype=author&query=Kietzmann%2C+T+C">Tim C. Kietzmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04742" title="Abstract">arXiv:2310.04742</a> (replaced) [<a href="/pdf/2310.04742" title="Download PDF">pdf</a>, <a href="/format/2310.04742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter Efficient Multi-task Model Fusion with Partial Linearization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Anke Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Y">Yibing Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Han Hu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04750" title="Abstract">arXiv:2310.04750</a> (replaced) [<a href="/pdf/2310.04750" title="Download PDF">pdf</a>, <a href="/format/2310.04750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffNAS: Bootstrapping Diffusion Models by Prompting for Better  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xiu Su</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+S">Shan You</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04768" title="Abstract">arXiv:2310.04768</a> (replaced) [<a href="/pdf/2310.04768" title="Download PDF">pdf</a>, <a href="/format/2310.04768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Corrupted User Detection and Regret Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jize Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Lui%2C+J+C+S">John C.S. Lui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04769" title="Abstract">arXiv:2310.04769</a> (replaced) [<a href="/pdf/2310.04769" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023  Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhishan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhi Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shihao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+M">Minqiang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mochen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiajun Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04821" title="Abstract">arXiv:2310.04821</a> (replaced) [<a href="/pdf/2310.04821" title="Download PDF">pdf</a>, <a href="/format/2310.04821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethink Baseline of Integrated Gradients from the Perspective of Shapley  Value
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zixuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Ge Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Ji Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Changjie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R+W+Y">Runze Wu Yujing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Ze Ji</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04835" title="Abstract">arXiv:2310.04835</a> (replaced) [<a href="/pdf/2310.04835" title="Download PDF">pdf</a>, <a href="/format/2310.04835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Evolution of Knowledge Graphs: A Survey and Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xuhui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengjin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yinghan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Lumingyuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Saizhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhongwu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanzhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jian Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04859" title="Abstract">arXiv:2310.04859</a> (replaced) [<a href="/pdf/2310.04859" title="Download PDF">pdf</a>, <a href="/format/2310.04859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Graph Random Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Reid%2C+I">Isaac Reid</a>, 
<a href="/search/stat?searchtype=author&query=Choromanski%2C+K">Krzysztof Choromanski</a>, 
<a href="/search/stat?searchtype=author&query=Berger%2C+E">Eli Berger</a>, 
<a href="/search/stat?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04870" title="Abstract">arXiv:2310.04870</a> (replaced) [<a href="/pdf/2310.04870" title="Download PDF">pdf</a>, <a href="/format/2310.04870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lemur: Integrating Large Language Models in Automated Program  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoze Wu</a>, 
<a href="/search/cs?searchtype=author&query=Barrett%2C+C">Clark Barrett</a>, 
<a href="/search/cs?searchtype=author&query=Narodytska%2C+N">Nina Narodytska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04884" title="Abstract">arXiv:2310.04884</a> (replaced) [<a href="/pdf/2310.04884" title="Download PDF">pdf</a>, <a href="/ps/2310.04884" title="Download PostScript">ps</a>, <a href="/format/2310.04884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Analysis of Repeated Delegated Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajiaghayi%2C+M">MohammadTaghi Hajiaghayi</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavi%2C+M">Mohammad Mahdavi</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+K">Keivan Rezaei</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Suho Shin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04898" title="Abstract">arXiv:2310.04898</a> (replaced) [<a href="/pdf/2310.04898" title="Download PDF">pdf</a>, <a href="/format/2310.04898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Multi-domain Trust Infrastructures for Segmented Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grierson%2C+S">Sam Grierson</a>, 
<a href="/search/cs?searchtype=author&query=Buchanan%2C+W+J">William J Buchanan</a>, 
<a href="/search/cs?searchtype=author&query=Thomson%2C+C">Craig Thomson</a>, 
<a href="/search/cs?searchtype=author&query=Ghaleb%2C+B">Baraq Ghaleb</a>, 
<a href="/search/cs?searchtype=author&query=Maglaras%2C+L">Leandros Maglaras</a>, 
<a href="/search/cs?searchtype=author&query=Eckl%2C+C">Chris Eckl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04923" title="Abstract">arXiv:2310.04923</a> (replaced) [<a href="/pdf/2310.04923" title="Download PDF">pdf</a>, <a href="/format/2310.04923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimal Unequal Error Protection LDPC Coded Recording System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chou%2C+H">Hong-fu Chou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04930" title="Abstract">arXiv:2310.04930</a> (replaced) [<a href="/pdf/2310.04930" title="Download PDF">pdf</a>, <a href="/format/2310.04930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via  Differentiable Physics Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yuqi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feitong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qinsi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gang%2C+Y">Yang Gang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xingyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Lin Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04991" title="Abstract">arXiv:2310.04991</a> (replaced) [<a href="/pdf/2310.04991" title="Download PDF">pdf</a>, <a href="/format/2310.04991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-Teller: Enhancing Cross-Modal Generation with Fusion and  Decoupling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haogeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Q">Qihang Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tingkai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunzhe Tao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huaibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04999" title="Abstract">arXiv:2310.04999</a> (replaced) [<a href="/pdf/2310.04999" title="Download PDF">pdf</a>, <a href="/format/2310.04999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetrical Linguistic Feature Distillation with CLIP for Scene Text  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hongtao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jianjun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM MM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05028" title="Abstract">arXiv:2310.05028</a> (replaced) [<a href="/pdf/2310.05028" title="Download PDF">pdf</a>, <a href="/format/2310.05028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Large Language Models as Zero-shot Relation Extractors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guozheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+W">Wenjun Ke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05035" title="Abstract">arXiv:2310.05035</a> (replaced) [<a href="/pdf/2310.05035" title="Download PDF">pdf</a>, <a href="/format/2310.05035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Convinced Prompting: Few-Shot Question Answering with Repeated  Introspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haodi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Min Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C+J">Chen Jason Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kaishun Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05036" title="Abstract">arXiv:2310.05036</a> (replaced) [<a href="/pdf/2310.05036" title="Download PDF">pdf</a>, <a href="/format/2310.05036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text to Tactic: Evaluating LLMs Playing the Game of Avalon
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Light%2C+J">Jonathan Light</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Min Cai</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Ziniu Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05056" title="Abstract">arXiv:2310.05056</a> (replaced) [<a href="/pdf/2310.05056" title="Download PDF">pdf</a>, <a href="/format/2310.05056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-driven Open-Vocabulary Keypoint Detection for Animal Body and  Face
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lumin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+S">Shenqi Lai</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+P">Ping Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05063" title="Abstract">arXiv:2310.05063</a> (replaced) [<a href="/pdf/2310.05063" title="Download PDF">pdf</a>, <a href="/format/2310.05063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing the Limits of Pre-training for Time Series Forecasting in the  CloudOps Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woo%2C+G">Gerald Woo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akshat Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+D">Doyen Sahoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05074" title="Abstract">arXiv:2310.05074</a> (replaced) [<a href="/pdf/2310.05074" title="Download PDF">pdf</a>, <a href="/format/2310.05074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengcheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xiaowei Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Che Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yixin Lian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Ming Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoyuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05136" title="Abstract">arXiv:2310.05136</a> (replaced) [<a href="/pdf/2310.05136" title="Download PDF">pdf</a>, <a href="/format/2310.05136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructDET: Diversifying Referring Object Detection with Generalized  Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dang%2C+R">Ronghao Dang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiangyan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haodong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+C">Chongjian Ge</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lin Song</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+L">Lijun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengju Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qijun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Feng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yibing Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages (include appendix), technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05167" title="Abstract">arXiv:2310.05167</a> (replaced) [<a href="/pdf/2310.05167" title="Download PDF">pdf</a>, <a href="/format/2310.05167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hieros: Hierarchical Imagination on Structured State Space Sequence  World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mattes%2C+P">Paul Mattes</a>, 
<a href="/search/cs?searchtype=author&query=Schlosser%2C+R">Rainer Schlosser</a>, 
<a href="/search/cs?searchtype=author&query=Herbrich%2C+R">Ralf Herbrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICLR 2024, 23 pages, 11 figures, code available at: <a href="https://github.com/Snagnar/Hieros">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05189" title="Abstract">arXiv:2310.05189</a> (replaced) [<a href="/pdf/2310.05189" title="Download PDF">pdf</a>, <a href="/ps/2310.05189" title="Download PostScript">ps</a>, <a href="/format/2310.05189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factuality Challenges in the Era of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+M">Meeyoung Cha</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Ciampaglia%2C+G+L">Giovanni Luca Ciampaglia</a>, 
<a href="/search/cs?searchtype=author&query=Corney%2C+D">David Corney</a>, 
<a href="/search/cs?searchtype=author&query=DiResta%2C+R">Renee DiResta</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S">Scott Hale</a>, 
<a href="/search/cs?searchtype=author&query=Halevy%2C+A">Alon Halevy</a>, 
<a href="/search/cs?searchtype=author&query=Hovy%2C+E">Eduard Hovy</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Menczer%2C+F">Filippo Menczer</a>, 
<a href="/search/cs?searchtype=author&query=Miguez%2C+R">Ruben Miguez</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Scheufele%2C+D">Dietram Scheufele</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shivam Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zagni%2C+G">Giovanni Zagni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our article offers a comprehensive examination of the challenges and risks associated with Large Language Models (LLMs), focusing on their potential impact on the veracity of information in today's digital landscape
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05202" title="Abstract">arXiv:2310.05202</a> (replaced) [<a href="/pdf/2310.05202" title="Download PDF">pdf</a>, <a href="/format/2310.05202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Cross-Dataset Performance of Distracted Driving Detection With  Score-Softmax Classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+C">Cong Duan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jiahao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minghai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jiacai Liao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Libo Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05240" title="Abstract">arXiv:2310.05240</a> (replaced) [<a href="/pdf/2310.05240" title="Download PDF">pdf</a>, <a href="/ps/2310.05240" title="Download PostScript">ps</a>, <a href="/format/2310.05240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limitations of Stochastic Selection Problems with Pairwise Independent  Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dughmi%2C+S">Shaddin Dughmi</a>, 
<a href="/search/cs?searchtype=author&query=Kalayci%2C+Y+H">Yusuf Hakan Kalayci</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Neel Patel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05242" title="Abstract">arXiv:2310.05242</a> (replaced) [<a href="/pdf/2310.05242" title="Download PDF">pdf</a>, <a href="/format/2310.05242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatRadio-Valuer: A Chat Large Language Model for Generalizable  Radiology Report Generation Based on Multi-institution and Multi-system Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+T">Tianyang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yutong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Peixin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zuowei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kui%2C+X">Xiaoyan Kui</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+Y">Youlan Shang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Li Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yaonai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Longtao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Ning Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiaqi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Ying Zeng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lei He</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhixue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Haixing Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xiaoyan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xintao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shijie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dajiang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+D">Dinggang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Junwei Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tuo Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05255" title="Abstract">arXiv:2310.05255</a> (replaced) [<a href="/pdf/2310.05255" title="Download PDF">pdf</a>, <a href="/format/2310.05255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persis: A Persian Font Recognition Pipeline Using Convolutional Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadian%2C+M">Mehrdad Mohammadian</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+N">Neda Maleki</a>, 
<a href="/search/cs?searchtype=author&query=Olsson%2C+T">Tobias Olsson</a>, 
<a href="/search/cs?searchtype=author&query=Ahlgren%2C+F">Fredrik Ahlgren</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 12th International Conference on Computer and Knowledge
  Engineering (ICCKE) (2022) 196-204
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05317" title="Abstract">arXiv:2310.05317</a> (replaced) [<a href="/pdf/2310.05317" title="Download PDF">pdf</a>, <a href="/format/2310.05317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Long-form Text Generation in Mental Health with Task-adaptive  Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+N">Naihao Deng</a>, 
<a href="/search/cs?searchtype=author&query=Sabour%2C+S">Sahand Sabour</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yilin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the main conference of The 2023 Conference on Empirical Methods in Natural Language Processing; 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing(EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05341" title="Abstract">arXiv:2310.05341</a> (replaced) [<a href="/pdf/2310.05341" title="Download PDF">pdf</a>, <a href="/format/2310.05341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Look at Classic Test-Time Adaptation Methods in Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+C">Chang&#x27;an Yi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haotian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05349" title="Abstract">arXiv:2310.05349</a> (replaced) [<a href="/pdf/2310.05349" title="Download PDF">pdf</a>, <a href="/format/2310.05349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALECE: An Attention-based Learned Cardinality Estimator for SPJ Queries  on Dynamic Workloads (Extended)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Rong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hua Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> VLDB 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05364" title="Abstract">arXiv:2310.05364</a> (replaced) [<a href="/pdf/2310.05364" title="Download PDF">pdf</a>, <a href="/format/2310.05364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Multi-modal Entity Alignment via Iteratively Fusing Modality  Similarity Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bolin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Xin Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05365" title="Abstract">arXiv:2310.05365</a> (replaced) [<a href="/pdf/2310.05365" title="Download PDF">pdf</a>, <a href="/format/2310.05365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Molecular De Novo Design through Transformer-based Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pengcheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianfan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Laghuvarapu%2C+S">Siddhartha Laghuvarapu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05371" title="Abstract">arXiv:2310.05371</a> (replaced) [<a href="/pdf/2310.05371" title="Download PDF">pdf</a>, <a href="/format/2310.05371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using  mpMRI Segmentation and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gavade%2C+A+B">Anil B. Gavade</a>, 
<a href="/search/eess?searchtype=author&query=Kanwal%2C+N">Neel Kanwal</a>, 
<a href="/search/eess?searchtype=author&query=Gavade%2C+P+A">Priyanka A. Gavade</a>, 
<a href="/search/eess?searchtype=author&query=Nerli%2C+R">Rajendra Nerli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CISCON-2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05377" title="Abstract">arXiv:2310.05377</a> (replaced) [<a href="/pdf/2310.05377" title="Download PDF">pdf</a>, <a href="/format/2310.05377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Evolution Strategies with Multi-Level Learning for  Large-Scale Black-Box Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Q">Qiqi Duan</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guochen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05393" title="Abstract">arXiv:2310.05393</a> (replaced) [<a href="/pdf/2310.05393" title="Download PDF">pdf</a>, <a href="/format/2310.05393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Side-Tuning for Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weifeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wentao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mingxin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lianwen Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05446" title="Abstract">arXiv:2310.05446</a> (replaced) [<a href="/pdf/2310.05446" title="Download PDF">pdf</a>, <a href="/format/2310.05446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RetSeg: Retention-based Colorectal Polyps Segmentation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=ELKarazle%2C+K">Khaled ELKarazle</a>, 
<a href="/search/eess?searchtype=author&query=Raman%2C+V">Valliappan Raman</a>, 
<a href="/search/eess?searchtype=author&query=Chua%2C+C">Caslon Chua</a>, 
<a href="/search/eess?searchtype=author&query=Then%2C+P">Patrick Then</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version with a PDF
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05493" title="Abstract">arXiv:2310.05493</a> (replaced) [<a href="/pdf/2310.05493" title="Download PDF">pdf</a>, <a href="/format/2310.05493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adaptable IoT Rule Engine Framework for Dataflow Monitoring and  Control Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Ken Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages,10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05507" title="Abstract">arXiv:2310.05507</a> (replaced) [<a href="/pdf/2310.05507" title="Download PDF">pdf</a>, <a href="/format/2310.05507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO  Radars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yilong Li</a>, 
<a href="/search/cs?searchtype=author&query=Sheshadri%2C+R+K">Ramanujan K Sheshadri</a>, 
<a href="/search/cs?searchtype=author&query=Sundaresan%2C+K">Karthik Sundaresan</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+E">Eugene Chai</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Suman Banerjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05537" title="Abstract">arXiv:2310.05537</a> (replaced) [<a href="/pdf/2310.05537" title="Download PDF">pdf</a>, <a href="/format/2310.05537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ParFam -- Symbolic Regression Based on Continuous Global Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scholl%2C+P">Philipp Scholl</a>, 
<a href="/search/cs?searchtype=author&query=Bieker%2C+K">Katharina Bieker</a>, 
<a href="/search/cs?searchtype=author&query=Hauger%2C+H">Hillary Hauger</a>, 
<a href="/search/cs?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/Philipp238/parfam">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05538" title="Abstract">arXiv:2310.05538</a> (replaced) [<a href="/pdf/2310.05538" title="Download PDF">pdf</a>, <a href="/format/2310.05538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion  for Polyp Localization in Colonoscopy Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nam%2C+J">Ju-Hyeon Nam</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+S">Seo-Hyeong Park</a>, 
<a href="/search/eess?searchtype=author&query=Syazwany%2C+N+S">Nur Suriza Syazwany</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+Y">Yerim Jung</a>, 
<a href="/search/eess?searchtype=author&query=Im%2C+Y">Yu-Han Im</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+S">Sang-Chul Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5pages. 2023 IEEE International Conference on Image Processing (ICIP). IEEE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05629" title="Abstract">arXiv:2310.05629</a> (replaced) [<a href="/pdf/2310.05629" title="Download PDF">pdf</a>, <a href="/ps/2310.05629" title="Download PostScript">ps</a>, <a href="/format/2310.05629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Super Denoise Net: Speech Super Resolution with Noise Cancellation in  Low Sampling Rate Noisy Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+J">Junkang Yang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Hongqing Liu</a>, 
<a href="/search/eess?searchtype=author&query=Gan%2C+L">Lu Gan</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05677" title="Abstract">arXiv:2310.05677</a> (replaced) [<a href="/pdf/2310.05677" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction of stock molecular system and the popularization of Density  Functional Theory in stock market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huajian Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longjian Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiajian Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 1 table, 18 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05720" title="Abstract">arXiv:2310.05720</a> (replaced) [<a href="/pdf/2310.05720" title="Download PDF">pdf</a>, <a href="/format/2310.05720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperLips: Hyper Control Lips with High Resolution Decoder for Talking  Face Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaosen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Han Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xuming Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05732" title="Abstract">arXiv:2310.05732</a> (replaced) [<a href="/pdf/2310.05732" title="Download PDF">pdf</a>, <a href="/format/2310.05732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Scheduling with a Shared Resource
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Damerius%2C+C">Christoph Damerius</a>, 
<a href="/search/cs?searchtype=author&query=Kling%2C+P">Peter Kling</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+F">Florian Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to COCOA 2023, Full Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05813" title="Abstract">arXiv:2310.05813</a> (replaced) [<a href="/pdf/2310.05813" title="Download PDF">pdf</a>, <a href="/format/2310.05813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio compression-assisted feature extraction for voice replay attack  detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiangyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haorui He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhizheng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05863" title="Abstract">arXiv:2310.05863</a> (replaced) [<a href="/pdf/2310.05863" title="Download PDF">pdf</a>, <a href="/format/2310.05863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained Audio-Visual Joint Representations for Multimodal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guangzhi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wenyi Yu</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+C">Changli Tang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xianzhao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+T">Tian Tan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+Z">Zejun Ma</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05873" title="Abstract">arXiv:2310.05873</a> (replaced) [<a href="/pdf/2310.05873" title="Download PDF">pdf</a>, <a href="/format/2310.05873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhili Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jianhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J">James Kwok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05914" title="Abstract">arXiv:2310.05914</a> (replaced) [<a href="/pdf/2310.05914" title="Download PDF">pdf</a>, <a href="/format/2310.05914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NEFTune: Noisy Embeddings Improve Instruction Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+N">Neel Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+P">Ping-yeh Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yuxin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Kirchenbauer%2C+J">John Kirchenbauer</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+H">Hong-Min Chu</a>, 
<a href="/search/cs?searchtype=author&query=Somepalli%2C+G">Gowthami Somepalli</a>, 
<a href="/search/cs?searchtype=author&query=Bartoldson%2C+B+R">Brian R. Bartoldson</a>, 
<a href="/search/cs?searchtype=author&query=Kailkhura%2C+B">Bhavya Kailkhura</a>, 
<a href="/search/cs?searchtype=author&query=Schwarzschild%2C+A">Avi Schwarzschild</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Aniruddha Saha</a>, 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>, 
<a href="/search/cs?searchtype=author&query=Geiping%2C+J">Jonas Geiping</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, Code is available on Github: <a href="https://github.com/neelsjain/NEFTune">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05916" title="Abstract">arXiv:2310.05916</a> (replaced) [<a href="/pdf/2310.05916" title="Download PDF">pdf</a>, <a href="/format/2310.05916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting CLIP&#x27;s Image Representation via Text-Based Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gandelsman%2C+Y">Yossi Gandelsman</a>, 
<a href="/search/cs?searchtype=author&query=Efros%2C+A+A">Alexei A. Efros</a>, 
<a href="/search/cs?searchtype=author&query=Steinhardt%2C+J">Jacob Steinhardt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page and code: <a href="https://yossigandelsman.github.io/clip_decomposition/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05921" title="Abstract">arXiv:2310.05921</a> (replaced) [<a href="/pdf/2310.05921" title="Download PDF">pdf</a>, <a href="/format/2310.05921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Decision Theory: Safe Autonomous Decisions from Imperfect  Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lekeufack%2C+J">Jordan Lekeufack</a>, 
<a href="/search/stat?searchtype=author&query=Angelopoulos%2C+A+N">Anastasios N. Angelopoulos</a>, 
<a href="/search/stat?searchtype=author&query=Bajcsy%2C+A">Andrea Bajcsy</a>, 
<a href="/search/stat?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/stat?searchtype=author&query=Malik%2C+J">Jitendra Malik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Robotics (cs.RO); Methodology (stat.ME)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item399">Cross-lists</a></li>
<li><a href="#item453">Replacements</a></li>
</ul>
<small>[ total of 751 entries:  <b>1-751</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
