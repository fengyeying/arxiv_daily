<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Mon 23 Oct 23  to  Tue 24 Oct 23, announced Wed, 25 Oct 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item365">Cross-lists</a></li>
<li><a href="#item416">Replacements</a></li>
</ul>
<small>[ total of 675 entries:  <b>1-675</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Wed, 25 Oct 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15188" title="Abstract">arXiv:2310.15188</a> [<a href="/pdf/2310.15188" title="Download PDF">pdf</a>, <a href="/format/2310.15188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Approaches for Dynamic Mechanical Analysis of Viscoelastic  Fiber Composites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+V">Victor Hoffmann</a> (1), 
<a href="/search/cs?searchtype=author&query=Nahmed%2C+I">Ilias Nahmed</a> (1), 
<a href="/search/cs?searchtype=author&query=Rastin%2C+P">Parisa Rastin</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Cabanes%2C+G">Gu&#xe9;na&#xeb;l Cabanes</a> (3), 
<a href="/search/cs?searchtype=author&query=Boisse%2C+J">Julien Boisse</a> (4) ((1) ENSMN, (2) LORIA UMR 7503, (3) LIPN UMR 7030, (4) LEMTA UMR 7563)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, <a href="https://hal.science/hal-04250557">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The increased adoption of reinforced polymer (RP) composite materials, driven
by eco-design standards, calls for a fine balance between lightness, stiffness,
and effective vibration control. These materials are integral to enhancing
comfort, safety, and energy efficiency. Dynamic Mechanical Analysis (DMA)
characterizes viscoelastic behavior, yet there's a growing interest in using
Machine Learning (ML) to expedite the design and understanding of
microstructures. In this paper we aim to map microstructures to their
mechanical properties using deep neural networks, speeding up the process and
allowing for the generation of microstructures from desired properties.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15189" title="Abstract">arXiv:2310.15189</a> [<a href="/pdf/2310.15189" title="Download PDF">pdf</a>, <a href="/format/2310.15189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Subject Agnostic Affective Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A+K">Amit Kumar Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+P">Prayag Tiwari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear in MUWS workshop at the 32nd ACM International Conference on Information and Knowledge Management (CIKM) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC); Multimedia (cs.MM)

</div>
<p class="mathjax">This paper focuses on affective emotion recognition, aiming to perform in the
subject-agnostic paradigm based on EEG signals. However, EEG signals manifest
subject instability in subject-agnostic affective Brain-computer interfaces
(aBCIs), which led to the problem of distributional shift. Furthermore, this
problem is alleviated by approaches such as domain generalisation and domain
adaptation. Typically, methods based on domain adaptation confer comparatively
better results than the domain generalisation methods but demand more
computational resources given new subjects. We propose a novel framework,
meta-learning based augmented domain adaptation for subject-agnostic aBCIs. Our
domain adaptation approach is augmented through meta-learning, which consists
of a recurrent neural network, a classifier, and a distributional shift
controller based on a sum-decomposable function. Also, we present that a neural
network explicating a sum-decomposable function can effectively estimate the
divergence between varied domains. The network setting for augmented domain
adaptation follows meta-learning and adversarial learning, where the controller
promptly adapts to new domains employing the target data via a few
self-adaptation steps in the test phase. Our proposed approach is shown to be
effective in experiments on a public aBICs dataset and achieves similar
performance to state-of-the-art domain adaptation methods while avoiding the
use of additional computational resources.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15190" title="Abstract">arXiv:2310.15190</a> [<a href="/pdf/2310.15190" title="Download PDF">pdf</a>, <a href="/format/2310.15190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Path Planning for Autonomous Vehicle Parking with Safety-Guarantee  using Hamilton-Jacobi Reachability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+X">Xuemin Chi</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jihao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhitao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hongye Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present a fast planning architecture called Hamilton-Jacobi-based
bidirectional A* (HJBA*) to solve general tight parking scenarios. The
algorithm is a two-layer composed of a high-level HJ-based reachability
analysis and a lower-level bidirectional A* search algorithm. In high-level
reachability analysis, a backward reachable tube (BRT) concerning vehicle
dynamics is computed by the HJ analysis and it intersects with a safe set to
get a safe reachable set. The safe set is defined by constraints of positive
signed distances for obstacles in the environment and computed by solving QP
optimization problems offline. For states inside the intersection set, i.e.,
the safe reachable set, the computed backward reachable tube ensures they are
reachable subjected to system dynamics and input bounds, and the safe set
guarantees they satisfy parking safety with respect to obstacles in different
shapes. For online computation, randomized states are sampled from the safe
reachable set, and used as heuristic guide points to be considered in the
bidirectional A* search. The bidirectional A* search is paralleled for each
randomized state from the safe reachable set. We show that the proposed
two-level planning algorithm is able to solve different parking scenarios
effectively and computationally fast for typical parking requests. We validate
our algorithm through simulations in large-scale randomized parking scenarios
and demonstrate it to be able to outperform other state-of-the-art parking
planning algorithms.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15191" title="Abstract">arXiv:2310.15191</a> [<a href="/pdf/2310.15191" title="Download PDF">pdf</a>, <a href="/format/2310.15191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of deep and reinforcement learning to boundary control  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panthakkalakath%2C+Z+E">Zenin Easa Panthakkalakath</a>, 
<a href="/search/cs?searchtype=author&query=Kardo%C5%A1%2C+J">Juraj Kardo&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Schenk%2C+O">Olaf Schenk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">The boundary control problem is a non-convex optimization and control problem
in many scientific domains, including fluid mechanics, structural engineering,
and heat transfer optimization. The aim is to find the optimal values for the
domain boundaries such that the enclosed domain adhering to the governing
equations attains the desired state values. Traditionally, non-linear
optimization methods, such as the Interior-Point method (IPM), are used to
solve such problems.
<br />This project explores the possibilities of using deep learning and
reinforcement learning to solve boundary control problems. We adhere to the
framework of iterative optimization strategies, employing a spatial neural
network to construct well-informed initial guesses, and a spatio-temporal
neural network learns the iterative optimization algorithm using policy
gradients. Synthetic data, generated from the problems formulated in the
literature, is used for training, testing and validation. The numerical
experiments indicate that the proposed method can rival the speed and accuracy
of existing solvers. In our preliminary results, the network attains costs
lower than IPOPT, a state-of-the-art non-linear IPM, in 51\% cases. The overall
number of floating point operations in the proposed method is similar to that
of IPOPT. Additionally, the informed initial guess method and the learned
momentum-like behaviour in the optimizer method are incorporated to avoid
convergence to local minima.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15195" title="Abstract">arXiv:2310.15195</a> [<a href="/pdf/2310.15195" title="Download PDF">pdf</a>, <a href="/format/2310.15195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Multi-Objective Combinatorial Optimization with Diversity  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinbiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zizhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yaoxin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yining Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+T">Te Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Most of existing neural methods for multi-objective combinatorial
optimization (MOCO) problems solely rely on decomposition, which often leads to
repetitive solutions for the respective subproblems, thus a limited Pareto set.
Beyond decomposition, we propose a novel neural heuristic with diversity
enhancement (NHDE) to produce more Pareto solutions from two perspectives. On
the one hand, to hinder duplicated solutions for different subproblems, we
propose an indicator-enhanced deep reinforcement learning method to guide the
model, and design a heterogeneous graph attention mechanism to capture the
relations between the instance graph and the Pareto front graph. On the other
hand, to excavate more solutions in the neighborhood of each subproblem, we
present a multiple Pareto optima strategy to sample and preserve desirable
solutions. Experimental results on classic MOCO problems show that our NHDE is
able to generate a Pareto front with higher diversity, thereby achieving
superior overall performance. Moreover, our NHDE is generic and can be applied
to different neural methods for MOCO.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15196" title="Abstract">arXiv:2310.15196</a> [<a href="/pdf/2310.15196" title="Download PDF">pdf</a>, <a href="/format/2310.15196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Meta Neural Heuristic for Multi-Objective Combinatorial  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinbiao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zizhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+T">Te Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siyuan Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, neural heuristics based on deep reinforcement learning have
exhibited promise in solving multi-objective combinatorial optimization
problems (MOCOPs). However, they are still struggling to achieve high learning
efficiency and solution quality. To tackle this issue, we propose an efficient
meta neural heuristic (EMNH), in which a meta-model is first trained and then
fine-tuned with a few steps to solve corresponding single-objective
subproblems. Specifically, for the training process, a (partial)
architecture-shared multi-task model is leveraged to achieve parallel learning
for the meta-model, so as to speed up the training; meanwhile, a scaled
symmetric sampling method with respect to the weight vectors is designed to
stabilize the training. For the fine-tuning process, an efficient hierarchical
method is proposed to systematically tackle all the subproblems. Experimental
results on the multi-objective traveling salesman problem (MOTSP),
multi-objective capacitated vehicle routing problem (MOCVRP), and
multi-objective knapsack problem (MOKP) show that, EMNH is able to outperform
the state-of-the-art neural heuristics in terms of solution quality and
learning efficiency, and yield competitive solutions to the strong traditional
heuristics while consuming much shorter time.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15197" title="Abstract">arXiv:2310.15197</a> [<a href="/pdf/2310.15197" title="Download PDF">pdf</a>, <a href="/ps/2310.15197" title="Download PostScript">ps</a>, <a href="/format/2310.15197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can strong structural encoding reduce the importance of Message Passing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eijkelboom%2C+F">Floor Eijkelboom</a> (1), 
<a href="/search/cs?searchtype=author&query=Bekkers%2C+E">Erik Bekkers</a> (1), 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a> (2), 
<a href="/search/cs?searchtype=author&query=Di+Giovanni%2C+F">Francesco Di Giovanni</a> (3) ((1) University of Amsterdam, (2) University of Oxford, (3) University of Cambridge)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of 2nd Annual Workshop on Topology, Algebra, and
  Geometry in Machine Learning (TAG-ML), PMLR 221:278-288, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The most prevalent class of neural networks operating on graphs are message
passing neural networks (MPNNs), in which the representation of a node is
updated iteratively by aggregating information in the 1-hop neighborhood. Since
this paradigm for computing node embeddings may prevent the model from learning
coarse topological structures, the initial features are often augmented with
structural information of the graph, typically in the form of Laplacian
eigenvectors or Random Walk transition probabilities. In this work, we explore
the contribution of message passing when strong structural encodings are
provided. We introduce a novel way of modeling the interaction between feature
and structural information based on their tensor product rather than the
standard concatenation. The choice of interaction is compared in common
scenarios and in settings where the capacity of the message-passing layer is
severely reduced and ultimately the message-passing phase is removed
altogether. Our results indicate that using tensor-based encodings is always at
least on par with the concatenation-based encoding and that it makes the model
much more robust when the message passing layers are removed, on some tasks
incurring almost no drop in performance. This suggests that the importance of
message passing is limited when the model can construct strong structural
encodings.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15200" title="Abstract">arXiv:2310.15200</a> [<a href="/pdf/2310.15200" title="Download PDF">pdf</a>, <a href="/format/2310.15200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inject Semantic Concepts into Image Tagging for Open-Set Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yi-Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Youcai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+W">Weiwei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+R">Rui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuejie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yanchun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Homepage: <a href="https://github.com/xinyu1205/recognize-anything">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we introduce the Recognize Anything Plus Model~(RAM++), a
fundamental image recognition model with strong open-set recognition
capabilities, by injecting semantic concepts into image tagging training
framework. Previous approaches are either image tagging models constrained by
limited semantics, or vision-language models with shallow interaction for
suboptimal performance in multi-tag recognition. In contrast, RAM++ integrates
image-text alignment and image-tagging within a unified fine-grained
interaction framework based on image-tags-text triplets. This design enables
RAM++ not only excel in identifying predefined categories, but also
significantly augment the recognition ability in open-set categories. Moreover,
RAM++ employs large language models~(LLMs) to generate diverse visual tag
descriptions, pioneering the integration of LLM's knowledge into image tagging
training. This approach empowers RAM++ to integrate visual description concepts
for open-set recognition during inference. Evaluations on comprehensive image
recognition benchmarks demonstrate RAM++ exceeds existing state-of-the-art
(SOTA) fundamental image recognition models on most aspects. Specifically, for
predefined common-used tag categories, RAM++ showcases 10.2 mAP and 15.4 mAP
enhancements over CLIP on OpenImages and ImageNet. For open-set categories
beyond predefined, RAM++ records improvements of 5 mAP and 6.4 mAP over CLIP
and RAM respectively on OpenImages. For diverse human-object interaction
phrases, RAM++ achieves 7.8 mAP and 4.7 mAP improvements on the HICO benchmark.
Code, datasets and pre-trained models are available at
\url{https://github.com/xinyu1205/recognize-anything}.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15204" title="Abstract">arXiv:2310.15204</a> [<a href="/pdf/2310.15204" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mid-Long Term Daily Electricity Consumption Forecasting Based on  Piecewise Linear Regression and Dilated Causal CNN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhou Lan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Ben Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+D">Danhuang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Key words: Daily electricity consumption forecasting; time series decomposition; piecewise linear regression; Dilated Causal CNN
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Daily electricity consumption forecasting is a classical problem. Existing
forecasting algorithms tend to have decreased accuracy on special dates like
holidays. This study decomposes the daily electricity consumption series into
three components: trend, seasonal, and residual, and constructs a two-stage
prediction method using piecewise linear regression as a filter and Dilated
Causal CNN as a predictor. The specific steps involve setting breakpoints on
the time axis and fitting the piecewise linear regression model with one-hot
encoded information such as month, weekday, and holidays. For the challenging
prediction of the Spring Festival, distance is introduced as a variable using a
third-degree polynomial form in the model. The residual sequence obtained in
the previous step is modeled using Dilated Causal CNN, and the final prediction
of daily electricity consumption is the sum of the two-stage predictions.
Experimental results demonstrate that this method achieves higher accuracy
compared to existing approaches.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15205" title="Abstract">arXiv:2310.15205</a> [<a href="/pdf/2310.15205" title="Download PDF">pdf</a>, <a href="/format/2310.15205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple  Experts Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiushi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zefei Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianyin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhongtian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bingxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiarong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 13 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose Multiple Experts Fine-tuning Framework to build a financial large
language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by
endowing them with multi-turn question answering abilities, domain text
processing capabilities, mathematical computation skills, and
retrieval-enhanced generation capabilities. We build a financial
instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of
four categories (consulting, NLP tasks, computing and retrieval-augmented
generation). Evaluations conducted on multiple benchmarks demonstrate that our
model performs better than baseline models in various financial scenarios.
Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15213" title="Abstract">arXiv:2310.15213</a> [<a href="/pdf/2310.15213" title="Download PDF">pdf</a>, <a href="/format/2310.15213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Function Vectors in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Todd%2C+E">Eric Todd</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M+L">Millicent L. Li</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A+S">Arnab Sen Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+A">Aaron Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C. Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 25 figures, 20 tables, Code and data at <a href="https://functions.baulab.info">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We report the presence of a simple neural mechanism that represents an
input-output function as a vector within autoregressive transformer language
models (LMs). Using causal mediation analysis on a diverse range of
in-context-learning (ICL) tasks, we find that a small number attention heads
transport a compact representation of the demonstrated task, which we call a
function vector (FV). FVs are robust to changes in context, i.e., they trigger
execution of the task on inputs such as zero-shot and natural text settings
that do not resemble the ICL contexts from which they are collected. We test
FVs across a range of tasks, models, and layers and find strong causal effects
across settings in middle layers. We investigate the internal structure of FVs
and find while that they often contain information that encodes the output
space of the function, this information alone is not sufficient to reconstruct
an FV. Finally, we test semantic vector composition in FVs, and find that to
some extent they can be summed to create vectors that trigger new complex
tasks. Taken together, our findings suggest that LLMs contain internal
abstractions of general-purpose functions that can be invoked in a variety of
contexts.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15239" title="Abstract">arXiv:2310.15239</a> [<a href="/pdf/2310.15239" title="Download PDF">pdf</a>, <a href="/format/2310.15239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ismayilzada%2C+M">Mete Ismayilzada</a>, 
<a href="/search/cs?searchtype=author&query=Paul%2C+D">Debjit Paul</a>, 
<a href="/search/cs?searchtype=author&query=Montariol%2C+S">Syrielle Montariol</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, camera-ready for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent efforts in natural language processing (NLP) commonsense reasoning
research have yielded a considerable number of new datasets and benchmarks.
However, most of these datasets formulate commonsense reasoning challenges in
artificial scenarios that are not reflective of the tasks which real-world NLP
systems are designed to solve. In this work, we present CRoW, a
manually-curated, multi-task benchmark that evaluates the ability of models to
apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is
constructed using a multi-stage data collection pipeline that rewrites examples
from existing datasets using commonsense-violating perturbations. We use CRoW
to study how NLP systems perform across different dimensions of commonsense
knowledge, such as physical, temporal, and social reasoning. We find a
significant performance gap when NLP systems are evaluated on CRoW compared to
humans, showcasing that commonsense reasoning is far from being solved in
real-world task settings. We make our dataset and leaderboard available to the
research community at https://github.com/mismayil/crow.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15247" title="Abstract">arXiv:2310.15247</a> [<a href="/pdf/2310.15247" title="Download PDF">pdf</a>, <a href="/format/2310.15247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Comunit%C3%A0%2C+M">Marco Comunit&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Gramaccioni%2C+R+F">Riccardo F. Gramaccioni</a>, 
<a href="/search/cs?searchtype=author&query=Postolache%2C+E">Emilian Postolache</a>, 
<a href="/search/cs?searchtype=author&query=Rodol%C3%A0%2C+E">Emanuele Rodol&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Comminiello%2C+D">Danilo Comminiello</a>, 
<a href="/search/cs?searchtype=author&query=Reiss%2C+J+D">Joshua D. Reiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Sound design involves creatively selecting, recording, and editing sound
effects for various media like cinema, video games, and virtual/augmented
reality. One of the most time-consuming steps when designing sound is
synchronizing audio with video. In some cases, environmental recordings from
video shoots are available, which can aid in the process. However, in video
games and animations, no reference audio exists, requiring manual annotation of
event timings from the video. We propose a system to extract repetitive actions
onsets from a video, which are then used - in conjunction with audio or textual
embeddings - to condition a diffusion model trained to generate a new
synchronized sound effects audio track. In this way, we leave complete creative
control to the sound designer while removing the burden of synchronization with
video. Furthermore, editing the onset track or changing the conditioning
embedding requires much less effort than editing the audio track itself,
simplifying the sonification process. We provide sound examples, source code,
and pretrained models to faciliate reproducibility
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15258" title="Abstract">arXiv:2310.15258</a> [<a href="/pdf/2310.15258" title="Download PDF">pdf</a>, <a href="/format/2310.15258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking the Language Barrier: Improving Cross-Lingual Reasoning with  Structured Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foroutan%2C+N">Negar Foroutan</a>, 
<a href="/search/cs?searchtype=author&query=Banaei%2C+M">Mohammadreza Banaei</a>, 
<a href="/search/cs?searchtype=author&query=Aberer%2C+K">Karl Aberer</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 - Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this work, we study whether multilingual language models (MultiLMs) can
transfer logical reasoning abilities to other languages when they are
fine-tuned for reasoning in a different language. We evaluate the cross-lingual
reasoning abilities of MultiLMs in two schemes: (1) where the language of the
context and the question remain the same in the new languages that are tested
(i.e., the reasoning is still monolingual, but the model must transfer the
learned reasoning ability across languages), and (2) where the language of the
context and the question is different (which we term code-switched reasoning).
On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate
that although MultiLMs can transfer reasoning ability across languages in a
monolingual setting, they struggle to transfer reasoning abilities in a
code-switched setting. Following this observation, we propose a novel attention
mechanism that uses a dedicated set of parameters to encourage cross-lingual
attention in code-switched sequences, which improves the reasoning performance
by up to 14% and 4% on the RuleTaker and LeapOfThought datasets, respectively.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15259" title="Abstract">arXiv:2310.15259</a> [<a href="/pdf/2310.15259" title="Download PDF">pdf</a>, <a href="/format/2310.15259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reference Free Domain Adaptation for Translation of Noisy Questions with  Question Specific Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gain%2C+B">Baban Gain</a>, 
<a href="/search/cs?searchtype=author&query=Appicharla%2C+R">Ramakrishna Appicharla</a>, 
<a href="/search/cs?searchtype=author&query=Chennabasavaraj%2C+S">Soumya Chennabasavaraj</a>, 
<a href="/search/cs?searchtype=author&query=Garera%2C+N">Nikesh Garera</a>, 
<a href="/search/cs?searchtype=author&query=Ekbal%2C+A">Asif Ekbal</a>, 
<a href="/search/cs?searchtype=author&query=Chelliah%2C+M">Muthusamy Chelliah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at: Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Community Question-Answering (CQA) portals serve as a valuable tool for
helping users within an organization. However, making them accessible to
non-English-speaking users continues to be a challenge. Translating questions
can broaden the community's reach, benefiting individuals with similar
inquiries in various languages. Translating questions using Neural Machine
Translation (NMT) poses more challenges, especially in noisy environments,
where the grammatical correctness of the questions is not monitored. These
questions may be phrased as statements by non-native speakers, with incorrect
subject-verb order and sometimes even missing question marks. Creating a
synthetic parallel corpus from such data is also difficult due to its noisy
nature. To address this issue, we propose a training methodology that
fine-tunes the NMT system only using source-side data. Our approach balances
adequacy and fluency by utilizing a loss function that combines BERTScore and
Masked Language Model (MLM) Score. Our method surpasses the conventional
Maximum Likelihood Estimation (MLE) based fine-tuning approach, which relies on
synthetic target data, by achieving a 1.9 BLEU score improvement. Our model
exhibits robustness while we add noise to our baseline, and still achieve 1.1
BLEU improvement and large improvements on TER and BLEURT metrics. Our proposed
methodology is model-agnostic and is only necessary during the training phase.
We make the codes and datasets publicly available at
\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DomainAdapt} for
facilitating further research.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15261" title="Abstract">arXiv:2310.15261</a> [<a href="/pdf/2310.15261" title="Download PDF">pdf</a>, <a href="/ps/2310.15261" title="Download PostScript">ps</a>, <a href="/format/2310.15261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modality Dropout for Multimodal Device Directed Speech Detection using  Verbal and Non-Verbal Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishna%2C+G">Gautam Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Dharur%2C+S">Sameer Dharur</a>, 
<a href="/search/cs?searchtype=author&query=Rudovic%2C+O">Oggi Rudovic</a>, 
<a href="/search/cs?searchtype=author&query=Dighe%2C+P">Pranay Dighe</a>, 
<a href="/search/cs?searchtype=author&query=Adya%2C+S">Saurabh Adya</a>, 
<a href="/search/cs?searchtype=author&query=Abdelaziz%2C+A+H">Ahmed Hussen Abdelaziz</a>, 
<a href="/search/cs?searchtype=author&query=Tewfik%2C+A+H">Ahmed H Tewfik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Device-directed speech detection (DDSD) is the binary classification task of
distinguishing between queries directed at a voice assistant versus side
conversation or background speech. State-of-the-art DDSD systems use verbal
cues, e.g acoustic, text and/or automatic speech recognition system (ASR)
features, to classify speech as device-directed or otherwise, and often have to
contend with one or more of these modalities being unavailable when deployed in
real-world settings. In this paper, we investigate fusion schemes for DDSD
systems that can be made more robust to missing modalities. Concurrently, we
study the use of non-verbal cues, specifically prosody features, in addition to
verbal cues for DDSD. We present different approaches to combine scores and
embeddings from prosody with the corresponding verbal cues, finding that
prosody improves DDSD performance by upto 8.5% in terms of false acceptance
rate (FA) at a given fixed operating point via non-linear intermediate fusion,
while our use of modality dropout techniques improves the performance of these
models by 7.4% in terms of FA when evaluated with missing modalities during
inference time.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15262" title="Abstract">arXiv:2310.15262</a> [<a href="/pdf/2310.15262" title="Download PDF">pdf</a>, <a href="/format/2310.15262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation Techniques for Machine Translation of Code-Switched  Texts: A Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamed%2C+I">Injy Hamed</a>, 
<a href="/search/cs?searchtype=author&query=Habash%2C+N">Nizar Habash</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+N+T">Ngoc Thang Vu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Code-switching (CSW) text generation has been receiving increasing attention
as a solution to address data scarcity. In light of this growing interest, we
need more comprehensive studies comparing different augmentation approaches. In
this work, we compare three popular approaches: lexical replacements,
linguistic theories, and back-translation (BT), in the context of Egyptian
Arabic-English CSW. We assess the effectiveness of the approaches on machine
translation and the quality of augmentations through human evaluation. We show
that BT and CSW predictive-based lexical replacement, being trained on CSW
parallel data, perform best on both tasks. Linguistic theories and random
lexical replacement prove to be effective in the lack of CSW parallel data,
where both approaches achieve similar results.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15264" title="Abstract">arXiv:2310.15264</a> [<a href="/pdf/2310.15264" title="Download PDF">pdf</a>, <a href="/format/2310.15264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Possibilities &amp; Impossibilities of AI-generated Text Detection:  A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosal%2C+S+S">Soumya Suvra Ghosal</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Geiping%2C+J">Jonas Geiping</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have revolutionized the domain of natural
language processing (NLP) with remarkable capabilities of generating human-like
text responses. However, despite these advancements, several works in the
existing literature have raised serious concerns about the potential misuse of
LLMs such as spreading misinformation, generating fake news, plagiarism in
academia, and contaminating the web. To address these concerns, a consensus
among the research community is to develop algorithmic solutions to detect
AI-generated text. The basic idea is that whenever we can tell if the given
text is either written by a human or an AI, we can utilize this information to
address the above-mentioned concerns. To that end, a plethora of detection
frameworks have been proposed, highlighting the possibilities of AI-generated
text detection. But in parallel to the development of detection frameworks,
researchers have also concentrated on designing strategies to elude detection,
i.e., focusing on the impossibilities of AI-generated text detection. This is a
crucial step in order to make sure the detection frameworks are robust enough
and it is not too easy to fool a detector. Despite the huge interest and the
flurry of research in this domain, the community currently lacks a
comprehensive analysis of recent developments. In this survey, we aim to
provide a concise categorization and overview of current work encompassing both
the prospects and the limitations of AI-generated text detection. To enrich the
collective knowledge, we engage in an exhaustive discussion on critical and
challenging open questions related to ongoing research on AI-generated text
detection.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15269" title="Abstract">arXiv:2310.15269</a> [<a href="/pdf/2310.15269" title="Download PDF">pdf</a>, <a href="/format/2310.15269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GradSim: Gradient-Based Language Grouping for Effective Multilingual  Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Adel%2C+H">Heike Adel</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+L">Lukas Lange</a>, 
<a href="/search/cs?searchtype=author&query=Str%C3%B6tgen%2C+J">Jannik Str&#xf6;tgen</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Most languages of the world pose low-resource challenges to natural language
processing models. With multilingual training, knowledge can be shared among
languages. However, not all languages positively influence each other and it is
an open research question how to select the most suitable set of languages for
multilingual training and avoid negative interference among languages whose
characteristics or data distributions are not compatible. In this paper, we
propose GradSim, a language grouping method based on gradient similarity. Our
experiments on three diverse multilingual benchmark datasets show that it leads
to the largest performance gains compared to other similarity measures and it
is better correlated with cross-lingual model performance. As a result, we set
the new state of the art on AfriSenti, a benchmark dataset for sentiment
analysis on low-resource African languages. In our extensive analysis, we
further reveal that besides linguistic features, the topics of the datasets
play an important role for language grouping and that lower layers of
transformer models encode language-specific features while higher layers
capture task-specific information.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15274" title="Abstract">arXiv:2310.15274</a> [<a href="/pdf/2310.15274" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI  Grand Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurshan%2C+E">Eren Kurshan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Journal on Semantic Computing (2024) Categories: Artificial Intelligence; AI; Artificial General Intelligence; AGI; System Design; System Architecture
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">AI faces a trifecta of grand challenges the Energy Wall, the Alignment
Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume
unsustainable amounts of energy during model training and daily
operations.Making things worse, the amount of computation required to train
each new AI model has been doubling every 2 months since 2020, directly
translating to increases in energy consumption.The leap from AI to AGI requires
multiple functional subsystems operating in a balanced manner, which requires a
system architecture. However, the current approach to artificial intelligence
lacks system design; even though system characteristics play a key role in the
human brain from the way it processes information to how it makes decisions.
Similarly, current alignment and AI ethics approaches largely ignore system
design, yet studies show that the brains system architecture plays a critical
role in healthy moral decisions.In this paper, we argue that system design is
critically important in overcoming all three grand challenges. We posit that
system design is the missing piece in overcoming the grand challenges.We
present a Systematic AI Approach for AGI that utilizes system design principles
for AGI, while providing ways to overcome the energy wall and the alignment
challenges.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15275" title="Abstract">arXiv:2310.15275</a> [<a href="/pdf/2310.15275" title="Download PDF">pdf</a>, <a href="/format/2310.15275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triple Simplex Matrix Completion for Expense Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Cheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Glass%2C+L">Lucas Glass</a>, 
<a href="/search/cs?searchtype=author&query=Sidiropoulos%2C+N">Nikos Sidiropoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Forecasting project expenses is a crucial step for businesses to avoid budget
overruns and project failures. Traditionally, this has been done by financial
analysts or data science techniques such as time-series analysis. However,
these approaches can be uncertain and produce results that differ from the
planned budget, especially at the start of a project with limited data points.
This paper proposes a constrained non-negative matrix completion model that
predicts expenses by learning the likelihood of the project correlating with
certain expense patterns in the latent space. The model is constrained on three
probability simplexes, two of which are on the factor matrices and the third on
the missing entries. Additionally, the predicted expense values are guaranteed
to meet the budget constraint without the need of post-processing. An inexact
alternating optimization algorithm is developed to solve the associated
optimization problem and is proven to converge to a stationary point. Results
from two real datasets demonstrate the effectiveness of the proposed method in
comparison to state-of-the-art algorithms.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15276" title="Abstract">arXiv:2310.15276</a> [<a href="/pdf/2310.15276" title="Download PDF">pdf</a>, <a href="/format/2310.15276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Butoi%2C+A">Alexandra Butoi</a>, 
<a href="/search/cs?searchtype=author&query=Vieira%2C+T">Tim Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+D">David Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 9 figures. Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">The class of tree-adjoining languages can be characterized by various
two-level formalisms, consisting of a context-free grammar (CFG) or pushdown
automaton (PDA) controlling another CFG or PDA. These four formalisms are
equivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG),
pushdown-adjoining automata (PAA), and embedded pushdown automata (EPDA). We
define semiring-weighted versions of the above two-level formalisms, and we
design new algorithms for computing their stringsums (the weight of all
derivations of a string) and allsums (the weight of all derivations). From
these, we also immediately obtain stringsum and allsum algorithms for TAG, LIG,
PAA, and EPDA. For LIG, our algorithm is more time-efficient by a factor of
$\mathcal{O}(n|\mathcal{N}|)$ (where $n$ is the string length and
$|\mathcal{N}|$ is the size of the nonterminal set) and more space-efficient by
a factor of $\mathcal{O}(|\Gamma|)$ (where $|\Gamma|$ is the size of the stack
alphabet) than the algorithm of Vijay-Shanker and Weir (1989). For EPDA, our
algorithm is both more space-efficient and time-efficient than the algorithm of
Alonso et al. (2001) by factors of $\mathcal{O}(|\Gamma|^2)$ and
$\mathcal{O}(|\Gamma|^3)$, respectively. Finally, we give the first PAA
stringsum and allsum algorithms.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15285" title="Abstract">arXiv:2310.15285</a> [<a href="/pdf/2310.15285" title="Download PDF">pdf</a>, <a href="/format/2310.15285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Dimensionality of Sentence Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Learning sentence embeddings is a fundamental problem in natural language
processing. While existing research primarily focuses on enhancing the quality
of sentence embeddings, the exploration of sentence embedding dimensions is
limited. Here we present a comprehensive and empirical analysis of the
dimensionality of sentence embeddings. First, we demonstrate that the optimal
dimension of sentence embeddings is usually smaller than the default value.
Subsequently, to compress the dimension of sentence embeddings with minimum
performance degradation, we identify two components contributing to the overall
performance loss: the encoder's performance loss and the pooler's performance
loss. Therefore, we propose a two-step training method for sentence
representation learning models, wherein the encoder and the pooler are
optimized separately to mitigate the overall performance loss in low-dimension
scenarios. Experimental results on seven STS tasks and seven sentence
classification tasks demonstrate that our method significantly improves the
performance of low-dimensional sentence embeddings.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15288" title="Abstract">arXiv:2310.15288</a> [<a href="/pdf/2310.15288" title="Download PDF">pdf</a>, <a href="/format/2310.15288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active teacher selection for reinforcement learning from human feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freedman%2C+R">Rachel Freedman</a>, 
<a href="/search/cs?searchtype=author&query=Svegliato%2C+J">Justin Svegliato</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+K">Kyle Wray</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+S">Stuart Russell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) enables machine learning
systems to learn objectives from human feedback. A core limitation of these
systems is their assumption that all feedback comes from a single human
teacher, despite querying a range of distinct teachers. We propose the Hidden
Utility Bandit (HUB) framework to model differences in teacher rationality,
expertise, and costliness, formalizing the problem of learning from multiple
teachers. We develop a variety of solution algorithms and apply them to two
real-world domains: paper recommendation systems and COVID-19 vaccine testing.
We find that the Active Teacher Selection (ATS) algorithm outperforms baseline
algorithms by actively selecting when and which teacher to query. The HUB
framework and ATS algorithm demonstrate the importance of leveraging
differences between teachers to learn accurate reward models, facilitating
future research on active teacher selection for robust reward modeling.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15289" title="Abstract">arXiv:2310.15289</a> [<a href="/pdf/2310.15289" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inclusion in Virtual Reality Technology: A Scoping Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yong%2C+X">Xiaofeng Yong</a>, 
<a href="/search/cs?searchtype=author&query=Arya%2C+A">Ali Arya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Despite the significant growth in virtual reality applications and research,
the notion of inclusion in virtual reality is not well studied. Inclusion
refers to the active involvement of different groups of people in the adoption,
use, design, and development of VR technology and applications. In this review,
we provide a scoping analysis of existing virtual reality research literature
about inclusion. We categorize the literature based on target group into
ability, gender, and age, followed by those that study community-based design
of VR experiences. In the latter group, we focus mainly on Indigenous Peoples
as a clearer and more important example. We also briefly review the approaches
to model and consider the role of users in technology adoption and design as a
background for inclusion studies. We identify a series of generic barriers and
research gaps and some specific ones for each group, resulting in suggested
directions for future research.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15290" title="Abstract">arXiv:2310.15290</a> [<a href="/pdf/2310.15290" title="Download PDF">pdf</a>, <a href="/format/2310.15290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Reliable Generation of EHR Time Series via Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+M">Muhang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bernie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Allan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shiyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A+R">Anru R. Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Electronic Health Records (EHRs) are rich sources of patient-level data,
including laboratory tests, medications, and diagnoses, offering valuable
resources for medical data analysis. However, concerns about privacy often
restrict access to EHRs, hindering downstream analysis. Researchers have
explored various methods for generating privacy-preserving EHR data. In this
study, we introduce a new method for generating diverse and realistic synthetic
EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We
conducted experiments on six datasets, comparing our proposed method with seven
existing methods. Our results demonstrate that our approach significantly
outperforms all existing methods in terms of data utility while requiring less
training effort. Our approach also enhances downstream medical data analysis by
providing diverse and realistic synthetic EHR data.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15294" title="Abstract">arXiv:2310.15294</a> [<a href="/pdf/2310.15294" title="Download PDF">pdf</a>, <a href="/format/2310.15294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot  Filling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuanjun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Linzhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+M">Minglai Shao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main, Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently slot filling has witnessed great development thanks to deep learning
and the availability of large-scale annotated data. However, it poses a
critical challenge to handle a novel domain whose samples are never seen during
training. The recognition performance might be greatly degraded due to severe
domain shifts. Most prior works deal with this problem in a two-pass pipeline
manner based on metric learning. In practice, these dominant pipeline models
may be limited in computational efficiency and generalization capacity because
of non-parallel inference and context-free discrete label embeddings. To this
end, we re-examine the typical metric-based methods, and propose a new adaptive
end-to-end metric learning scheme for the challenging zero-shot slot filling.
Considering simplicity, efficiency and generalizability, we present a
cascade-style joint learning framework coupled with context-aware soft label
representations and slot-level contrastive representation learning to mitigate
the data and label shift problems effectively. Extensive experiments on public
benchmarks demonstrate the superiority of the proposed approach over a series
of competitive baselines.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15296" title="Abstract">arXiv:2310.15296</a> [<a href="/pdf/2310.15296" title="Download PDF">pdf</a>, <a href="/format/2310.15296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based  LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weijie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenxiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fanyou Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sengamedu%2C+S">Srinivasan Sengamedu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures, EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the burgeoning field of natural language processing, Neural Topic Models
(NTMs) and Large Language Models (LLMs) have emerged as areas of significant
research interest. Despite this, NTMs primarily utilize contextual embeddings
from LLMs, which are not optimal for clustering or capable for topic
generation. Our study addresses this gap by introducing a novel framework named
Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME).
DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable
embeddings that could generate topics that exhibit both superior clusterability
and enhanced semantic coherence compared to existing methods. Additionally, by
exploiting the power of diffusion, our framework also provides the capability
to generate content relevant to the identified topics. This dual functionality
allows users to efficiently produce highly clustered topics and related content
simultaneously. DeTiME's potential extends to generating clustered embeddings
as well. Notably, our proposed framework proves to be efficient to train and
exhibits high adaptability, demonstrating its potential for a wide array of
applications.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15298" title="Abstract">arXiv:2310.15298</a> [<a href="/pdf/2310.15298" title="Download PDF">pdf</a>, <a href="/format/2310.15298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TaskDiff: A Similarity Metric for Task-Oriented Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhaumik%2C+A">Ankita Bhaumik</a>, 
<a href="/search/cs?searchtype=author&query=Venkateswaran%2C+P">Praveen Venkateswaran</a>, 
<a href="/search/cs?searchtype=author&query=Rizk%2C+Y">Yara Rizk</a>, 
<a href="/search/cs?searchtype=author&query=Isahagian%2C+V">Vatche Isahagian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the main conference at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The popularity of conversational digital assistants has resulted in the
availability of large amounts of conversational data which can be utilized for
improved user experience and personalized response generation. Building these
assistants using popular large language models like ChatGPT also require
additional emphasis on prompt engineering and evaluation methods. Textual
similarity metrics are a key ingredient for such analysis and evaluations.
While many similarity metrics have been proposed in the literature, they have
not proven effective for task-oriented conversations as they do not take
advantage of unique conversational features. To address this gap, we present
TaskDiff, a novel conversational similarity metric that utilizes different
dialogue components (utterances, intents, and slots) and their distributions to
compute similarity. Extensive experimental evaluation of TaskDiff on a
benchmark dataset demonstrates its superior performance and improved robustness
over other related approaches.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15299" title="Abstract">arXiv:2310.15299</a> [<a href="/pdf/2310.15299" title="Download PDF">pdf</a>, <a href="/format/2310.15299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Network with Local Converging Input (NNLCI) for Supersonic Flow  Problems with Unstructured Grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ding%2C+W">Weiming Ding</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+H">Haoxiang Huang</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+T+J">Tzu Jung Lee</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yingjie Liu</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+V">Vigor Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">In recent years, surrogate models based on deep neural networks (DNN) have
been widely used to solve partial differential equations, which were
traditionally handled by means of numerical simulations. This kind of surrogate
models, however, focuses on global interpolation of the training dataset, and
thus requires a large network structure. The process is both time consuming and
computationally costly, thereby restricting their use for high-fidelity
prediction of complex physical problems. In the present study, we develop a
neural network with local converging input (NNLCI) for high-fidelity prediction
using unstructured data. The framework utilizes the local domain of dependence
with converging coarse solutions as input, which greatly reduces computational
resource and training time. As a validation case, the NNLCI method is applied
to study inviscid supersonic flows in channels with bumps. Different bump
geometries and locations are considered to benchmark the effectiveness and
versability of the proposed approach. Detailed flow structures, including
shock-wave interactions, are examined systematically.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15301" title="Abstract">arXiv:2310.15301</a> [<a href="/pdf/2310.15301" title="Download PDF">pdf</a>, <a href="/format/2310.15301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital  Biomarkers of Alzheimer&#x27;s Disease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+X">Xiaomin Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Shuai%2C+X">Xian Shuai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Li Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Heming Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shihua Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jiang Xin</a>, 
<a href="/search/cs?searchtype=author&query=Mok%2C+H">Hazel Mok</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhenyu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D+S+F">Doris Sau Fung Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+T">Timothy Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+G">Guoliang Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Alzheimer's Disease (AD) and related dementia are a growing global health
challenge due to the aging population. In this paper, we present ADMarker, the
first end-to-end system that integrates multi-modal sensors and new federated
learning algorithms for detecting multidimensional AD digital biomarkers in
natural living environments. ADMarker features a novel three-stage multi-modal
federated learning architecture that can accurately detect digital biomarkers
in a privacy-preserving manner. Our approach collectively addresses several
major real-world challenges, such as limited data labels, data heterogeneity,
and limited computing resources. We built a compact multi-modality hardware
system and deployed it in a four-week clinical trial involving 91 elderly
participants. The results indicate that ADMarker can accurately detect a
comprehensive set of digital biomarkers with up to 93.8% accuracy and identify
early AD with an average of 88.9% accuracy. ADMarker offers a new platform that
can allow AD clinicians to characterize and track the complex correlation
between multidimensional interpretable digital biomarkers, demographic factors
of patients, and AD diagnosis in a longitudinal manner.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15302" title="Abstract">arXiv:2310.15302</a> [<a href="/pdf/2310.15302" title="Download PDF">pdf</a>, <a href="/format/2310.15302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward a Critical Toponymy Framework for Named Entity Recognition: A  Case Study of Airbnb in New York City
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brunila%2C+M">Mikael Brunila</a>, 
<a href="/search/cs?searchtype=author&query=LaViolette%2C+J">Jack LaViolette</a>, 
<a href="/search/cs?searchtype=author&query=CH-Wang%2C+S">Sky CH-Wang</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Priyanka Verma</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A9r%C3%A9%2C+C">Clara F&#xe9;r&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=McKenzie%2C+G">Grant McKenzie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (main track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Critical toponymy examines the dynamics of power, capital, and resistance
through place names and the sites to which they refer. Studies here have
traditionally focused on the semantic content of toponyms and the top-down
institutional processes that produce them. However, they have generally ignored
the ways in which toponyms are used by ordinary people in everyday discourse,
as well as the other strategies of geospatial description that accompany and
contextualize toponymic reference. Here, we develop computational methods to
measure how cultural and economic capital shape the ways in which people refer
to places, through a novel annotated dataset of 47,440 New York City Airbnb
listings from the 2010s. Building on this dataset, we introduce a new named
entity recognition (NER) model able to identify important discourse categories
integral to the characterization of place. Our findings point toward new
directions for critical toponymy and to a range of previously understudied
linguistic signals relevant to research on neighborhood status, housing and
tourism markets, and gentrification.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15308" title="Abstract">arXiv:2310.15308</a> [<a href="/pdf/2310.15308" title="Download PDF">pdf</a>, <a href="/format/2310.15308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Vasu%2C+P+K+A">Pavan Kumar Anasosalu Vasu</a>, 
<a href="/search/cs?searchtype=author&query=Faghri%2C+F">Fartash Faghri</a>, 
<a href="/search/cs?searchtype=author&query=Vemulapalli%2C+R">Raviteja Vemulapalli</a>, 
<a href="/search/cs?searchtype=author&query=Farajtabar%2C+M">Mehrdad Farajtabar</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Sachin Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Tuzel%2C+O">Oncel Tuzel</a>, 
<a href="/search/cs?searchtype=author&query=Pouransari%2C+H">Hadi Pouransari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that assimilates their
expertise. Our proposed method integrates multi-task learning, continual
learning techniques, and teacher-student distillation. This strategy entails
significantly less computational cost compared to traditional multi-task
training from scratch. Additionally, it only demands a small fraction of the
pre-training datasets that were initially used to train individual models. By
applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that
amalgamates the strengths of SAM and CLIP into a single backbone, making it apt
for edge device applications. We show that SAM-CLIP learns richer visual
representations, equipped with both localization and semantic features,
suitable for a broad range of vision tasks. SAM-CLIP obtains improved
performance on several head probing tasks when compared with SAM and CLIP. We
further show that SAM-CLIP not only retains the foundational strengths of its
precursor models but also introduces synergistic functionalities, most notably
in zero-shot semantic segmentation, where SAM-CLIP establishes new
state-of-the-art results on 5 benchmarks. It outperforms previous models that
are specifically designed for this task by a large margin, including +6.8% and
+5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15310" title="Abstract">arXiv:2310.15310</a> [<a href="/pdf/2310.15310" title="Download PDF">pdf</a>, <a href="/format/2310.15310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A direct solution to the interpolative inverse non-uniform fast Fourier  transform problem, for spectral analyses of non-equidistant time-series data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Armstrong%2C+M+S">Michael Sorochan Armstrong</a>, 
<a href="/search/math?searchtype=author&query=P%C3%A9rez-Gir%C3%B3n%2C+J+C">Jos&#xe9; Carlos P&#xe9;rez-Gir&#xf3;n</a>, 
<a href="/search/math?searchtype=author&query=Camacho%2C+J">Jos&#xe9; Camacho</a>, 
<a href="/search/math?searchtype=author&query=Zamora%2C+R">Regino Zamora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A simple least-squares optimisation enables the determination of the spectrum
for irregularly sampled data that is readily reconstructed using an adjoint
transformation of the Non-Uniform Fast Fourier Transform (NFFT). This is an
improvement upon previously reported iterative methods for such problems, and
is competitive in terms of time complexity with more recently proposed direct
NFFT inversions when considering comparable matrix pre-computation steps. The
software is highly portable, and available as a convenient Python package using
standard libraries. Given its mathematical simplicity however, it can be
conveniently implemented on any platform.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15316" title="Abstract">arXiv:2310.15316</a> [<a href="/pdf/2310.15316" title="Download PDF">pdf</a>, <a href="/format/2310.15316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Representations for Document-level Event Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Barry Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinya Du</a>, 
<a href="/search/cs?searchtype=author&query=Cardie%2C+C">Claire Cardie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The probing classifiers framework has been employed for interpreting deep
neural network models for a variety of natural language processing (NLP)
applications. Studies, however, have largely focused on sentencelevel NLP
tasks. This work is the first to apply the probing paradigm to representations
learned for document-level information extraction (IE). We designed eight
embedding probes to analyze surface, semantic, and event-understanding
capabilities relevant to document-level event extraction. We apply them to the
representations acquired by learning models from three different LLM-based
document-level IE approaches on a standard dataset. We found that trained
encoders from these models yield embeddings that can modestly improve argument
detections and labeling but only slightly enhance event-level tasks, albeit
trade-offs in information helpful for coherence and event-type prediction. We
further found that encoder models struggle with document length and
cross-sentence discourse.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15317" title="Abstract">arXiv:2310.15317</a> [<a href="/pdf/2310.15317" title="Download PDF">pdf</a>, <a href="/format/2310.15317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Potential of Large Language Models in Generating  Code-Tracing Questions for Introductory Programming Courses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+A+X">Aysa Xuemo Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R+H">Ranran Haoran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Paquette%2C+L">Luc Paquette</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">In this paper, we explore the application of large language models (LLMs) for
generating code-tracing questions in introductory programming courses. We
designed targeted prompts for GPT4, guiding it to generate code-tracing
questions based on code snippets and descriptions. We established a set of
human evaluation metrics to assess the quality of questions produced by the
model compared to those created by human experts. Our analysis provides
insights into the capabilities and potential of LLMs in generating diverse
code-tracing questions. Additionally, we present a unique dataset of human and
LLM-generated tracing questions, serving as a valuable resource for both the
education and NLP research communities. This work contributes to the ongoing
dialogue on the potential uses of LLMs in educational settings.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15318" title="Abstract">arXiv:2310.15318</a> [<a href="/pdf/2310.15318" title="Download PDF">pdf</a>, <a href="/format/2310.15318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained  Heterogeneous Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yihong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+N">Ning Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiayu Li</a>, 
<a href="/search/cs?searchtype=author&query=Mortazavi%2C+M">Masood Mortazavi</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to ACM TheWebConf 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graphs have emerged as a natural choice to represent and analyze the
intricate patterns and rich information of the Web, enabling applications such
as online page classification and social recommendation. The prevailing
"pre-train, fine-tune" paradigm has been widely adopted in graph machine
learning tasks, particularly in scenarios with limited labeled nodes. However,
this approach often exhibits a misalignment between the training objectives of
pretext tasks and those of downstream tasks. This gap can result in the
"negative transfer" problem, wherein the knowledge gained from pre-training
adversely affects performance in the downstream tasks. The surge in
prompt-based learning within Natural Language Processing (NLP) suggests the
potential of adapting a "pre-train, prompt" paradigm to graphs as an
alternative. However, existing graph prompting techniques are tailored to
homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To
bridge this gap, we propose HetGPT, a general post-training prompting framework
to improve the predictive performance of pre-trained heterogeneous graph neural
networks (HGNNs). The key is the design of a novel prompting function that
integrates a virtual class prompt and a heterogeneous feature prompt, with the
aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT
introduces a multi-view neighborhood aggregation mechanism, capturing the
complex neighborhood structure in heterogeneous graphs. Extensive experiments
on three benchmark datasets demonstrate HetGPT's capability to enhance the
performance of state-of-the-art HGNNs on semi-supervised node classification.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15319" title="Abstract">arXiv:2310.15319</a> [<a href="/pdf/2310.15319" title="Download PDF">pdf</a>, <a href="/format/2310.15319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hallucination Detection for Grounded Instruction Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingjun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate the problem of generating instructions to guide humans to
navigate in simulated residential environments. A major issue with current
models is hallucination: they generate references to actions or objects that
are inconsistent with what a human follower would perform or encounter along
the described path. We develop a model that detects these hallucinated
references by adopting a model pre-trained on a large corpus of image-text
pairs, and fine-tuning it with a contrastive loss that separates correct
instructions from instructions containing synthesized hallucinations. Our final
model outperforms several baselines, including using word probability estimated
by the instruction-generation model, and supervised models based on LSTM and
Transformer.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15320" title="Abstract">arXiv:2310.15320</a> [<a href="/pdf/2310.15320" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conflating point of interest (POI) data: A systematic review of matching  methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Kai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yingjie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yue Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R+Z">Ryan Zhenqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yunqiang Zhu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computers, Environment and Urban Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Point of interest (POI) data provide digital representations of places in the
real world, and have been increasingly used to understand human-place
interactions, support urban management, and build smart cities. Many POI
datasets have been developed, which often have different geographic coverages,
attribute focuses, and data quality. From time to time, researchers may need to
conflate two or more POI datasets in order to build a better representation of
the places in the study areas. While various POI conflation methods have been
developed, there lacks a systematic review, and consequently, it is difficult
for researchers new to POI conflation to quickly grasp and use these existing
methods. This paper fills such a gap. Following the protocol of Preferred
Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA), we conduct a
systematic review by searching through three bibliographic databases using
reproducible syntax to identify related studies. We then focus on a main step
of POI conflation, i.e., POI matching, and systematically summarize and
categorize the identified methods. Current limitations and future opportunities
are discussed afterwards. We hope that this review can provide some guidance
for researchers interested in conflating POI datasets for their research.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15321" title="Abstract">arXiv:2310.15321</a> [<a href="/pdf/2310.15321" title="Download PDF">pdf</a>, <a href="/format/2310.15321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How language of interaction affects the user perception of a robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sienkiewicz%2C+B">Barbara Sienkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Sejnova%2C+G">Gabriela Sejnova</a>, 
<a href="/search/cs?searchtype=author&query=Gajewski%2C+P">Paul Gajewski</a>, 
<a href="/search/cs?searchtype=author&query=Vavrecka%2C+M">Michal Vavrecka</a>, 
<a href="/search/cs?searchtype=author&query=Indurkhya%2C+B">Bipin Indurkhya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICSR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Spoken language is the most natural way for a human to communicate with a
robot. It may seem intuitive that a robot should communicate with users in
their native language. However, it is not clear if a user's perception of a
robot is affected by the language of interaction.
<br />We investigated this question by conducting a study with twenty-three native
Czech participants who were also fluent in English. The participants were
tasked with instructing the Pepper robot on where to place objects on a shelf.
The robot was controlled remotely using the Wizard-of-Oz technique. We
collected data through questionnaires, video recordings, and a post-experiment
feedback session. The results of our experiment show that people perceive an
English-speaking robot as more intelligent than a Czech-speaking robot (z =
18.00, p-value = 0.02). This finding highlights the influence of language on
human-robot interaction. Furthermore, we discuss the feedback obtained from the
participants via the post-experiment sessions and its implications for HRI
design.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15324" title="Abstract">arXiv:2310.15324</a> [<a href="/pdf/2310.15324" title="Download PDF">pdf</a>, <a href="/format/2310.15324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Videoprompter: an ensemble of foundational models for zero-shot video  understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousaf%2C+A">Adeel Yousaf</a>, 
<a href="/search/cs?searchtype=author&query=Naseer%2C+M">Muzammal Naseer</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Salman Khan</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Mubarak Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision-language models (VLMs) classify the query video by calculating a
similarity score between the visual features and text-based class label
representations. Recently, large language models (LLMs) have been used to
enrich the text-based class labels by enhancing the descriptiveness of the
class names. However, these improvements are restricted to the text-based
classifier only, and the query visual features are not considered. In this
paper, we propose a framework which combines pre-trained discriminative VLMs
with pre-trained generative video-to-text and text-to-text models. We introduce
two key modifications to the standard zero-shot setting. First, we propose
language-guided visual feature enhancement and employ a video-to-text model to
convert the query video to its descriptive form. The resulting descriptions
contain vital visual cues of the query video, such as what objects are present
and their spatio-temporal interactions. These descriptive cues provide
additional semantic knowledge to VLMs to enhance their zeroshot performance.
Second, we propose video-specific prompts to LLMs to generate more meaningful
descriptions to enrich class label representations. Specifically, we introduce
prompt techniques to create a Tree Hierarchy of Categories for class names,
offering a higher-level action context for additional visual cues, We
demonstrate the effectiveness of our approach in video understanding across
three different zero-shot settings: 1) video action recognition, 2)
video-to-text and textto-video retrieval, and 3) time-sensitive video tasks.
Consistent improvements across multiple benchmarks and with various VLMs
demonstrate the effectiveness of our proposed framework. Our code will be made
publicly available.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15325" title="Abstract">arXiv:2310.15325</a> [<a href="/pdf/2310.15325" title="Download PDF">pdf</a>, <a href="/format/2310.15325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LXMERT Model Compression for Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+M">Maryam Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=Mahmoudi%2C+G">Ghazaleh Mahmoudi</a>, 
<a href="/search/cs?searchtype=author&query=Kodeiri%2C+S">Sara Kodeiri</a>, 
<a href="/search/cs?searchtype=author&query=Sheikhi%2C+H">Hadi Sheikhi</a>, 
<a href="/search/cs?searchtype=author&query=Eetemadi%2C+S">Sauleh Eetemadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in The Fourth Annual West Coast NLP (WeCNLP) Summit
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large-scale pretrained models such as LXMERT are becoming popular for
learning cross-modal representations on text-image pairs for vision-language
tasks. According to the lottery ticket hypothesis, NLP and computer vision
models contain smaller subnetworks capable of being trained in isolation to
full performance. In this paper, we combine these observations to evaluate
whether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA
task. In addition, we perform a model size cost-benefit analysis by
investigating how much pruning can be done without significant loss in
accuracy. Our experiment results demonstrate that LXMERT can be effectively
pruned by 40%-60% in size with 3% loss in accuracy.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15326" title="Abstract">arXiv:2310.15326</a> [<a href="/pdf/2310.15326" title="Download PDF">pdf</a>, <a href="/format/2310.15326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specialist or Generalist? Instruction Tuning for Specific NLP Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chufan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yixuan Su</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The potential of large language models (LLMs) to simultaneously perform a
wide range of natural language processing (NLP) tasks has been the subject of
extensive research. Although instruction tuning has proven to be a
data-efficient method for transforming LLMs into such generalist models, their
performance still lags behind specialist models trained exclusively for
specific tasks. In this paper, we investigate whether incorporating
broad-coverage generalist instruction tuning can contribute to building a
specialist model. We hypothesize that its efficacy depends on task specificity
and skill requirements. Our experiments assess four target tasks with distinct
coverage levels, revealing that integrating generalist instruction tuning
consistently enhances model performance when the task coverage is broad. The
effect is particularly pronounced when the amount of task-specific training
data is limited. Further investigation into three target tasks focusing on
different capabilities demonstrates that generalist instruction tuning improves
understanding and reasoning abilities. However, for tasks requiring factual
knowledge, generalist data containing hallucinatory information may negatively
affect the model's performance. Overall, our work provides a systematic guide
for developing specialist models with general instruction tuning. Our code and
other related resources can be found at
https://github.com/DavidFanzz/Generalist_or_Specialist.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15329" title="Abstract">arXiv:2310.15329</a> [<a href="/pdf/2310.15329" title="Download PDF">pdf</a>, <a href="/format/2310.15329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Serverless Federated Learning with flwr-serverless
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Namjoshi%2C+S+V">Sanjeev V. Namjoshi</a>, 
<a href="/search/cs?searchtype=author&query=Green%2C+R">Reese Green</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Krishi Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+Z">Zhangzhang Si</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report for an open source machine learning python package
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated learning is becoming increasingly relevant and popular as we
witness a surge in data collection and storage of personally identifiable
information. Alongside these developments there have been many proposals from
governments around the world to provide more protections for individuals' data
and a heightened interest in data privacy measures. As deep learning continues
to become more relevant in new and existing domains, it is vital to develop
strategies like federated learning that can effectively train data from
different sources, such as edge devices, without compromising security and
privacy. Recently, the Flower (\texttt{Flwr}) Python package was introduced to
provide a scalable, flexible, and easy-to-use framework for implementing
federated learning. However, to date, Flower is only able to run synchronous
federated learning which can be costly and time-consuming to run because the
process is bottlenecked by client-side training jobs that are slow or fragile.
Here, we introduce \texttt{flwr-serverless}, a wrapper around the Flower
package that extends its functionality to allow for both synchronous and
asynchronous federated learning with minimal modification to Flower's design
paradigm. Furthermore, our approach to federated learning allows the process to
run without a central server, which increases the domains of application and
accessibility of its use. This paper presents the design details and usage of
this approach through a series of experiments that were conducted using public
datasets. Overall, we believe that our approach decreases the time and cost to
run federated training and provides an easier way to implement and experiment
with federated learning systems.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15333" title="Abstract">arXiv:2310.15333</a> [<a href="/pdf/2310.15333" title="Download PDF">pdf</a>, <a href="/format/2310.15333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Trustworthy and Safe Optimal Treatment Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parikh%2C+H">Harsh Parikh</a>, 
<a href="/search/cs?searchtype=author&query=Lanners%2C+Q">Quinn Lanners</a>, 
<a href="/search/cs?searchtype=author&query=Akras%2C+Z">Zade Akras</a>, 
<a href="/search/cs?searchtype=author&query=Zafar%2C+S+F">Sahar F. Zafar</a>, 
<a href="/search/cs?searchtype=author&query=Westover%2C+M+B">M. Brandon Westover</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+C">Cynthia Rudin</a>, 
<a href="/search/cs?searchtype=author&query=Volfovsky%2C+A">Alexander Volfovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Methodology (stat.ME)

</div>
<p class="mathjax">Recent statistical and reinforcement learning methods have significantly
advanced patient care strategies. However, these approaches face substantial
challenges in high-stakes contexts, including missing data, inherent
stochasticity, and the critical requirements for interpretability and patient
safety. Our work operationalizes a safe and interpretable framework to identify
optimal treatment regimes. This approach involves matching patients with
similar medical and pharmacological characteristics, allowing us to construct
an optimal policy via interpolation. We perform a comprehensive simulation
study to demonstrate the framework's ability to identify optimal policies even
in complex settings. Ultimately, we operationalize our approach to study
regimes for treating seizures in critically ill patients. Our findings strongly
support personalized treatment strategies based on a patient's medical history
and pharmacological features. Notably, we identify that reducing medication
doses for patients with mild and brief seizure episodes while adopting
aggressive treatment for patients in intensive care unit experiencing intense
seizures leads to more favorable outcomes.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15334" title="Abstract">arXiv:2310.15334</a> [<a href="/pdf/2310.15334" title="Download PDF">pdf</a>, <a href="/format/2310.15334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ADMM Training Algorithms for Residual Networks: Convergence, Complexity  and Parallel Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jintao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+W">Wenxun Xing</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We design a series of serial and parallel proximal point (gradient) ADMMs for
the fully connected residual networks (FCResNets) training problem by
introducing auxiliary variables. Convergence of the proximal point version is
proven based on a Kurdyka-Lojasiewicz (KL) property analysis framework, and we
can ensure a locally R-linear or sublinear convergence rate depending on the
different ranges of the Kurdyka-Lojasiewicz (KL) exponent, in which a necessary
auxiliary function is constructed to realize our goal. Moreover, the advantages
of the parallel implementation in terms of lower time complexity and less
(per-node) memory consumption are analyzed theoretically. To the best of our
knowledge, this is the first work analyzing the convergence, convergence rate,
time complexity and (per-node) runtime memory requirement of the ADMM applied
in the FCResNets training problem theoretically. Experiments are reported to
show the high speed, better performance, robustness and potential in the deep
network training tasks. Finally, we present the advantage and potential of our
parallel training in large-scale problems.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15337" title="Abstract">arXiv:2310.15337</a> [<a href="/pdf/2310.15337" title="Download PDF">pdf</a>, <a href="/format/2310.15337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moral Foundations of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdulhai%2C+M">Marwa Abdulhai</a>, 
<a href="/search/cs?searchtype=author&query=Serapio-Garcia%2C+G">Gregory Serapio-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Crepy%2C+C">Cl&#xe9;ment Crepy</a>, 
<a href="/search/cs?searchtype=author&query=Valter%2C+D">Daria Valter</a>, 
<a href="/search/cs?searchtype=author&query=Canny%2C+J">John Canny</a>, 
<a href="/search/cs?searchtype=author&query=Jaques%2C+N">Natasha Jaques</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">Moral foundations theory (MFT) is a psychological assessment tool that
decomposes human moral reasoning into five factors, including care/harm,
liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary
in the weight they place on these dimensions when making moral decisions, in
part due to their cultural upbringing and political ideology. As large language
models (LLMs) are trained on datasets collected from the internet, they may
reflect the biases that are present in such corpora. This paper uses MFT as a
lens to analyze whether popular LLMs have acquired a bias towards a particular
set of moral values. We analyze known LLMs and find they exhibit particular
moral foundations, and show how these relate to human moral foundations and
political affiliations. We also measure the consistency of these biases, or
whether they vary strongly depending on the context of how the model is
prompted. Finally, we show that we can adversarially select prompts that
encourage the moral to exhibit a particular set of moral foundations, and that
this can affect the model's behavior on downstream tasks. These findings help
illustrate the potential risks and unintended consequences of LLMs assuming a
particular moral stance.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15340" title="Abstract">arXiv:2310.15340</a> [<a href="/pdf/2310.15340" title="Download PDF">pdf</a>, <a href="/format/2310.15340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calculational Design of [In]Correctness Transformational Program Logics  by Abstract Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cousot%2C+P">Patrick Cousot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 62 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We study transformational program logics for correctness and incorrectness
that we extend to explicitly handle both termination and nontermination. We
show that the logics are abstract interpretations of the right image
transformer for a natural relational semantics covering both finite and
infinite executions. This understanding of logics as abstractions of a
semantics facilitates their comparisons through their respective abstractions
of the semantics (rather that the much more difficult comparison through their
formal proof systems). More importantly, the formalization provides a
calculational method for constructively designing the sound and complete formal
proof system by abstraction of the semantics. As an example, we extend Hoare
logic to cover all possible behaviors of nondeterministic programs and design a
new precondition (in)correctness logic.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15341" title="Abstract">arXiv:2310.15341</a> [<a href="/pdf/2310.15341" title="Download PDF">pdf</a>, <a href="/format/2310.15341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the monotonicity of $Q^2$ spectral element method for Laplacian on  quasi-uniform rectangular meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cross%2C+L+J">Logan J. Cross</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiangxiong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2010.07282">arXiv:2010.07282</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The monotonicity of discrete Laplacian implies discrete maximum principle,
which in general does not hold for high order schemes. The $Q^2$ spectral
element method has been proven monotone on a uniform rectangular mesh. In this
paper we prove the monotonicity of the $Q^2$ spectral element method on
quasi-uniform rectangular meshes under certain mesh constraints. In particular,
we propose a relaxed Lorenz's condition for proving monotonicity.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15342" title="Abstract">arXiv:2310.15342</a> [<a href="/pdf/2310.15342" title="Download PDF">pdf</a>, <a href="/format/2310.15342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Hybrid-grained Feature Interaction Selection for Deep Sparse  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+F">Fuyuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dugang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Weihong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiuqiang He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Deep sparse networks are widely investigated as a neural network architecture
for prediction tasks with high-dimensional sparse features, with which feature
interaction selection is a critical component. While previous methods primarily
focus on how to search feature interaction in a coarse-grained space, less
attention has been given to a finer granularity. In this work, we introduce a
hybrid-grained feature interaction selection approach that targets both feature
field and feature value for deep sparse networks. To explore such expansive
space, we propose a decomposed space which is calculated on the fly. We then
develop a selection algorithm called OptFeature, which efficiently selects the
feature interaction from both the feature field and the feature value
simultaneously. Results from experiments on three large real-world benchmark
datasets demonstrate that OptFeature performs well in terms of accuracy and
efficiency. Additional studies support the feasibility of our method.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15343" title="Abstract">arXiv:2310.15343</a> [<a href="/pdf/2310.15343" title="Download PDF">pdf</a>, <a href="/format/2310.15343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Burgers&#x27; pinns with implicit euler transfer learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biesek%2C+V">Vit&#xf3;ria Biesek</a>, 
<a href="/search/cs?searchtype=author&query=de+Almeida+Konzen%2C+P+H">Pedro Henrique de Almeida Konzen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, conference paper XXVI ENMC/XIV ECTM 2023, Nova Friburgo, Brazil
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The Burgers equation is a well-established test case in the computational
modeling of several phenomena such as fluid dynamics, gas dynamics, shock
theory, cosmology, and others. In this work, we present the application of
Physics-Informed Neural Networks (PINNs) with an implicit Euler transfer
learning approach to solve the Burgers equation. The proposed approach consists
in seeking a time-discrete solution by a sequence of Artificial Neural Networks
(ANNs). At each time step, the previous ANN transfers its knowledge to the next
network model, which learns the current time solution by minimizing a loss
function based on the implicit Euler approximation of the Burgers equation. The
approach is tested for two benchmark problems: the first with an exact solution
and the other with an alternative analytical solution. In comparison to the
usual PINN models, the proposed approach has the advantage of requiring smaller
neural network architectures with similar accurate results and potentially
decreasing computational costs.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15351" title="Abstract">arXiv:2310.15351</a> [<a href="/pdf/2310.15351" title="Download PDF">pdf</a>, <a href="/format/2310.15351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Exploration in Bayesian Optimization: Order-Optimal Regret and  Computational Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salgia%2C+S">Sudeep Salgia</a>, 
<a href="/search/cs?searchtype=author&query=Vakili%2C+S">Sattar Vakili</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider Bayesian optimization using Gaussian Process models, also
referred to as kernel-based bandit optimization. We study the methodology of
exploring the domain using random samples drawn from a distribution. We show
that this random exploration approach achieves the optimal error rates. Our
analysis is based on novel concentration bounds in an infinite dimensional
Hilbert space established in this work, which may be of independent interest.
We further develop an algorithm based on random exploration with domain
shrinking and establish its order-optimal regret guarantees under both
noise-free and noisy settings. In the noise-free setting, our analysis closes
the existing gap in regret performance and thereby resolves a COLT open
problem. The proposed algorithm also enjoys a computational advantage over
prevailing methods due to the random exploration that obviates the expensive
optimization of a non-convex acquisition function for choosing the query points
at each iteration.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15355" title="Abstract">arXiv:2310.15355</a> [<a href="/pdf/2310.15355" title="Download PDF">pdf</a>, <a href="/format/2310.15355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual,  Intensional, and Extensional Learning for Faithful Natural Language  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouyamourn%2C+A">Adam Bouyamourn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We show that LLMs hallucinate because their output is not constrained to be
synonymous with claims for which they have evidence: a condition that we call
evidential closure. Information about the truth or falsity of sentences is not
statistically identified in the standard neural probabilistic language model
setup, and so cannot be conditioned on to generate new strings. We then show
how to constrain LLMs to produce output that does satisfy evidential closure. A
multimodal LLM must learn about the external world (perceptual learning); it
must learn a mapping from strings to states of the world (extensional
learning); and, to achieve fluency when generalizing beyond a body of evidence,
it must learn mappings from strings to their synonyms (intensional learning).
The output of a unimodal LLM must be synonymous with strings in a validated
evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,
that yields faithful output from an LLM by rejecting output that is not
synonymous with claims for which the LLM has evidence.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15356" title="Abstract">arXiv:2310.15356</a> [<a href="/pdf/2310.15356" title="Download PDF">pdf</a>, <a href="/format/2310.15356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lie Group Variational Collision Integrators for a Class of Hybrid  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tran%2C+K">Khoa Tran</a>, 
<a href="/search/math?searchtype=author&query=Leok%2C+M">Melvin Leok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The problem of 3-dimensional, convex rigid-body collision over a plane is
fully investigated; this includes bodies with sharp corners that is resolved
without the need for nonsmooth convex analysis of tangent and normal cones. In
particular, using nonsmooth Lagrangian mechanics, the equations of motion and
jump equations are derived, which are largely dependent on the collision
detection function. Following the variational approach, a Lie group variational
collision integrator (LGVCI) is systematically derived that is symplectic,
momentum-preserving, and has excellent long-time, near energy conservation.
Furthermore, systems with corner impacts are resolved adeptly using
$\epsilon$-rounding on the sign distance function (SDF) of the body. Extensive
numerical experiments are conducted to demonstrate the conservation properties
of the LGVCI.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15358" title="Abstract">arXiv:2310.15358</a> [<a href="/pdf/2310.15358" title="Download PDF">pdf</a>, <a href="/format/2310.15358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Fair Representations with High-Confidence Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuhong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hoag%2C+A">Austin Hoag</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+P+S">Philip S. Thomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
<p class="mathjax">Representation learning is increasingly employed to generate representations
that are predictive across multiple downstream tasks. The development of
representation learning algorithms that provide strong fairness guarantees is
thus important because it can prevent unfairness towards disadvantaged groups
for all downstream prediction tasks. To prevent unfairness towards
disadvantaged groups in all downstream tasks, it is crucial to provide
representation learning algorithms that provide fairness guarantees. In this
paper, we formally define the problem of learning representations that are fair
with high confidence. We then introduce the Fair Representation learning with
high-confidence Guarantees (FRG) framework, which provides high-confidence
guarantees for limiting unfairness across all downstream models and tasks, with
user-defined upper bounds. After proving that FRG ensures fairness for all
downstream models and tasks with high probability, we present empirical
evaluations that demonstrate FRG's effectiveness at upper bounding unfairness
for multiple downstream models and tasks.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15360" title="Abstract">arXiv:2310.15360</a> [<a href="/pdf/2310.15360" title="Download PDF">pdf</a>, <a href="/format/2310.15360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithm for Invalidation of Cached Results of Queries to a Single  Table
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%81opusza%C5%84ski%2C+J">Jakub &#x141;opusza&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> my rejected submission to SIGMOD 2013
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">One of the most popular setups for a back-end of a high performance website
consists of a relational database and a cache which stores results of performed
queries. Several application frameworks support caching of queries made to the
database, but few of them handle cache invalidation correctly, resorting to
simpler solutions such as short TTL values, or flushing the whole cache after
any write to the database. In this paper a simple, correct, efficient and
tested in real world application solution is presented, which allows for
infinite TTL, and very fine grained cache invalidation. Algorithm is proven to
be correct in a concurrent environment, both theoretically and in practice.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15361" title="Abstract">arXiv:2310.15361</a> [<a href="/pdf/2310.15361" title="Download PDF">pdf</a>, <a href="/format/2310.15361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curved Space-Filling Tiles Using Voronoi Decomposition with Line, and  Curve Segments Closed Under Wallpaper Symmetries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panchal%2C+H">Haard Panchal</a>, 
<a href="/search/cs?searchtype=author&query=Akleman%2C+E">Ergun Akleman</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+V">Vinayak Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Yildiz%2C+T+T">Tolga Talha Yildiz</a>, 
<a href="/search/cs?searchtype=author&query=Grover%2C+V">Varda Grover</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">In this paper, we present a new approach to obtain symmetric tiles with
curved edges. Our approach is based on using higher-order Voronoi sites that
are closed under wallpaper symmetries. The resulting Voronoi tessellations
provide us with symmetric tiles with curved edges. We have developed a web
application that provides real-time tile design. Our application can be found
at https://voronoi.viz.tamu.edu. One of our key findings in this paper is that
not all symmetry operations are useful for creating curved tiles. In
particular, all symmetries that use mirror operation produce straight lines
that are useless for creating new tiles. This result is interesting because it
suggests that we need to avoid mirror transformations to produce unusual
space-filling tiles in 2D and 3D using Voronoi tessellations.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15363" title="Abstract">arXiv:2310.15363</a> [<a href="/pdf/2310.15363" title="Download PDF">pdf</a>, <a href="/format/2310.15363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimal Algorithm for Higher-Order Voronoi Diagrams in the Plane: The  Usefulness of Nondeterminism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+M">Timothy M. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pingan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D+W">Da Wei Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SODA 2024. 16 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">We present the first optimal randomized algorithm for constructing the
order-$k$ Voronoi diagram of $n$ points in two dimensions. The expected running
time is $O(n\log n + nk)$, which improves the previous, two-decades-old result
of Ramos (SoCG'99) by a $2^{O(\log^*k)}$ factor. To obtain our result, we (i)
use a recent decision-tree technique of Chan and Zheng (SODA'22) in combination
with Ramos's cutting construction, to reduce the problem to verifying an
order-$k$ Voronoi diagram, and (ii) solve the verification problem by a new
divide-and-conquer algorithm using planar-graph separators.
<br />We also describe a deterministic algorithm for constructing the $k$-level of
$n$ lines in two dimensions in $O(n\log n + nk^{1/3})$ time, and constructing
the $k$-level of $n$ planes in three dimensions in $O(n\log n + nk^{3/2})$
time. These time bounds (ignoring the $n\log n$ term) match the current best
upper bounds on the combinatorial complexity of the $k$-level. Previously, the
same time bound in two dimensions was obtained by Chan (1999) but with
randomization.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15364" title="Abstract">arXiv:2310.15364</a> [<a href="/pdf/2310.15364" title="Download PDF">pdf</a>, <a href="/format/2310.15364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filter-adapted spatiotemporal sampling for real-time rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Donnelly%2C+W">William Donnelly</a>, 
<a href="/search/cs?searchtype=author&query=Wolfe%2C+A">Alan Wolfe</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCtepage%2C+J">Judith B&#xfc;tepage</a>, 
<a href="/search/cs?searchtype=author&query=Vald%C3%A9s%2C+J">Jon Vald&#xe9;s</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Stochastic sampling techniques are ubiquitous in real-time rendering, where
performance constraints force the use of low sample counts, leading to noisy
intermediate results. To remove this noise, the post-processing step of
temporal and spatial denoising is an integral part of the real-time graphics
pipeline. The main insight presented in this paper is that we can optimize the
samples used in stochastic sampling such that the post-processing error is
minimized. The core of our method is an analytical loss function which measures
post-filtering error for a class of integrands - multidimensional Heaviside
functions. These integrands are an approximation of the discontinuous functions
commonly found in rendering. Our analysis applies to arbitrary spatial and
spatiotemporal filters, scalar and vector sample values, and uniform and
non-uniform probability distributions. We show that the spectrum of Monte Carlo
noise resulting from our sampling method is adapted to the shape of the filter,
resulting in less noisy final images. We demonstrate improvements over
state-of-the-art sampling methods in three representative rendering tasks:
ambient occlusion, volumetric ray-marching, and color image dithering. Common
use noise textures, and noise generation code is available at
https://github.com/electronicarts/fastnoise.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15368" title="Abstract">arXiv:2310.15368</a> [<a href="/pdf/2310.15368" title="Download PDF">pdf</a>, <a href="/format/2310.15368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Integrated Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barkan%2C+O">Oren Barkan</a>, 
<a href="/search/cs?searchtype=author&query=Elisha%2C+Y">Yehonathan Elisha</a>, 
<a href="/search/cs?searchtype=author&query=Weill%2C+J">Jonathan Weill</a>, 
<a href="/search/cs?searchtype=author&query=Asher%2C+Y">Yuval Asher</a>, 
<a href="/search/cs?searchtype=author&query=Eshel%2C+A">Amit Eshel</a>, 
<a href="/search/cs?searchtype=author&query=Koenigstein%2C+N">Noam Koenigstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents Deep Integrated Explanations (DIX) - a universal method
for explaining vision models. DIX generates explanation maps by integrating
information from the intermediate representations of the model, coupled with
their corresponding gradients. Through an extensive array of both objective and
subjective evaluations spanning diverse tasks, datasets, and model
configurations, we showcase the efficacy of DIX in generating faithful and
accurate explanation maps, while surpassing current state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15372" title="Abstract">arXiv:2310.15372</a> [<a href="/pdf/2310.15372" title="Download PDF">pdf</a>, <a href="/format/2310.15372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EpiK-Eval: Evaluation for Language Models as Epistemic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prato%2C+G">Gabriele Prato</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jerry Huang</a>, 
<a href="/search/cs?searchtype=author&query=Parthasarathi%2C+P">Prasannna Parthasarathi</a>, 
<a href="/search/cs?searchtype=author&query=Sodhani%2C+S">Shagun Sodhani</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the age of artificial intelligence, the role of large language models
(LLMs) is becoming increasingly central. Despite their growing prevalence,
their capacity to consolidate knowledge from different training documents - a
crucial ability in numerous applications - remains unexplored. This paper
presents the first study examining the capability of LLMs to effectively
combine such information within their parameter space. We introduce EpiK-Eval,
a novel question-answering benchmark tailored to evaluate LLMs' proficiency in
formulating a coherent and consistent knowledge representation from segmented
narratives. Evaluations across various LLMs reveal significant weaknesses in
this domain. We contend that these shortcomings stem from the intrinsic nature
of prevailing training objectives. Consequently, we advocate for refining the
approach towards knowledge consolidation, as it harbors the potential to
dramatically improve their overall effectiveness and performance. The findings
from this study offer insights for developing more robust and reliable LLMs.
Our code and benchmark are available at
https://github.com/chandar-lab/EpiK-Eval
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15373" title="Abstract">arXiv:2310.15373</a> [<a href="/pdf/2310.15373" title="Download PDF">pdf</a>, <a href="/format/2310.15373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Data Management in Data Lakes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoseini%2C+S">Sayed Hoseini</a>, 
<a href="/search/cs?searchtype=author&query=Theissen-Lipp%2C+J">Johannes Theissen-Lipp</a>, 
<a href="/search/cs?searchtype=author&query=Quix%2C+C">Christoph Quix</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In recent years, data lakes emerged as away to manage large amounts of
heterogeneous data for modern data analytics. One way to prevent data lakes
from turning into inoperable data swamps is semantic data management. Some
approaches propose the linkage of metadata to knowledge graphs based on the
Linked Data principles to provide more meaning and semantics to the data in the
lake. Such a semantic layer may be utilized not only for data management but
also to tackle the problem of data integration from heterogeneous sources, in
order to make data access more expressive and interoperable. In this survey, we
review recent approaches with a specific focus on the application within data
lake systems and scalability to Big Data. We classify the approaches into (i)
basic semantic data management, (ii) semantic modeling approaches for enriching
metadata in data lakes, and (iii) methods for ontologybased data access. In
each category, we cover the main techniques and their background, and compare
latest research. Finally, we point out challenges for future work in this
research area, which needs a closer integration of Big Data and Semantic Web
technologies.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15383" title="Abstract">arXiv:2310.15383</a> [<a href="/pdf/2310.15383" title="Download PDF">pdf</a>, <a href="/format/2310.15383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GD-COMET: A Geo-Diverse Commonsense Inference Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+M">Mehar Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Shwartz%2C+V">Vered Shwartz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the increasing integration of AI into everyday life, it's becoming
crucial to design AI systems that serve users from diverse backgrounds by
making them culturally aware. In this paper, we present GD-COMET, a geo-diverse
version of the COMET commonsense inference model. GD-COMET goes beyond Western
commonsense knowledge and is capable of generating inferences pertaining to a
broad range of cultures. We demonstrate the effectiveness of GD-COMET through a
comprehensive human evaluation across 5 diverse cultures, as well as extrinsic
evaluation on a geo-diverse task. The evaluation shows that GD-COMET captures
and generates culturally nuanced commonsense knowledge, demonstrating its
potential to benefit NLP applications across the board and contribute to making
NLP more inclusive.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15385" title="Abstract">arXiv:2310.15385</a> [<a href="/pdf/2310.15385" title="Download PDF">pdf</a>, <a href="/format/2310.15385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Containerized Vertical Farming Using Cobots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahalingam%2C+D">Dasharadhan Mahalingam</a>, 
<a href="/search/cs?searchtype=author&query=Patankar%2C+A">Aditya Patankar</a>, 
<a href="/search/cs?searchtype=author&query=Phi%2C+K">Khiem Phi</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+N">Nilanjan Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=McGann%2C+R">Ryan McGann</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+I">IV Ramakrishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Containerized vertical farming is a type of vertical farming practice using
hydroponics in which plants are grown in vertical layers within a mobile
shipping container. Space limitations within shipping containers make the
automation of different farming operations challenging. In this paper, we
explore the use of cobots (i.e., collaborative robots) to automate two key
farming operations, namely, the transplantation of saplings and the harvesting
of grown plants. Our method uses a single demonstration from a farmer to
extract the motion constraints associated with the tasks, namely, transplanting
and harvesting, and can then generalize to different instances of the same
task. For transplantation, the motion constraint arises during insertion of the
sapling within the growing tube, whereas for harvesting, it arises during
extraction from the growing tube. We present experimental results to show that
using RGBD camera images (obtained from an eye-in-hand configuration) and one
demonstration for each task, it is feasible to perform transplantation of
saplings and harvesting of leafy greens using a cobot, without task-specific
programming.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15386" title="Abstract">arXiv:2310.15386</a> [<a href="/pdf/2310.15386" title="Download PDF">pdf</a>, <a href="/format/2310.15386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Course Correcting Koopman Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fathi%2C+M">Mahan Fathi</a>, 
<a href="/search/cs?searchtype=author&query=Gehring%2C+C">Clement Gehring</a>, 
<a href="/search/cs?searchtype=author&query=Pilault%2C+J">Jonathan Pilault</a>, 
<a href="/search/cs?searchtype=author&query=Kanaa%2C+D">David Kanaa</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>, 
<a href="/search/cs?searchtype=author&query=Goroshin%2C+R">Ross Goroshin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">Koopman representations aim to learn features of nonlinear dynamical systems
(NLDS) which lead to linear dynamics in the latent space. Theoretically, such
features can be used to simplify many problems in modeling and control of NLDS.
In this work we study autoencoder formulations of this problem, and different
ways they can be used to model dynamics, specifically for future state
prediction over long horizons. We discover several limitations of predicting
future states in the latent space and propose an inference-time mechanism,
which we refer to as Periodic Reencoding, for faithfully capturing long term
dynamics. We justify this method both analytically and empirically via
experiments in low and high dimensional NLDS.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15388" title="Abstract">arXiv:2310.15388</a> [<a href="/pdf/2310.15388" title="Download PDF">pdf</a>, <a href="/format/2310.15388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remote Heart Rate Monitoring in Smart Environments from Videos with  Self-supervised Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Divij Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Etemad%2C+A">Ali Etemad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE Internet of Things Journal 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in deep learning have made it increasingly feasible to
estimate heart rate remotely in smart environments by analyzing videos.
However, a notable limitation of deep learning methods is their heavy reliance
on extensive sets of labeled data for effective training. To address this
issue, self-supervised learning has emerged as a promising avenue. Building on
this, we introduce a solution that utilizes self-supervised contrastive
learning for the estimation of remote photoplethysmography (PPG) and heart rate
monitoring, thereby reducing the dependence on labeled data and enhancing
performance. We propose the use of 3 spatial and 3 temporal augmentations for
training an encoder through a contrastive framework, followed by utilizing the
late-intermediate embeddings of the encoder for remote PPG and heart rate
estimation. Our experiments on two publicly available datasets showcase the
improvement of our proposed approach over several related works as well as
supervised learning baselines, as our results approach the state-of-the-art. We
also perform thorough experiments to showcase the effects of using different
design choices such as the video representation learning method, the
augmentations used in the pre-training stage, and others. We also demonstrate
the robustness of our proposed method over the supervised learning approaches
on reduced amounts of labeled data.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15389" title="Abstract">arXiv:2310.15389</a> [<a href="/pdf/2310.15389" title="Download PDF">pdf</a>, <a href="/format/2310.15389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Irreducible Curriculum for Language Model Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Simin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic data selection and curriculum design for training large language
models is challenging, with only a few existing methods showing improvements
over standard training. Furthermore, current schemes focus on domain-level
selection, overlooking the more fine-grained contributions of each individual
training point. It is difficult to apply traditional datapoint selection
methods on large language models: most online batch selection methods perform
two-times forward or backward passes, which introduces considerable extra costs
with large-scale models. To mitigate these obstacles, we propose irreducible
curriculum as a curriculum learning algorithm for language model pretraining,
which prioritizes samples with higher learnability. Specifically, to avoid
prohibitive extra computation overhead, we simulate the sample loss along the
main model's training trajectory using a small-scale proxy model. Our
experiments on the RedPajama-1B dataset demonstrate a consistent improvement on
validation perplexity across all 7 domains compared to random uniform baseline
and the anti-curriculum strategy. Our method also reduces the sharpness of the
network and illustrates a better 5-shot accuracy on MMLU benchmarks.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15393" title="Abstract">arXiv:2310.15393</a> [<a href="/pdf/2310.15393" title="Download PDF">pdf</a>, <a href="/format/2310.15393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoGE: Domain Reweighting with Generalization Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Simin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Pagliardini%2C+M">Matteo Pagliardini</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The coverage and composition of the pretraining data corpus significantly
impacts the generalization ability of large language models. Conventionally,
the pretraining corpus is composed of various source domains (e.g. CommonCrawl,
Wikipedia, Github etc.) according to certain sampling probabilities (domain
weights). However, current methods lack a principled way to optimize domain
weights for ultimate goal for generalization. We propose DOmain reweighting
with Generalization Estimation (DoGE), where we reweigh the sampling
probability from each domain based on its contribution to the final
generalization objective assessed by a gradient-based generalization estimation
function. First, we train a small-scale proxy model with a min-max optimization
to obtain the reweighted domain weights. At each step, the domain weights are
updated to maximize the overall generalization gain by mirror descent. Finally
we use the obtained domain weights to train a larger scale full-size language
model. On SlimPajama-6B dataset, with universal generalization objective, DoGE
achieves better average perplexity and zero-shot reasoning accuracy. On
out-of-domain generalization tasks, DoGE reduces perplexity on the target
domain by a large margin. We further apply a parameter-selection scheme which
improves the efficiency of generalization estimation.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15398" title="Abstract">arXiv:2310.15398</a> [<a href="/pdf/2310.15398" title="Download PDF">pdf</a>, <a href="/format/2310.15398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;One-size-fits-all&quot;? Observations and Expectations of NLG Systems Across  Identity-Related Language Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lucy%2C+L">Li Lucy</a>, 
<a href="/search/cs?searchtype=author&query=Blodgett%2C+S+L">Su Lin Blodgett</a>, 
<a href="/search/cs?searchtype=author&query=Shokouhi%2C+M">Milad Shokouhi</a>, 
<a href="/search/cs?searchtype=author&query=Wallach%2C+H">Hanna Wallach</a>, 
<a href="/search/cs?searchtype=author&query=Olteanu%2C+A">Alexandra Olteanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Fairness-related assumptions about what constitutes appropriate NLG system
behaviors range from invariance, where systems are expected to respond
identically to social groups, to adaptation, where responses should instead
vary across them. We design and conduct five case studies, in which we perturb
different types of identity-related language features (names, roles, locations,
dialect, and style) in NLG system inputs to illuminate tensions around
invariance and adaptation. We outline people's expectations of system
behaviors, and surface potential caveats of these two contrasting yet
commonly-held assumptions. We find that motivations for adaptation include
social norms, cultural differences, feature-specific information, and
accommodation; motivations for invariance include perspectives that favor
prescriptivism, view adaptation as unnecessary or too difficult for NLG systems
to do appropriately, and are wary of false assumptions. Our findings highlight
open challenges around defining what constitutes fair NLG system behavior.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15400" title="Abstract">arXiv:2310.15400</a> [<a href="/pdf/2310.15400" title="Download PDF">pdf</a>, <a href="/format/2310.15400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A practical approach to computing Lyapunov exponents of renewal and  delay equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Breda%2C+D">Dimitri Breda</a>, 
<a href="/search/math?searchtype=author&query=Liessi%2C+D">Davide Liessi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">We propose a method for computing the Lyapunov exponents of renewal equations
(delay equations of Volterra type) and of coupled systems of renewal and delay
differential equations. The method consists in the reformulation of the delay
equation as an abstract differential equation, the reduction of the latter to a
system of ordinary differential equations via pseudospectral collocation, and
the application of the standard discrete QR method. The effectiveness of the
method is shown experimentally and a MATLAB implementation is provided.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15403" title="Abstract">arXiv:2310.15403</a> [<a href="/pdf/2310.15403" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Study between Silicon Carbide and Silicon Nitride based  Single Cell CMUT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kanjilal%2C+R">Rakesh Kanjilal</a>, 
<a href="/search/eess?searchtype=author&query=Maity%2C+R">Reshmi Maity</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Electronics and Informatics, September 2023, Volume 5,
  Issue 3, Pages 320-334
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This research explores the design and conducts a comparative analysis of a
noninsulated Capacitive Micromachined Ultrasonic Transducer (CMUT) featuring an
innovative asymmetric electrode configuration to improve the performance of the
device. Specifically, this configuration involves the utilization of a top
electrode with a smaller radius in comparison to the bottom electrode. The
study encompasses an investigation into the effects of varying biasing voltage
within the range of 40 V to 100 V. The materials employed in this study are
carefully selected to optimize the CMUT's performance. The substrate material
is silicon, and the bottom and top electrodes are made from aluminium.
Additionally, silicon dioxide is utilized as the foundation material within the
device's structure.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15405" title="Abstract">arXiv:2310.15405</a> [<a href="/pdf/2310.15405" title="Download PDF">pdf</a>, <a href="/format/2310.15405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+T">Ting-Yao Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chieh-Yang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+R">Ryan Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungchul Kim</a>, 
<a href="/search/cs?searchtype=author&query=Giles%2C+C+L">C. Lee Giles</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T+K">Ting-Hao K. Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear in EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">There is growing interest in systems that generate captions for scientific
figures. However, assessing these systems output poses a significant challenge.
Human evaluation requires academic expertise and is costly, while automatic
evaluation depends on often low-quality author-written captions. This paper
investigates using large language models (LLMs) as a cost-effective,
reference-free method for evaluating figure captions. We first constructed
SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600
scientific figure captions, both original and machine-made, for 600 arXiv
figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption
based on its potential to aid reader understanding, given relevant context such
as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot
evaluator, outperformed all other models and even surpassed assessments made by
Computer Science and Informatics undergraduates, achieving a Kendall
correlation score of 0.401 with Ph.D. students rankings
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15406" title="Abstract">arXiv:2310.15406</a> [<a href="/pdf/2310.15406" title="Download PDF">pdf</a>, <a href="/format/2310.15406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Elements and Cognitive Biases Influence Interpretations of Trends  in Scatter Plots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filipowicz%2C+A">Alexandre Filipowicz</a>, 
<a href="/search/cs?searchtype=author&query=Carter%2C+S">Scott Carter</a>, 
<a href="/search/cs?searchtype=author&query=Bravo%2C+N">Nayeli Bravo</a>, 
<a href="/search/cs?searchtype=author&query=Iliev%2C+R">Rumen Iliev</a>, 
<a href="/search/cs?searchtype=author&query=Hakimi%2C+S">Shabnam Hakimi</a>, 
<a href="/search/cs?searchtype=author&query=Shamma%2C+D+A">David Ayman Shamma</a>, 
<a href="/search/cs?searchtype=author&query=Lyons%2C+K">Kent Lyons</a>, 
<a href="/search/cs?searchtype=author&query=Hogan%2C+C">Candice Hogan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Charlene Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figure, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Graphics (cs.GR); Multimedia (cs.MM); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Visualizations are common methods to convey information but also increasingly
used to spread misinformation. It is therefore important to understand the
factors people use to interpret visualizations. In this paper, we focus on
factors that influence interpretations of scatter plots, investigating the
extent to which common visual aspects of scatter plots (outliers and trend
lines) and cognitive biases (people's beliefs) influence perception of
correlation trends. We highlight three main findings: outliers skew trend
perception but exert less influence than other points; trend lines make trends
seem stronger but also mitigate the influence of some outliers; and people's
beliefs have a small influence on perceptions of weak, but not strong
correlations. From these results we derive guidelines for adjusting visual
elements to mitigate the influence of factors that distort interpretations of
scatter plots. We explore how these guidelines may generalize to other
visualization types and make recommendations for future studies.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15407" title="Abstract">arXiv:2310.15407</a> [<a href="/pdf/2310.15407" title="Download PDF">pdf</a>, <a href="/ps/2310.15407" title="Download PostScript">ps</a>, <a href="/format/2310.15407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-Time Adaptive Fuzzy Tracking Control for Nonlinear State  Constrained Pure-Feedback Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Ju Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+M">Min Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper investigates the finite-time adaptive fuzzy tracking control
problem for a class of pure-feedback system with full-state constraints. With
the help of Mean-Value Theorem, the pure-feedback nonlinear system is
transformed into strict-feedback case. By employing finite-time-stable like
function and state transformation for output tracking error, the output
tracking error converges to a predefined set in a fixed finite interval. To
tackle the problem of state constraints, integral Barrier Lyapunov functions
are utilized to guarantee that the state variables remain within the prescribed
constraints with feasibility check. Fuzzy logic systems are utilized to
approximate the unknown nonlinear functions. In addition, all the signals in
the closed-loop system are guaranteed to be semi-global ultimately uniformly
bounded. Finally, two simulation examples are given to show the effectiveness
of the proposed control strategy.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15411" title="Abstract">arXiv:2310.15411</a> [<a href="/pdf/2310.15411" title="Download PDF">pdf</a>, <a href="/ps/2310.15411" title="Download PostScript">ps</a>, <a href="/format/2310.15411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Active Learning Halfspaces with Tsybakov Noise: A Non-convex  Optimization Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chicheng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of computationally and label efficient PAC active
learning $d$-dimensional halfspaces with Tsybakov
Noise~\citep{tsybakov2004optimal} under structured unlabeled data
distributions. Inspired by~\cite{diakonikolas2020learning}, we prove that any
approximate first-order stationary point of a smooth nonconvex loss function
yields a halfspace with a low excess error guarantee. In light of the above
structural result, we design a nonconvex optimization-based algorithm with a
label complexity of $\tilde{O}(d
(\frac{1}{\epsilon})^{\frac{8-6\alpha}{3\alpha-1}})$\footnote{In the main body
of this work, we use $\tilde{O}(\cdot), \tilde{\Theta}(\cdot)$ to hide factors
of the form $\polylog(d, \frac{1}{\epsilon}, \frac{1}{\delta})$}, under the
assumption that the Tsybakov noise parameter $\alpha \in (\frac13, 1]$, which
narrows down the gap between the label complexities of the previously known
efficient passive or active
algorithms~\citep{diakonikolas2020polynomial,zhang2021improved} and the
information-theoretic lower bound in this setting.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15413" title="Abstract">arXiv:2310.15413</a> [<a href="/pdf/2310.15413" title="Download PDF">pdf</a>, <a href="/format/2310.15413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensor Attacks and Resilient Defense on HVAC Systems for Energy Market  Signal Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tian%2C+G">Guanyu Tian</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Q+Z">Qun Zhou Sun</a>, 
<a href="/search/eess?searchtype=author&query=Qiao%2C+Y">Yiyuan Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The power flexibility from smart buildings makes them suitable candidates for
providing grid services. The building automation system (BAS) that employs
model predictive control (MPC) for grid services relies heavily on sensor data
gathered from IoT-based HVAC systems through communication networks. However,
cyber-attacks that tamper sensor values can compromise the accuracy and
flexibility of HVAC system power adjustment. Existing studies on
grid-interactive buildings mainly focus on the efficiency and flexibility of
buildings' participation in grid operations, while the security aspect is
lacking. In this paper, we investigate the effects of cyber-attacks on HVAC
systems in grid-interactive buildings, specifically their power-tracking
performance. We design a stochastic optimization-based stealthy sensor attack
and a corresponding defense strategy using a resilient control framework. The
attack and its defense are tested in a physical model of a test building with a
single-chiller HVAC system. Simulation results demonstrate that minor
falsifications caused by a stealthy sensor attack can significantly alter the
power profile, leading to large power tracking errors. However, the resilient
control framework can reduce the power tracking error by over 70% under such
attacks without filtering out compromised data.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15414" title="Abstract">arXiv:2310.15414</a> [<a href="/pdf/2310.15414" title="Download PDF">pdf</a>, <a href="/format/2310.15414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diverse Conventions for Human-AI Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+B">Bidipta Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Shih%2C+A">Andy Shih</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 9 figures, 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Conventions are crucial for strong performance in cooperative multi-agent
games, because they allow players to coordinate on a shared strategy without
explicit communication. Unfortunately, standard multi-agent reinforcement
learning techniques, such as self-play, converge to conventions that are
arbitrary and non-diverse, leading to poor generalization when interacting with
new partners. In this work, we present a technique for generating diverse
conventions by (1) maximizing their rewards during self-play, while (2)
minimizing their rewards when playing with previously discovered conventions
(cross-play), stimulating conventions to be semantically different. To ensure
that learned policies act in good faith despite the adversarial optimization of
cross-play, we introduce \emph{mixed-play}, where an initial state is randomly
generated by sampling self-play and cross-play transitions and the player
learns to maximize the self-play reward from this initial state. We analyze the
benefits of our technique on various multi-agent collaborative games, including
Overcooked, and find that our technique can adapt to the conventions of humans,
surpassing human-level performance when paired with real users.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15415" title="Abstract">arXiv:2310.15415</a> [<a href="/pdf/2310.15415" title="Download PDF">pdf</a>, <a href="/format/2310.15415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mind the Gap Between Conversations for Improved Long-Term Dialogue  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Naradowsky%2C+J">Jason Naradowsky</a>, 
<a href="/search/cs?searchtype=author&query=Miyao%2C+Y">Yusuke Miyao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Knowing how to end and resume conversations over time is a natural part of
communication, allowing for discussions to span weeks, months, or years. The
duration of gaps between conversations dictates which topics are relevant and
which questions to ask, and dialogue systems which do not explicitly model time
may generate responses that are unnatural. In this work we explore the idea of
making dialogue models aware of time, and present GapChat, a multi-session
dialogue dataset in which the time between each session varies. While the
dataset is constructed in real-time, progress on events in speakers' lives is
simulated in order to create realistic dialogues occurring across a long
timespan. We expose time information to the model and compare different
representations of time and event progress. In human evaluation we show that
time-aware models perform better in metrics that judge the relevance of the
chosen topics and the information gained from the conversation.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15416" title="Abstract">arXiv:2310.15416</a> [<a href="/pdf/2310.15416" title="Download PDF">pdf</a>, <a href="/format/2310.15416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nominality Score Conditioned Time Series Anomaly Detection by  Point/Sequential Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+C">Chih-Yu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fan-Keng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhengqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+J+H">Jeffrey H. Lang</a>, 
<a href="/search/cs?searchtype=author&query=Boning%2C+D+S">Duane S. Boning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (<a href="https://neurips.cc/virtual/2023/poster/70582">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time series anomaly detection is challenging due to the complexity and
variety of patterns that can occur. One major difficulty arises from modeling
time-dependent relationships to find contextual anomalies while maintaining
detection accuracy for point anomalies. In this paper, we propose a framework
for unsupervised time series anomaly detection that utilizes point-based and
sequence-based reconstruction models. The point-based model attempts to
quantify point anomalies, and the sequence-based model attempts to quantify
both point and contextual anomalies. Under the formulation that the observed
time point is a two-stage deviated value from a nominal time point, we
introduce a nominality score calculated from the ratio of a combined value of
the reconstruction errors. We derive an induced anomaly score by further
integrating the nominality score and anomaly score, then theoretically prove
the superiority of the induced anomaly score over the original anomaly score
under certain conditions. Extensive studies conducted on several public
datasets show that the proposed framework outperforms most state-of-the-art
baselines for time series anomaly detection.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15417" title="Abstract">arXiv:2310.15417</a> [<a href="/pdf/2310.15417" title="Download PDF">pdf</a>, <a href="/format/2310.15417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Semantic-driven Approach for Maintenance Digitalization in the  Pharmaceutical Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Ju Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+X">Xiaochen Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Madlena%2C+M">Marco Madlena</a>, 
<a href="/search/eess?searchtype=author&query=Kyritsis%2C+D">Dimitrios Kyritsis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The digital transformation of pharmaceutical industry is a challenging task
due to the high complexity of involved elements and the strict regulatory
compliance. Maintenance activities in the pharmaceutical industry play an
essential role in ensuring product quality and integral functioning of
equipment and premises. This paper first identifies the key challenges of
digitalization in pharmaceutical industry and creates the corresponding problem
space for key involved elements. A literature review is conducted to
investigate the mainstream maintenance strategies, digitalization models, tools
and official guidance from authorities in pharmaceutical industry. Based on the
review result, a semantic-driven digitalization framework is proposed aiming to
improve the digital continuity and cohesion of digital resources and
technologies for maintenance activities in the pharmaceutical industry. A case
study is conducted to verify the feasibility of the proposed framework based on
the water sampling activities in Merck Serono facility in Switzerland. A
tool-chain is presented to enable the functional modules of the framework. Some
of the key functional modules within the framework are implemented and have
demonstrated satisfactory performance. As one of the outcomes, a digital
sampling assistant with web-based services is created to support the automated
workflow of water sampling activities. The implementation result proves the
potential of the proposed framework to solve the identified problems of
maintenance digitalization in the pharmaceutical industry.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15418" title="Abstract">arXiv:2310.15418</a> [<a href="/pdf/2310.15418" title="Download PDF">pdf</a>, <a href="/format/2310.15418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fractal Landscapes in Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Herbert%2C+S">Sylvia Herbert</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Sicun Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages and 28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Policy gradient lies at the core of deep reinforcement learning (RL) in
continuous domains. Despite much success, it is often observed in practice that
RL training with policy gradient can fail for many reasons, even on standard
control problems with known solutions. We propose a framework for understanding
one inherent limitation of the policy gradient approach: the optimization
landscape in the policy space can be extremely non-smooth or fractal for
certain classes of MDPs, such that there does not exist gradient to be
estimated in the first place. We draw on techniques from chaos theory and
non-smooth analysis, and analyze the maximal Lyapunov exponents and H\"older
exponents of the policy optimization objectives. Moreover, we develop a
practical method that can estimate the local smoothness of objective function
from samples to identify when the training process has encountered fractal
landscapes. We show experiments to illustrate how some failure cases of policy
optimization can be explained by such fractal landscapes.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15419" title="Abstract">arXiv:2310.15419</a> [<a href="/pdf/2310.15419" title="Download PDF">pdf</a>, <a href="/format/2310.15419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast multiplication of random dense matrices with fixed sparse matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+T">Tianyu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+R">Riley Murray</a>, 
<a href="/search/cs?searchtype=author&query=Bulu%C3%A7%2C+A">Ayd&#x131;n Bulu&#xe7;</a>, 
<a href="/search/cs?searchtype=author&query=Demmel%2C+J">James Demmel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">This work focuses on accelerating the multiplication of a dense random matrix
with a (fixed) sparse matrix, which is frequently used in sketching algorithms.
We develop a novel scheme that takes advantage of blocking and recomputation
(on-the-fly random number generation) to accelerate this operation. The
techniques we propose decrease memory movement, thereby increasing the
algorithm's parallel scalability in shared memory architectures. On the Intel
Frontera architecture, our algorithm can achieve 2x speedups over libraries
such as Eigen and Intel MKL on some examples. In addition, with 32 threads, we
can obtain a parallel efficiency of up to approximately 45%. We also present a
theoretical analysis for the memory movement lower bound of our algorithm,
showing that under mild assumptions, it's possible to beat the data movement
lower bound of general matrix-matrix multiply (GEMM) by a factor of $\sqrt M$,
where $M$ is the cache size. Finally, we incorporate our sketching algorithm
into a randomized least squares solver. For extremely over-determined sparse
input matrices, we show that our results are competitive with SuiteSparse; in
some cases, we obtain a speedup of 10x over SuiteSparse.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15420" title="Abstract">arXiv:2310.15420</a> [<a href="/pdf/2310.15420" title="Download PDF">pdf</a>, <a href="/format/2310.15420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let the Pretrained Language Models &quot;Imagine&quot; for Short Texts Topic  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akash%2C+P+S">Pritom Saha Akash</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Topic models are one of the compelling methods for discovering latent
semantics in a document collection. However, it assumes that a document has
sufficient co-occurrence information to be effective. However, in short texts,
co-occurrence information is minimal, which results in feature sparsity in
document representation. Therefore, existing topic models (probabilistic or
neural) mostly fail to mine patterns from them to generate coherent topics. In
this paper, we take a new approach to short-text topic modeling to address the
data-sparsity issue by extending short text into longer sequences using
existing pre-trained language models (PLMs). Besides, we provide a simple
solution extending a neural topic model to reduce the effect of noisy
out-of-topics text generation from PLMs. We observe that our model can
substantially improve the performance of short-text topic modeling. Extensive
experiments on multiple real-world datasets under extreme data sparsity
scenarios show that our models can generate high-quality topics outperforming
state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15421" title="Abstract">arXiv:2310.15421</a> [<a href="/pdf/2310.15421" title="Download PDF">pdf</a>, <a href="/format/2310.15421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FANToM: A Benchmark for Stress-testing Machine Theory of Mind in  Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sclar%2C+M">Melanie Sclar</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bras%2C+R+L">Ronan Le Bras</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Code and dataset can be found here: <a href="https://hyunw.kim/fantom">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Theory of mind (ToM) evaluations currently focus on testing models using
passive narratives that inherently lack interactivity. We introduce FANToM, a
new benchmark designed to stress-test ToM within information-asymmetric
conversational contexts via question answering. Our benchmark draws upon
important theoretical requisites from psychology and necessary empirical
considerations when evaluating large language models (LLMs). In particular, we
formulate multiple types of questions that demand the same underlying reasoning
to identify illusory or false sense of ToM capabilities in LLMs. We show that
FANToM is challenging for state-of-the-art LLMs, which perform significantly
worse than humans even with chain-of-thought reasoning or fine-tuning.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15422" title="Abstract">arXiv:2310.15422</a> [<a href="/pdf/2310.15422" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G2-MonoDepth: A General Framework of Generalized Depth Inference from  Monocular RGB+X Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haotian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Meng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Monocular depth inference is a fundamental problem for scene perception of
robots. Specific robots may be equipped with a camera plus an optional depth
sensor of any type and located in various scenes of different scales, whereas
recent advances derived multiple individual sub-tasks. It leads to additional
burdens to fine-tune models for specific robots and thereby high-cost
customization in large-scale industrialization. This paper investigates a
unified task of monocular depth inference, which infers high-quality depth maps
from all kinds of input raw data from various robots in unseen scenes. A basic
benchmark G2-MonoDepth is developed for this task, which comprises four
components: (a) a unified data representation RGB+X to accommodate RGB plus raw
depth with diverse scene scale/semantics, depth sparsity ([0%, 100%]) and
errors (holes/noises/blurs), (b) a novel unified loss to adapt to diverse depth
sparsity/errors of input raw data and diverse scales of output scenes, (c) an
improved network to well propagate diverse scene scales from input to output,
and (d) a data augmentation pipeline to simulate all types of real artifacts in
raw depth maps for training. G2-MonoDepth is applied in three sub-tasks
including depth estimation, depth completion with different sparsity, and depth
enhancement in unseen scenes, and it always outperforms SOTA baselines on both
real-world data and synthetic data.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15426" title="Abstract">arXiv:2310.15426</a> [<a href="/pdf/2310.15426" title="Download PDF">pdf</a>, <a href="/format/2310.15426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> zonoLAB: A MATLAB toolbox for set-based control systems analysis using  hybrid zonotopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Koeln%2C+J">Justin Koeln</a>, 
<a href="/search/eess?searchtype=author&query=Bird%2C+T+J">Trevor J. Bird</a>, 
<a href="/search/eess?searchtype=author&query=Siefert%2C+J">Jacob Siefert</a>, 
<a href="/search/eess?searchtype=author&query=Ruths%2C+J">Justin Ruths</a>, 
<a href="/search/eess?searchtype=author&query=Pangborn%2C+H">Herschel Pangborn</a>, 
<a href="/search/eess?searchtype=author&query=Jain%2C+N">Neera Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces zonoLAB, a MATLAB-based toolbox for set-based control
system analysis using the hybrid zonotope set representation. Hybrid zonotopes
have proven to be an expressive set representation that can exactly represent
the reachable sets of mixed-logical dynamical systems and tightly approximate
the reachable sets of nonlinear dynamic systems. Moreover, hybrid zonotopes can
exactly represent the continuous piecewise linear control laws associated with
model predictive control and the input-output mappings of neural networks with
piecewise linear activation functions. The hybrid zonotope set representation
is also highly exploitable, where efficient methods developed for mixed-integer
linear programming can be directly used for set operation and analysis. The
zonoLAB toolbox is designed to make these capabilities accessible to the
dynamic systems and controls community, with functionality spanning fundamental
operations with hybrid zonotope, constrained zonotope, and zonotope set
representations, powerful set analysis tools, and general-purpose algorithms
for reachability analysis of open- and closed-loop systems.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15428" title="Abstract">arXiv:2310.15428</a> [<a href="/pdf/2310.15428" title="Download PDF">pdf</a>, <a href="/format/2310.15428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConstitutionMaker: Interactively Critiquing Large Language Models by  Converting Feedback into Principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petridis%2C+S">Savvas Petridis</a>, 
<a href="/search/cs?searchtype=author&query=Wedin%2C+B">Ben Wedin</a>, 
<a href="/search/cs?searchtype=author&query=Wexler%2C+J">James Wexler</a>, 
<a href="/search/cs?searchtype=author&query=Donsbach%2C+A">Aaron Donsbach</a>, 
<a href="/search/cs?searchtype=author&query=Pushkarna%2C+M">Mahima Pushkarna</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+N">Nitesh Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+C+J">Carrie J. Cai</a>, 
<a href="/search/cs?searchtype=author&query=Terry%2C+M">Michael Terry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language model (LLM) prompting is a promising new approach for users to
create and customize their own chatbots. However, current methods for steering
a chatbot's outputs, such as prompt engineering and fine-tuning, do not support
users in converting their natural feedback on the model's outputs to changes in
the prompt or model. In this work, we explore how to enable users to
interactively refine model outputs through their feedback, by helping them
convert their feedback into a set of principles (i.e. a constitution) that
dictate the model's behavior. From a formative study, we (1) found that users
needed support converting their feedback into principles for the chatbot and
(2) classified the different principle types desired by users. Inspired by
these findings, we developed ConstitutionMaker, an interactive tool for
converting user feedback into principles, to steer LLM-based chatbots. With
ConstitutionMaker, users can provide either positive or negative feedback in
natural language, select auto-generated feedback, or rewrite the chatbot's
response; each mode of feedback automatically generates a principle that is
inserted into the chatbot's prompt. In a user study with 14 participants, we
compare ConstitutionMaker to an ablated version, where users write their own
principles. With ConstitutionMaker, participants felt that their principles
could better guide the chatbot, that they could more easily convert their
feedback into principles, and that they could write principles more
efficiently, with less mental demand. ConstitutionMaker helped users identify
ways to improve the chatbot, formulate their intuitive responses to the model
into feedback, and convert this feedback into specific and clear principles.
Together, these findings inform future tools that support the interactive
critiquing of LLM outputs.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15429" title="Abstract">arXiv:2310.15429</a> [<a href="/pdf/2310.15429" title="Download PDF">pdf</a>, <a href="/format/2310.15429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Sentiment: Leveraging Topic Metrics for Political Stance  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+W">Weihong Qi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Sentiment analysis, widely critiqued for capturing merely the overall tone of
a corpus, falls short in accurately reflecting the latent structures and
political stances within texts. This study introduces topic metrics, dummy
variables converted from extracted topics, as both an alternative and
complement to sentiment metrics in stance classification. By employing three
datasets identified by Bestvater and Monroe (2023), this study demonstrates
BERTopic's proficiency in extracting coherent topics and the effectiveness of
topic metrics in stance classification. The experiment results show that
BERTopic improves coherence scores by 17.07% to 54.20% when compared to
traditional approaches such as Dirichlet Allocation (LDA) and Non-negative
Matrix Factorization (NMF), prevalent in earlier political science research.
Additionally, our results indicate topic metrics outperform sentiment metrics
in stance classification, increasing performance by as much as 18.95%. Our
findings suggest topic metrics are especially effective for context-rich texts
and corpus where stance and sentiment correlations are weak. The combination of
sentiment and topic metrics achieve an optimal performance in most of the
scenarios and can further address the limitations of relying solely on
sentiment as well as the low coherence score of topic metrics.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15431" title="Abstract">arXiv:2310.15431</a> [<a href="/pdf/2310.15431" title="Download PDF">pdf</a>, <a href="/format/2310.15431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts  and Rationales for Disambiguating Defeasible Social and Moral Situations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+K">Kavel Rao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pyatkin%2C+V">Valentina Pyatkin</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuling Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Dziri%2C+N">Nouha Dziri</a>, 
<a href="/search/cs?searchtype=author&query=Brahman%2C+F">Faeze Brahman</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready EMNLP Findings 2023. First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Moral or ethical judgments rely heavily on the specific contexts in which
they occur. Understanding varying shades of defeasible contextualizations
(i.e., additional information that strengthens or attenuates the moral
acceptability of an action) is critical to accurately represent the subtlety
and intricacy of grounded human moral judgment in real-life scenarios.
<br />We introduce defeasible moral reasoning: a task to provide grounded contexts
that make an action more or less morally acceptable, along with commonsense
rationales that justify the reasoning. To elicit high-quality task data, we
take an iterative self-distillation approach that starts from a small amount of
unstructured seed knowledge from GPT-3 and then alternates between (1)
self-distillation from student models; (2) targeted filtering with a critic
model trained by human judgment (to boost validity) and NLI (to boost
diversity); (3) self-imitation learning (to amplify the desired data quality).
This process yields a student model that produces defeasible contexts with
improved validity, diversity, and defeasibility. From this model we distill a
high-quality dataset, \delta-Rules-of-Thumb, of 1.2M entries of
contextualizations and rationales for 115K defeasible moral actions rated
highly by human annotators 85.9% to 99.8% of the time. Using \delta-RoT we
obtain a final student model that wins over all intermediate student models by
a notable margin.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15432" title="Abstract">arXiv:2310.15432</a> [<a href="/pdf/2310.15432" title="Download PDF">pdf</a>, <a href="/format/2310.15432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Economic Incentives for Efficient Operation of Flexible  Transmission
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rui%2C+X">Xinyang Rui</a>, 
<a href="/search/eess?searchtype=author&query=Mirzapour%2C+O">Omid Mirzapour</a>, 
<a href="/search/eess?searchtype=author&query=Pruneau%2C+B">Brittany Pruneau</a>, 
<a href="/search/eess?searchtype=author&query=Sahraei-Ardakani%2C+M">Mostafa Sahraei-Ardakani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 55th North American Power Symposium (NAPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The growing penetration of renewable energy requires upgrades to the
transmission network to ensure the deliverability of renewable generation. As
an efficient alternative to transmission expansion, flexible transmission
technologies, whose benefits have been widely studied, can alleviate
transmission system congestion and enhance renewable energy integration.
However, under the current market structure, investments for these technologies
only receive a regulated rate of return, providing little to no incentive for
efficient operation. Additionally, a regulated rate of return creates an
incentive for building more transmission lines rather than efficient
utilization of the existing system. Therefore, investments in flexible
transmission technologies remain rather limited. To facilitate the deployment
of flexible transmission, improve system efficiency, and accommodate renewable
energy integration, a proper incentive structure for flexible transmission
technologies, compatible with the current market design, is vital. This paper
reviews the current market-based mechanisms for various flexible transmission
technologies, including impedance control, dynamic line rating, and
transmission switching. This review pinpoints current challenges of the
market-based operation of flexible transmission and provides insights for
future endeavors in designing efficient price signals for flexible transmission
operation.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15433" title="Abstract">arXiv:2310.15433</a> [<a href="/pdf/2310.15433" title="Download PDF">pdf</a>, <a href="/format/2310.15433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Off-Policy Evaluation for Large Action Spaces via Policy Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lequn Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Dawen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. 36 pages, 31 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Developing accurate off-policy estimators is crucial for both evaluating and
optimizing for new policies. The main challenge in off-policy estimation is the
distribution shift between the logging policy that generates data and the
target policy that we aim to evaluate. Typically, techniques for correcting
distribution shift involve some form of importance sampling. This approach
results in unbiased value estimation but often comes with the trade-off of high
variance, even in the simpler case of one-step contextual bandits. Furthermore,
importance sampling relies on the common support assumption, which becomes
impractical when the action space is large. To address these challenges, we
introduce the Policy Convolution (PC) family of estimators. These methods
leverage latent structure within actions -- made available through action
embeddings -- to strategically convolve the logging and target policies. This
convolution introduces a unique bias-variance trade-off, which can be
controlled by adjusting the amount of convolution. Our experiments on synthetic
and benchmark datasets demonstrate remarkable mean squared error (MSE)
improvements when using PC, especially when either the action space or policy
mismatch becomes large, with gains of up to 5 - 6 orders of magnitude over
existing estimators.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15434" title="Abstract">arXiv:2310.15434</a> [<a href="/pdf/2310.15434" title="Download PDF">pdf</a>, <a href="/format/2310.15434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Dual-Output Step-Down Soft-Switching Current-Fed Push-Pull  DC-DC Converter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mohebbifar%2C+M">Minoo Mohebbifar</a>, 
<a href="/search/eess?searchtype=author&query=Panahazari%2C+M">Mohammad Panahazari</a>, 
<a href="/search/eess?searchtype=author&query=Mirzapour%2C+O">Omid Mirzapour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 55th North American Power Symposium (NAPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Multi-port DC-DC converters are gaining more significance in modern power
system environments by enabling the connection of multiple renewable energy
sources, so the efficient operation of these converters is paramount. Soft
switching methods increase efficiency in DC-DC converters and increase the
reliability and lifespan of devices by relieving stress on components. This
paper proposes a method for soft-switching of a dual-output step-down
current-fed full-bridge push-pull DC-DC converter. The converter enables two
independent outputs to supply different loads. The topology achieves
zero-current switching on the primary side and zero-voltage switching on the
secondary side, eliminating the need for active-clamp circuits and passive
snubbers to absorb surge voltage. This reduces switching losses and lower
voltage and current stresses on power electronic devices. The paper thoroughly
investigates the proposed converter's operation principle, control strategy,
and characteristics. Equations for the voltage and current of all components
are derived, and the conditions for achieving soft switching are calculated.
Simulation results in EMTDC/PSCAD software validate the accuracy of the
proposed method.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15435" title="Abstract">arXiv:2310.15435</a> [<a href="/pdf/2310.15435" title="Download PDF">pdf</a>, <a href="/format/2310.15435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers&#x27;  Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petridis%2C+S">Savvas Petridis</a>, 
<a href="/search/cs?searchtype=author&query=Terry%2C+M">Michael Terry</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+C+J">Carrie J. Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Prototyping AI applications is notoriously difficult. While large language
model (LLM) prompting has dramatically lowered the barriers to AI prototyping,
designers are still prototyping AI functionality and UI separately. We
investigate how coupling prompt and UI design affects designers' workflows.
Grounding this research, we developed PromptInfuser, a Figma plugin that
enables users to create semi-functional mockups, by connecting UI elements to
the inputs and outputs of prompts. In a study with 14 designers, we compare
PromptInfuser to designers' current AI-prototyping workflow. PromptInfuser was
perceived to be significantly more useful for communicating product ideas, more
capable of producing prototypes that realistically represent the envisioned
artifact, more efficient for prototyping, and more helpful for anticipating UI
issues and technical constraints. PromptInfuser encouraged iteration over
prompt and UI together, which helped designers identify UI and prompt
incompatibilities and reflect upon their total solution. Together, these
findings inform future systems for prototyping AI applications.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15436" title="Abstract">arXiv:2310.15436</a> [<a href="/pdf/2310.15436" title="Download PDF">pdf</a>, <a href="/format/2310.15436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VGX: Large-Scale Sample Generation for Boosting Learning-Based Software  Vulnerability Analyses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nong%2C+Y">Yu Nong</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Richard Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+G">Guangbei Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kunsong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiapu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Haipeng Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Accompanying the successes of learning-based defensive software vulnerability
analyses is the lack of large and quality sets of labeled vulnerable program
samples, which impedes further advancement of those defenses. Existing
automated sample generation approaches have shown potentials yet still fall
short of practical expectations due to the high noise in the generated samples.
This paper proposes VGX, a new technique aimed for large-scale generation of
high-quality vulnerability datasets. Given a normal program, VGX identifies the
code contexts in which vulnerabilities can be injected, using a customized
Transformer featured with a new value-flowbased position encoding and
pre-trained against new objectives particularly for learning code structure and
context. Then, VGX materializes vulnerability-injection code editing in the
identified contexts using patterns of such edits obtained from both historical
fixes and human knowledge about real-world vulnerabilities. Compared to four
state-of-the-art (SOTA) baselines (pattern-, Transformer-, GNN-, and
pattern+Transformer-based), VGX achieved 99.09-890.06% higher F1 and
22.45%-328.47% higher label accuracy. For in-the-wild sample production, VGX
generated 150,392 vulnerable samples, from which we randomly chose 10% to
assess how much these samples help vulnerability detection, localization, and
repair. Our results show SOTA techniques for these three application tasks
achieved 19.15-330.80% higher F1, 12.86-19.31% higher top-10 accuracy, and
85.02-99.30% higher top-50 accuracy, respectively, by adding those samples to
their original training data. These samples also helped a SOTA vulnerability
detector discover 13 more real-world vulnerabilities (CVEs) in critical systems
(e.g., Linux kernel) that would be missed by the original model.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15439" title="Abstract">arXiv:2310.15439</a> [<a href="/pdf/2310.15439" title="Download PDF">pdf</a>, <a href="/format/2310.15439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific  Ratings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chaewon Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Soohwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Kyubyong Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Kunwoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Numerous datasets have been proposed to combat the spread of online hate.
Despite these efforts, a majority of these resources are English-centric,
primarily focusing on overt forms of hate. This research gap calls for
developing high-quality corpora in diverse languages that also encapsulate more
subtle hate expressions. This study introduces K-HATERS, a new corpus for hate
speech detection in Korean, comprising approximately 192K news comments with
target-specific offensiveness ratings. This resource is the largest offensive
language corpus in Korean and is the first to offer target-specific ratings on
a three-point Likert scale, enabling the detection of hate expressions in
Korean across varying degrees of offensiveness. We conduct experiments showing
the effectiveness of the proposed corpus, including a comparison with existing
datasets. Additionally, to address potential noise and bias in human
annotations, we explore a novel idea of adopting the Cognitive Reflection Test,
which is widely used in social science for assessing an individual's cognitive
ability, as a proxy of labeling quality. Findings indicate that annotations
from individuals with the lowest test scores tend to yield detection models
that make biased predictions toward specific target groups and are less
accurate. This study contributes to the NLP research on hate speech detection
and resource construction. The code and dataset can be accessed at
https://github.com/ssu-humane/K-HATERS.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15444" title="Abstract">arXiv:2310.15444</a> [<a href="/pdf/2310.15444" title="Download PDF">pdf</a>, <a href="/format/2310.15444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Propagation is Better: Accelerating Single-Step Adversarial  Training via Sampling Subnetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaojun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianshu Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jindong Gu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Adversarial training has shown promise in building robust models against
adversarial examples. A major drawback of adversarial training is the
computational overhead introduced by the generation of adversarial examples. To
overcome this limitation, adversarial training based on single-step attacks has
been explored. Previous work improves the single-step adversarial training from
different perspectives, e.g., sample initialization, loss regularization, and
training strategy. Almost all of them treat the underlying model as a black
box. In this work, we propose to exploit the interior building blocks of the
model to improve efficiency. Specifically, we propose to dynamically sample
lightweight subnetworks as a surrogate model during training. By doing this,
both the forward and backward passes can be accelerated for efficient
adversarial training. Besides, we provide theoretical analysis to show the
model robustness can be improved by the single-step adversarial training with
sampled subnetworks. Furthermore, we propose a novel sampling strategy where
the sampling varies from layer to layer and from iteration to iteration.
Compared with previous methods, our method not only reduces the training cost
but also achieves better model robustness. Evaluations on a series of popular
datasets demonstrate the effectiveness of the proposed FB-Better. Our code has
been released at https://github.com/jiaxiaojunQAQ/FP-Better.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15445" title="Abstract">arXiv:2310.15445</a> [<a href="/pdf/2310.15445" title="Download PDF">pdf</a>, <a href="/format/2310.15445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A DPLL Procedure with Dichotomous Branching for Propositional Product  Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guller%2C+D">Dusan Guller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The propositional product logic is one of the basic fuzzy logics with
continuous t-norms, exploiting the multiplication t-norm on the unit interval
[0,1]. Our aim is to combine well-established automated deduction (theorem
proving) with fuzzy inference. As a first step, we devise a modification of the
procedure of Davis, Putnam, Logemann, and Loveland (DPLL) with dichotomous
branching inferring in the product logic. We prove that the procedure is
refutation sound and finitely complete. As a consequence, solutions to the
deduction, satisfiability, and validity problems will be proposed for the
finite case. The presented results are applicable to a design of intelligent
systems, exploiting some kind of multi-step fuzzy inference.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15446" title="Abstract">arXiv:2310.15446</a> [<a href="/pdf/2310.15446" title="Download PDF">pdf</a>, <a href="/ps/2310.15446" title="Download PostScript">ps</a>, <a href="/format/2310.15446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What are acceptable reductions? Perspectives from proof-theoretic  semantics and type theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayhan%2C+S">Sara Ayhan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Australasian Journal of Logic, 20(3), 2023, pp. 412-428
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">It has been argued that reduction procedures are closely connected to the
question about identity of proofs and that accepting certain reductions would
lead to a trivialization of identity of proofs in the sense that every
derivation of the same conclusion would have to be identified. In this paper it
will be shown that the question, which reductions we accept in our system, is
not only important if we see them as generating a theory of proof identity but
is also decisive for the more general question whether a proof has meaningful
content. There are certain reductions which would not only force us to identify
proofs of different arbitrary formulas but which would render derivations in a
system allowing them meaningless. To exclude such cases, a minimal criterion is
proposed which reductions have to fulfill to be acceptable.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15447" title="Abstract">arXiv:2310.15447</a> [<a href="/pdf/2310.15447" title="Download PDF">pdf</a>, <a href="/format/2310.15447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepIron: Predicting Unwarped Garment Texture from a Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H">Hyun-Song Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sung-Hee Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Realistic reconstruction of 3D clothing from an image has wide applications,
such as avatar creation and virtual try-on. This paper presents a novel
framework that reconstructs the texture map for 3D garments from a single image
with pose. Assuming that 3D garments are modeled by stitching 2D garment sewing
patterns, our specific goal is to generate a texture image for the sewing
patterns. A key component of our framework, the Texture Unwarper, infers the
original texture image from the input clothing image, which exhibits warping
and occlusion of texture due to the user's body shape and pose. The Texture
Unwarper effectively transforms between the input and output images by mapping
the latent spaces of the two images. By inferring the unwarped original texture
of the input garment, our method helps reconstruct 3D garment models that can
show high-quality texture images realistically deformed for new poses. We
validate the effectiveness of our approach through a comparison with other
methods and ablation studies. Additionally, we release a large dataset of
garment sewing patterns with textures and images of avatars wearing the
garments, which will be useful for future research on garment texture
reconstruction and synthesis.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15450" title="Abstract">arXiv:2310.15450</a> [<a href="/pdf/2310.15450" title="Download PDF">pdf</a>, <a href="/ps/2310.15450" title="Download PostScript">ps</a>, <a href="/format/2310.15450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Identifiability and Achievability for Causal Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Var%C4%B1c%C4%B1%2C+B">Burak Var&#x131;c&#x131;</a>, 
<a href="/search/cs?searchtype=author&query=Acart%C3%BCrk%2C+E">Emre Acart&#xfc;rk</a>, 
<a href="/search/cs?searchtype=author&query=Shanmugam%2C+K">Karthikeyan Shanmugam</a>, 
<a href="/search/cs?searchtype=author&query=Tajer%2C+A">Ali Tajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper focuses on causal representation learning (CRL) under a general
nonparametric causal latent model and a general transformation model that maps
the latent data to the observational data. It establishes
\textbf{identifiability} and \textbf{achievability} results using two hard
\textbf{uncoupled} interventions per node in the latent causal graph. Notably,
one does not know which pair of intervention environments have the same node
intervened (hence, uncoupled environments). For identifiability, the paper
establishes that perfect recovery of the latent causal model and variables is
guaranteed under uncoupled interventions. For achievability, an algorithm is
designed that uses observational and interventional data and recovers the
latent causal model and variables with provable guarantees for the algorithm.
This algorithm leverages score variations across different environments to
estimate the inverse of the transformer and, subsequently, the latent
variables. The analysis, additionally, recovers the existing identifiability
result for two hard \textbf{coupled} interventions, that is when metadata about
the pair of environments that have the same node intervened is known. It is
noteworthy that the existing results on non-parametric identifiability require
assumptions on interventions and additional faithfulness assumptions. This
paper shows that when observational data is available, additional faithfulness
assumptions are unnecessary.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15454" title="Abstract">arXiv:2310.15454</a> [<a href="/pdf/2310.15454" title="Download PDF">pdf</a>, <a href="/format/2310.15454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Learning with Public Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krichene%2C+W">Walid Krichene</a>, 
<a href="/search/cs?searchtype=author&query=Mayoraz%2C+N">Nicolas Mayoraz</a>, 
<a href="/search/cs?searchtype=author&query=Rendle%2C+S">Steffen Rendle</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuang Song</a>, 
<a href="/search/cs?searchtype=author&query=Thakurta%2C+A">Abhradeep Thakurta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study a class of private learning problems in which the data is a join of
private and public features. This is often the case in private personalization
tasks such as recommendation or ad prediction, in which features related to
individuals are sensitive, while features related to items (the movies or songs
to be recommended, or the ads to be shown to users) are publicly available and
do not require protection. A natural question is whether private algorithms can
achieve higher utility in the presence of public features. We give a positive
answer for multi-encoder models where one of the encoders operates on public
features. We develop new algorithms that take advantage of this separation by
only protecting certain sufficient statistics (instead of adding noise to the
gradient). This method has a guaranteed utility improvement for linear
regression, and importantly, achieves the state of the art on two standard
private recommendation benchmarks, demonstrating the importance of methods that
adapt to the private-public feature separation.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15455" title="Abstract">arXiv:2310.15455</a> [<a href="/pdf/2310.15455" title="Download PDF">pdf</a>, <a href="/format/2310.15455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UI Layout Generation with LLMs Guided by UI Grammar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuwen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+Z">Ziang Tong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qinyi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2023 Workshop on AI and HCI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The recent advances in Large Language Models (LLMs) have stimulated interest
among researchers and industry professionals, particularly in their application
to tasks concerning mobile user interfaces (UIs). This position paper
investigates the use of LLMs for UI layout generation. Central to our
exploration is the introduction of UI grammar -- a novel approach we proposed
to represent the hierarchical structure inherent in UI screens. The aim of this
approach is to guide the generative capacities of LLMs more effectively and
improve the explainability and controllability of the process. Initial
experiments conducted with GPT-4 showed the promising capability of LLMs to
produce high-quality user interfaces via in-context learning. Furthermore, our
preliminary comparative study suggested the potential of the grammar-based
approach in improving the quality of generative results in specific aspects.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15457" title="Abstract">arXiv:2310.15457</a> [<a href="/pdf/2310.15457" title="Download PDF">pdf</a>, <a href="/ps/2310.15457" title="Download PostScript">ps</a>, <a href="/format/2310.15457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Unconditionally Stable Iterative Decoupled Algorithm for  Multiple-Network Poroelasticity Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lei%2C+M">Meng Lei</a>, 
<a href="/search/math?searchtype=author&query=Cai%2C+M">Mingchao Cai</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+F">Feng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, we introduce an iterative decoupled algorithm designed for
addressing the quasi-static multiple-network poroelasticity problem. This
problem pertains to the simultaneous modeling of fluid flow and deformations
within an elastic porous medium permeated by multiple fluid networks, each with
distinct characteristics. Our approach focuses on the total-pressure-based
formulation, which treats the solid displacement, total pressure, and network
pressures as primary unknowns. This formulation transforms the original problem
into a combination of the generalized Stokes problem and the parabolic problem,
offering certain advantages such as mitigating elastic locking effects and
streamlining the discretization process. Notably, the algorithm ensures
unconditional convergence to the solution of the total-pressure-based coupled
algorithm. To validate the accuracy and efficiency of our method, we present
numerical experiments. The robustness of the algorithm with respect to the
physical parameters and the discretization parameters is carefully
investigated.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15458" title="Abstract">arXiv:2310.15458</a> [<a href="/pdf/2310.15458" title="Download PDF">pdf</a>, <a href="/format/2310.15458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A distributed-memory parallel algorithm for discretized integral  equations using Julia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+T">Tianyu Liang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/math?searchtype=author&query=Martinsson%2C+P">Per-Gunnar Martinsson</a>, 
<a href="/search/math?searchtype=author&query=Biros%2C+G">George Biros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Boundary value problems involving elliptic PDEs such as the Laplace and the
Helmholtz equations are ubiquitous in physics and engineering. Many such
problems have alternative formulations as integral equations that are
mathematically more tractable than their PDE counterparts. However, the
integral equation formulation poses a challenge in solving the dense linear
systems that arise upon discretization. In cases where iterative methods
converge rapidly, existing methods that draw on fast summation schemes such as
the Fast Multipole Method are highly efficient and well established. More
recently, linear complexity direct solvers that sidestep convergence issues by
directly computing an invertible factorization have been developed. However,
storage and compute costs are high, which limits their ability to solve
large-scale problems in practice. In this work, we introduce a
distributed-memory parallel algorithm based on an existing direct solver named
``strong recursive skeletonization factorization.'' The analysis of its
parallel scalability applies generally to a class of existing methods that
exploit the so-called strong admissibility. Specifically, we apply low-rank
compression to certain off-diagonal matrix blocks in a way that minimizes data
movement. Given a compression tolerance, our method constructs an approximate
factorization of a discretized integral operator (dense matrix), which can be
used to solve linear systems efficiently in parallel. Compared to iterative
algorithms, our method is particularly suitable for problems involving
ill-conditioned matrices or multiple right-hand sides. Large-scale numerical
experiments are presented to demonstrate the performance of our implementation
using the Julia language.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15460" title="Abstract">arXiv:2310.15460</a> [<a href="/pdf/2310.15460" title="Download PDF">pdf</a>, <a href="/format/2310.15460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HL-DPoS: An Enhanced Anti-Long-Range Attack DPoS Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+C">Chunhe Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianbo Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The consensus algorithm is crucial in blockchain for ensuring the validity
and security of transactions across the decentralized network. However,
achieving consensus among nodes and packaging blocks in blockchain networks is
a complex task that requires efficient and secure consensus algorithms. The
DPoS consensus algorithm has emerged as a popular choice due to its fast
transaction processing and high throughput. Despite these advantages, the
algorithm still suffers from weaknesses such as centralization and
vulnerability to long-range attacks, which can compromise the integrity of the
blockchain network.
<br />To combat these problems, we developed an Enhanced Anti-Long-Range Attack
DPoS algorithm (HL-DPoS). First, we split nodes into pieces to reduce
centralization issues while giving witness nodes the power to report and
benefit from malicious node's reports, maintaining high efficiency and high
security. Second, we propose a validation method in HL-DPoS that compares
consensuses transactions with the longest chain to detect long-range attacks.
Algorithm analysis and simulation experiment results demonstrate that our
HL-DPoS consensus algorithm improves security while achieving better consensus
performance.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15461" title="Abstract">arXiv:2310.15461</a> [<a href="/pdf/2310.15461" title="Download PDF">pdf</a>, <a href="/format/2310.15461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Facilitating Self-Guided Mental Health Interventions Through  Human-Language Model Interaction: A Case Study of Cognitive Restructuring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Ashish Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Rushton%2C+K">Kevin Rushton</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+I+W">Inna Wanyin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Theresa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Althoff%2C+T">Tim Althoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Self-guided mental health interventions, such as "do-it-yourself" tools to
learn and practice coping strategies, show great promise to improve access to
mental health care. However, these interventions are often cognitively
demanding and emotionally triggering, creating accessibility barriers that
limit their wide-scale implementation and adoption. In this paper, we study how
human-language model interaction can support self-guided mental health
interventions. We take cognitive restructuring, an evidence-based therapeutic
technique to overcome negative thinking, as a case study. In an IRB-approved
randomized field study on a large mental health website with 15,531
participants, we design and evaluate a system that uses language models to
support people through various steps of cognitive restructuring. Our findings
reveal that our system positively impacts emotional intensity for 67% of
participants and helps 65% overcome negative thoughts. Although adolescents
report relatively worse outcomes, we find that tailored interventions that
simplify language model generations improve overall effectiveness and equity.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15463" title="Abstract">arXiv:2310.15463</a> [<a href="/pdf/2310.15463" title="Download PDF">pdf</a>, <a href="/format/2310.15463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nested Control Co-design of a Spar Buoy Horizontal-axis Floating  Offshore Wind Turbine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bayat%2C+S">Saeid Bayat</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+Y+H">Yong Hoon Lee</a>, 
<a href="/search/eess?searchtype=author&query=Allison%2C+J+T">James T. Allison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Floating offshore wind turbine (FOWT) systems involve several coupled
physical analysis disciplines, including aeroelasticity, multi-body structural
dynamics, hydrodynamics, and controls. Conventionally, physical structure
(plant) and control design decisions are treated as two separate problems, and
generally, control design is performed after the plant design is complete.
However, this sequential design approach cannot fully capitalize upon the
synergy between plant and control design decisions. These conventional design
practices produce suboptimal designs, especially in cases with strong coupling
between plant and control design decisions. Control co-design (CCD) is a
holistic design approach that accounts fully for plant-control design coupling
by optimizing these decisions simultaneously. CCD is especially advantageous
for system design problems with complex interactions between physics
disciplines, which is the case for FOWT systems. This paper presents and
demonstrates a nested CCD approach using open-loop optimal control (OLOC) for a
simplified reduced-order model that simulates FOWT dynamic behavior. This
simplified model is helpful for optimization studies due to its computational
efficiency, but is still sufficiently rich enough to capture important
multidisciplinary physics couplings and plant-control design coupling
associated with a horizontal-axis FOWT system with a spar buoy floating
platform. The CCD result shows an improvement in the objective function, annual
energy production (AEP), compared to the baseline design by more than eleven
percent. Optimization studies at this fidelity level can provide system design
engineers with insights into design directions that leverage design coupling to
improve performance. These studies also provide a template for future more
detailed turbine CCD optimization studies that utilize higher fidelity models
and design representations.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15464" title="Abstract">arXiv:2310.15464</a> [<a href="/pdf/2310.15464" title="Download PDF">pdf</a>, <a href="/format/2310.15464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Answers to Yes-No Questions in User-Generated Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mathur%2C+S">Shivam Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K+H">Keun Hee Park</a>, 
<a href="/search/cs?searchtype=author&query=Chinnappa%2C+D">Dhivya Chinnappa</a>, 
<a href="/search/cs?searchtype=author&query=Kotamraju%2C+S">Saketh Kotamraju</a>, 
<a href="/search/cs?searchtype=author&query=Blanco%2C+E">Eduardo Blanco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Interpreting answers to yes-no questions in social media is difficult. Yes
and no keywords are uncommon, and the few answers that include them are rarely
to be interpreted what the keywords suggest. In this paper, we present a new
corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss
linguistic characteristics of answers whose interpretation is yes or no, as
well as answers whose interpretation is unknown. We show that large language
models are far from solving this problem, even after fine-tuning and blending
other corpora for the same problem but outside social media.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15466" title="Abstract">arXiv:2310.15466</a> [<a href="/pdf/2310.15466" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EKGNet: A 10.96&#x3bc;W Fully Analog Neural Network for Intra-Patient  Arrhythmia Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haghi%2C+B">Benyamin Haghi</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lale%2C+S">Sahin Lale</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>, 
<a href="/search/cs?searchtype=author&query=Emami%2C+A">Azita Emami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted on IEEE Biomedical Circuits and Systems (BioCAS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present an integrated approach by combining analog computing and deep
learning for electrocardiogram (ECG) arrhythmia classification. We propose
EKGNet, a hardware-efficient and fully analog arrhythmia classification
architecture that archives high accuracy with low power consumption. The
proposed architecture leverages the energy efficiency of transistors operating
in the subthreshold region, eliminating the need for analog-to-digital
converters (ADC) and static random access memory (SRAM). The system design
includes a novel analog sequential Multiply-Accumulate (MAC) circuit that
mitigates process, supply voltage, and temperature variations. Experimental
evaluations on PhysioNet's MIT-BIH and PTB Diagnostics datasets demonstrate the
effectiveness of the proposed method, achieving average balanced accuracy of
95% and 94.25% for intra-patient arrhythmia classification and myocardial
infarction (MI) classification, respectively. This innovative approach presents
a promising avenue for developing low-power arrhythmia classification systems
with enhanced accuracy and transferability in biomedical applications.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15468" title="Abstract">arXiv:2310.15468</a> [<a href="/pdf/2310.15468" title="Download PDF">pdf</a>, <a href="/ps/2310.15468" title="Download PostScript">ps</a>, <a href="/format/2310.15468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Distributed Solutions in Renewable Energy Systems and Grid  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+M">Mohammad Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+A">Ali Mohammadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">This study delves into the shift from centralized to decentralized approaches
in the electricity industry, with a particular focus on how machine learning
(ML) advancements play a crucial role in empowering renewable energy sources
and improving grid management. ML models have become increasingly important in
predicting renewable energy generation and consumption, utilizing various
techniques like artificial neural networks, support vector machines, and
decision trees. Furthermore, data preprocessing methods, such as data
splitting, normalization, decomposition, and discretization, are employed to
enhance prediction accuracy.
<br />The incorporation of big data and ML into smart grids offers several
advantages, including heightened energy efficiency, more effective responses to
demand, and better integration of renewable energy sources. Nevertheless,
challenges like handling large data volumes, ensuring cybersecurity, and
obtaining specialized expertise must be addressed. The research investigates
various ML applications within the realms of solar energy, wind energy, and
electric distribution and storage, illustrating their potential to optimize
energy systems. To sum up, this research demonstrates the evolving landscape of
the electricity sector as it shifts from centralized to decentralized solutions
through the application of ML innovations and distributed decision-making,
ultimately shaping a more efficient and sustainable energy future.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15469" title="Abstract">arXiv:2310.15469</a> [<a href="/pdf/2310.15469" title="Download PDF">pdf</a>, <a href="/format/2310.15469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Janus Interface: How Fine-Tuning in Large Language Models Amplifies  the Privacy Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siyuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Rui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shijun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Liya Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">XiaoFeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Haixu Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The era post-2018 marked the advent of Large Language Models (LLMs), with
innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess.
As the industry galloped toward augmenting model parameters and capitalizing on
vast swaths of human language data, security and privacy challenges also
emerged. Foremost among these is the potential inadvertent accrual of Personal
Identifiable Information (PII) during web-based data acquisition, posing risks
of unintended PII disclosure. While strategies like RLHF during training and
Catastrophic Forgetting have been marshaled to control the risk of privacy
infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning
interface for GPT-3.5, have reignited concerns. One may ask: can the
fine-tuning of LLMs precipitate the leakage of personal information embedded
within training datasets? This paper reports the first endeavor to seek the
answer to the question, particularly our discovery of a new LLM exploitation
avenue, called the Janus attack. In the attack, one can construct a PII
association task, whereby an LLM is fine-tuned using a minuscule PII dataset,
to potentially reinstate and reveal concealed PIIs. Our findings indicate that,
with a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from
being impermeable to PII extraction to a state where they divulge a substantial
proportion of concealed PII. This research, through its deep dive into the
Janus attack vector, underscores the imperative of navigating the intricate
interplay between LLM utility and privacy preservation.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15470" title="Abstract">arXiv:2310.15470</a> [<a href="/pdf/2310.15470" title="Download PDF">pdf</a>, <a href="/format/2310.15470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Event Extraction with Semantic Confusion Rectification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zitao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wei Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study continual event extraction, which aims to extract incessantly
emerging event information while avoiding forgetting. We observe that the
semantic confusion on event types stems from the annotations of the same text
being updated over time. The imbalance between event types even aggravates this
issue. This paper proposes a novel continual event extraction model with
semantic confusion rectification. We mark pseudo labels for each sentence to
alleviate semantic confusion. We transfer pivotal knowledge between current and
previous models to enhance the understanding of event types. Moreover, we
encourage the model to focus on the semantics of long-tailed event types by
leveraging other associated types. Experimental results show that our model
outperforms state-of-the-art baselines and is proficient in imbalanced
datasets.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15471" title="Abstract">arXiv:2310.15471</a> [<a href="/pdf/2310.15471" title="Download PDF">pdf</a>, <a href="/format/2310.15471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Path Bound for DAG Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qingqiang He</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+N">Nan Guan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+M">Mingsong Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">This paper studies the response time bound of a DAG (directed acyclic graph)
task. Recently, the idea of using multiple paths to bound the response time of
a DAG task, instead of using a single longest path in previous results, was
proposed and leads to the so-called multi-path bound. Multi-path bounds can
greatly reduce the response time bound and significantly improve the
schedulability of DAG tasks. This paper derives a new multi-path bound and
proposes an optimal algorithm to compute this bound. We further present a
systematic analysis on the dominance and the sustainability of three existing
multi-path bounds and the proposed multi-path bound. Our bound theoretically
dominates and empirically outperforms all existing multi-path bounds. What's
more, the proposed bound is the only multi-path bound that is proved to be
self-sustainable.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15472" title="Abstract">arXiv:2310.15472</a> [<a href="/pdf/2310.15472" title="Download PDF">pdf</a>, <a href="/format/2310.15472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Survival Analysis for Heart Failure Risk Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Ness%2C+M">Mike Van Ness</a>, 
<a href="/search/cs?searchtype=author&query=Bosschieter%2C+T">Tomas Bosschieter</a>, 
<a href="/search/cs?searchtype=author&query=Din%2C+N">Natasha Din</a>, 
<a href="/search/cs?searchtype=author&query=Ambrosy%2C+A">Andrew Ambrosy</a>, 
<a href="/search/cs?searchtype=author&query=Sandhu%2C+A">Alexander Sandhu</a>, 
<a href="/search/cs?searchtype=author&query=Udell%2C+M">Madeleine Udell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Survival analysis, or time-to-event analysis, is an important and widespread
problem in healthcare research. Medical research has traditionally relied on
Cox models for survival analysis, due to their simplicity and interpretability.
Cox models assume a log-linear hazard function as well as proportional hazards
over time, and can perform poorly when these assumptions fail. Newer survival
models based on machine learning avoid these assumptions and offer improved
accuracy, yet sometimes at the expense of model interpretability, which is
vital for clinical use. We propose a novel survival analysis pipeline that is
both interpretable and competitive with state-of-the-art survival models.
Specifically, we use an improved version of survival stacking to transform a
survival analysis problem to a classification problem, ControlBurn to perform
feature selection, and Explainable Boosting Machines to generate interpretable
predictions. To evaluate our pipeline, we predict risk of heart failure using a
large-scale EHR database. Our pipeline achieves state-of-the-art performance
and provides interesting and novel insights about risk factors for heart
failure.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15477" title="Abstract">arXiv:2310.15477</a> [<a href="/pdf/2310.15477" title="Download PDF">pdf</a>, <a href="/format/2310.15477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without  Full Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+B">Biqing Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xuekai Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xinwei Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bowen Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction tuning has recently been recognized as an effective way of
aligning Large Language Models (LLMs) to enhance their generalization ability
across various tasks. However, when tuning publicly accessible, centralized
LLMs with private instruction data, privacy concerns are inevitable. While
direct transfer of parameterized modules between models is a plausible approach
to address this, its implications and effectiveness need further exploration.
This paper focuses on Offsite-Tuning (OFT), a representative technique that
transfers transformer blocks between centralized LLMs and downstream emulators.
Given the limited understanding of the underlying mechanism of OFT, we perform
an empirical analysis on LLMs from the perspectives of representation and
functional similarity. Interestingly, our findings reveal a unique modular
structure within the layers of LLMs that appears to emerge as the model size
expands. Simultaneously, we note subtle but potentially significant changes in
representation and intermediate predictions across the layers. Inspired by
these observations, we propose CRaSh, involving Clustering, Removing, and
Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh
significantly boosts performance of OFT with billions of parameters.
Furthermore, we investigate the optimal solutions yielded by fine-tuning with
and without full model through the lens of loss landscape. Our findings
demonstrate a linear connectivity among these optima falling over the same
basin, thereby highlighting the effectiveness of CRaSh and OFT. The source code
is publicly available at https://github.com/TsinghuaC3I/CRaSh.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15482" title="Abstract">arXiv:2310.15482</a> [<a href="/pdf/2310.15482" title="Download PDF">pdf</a>, <a href="/format/2310.15482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Salient Object Detection in RGB-D Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mou%2C+A">Ao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yukang Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiahao He</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+D">Dingyao Min</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+K">Keren Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qijun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Given the widespread adoption of depth-sensing acquisition devices, RGB-D
videos and related data/media have gained considerable traction in various
aspects of daily life. Consequently, conducting salient object detection (SOD)
in RGB-D videos presents a highly promising and evolving avenue. Despite the
potential of this area, SOD in RGB-D videos remains somewhat under-explored,
with RGB-D SOD and video SOD (VSOD) traditionally studied in isolation. To
explore this emerging field, this paper makes two primary contributions: the
dataset and the model. On one front, we construct the RDVS dataset, a new RGB-D
VSOD dataset with realistic depth and characterized by its diversity of scenes
and rigorous frame-by-frame annotations. We validate the dataset through
comprehensive attribute and object-oriented analyses, and provide training and
testing splits. Moreover, we introduce DCTNet+, a three-stream network tailored
for RGB-D VSOD, with an emphasis on RGB modality and treats depth and optical
flow as auxiliary modalities. In pursuit of effective feature enhancement,
refinement, and fusion for precise final prediction, we propose two modules:
the multi-modal attention module (MAM) and the refinement fusion module (RFM).
To enhance interaction and fusion within RFM, we design a universal interaction
module (UIM) and then integrate holistic multi-modal attentive paths (HMAPs)
for refining multi-modal low-level features before reaching RFMs. Comprehensive
experiments, conducted on pseudo RGB-D video datasets alongside our RDVS,
highlight the superiority of DCTNet+ over 17 VSOD models and 14 RGB-D SOD
models. Ablation experiments were performed on both pseudo and realistic RGB-D
video datasets to demonstrate the advantages of individual modules as well as
the necessity of introducing realistic depth. Our code together with RDVS
dataset will be available at https://github.com/kerenfu/RDVS/.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15484" title="Abstract">arXiv:2310.15484</a> [<a href="/pdf/2310.15484" title="Download PDF">pdf</a>, <a href="/format/2310.15484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+H+K">Hyeong Kyu Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seunghun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Jaewon Chu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyunwoo J. Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neural Information Processing Systems (NeurIPS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves
retrieving nodes from a knowledge graph (KG) to answer natural language
questions. Recent GNN-based approaches formulate this task as a KG path
searching problem, where messages are sequentially propagated from the seed
node towards the answer nodes. However, these messages are past-oriented, and
they do not consider the full KG context. To make matters worse, KG nodes often
represent proper noun entities and are sometimes encrypted, being uninformative
in selecting between paths. To address these problems, we propose Neural Tree
Search (NuTrea), a tree search-based GNN model that incorporates the broader KG
context. Our model adopts a message-passing scheme that probes the unreached
subtree regions to boost the past-oriented embeddings. In addition, we
introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node
embedding that considers the global KG context to better characterize ambiguous
KG nodes. The general effectiveness of our approach is demonstrated through
experiments on three major multi-hop KGQA benchmark datasets, and our extensive
analyses further validate its expressiveness and robustness. Overall, NuTrea
provides a powerful means to query the KG with complex natural language
questions. Code is available at https://github.com/mlvlab/NuTrea.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15486" title="Abstract">arXiv:2310.15486</a> [<a href="/pdf/2310.15486" title="Download PDF">pdf</a>, <a href="/format/2310.15486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-based IMT-2030 Testbed for MmWave Multi-stream Ultra-massive MIMO  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shuhao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+B">Boya Di</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiahao Gao</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+S">Shaohua Yue</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+R">Rui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiaqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haobo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shaohui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haichao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+X">Xin Su</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Lingyang Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, to be published in IEEE Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">As one enabling technique of the future sixth generation (6G) network,
ultra-massive multiple-input-multiple-output (MIMO) can support high-speed data
transmissions and cell coverage extension. However, it is hard to realize the
ultra-massive MIMO via traditional phased arrays due to unacceptable power
consumption. To address this issue, reconfigurable intelligent surface-based
(RIS-based) antennas are an energy-efficient enabler of the ultra-massive MIMO,
since they are free of energy-hungry phase shifters. In this article, we report
the performances of the RIS-enabled ultra-massive MIMO via a project called
Verification of MmWave Multi-stream Transmissions Enabled by RIS-based
Ultra-massive MIMO for 6G (V4M), which was proposed to promote the evolution
towards IMT-2030. In the V4M project, we manufacture RIS-based antennas with
1024 one-bit elements working at 26 GHz, based on which an mmWave dual-stream
ultra-massive MIMO prototype is implemented for the first time. To approach
practical settings, the Tx and Rx of the prototype are implemented by one
commercial new radio base station and one off-the-shelf user equipment,
respectively. The measured data rate of the dual-stream prototype approaches
the theoretical peak rate. Our contributions to the V4M project are also
discussed by presenting technological challenges and corresponding solutions.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15492" title="Abstract">arXiv:2310.15492</a> [<a href="/pdf/2310.15492" title="Download PDF">pdf</a>, <a href="/format/2310.15492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Representation Learning for Unified Online Top-K Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+M">Minfang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuchen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Huihui Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziru Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanlin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lixia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Han Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuning Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bo Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, submitted to ICDE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In large-scale industrial e-commerce, the efficiency of an online
recommendation system is crucial in delivering highly relevant item/content
advertising that caters to diverse business scenarios. However, most existing
studies focus solely on item advertising, neglecting the significance of
content advertising. This oversight results in inconsistencies within the
multi-entity structure and unfair retrieval. Furthermore, the challenge of
retrieving top-k advertisements from multi-entity advertisements across
different domains adds to the complexity. Recent research proves that
user-entity behaviors within different domains exhibit characteristics of
differentiation and homogeneity. Therefore, the multi-domain matching models
typically rely on the hybrid-experts framework with domain-invariant and
domain-specific representations. Unfortunately, most approaches primarily focus
on optimizing the combination mode of different experts, failing to address the
inherent difficulty in optimizing the expert modules themselves. The existence
of redundant information across different domains introduces interference and
competition among experts, while the distinct learning objectives of each
domain lead to varying optimization challenges among experts. To tackle these
issues, we propose robust representation learning for the unified online top-k
recommendation. Our approach constructs unified modeling in entity space to
ensure data fairness. The robust representation learning employs domain
adversarial learning and multi-view wasserstein distribution learning to learn
robust representations. Moreover, the proposed method balances conflicting
objectives through the homoscedastic uncertainty weights and orthogonality
constraints. Various experiments validate the effectiveness and rationality of
our proposed method, which has been successfully deployed online to serve real
business scenarios.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15494" title="Abstract">arXiv:2310.15494</a> [<a href="/pdf/2310.15494" title="Download PDF">pdf</a>, <a href="/format/2310.15494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRAMS: Training-free Memory Selection for Long-range Language Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haofei Yu</a>, 
<a href="/search/cs?searchtype=author&query=wang%2C+C">Cunxiang wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The Transformer architecture is crucial for numerous AI models, but it still
faces challenges in long-range language modeling. Though several specific
transformer architectures have been designed to tackle issues of long-range
dependencies, existing methods like Transformer-XL are plagued by a high
percentage of ineffective memories. In this study, we present a plug-and-play
strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens
participating in attention calculation based on one simple metric. This
strategy allows us to keep tokens that are likely to have a high attention
score with the current queries and ignore the other ones. We have tested our
approach on the word-level benchmark (WikiText-103) and the character-level
benchmark (enwik8), and the results indicate an improvement without having
additional training or adding additional parameters.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15495" title="Abstract">arXiv:2310.15495</a> [<a href="/pdf/2310.15495" title="Download PDF">pdf</a>, <a href="/format/2310.15495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMG: Automated Efficient Approximate Multiplier Generator for FPGAs via  Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingli Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, accepted by 2023 International Conference on Field-Programmable Technology (ICFPT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Approximate computing is a promising approach to reduce the power, delay, and
area in hardware design for many error-resilient applications such as machine
learning (ML) and digital signal processing (DSP) systems, in which multipliers
usually are key arithmetic units. Due to the underlying architectural
differences between ASICs and FPGAs, existing ASIC-based approximate
multipliers do not offer symmetrical gains when they are implemented by FPGA
resources. In this paper, we propose AMG, an open-source automated approximate
multiplier generator for FPGAs driven by Bayesian optimization (BO) with
parallel evaluation. The proposed method simplifies the exact half adders (HAs)
for the initial partial product (PP) compression in a multiplier while
preserving coarse-grained additions for the following accumulation. The
generated multipliers can be effectively mapped to lookup tables (LUTs) and
carry chains provided by modern FPGAs, reducing hardware costs with acceptable
errors. Compared with 1167 multipliers from previous works, our generated
multipliers can form a Pareto front with 28.70%-38.47% improvements in terms of
the product of hardware cost and error on average. All source codes, reproduced
multipliers, and our generated multipliers are available at
https://github.com/phyzhenli/AMG.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15500" title="Abstract">arXiv:2310.15500</a> [<a href="/pdf/2310.15500" title="Download PDF">pdf</a>, <a href="/format/2310.15500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-split configuration design for fluid-based thermal management  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bayat%2C+S">Saeid Bayat</a>, 
<a href="/search/eess?searchtype=author&query=Shahmansouri%2C+N">Nastaran Shahmansouri</a>, 
<a href="/search/eess?searchtype=author&query=Peddada%2C+S+R">Satya RT Peddada</a>, 
<a href="/search/eess?searchtype=author&query=Tessier%2C+A">Alexander Tessier</a>, 
<a href="/search/eess?searchtype=author&query=Butscher%2C+A">Adrian Butscher</a>, 
<a href="/search/eess?searchtype=author&query=Allison%2C+J+T">James T Allison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">High power density systems require efficient cooling to maintain their
thermal performance. Despite this, as systems get larger and more complex,
human practice and insight may not suffice to determine the desired thermal
management system designs. To this end, a framework for automatic architecture
exploration is presented in this article for a class of single-phase,
multi-split cooling systems. For this class of systems, heat generation devices
are clustered based on their spatial information, and flow-split are added only
when required and at the location of heat devices. To generate different
architectures, candidate architectures are represented as graphs. From these
graphs, dynamic physics models are created automatically using a graph-based
thermal modeling framework. Then, an optimal fluid flow distribution problem is
solved by addressing temperature constraints in the presence of exogenous heat
loads to achieve optimal performance. The focus in this work is on the design
of general multi-split heat management systems. The architectures discussed
here can be used for various applications in the domain of configuration
design. The multi-split algorithm can produce configurations where splitting
can occur at any of the vertices. The results presented include 3 categories of
cases and are discussed in detail.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15504" title="Abstract">arXiv:2310.15504</a> [<a href="/pdf/2310.15504" title="Download PDF">pdf</a>, <a href="/ps/2310.15504" title="Download PostScript">ps</a>, <a href="/format/2310.15504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-view Self-localization from Synthesized Scene-graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamamoto%2C+R">Ryogo Yamamoto</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+K">Kanji Tanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Cross-view self-localization is a challenging scenario of visual place
recognition in which database images are provided from sparse viewpoints.
Recently, an approach for synthesizing database images from unseen viewpoints
using NeRF (Neural Radiance Fields) technology has emerged with impressive
performance. However, synthesized images provided by these techniques are often
of lower quality than the original images, and furthermore they significantly
increase the storage cost of the database. In this study, we explore a new
hybrid scene model that combines the advantages of view-invariant appearance
features computed from raw images and view-dependent spatial-semantic features
computed from synthesized images. These two types of features are then fused
into scene graphs, and compressively learned and recognized by a graph neural
network. The effectiveness of the proposed method was verified using a novel
cross-view self-localization dataset with many unseen views generated using a
photorealistic Habitat simulator.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15505" title="Abstract">arXiv:2310.15505</a> [<a href="/pdf/2310.15505" title="Download PDF">pdf</a>, <a href="/format/2310.15505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Quantum Tortoise and the Classical Hare: A simple framework for  understanding which problems quantum computing will accelerate (and which it  will not)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sukwoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Moses%2C+W+S">William S. Moses</a>, 
<a href="/search/cs?searchtype=author&query=Thompson%2C+N">Neil Thompson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Emerging Technologies (cs.ET); General Economics (econ.GN); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Quantum computing promises transformational gains for solving some problems,
but little to none for others. For anyone hoping to use quantum computers now
or in the future, it is important to know which problems will benefit. In this
paper, we introduce a framework for answering this question both intuitively
and quantitatively. The underlying structure of the framework is a race between
quantum and classical computers, where their relative strengths determine when
each wins. While classical computers operate faster, quantum computers can
sometimes run more efficient algorithms. Whether the speed advantage or the
algorithmic advantage dominates determines whether a problem will benefit from
quantum computing or not. Our analysis reveals that many problems, particularly
those of small to moderate size that can be important for typical businesses,
will not benefit from quantum computing. Conversely, larger problems or those
with particularly big algorithmic gains will benefit from near-term quantum
computing. Since very large algorithmic gains are rare in practice and
theorized to be rare even in principle, our analysis suggests that the benefits
from quantum computing will flow either to users of these rare cases, or
practitioners processing very large data.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15506" title="Abstract">arXiv:2310.15506</a> [<a href="/pdf/2310.15506" title="Download PDF">pdf</a>, <a href="/format/2310.15506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology Optimization with Text-Guided Stylization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Shengze Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Punpongsanon%2C+P">Parinya Punpongsanon</a>, 
<a href="/search/cs?searchtype=author&query=Iwai%2C+D">Daisuke Iwai</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+K">Kosuke Sato</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">We propose an approach for the generation of topology-optimized structures
with text-guided appearance stylization. This methodology aims to enrich the
concurrent design of a structure's physical functionality and aesthetic
appearance. Users can effortlessly input descriptive text to govern the style
of the structure. Our system employs a hash-encoded neural network as the
implicit structure representation backbone, which serves as the foundation for
the co-optimization of structural mechanical performance, style, and
connectivity, to ensure full-color, high-quality 3D-printable solutions. We
substantiate the effectiveness of our system through extensive comparisons,
demonstrations, and a 3D printing test.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15511" title="Abstract">arXiv:2310.15511</a> [<a href="/pdf/2310.15511" title="Download PDF">pdf</a>, <a href="/format/2310.15511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KITAB: Evaluating LLMs on Constraint Satisfaction for Information  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdin%2C+M+I">Marah I Abdin</a>, 
<a href="/search/cs?searchtype=author&query=Gunasekar%2C+S">Suriya Gunasekar</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+V">Varun Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jerry Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuksekgonul%2C+M">Mert Yuksekgonul</a>, 
<a href="/search/cs?searchtype=author&query=Peshawaria%2C+R+G">Rahee Ghosh Peshawaria</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+R">Ranjita Naik</a>, 
<a href="/search/cs?searchtype=author&query=Nushi%2C+B">Besmira Nushi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">We study the ability of state-of-the art models to answer constraint
satisfaction queries for information retrieval (e.g., 'a list of ice cream
shops in San Diego'). In the past, such queries were considered to be tasks
that could only be solved via web-search or knowledge bases. More recently,
large language models (LLMs) have demonstrated initial emergent abilities in
this task. However, many current retrieval benchmarks are either saturated or
do not measure constraint satisfaction. Motivated by rising concerns around
factual incorrectness and hallucinations of LLMs, we present KITAB, a new
dataset for measuring constraint satisfaction abilities of language models.
KITAB consists of book-related data across more than 600 authors and 13,000
queries, and also offers an associated dynamic data collection and constraint
verification approach for acquiring similar test data for other authors. Our
extended experiments on GPT4 and GPT3.5 characterize and decouple common
failure modes across dimensions such as information popularity, constraint
types, and context availability. Results show that in the absence of context,
models exhibit severe limitations as measured by irrelevant information,
factual errors, and incompleteness, many of which exacerbate as information
popularity decreases. While context availability mitigates irrelevant
information, it is not helpful for satisfying constraints, identifying
fundamental barriers to constraint satisfaction. We open source our
contributions to foster further research on improving constraint satisfaction
abilities of future models.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15513" title="Abstract">arXiv:2310.15513</a> [<a href="/pdf/2310.15513" title="Download PDF">pdf</a>, <a href="/format/2310.15513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Joint Matrix Factorization Analysis of Multilingual Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ziser%2C+Y">Yftah Ziser</a>, 
<a href="/search/cs?searchtype=author&query=Webber%2C+B">Bonnie Webber</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present an analysis tool based on joint matrix factorization for comparing
latent representations of multilingual and monolingual models. An alternative
to probing, this tool allows us to analyze multiple sets of representations in
a joint manner. Using this tool, we study to what extent and how
morphosyntactic features are reflected in the representations learned by
multilingual pre-trained models. We conduct a large-scale empirical study of
over 33 languages and 17 morphosyntactic categories. Our findings demonstrate
variations in the encoding of morphosyntactic information across upper and
lower layers, with category-specific differences influenced by language
properties. Hierarchical clustering of the factorization outputs yields a tree
structure that is related to phylogenetic trees manually crafted by linguists.
Moreover, we find the factorization outputs exhibit strong associations with
performance observed across different cross-lingual tasks. We release our code
to facilitate future research.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15515" title="Abstract">arXiv:2310.15515</a> [<a href="/pdf/2310.15515" title="Download PDF">pdf</a>, <a href="/format/2310.15515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting  Elusive Disinformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lucas%2C+J">Jason Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Uchendu%2C+A">Adaku Uchendu</a>, 
<a href="/search/cs?searchtype=author&query=Yamashita%2C+M">Michiharu Yamashita</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jooyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Rohatgi%2C+S">Shaurya Rohatgi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel "Fighting Fire with Fire" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15516" title="Abstract">arXiv:2310.15516</a> [<a href="/pdf/2310.15516" title="Download PDF">pdf</a>, <a href="/format/2310.15516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Attention-based Deep Reinforcement Learning for solving the  Chinese Postman Problem with Load-dependent costs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+C+D">Cong Dao Tran</a>, 
<a href="/search/cs?searchtype=author&query=Hy%2C+T+S">Truong Son Hy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, Deep reinforcement learning (DRL) models have shown promising
results in solving routing problems. However, most DRL solvers are commonly
proposed to solve node routing problems, such as the Traveling Salesman Problem
(TSP). Meanwhile, there has been limited research on applying neural methods to
arc routing problems, such as the Chinese Postman Problem (CPP), since they
often feature irregular and complex solution spaces compared to TSP. To fill
these gaps, this paper proposes a novel DRL framework to address the CPP with
load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc
routing problem with load constraints. The novelty of our method is two-fold.
First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential
model. Subsequently, we introduce an autoregressive model based on DRL, namely
Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge
effectively. Such a framework allows the DRL model to work efficiently and
scalably to arc routing problems. Furthermore, we propose a new bio-inspired
meta-heuristic solution based on Evolutionary Algorithm (EA) for CPP-LC.
Extensive experiments show that Arc-DRL outperforms existing meta-heuristic
methods such as Iterative Local Search (ILS) and Variable Neighborhood Search
(VNS) proposed by (Corberan et al., 2018) on large benchmark datasets for
CPP-LC regarding both solution quality and running time; while the EA gives the
best solution quality with much more running time. We release our C++
implementations for metaheuristics such as EA, ILS and VNS along with the code
for data generation and our generated data at
https://github.com/HySonLab/Chinese_Postman_Problem
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15517" title="Abstract">arXiv:2310.15517</a> [<a href="/pdf/2310.15517" title="Download PDF">pdf</a>, <a href="/format/2310.15517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MarkQA: A large scale KBQA dataset with numerical reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sitao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yuheng Bao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shanshan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yuzhong Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> camera ready for EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While question answering over knowledge bases (KBQA) has shown progress in
addressing factoid questions, KBQA with numerical reasoning remains relatively
unexplored. In this paper, we focus on the complex numerical reasoning in KBQA
and propose a new task, NR-KBQA, which necessitates the ability to perform both
multi-hop reasoning and numerical reasoning. We design a logic form in Python
format called PyQL to represent the reasoning process of numerical reasoning
questions. To facilitate the development of NR-KBQA, we present a large dataset
called MarkQA, which is automatically constructed from a small set of seeds.
Each question in MarkQA is equipped with its corresponding SPARQL query,
alongside the step-by-step reasoning process in the QDMR format and PyQL
program. Experimental results of some state-of-the-art QA methods on the MarkQA
show that complex numerical reasoning in KBQA faces great challenges.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15518" title="Abstract">arXiv:2310.15518</a> [<a href="/pdf/2310.15518" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Fungible Token Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McKinney%2C+R">Ryleigh McKinney</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sundar Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Non-fungible tokens (NFTs) are unique digital assets stored on the blockchain
and is used to certify ownership and authenticity of the digital asset. NFTs
were first created in 2014 while their popularity peaked between 2021 and 2022.
In this paper, the authors dive into the world of Non-Fungible Tokens (NFTs),
their history, the Future of NFTs, as well as the security concerns.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15519" title="Abstract">arXiv:2310.15519</a> [<a href="/pdf/2310.15519" title="Download PDF">pdf</a>, <a href="/format/2310.15519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covert communication with Gaussian noise: from random access channel to  point-to-point channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+M">Masahito Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Vazquez-Castro%2C+A">Angeles Vazquez-Castro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We propose a covert communication protocol for the spread-spectrum multiple
random access with additive white Gaussian noise (AWGN) channel. No existing
paper has studied covert communication for the random access channel. Our
protocol assumes binary discrete phase-shift keying (BPSK) modulation, and it
works well under imperfect channel state information (I-CSI) for both the
legitimate and adversary receivers, which is a realistic assumption in the low
power regime. Also, our method assumes that the legitimate users share secret
variables in a similar way as the preceding studies. Although several studies
investigated the covert communication for the point-to-point communication, no
existing paper considers the covert communication under the above uncertainty
assumption even for point-to-point communication. Our protocol under the above
uncertainty assumption allows O(n) legitimate senders and O(n/log n) active
legitimate senders. Furthermore, our protocol can be converted to a protocol
for point-to-point communication that works under the above uncertainty
assumption.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15523" title="Abstract">arXiv:2310.15523</a> [<a href="/pdf/2310.15523" title="Download PDF">pdf</a>, <a href="/format/2310.15523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative and Contrastive Paradigms Are Complementary for Graph  Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chuang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+F">Fangcheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+S">Shuo Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiawei Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows
the generative paradigm and learns to reconstruct masked graph edges or node
features. Contrastive Learning (CL) maximizes the similarity between augmented
views of the same graph and is widely used for GSSL. However, MAE and CL are
considered separately in existing works for GSSL. We observe that the MAE and
CL paradigms are complementary and propose the graph contrastive masked
autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local
edges or node features, MAE cannot capture global information of the graph and
is sensitive to particular edges and features. On the contrary, CL excels in
extracting global information because it considers the relation between graphs.
As such, we equip GCMAE with an MAE branch and a CL branch, and the two
branches share a common encoder, which allows the MAE branch to exploit the
global information extracted by the CL branch. To force GCMAE to capture global
graph structures, we train it to reconstruct the entire adjacency matrix
instead of only the masked edges as in existing works. Moreover, a
discrimination loss is proposed for feature reconstruction, which improves the
disparity between node embeddings rather than reducing the reconstruction error
to tackle the feature smoothing problem of MAE. We evaluate GCMAE on four
popular graph tasks (i.e., node classification, node clustering, link
prediction, and graph classification) and compare with 14 state-of-the-art
baselines. The results show that GCMAE consistently provides good accuracy
across these tasks, and the maximum accuracy improvement is up to 3.2% compared
with the best-performing baseline.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15524" title="Abstract">arXiv:2310.15524</a> [<a href="/pdf/2310.15524" title="Download PDF">pdf</a>, <a href="/format/2310.15524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Inherent Privacy Properties of Discrete Denoising Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+R">Rongzhe Wei</a>, 
<a href="/search/cs?searchtype=author&query=Krea%C4%8Di%C4%87%2C+E">Eleonora Krea&#x10d;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Haoteng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Potluru%2C+V+K">Vamsi K. Potluru</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Privacy concerns have led to a surge in the creation of synthetic datasets,
with diffusion models emerging as a promising avenue. Although prior studies
have performed empirical evaluations on these models, there has been a gap in
providing a mathematical characterization of their privacy-preserving
capabilities. To address this, we present the pioneering theoretical
exploration of the privacy preservation inherent in discrete diffusion models
(DDMs) for discrete dataset generation. Focusing on per-instance differential
privacy (pDP), our framework elucidates the potential privacy leakage for each
data point in a given training dataset, offering insights into data
preprocessing to reduce privacy risks of the synthetic dataset generation via
DDMs. Our bounds also show that training with $s$-sized data points leads to a
surge in privacy leakage from $(\epsilon,
\mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon,
\mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pure
noise to the synthetic clean data phase, and a faster decay in diffusion
coefficients amplifies the privacy guarantee. Finally, we empirically verify
our theoretical findings on both synthetic and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15525" title="Abstract">arXiv:2310.15525</a> [<a href="/pdf/2310.15525" title="Download PDF">pdf</a>, <a href="/format/2310.15525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization of process parameters in additive manufacturing based on  the finite element method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+J">Jingyi Wang</a>, 
<a href="/search/math?searchtype=author&query=Papadopoulos%2C+P">Panayiotis Papadopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A design optimization framework for process parameters of additive
manufacturing based on finite element simulation is proposed. The finite
element method uses a coupled thermomechanical model developed for fused
deposition modeling from the authors' previous work. Both gradient-based and
gradient-free optimization methods are proposed. The gradient-based approach,
which solves a PDE-constrained optimization problem, requires sensitivities
computed from the fully discretized finite element model. We show the
derivation of the sensitivities and apply them in a projected gradient descent
algorithm. For the gradient-free approach, we propose two distinct algorithms:
a local search algorithm called the method of local variations and a Bayesian
optimization algorithm using Gaussian processes. To illustrate the
effectiveness and differences of the methods, we provide two-dimensional design
optimization examples using all three proposed algorithms.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15526" title="Abstract">arXiv:2310.15526</a> [<a href="/pdf/2310.15526" title="Download PDF">pdf</a>, <a href="/format/2310.15526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Amplification for Matrix Mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choquette-Choo%2C+C+A">Christopher A. Choquette-Choo</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+A">Arun Ganesh</a>, 
<a href="/search/cs?searchtype=author&query=Steinke%2C+T">Thomas Steinke</a>, 
<a href="/search/cs?searchtype=author&query=Thakurta%2C+A">Abhradeep Thakurta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Privacy amplification exploits randomness in data selection to provide
tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's
success in machine learning, but, is not readily applicable to the newer
state-of-the-art algorithms. This is because these algorithms, known as
DP-FTRL, use the matrix mechanism to add correlated noise instead of
independent noise as in DP-SGD.
<br />In this paper, we propose "MMCC", the first algorithm to analyze privacy
amplification via sampling for any generic matrix mechanism. MMCC is nearly
tight in that it approaches a lower bound as $\epsilon\to0$. To analyze
correlated outputs in MMCC, we prove that they can be analyzed as if they were
independent, by conditioning them on prior outputs. Our "conditional
composition theorem" has broad utility: we use it to show that the noise added
to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with
amplification. Our amplification algorithm also has practical empirical
utility: we show it leads to significant improvement in the privacy-utility
trade-offs for DP-FTRL algorithms on standard benchmarks.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15529" title="Abstract">arXiv:2310.15529</a> [<a href="/pdf/2310.15529" title="Download PDF">pdf</a>, <a href="/format/2310.15529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric Strategies for Multi-Access IoT Network Optimization: A Common  Information Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sudhakara%2C+S">Sagar Sudhakara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">In the context of IoT deployments, a multitude of devices concurrently
require network access to transmit data over a shared communication channel.
Employing symmetric strategies can effectively facilitate the collaborative use
of the communication medium among these devices. By adopting such strategies,
devices collectively optimize their transmission parameters, resulting in
minimized collisions and enhanced overall network throughput.
<br />Our primary focus centers on the formulation of symmetric (i.e., identical)
strategies for the sensors, aiming to optimize a finite horizon team objective.
The imposition of symmetric strategies introduces novel facets and complexities
into the team problem. To address this, we embrace the common information
approach and adapt it to accommodate the use of symmetric strategies. This
adaptation yields a dynamic programming framework grounded in common
information, wherein each step entails the minimization of a single function
mapping from an agent's private information space to the space of probability
distributions over possible actions.
<br />Our proposed policy/method incurs a reduced cumulative cost compared to other
methods employing symmetric strategies, a point substantiated by our simulation
results.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15533" title="Abstract">arXiv:2310.15533</a> [<a href="/pdf/2310.15533" title="Download PDF">pdf</a>, <a href="/format/2310.15533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning with Noisy Labels Using Collaborative Sample Selection and  Contrastive Semi-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+Q">Qing Miao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaohe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yanli Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zhaopeng Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Learning with noisy labels (LNL) has been extensively studied, with existing
approaches typically following a framework that alternates between clean sample
selection and semi-supervised learning (SSL). However, this approach has a
limitation: the clean set selected by the Deep Neural Network (DNN) classifier,
trained through self-training, inevitably contains noisy samples. This mixture
of clean and noisy samples leads to misguidance in DNN training during SSL,
resulting in impaired generalization performance due to confirmation bias
caused by error accumulation in sample selection. To address this issue, we
propose a method called Collaborative Sample Selection (CSS), which leverages
the large-scale pre-trained model CLIP. CSS aims to remove the mixed noisy
samples from the identified clean set. We achieve this by training a
2-Dimensional Gaussian Mixture Model (2D-GMM) that combines the probabilities
from CLIP with the predictions from the DNN classifier. To further enhance the
adaptation of CLIP to LNL, we introduce a co-training mechanism with a
contrastive loss in semi-supervised learning. This allows us to jointly train
the prompt of CLIP and the DNN classifier, resulting in improved feature
representation, boosted classification performance of DNNs, and reciprocal
benefits to our Collaborative Sample Selection. By incorporating auxiliary
information from CLIP and utilizing prompt fine-tuning, we effectively
eliminate noisy samples from the clean set and mitigate confirmation bias
during training. Experimental results on multiple benchmark datasets
demonstrate the effectiveness of our proposed method in comparison with the
state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15539" title="Abstract">arXiv:2310.15539</a> [<a href="/pdf/2310.15539" title="Download PDF">pdf</a>, <a href="/format/2310.15539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jialing Pan</a>, 
<a href="/search/cs?searchtype=author&query=Sad%C3%A9%2C+A">Adrien Sad&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Soriano%2C+E">Eric Soriano</a>, 
<a href="/search/cs?searchtype=author&query=Sole%2C+G">Guillem Sole</a>, 
<a href="/search/cs?searchtype=author&query=Flamant%2C+S">Sylvain Flamant</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the recent focus on Large Language Models (LLMs), both StarCoder (Li et
al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable
performance in code generation. However, there is still a need for improvement
in code translation functionality with efficient training techniques. In
response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM
designed specifically for multi-programming language-to-Python code
translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or
PHP-to-Python code translation without specifying the input programming
language. We modified StarCoder model architecture by incorporating a
Mixture-of-Experts (MoE) technique featuring five experts and a gating network
for multi-task handling. Experts are obtained by StarCoder fine-tuning.
Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each
expert size as only 0.06% of number of StarCoder's parameters. At the same
time, to enhance training efficiency in terms of time, we adopt curriculum
learning strategy and use self-instruct data for efficient fine-tuning. As a
result, each expert takes only 6 hours to train on one single 80Gb A100 HBM.
With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76
CodeBLEU score in multi-programming language-to-Python translation, surpassing
the top performance from the leaderboard by at least 3.5. This accomplishment
is attributed to only 45M extra parameters with StarCoder as the backbone and
32 hours of valid training on one 80GB A100 HBM. The source code is release
here: https://github.com/sade-adrien/SteloCoder.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15541" title="Abstract">arXiv:2310.15541</a> [<a href="/pdf/2310.15541" title="Download PDF">pdf</a>, <a href="/format/2310.15541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Language Models Meaning Understanding and Consistency by  Learning Conceptual Roles from Dictionary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+M+E">Myeongjun Erik Jang</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The non-humanlike behaviour of contemporary pre-trained language models
(PLMs) is a leading cause undermining their trustworthiness. A striking
phenomenon of such faulty behaviours is the generation of inconsistent
predictions, which produces logically contradictory results, such as generating
different predictions for texts delivering the same meaning or violating
logical properties. Previous studies exploited data augmentation or implemented
specialised loss functions to alleviate the issue. However, their usage is
limited, because they consume expensive training resources for large-sized PLMs
and can only handle a certain consistency type. To this end, we propose a
practical approach that alleviates the inconsistent behaviour issue by
fundamentally improving PLMs' meaning awareness. Based on the conceptual role
theory, our method allows PLMs to capture accurate meaning by learning precise
interrelationships between concepts from word-definition pairs in a dictionary.
Next, we propose an efficient parameter integration technique that updates only
a few additional parameters to combine the learned interrelationship with PLMs'
pre-trained knowledge. Our experimental results reveal that the approach can
concurrently improve multiple types of consistency, enables efficient knowledge
integration, and easily applies to other languages.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15542" title="Abstract">arXiv:2310.15542</a> [<a href="/pdf/2310.15542" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the gaze control ability of VALORANT players using a  Python based tool
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+I">Inhyeok Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Nobuto%2C+T">Takuma Nobuto</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+N">Naotsugu Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Kato%2C+T">Takaaki Kato</a>, 
<a href="/search/cs?searchtype=author&query=Nakazawa%2C+K">Kimitaka Nakazawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, 8 figures, submitted in IEEE Transactions on Games
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The current study investigated the gaze movements of FPS gamers in actual
game environments. We developed a low-cost analysis tool using Python to
identify gaze movements in real-world gaming environments. In Experiment 1, 11
middle-skilled and ten high-skilled FPS gamers performed a task under the
experimental condition. Gaze position, reaction time, and accuracy were
calculated during the task. Reaction time exhibited a significant positive
correlation with task accuracy, suggesting that speed and accuracy were
associated with higher game performance. The middle-skilled gamers had a
significantly wider horizontal gaze distribution than the high-skilled gamers,
and gaze distribution and reaction time showed a negative correlation. These
results suggested that high-skilled players utilize peripheral vision during
gameplay. In Experiment 2, 15 middle-skilled and 12 high-skilled FPS gamers
performed an actual FPS game match. The gaze distribution, kill/death/assist
ratio (KDA), and percentage of gaze on game information were calculated. In
experiment 2, gaze locations in less important areas were positively correlated
with KDA. Thus, performance was determined by the important areas where the
gaze was focused rather than by the coordination of gaze position alone.
Therefore, a broader range of environments is necessary to comprehend the
superior performance of FPS gamers.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15543" title="Abstract">arXiv:2310.15543</a> [<a href="/pdf/2310.15543" title="Download PDF">pdf</a>, <a href="/format/2310.15543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetry-preserving graph attention network to solve routing problems at  multiple resolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+C+D">Cong Dao Tran</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+T">Thong Bach</a>, 
<a href="/search/cs?searchtype=author&query=Hy%2C+T+S">Truong Son Hy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Travelling Salesperson Problems (TSPs) and Vehicle Routing Problems (VRPs)
have achieved reasonable improvement in accuracy and computation time with the
adaptation of Machine Learning (ML) methods. However, none of the previous
works completely respects the symmetries arising from TSPs and VRPs including
rotation, translation, permutation, and scaling. In this work, we introduce the
first-ever completely equivariant model and training to solve combinatorial
problems. Furthermore, it is essential to capture the multiscale structure
(i.e. from local to global information) of the input graph, especially for the
cases of large and long-range graphs, while previous methods are limited to
extracting only local information that can lead to a local or sub-optimal
solution. To tackle the above limitation, we propose a Multiresolution scheme
in combination with Equivariant Graph Attention network (mEGAT) architecture,
which can learn the optimal route based on low-level and high-level graph
resolutions in an efficient way. In particular, our approach constructs a
hierarchy of coarse-graining graphs from the input graph, in which we try to
solve the routing problems on simple low-level graphs first, then utilize that
knowledge for the more complex high-level graphs. Experimentally, we have shown
that our model outperforms existing baselines and proved that symmetry
preservation and multiresolution are important recipes for solving
combinatorial problems in a data-driven manner. Our source code is publicly
available at https://github.com/HySonLab/Multires-NP-hard
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15552" title="Abstract">arXiv:2310.15552</a> [<a href="/pdf/2310.15552" title="Download PDF">pdf</a>, <a href="/format/2310.15552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Multilinguality in Transformer Models: Exploring Language  Specificity in Feed-Forward Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sunit Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Bojar%2C+O">Ondrej Bojar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent research suggests that the feed-forward module within Transformers can
be viewed as a collection of key-value memories, where the keys learn to
capture specific patterns from the input based on the training examples. The
values then combine the output from the 'memories' of the keys to generate
predictions about the next token. This leads to an incremental process of
prediction that gradually converges towards the final token choice near the
output layers. This interesting perspective raises questions about how
multilingual models might leverage this mechanism. Specifically, for
autoregressive models trained on two or more languages, do all neurons (across
layers) respond equally to all languages? No! Our hypothesis centers around the
notion that during pretraining, certain model parameters learn strong
language-specific features, while others learn more language-agnostic (shared
across languages) features. To validate this, we conduct experiments utilizing
parallel corpora of two languages that the model was initially pretrained on.
Our findings reveal that the layers closest to the network's input or output
tend to exhibit more language-specific behaviour compared to the layers in the
middle.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15555" title="Abstract">arXiv:2310.15555</a> [<a href="/pdf/2310.15555" title="Download PDF">pdf</a>, <a href="/format/2310.15555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer learning for day-ahead load forecasting: a case study on  European national electricity demand time series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tzortzis%2C+A">Alexandros-Menelaos Tzortzis</a>, 
<a href="/search/cs?searchtype=author&query=Pelekis%2C+S">Sotiris Pelekis</a>, 
<a href="/search/cs?searchtype=author&query=Spiliotis%2C+E">Evangelos Spiliotis</a>, 
<a href="/search/cs?searchtype=author&query=Mouzakitis%2C+S">Spiros Mouzakitis</a>, 
<a href="/search/cs?searchtype=author&query=Psarras%2C+J">John Psarras</a>, 
<a href="/search/cs?searchtype=author&query=Askounis%2C+D">Dimitris Askounis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Short-term load forecasting (STLF) is crucial for the daily operation of
power grids. However, the non-linearity, non-stationarity, and randomness
characterizing electricity demand time series renders STLF a challenging task.
Various forecasting approaches have been proposed for improving STLF, including
neural network (NN) models which are trained using data from multiple
electricity demand series that may not necessary include the target series. In
the present study, we investigate the performance of this special case of STLF,
called transfer learning (TL), by considering a set of 27 time series that
represent the national day-ahead electricity demand of indicative European
countries. We employ a popular and easy-to-implement NN model and perform a
clustering analysis to identify similar patterns among the series and assist
TL. In this context, two different TL approaches, with and without the
clustering step, are compiled and compared against each other as well as a
typical NN training setup. Our results demonstrate that TL can outperform the
conventional approach, especially when clustering techniques are considered.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15556" title="Abstract">arXiv:2310.15556</a> [<a href="/pdf/2310.15556" title="Download PDF">pdf</a>, <a href="/format/2310.15556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for  Inference Cost Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tong Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yiming Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Since ChatGPT released its API for public use, the number of applications
built on top of commercial large language models (LLMs) increase exponentially.
One popular usage of such models is leveraging its in-context learning ability
and generating responses given user queries leveraging knowledge obtained by
retrieval augmentation. One problem of deploying commercial retrieval-augmented
LLMs is the cost due to the additionally retrieved context that largely
increases the input token size of the LLMs. To mitigate this, we propose a
token compression scheme that includes two methods: summarization compression
and semantic compression. The first method applies a T5-based model that is
fine-tuned by datasets generated using self-instruct containing samples with
varying lengths and reduce token size by doing summarization. The second method
further compresses the token size by removing words with lower impact on the
semantic. In order to adequately evaluate the effectiveness of the proposed
methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)
focusing on food recommendation for women around pregnancy period or infants.
Our summarization compression can reduce 65% of the retrieval token size with
further 0.3% improvement on the accuracy; semantic compression provides a more
flexible way to trade-off the token size with performance, for which we can
reduce the token size by 20% with only 1.6% of accuracy drop.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15560" title="Abstract">arXiv:2310.15560</a> [<a href="/pdf/2310.15560" title="Download PDF">pdf</a>, <a href="/ps/2310.15560" title="Download PostScript">ps</a>, <a href="/format/2310.15560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling and Design of the Communication Sensing and Control Coupled  Closed-Loop Industrial System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zeyang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Dingyou Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, received by GlobeCom 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">With the advent of 5G era, factories are transitioning towards wireless
networks to break free from the limitations of wired networks. In 5G-enabled
factories, unmanned automatic devices such as automated guided vehicles and
robotic arms complete production tasks cooperatively through the periodic
control loops. In such loops, the sensing data is generated by sensors, and
transmitted to the control center through uplink wireless communications. The
corresponding control commands are generated and sent back to the devices
through downlink wireless communications. Since wireless communications,
sensing and control are tightly coupled, there are big challenges on the
modeling and design of such closed-loop systems. In particular, existing
theoretical tools of these functionalities have different modelings and
underlying assumptions, which make it difficult for them to collaborate with
each other. Therefore, in this paper, an analytical closed-loop model is
proposed, where the performances and resources of communication, sensing and
control are deeply related. To achieve the optimal control performance, a
co-design of communication resource allocation and control method is proposed,
inspired by the model predictive control algorithm. Numerical results are
provided to demonstrate the relationships between the resources and control
performances.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15565" title="Abstract">arXiv:2310.15565</a> [<a href="/pdf/2310.15565" title="Download PDF">pdf</a>, <a href="/format/2310.15565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capacity-based Spatial Modulation Constellation and Pre-scaling Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xinghao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Hanjiang Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi-yan Wu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Dazhi He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages,conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Spatial Modulation (SM) can utilize the index of the transmit antenna (TA) to
transmit additional information. In this paper, to improve the performance of
SM, a non-uniform constellation (NUC) and pre-scaling coefficients optimization
design scheme is proposed. The bit-interleaved coded modulation (BICM) capacity
calculation formula of SM system is firstly derived. The constellation and
pre-scaling coefficients are optimized by maximizing the BICM capacity without
channel state information (CSI) feedback. Optimization results are given for
the multiple-input-single-output (MISO) system with Rayleigh channel.
Simulation result shows the proposed scheme provides a meaningful performance
gain compared to conventional SM system without CSI feedback. The proposed
optimization design scheme can be a promising technology for future 6G to
achieve high-efficiency.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15566" title="Abstract">arXiv:2310.15566</a> [<a href="/pdf/2310.15566" title="Download PDF">pdf</a>, <a href="/format/2310.15566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surface-Based Receive Generalized Spatial  Modulation Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xinghao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Hanjiang Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi-yan Wu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Dazhi He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenjun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, the receive generalized spatial modulation (RGSM) scheme with
reconfigurable intelligent surfaces (RIS) assistance is proposed. The RIS group
controllers change the reflected phases of the RIS elements to achieve the
selection of receive antennas and phase shift keying (PSK) modulation, and the
amplitudes of the received symbols are adjusted by changing the activation
states of the elements to achieve amplitude phase shift keying (APSK)
modulation. Compared with the existing RIS-aided receive generalized space
shift keying (RIS-RGSSK) scheme, the proposed scheme realizes that the selected
antennas respectively receive different modulation symbols, and only adds the
process to control the modulated phases and the activation states of elements.
The proposed scheme has better bit error rate (BER) performance than the
RIS-RGSSK scheme at the same rate. In addition, the results show that for low
modulation orders, the proposed scheme will perform better with PSK, while for
high modulation order, APSK is better. The proposed scheme is a promising
scheme for future wireless communication to achieve high-efficiency.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15568" title="Abstract">arXiv:2310.15568</a> [<a href="/pdf/2310.15568" title="Download PDF">pdf</a>, <a href="/format/2310.15568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I$^2$MD: 3D Action Representation Learning with Inter- and Intra-modal  Mutual Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yunyao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiajun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wengang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhenbo Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IJCV. arXiv admin note: substantial text overlap with <a href="/abs/2208.12448">arXiv:2208.12448</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent progresses on self-supervised 3D human action representation learning
are largely attributed to contrastive learning. However, in conventional
contrastive frameworks, the rich complementarity between different skeleton
modalities remains under-explored. Moreover, optimized with distinguishing
self-augmented samples, models struggle with numerous similar positive
instances in the case of limited action categories. In this work, we tackle the
aforementioned problems by introducing a general Inter- and Intra-modal Mutual
Distillation (I$^2$MD) framework. In I$^2$MD, we first re-formulate the
cross-modal interaction as a Cross-modal Mutual Distillation (CMD) process.
Different from existing distillation solutions that transfer the knowledge of a
pre-trained and fixed teacher to the student, in CMD, the knowledge is
continuously updated and bidirectionally distilled between modalities during
pre-training. To alleviate the interference of similar samples and exploit
their underlying contexts, we further design the Intra-modal Mutual
Distillation (IMD) strategy, In IMD, the Dynamic Neighbors Aggregation (DNA)
mechanism is first introduced, where an additional cluster-level discrimination
branch is instantiated in each modality. It adaptively aggregates
highly-correlated neighboring features, forming local cluster-level
contrasting. Mutual distillation is then performed between the two branches for
cross-level knowledge exchange. Extensive experiments on three datasets show
that our approach sets a series of new records.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15569" title="Abstract">arXiv:2310.15569</a> [<a href="/pdf/2310.15569" title="Download PDF">pdf</a>, <a href="/format/2310.15569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in  the Materials Science Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schrader%2C+T+P">Timo Pierre Schrader</a>, 
<a href="/search/cs?searchtype=author&query=Finco%2C+M">Matteo Finco</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%BCnewald%2C+S">Stefan Gr&#xfc;newald</a>, 
<a href="/search/cs?searchtype=author&query=Hildebrand%2C+F">Felix Hildebrand</a>, 
<a href="/search/cs?searchtype=author&query=Friedrich%2C+A">Annemarie Friedrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures, 28 tables, to be published in "Proceedings of the second Workshop on Information Extraction from Scientific Publications"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Keeping track of all relevant recent publications and experimental results
for a research area is a challenging task. Prior work has demonstrated the
efficacy of information extraction models in various scientific areas.
Recently, several datasets have been released for the yet understudied
materials science domain. However, these datasets focus on sub-problems such as
parsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells.
In this resource paper, we present MuLMS, a new dataset of 50 open-access
articles, spanning seven sub-domains of materials science. The corpus has been
annotated by domain experts with several layers ranging from named entities
over relations to frame structures. We present competitive neural models for
all tasks and demonstrate that multi-task training with existing related
resources leads to benefits.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15570" title="Abstract">arXiv:2310.15570</a> [<a href="/pdf/2310.15570" title="Download PDF">pdf</a>, <a href="/format/2310.15570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimal Ansatz Space for Moving Least Squares Approximation on  Spheres
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hielscher%2C+R">Ralf Hielscher</a>, 
<a href="/search/math?searchtype=author&query=P%C3%B6schl%2C+T">Tim P&#xf6;schl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures. Will be submitted to 'Advances in Computational Mathematics'
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We revisit the moving least squares (MLS) approximation scheme on the sphere
$\mathbb S^{d-1} \subset \mathbb R^d$, where $d&gt;1$. It is well known that using
the spherical harmonics up to degree $L \in \mathbb N$ as ansatz space yields
for functions in $\mathcal C^{L+1}(\mathbb S^{d-1})$ the approximation order
$\mathcal O \left( h^{L+1} \right)$, where $h$ denotes the fill distance of the
sampling nodes.
<br />In this paper we show that the dimension of the ansatz space can be almost
halved, by including only spherical harmonics of even or odd degree up to $L$,
while preserving the same order of approximation. Numerical experiments
indicate that using the reduced ansatz space is essential to ensure the
numerical stability of the MLS approximation scheme as $h \to 0$. Finally, we
compare our approach with an MLS approximation scheme that uses polynomials on
the tangent space as ansatz space.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15571" title="Abstract">arXiv:2310.15571</a> [<a href="/pdf/2310.15571" title="Download PDF">pdf</a>, <a href="/format/2310.15571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visually Grounded Continual Language Learning with Selective  Specialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahrens%2C+K">Kyra Ahrens</a>, 
<a href="/search/cs?searchtype=author&query=Bengtson%2C+L">Lennart Bengtson</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+H">Jae Hee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wermter%2C+S">Stefan Wermter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A desirable trait of an artificial agent acting in the visual world is to
continually learn a sequence of language-informed tasks while striking a
balance between sufficiently specializing in each task and building a
generalized knowledge for transfer. Selective specialization, i.e., a careful
selection of model components to specialize in each task, is a strategy to
provide control over this trade-off. However, the design of selection
strategies requires insights on the role of each model component in learning
rather specialized or generalizable representations, which poses a gap in
current research. Thus, our aim with this work is to provide an extensive
analysis of selection strategies for visually grounded continual language
learning. Due to the lack of suitable benchmarks for this purpose, we introduce
two novel diagnostic datasets that provide enough control and flexibility for a
thorough model analysis. We assess various heuristics for module specialization
strategies as well as quantifiable measures for two different types of model
architectures. Finally, we design conceptually simple approaches based on our
analysis that outperform common continual learning baselines. Our results
demonstrate the need for further efforts towards better aligning continual
learning algorithms with the learning behaviors of individual model parts.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15572" title="Abstract">arXiv:2310.15572</a> [<a href="/pdf/2310.15572" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Processing for Drug Discovery Knowledge Graphs:  promises and pitfalls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeynes%2C+J+C+G">J. Charles G. Jeynes</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+T">Tim James</a>, 
<a href="/search/cs?searchtype=author&query=Corney%2C+M">Matthew Corney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Methods Mol Biol . 2024:2716:223-240
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Building and analysing knowledge graphs (KGs) to aid drug discovery is a
topical area of research. A salient feature of KGs is their ability to combine
many heterogeneous data sources in a format that facilitates discovering
connections. The utility of KGs has been exemplified in areas such as drug
repurposing, with insights made through manual exploration and modelling of the
data. In this article, we discuss promises and pitfalls of using natural
language processing (NLP) to mine unstructured text typically from scientific
literature as a data source for KGs. This draws on our experience of initially
parsing structured data sources such as ChEMBL as the basis for data within a
KG, and then enriching or expanding upon them using NLP. The fundamental
promise of NLP for KGs is the automated extraction of data from millions of
documents a task practically impossible to do via human curation alone.
However, there are many potential pitfalls in NLP-KG pipelines such as
incorrect named entity recognition and ontology linking all of which could
ultimately lead to erroneous inferences and conclusions.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15574" title="Abstract">arXiv:2310.15574</a> [<a href="/pdf/2310.15574" title="Download PDF">pdf</a>, <a href="/format/2310.15574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Multi-Target Localization Via Intelligent Reflecting Surface:  Protocol and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+M">Meng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangji Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shaodan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+H+C">Hing Cheung So</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to IEEE journal for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">With the emerging environment-aware applications, ubiquitous sensing is
expected to play a key role in future networks. In this paper, we study a
3-dimensional (3D) multi-target localization system where multiple intelligent
reflecting surfaces (IRSs) are applied to create virtual line-of-sight (LoS)
links that bypass the base station (BS) and targets. To fully unveil the
fundamental limit of IRS for sensing, we first study a single-target-single-IRS
case and propose a novel \textit{two-stage localization protocol} by
controlling the on/off state of IRS. To be specific, in the IRS-off stage, we
derive the Cram\'{e}r-Rao bound (CRB) of the azimuth/elevation
direction-of-arrival (DoA) of the BS-target link and design a DoA estimator
based on the MUSIC algorithm. In the IRS-on stage, the CRB of the
azimuth/elevation DoA of the IRS-target link is derived and a simple DoA
estimator based on the on-grid IRS beam scanning method is proposed.
Particularly, the impact of echo signals reflected by IRS from different paths
on sensing performance is analyzed. Moreover, we prove that the single-beam of
the IRS is not capable of sensing, but it can be achieved with
\textit{multi-beam}. Based on the two obtained DoAs, the 3D single-target
location is constructed. We then extend to the multi-target-multi-IRS case and
propose an \textit{IRS-adaptive sensing protocol} by controlling the on/off
state of multiple IRSs, and a multi-target localization algorithm is developed.
Simulation results demonstrate the effectiveness of our scheme and show that
sub-meter-level positioning accuracy can be achieved.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15575" title="Abstract">arXiv:2310.15575</a> [<a href="/pdf/2310.15575" title="Download PDF">pdf</a>, <a href="/format/2310.15575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POE: Process of Elimination for Multiple Choice Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenkai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinya Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a short paper at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language models (LMs) are capable of conducting in-context learning for
multiple choice reasoning tasks, but the options in these tasks are treated
equally. As humans often first eliminate wrong options before picking the final
correct answer, we argue a similar two-step strategy can make LMs better at
these tasks. To this end, we present the Process of Elimination (POE), a
two-step scoring method. In the first step, POE scores each option, and
eliminates seemingly wrong options. In the second step, POE masks these wrong
options, and makes the final prediction from the remaining options. Zero-shot
experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a
following analysis finds our method to be especially performant on logical
reasoning tasks. We further analyze the effect of masks, and show that POE
applies to few-shot settings and large language models (LLMs) like ChatGPT.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15577" title="Abstract">arXiv:2310.15577</a> [<a href="/pdf/2310.15577" title="Download PDF">pdf</a>, <a href="/format/2310.15577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts  For Aspect Sentiment Triplet Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+R">Rajdeep Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Kannen%2C+N">Nithish Kannen</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S+K">Saurabh Kumar Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a Long Paper at EMNLP 2023 (Findings); 16 pages; Codes: <a href="https://github.com/nitkannen/CONTRASTE/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus
on developing more efficient fine-tuning techniques for the task. Instead, our
motivation is to come up with a generic approach that can improve the
downstream performances of multiple ABSA tasks simultaneously. Towards this, we
present CONTRASTE, a novel pre-training strategy using CONTRastive learning to
enhance the ASTE performance. While we primarily focus on ASTE, we also
demonstrate the advantage of our proposed technique on other ABSA tasks such as
ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion,
sentiment) triplets, first, we design aspect-based prompts with corresponding
sentiments masked. We then (pre)train an encoder-decoder model by applying
contrastive learning on the decoder-generated aspect-aware sentiment
representations of the masked terms. For fine-tuning the model weights thus
obtained, we then propose a novel multi-task approach where the base
encoder-decoder model is combined with two complementary modules, a
tagging-based Opinion Term Detector, and a regression-based Triplet Count
Estimator. Exhaustive experiments on four benchmark datasets and a detailed
ablation study establish the importance of each of our proposed components as
we achieve new state-of-the-art ASTE results.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15578" title="Abstract">arXiv:2310.15578</a> [<a href="/pdf/2310.15578" title="Download PDF">pdf</a>, <a href="/ps/2310.15578" title="Download PostScript">ps</a>, <a href="/format/2310.15578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VMAF Re-implementation on PyTorch: Some Experimental Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aistov%2C+K">Kirill Aistov</a>, 
<a href="/search/cs?searchtype=author&query=Koroteev%2C+M">Maxim Koroteev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Based on the standard VMAF implementation we propose an implementation of
VMAF using PyTorch framework. For this implementation comparisons with the
standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We
investigate gradients computation when using VMAF as an objective function and
demonstrate that training using this function does not result in ill-behaving
gradients.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15580" title="Abstract">arXiv:2310.15580</a> [<a href="/pdf/2310.15580" title="Download PDF">pdf</a>, <a href="/format/2310.15580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifiable Latent Polynomial Causal Models Through the Lens of Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Dong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Mingming Gong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Biwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Hengel%2C+A">Anton van den Hengel</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J+Q">Javen Qinfeng Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Causal representation learning aims to unveil latent high-level causal
representations from observed low-level data. One of its primary tasks is to
provide reliable assurance of identifying these latent causal models, known as
identifiability. A recent breakthrough explores identifiability by leveraging
the change of causal influences among latent causal variables across multiple
environments \citep{liu2022identifying}. However, this progress rests on the
assumption that the causal relationships among latent causal variables adhere
strictly to linear Gaussian models. In this paper, we extend the scope of
latent causal models to involve nonlinear causal relationships, represented by
polynomial models, and general noise distributions conforming to the
exponential family. Additionally, we investigate the necessity of imposing
changes on all causal parameters and present partial identifiability results
when part of them remains unchanged. Further, we propose a novel empirical
estimation method, grounded in our theoretical finding, that enables learning
consistent latent causal representations. Our experimental results, obtained
from both synthetic and real-world data, validate our theoretical contributions
concerning identifiability and consistency.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15581" title="Abstract">arXiv:2310.15581</a> [<a href="/pdf/2310.15581" title="Download PDF">pdf</a>, <a href="/format/2310.15581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep ReLU neural networks overcome the curse of dimensionality in the  numerical approximation of semilinear partial integro-differential equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neufeld%2C+A">Ariel Neufeld</a>, 
<a href="/search/math?searchtype=author&query=Nguyen%2C+T+A">Tuan Anh Nguyen</a>, 
<a href="/search/math?searchtype=author&query=Wu%2C+S">Sizhou Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.14398">arXiv:2205.14398</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP); Probability (math.PR)

</div>
<p class="mathjax">We prove that deep neural networks with ReLU activation function are capable
of approximating solutions of semilinear partial integro-differential equations
in the case of gradient-independent and Lipschitz-continuous nonlinearities,
while the required number of parameters in the neural networks grows at most
polynomially in both the dimension $ d\in\mathbb{N} $ and the reciprocal of the
prescribed accuracy $ \epsilon $.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15582" title="Abstract">arXiv:2310.15582</a> [<a href="/pdf/2310.15582" title="Download PDF">pdf</a>, <a href="/format/2310.15582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SecV: Secure Code Partitioning via Multi-Language Secure Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuhala%2C+P">Peterson Yuhala</a>, 
<a href="/search/cs?searchtype=author&query=Felber%2C+P">Pascal Felber</a>, 
<a href="/search/cs?searchtype=author&query=Guiroux%2C+H">Hugo Guiroux</a>, 
<a href="/search/cs?searchtype=author&query=Lozi%2C+J">Jean-Pierre Lozi</a>, 
<a href="/search/cs?searchtype=author&query=Tchana%2C+A">Alain Tchana</a>, 
<a href="/search/cs?searchtype=author&query=Schiavoni%2C+V">Valerio Schiavoni</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+G">Ga&#xeb;l Thomas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Trusted execution environments like Intel SGX provide \emph{enclaves}, which
offer strong security guarantees for applications. Running entire applications
inside enclaves is possible, but this approach leads to a large trusted
computing base (TCB). As such, various tools have been developed to partition
programs written in languages such as C or Java into \emph{trusted} and
\emph{untrusted} parts, which are run in and out of enclaves respectively.
However, those tools depend on language-specific taint-analysis and
partitioning techniques. They cannot be reused for other languages and there is
thus a need for tools that transcend this language barrier.
<br />We address this challenge by proposing a multi-language technique to specify
sensitive code or data, as well as a multi-language tool to analyse and
partition the resulting programs for trusted execution environments like Intel
SGX. We leverage GraalVM's Truffle framework, which provides a
language-agnostic abstract syntax tree (AST) representation for programs, to
provide special AST nodes called \emph{secure nodes} that encapsulate sensitive
program information. Secure nodes can easily be embedded into the ASTs of a
wide range of languages via Truffle's \emph{polyglot API}. Our technique
includes a multi-language dynamic taint tracking tool to analyse and partition
applications based on our generic secure nodes. Our extensive evaluation with
micro- and macro-benchmarks shows that we can use our technique for two
languages (Javascript and \python), and that partitioned programs can obtain up
to $14.5\%$ performance improvement as compared to unpartitioned versions.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15583" title="Abstract">arXiv:2310.15583</a> [<a href="/pdf/2310.15583" title="Download PDF">pdf</a>, <a href="/format/2310.15583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Agility and Adaptive Legged Locomotion via Curricular Hindsight  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sicen Li</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yiming Pang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+P">Panju Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhaojin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liquan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Agile and adaptive maneuvers such as fall recovery, high-speed turning, and
sprinting in the wild are challenging for legged systems. We propose a
Curricular Hindsight Reinforcement Learning (CHRL) that learns an end-to-end
tracking controller that achieves powerful agility and adaptation for the
legged robot. The two key components are (I) a novel automatic curriculum
strategy on task difficulty and (ii) a Hindsight Experience Replay strategy
adapted to legged locomotion tasks. We demonstrated successful agile and
adaptive locomotion on a real quadruped robot that performed fall recovery
autonomously, coherent trotting, sustained outdoor speeds up to 3.45 m/s, and
tuning speeds up to 3.2 rad/s. This system produces adaptive behaviours
responding to changing situations and unexpected disturbances on natural
terrains like grass and dirt.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15584" title="Abstract">arXiv:2310.15584</a> [<a href="/pdf/2310.15584" title="Download PDF">pdf</a>, <a href="/format/2310.15584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Split Federated Learning over Wireless Communication  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Ce Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Y">Yushi Ling</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Miaowen Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">The development of artificial intelligence (AI) provides opportunities for
the promotion of deep neural network (DNN)-based applications. However, the
large amount of parameters and computational complexity of DNN makes it
difficult to deploy it on edge devices which are resource-constrained. An
efficient method to address this challenge is model partition/splitting, in
which DNN is divided into two parts which are deployed on device and server
respectively for co-training or co-inference. In this paper, we consider a
split federated learning (SFL) framework that combines the parallel model
training mechanism of federated learning (FL) and the model splitting structure
of split learning (SL). We consider a practical scenario of heterogeneous
devices with individual split points of DNN. We formulate a joint problem of
split point selection and bandwidth allocation to minimize the system latency.
By using alternating optimization, we decompose the problem into two
sub-problems and solve them optimally. Experiment results demonstrate the
superiority of our work in latency reduction and accuracy improvement.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15585" title="Abstract">arXiv:2310.15585</a> [<a href="/pdf/2310.15585" title="Download PDF">pdf</a>, <a href="/format/2310.15585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Representations for Teacher-Guided Compositional Visual  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aissa%2C+W">Wafa Aissa</a> (CEDRIC - VERTIGO), 
<a href="/search/cs?searchtype=author&query=Ferecatu%2C+M">Marin Ferecatu</a> (CEDRIC - VERTIGO), 
<a href="/search/cs?searchtype=author&query=Crucianu%2C+M">Michel Crucianu</a> (CEDRIC - VERTIGO)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advanced Concepts for Intelligent Vision Systems, 21st
  International Conference (ACIVS 2023), Aug 2023, Kumamoto, Japan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural Module Networks (NMN) are a compelling method for visual question
answering, enabling the translation of a question into a program consisting of
a series of reasoning sub-tasks that are sequentially executed on the image to
produce an answer. NMNs provide enhanced explainability compared to integrated
models, allowing for a better understanding of the underlying reasoning
process. To improve the effectiveness of NMNs we propose to exploit features
obtained by a large-scale cross-modal encoder. Also, the current training
approach of NMNs relies on the propagation of module outputs to subsequent
modules, leading to the accumulation of prediction errors and the generation of
false answers. To mitigate this, we introduce an NMN learning strategy
involving scheduled teacher guidance. Initially, the model is fully guided by
the ground-truth intermediate outputs, but gradually transitions to an
autonomous behavior as training progresses. This reduces error accumulation,
thus improving training efficiency and final performance.We demonstrate that by
incorporating cross-modal features and employing more effective training
techniques for NMN, we achieve a favorable balance between performance and
transparency in the reasoning process.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15586" title="Abstract">arXiv:2310.15586</a> [<a href="/pdf/2310.15586" title="Download PDF">pdf</a>, <a href="/format/2310.15586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance  Using Self-Supervised Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernab%C3%A9%2C+P">Pierre Bernab&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Gotlieb%2C+A">Arnaud Gotlieb</a>, 
<a href="/search/cs?searchtype=author&query=Legeard%2C+B">Bruno Legeard</a>, 
<a href="/search/cs?searchtype=author&query=Marijan%2C+D">Dusica Marijan</a>, 
<a href="/search/cs?searchtype=author&query=Sem-Jacobsen%2C+F+O">Frank Olaf Sem-Jacobsen</a>, 
<a href="/search/cs?searchtype=author&query=Spieker%2C+H">Helge Spieker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Intelligent Transportation Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In maritime traffic surveillance, detecting illegal activities, such as
illegal fishing or transshipment of illicit products is a crucial task of the
coastal administration. In the open sea, one has to rely on Automatic
Identification System (AIS) message transmitted by on-board transponders, which
are captured by surveillance satellites. However, insincere vessels often
intentionally shut down their AIS transponders to hide illegal activities. In
the open sea, it is very challenging to differentiate intentional AIS shutdowns
from missing reception due to protocol limitations, bad weather conditions or
restricting satellite positions. This paper presents a novel approach for the
detection of abnormal AIS missing reception based on self-supervised deep
learning techniques and transformer models. Using historical data, the trained
model predicts if a message should be received in the upcoming minute or not.
Afterwards, the model reports on detected anomalies by comparing the prediction
with what actually happens. Our method can process AIS messages in real-time,
in particular, more than 500 Millions AIS messages per month, corresponding to
the trajectories of more than 60 000 ships. The method is evaluated on 1-year
of real-world data coming from four Norwegian surveillance satellites. Using
related research results, we validated our method by rediscovering already
detected intentional AIS shutdowns.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15587" title="Abstract">arXiv:2310.15587</a> [<a href="/pdf/2310.15587" title="Download PDF">pdf</a>, <a href="/format/2310.15587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bolliger%2C+L+S">Lena S. Bolliger</a>, 
<a href="/search/cs?searchtype=author&query=Reich%2C+D+R">David R. Reich</a>, 
<a href="/search/cs?searchtype=author&query=Haller%2C+P">Patrick Haller</a>, 
<a href="/search/cs?searchtype=author&query=Jakobi%2C+D+N">Deborah N. Jakobi</a>, 
<a href="/search/cs?searchtype=author&query=Prasse%2C+P">Paul Prasse</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A4ger%2C+L+A">Lena A. J&#xe4;ger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Eye movements in reading play a crucial role in psycholinguistic research
studying the cognitive mechanisms underlying human language processing. More
recently, the tight coupling between eye movements and cognition has also been
leveraged for language-related machine learning tasks such as the
interpretability, enhancement, and pre-training of language models, as well as
the inference of reader- and text-specific properties. However, scarcity of eye
movement data and its unavailability at application time poses a major
challenge for this line of research. Initially, this problem was tackled by
resorting to cognitive models for synthesizing eye movement data. However, for
the sole purpose of generating human-like scanpaths, purely data-driven
machine-learning-based methods have proven to be more suitable. Following
recent advances in adapting diffusion processes to discrete data, we propose
ScanDL, a novel discrete sequence-to-sequence diffusion model that generates
synthetic scanpaths on texts. By leveraging pre-trained word representations
and jointly embedding both the stimulus text and the fixation sequence, our
model captures multi-modal interactions between the two inputs. We evaluate
ScanDL within- and across-dataset and demonstrate that it significantly
outperforms state-of-the-art scanpath generation methods. Finally, we provide
an extensive psycholinguistic analysis that underlines the model's ability to
exhibit human-like reading behavior. Our implementation is made available at
https://github.com/DiLi-Lab/ScanDL.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15588" title="Abstract">arXiv:2310.15588</a> [<a href="/pdf/2310.15588" title="Download PDF">pdf</a>, <a href="/format/2310.15588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed Loop Molecular Communication Testbed: Setup, Interference  Analysis, and Experimental Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brand%2C+L">Lukas Brand</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+M">Maike Scherer</a>, 
<a href="/search/cs?searchtype=author&query=Dieck%2C+T+t">Teena tom Dieck</a>, 
<a href="/search/cs?searchtype=author&query=Lotter%2C+S">Sebastian Lotter</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+M">Maximilian Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=Burkovski%2C+A">Andreas Burkovski</a>, 
<a href="/search/cs?searchtype=author&query=Sticht%2C+H">Heinrich Sticht</a>, 
<a href="/search/cs?searchtype=author&query=Castiglione%2C+K">Kathrin Castiglione</a>, 
<a href="/search/cs?searchtype=author&query=Schober%2C+R">Robert Schober</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 1 table. This work has been submitted for possible publication to the IEEE International Conference on Communications (ICC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
<p class="mathjax">In this paper, we present a fluid-based experimental molecular communication
(MC) testbed that, similar to the human cardiovascular system, operates in a
closed circuit pipe system. The proposed system is designed to be
biocompatible, resource-efficient, and controllable from outside the pipe. As
signaling molecule the testbed employs the green fluorescent protein variant
"Dreiklang" (GFPD). GFPDs can be reversibly switched via light of different
wavelengths between a bright fluorescent state and a less fluorescent state.
Hence, this property allows for writing and erasing information encoded in the
state of the GFPDs already existing in the fluid via radiation from outside the
pipe. The concept of modulating the GFPDs existing in the channel at the
transmitter for information transmission instead of releasing new molecules is
a form of media modulation. In our testbed, due to the closed loop setup and
the long experiment durations of up to 250 min, we observe new forms of
inter-symbol interferences (ISI), which do not occur in short experiments and
open loop systems. In particular, four different forms of inter-symbol
interference (ISI), namely channel ISI, inter-loop ISI, offset ISI, and
permanent ISI, can occur in the considered system. To mitigate inter-loop ISI
and offset ISI, we propose a powerful light based eraser. Finally, we
experimentally demonstrate the reliability of information transmission in our
testbed achieving error-free transmission of 500 bit at a data rate of 6
bit/min, while employing a sub-optimal low-complexity detection scheme.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15590" title="Abstract">arXiv:2310.15590</a> [<a href="/pdf/2310.15590" title="Download PDF">pdf</a>, <a href="/format/2310.15590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Facial Data Minimization: Shallow Model as Your Privacy Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yuwen Pu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiahao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jiayu Pan</a>, 
<a href="/search/cs?searchtype=author&query=li%2C+H">Hao li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Diqun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shouling Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Face recognition service has been used in many fields and brings much
convenience to people. However, once the user's facial data is transmitted to a
service provider, the user will lose control of his/her private data. In recent
years, there exist various security and privacy issues due to the leakage of
facial data. Although many privacy-preserving methods have been proposed, they
usually fail when they are not accessible to adversaries' strategies or
auxiliary data. Hence, in this paper, by fully considering two cases of
uploading facial images and facial features, which are very typical in face
recognition service systems, we proposed a data privacy minimization
transformation (PMT) method. This method can process the original facial data
based on the shallow model of authorized services to obtain the obfuscated
data. The obfuscated data can not only maintain satisfactory performance on
authorized models and restrict the performance on other unauthorized models but
also prevent original privacy data from leaking by AI methods and human visual
theft. Additionally, since a service provider may execute preprocessing
operations on the received data, we also propose an enhanced perturbation
method to improve the robustness of PMT. Besides, to authorize one facial image
to multiple service models simultaneously, a multiple restriction mechanism is
proposed to improve the scalability of PMT. Finally, we conduct extensive
experiments and evaluate the effectiveness of the proposed PMT in defending
against face reconstruction, data abuse, and face attribute estimation attacks.
These experimental results demonstrate that PMT performs well in preventing
facial data abuse and privacy leakage while maintaining face recognition
accuracy.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15593" title="Abstract">arXiv:2310.15593</a> [<a href="/pdf/2310.15593" title="Download PDF">pdf</a>, <a href="/format/2310.15593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecipeMeta: Metapath-enhanced Recipe Recommendation on Heterogeneous  Recipe Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jialiang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Komamizu%2C+T">Takahiro Komamizu</a>, 
<a href="/search/cs?searchtype=author&query=Doman%2C+K">Keisuke Doman</a>, 
<a href="/search/cs?searchtype=author&query=Kyutoku%2C+H">Haruya Kyutoku</a>, 
<a href="/search/cs?searchtype=author&query=Ide%2C+I">Ichiro Ide</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
<p class="mathjax">Recipe is a set of instructions that describes how to make food. It can help
people from the preparation of ingredients, food cooking process, etc. to
prepare the food, and increasingly in demand on the Web. To help users find the
vast amount of recipes on the Web, we address the task of recipe
recommendation. Due to multiple data types and relationships in a recipe, we
can treat it as a heterogeneous network to describe its information more
accurately. To effectively utilize the heterogeneous network, metapath was
proposed to describe the higher-level semantic information between two entities
by defining a compound path from peer entities. Therefore, we propose a
metapath-enhanced recipe recommendation framework, RecipeMeta, that combines
GNN (Graph Neural Network)-based representation learning and specific
metapath-based information in a recipe to predict User-Recipe pairs for
recommendation. Through extensive experiments, we demonstrate that the proposed
model, RecipeMeta, outperforms state-of-the-art methods for recipe
recommendation.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15594" title="Abstract">arXiv:2310.15594</a> [<a href="/pdf/2310.15594" title="Download PDF">pdf</a>, <a href="/format/2310.15594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-based Knowledge Transfer: An Effective Approach for Extreme  Large Language Model Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiduan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xunliang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R+L">Ran Lucien Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large-scale pre-trained language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, the
massive size of these models poses huge challenges for their deployment in
real-world applications. While numerous model compression techniques have been
proposed, most of them are not well-suited for achieving extreme model
compression when there is a significant gap in model scale. In this paper, we
introduce a novel compression paradigm called Retrieval-based Knowledge
Transfer (RetriKT), which effectively transfers the knowledge of LLMs to
extremely small-scale models (e.g., 1%). In particular, our approach extracts
knowledge from LLMs to construct a knowledge store, from which the small-scale
model can retrieve relevant information and leverage it for effective
inference. To improve the quality of the model, soft prompt tuning and Proximal
Policy Optimization (PPO) reinforcement learning techniques are employed.
Extensive experiments are conducted on low-resource tasks from SuperGLUE and
GLUE benchmarks. The results demonstrate that the proposed approach
significantly enhances the performance of small-scale models by leveraging the
knowledge from LLMs.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15597" title="Abstract">arXiv:2310.15597</a> [<a href="/pdf/2310.15597" title="Download PDF">pdf</a>, <a href="/format/2310.15597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent Communication in Interactive Sketch Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zixing Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yuxin Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Vision-based emergent communication (EC) aims to learn to communicate through
sketches and demystify the evolution of human communication. Ironically,
previous works neglect multi-round interaction, which is indispensable in human
communication. To fill this gap, we first introduce a novel Interactive Sketch
Question Answering (ISQA) task, where two collaborative players are interacting
through sketches to answer a question about an image in a multi-round manner.
To accomplish this task, we design a new and efficient interactive EC system,
which can achieve an effective balance among three evaluation factors,
including the question answering accuracy, drawing complexity and human
interpretability. Our experimental results including human evaluation
demonstrate that multi-round interactive mechanism facilitates targeted and
efficient communication between intelligent agents with decent human
interpretability.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15598" title="Abstract">arXiv:2310.15598</a> [<a href="/pdf/2310.15598" title="Download PDF">pdf</a>, <a href="/format/2310.15598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coded Computing for Half-Duplex Wireless Distributed Computing Systems  via Interference Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Youlong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+K">Kai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+Y">Yue Bi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Distributed computing frameworks such as MapReduce and Spark are often used
to process large-scale data computing jobs. In wireless scenarios, exchanging
data among distributed nodes would seriously suffer from the communication
bottleneck due to limited communication resources such as bandwidth and power.
To address this problem, we propose a coded parallel computing (CPC) scheme for
distributed computing systems where distributed nodes exchange information over
a half-duplex wireless interference network. The CPC scheme achieves the
multicast gain by utilizing coded computing to multicast coded symbols
{intended to} multiple receiver nodes and the cooperative transmission gain by
allowing multiple {transmitter} nodes to jointly deliver messages via
interference alignment. To measure communication performance, we apply the
widely used latency-oriented metric: \emph{normalized delivery time (NDT)}. It
is shown that CPC can significantly reduce the NDT by jointly exploiting the
parallel transmission and coded multicasting opportunities. Surprisingly, when
$K$ tends to infinity and the computation load is fixed, CPC approaches zero
NDT while all state-of-the-art schemes achieve positive values of NDT. Finally,
we establish an information-theoretic lower bound for the NDT-computation load
trade-off over \emph{half-duplex} network, and prove our scheme achieves the
minimum NDT within a multiplicative gap of $3$, i.e., our scheme is order
optimal.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15599" title="Abstract">arXiv:2310.15599</a> [<a href="/pdf/2310.15599" title="Download PDF">pdf</a>, <a href="/format/2310.15599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grasp Multiple Objects with One Hand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yiran Geng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Puhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tengyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyuan Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The human hand's complex kinematics allow for simultaneous grasping and
manipulation of multiple objects, essential for tasks like object transfer and
in-hand manipulation. Despite its importance, robotic multi-object grasping
remains underexplored and presents challenges in kinematics, dynamics, and
object configurations. This paper introduces MultiGrasp, a two-stage method for
multi-object grasping on a tabletop with a multi-finger dexterous hand. It
involves (i) generating pre-grasp proposals and (ii) executing the grasp and
lifting the objects. Experimental results primarily focus on dual-object
grasping and report a 44.13% success rate, showcasing adaptability to unseen
object configurations and imprecise grasps. The framework also demonstrates the
capability to grasp more than two objects, albeit at a reduced inference speed.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15602" title="Abstract">arXiv:2310.15602</a> [<a href="/pdf/2310.15602" title="Download PDF">pdf</a>, <a href="/format/2310.15602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MUSER: A Multi-View Similar Case Retrieval Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingquan Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yiran Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+F">Feng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Weixing Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CIKM 2023 Resource Track
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CIKM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Similar case retrieval (SCR) is a representative legal AI application that
plays a pivotal role in promoting judicial fairness. However, existing SCR
datasets only focus on the fact description section when judging the similarity
between cases, ignoring other valuable sections (e.g., the court's opinion)
that can provide insightful reasoning process behind. Furthermore, the case
similarities are typically measured solely by the textual semantics of the fact
descriptions, which may fail to capture the full complexity of legal cases from
the perspective of legal knowledge. In this work, we present MUSER, a similar
case retrieval dataset based on multi-view similarity measurement and
comprehensive legal element with sentence-level legal element annotations.
Specifically, we select three perspectives (legal fact, dispute focus, and law
statutory) and build a comprehensive and structured label schema of legal
elements for each of them, to enable accurate and knowledgeable evaluation of
case similarities. The constructed dataset originates from Chinese civil cases
and contains 100 query cases and 4,024 candidate cases. We implement several
text classification algorithms for legal element prediction and various
retrieval methods for retrieving similar cases on MUSER. The experimental
results indicate that incorporating legal elements can benefit the performance
of SCR models, but further efforts are still required to address the remaining
challenges posed by MUSER. The source code and dataset are released at
https://github.com/THUlawtech/MUSER.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15605" title="Abstract">arXiv:2310.15605</a> [<a href="/pdf/2310.15605" title="Download PDF">pdf</a>, <a href="/format/2310.15605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> tagE: Enabling an Embodied Agent to Understand Human Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+C">Chayan Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+A">Avik Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Pramanick%2C+P">Pradip Pramanick</a>, 
<a href="/search/cs?searchtype=author&query=Nayak%2C+T">Tapas Nayak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Natural language serves as the primary mode of communication when an
intelligent agent with a physical presence engages with human beings. While a
plethora of research focuses on natural language understanding (NLU),
encompassing endeavors such as sentiment analysis, intent prediction, question
answering, and summarization, the scope of NLU directed at situations
necessitating tangible actions by an embodied agent remains limited. The
inherent ambiguity and incompleteness inherent in natural language present
challenges for intelligent agents striving to decipher human intention. To
tackle this predicament head-on, we introduce a novel system known as task and
argument grounding for Embodied agents (tagE). At its core, our system employs
an inventive neural network model designed to extract a series of tasks from
complex task instructions expressed in natural language. Our proposed model
adopts an encoder-decoder framework enriched with nested decoding to
effectively extract tasks and their corresponding arguments from these
intricate instructions. These extracted tasks are then mapped (or grounded) to
the robot's established collection of skills, while the arguments find
grounding in objects present within the environment. To facilitate the training
and evaluation of our system, we have curated a dataset featuring complex
instructions. The results of our experiments underscore the prowess of our
approach, as it outperforms robust baseline models.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15610" title="Abstract">arXiv:2310.15610</a> [<a href="/pdf/2310.15610" title="Download PDF">pdf</a>, <a href="/format/2310.15610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Slisemap to interpret physical data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sepp%C3%A4l%C3%A4inen%2C+L">Lauri Sepp&#xe4;l&#xe4;inen</a>, 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rklund%2C+A">Anton Bj&#xf6;rklund</a>, 
<a href="/search/cs?searchtype=author&query=Besel%2C+V">Vitus Besel</a>, 
<a href="/search/cs?searchtype=author&query=Puolam%C3%A4ki%2C+K">Kai Puolam&#xe4;ki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 + 1 figures, 1 table. The datasets and source code used in the paper are available at <a href="https://www.edahelsinki.fi/papers/slisemap_phys">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Manifold visualisation techniques are commonly used to visualise
high-dimensional datasets in physical sciences. In this paper we apply a
recently introduced manifold visualisation method, called Slise, on datasets
from physics and chemistry. Slisemap combines manifold visualisation with
explainable artificial intelligence. Explainable artificial intelligence is
used to investigate the decision processes of black box machine learning models
and complex simulators. With Slisemap we find an embedding such that data items
with similar local explanations are grouped together. Hence, Slisemap gives us
an overview of the different behaviours of a black box model. This makes
Slisemap into a supervised manifold visualisation method, where the patterns in
the embedding reflect a target property. In this paper we show how Slisemap can
be used and evaluated on physical data and that Slisemap is helpful in finding
meaningful information on classification and regression models trained on these
datasets.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15612" title="Abstract">arXiv:2310.15612</a> [<a href="/pdf/2310.15612" title="Download PDF">pdf</a>, <a href="/format/2310.15612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Translation for Nko: Tools, Corpora and Baseline Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doumbouya%2C+M+K+B">Moussa Koulako Bala Doumbouya</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+B+M">Baba Mamadi Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Ciss%C3%A9%2C+S+F">Solo Farabado Ciss&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+D">Djibrila Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Sow%2C+A">Abdoulaye Sow</a>, 
<a href="/search/cs?searchtype=author&query=Doumbouya%2C+S+M">S&#xe9;r&#xe9; Moussa Doumbouya</a>, 
<a href="/search/cs?searchtype=author&query=Bangoura%2C+D">Daouda Bangoura</a>, 
<a href="/search/cs?searchtype=author&query=Bayo%2C+F+M">Fod&#xe9; Moriba Bayo</a>, 
<a href="/search/cs?searchtype=author&query=Cond%C3%A9%2C+I+S+2">Ibrahima Sory 2. Cond&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+K+M">Kalo Mory Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Piech%2C+C">Chris Piech</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C">Christopher Manning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Currently, there is no usable machine translation system for Nko, a language
spoken by tens of millions of people across multiple West African countries,
which holds significant cultural and educational value. To address this issue,
we present a set of tools, resources, and baseline results aimed towards the
development of usable machine translation systems for Nko and other languages
that do not currently have sufficiently large parallel text corpora available.
(1) Friallel: A novel collaborative parallel text curation software that
incorporates quality control through copyedit-based workflows. (2) Expansion of
the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko
translations in parallel with 204 and 40 other languages. (3) nicolingua-0005:
A collection of trilingual and bilingual corpora with 130,850 parallel segments
and monolingual corpora containing over 3 million Nko words. (4) Baseline
bilingual and multilingual neural machine translation results with the best
model scoring 30.83 English-Nko chrF++ on FLoRes-devtest.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15614" title="Abstract">arXiv:2310.15614</a> [<a href="/pdf/2310.15614" title="Download PDF">pdf</a>, <a href="/format/2310.15614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Bayesian neural networks for regression: Tackling overfitting and  computational challenges in uncertainty quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dabiran%2C+N">Nastaran Dabiran</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+B">Brandon Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Sandhu%2C+R">Rimple Sandhu</a>, 
<a href="/search/cs?searchtype=author&query=Khalil%2C+M">Mohammad Khalil</a>, 
<a href="/search/cs?searchtype=author&query=Poirel%2C+D">Dominique Poirel</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Abhijit Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Neural networks (NNs) are primarily developed within the frequentist
statistical framework. Nevertheless, frequentist NNs lack the capability to
provide uncertainties in the predictions, and hence their robustness can not be
adequately assessed. Conversely, the Bayesian neural networks (BNNs) naturally
offer predictive uncertainty by applying Bayes' theorem. However, their
computational requirements pose significant challenges. Moreover, both
frequentist NNs and BNNs suffer from overfitting issues when dealing with noisy
and sparse data, which render their predictions unwieldy away from the
available data space. To address both these problems simultaneously, we
leverage insights from a hierarchical setting in which the parameter priors are
conditional on hyperparameters to construct a BNN by applying a semi-analytical
framework known as nonlinear sparse Bayesian learning (NSBL). We call our
network sparse Bayesian neural network (SBNN) which aims to address the
practical and computational issues associated with BNNs. Simultaneously,
imposing a sparsity-inducing prior encourages the automatic pruning of
redundant parameters based on the automatic relevance determination (ARD)
concept. This process involves removing redundant parameters by optimally
selecting the precision of the parameters prior probability density functions
(pdfs), resulting in a tractable treatment for overfitting. To demonstrate the
benefits of the SBNN algorithm, the study presents an illustrative regression
problem and compares the results of a BNN using standard Bayesian inference,
hierarchical Bayesian inference, and a BNN equipped with the proposed
algorithm. Subsequently, we demonstrate the importance of considering the full
parameter posterior by comparing the results with those obtained using the
Laplace approximation with and without NSBL.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15624" title="Abstract">arXiv:2310.15624</a> [<a href="/pdf/2310.15624" title="Download PDF">pdf</a>, <a href="/format/2310.15624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D  Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinzhu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianzhu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yating Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Q">Qi Chu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yonghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Geometry plays a significant role in monocular 3D object detection. It can be
used to estimate object depth by using the perspective projection between
object's physical size and 2D projection in the image plane, which can
introduce mathematical priors into deep models. However, this projection
process also introduces error amplification, where the error of the estimated
height is amplified and reflected into the projected depth. It leads to
unreliable depth inferences and also impairs training stability. To tackle this
problem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++)
by modeling geometry projection in a probabilistic manner. This ensures depth
predictions are well-bounded and associated with a reasonable uncertainty. The
significance of introducing such geometric uncertainty is two-fold: (1). It
models the uncertainty propagation relationship of the geometry projection
during training, improving the stability and efficiency of the end-to-end model
learning. (2). It can be derived to a highly reliable confidence to indicate
the quality of the 3D detection result, enabling more reliable detection
inference. Experiments show that the proposed approach not only obtains
(state-of-the-art) SOTA performance in image-based monocular 3D detection but
also demonstrates superiority in efficacy with a simplified framework.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15632" title="Abstract">arXiv:2310.15632</a> [<a href="/pdf/2310.15632" title="Download PDF">pdf</a>, <a href="/format/2310.15632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tips for making the most of 64-bit architectures in langage design,  libraries or garbage collection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonntag%2C+B">Beno&#xee;t Sonntag</a> (UNISTRA), 
<a href="/search/cs?searchtype=author&query=Colnet%2C+D">Dominique Colnet</a> (LORIA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The 64-bit architectures that have become standard today offer unprecedented
low-level programming possibilities. For the first time in the history of
computing, the size of address registers far exceeded the physical capacity of
their bus.After a brief reminder of the possibilities offered by the small size
of addresses compared to the available 64 bits,we develop three concrete
examples of how the vacant bits of these registers can be used.Among these
examples, two of them concern the implementation of a library for a new
statically typed programming language.Firstly, the implementation of
multi-precision integers, with the aim of improving performance in terms of
both calculation speed and RAM savings.The second example focuses on the
library's handling of UTF-8 character strings.Here, the idea is to make
indexing easier by ignoring the physical size of each UTF-8 characters.Finally,
the third example is a possible enhancement of garbage collectors, in
particular the mark \&amp; sweep for the object marking phase.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15634" title="Abstract">arXiv:2310.15634</a> [<a href="/pdf/2310.15634" title="Download PDF">pdf</a>, <a href="/format/2310.15634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-Memory Architecture for High-Performance Long-Read Pre-Alignment  Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahroodi%2C+T">Taha Shahroodi</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+M">Michael Miao</a>, 
<a href="/search/cs?searchtype=author&query=Lindegger%2C+J">Joel Lindegger</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+S">Stephan Wong</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>, 
<a href="/search/cs?searchtype=author&query=Hamdioui%2C+S">Said Hamdioui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Genomics (q-bio.GN)

</div>
<p class="mathjax">With the recent move towards sequencing of accurate long reads, finding
solutions that support efficient analysis of these reads becomes more
necessary. The long execution time required for sequence alignment of long
reads negatively affects genomic studies relying on sequence alignment.
Although pre-alignment filtering as an extra step before alignment was recently
introduced to mitigate sequence alignment for short reads, these filters do not
work as efficiently for long reads. Moreover, even with efficient pre-alignment
filters, the overall end-to-end (i.e., filtering + original alignment)
execution time of alignment for long reads remains high, while the filtering
step is now a major portion of the end-to-end execution time.
<br />Our paper makes three contributions. First, it identifies data movement of
sequences between memory units and computing units as the main source of
inefficiency for pre-alignment filters of long reads. This is because although
filters reject many of these long sequencing pairs before they get to the
alignment stage, they still require a huge cost regarding time and energy
consumption for the large data transferred between memory and processor.
Second, this paper introduces an adaptation of a short-read pre-alignment
filtering algorithm suitable for long reads. We call this LongGeneGuardian.
Finally, it presents Filter-Fuse as an architecture that supports
LongGeneGuardian inside the memory. FilterFuse exploits the
Computation-In-Memory computing paradigm, eliminating the cost of data movement
in LongGeneGuardian.
<br />Our evaluations show that FilterFuse improves the execution time of filtering
by 120.47x for long reads compared to State-of-the-Art (SoTA) filter,
SneakySnake. FilterFuse also improves the end-to-end execution time of sequence
alignment by up to 49.14x and 5207.63x compared to SneakySnake with SoTA
aligner and only SoTA aligner, respectively.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15636" title="Abstract">arXiv:2310.15636</a> [<a href="/pdf/2310.15636" title="Download PDF">pdf</a>, <a href="/format/2310.15636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Career Path Prediction using Resume Representation Learning and  Skill-based Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decorte%2C+J">Jens-Joris Decorte</a>, 
<a href="/search/cs?searchtype=author&query=Van+Hautte%2C+J">Jeroen Van Hautte</a>, 
<a href="/search/cs?searchtype=author&query=Deleu%2C+J">Johannes Deleu</a>, 
<a href="/search/cs?searchtype=author&query=Develder%2C+C">Chris Develder</a>, 
<a href="/search/cs?searchtype=author&query=Demeester%2C+T">Thomas Demeester</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 3nd Workshop on Recommender Systems for Human Resources (RecSys in HR 2023) as part of RecSys 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The impact of person-job fit on job satisfaction and performance is widely
acknowledged, which highlights the importance of providing workers with next
steps at the right time in their career. This task of predicting the next step
in a career is known as career path prediction, and has diverse applications
such as turnover prevention and internal job mobility. Existing methods to
career path prediction rely on large amounts of private career history data to
model the interactions between job titles and companies. We propose leveraging
the unexplored textual descriptions that are part of work experience sections
in resumes. We introduce a structured dataset of 2,164 anonymized career
histories, annotated with ESCO occupation labels. Based on this dataset, we
present a novel representation learning approach, CareerBERT, specifically
designed for work history data. We develop a skill-based model and a text-based
model for career path prediction, which achieve 35.24% and 39.61% recall@10
respectively on our dataset. Finally, we show that both approaches are
complementary as a hybrid approach achieves the strongest result with 43.01%
recall@10.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15638" title="Abstract">arXiv:2310.15638</a> [<a href="/pdf/2310.15638" title="Download PDF">pdf</a>, <a href="/format/2310.15638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large  Language Models for Data Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Minzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Taiwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Ziems%2C+C">Caleb Ziems</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Annotated data plays a critical role in Natural Language Processing (NLP) in
training models and evaluating their performance. Given recent developments in
Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot
capability on many text-annotation tasks, comparable with or even exceeding
human annotators. Such LLMs can serve as alternatives for manual annotation,
due to lower costs and higher scalability. However, limited work has leveraged
LLMs as complementary annotators, nor explored how annotation work is best
allocated among humans and LLMs to achieve both quality and cost objectives. We
propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of
unstructured texts at scale. Under this framework, we utilize uncertainty to
estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to
be an effective means to allocate work from results on different datasets, with
up to 21% performance improvement over random baseline. For code
implementation, see https://github.com/SALT-NLP/CoAnnotating.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15641" title="Abstract">arXiv:2310.15641</a> [<a href="/pdf/2310.15641" title="Download PDF">pdf</a>, <a href="/format/2310.15641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guaranteed Coverage Prediction Intervals with Gaussian Process  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papadopoulos%2C+H">Harris Papadopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages. This work has been submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Gaussian Process Regression (GPR) is a popular regression method, which
unlike most Machine Learning techniques, provides estimates of uncertainty for
its predictions. These uncertainty estimates however, are based on the
assumption that the model is well-specified, an assumption that is violated in
most practical applications, since the required knowledge is rarely available.
As a result, the produced uncertainty estimates can become very misleading; for
example the prediction intervals (PIs) produced for the 95\% confidence level
may cover much less than 95\% of the true labels. To address this issue, this
paper introduces an extension of GPR based on a Machine Learning framework
called, Conformal Prediction (CP). This extension guarantees the production of
PIs with the required coverage even when the model is completely misspecified.
The proposed approach combines the advantages of GPR with the valid coverage
guarantee of CP, while the performed experimental results demonstrate its
superiority over existing methods.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15642" title="Abstract">arXiv:2310.15642</a> [<a href="/pdf/2310.15642" title="Download PDF">pdf</a>, <a href="/format/2310.15642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GitBug-Actions: Building Reproducible Bug-Fix Benchmarks with GitHub  Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saavedra%2C+N">Nuno Saavedra</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Andr&#xe9; Silva</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Bug-fix benchmarks are fundamental in advancing various sub-fields of
software engineering such as automatic program repair (APR) and fault
localization (FL). A good benchmark must include recent examples that
accurately reflect technologies and development practices of today. To be
executable in the long term, a benchmark must feature test suites that do not
degrade overtime due to, for example, dependencies that are no longer
available. Existing benchmarks fail in meeting both criteria. For instance,
Defects4J, one of the foremost Java benchmarks, last received an update in
2020. Moreover, full-reproducibility has been neglected by the majority of
existing benchmarks. In this paper, we present GitBug-Actions: a novel tool for
building bug-fix benchmarks with modern and fully-reproducible bug-fixes.
GitBug-Actions relies on the most popular CI platform, GitHub Actions, to
detect bug-fixes and smartly locally execute the CI pipeline in a controlled
and reproducible environment. To the best of our knowledge, we are the first to
rely on GitHub Actions to collect bug-fixes. To demonstrate our toolchain, we
deploy GitBug-Actions to build a proof-of-concept Go bug-fix benchmark
containing executable, fully-reproducible bug-fixes from different
repositories. A video demonstrating GitBug-Actions is available at:
https://youtu.be/aBWwa1sJYBs.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15643" title="Abstract">arXiv:2310.15643</a> [<a href="/pdf/2310.15643" title="Download PDF">pdf</a>, <a href="/format/2310.15643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating ICT In-House Procurement in Finland: Evaluating Legal  Frameworks and Practical Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghezzi%2C+R">Reetta Ghezzi</a>, 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+M">Minnamaria Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Vilpponen%2C+H">Hannu Vilpponen</a>, 
<a href="/search/cs?searchtype=author&query=Mikkonen%2C+T">Tommi Mikkonen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In-house procurement is a controversial issue in the field of public
procurement. Simply put, such procurement allows overlooking certain aspects of
fair and equal treatment of vendors. This paper presents qualitative research
on in-house ICT procurement within Finnish municipalities. Semi-structured
interviews were conducted to gather insights from municipal stakeholders. Using
grounded theory approach, data analysis shows intricate dynamics between
Finnish municipalities and in-house entities associated with them. Still, it is
clear that the legal framework governing in-house procurement remains intricate
and debated.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15644" title="Abstract">arXiv:2310.15644</a> [<a href="/pdf/2310.15644" title="Download PDF">pdf</a>, <a href="/format/2310.15644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear-in-Complexity Computational Strategies for Modeling and Dosimetry  at TeraHertz
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Giunzioni%2C+V">Viviana Giunzioni</a>, 
<a href="/search/math?searchtype=author&query=Ciacco%2C+G">Giuseppe Ciacco</a>, 
<a href="/search/math?searchtype=author&query=Henry%2C+C">Cl&#xe9;ment Henry</a>, 
<a href="/search/math?searchtype=author&query=Merlini%2C+A">Adrien Merlini</a>, 
<a href="/search/math?searchtype=author&query=Andriulli%2C+F+P">Francesco P. Andriulli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This work presents a fast direct solver strategy allowing full-wave modeling
and dosimetry at terahertz (THz) frequencies. The novel scheme leverages a
preconditioned combined field integral equation together with a regularizer for
its elliptic spectrum to enable its compression into a non-hierarchical
skeleton, invertible in quasi-linear complexity. Numerical results will show
the effectiveness of the new scheme in a realistic skin modeling scenario.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15645" title="Abstract">arXiv:2310.15645</a> [<a href="/pdf/2310.15645" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Light up that Droid! On the Effectiveness of Static Analysis Features  against App Obfuscation for Android Malware Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Molina-Coronado%2C+B">Borja Molina-Coronado</a>, 
<a href="/search/cs?searchtype=author&query=Ruggia%2C+A">Antonio Ruggia</a>, 
<a href="/search/cs?searchtype=author&query=Mori%2C+U">Usue Mori</a>, 
<a href="/search/cs?searchtype=author&query=Merlo%2C+A">Alessio Merlo</a>, 
<a href="/search/cs?searchtype=author&query=Mendiburu%2C+A">Alexander Mendiburu</a>, 
<a href="/search/cs?searchtype=author&query=Miguel-Alonso%2C+J">Jose Miguel-Alonso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Malware authors have seen obfuscation as the mean to bypass malware detectors
based on static analysis features. For Android, several studies have confirmed
that many anti-malware products are easily evaded with simple program
transformations. As opposed to these works, ML detection proposals for Android
leveraging static analysis features have also been proposed as
obfuscation-resilient. Therefore, it needs to be determined to what extent the
use of a specific obfuscation strategy or tool poses a risk for the validity of
ML malware detectors for Android based on static analysis features. To shed
some light in this regard, in this article we assess the impact of specific
obfuscation techniques on common features extracted using static analysis and
determine whether the changes are significant enough to undermine the
effectiveness of ML malware detectors that rely on these features. The
experimental results suggest that obfuscation techniques affect all static
analysis features to varying degrees across different tools. However, certain
features retain their validity for ML malware detection even in the presence of
obfuscation. Based on these findings, we propose a ML malware detector for
Android that is robust against obfuscation and outperforms current
state-of-the-art detectors.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15646" title="Abstract">arXiv:2310.15646</a> [<a href="/pdf/2310.15646" title="Download PDF">pdf</a>, <a href="/format/2310.15646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean Teacher DETR with Masked Feature Alignment: A Robust Domain  Adaptive Detection Transformer Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+W">Weixi Weng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chun Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Unsupervised domain adaptation object detection(UDAOD) research on Detection
Transformer(DETR) mainly focuses on feature alignment and existing methods can
be divided into two kinds, each of which has its unresolved issues. One-stage
feature alignment methods can easily lead to performance fluctuation and
training stagnation. Two-stage feature alignment method based on mean teacher
comprises a pretraining stage followed by a self-training stage, each facing
problems in obtaining reliable pretrained model and achieving consistent
performance gains. Methods mentioned above have not yet explore how to utilize
the third related domain such as target-like domain to assist adaptation. To
address these issues, we propose a two-stage framework named MTM, i.e. Mean
Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we
utilize labeled target-like images produced by image style transfer to avoid
performance fluctuation. In the self-training stage, we leverage unlabeled
target images by pseudo labels based on mean teacher and propose a module
called Object Queries Knowledge Transfer(OQKT) to ensure consistent performance
gains of the student model. Most importantly, we propose masked feature
alignment methods including Masked Domain Query-based Feature Alignment(MDQFA)
and Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a
more robust way, which not only prevent training stagnation and lead to a
robust pretrained model in the pretraining stage, but also enhance the model's
target performance in the self-training stage. Experiments on three challenging
scenarios and a theoretical analysis verify the effectiveness of MTM.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15648" title="Abstract">arXiv:2310.15648</a> [<a href="/pdf/2310.15648" title="Download PDF">pdf</a>, <a href="/format/2310.15648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Convolutional Neural Networks as Efficient Pre-trained Audio  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmid%2C+F">Florian Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Koutini%2C+K">Khaled Koutini</a>, 
<a href="/search/cs?searchtype=author&query=Widmer%2C+G">Gerhard Widmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. Source Code available at: <a href="https://github.com/fschmid56/EfficientAT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The introduction of large-scale audio datasets, such as AudioSet, paved the
way for Transformers to conquer the audio domain and replace CNNs as the
state-of-the-art neural network architecture for many tasks. Audio Spectrogram
Transformers are excellent at exploiting large datasets, creating powerful
pre-trained models that surpass CNNs when fine-tuned on downstream tasks.
However, current popular Audio Spectrogram Transformers are demanding in terms
of computational complexity compared to CNNs. Recently, we have shown that, by
employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch
up with and even outperform Transformers on large datasets. In this work, we
extend this line of research and increase the capacity of efficient CNNs by
introducing dynamic CNN blocks, constructed of dynamic non-linearities, dynamic
convolutions and attention mechanisms. We show that these dynamic CNNs
outperform traditional efficient CNNs, in terms of the performance-complexity
trade-off and parameter efficiency, at the task of audio tagging on the
large-scale AudioSet. Our experiments further indicate that the introduced
dynamic CNNs achieve better performance on downstream tasks and scale up well,
attaining Transformer performance and even outperforming them on AudioSet and
several downstream tasks.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15653" title="Abstract">arXiv:2310.15653</a> [<a href="/pdf/2310.15653" title="Download PDF">pdf</a>, <a href="/format/2310.15653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deceptive Fairness Attacks on Graphs via Meta Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jian Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yinglong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Maciejewski%2C+R">Ross Maciejewski</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiebo Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study deceptive fairness attacks on graphs to answer the following
question: How can we achieve poisoning attacks on a graph learning model to
exacerbate the bias deceptively? We answer this question via a bi-level
optimization problem and propose a meta learning-based framework named FATE.
FATE is broadly applicable with respect to various fairness definitions and
graph learning models, as well as arbitrary choices of manipulation operations.
We further instantiate FATE to attack statistical parity and individual
fairness on graph neural networks. We conduct extensive experimental
evaluations on real-world datasets in the task of semi-supervised node
classification. The experimental results demonstrate that FATE could amplify
the bias of graph neural networks with or without fairness consideration while
maintaining the utility on the downstream task. We hope this paper provides
insights into the adversarial robustness of fair graph learning and can shed
light on designing robust and fair graph learning in future studies.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15654" title="Abstract">arXiv:2310.15654</a> [<a href="/pdf/2310.15654" title="Download PDF">pdf</a>, <a href="/format/2310.15654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Detection of LLMs-Generated Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuandong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Petzold%2C+L">Linda Petzold</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We will keep updating at <a href="https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">The burgeoning capabilities of advanced large language models (LLMs) such as
ChatGPT have led to an increase in synthetic content generation with
implications across a variety of sectors, including media, cybersecurity,
public discourse, and education. As such, the ability to detect LLMs-generated
content has become of paramount importance. We aim to provide a detailed
overview of existing detection strategies and benchmarks, scrutinizing their
differences and identifying key challenges and prospects in the field,
advocating for more adaptable and robust models to enhance detection accuracy.
We also posit the necessity for a multi-faceted approach to defend against
various attacks to counter the rapidly advancing capabilities of LLMs. To the
best of our knowledge, this work is the first comprehensive survey on the
detection in the era of LLMs. We hope it will provide a broad understanding of
the current landscape of LLMs-generated content detection, offering a guiding
reference for researchers and practitioners striving to uphold the integrity of
digital information in an era increasingly dominated by synthetic content. The
relevant papers are summarized and will be consistently updated at
https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15655" title="Abstract">arXiv:2310.15655</a> [<a href="/pdf/2310.15655" title="Download PDF">pdf</a>, <a href="/format/2310.15655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking of brightness consistency in optical flow with a lightweight  CNN network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yicheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yunlong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages,7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Sparse optical flow is widely used in various computer vision tasks, however
assuming brightness consistency limits its performance in High Dynamic Range
(HDR) environments. In this work, a lightweight network is used to extract
illumination robust convolutional features and corners with strong invariance.
Modifying the typical brightness consistency of the optical flow method to the
convolutional feature consistency yields the light-robust hybrid optical flow
method. The proposed network runs at 190 FPS on a commercial CPU because it
uses only four convolutional layers to extract feature maps and score maps
simultaneously. Since the shallow network is difficult to train directly, a
deep network is designed to compute the reliability map that helps it. An
end-to-end unsupervised training mode is used for both networks. To validate
the proposed method, we compare corner repeatability and matching performance
with origin optical flow under dynamic illumination. In addition, a more
accurate visual inertial system is constructed by replacing the optical flow
method in VINS-Mono. In a public HDR dataset, it reduces translation errors by
93\%. The code is publicly available at https://github.com/linyicheng1/LET-NET.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15656" title="Abstract">arXiv:2310.15656</a> [<a href="/pdf/2310.15656" title="Download PDF">pdf</a>, <a href="/format/2310.15656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Momentum Gradient-based Untargeted Attack on Hypergraph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+S">Stjepan Picek</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhonglin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haixing Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Hypergraph Neural Networks (HGNNs) have been successfully applied in various
hypergraph-related tasks due to their excellent higher-order representation
capabilities. Recent works have shown that deep learning models are vulnerable
to adversarial attacks. Most studies on graph adversarial attacks have focused
on Graph Neural Networks (GNNs), and the study of adversarial attacks on HGNNs
remains largely unexplored. In this paper, we try to reduce this gap. We design
a new HGNNs attack model for the untargeted attack, namely MGHGA, which focuses
on modifying node features. We consider the process of HGNNs training and use a
surrogate model to implement the attack before hypergraph modeling.
Specifically, MGHGA consists of two parts: feature selection and feature
modification. We use a momentum gradient mechanism to choose the attack node
features in the feature selection module. In the feature modification module,
we use two feature generation approaches (direct modification and sign
gradient) to enable MGHGA to be employed on discrete and continuous datasets.
We conduct extensive experiments on five benchmark datasets to validate the
attack performance of MGHGA in the node and the visual object classification
tasks. The results show that MGHGA improves performance by an average of 2%
compared to the than the baselines.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15657" title="Abstract">arXiv:2310.15657</a> [<a href="/pdf/2310.15657" title="Download PDF">pdf</a>, <a href="/format/2310.15657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash  Detection with Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengzhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Boyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+X">Xing Che</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dandan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE/ACM International Conference on Software Engineering 2024 (ICSE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Mobile applications have become a ubiquitous part of our daily life,
providing users with access to various services and utilities. Text input, as
an important interaction channel between users and applications, plays an
important role in core functionality such as search queries, authentication,
messaging, etc. However, certain special text (e.g., -18 for Font Size) can
cause the app to crash, and generating diversified unusual inputs for fully
testing the app is highly demanded. Nevertheless, this is also challenging due
to the combination of explosion dilemma, high context sensitivity, and complex
constraint relations. This paper proposes InputBlaster which leverages the LLM
to automatically generate unusual text inputs for mobile app crash detection.
It formulates the unusual inputs generation problem as a task of producing a
set of test generators, each of which can yield a batch of unusual text inputs
under the same mutation rule. In detail, InputBlaster leverages LLM to produce
the test generators together with the mutation rules serving as the reasoning
chain, and utilizes the in-context learning schema to demonstrate the LLM with
examples for boosting the performance. InputBlaster is evaluated on 36 text
input widgets with cash bugs involving 31 popular Android apps, and results
show that it achieves 78% bug detection rate, with 136% higher than the best
baseline. Besides, we integrate it with the automated GUI testing tool and
detect 37 unseen crashes in real-world apps from Google Play.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15658" title="Abstract">arXiv:2310.15658</a> [<a href="/pdf/2310.15658" title="Download PDF">pdf</a>, <a href="/format/2310.15658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Region-controlled Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Junjie Kang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jinsong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shiqi Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image style transfer is a challenging task in computational vision. Existing
algorithms transfer the color and texture of style images by controlling the
neural network's feature layers. However, they fail to control the strength of
textures in different regions of the content image. To address this issue, we
propose a training method that uses a loss function to constrain the style
intensity in different regions. This method guides the transfer strength of
style features in different regions based on the gradient relationship between
style and content images. Additionally, we introduce a novel feature fusion
method that linearly transforms content features to resemble style features
while preserving their semantic relationships. Extensive experiments have
demonstrated the effectiveness of our proposed approach.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15662" title="Abstract">arXiv:2310.15662</a> [<a href="/pdf/2310.15662" title="Download PDF">pdf</a>, <a href="/format/2310.15662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Generalized Additive Model and Its Applications in Electric  Load Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+R">Rui Ren</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xinyue Gu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liang Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Electric load forecasting is an indispensable component of electric power
system planning and management. Inaccurate load forecasting may lead to the
threat of outages or a waste of energy. Accurate electric load forecasting is
challenging when there is limited data or even no data, such as load
forecasting in holiday, or under extreme weather conditions. As high-stakes
decision-making usually follows after load forecasting, model interpretability
is crucial for the adoption of forecasting models. In this paper, we propose an
interactive GAM which is not only interpretable but also can incorporate
specific domain knowledge in electric power industry for improved performance.
This boosting-based GAM leverages piecewise linear functions and can be learned
through our efficient algorithm. In both public benchmark and electricity
datasets, our interactive GAM outperforms current state-of-the-art methods and
demonstrates good generalization ability in the cases of extreme weather
events. We launched a user-friendly web-based tool based on interactive GAM and
already incorporated it into our eForecaster product, a unified AI platform for
electricity forecasting.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15664" title="Abstract">arXiv:2310.15664</a> [<a href="/pdf/2310.15664" title="Download PDF">pdf</a>, <a href="/format/2310.15664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expression Syntax Information Bottleneck for Math Word Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by SIGIR 2022. The code can be found at <a href="https://github.com/menik1126/math_ESIB">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Math Word Problems (MWP) aims to automatically solve mathematical questions
given in texts. Previous studies tend to design complex models to capture
additional information in the original text so as to enable the model to gain
more comprehensive features. In this paper, we turn our attention in the
opposite direction, and work on how to discard redundant features containing
spurious correlations for MWP. To this end, we design an Expression Syntax
Information Bottleneck method for MWP (called ESIB) based on variational
information bottleneck, which extracts essential features of expression syntax
tree while filtering latent-specific redundancy containing syntax-irrelevant
features. The key idea of ESIB is to encourage multiple models to predict the
same expression syntax tree for different problem representations of the same
problem by mutual learning so as to capture consistent information of
expression syntax tree and discard latent-specific redundancy. To improve the
generalization ability of the model and generate more diverse expressions, we
design a self-distillation loss to encourage the model to rely more on the
expression syntax information in the latent space. Experimental results on two
large-scale benchmarks show that our model not only achieves state-of-the-art
results but also generates more diverse solutions. The code is available.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15665" title="Abstract">arXiv:2310.15665</a> [<a href="/pdf/2310.15665" title="Download PDF">pdf</a>, <a href="/format/2310.15665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A posteriori error estimates for nonconforming discretizations of  singularly perturbed biharmonic operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gallistl%2C+D">Dietmar Gallistl</a>, 
<a href="/search/math?searchtype=author&query=Tian%2C+S">Shudan Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">For the pure biharmonic equation and a biharmonic singular perturbation
problem, a residual-based error estimator is introduced which applies to many
existing nonconforming finite elements. The error estimator involves the local
best-approximation error of the finite element function by piecewise polynomial
functions of the degree determining the expected approximation order, which
need not coincide with the maximal polynomial degree of the element, for
example if bubble functions are used. The error estimator is shown to be
reliable and locally efficient up to this polynomial best-approximation error
and oscillations of the right-hand side.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15668" title="Abstract">arXiv:2310.15668</a> [<a href="/pdf/2310.15668" title="Download PDF">pdf</a>, <a href="/format/2310.15668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph Motifs and Their Extensions Beyond Binary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Geon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Seokbum Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Jihoon Ko</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunju Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of VLDB 2020 paper <a href="/abs/2003.01853">arXiv:2003.01853</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Hypergraphs naturally represent group interactions, which are omnipresent in
many domains: collaborations of researchers, co-purchases of items, and joint
interactions of proteins, to name a few. In this work, we propose tools for
answering the following questions: (Q1) what are the structural design
principles of real-world hypergraphs? (Q2) how can we compare local structures
of hypergraphs of different sizes? (Q3) how can we identify domains from which
hypergraphs are? We first define hypergraph motifs (h-motifs), which describe
the overlapping patterns of three connected hyperedges. Then, we define the
significance of each h-motif in a hypergraph as its occurrences relative to
those in properly randomized hypergraphs. Lastly, we define the characteristic
profile (CP) as the vector of the normalized significance of every h-motif.
Regarding Q1, we find that h-motifs' occurrences in 11 real-world hypergraphs
from 5 domains are clearly distinguished from those of randomized hypergraphs.
Then, we demonstrate that CPs capture local structural patterns unique to each
domain, and thus comparing CPs of hypergraphs addresses Q2 and Q3. The concept
of CP is extended to represent the connectivity pattern of each node or
hyperedge as a vector, which proves useful in node classification and hyperedge
prediction. Our algorithmic contribution is to propose MoCHy, a family of
parallel algorithms for counting h-motifs' occurrences in a hypergraph. We
theoretically analyze their speed and accuracy and show empirically that the
advanced approximate version MoCHy-A+ is more accurate and faster than the
basic approximate and exact versions, respectively. Furthermore, we explore
ternary hypergraph motifs that extends h-motifs by taking into account not only
the presence but also the cardinality of intersections among hyperedges. This
extension proves beneficial for all previously mentioned applications.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15669" title="Abstract">arXiv:2310.15669</a> [<a href="/pdf/2310.15669" title="Download PDF">pdf</a>, <a href="/format/2310.15669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Methods for Multiscale Coarse Approximations of Diffusion Models  in Perforated Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boutilier%2C+M">Miranda Boutilier</a>, 
<a href="/search/math?searchtype=author&query=Brenner%2C+K">Konstantin Brenner</a>, 
<a href="/search/math?searchtype=author&query=Dolean%2C+V">Victorita Dolean</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 14 figures, submitted to Journal of Computational Physics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">For the Poisson equation posed in a domain containing a large number of
polygonal perforations, we propose a low-dimensional coarse approximation space
based on a coarse polygonal partitioning of the domain. Similarly to other
multiscale numerical methods, this coarse space is spanned by locally discrete
harmonic basis functions. Along the subdomain boundaries, the basis functions
are piecewise polynomial. The main contribution of this article is an error
estimate regarding the H1-projection over the coarse space which depends only
on the regularity of the solution over the edges of the coarse partitioning.
For a specific edge refinement procedure, the error analysis establishes
superconvergence of the method even if the true solution has a low general
regularity. Combined with domain decomposition (DD) methods, the coarse space
leads to an efficient two-level iterative linear solver which reaches the
fine-scale finite element error in few iterations. It also bodes well as a
preconditioner for Krylov methods and provides scalability with respect to the
number of subdomains. Numerical experiments showcase the increased precision of
the coarse approximation as well as the efficiency and scalability of the
coarse space as a component of a DD algorithm.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15670" title="Abstract">arXiv:2310.15670</a> [<a href="/pdf/2310.15670" title="Download PDF">pdf</a>, <a href="/format/2310.15670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Linyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Sima%2C+C">Chonghao Sima</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingdong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongyang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current research is primarily dedicated to advancing the accuracy of
camera-only 3D object detectors (apprentice) through the knowledge transferred
from LiDAR- or multi-modal-based counterparts (expert). However, the presence
of the domain gap between LiDAR and camera features, coupled with the inherent
incompatibility in temporal fusion, significantly hinders the effectiveness of
distillation-based enhancements for apprentices. Motivated by the success of
uni-modal distillation, an apprentice-friendly expert model would predominantly
rely on camera features, while still achieving comparable performance to
multi-modal models. To this end, we introduce VCD, a framework to improve the
camera-only apprentice model, including an apprentice-friendly multi-modal
expert and temporal-fusion-friendly distillation supervision. The multi-modal
expert VCD-E adopts an identical structure as that of the camera-only
apprentice in order to alleviate the feature disparity, and leverages LiDAR
input as a depth prior to reconstruct the 3D scene, achieving the performance
on par with other heterogeneous multi-modal experts. Additionally, a
fine-grained trajectory-based distillation module is introduced with the
purpose of individually rectifying the motion misalignment for each object in
the scene. With those improvements, our camera-only apprentice VCD-A sets new
state-of-the-art on nuScenes with a score of 63.1% NDS.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15672" title="Abstract">arXiv:2310.15672</a> [<a href="/pdf/2310.15672" title="Download PDF">pdf</a>, <a href="/format/2310.15672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Much Context Does My Attention-Based ASR System Need?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flynn%2C+R">Robert Flynn</a>, 
<a href="/search/cs?searchtype=author&query=Ragni%2C+A">Anton Ragni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">For the task of speech recognition, the use of more than 30 seconds of
acoustic context during training is uncommon, and under-investigated in
literature. In this work, we examine the effect of scaling the sequence length
used to train/evaluate (dense-attention based) acoustic and language models on
speech recognition performance. For these experiments a dataset of roughly
100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5
seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets
Earnings-22 and Tedlium demonstrate a benefit from training with around 80
seconds of acoustic context, showing up to a 14.9% relative improvement from a
limited context baseline. Furthermore, we perform a system combination with
long-context transformer language models via beam search for a fully
long-context ASR system, with results that are competitive with the current
state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15676" title="Abstract">arXiv:2310.15676</a> [<a href="/pdf/2310.15676" title="Download PDF">pdf</a>, <a href="/format/2310.15676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive  Survey and Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yinjie Lei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-modal 3D scene understanding has gained considerable attention due to
its wide applications in many areas, such as autonomous driving and
human-computer interaction. Compared to conventional single-modal 3D
understanding, introducing an additional modality not only elevates the
richness and precision of scene interpretation but also ensures a more robust
and resilient understanding. This becomes especially crucial in varied and
challenging environments where solely relying on 3D data might be inadequate.
While there has been a surge in the development of multi-modal 3D methods over
past three years, especially those integrating multi-camera images (3D+2D) and
textual descriptions (3D+language), a comprehensive and in-depth review is
notably absent. In this article, we present a systematic survey of recent
progress to bridge this gap. We begin by briefly introducing a background that
formally defines various 3D multi-modal tasks and summarizes their inherent
challenges. After that, we present a novel taxonomy that delivers a thorough
categorization of existing methods according to modalities and tasks, exploring
their respective strengths and limitations. Furthermore, comparative results of
recent approaches on several benchmark datasets, together with insightful
analysis, are offered. Finally, we discuss the unresolved issues and provide
several potential avenues for future research.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15677" title="Abstract">arXiv:2310.15677</a> [<a href="/pdf/2310.15677" title="Download PDF">pdf</a>, <a href="/format/2310.15677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot-Relay : Building-Wide, Calibration-Less Visual Servoing with  Learned Sensor Handover Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Robinson%2C+L">Luke Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+P">Paul Newman</a>, 
<a href="/search/cs?searchtype=author&query=De+Martini%2C+D">Daniele De Martini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted to the 18th International Symposium on Experimental Robotics (ISER 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present a system which grows and manages a network of remote viewpoints
during the natural installation cycle for a newly installed camera network or a
newly deployed robot fleet. No explicit notion of camera position or
orientation is required, neither global - i.e. relative to a building plan -
nor local - i.e. relative to an interesting point in a room. Furthermore, no
metric relationship between viewpoints is required. Instead, we leverage our
prior work in effective remote control without extrinsic or intrinsic
calibration and extend it to the multi-camera setting. In this, we memorise,
from simultaneous robot detections in the tracker thread, soft pixel-wise
topological connections between viewpoints. We demonstrate our system with
repeated autonomous traversals of workspaces connected by a network of six
cameras across a productive office environment.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15681" title="Abstract">arXiv:2310.15681</a> [<a href="/pdf/2310.15681" title="Download PDF">pdf</a>, <a href="/format/2310.15681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed  Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+S">Shintaro Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the real-valued combinatorial pure exploration of the multi-armed
bandit in the fixed-budget setting. We first introduce the Combinatorial
Successive Asign (CSA) algorithm, which is the first algorithm that can
identify the best action even when the size of the action class is
exponentially large with respect to the number of arms. We show that the upper
bound of the probability of error of the CSA algorithm matches a lower bound up
to a logarithmic factor in the exponent. Then, we introduce another algorithm
named the Minimax Combinatorial Successive Accepts and Rejects
(Minimax-CombSAR) algorithm for the case where the size of the action class is
polynomial, and show that it is optimal, which matches a lower bound. Finally,
we experimentally compare the algorithms with previous methods and show that
our algorithm performs better.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15683" title="Abstract">arXiv:2310.15683</a> [<a href="/pdf/2310.15683" title="Download PDF">pdf</a>, <a href="/format/2310.15683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prevalence and prevention of large language model use in crowd work
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veselovsky%2C+V">Veniamin Veselovsky</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+M+H">Manoel Horta Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Cozzolino%2C+P">Philip Cozzolino</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+A">Andrew Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Rothschild%2C+D">David Rothschild</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> VV and MHR equal contribution. 14 pages, 1 figure, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We show that the use of large language models (LLMs) is prevalent among crowd
workers, and that targeted mitigation strategies can significantly reduce, but
not eliminate, LLM use. On a text summarization task where workers were not
directed in any way regarding their LLM use, the estimated prevalence of LLM
use was around 30%, but was reduced by about half by asking workers to not use
LLMs and by raising the cost of using them, e.g., by disabling copy-pasting.
Secondary analyses give further insight into LLM use and its prevention: LLM
use yields high-quality but homogeneous responses, which may harm research
concerned with human (rather than model) behavior and degrade future models
trained with crowdsourced data. At the same time, preventing LLM use may be at
odds with obtaining high-quality responses; e.g., when requesting workers not
to use LLMs, summaries contained fewer keywords carrying essential information.
Our estimates will likely change as LLMs increase in popularity or
capabilities, and as norms around their usage change. Yet, understanding the
co-evolution of LLM-based tools and users is key to maintaining the validity of
research done using crowdsourcing, and we provide a critical baseline before
widespread adoption ensues.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15684" title="Abstract">arXiv:2310.15684</a> [<a href="/pdf/2310.15684" title="Download PDF">pdf</a>, <a href="/format/2310.15684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Biomedical Abstractive Summarisation with Knowledge  Aggregation from Citation Papers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Goldsack%2C+T">Tomas Goldsack</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Abstracts derived from biomedical literature possess distinct domain-specific
characteristics, including specialised writing styles and biomedical
terminologies, which necessitate a deep understanding of the related
literature. As a result, existing language models struggle to generate
technical summaries that are on par with those produced by biomedical experts,
given the absence of domain-specific background knowledge. This paper aims to
enhance the performance of language models in biomedical abstractive
summarisation by aggregating knowledge from external papers cited within the
source article. We propose a novel attention-based citation aggregation model
that integrates domain-specific knowledge from citation papers, allowing neural
networks to generate summaries by leveraging both the paper content and
relevant knowledge from citation papers. Furthermore, we construct and release
a large-scale biomedical summarisation dataset that serves as a foundation for
our research. Extensive experiments demonstrate that our model outperforms
state-of-the-art approaches and achieves substantial improvements in
abstractive biomedical text summarisation.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15686" title="Abstract">arXiv:2310.15686</a> [<a href="/pdf/2310.15686" title="Download PDF">pdf</a>, <a href="/ps/2310.15686" title="Download PostScript">ps</a>, <a href="/format/2310.15686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assume-Guarantee Verification of Strategic Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mikulski%2C+%C5%81">&#x141;ukasz Mikulski</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes in Computer Science, 13753 (2022), 173--191
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Model checking of strategic abilities is a notoriously hard problem, even
more so in the realistic case of agents with imperfect information.
Assume-guarantee reasoning can be of great help here, providing a way to
decompose the complex problem into a small set of exponentially easier
subproblems. In this paper, we propose two schemes for assume-guarantee
verification of alternating-time temporal logic with imperfect information. We
prove the soundness of both schemes, and discuss their completeness. We
illustrate the method by examples based on known benchmarks, and show
experimental results that demonstrate the practical benefits of the approach.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15688" title="Abstract">arXiv:2310.15688</a> [<a href="/pdf/2310.15688" title="Download PDF">pdf</a>, <a href="/format/2310.15688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nighttime Thermal Infrared Image Colorization with Feedback-based Object  Appearance Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fu-Ya Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shu-Lin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yi-Jun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kai-Fu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chang-Yong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong-Jie Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures. arXiv admin note: text overlap with <a href="/abs/2208.02960">arXiv:2208.02960</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Stable imaging in adverse environments (e.g., total darkness) makes thermal
infrared (TIR) cameras a prevalent option for night scene perception. However,
the low contrast and lack of chromaticity of TIR images are detrimental to
human interpretation and subsequent deployment of RGB-based vision algorithms.
Therefore, it makes sense to colorize the nighttime TIR images by translating
them into the corresponding daytime color images (NTIR2DC). Despite the
impressive progress made in the NTIR2DC task, how to improve the translation
performance of small object classes is under-explored. To address this problem,
we propose a generative adversarial network incorporating feedback-based object
appearance learning (FoalGAN). Specifically, an occlusion-aware mixup module
and corresponding appearance consistency loss are proposed to reduce the
context dependence of object translation. As a representative example of small
objects in nighttime street scenes, we illustrate how to enhance the realism of
traffic light by designing a traffic light appearance loss. To further improve
the appearance learning of small objects, we devise a dual feedback learning
strategy to selectively adjust the learning frequency of different samples. In
addition, we provide pixel-level annotation for a subset of the Brno dataset,
which can facilitate the research of NTIR image understanding under multiple
weather conditions. Extensive experiments illustrate that the proposed FoalGAN
is not only effective for appearance learning of small objects, but also
outperforms other image translation methods in terms of semantic preservation
and edge consistency for the NTIR2DC task.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15689" title="Abstract">arXiv:2310.15689</a> [<a href="/pdf/2310.15689" title="Download PDF">pdf</a>, <a href="/format/2310.15689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creating a silver standard for patent simplification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casola%2C+S">Silvia Casola</a>, 
<a href="/search/cs?searchtype=author&query=Lavelli%2C+A">Alberto Lavelli</a>, 
<a href="/search/cs?searchtype=author&query=Saggion%2C+H">Horacio Saggion</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been published at SIGIR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Patents are legal documents that aim at protecting inventions on the one hand
and at making technical knowledge circulate on the other. Their complex style
-- a mix of legal, technical, and extremely vague language -- makes their
content hard to access for humans and machines and poses substantial challenges
to the information retrieval community. This paper proposes an approach to
automatically simplify patent text through rephrasing. Since no in-domain
parallel simplification data exist, we propose a method to automatically
generate a large-scale silver standard for patent sentences. To obtain
candidates, we use a general-domain paraphrasing system; however, the process
is error-prone and difficult to control. Thus, we pair it with proper filters
and construct a cleaner corpus that can successfully be used to train a
simplification system. Human evaluation of the synthetic silver corpus shows
that it is considered grammatical, adequate, and contains simple sentences.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15690" title="Abstract">arXiv:2310.15690</a> [<a href="/pdf/2310.15690" title="Download PDF">pdf</a>, <a href="/ps/2310.15690" title="Download PostScript">ps</a>, <a href="/format/2310.15690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed with Power-Enhanced Residual Network for Interpolation  and Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noorizadegan%2C+A">Amir Noorizadegan</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+D+L">D.L. Young</a>, 
<a href="/search/cs?searchtype=author&query=Hon%2C+Y+C">Y.C. Hon</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+S">C.S. Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Analysis of PDEs (math.AP)

</div>
<p class="mathjax">This paper introduces a novel neural network structure called the
Power-Enhancing residual network, designed to improve interpolation
capabilities for both smooth and non-smooth functions in 2D and 3D settings. By
adding power terms to residual elements, the architecture boosts the network's
expressive power. The study explores network depth, width, and optimization
methods, showing the architecture's adaptability and performance advantages.
Consistently, the results emphasize the exceptional accuracy of the proposed
Power-Enhancing residual network, particularly for non-smooth functions.
Real-world examples also confirm its superiority over plain neural network in
terms of accuracy, convergence, and efficiency. The study also looks at the
impact of deeper network. Moreover, the proposed architecture is also applied
to solving the inverse Burgers' equation, demonstrating superior performance.
In conclusion, the Power-Enhancing residual network offers a versatile solution
that significantly enhances neural network capabilities. The codes implemented
are available at: \url{https://github.com/CMMAi/ResNet_for_PINN}.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15692" title="Abstract">arXiv:2310.15692</a> [<a href="/pdf/2310.15692" title="Download PDF">pdf</a>, <a href="/format/2310.15692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Trajectory Prediction with Cooperative Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Strohbeck%2C+J">Jan Strohbeck</a>, 
<a href="/search/cs?searchtype=author&query=Maschke%2C+S">Sebastian Maschke</a>, 
<a href="/search/cs?searchtype=author&query=Mertens%2C+M">Max Mertens</a>, 
<a href="/search/cs?searchtype=author&query=Buchholz%2C+M">Michael Buchholz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 26th IEEE International Conference on Intelligent Transportation Systems 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">For automated driving, predicting the future trajectories of other road users
in complex traffic situations is a hard problem. Modern neural networks use the
past trajectories of traffic participants as well as map data to gather hints
about the possible driver intention and likely maneuvers. With increasing
connectivity between cars and other traffic actors, cooperative information is
another source of data that can be used as inputs for trajectory prediction
algorithms. Connected actors might transmit their intended path or even
complete planned trajectories to other actors, which simplifies the prediction
problem due to the imposed constraints. In this work, we outline the benefits
of using this source of data for trajectory prediction and propose a
graph-based neural network architecture that can leverage this additional data.
We show that the network performance increases substantially if cooperative
data is present. Also, our proposed training scheme improves the network's
performance even for cases where no cooperative information is available. We
also show that the network can deal with inaccurate cooperative data, which
allows it to be used in real automated driving environments.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15693" title="Abstract">arXiv:2310.15693</a> [<a href="/pdf/2310.15693" title="Download PDF">pdf</a>, <a href="/format/2310.15693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automated Recipe Genre Classification using Semi-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sakib%2C+N">Nazmus Sakib</a>, 
<a href="/search/cs?searchtype=author&query=Shahariar%2C+G+M">G. M. Shahariar</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+M">Md. Mohsinul Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+K">Md. Kamrul Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+H">Hasan Mahmud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Sharing cooking recipes is a great way to exchange culinary ideas and provide
instructions for food preparation. However, categorizing raw recipes found
online into appropriate food genres can be challenging due to a lack of
adequate labeled data. In this study, we present a dataset named the
``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking
Recipe Dataset" that contains two million culinary recipes labeled in
respective categories with extended named entities extracted from recipe
descriptions. This collection of data includes various features such as title,
NER, directions, and extended NER, as well as nine different labels
representing genres including bakery, drinks, non-veg, vegetables, fast food,
cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends
the size of the Named Entity Recognition (NER) list to address missing named
entities like heat, time or process from the recipe directions using two NER
extraction tools. 3A2M+ dataset provides a comprehensive solution to the
various challenging recipe-related tasks, including classification, named
entity recognition, and recipe generation. Furthermore, we have demonstrated
traditional machine learning, deep learning and pre-trained language models to
classify the recipes into their corresponding genre and achieved an overall
accuracy of 98.6\%. Our investigation indicates that the title feature played a
more significant role in classifying the genre.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15694" title="Abstract">arXiv:2310.15694</a> [<a href="/pdf/2310.15694" title="Download PDF">pdf</a>, <a href="/format/2310.15694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COPF: Continual Learning Human Preference through Optimal Policy Fitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Han Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yuanzhao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Fitting (COPF), in which we estimate a series of optimal policies using
the Monte Carlo method, and then continually fit the policy sequence with the
function regularization. COPF involves a single learning phase and doesn't
necessitate complex reinforcement learning. Importantly, it shares the
capability with RLHF to learn from unlabeled data, making it flexible for
continual preference learning. Our experimental results show that COPF
outperforms strong Continuous learning (CL) baselines when it comes to
consistently aligning with human preferences on different tasks and domains.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15699" title="Abstract">arXiv:2310.15699</a> [<a href="/pdf/2310.15699" title="Download PDF">pdf</a>, <a href="/format/2310.15699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DACOOP-A: Decentralized Adaptive Cooperative Pursuit via Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+D">Dengyu Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+W">Wei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianjiang Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages; This manuscript has been accepted by IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Integrating rule-based policies into reinforcement learning promises to
improve data efficiency and generalization in cooperative pursuit problems.
However, most implementations do not properly distinguish the influence of
neighboring robots in observation embedding or inter-robot interaction rules,
leading to information loss and inefficient cooperation. This paper proposes a
cooperative pursuit algorithm named Decentralized Adaptive COOperative Pursuit
via Attention (DACOOP-A) by empowering reinforcement learning with artificial
potential field and attention mechanisms. An attention-based framework is
developed to emphasize important neighbors by concurrently integrating the
learned attention scores into observation embedding and inter-robot interaction
rules. A KL divergence regularization is introduced to alleviate the resultant
learning stability issue. Improvements in data efficiency and generalization
are demonstrated through numerical simulations. Extensive quantitative analysis
and ablation studies are performed to illustrate the advantages of the proposed
modules. Real-world experiments are performed to justify the feasibility of
deploying DACOOP-A in physical systems.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15702" title="Abstract">arXiv:2310.15702</a> [<a href="/pdf/2310.15702" title="Download PDF">pdf</a>, <a href="/format/2310.15702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Biomedical Lay Summarisation with External Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldsack%2C+T">Tomas Goldsack</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chen Tang</a>, 
<a href="/search/cs?searchtype=author&query=Scarton%2C+C">Carolina Scarton</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Previous approaches for automatic lay summarisation are exclusively reliant
on the source article that, given it is written for a technical audience (e.g.,
researchers), is unlikely to explicitly define all technical concepts or state
all of the background information that is relevant for a lay audience. We
address this issue by augmenting eLife, an existing biomedical lay
summarisation dataset, with article-specific knowledge graphs, each containing
detailed information on relevant biomedical concepts. Using both automatic and
human evaluations, we systematically investigate the effectiveness of three
different approaches for incorporating knowledge graphs within lay
summarisation models, with each method targeting a distinct area of the
encoder-decoder model architecture. Our results confirm that integrating
graph-based domain knowledge can significantly benefit lay summarisation by
substantially increasing the readability of generated text and improving the
explanation of technical concepts.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15703" title="Abstract">arXiv:2310.15703</a> [<a href="/pdf/2310.15703" title="Download PDF">pdf</a>, <a href="/ps/2310.15703" title="Download PostScript">ps</a>, <a href="/format/2310.15703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally recoverable codes from the matrix-product construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galindo%2C+C">Carlos Galindo</a>, 
<a href="/search/cs?searchtype=author&query=Hernando%2C+F">Fernando Hernando</a>, 
<a href="/search/cs?searchtype=author&query=Munuera%2C+C">Carlos Munuera</a>, 
<a href="/search/cs?searchtype=author&query=Ruano%2C+D">Diego Ruano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Matrix-product constructions giving rise to locally recoverable codes are
considered, both the classical $r$ and $(r,\delta)$ localities. We study the
recovery advantages offered by the constituent codes and also by the defining
matrices of the matrix product codes. Finally, we extend these methods to a
particular variation of matrix-product codes and quasi-cyclic codes.
Singleton-optimal locally recoverable codes and almost Singleton-optimal codes,
with length larger than the finite field size, are obtained, some of them with
superlinear length.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15705" title="Abstract">arXiv:2310.15705</a> [<a href="/pdf/2310.15705" title="Download PDF">pdf</a>, <a href="/format/2310.15705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-based Scheduling for Information Accuracy and Freshness in  Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gudwani%2C+H">Hitesh Gudwani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">We consider a system of multiple sources, a single communication channel, and
a single monitoring station. Each source measures a time-varying quantity with
varying levels of accuracy and one of them sends its update to the monitoring
station via the channel. The probability of success of each attempted
communication is a function of the source scheduled for transmitting its
update. Both the probability of correct measurement and the probability of
successful transmission of all the sources are unknown to the scheduler. The
metric of interest is the reward received by the system which depends on the
accuracy of the last update received by the destination and the
Age-of-Information (AoI) of the system. We model our scheduling problem as a
variant of the multi-arm bandit problem with sources as different arms. We
compare the performance of all $4$ standard bandit policies, namely, ETC,
$\epsilon$-greedy, UCB, and TS suitably adjusted to our system model via
simulations. In addition, we provide analytical guarantees of $2$ of these
policies, ETC, and $\epsilon$-greedy. Finally, we characterize the lower bound
on the cumulative regret achievable by any policy.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15706" title="Abstract">arXiv:2310.15706</a> [<a href="/pdf/2310.15706" title="Download PDF">pdf</a>, <a href="/format/2310.15706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving large flexible job shop scheduling instances by generating a  diverse set of scheduling policies with deep reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Echeverria%2C+I">Imanol Echeverria</a>, 
<a href="/search/cs?searchtype=author&query=Murua%2C+M">Maialen Murua</a>, 
<a href="/search/cs?searchtype=author&query=Santana%2C+R">Roberto Santana</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied
in the literature, and multiple approaches have been proposed within the
heuristic, exact, and metaheuristic methods. However, the industry's demand to
be able to respond in real-time to disruptive events has generated the
necessity to be able to generate new schedules within a few seconds. Among
these methods, under this constraint, only dispatching rules (DRs) are capable
of generating schedules, even though their quality can be improved. To improve
the results, recent methods have been proposed for modeling the FJSSP as a
Markov Decision Process (MDP) and employing reinforcement learning to create a
policy that generates an optimal solution assigning operations to machines.
Nonetheless, there is still room for improvement, particularly in the larger
FJSSP instances which are common in real-world scenarios. Therefore, the
objective of this paper is to propose a method capable of robustly solving
large instances of the FJSSP. To achieve this, we propose a novel way of
modeling the FJSSP as an MDP using graph neural networks. We also present two
methods to make inference more robust: generating a diverse set of scheduling
policies that can be parallelized and limiting them using DRs. We have tested
our approach on synthetically generated instances and various public benchmarks
and found that our approach outperforms dispatching rules and achieves better
results than three other recent deep reinforcement learning methods on larger
FJSSP instances.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15707" title="Abstract">arXiv:2310.15707</a> [<a href="/pdf/2310.15707" title="Download PDF">pdf</a>, <a href="/ps/2310.15707" title="Download PostScript">ps</a>, <a href="/format/2310.15707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Clustering for Coexistence between Near-field and Far-field  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaidi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zhiguo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Karagiannidis%2C+G+K">George K. Karagiannidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This letter investigates the coexistence between near-field (NF) and
far-field (FF) communications, where multiple FF users are clustered to be
served on the beams of legacy NF users, via non-orthogonal multiple access
(NOMA). Three different successive interference cancellation (SIC) decoding
strategies are proposed and a sum rate maximization problem is formulated to
optimize the assignment and decoding order. The beam allocation problem is
further reformulated as an overlapping coalitional game, which facilitates the
the design of the proposed clustering algorithm. The optimal decoding order in
each cluster is also derived, which can be integrated into the proposed
clustering. Simulation results demonstrate that the proposed clustering
algorithm is able to significantly improve the sum rate of the considered
system, and the developed strategies achieve different trade-offs between sum
rate and fairness.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15711" title="Abstract">arXiv:2310.15711</a> [<a href="/pdf/2310.15711" title="Download PDF">pdf</a>, <a href="/format/2310.15711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Online String Matching through Linked Weak Factors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Palmer%2C+M+N">Matthew N. Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Faro%2C+S">Simone Faro</a>, 
<a href="/search/cs?searchtype=author&query=Scafiti%2C+S">Stefano Scafiti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Online string matching is a computational problem involving the search for
patterns or substrings in a large text dataset, with the pattern and text being
processed sequentially, without prior access to the entire text. Its relevance
stems from applications in data compression, data mining, text editing, and
bioinformatics, where rapid and efficient pattern matching is crucial. Various
solutions have been proposed over the past few decades, employing diverse
techniques. Recently, weak recognition approaches have attracted increasing
attention. This paper presents Hash Chain, a new algorithm based on a robust
weak factor recognition approach that connects adjacent factors through
hashing. Despite its O(nm) complexity, the algorithm exhibits a sublinear
behavior in practice and achieves superior performance compared to the most
effective algorithms.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15712" title="Abstract">arXiv:2310.15712</a> [<a href="/pdf/2310.15712" title="Download PDF">pdf</a>, <a href="/format/2310.15712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNeSF: Generalizable Neural Semantic Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanlin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mengqi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiwen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G+H">Gim Hee Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurPIS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">3D scene segmentation based on neural implicit representation has emerged
recently with the advantage of training only on 2D supervision. However,
existing approaches still requires expensive per-scene optimization that
prohibits generalization to novel scenes during inference. To circumvent this
problem, we introduce a generalizable 3D segmentation framework based on
implicit representation. Specifically, our framework takes in multi-view image
features and semantic maps as the inputs instead of only spatial information to
avoid overfitting to scene-specific geometric and semantic information. We
propose a novel soft voting mechanism to aggregate the 2D semantic information
from different views for each 3D point. In addition to the image features, view
difference information is also encoded in our framework to predict the voting
scores. Intuitively, this allows the semantic information from nearby views to
contribute more compared to distant ones. Furthermore, a visibility module is
also designed to detect and filter out detrimental information from occluded
views. Due to the generalizability of our proposed method, we can synthesize
semantic maps or conduct 3D semantic segmentation for novel scenes with solely
2D semantic supervision. Experimental results show that our approach achieves
comparable performance with scene-specific approaches. More importantly, our
approach can even outperform existing strong supervision-based approaches with
only 2D annotations. Our source code is available at:
https://github.com/HLinChen/GNeSF.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15719" title="Abstract">arXiv:2310.15719</a> [<a href="/pdf/2310.15719" title="Download PDF">pdf</a>, <a href="/format/2310.15719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Linear Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pramanik%2C+S">Subhojeet Pramanik</a>, 
<a href="/search/cs?searchtype=author&query=Elelimy%2C+E">Esraa Elelimy</a>, 
<a href="/search/cs?searchtype=author&query=Machado%2C+M+C">Marlos C. Machado</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+A">Adam White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> transformers, reinforcement learning, partial observability
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The self-attention mechanism in the transformer architecture is capable of
capturing long-range dependencies and it is the main reason behind its
effectiveness in processing sequential data. Nevertheless, despite their
success, transformers have two significant drawbacks that still limit their
broader applicability: (1) In order to remember past information, the
self-attention mechanism requires access to the whole history to be provided as
context. (2) The inference cost in transformers is expensive. In this paper we
introduce recurrent alternatives to the transformer self-attention mechanism
that offer a context-independent inference cost, leverage long-range
dependencies effectively, and perform well in practice. We evaluate our
approaches in reinforcement learning problems where the aforementioned
computational limitations make the application of transformers nearly
infeasible. We quantify the impact of the different components of our
architecture in a diagnostic environment and assess performance gains in 2D and
3D pixel-based partially-observable environments. When compared to a
state-of-the-art architecture, GTrXL, inference in our approach is at least 40%
cheaper while reducing memory use in more than 50%. Our approach either
performs similarly or better than GTrXL, improving more than 37% upon GTrXL
performance on harder tasks.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15720" title="Abstract">arXiv:2310.15720</a> [<a href="/pdf/2310.15720" title="Download PDF">pdf</a>, <a href="/format/2310.15720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble of Task-Specific Language Models for Brain Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumaran%2C+S">Sanjai Kumaran</a>, 
<a href="/search/cs?searchtype=author&query=Arun%2C+A">Arvindh Arun</a>, 
<a href="/search/cs?searchtype=author&query=John%2C+J">Jerrin John</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Language models have been shown to be rich enough to encode fMRI activations
of certain Regions of Interest in our Brains. Previous works have explored
transfer learning from representations learned for popular natural language
processing tasks for predicting brain responses. In our work, we improve the
performance of such encoders by creating an ensemble model out of 10 popular
Language Models (2 syntactic and 8 semantic). We beat the current baselines by
10% on average across all ROIs through our ensembling methods.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15722" title="Abstract">arXiv:2310.15722</a> [<a href="/pdf/2310.15722" title="Download PDF">pdf</a>, <a href="/format/2310.15722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Temp: Relation-Aware Temporal Representation Learning for Temporal  Knowledge Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kunze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S+C">Soyeon Caren Han</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+J">Josiah Poon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting
aims to predict the missing entity from a fact in the future, posing a
challenge that aligns more closely with real-world prediction problems.
Existing research mostly encodes entities and relations using sequential graph
neural networks applied to recent snapshots. However, these approaches tend to
overlook the ability to skip irrelevant snapshots according to entity-related
relations in the query and disregard the importance of explicit temporal
information. To address this, we propose our model, Re-Temp (Relation-Aware
Temporal Representation Learning), which leverages explicit temporal embedding
as input and incorporates skip information flow after each timestamp to skip
unnecessary information for prediction. Additionally, we introduce a two-phase
forward propagation method to prevent information leakage. Through the
evaluation on six TKGC (extrapolation) datasets, we demonstrate that our model
outperforms all eight recent state-of-the-art models by a significant margin.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15724" title="Abstract">arXiv:2310.15724</a> [<a href="/pdf/2310.15724" title="Download PDF">pdf</a>, <a href="/format/2310.15724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variator: Accelerating Pre-trained Models with Plug-and-Play Compression  Modules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaojun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuqi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenbin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pengle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhengyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Ruobing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Findings of EMNLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models (PLMs) have achieved remarkable results on NLP
tasks but at the expense of huge parameter sizes and the consequent
computational costs. In this paper, we propose Variator, a parameter-efficient
acceleration method that enhances computational efficiency through
plug-and-play compression plugins. Compression plugins are designed to reduce
the sequence length via compressing multiple hidden vectors into one and
trained with original PLMs frozen. Different from traditional model
acceleration methods, which compress PLMs to smaller sizes, Variator offers two
distinct advantages: (1) In real-world applications, the plug-and-play nature
of our compression plugins enables dynamic selection of different compression
plugins with varying acceleration ratios based on the current workload. (2) The
compression plugin comprises a few compact neural network layers with minimal
parameters, significantly saving storage and memory overhead, particularly in
scenarios with a growing number of tasks. We validate the effectiveness of
Variator on seven datasets. Experimental results show that Variator can save
53% computational costs using only 0.9% additional parameters with a
performance drop of less than 2%. Moreover, when the model scales to billions
of parameters, Variator matches the strong performance of uncompressed PLMs.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15725" title="Abstract">arXiv:2310.15725</a> [<a href="/pdf/2310.15725" title="Download PDF">pdf</a>, <a href="/format/2310.15725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-adaptive DETR for Crowded Pedestrian Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Feng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+J">Jiaxu Leng</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+J">Ji Gan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">DEtection TRansformer (DETR) and its variants (DETRs) have been successfully
applied to crowded pedestrian detection, which achieved promising performance.
However, we find that, in different degrees of crowded scenes, the number of
DETRs' queries must be adjusted manually, otherwise, the performance would
degrade to varying degrees. In this paper, we first analyze the two current
query generation methods and summarize four guidelines for designing the
adaptive query generation method. Then, we propose Rank-based Adaptive Query
Generation (RAQG) to alleviate the problem. Specifically, we design a rank
prediction head that can predict the rank of the lowest confidence positive
training sample produced by the encoder. Based on the predicted rank, we design
an adaptive selection method that can adaptively select coarse detection
results produced by the encoder to generate queries. Moreover, to train the
rank prediction head better, we propose Soft Gradient L1 Loss. The gradient of
Soft Gradient L1 Loss is continuous, which can describe the relationship
between the loss value and the updated value of model parameters granularly.
Our method is simple and effective, which can be plugged into any DETRs to make
it query-adaptive in theory. The experimental results on Crowdhuman dataset and
Citypersons dataset show that our method can adaptively generate queries for
DETRs and achieve competitive results. Especially, our method achieves
state-of-the-art 39.4% MR on Crowdhuman dataset.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15727" title="Abstract">arXiv:2310.15727</a> [<a href="/pdf/2310.15727" title="Download PDF">pdf</a>, <a href="/ps/2310.15727" title="Download PostScript">ps</a>, <a href="/format/2310.15727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Assume-Guarantee Verification of Strategic Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mikulski%2C+%C5%81">&#x141;ukasz Mikulski</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 21st International Conference on Autonomous Agents and Multiagent
  Systems, {AAMAS} 2022, Auckland, New Zealand, May 9-13, 2022, 1702--1704
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Formal verification of strategic abilities is a hard problem. We propose to
use the methodology of assume-guarantee reasoning in order to facilitate model
checking of alternating-time temporal logic with imperfect information and
imperfect recall.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15737" title="Abstract">arXiv:2310.15737</a> [<a href="/pdf/2310.15737" title="Download PDF">pdf</a>, <a href="/format/2310.15737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-preserving image coding based on Conditional Diffusion models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pezone%2C+F">Francesco Pezone</a>, 
<a href="/search/cs?searchtype=author&query=Musa%2C+O">Osman Musa</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>, 
<a href="/search/cs?searchtype=author&query=Barbarossa%2C+S">Sergio Barbarossa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted at ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Semantic communication, rather than on a bit-by-bit recovery of the
transmitted messages, focuses on the meaning and the goal of the communication
itself. In this paper, we propose a novel semantic image coding scheme that
preserves the semantic content of an image, while ensuring a good trade-off
between coding rate and image quality. The proposed Semantic-Preserving Image
Coding based on Conditional Diffusion Models (SPIC) transmitter encodes a
Semantic Segmentation Map (SSM) and a low-resolution version of the image to be
transmitted. The receiver then reconstructs a high-resolution image using a
Denoising Diffusion Probabilistic Models (DDPM) doubly conditioned to the SSM
and the low-resolution image. As shown by the numerical examples, compared to
state-of-the-art (SOTA) approaches, the proposed SPIC exhibits a better balance
between the conventional rate-distortion trade-off and the preservation of
semantically-relevant features.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15741" title="Abstract">arXiv:2310.15741</a> [<a href="/pdf/2310.15741" title="Download PDF">pdf</a>, <a href="/format/2310.15741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Medical Image Classification using Prototype Learning and  Privileged Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallee%2C+L">Luisa Gallee</a>, 
<a href="/search/cs?searchtype=author&query=Beer%2C+M">Meinrad Beer</a>, 
<a href="/search/cs?searchtype=author&query=Goetz%2C+M">Michael Goetz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023 Medical Image Computing and Computer Assisted Intervention
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Medical Image Computing and
  Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023. S.
  435-445
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Interpretability is often an essential requirement in medical imaging.
Advanced deep learning methods are required to address this need for
explainability and high performance. In this work, we investigate whether
additional information available during the training process can be used to
create an understandable and powerful model. We propose an innovative solution
called Proto-Caps that leverages the benefits of capsule networks, prototype
learning and the use of privileged information. Evaluating the proposed
solution on the LIDC-IDRI dataset shows that it combines increased
interpretability with above state-of-the-art prediction performance. Compared
to the explainable baseline model, our method achieves more than 6 % higher
accuracy in predicting both malignancy (93.0 %) and mean characteristic
features of lung nodules. Simultaneously, the model provides case-based
reasoning with prototype representations that allow visual validation of
radiologist-defined attributes.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15742" title="Abstract">arXiv:2310.15742</a> [<a href="/pdf/2310.15742" title="Download PDF">pdf</a>, <a href="/format/2310.15742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Diffusion Models for ECG Imputation with an Augmented Template  Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jenkins%2C+A">Alexander Jenkins</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zehua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+F+S">Fu Siong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Mandic%2C+D">Danilo Mandic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pulsative signals such as the electrocardiogram (ECG) are extensively
collected as part of routine clinical care. However, noisy and poor-quality
recordings, leading to missing values, are a major issue for signals collected
using mobile health systems, decreasing the signal quality and affecting the
automated downstream tasks. Recent studies have explored imputation of missing
values for ECG with probabilistic time-series models. Nevertheless, in
comparison with the deterministic models, their performance is still limited,
as the variations across subjects and heart-beat relationships are not
explicitly considered in the training objective. In this work, to improve the
ECG imputation and forecasting accuracy with probabilistic models, we present
an template-guided denoising diffusion probabilistic model, PulseDiff, which is
conditioned an informative prior for a range of health conditions.
Specifically, 1) we first extract a subject-level pulsative template from the
observation as an informative prior of missing values, which captures the
personal characteristics; 2) we then add beat-level stochastic shift terms on
the template for prior augmentation, which considers the beat-level variance of
positioning and amplitude; 3) we finally design a confidence score to consider
the health condition of subject, which ensures our prior is provided in a safe
way. Experiments with the PTBXL dataset reveal PulseDiff improves the
performance of two strong DDPMs baseline models, CSDI and SSSD$^{S4}$,
verifying our method guides the generation of DDPMs while managing the
uncertainty. When combining with SSSD$^{S4}$, our PulseDiff method outperforms
the leading deterministic model for short-interval missing data and is
comparable for long-interval data loss.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15743" title="Abstract">arXiv:2310.15743</a> [<a href="/pdf/2310.15743" title="Download PDF">pdf</a>, <a href="/format/2310.15743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot  Document-Level Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+S">Shiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shu&#x27;ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fukun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yawen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lijie Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">How to identify semantic relations among entities in a document when only a
few labeled documents are available? Few-shot document-level relation
extraction (FSDLRE) is crucial for addressing the pervasive data scarcity
problem in real-world scenarios. Metric-based meta-learning is an effective
framework widely adopted for FSDLRE, which constructs class prototypes for
classification. However, existing works often struggle to obtain class
prototypes with accurate relational semantics: 1) To build prototype for a
target relation type, they aggregate the representations of all entity pairs
holding that relation, while these entity pairs may also hold other relations,
thus disturbing the prototype. 2) They use a set of generic NOTA
(none-of-the-above) prototypes across all tasks, neglecting that the NOTA
semantics differs in tasks with different target relation types. In this paper,
we propose a relation-aware prototype learning method for FSDLRE to strengthen
the relational semantics of prototype representations. By judiciously
leveraging the relation descriptions and realistic NOTA instances as guidance,
our method effectively refines the relation prototypes and generates
task-specific NOTA prototypes. Extensive experiments demonstrate that our
method outperforms state-of-the-art approaches by average 2.61% $F_1$ across
various settings of two FSDLRE benchmarks.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15745" title="Abstract">arXiv:2310.15745</a> [<a href="/pdf/2310.15745" title="Download PDF">pdf</a>, <a href="/format/2310.15745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Battery-Less Energy Harvesting Devices in Multi-hop  Industrial Wireless Sensor Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Leemput%2C+D">Dries Van Leemput</a>, 
<a href="/search/cs?searchtype=author&query=Hoebeke%2C+J">Jeroen Hoebeke</a>, 
<a href="/search/cs?searchtype=author&query=De+Poorter%2C+E">Eli De Poorter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Industrial wireless sensor networks enable real-time data collection,
analysis, and control by interconnecting diverse industrial devices. In these
industrial settings, power outlets are not always available, and reliance on
battery power can be impractical due to the need for frequent battery
replacement or stringent safety regulations. Battery-less energy harvesters
present a suitable alternative for powering these devices. However, these
energy harvesters, equipped with supercapacitors instead of batteries, suffer
from intermittent on-off behavior due to their limited energy storage capacity.
As a result, they struggle with extended or frequent energy-consuming phases of
multi-hop network formation, such as network joining and synchronization. To
address these challenges, our work proposes three strategies for integrating
battery-less energy harvesting devices into industrial multi-hop wireless
sensor networks. In contrast to other works, our work prioritizes the
mitigation of intermittency-related issues, rather than focusing solely on
average energy consumption, as is typically the case with battery-powered
devices. For each of the proposed strategies, we provide an in-depth discussion
of their suitability based on several critical factors, including the type of
energy source, storage capacity, device mobility, latency, and reliability.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15746" title="Abstract">arXiv:2310.15746</a> [<a href="/pdf/2310.15746" title="Download PDF">pdf</a>, <a href="/format/2310.15746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Failures Pave the Way: Enhancing Large Language Models through  Tuning-free Rule Accumulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zeyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by the EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have showcased impressive performance. However,
due to their inability to capture relationships among samples, these frozen
LLMs inevitably keep repeating similar mistakes. In this work, we propose our
Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving
their performance by learning from previous mistakes. Considering data arrives
sequentially, LLMs gradually accumulate rules from incorrect cases, forming a
rule collection. These rules are then utilized by the LLMs to avoid making
similar mistakes when processing subsequent inputs. Moreover, the rules remain
independent of the primary prompts, seamlessly complementing prompt design
strategies. Experimentally, we show that TRAN improves over recent baselines by
a large margin.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15747" title="Abstract">arXiv:2310.15747</a> [<a href="/pdf/2310.15747" title="Download PDF">pdf</a>, <a href="/format/2310.15747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Temporal and Causal Reasoners for Video  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+D">Dohwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+S">Ji Soo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+W">Wooyoung Kang</a>, 
<a href="/search/cs?searchtype=author&query=Roh%2C+B">Byungseok Roh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H+J">Hyunwoo J. Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted paper at EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have shown remarkable performances on a wide
range of natural language understanding and generation tasks. We observe that
the LLMs provide effective priors in exploiting $\textit{linguistic shortcuts}$
for temporal and causal reasoning in Video Question Answering (VideoQA).
However, such priors often cause suboptimal results on VideoQA by leading the
model to over-rely on questions, $\textit{i.e.}$, $\textit{linguistic bias}$,
while ignoring visual content. This is also known as `ungrounded guesses' or
`hallucinations'. To address this problem while leveraging LLMs' prior on
VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to
predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping
the source pair and the target label to understand their complex relationships,
$\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs,
respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to
LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five
challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general
framework that is applicable to various LLMs (OPT and GPT-J) and consistently
improves their performances. We empirically demonstrate that Flipped-VQA not
only enhances the exploitation of linguistic shortcuts but also mitigates the
linguistic bias, which causes incorrect answers over-relying on the question.
Code is available at https://github.com/mlvlab/Flipped-VQA.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15751" title="Abstract">arXiv:2310.15751</a> [<a href="/pdf/2310.15751" title="Download PDF">pdf</a>, <a href="/format/2310.15751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-Based Eigenvalue Optimization for Electromagnetic Cavities with  Built-in Mode Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziegler%2C+A">Anna Ziegler</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+R">Robert Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Isensee%2C+V">Victoria Isensee</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A+D">Anh Duc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6ps%2C+S">Sebastian Sch&#xf6;ps</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Shape optimization with respect to eigenvalues of a cavity plays an important
role in the design of new resonators or in the optimization of existing ones.
In our paper, we propose a gradient-based optimization scheme, which we enhance
with closed-form shape derivatives of the system matrices. Based on these, we
can compute accurate derivatives of eigenvalues, eigenmodes and the cost
function with respect to the geometry, which significantly reduces the
computational effort of the optimizer. We demonstrate our work by applying it
to the 9-cell TESLA cavity, for which we tune the design parameters of the
computational model to match the design criteria for devices in realistic use
cases. Since eigenvalues may cross during the shape optimization of a cavity,
we propose a new algorithm based on an eigenvalue matching procedure, to ensure
the optimization of the desired mode in order to also enable successful
matching along large shape variations.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15752" title="Abstract">arXiv:2310.15752</a> [<a href="/pdf/2310.15752" title="Download PDF">pdf</a>, <a href="/format/2310.15752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Language Models into Direct Speech Translation: An  Inference-Time Solution to Control Gender Inflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fucci%2C+D">Dennis Fucci</a>, 
<a href="/search/cs?searchtype=author&query=Gaido%2C+M">Marco Gaido</a>, 
<a href="/search/cs?searchtype=author&query=Papi%2C+S">Sara Papi</a>, 
<a href="/search/cs?searchtype=author&query=Cettolo%2C+M">Mauro Cettolo</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+M">Matteo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Bentivogli%2C+L">Luisa Bentivogli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">When translating words referring to the speaker, speech translation (ST)
systems should not resort to default masculine generics nor rely on potentially
misleading vocal traits. Rather, they should assign gender according to the
speakers' preference. The existing solutions to do so, though effective, are
hardly feasible in practice as they involve dedicated model re-training on
gender-labeled ST data. To overcome these limitations, we propose the first
inference-time solution to control speaker-related gender inflections in ST.
Our approach partially replaces the (biased) internal language model (LM)
implicitly learned by the ST decoder with gender-specific external LMs.
Experiments on en-&gt;es/fr/it show that our solution outperforms the base models
and the best training-time mitigation strategy by up to 31.0 and 1.6 points in
gender accuracy, respectively, for feminine forms. The gains are even larger
(up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits
conflict with their gender.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15754" title="Abstract">arXiv:2310.15754</a> [<a href="/pdf/2310.15754" title="Download PDF">pdf</a>, <a href="/format/2310.15754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear MIM-width of the Square of Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%B8gemo%2C+S">Svein H&#xf8;gemo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages. To appear in NIKT 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Graph parameters measure the amount of structure (or lack thereof) in a graph
that makes it amenable to being decomposed in a way that facilitates dynamic
programming. Graph decompositions and their associated parameters are important
both in practice (as a tool for designing robust algorithms for NP-hard
problems) and in theory (relating large classes of problems to the graphs on
which they are solvable in polynomial time).
<br />Linear MIM-width is a variant of the graph parameter MIM-width, introduced by
Vatshelle. MIM-width is a parameter that is constant for many classes of
graphs. Most graph classes which have been shown to have constant MIM-width
also have constant linear MIM-width. However, computing the (linear) MIM-width
of graphs, or showing that it is hard, has proven to be a huge challenge. To
date, the only graph class with unbounded linear MIM-width, whose linear
MIM-width can be computed in polynomial time, is the trees. In this follow-up,
we show that for any tree $T$ with linear MIM-width $k$, the linear MIM-width
of its square $T^2$ always lies between $k$ and $2k$, and that these bounds are
tight for all $k$.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15756" title="Abstract">arXiv:2310.15756</a> [<a href="/pdf/2310.15756" title="Download PDF">pdf</a>, <a href="/ps/2310.15756" title="Download PostScript">ps</a>, <a href="/format/2310.15756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Low-SNR Asymptotic Capacity of Optical Wireless Channels with  Average-Intensity Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longguang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper investigates the capacity of an optical wireless channel with an
average-intensity constraint in the low signal-to-noise ratio (SNR) regime.
When the average intensity of the input is no larger than a small
$\mathcal{E}$, the capacity scales as $\mathcal{E}\sqrt{ \frac{\log
\frac{1}{\mathcal{E}}}{2} }$.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15757" title="Abstract">arXiv:2310.15757</a> [<a href="/pdf/2310.15757" title="Download PDF">pdf</a>, <a href="/format/2310.15757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Differences in Values Influence Disagreements in Online Discussions?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Meer%2C+M">Michiel van der Meer</a>, 
<a href="/search/cs?searchtype=author&query=Vossen%2C+P">Piek Vossen</a>, 
<a href="/search/cs?searchtype=author&query=Jonker%2C+C+M">Catholijn M. Jonker</a>, 
<a href="/search/cs?searchtype=author&query=Murukannaiah%2C+P+K">Pradeep K. Murukannaiah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as main paper at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Disagreements are common in online discussions. Disagreement may foster
collaboration and improve the quality of a discussion under some conditions.
Although there exist methods for recognizing disagreement, a deeper
understanding of factors that influence disagreement is lacking in the
literature. We investigate a hypothesis that differences in personal values are
indicative of disagreement in online discussions. We show how state-of-the-art
models can be used for estimating values in online discussions and how the
estimated values can be aggregated into value profiles. We evaluate the
estimated value profiles based on human-annotated agreement labels. We find
that the dissimilarity of value profiles correlates with disagreement in
specific cases. We also find that including value information in agreement
prediction improves performance.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15758" title="Abstract">arXiv:2310.15758</a> [<a href="/pdf/2310.15758" title="Download PDF">pdf</a>, <a href="/format/2310.15758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend  Existing Ones?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petrak%2C+D">Dominic Petrak</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+N+S">Nafise Sadat Moosavi</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Rozanov%2C+N">Nikolai Rozanov</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be presented at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Learning from free-text human feedback is essential for dialog systems, but
annotated data is scarce and usually covers only a small fraction of error
types known in conversational AI. Instead of collecting and annotating new
datasets from scratch, recent advances in synthetic dialog generation could be
used to augment existing dialog datasets with the necessary annotations.
However, to assess the feasibility of such an effort, it is important to know
the types and frequency of free-text human feedback included in these datasets.
In this work, we investigate this question for a variety of commonly used
dialog datasets, including MultiWoZ, SGD, BABI, PersonaChat,
Wizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot.
Using our observations, we derive new taxonomies for the annotation of
free-text human feedback in dialogs and investigate the impact of including
such data in response generation for three SOTA language generation models,
including GPT-2, LLAMA, and Flan-T5. Our findings provide new insights into the
composition of the datasets examined, including error types, user response
types, and the relations between them.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15761" title="Abstract">arXiv:2310.15761</a> [<a href="/pdf/2310.15761" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent-based models of social behaviour and communication in evacuations:  A systematic review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Templeton%2C+A">Anne Templeton</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+H">Hui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Gwynne%2C+S">Steve Gwynne</a>, 
<a href="/search/cs?searchtype=author&query=Hunt%2C+A">Aoife Hunt</a>, 
<a href="/search/cs?searchtype=author&query=Thompson%2C+P">Pete Thompson</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6ster%2C+G">Gerta K&#xf6;ster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print submitted to Safety Science special issue following the 2023 Pedestrian and Evacuation Dynamics conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Most modern agent-based evacuation models involve interactions between
evacuees. However, the assumed reasons for interactions and portrayal of them
may be overly simple. Research from social psychology suggests that people
interact and communicate with one another when evacuating and evacuee response
is impacted by the way information is communicated. Thus, we conducted a
systematic review of agent-based evacuation models to identify 1) how social
interactions and communication approaches between agents are simulated, and 2)
what key variables related to evacuation are addressed in these models. We
searched Web of Science and ScienceDirect to identify articles that simulated
information exchange between agents during evacuations, and social behaviour
during evacuations. From the final 70 included articles, we categorised eight
types of social interaction that increased in social complexity from collision
avoidance to social influence based on strength of social connections with
other agents. In the 17 models which simulated communication, we categorised
four ways that agents communicate information: spatially through information
trails or radii around agents, via social networks and via external
communication. Finally, the variables either manipulated or measured in the
models were categorised into the following groups: environmental condition,
personal attributes of the agents, procedure, and source of information. We
discuss promising directions for agent-based evacuation models to capture the
effects of communication and group dynamics on evacuee behaviour. Moreover, we
demonstrate how communication and group dynamics may impact the variables
commonly used in agent-based evacuation models.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15762" title="Abstract">arXiv:2310.15762</a> [<a href="/pdf/2310.15762" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SharkGraph: A Time Series Distributed Graph System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Derong Tang</a> (tencent)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Current graph systems can easily process billions of data, however when
increased to exceed hundred billions, the performance decreases dramatically,
time series data always be very huge, consequently computation on time series
graphs still remains challenging nowadays. In current piece of work, we
introduces SharkGraph, a (distributed file system) DFS-based time series graph
system, used a novel storage structure (Time Series Graph Data File) TGF, By
reading file stream to iterate graph computation, SharkGraph is able to execute
batch graph query, simulation, data mining, or clustering algorithm on exceed
hundred billions edge size industry graph. Through well defined experiments
that shows SharkGraph performs well on large-scale graph processing, also can
support time traversal for graphs, and recover state at any position in the
timeline. By repeating experiments reported for existing distributed systems
like GraphX, we demonstrate that SharkGraph can easily handle hundreds billions
of data, rather than GraphX which met many problems such as memory issues and
skewed distribution on graph traversal. Compared with other graph systems
SharkGraph uses less memory and more efficiently to process the same graph.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15764" title="Abstract">arXiv:2310.15764</a> [<a href="/pdf/2310.15764" title="Download PDF">pdf</a>, <a href="/format/2310.15764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing, calibrating, and improving Semi-supervised Learning  performance via simple Ensemble Projector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh-Binh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent studies on semi-supervised learning (SSL) have achieved great success.
Despite their promising performance, current state-of-the-art methods tend
toward increasingly complex designs at the cost of introducing more network
components and additional training procedures. In this paper, we propose a
simple method named Ensemble Projectors Aided for Semi-supervised Learning
(EPASS), which focuses mainly on improving the learned embeddings to boost the
performance of the existing contrastive joint-training semi-supervised learning
frameworks. Unlike standard methods, where the learned embeddings from one
projector are stored in memory banks to be used with contrastive learning,
EPASS stores the ensemble embeddings from multiple projectors in memory banks.
As a result, EPASS improves generalization, strengthens feature representation,
and boosts performance. For instance, EPASS improves strong baselines for
semi-supervised learning by 39.47\%/31.39\%/24.70\% top-1 error rate, while
using only 100k/1\%/10\% of labeled data for SimMatch, and achieves
40.24\%/32.64\%/25.90\% top-1 error rate for CoMatch on the ImageNet dataset.
These improvements are consistent across methods, network architectures, and
datasets, proving the general effectiveness of the proposed methods. Code is
available at https://github.com/beandkay/EPASS.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15766" title="Abstract">arXiv:2310.15766</a> [<a href="/pdf/2310.15766" title="Download PDF">pdf</a>, <a href="/format/2310.15766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Learning via Conditional Prevalence Adjustment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M">Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A+Q">Alan Q. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heejong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sabuncu%2C+M+R">Mert R. Sabuncu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WACV
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Healthcare data often come from multiple sites in which the correlations
between confounding variables can vary widely. If deep learning models exploit
these unstable correlations, they might fail catastrophically in unseen sites.
Although many methods have been proposed to tackle unstable correlations, each
has its limitations. For example, adversarial training forces models to
completely ignore unstable correlations, but doing so may lead to poor
predictive performance. Other methods (e.g. Invariant risk minimization [4])
try to learn domain-invariant representations that rely only on stable
associations by assuming a causal data-generating process (input X causes class
label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X),
which are common in computer vision. We propose a method called CoPA
(Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that
(1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z
generate X, and (2) the unstable conditional prevalence in each site E fully
accounts for the unstable correlations between X and Y . Our crucial
observation is that confounding variables are routinely recorded in healthcare
settings and the prevalence can be readily estimated, for example, from a set
of (Y, Z) samples (no need for corresponding samples of X). CoPA can work even
if there is a single training site, a scenario which is often overlooked by
existing methods. Our experiments on synthetic and real data show CoPA beating
competitive baselines.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15768" title="Abstract">arXiv:2310.15768</a> [<a href="/pdf/2310.15768" title="Download PDF">pdf</a>, <a href="/ps/2310.15768" title="Download PostScript">ps</a>, <a href="/format/2310.15768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Message-Cognizant Assistance and Feedback for the Gaussian Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lapidoth%2C+A">Amos Lapidoth</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Ligong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yiming Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A formula is derived for the capacity of the Gaussian channel with a
benevolent message-cognizant rate-limited helper that provides a noncausal
description of the noise to the encoder and decoder. This capacity is strictly
larger than when the helper is message oblivious, with the difference being
particularly pronounced at low signal-to-noise ratios. It is shown that in this
setup, a feedback link from the receiver to the encoder does not increase
capacity. However, in the presence of such a link, said capacity can be
achieved even if the helper is oblivious to the transmitted message.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15772" title="Abstract">arXiv:2310.15772</a> [<a href="/pdf/2310.15772" title="Download PDF">pdf</a>, <a href="/format/2310.15772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Understanding of Why Users Share Hate Speech on Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geissler%2C+D">Dominique Geissler</a>, 
<a href="/search/cs?searchtype=author&query=Maarouf%2C+A">Abdurahman Maarouf</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Hate speech on social media threatens the mental and physical well-being of
individuals and is further responsible for real-world violence. An important
driver behind the spread of hate speech and thus why hateful posts can go viral
are reshares, yet little is known about why users reshare hate speech. In this
paper, we present a comprehensive, causal analysis of the user attributes that
make users reshare hate speech. However, causal inference from observational
social media data is challenging, because such data likely suffer from
selection bias, and there is further confounding due to differences in the
vulnerability of users to hate speech. We develop a novel, three-step causal
framework: (1) We debias the observational social media data by applying
inverse propensity scoring. (2) We use the debiased propensity scores to model
the latent vulnerability of users to hate speech as a latent embedding. (3) We
model the causal effects of user attributes on users' probability of sharing
hate speech, while controlling for the latent vulnerability of users to hate
speech. Compared to existing baselines, a particular strength of our framework
is that it models causal effects that are non-linear, yet still explainable. We
find that users with fewer followers, fewer friends, and fewer posts share more
hate speech. Younger accounts, in return, share less hate speech. Overall,
understanding the factors that drive users to share hate speech is crucial for
detecting individuals at risk of engaging in harmful behavior and for designing
effective mitigation strategies.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15773" title="Abstract">arXiv:2310.15773</a> [<a href="/pdf/2310.15773" title="Download PDF">pdf</a>, <a href="/format/2310.15773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BLESS: Benchmarking Large Language Models on Sentence Simplification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kew%2C+T">Tannon Kew</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+A">Alison Chi</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%A1squez-Rodr%C3%ADguez%2C+L">Laura V&#xe1;squez-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+S">Sweta Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Aumiller%2C+D">Dennis Aumiller</a>, 
<a href="/search/cs?searchtype=author&query=Alva-Manchego%2C+F">Fernando Alva-Manchego</a>, 
<a href="/search/cs?searchtype=author&query=Shardlow%2C+M">Matthew Shardlow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted to EMNLP 2023 as a main long paper. 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We present BLESS, a comprehensive performance benchmark of the most recent
state-of-the-art large language models (LLMs) on the task of text
simplification (TS). We examine how well off-the-shelf LLMs can solve this
challenging task, assessing a total of 44 models, differing in size,
architecture, pre-training methods, and accessibility, on three test sets from
different domains (Wikipedia, news, and medical) under a few-shot setting. Our
analysis considers a suite of automatic metrics as well as a large-scale
quantitative investigation into the types of common edit operations performed
by the different models. Furthermore, we perform a manual qualitative analysis
on a subset of model outputs to better gauge the quality of the generated
simplifications. Our evaluation indicates that the best LLMs, despite not being
trained on TS, perform comparably with state-of-the-art TS baselines.
Additionally, we find that certain LLMs demonstrate a greater range and
diversity of edit operations. Our performance benchmark will be available as a
resource for the development of future TS methods and evaluation metrics.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15774" title="Abstract">arXiv:2310.15774</a> [<a href="/pdf/2310.15774" title="Download PDF">pdf</a>, <a href="/format/2310.15774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> navlie: A Python Package for State Estimation on Lie Groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cossette%2C+C+C">Charles Champagne Cossette</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+M">Mitchell Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Korotkine%2C+V">Vassili Korotkine</a>, 
<a href="/search/cs?searchtype=author&query=del+Castillo+Bernal%2C+A">Arturo del Castillo Bernal</a>, 
<a href="/search/cs?searchtype=author&query=Shalaby%2C+M+A">Mohammed Ayman Shalaby</a>, 
<a href="/search/cs?searchtype=author&query=Forbes%2C+J+R">James Richard Forbes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures, presented at the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The ability to rapidly test a variety of algorithms for an arbitrary state
estimation task is valuable in the prototyping phase of navigation systems. Lie
group theory is now mainstream in the robotics community, and hence estimation
prototyping tools should allow state definitions that belong to manifolds. A
new package, called navlie, provides a framework that allows a user to model a
large class of problems by implementing a set of classes complying with a
generic interface. Once accomplished, navlie provides a variety of on-manifold
estimation algorithms that can run directly on these classes. The package also
provides a built-in library of common models, as well as many useful utilities.
The open-source project can be found at https://github.com/decargroup/navlie.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15777" title="Abstract">arXiv:2310.15777</a> [<a href="/pdf/2310.15777" title="Download PDF">pdf</a>, <a href="/format/2310.15777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MindLLM: Pre-training Lightweight Large Language Model from Scratch,  Evaluations and Domain Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yizhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huashan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Runheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language tasks, marking significant strides towards general
artificial intelligence. While general artificial intelligence is leveraged by
developing increasingly large-scale models, there could be another branch to
develop lightweight custom models that better serve certain domains, taking
into account the high cost of training and deploying LLMs and the scarcity of
resources. In this paper, we present MindLLM, a novel series of bilingual
lightweight large language models, trained from scratch, alleviating such
burdens by offering models with 1.3 billion and 3 billion parameters. A
thorough account of experiences accrued during large model development is
given, covering every step of the process, including data construction, model
architecture, evaluation, and applications. Such insights are hopefully
valuable for fellow academics and developers. MindLLM consistently matches or
surpasses the performance of other open-source larger models on some public
benchmarks. We also introduce an innovative instruction tuning framework
tailored for smaller models to enhance their capabilities efficiently.
Moreover, we explore the application of MindLLM in specific vertical domains
such as law and finance, underscoring the agility and adaptability of our
lightweight models.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15778" title="Abstract">arXiv:2310.15778</a> [<a href="/pdf/2310.15778" title="Download PDF">pdf</a>, <a href="/format/2310.15778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Masked Autoencoders for Enhanced Privacy in MRI Scans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+der+Goten%2C+L+A">Lennart Alexander Van der Goten</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+K">Kevin Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">MRI scans provide valuable medical information, however they also contain
sensitive and personally identifiable information (PII) that needs to be
protected. Whereas MRI metadata is easily sanitized, MRI image data is a
privacy risk because it contains information to render highly-realistic 3D
visualizations of a patient's head, enabling malicious actors to possibly
identify the subject by cross-referencing a database. Data anonymization and
de-identification is concerned with ensuring the privacy and confidentiality of
individuals' personal information. Traditional MRI de-identification methods
remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This
comes at the expense of introducing a domain shift that can throw off
downstream analyses. Recently, a GAN-based approach was proposed to de-identify
a patient's scan by remodeling it (e.g. changing the face) rather than by
removing parts. In this work, we propose CP-MAE, a model that de-identifies the
face using masked autoencoders and that outperforms all previous approaches in
terms of downstream task performance as well as de-identification. With our
method we are able to synthesize scans of resolution up to $256^3$ (previously
128 cubic) which constitutes an eight-fold increase in the number of voxels.
Using our construction we were able to design a system that exhibits a highly
robust training stage, making it easy to fit the network on novel data.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15780" title="Abstract">arXiv:2310.15780</a> [<a href="/pdf/2310.15780" title="Download PDF">pdf</a>, <a href="/format/2310.15780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI  Testing via Functionality-aware Decisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengzhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Boyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+X">Xing Che</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dandan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE/ACM International Conference on Software Engineering 2024 (ICSE 2024). arXiv admin note: substantial text overlap with <a href="/abs/2305.09434">arXiv:2305.09434</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Automated Graphical User Interface (GUI) testing plays a crucial role in
ensuring app quality, especially as mobile applications have become an integral
part of our daily lives. Despite the growing popularity of learning-based
techniques in automated GUI testing due to their ability to generate human-like
interactions, they still suffer from several limitations, such as low testing
coverage, inadequate generalization capabilities, and heavy reliance on
training data. Inspired by the success of Large Language Models (LLMs) like
ChatGPT in natural language understanding and question answering, we formulate
the mobile GUI testing problem as a Q&amp;A task. We propose GPTDroid, asking LLM
to chat with the mobile apps by passing the GUI page information to LLM to
elicit testing scripts, and executing them to keep passing the app feedback to
LLM, iterating the whole process. Within this framework, we have also
introduced a functionality-aware memory prompting mechanism that equips the LLM
with the ability to retain testing knowledge of the whole process and conduct
long-term, functionality-based reasoning to guide exploration. We evaluate it
on 93 apps from Google Play and demonstrate that it outperforms the best
baseline by 32% in activity coverage, and detects 31% more bugs at a faster
rate. Moreover, GPTDroid identify 53 new bugs on Google Play, of which 35 have
been confirmed and fixed.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15784" title="Abstract">arXiv:2310.15784</a> [<a href="/pdf/2310.15784" title="Download PDF">pdf</a>, <a href="/format/2310.15784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Impact and Risk Assessment Framework for National Electronic Identity  (eID) Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edu%2C+J">Jide Edu</a>, 
<a href="/search/cs?searchtype=author&query=Hooper%2C+M">Mark Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>, 
<a href="/search/cs?searchtype=author&query=Crowcroft%2C+J">Jon Crowcroft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Electronic identification (eID) systems allow citizens to assert and
authenticate their identities for various purposes, such as accessing
government services or conducting financial transactions. These systems improve
user access to rights, services, and the formal economy. As eID systems become
an essential facet of national development, any failure, compromise, or misuse
can be costly and damaging to the government, users, and society. Therefore, an
effective risk assessment is vital for identifying emerging risks to the system
and assessing their impact. However, developing a comprehensive risk assessment
for these systems must extend far beyond focusing on technical security and
privacy impacts and must be conducted with a contextual understanding of
stakeholders and the communities these systems serve. In this study, we posit
that current risk assessments do not address risk factors for all key
stakeholders and explore how potential compromise could impact them each in
turn. In the examination of the broader impact of risks and the potentially
significant consequences for stakeholders, we propose a framework that
considers a wide range of factors, including the social, economic, and
political contexts in which these systems were implemented. This provides a
holistic platform for a better assessment of risk to the eID system.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15787" title="Abstract">arXiv:2310.15787</a> [<a href="/pdf/2310.15787" title="Download PDF">pdf</a>, <a href="/format/2310.15787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SequenceMatch: Revisiting the design of weak-strong augmentations for  Semi-supervised learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh-Binh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Semi-supervised learning (SSL) has become popular in recent years because it
allows the training of a model using a large amount of unlabeled data. However,
one issue that many SSL methods face is the confirmation bias, which occurs
when the model is overfitted to the small labeled training dataset and produces
overconfident, incorrect predictions. To address this issue, we propose
SequenceMatch, an efficient SSL method that utilizes multiple data
augmentations. The key element of SequenceMatch is the inclusion of a medium
augmentation for unlabeled data. By taking advantage of different augmentations
and the consistency constraints between each pair of augmented examples,
SequenceMatch helps reduce the divergence between the prediction distribution
of the model for weakly and strongly augmented examples. In addition,
SequenceMatch defines two different consistency constraints for high and
low-confidence predictions. As a result, SequenceMatch is more data-efficient
than ReMixMatch, and more time-efficient than both ReMixMatch ($\times4$) and
CoMatch ($\times2$) while having higher accuracy. Despite its simplicity,
SequenceMatch consistently outperforms prior methods on standard benchmarks,
such as CIFAR-10/100, SVHN, and STL-10. It also surpasses prior
state-of-the-art methods by a large margin on large-scale datasets such as
ImageNet, with a 38.46\% error rate. Code is available at
https://github.com/beandkay/SequenceMatch.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15789" title="Abstract">arXiv:2310.15789</a> [<a href="/pdf/2310.15789" title="Download PDF">pdf</a>, <a href="/ps/2310.15789" title="Download PostScript">ps</a>, <a href="/format/2310.15789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verification of Multi-Agent Properties in Electronic Voting: A Case  Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurpiewski%2C+D">Damian Kurpiewski</a>, 
<a href="/search/cs?searchtype=author&query=Jamroga%2C+W">Wojciech Jamroga</a>, 
<a href="/search/cs?searchtype=author&query=Ma%C5%9Bko%2C+%C5%81">&#x141;ukasz Ma&#x15b;ko</a>, 
<a href="/search/cs?searchtype=author&query=Mikulski%2C+%C5%81">&#x141;ukasz Mikulski</a>, 
<a href="/search/cs?searchtype=author&query=Pazderski%2C+W">Witold Pazderski</a>, 
<a href="/search/cs?searchtype=author&query=Penczek%2C+W">Wojciech Penczek</a>, 
<a href="/search/cs?searchtype=author&query=Sidoruk%2C+T">Teofil Sidoruk</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Modal Logic, AiML 2022, Rennes, France, August 22-25,
  2022, 531--556
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Formal verification of multi-agent systems is hard, both theoretically and in
practice. In particular, studies that use a single verification technique
typically show limited efficiency, and allow to verify only toy examples. Here,
we propose some new techniques and combine them with several recently developed
ones to see what progress can be achieved for a real-life scenario. Namely, we
use fixpoint approximation, domination-based strategy search, partial order
reduction, and parallelization to verify heterogeneous scalable models of the
Selene e-voting protocol. The experimental results show that the combination
allows to verify requirements for much more sophisticated models than
previously.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15790" title="Abstract">arXiv:2310.15790</a> [<a href="/pdf/2310.15790" title="Download PDF">pdf</a>, <a href="/format/2310.15790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A statistical significance testing approach for measuring term  burstiness with applications to domain-specific terminology extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hurtado%2C+S+S">Samuel Sarria Hurtado</a>, 
<a href="/search/cs?searchtype=author&query=Mullen%2C+T">Todd Mullen</a>, 
<a href="/search/cs?searchtype=author&query=Onodera%2C+T">Taku Onodera</a>, 
<a href="/search/cs?searchtype=author&query=Sheridan%2C+P">Paul Sheridan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 1 figure, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Domain-specific terminology extraction is an important task in text analysis.
A term in a corpus is said to be "bursty" when its occurrences are concentrated
in few out of many documents. Being content rich, bursty terms are highly
suited for subject matter characterization, and serve as natural candidates for
identifying with technical terminology. Multiple measures of term burstiness
have been proposed in the literature. However, the statistical significance
testing paradigm has remained underexplored in text analysis, including in
relation to term burstiness. To test these waters, we propose as our main
contribution a multinomial language model-based exact test of statistical
significance for term burstiness. Due to its prohibitive computational cost, we
advance a heuristic formula designed to serve as a proxy for test P-values. As
a complementary theoretical contribution, we derive a previously unreported
relationship connecting the inverse document frequency and inverse collection
frequency (two foundational quantities in text analysis) under the multinomial
language model. The relation is used in the evaluation of our heuristic. Using
the GENIA Term corpus benchmark, we compare our approach against established
methods, demonstrating our heuristic's potential in identifying domain-specific
technical terms. We hope this demonstration of statistical significance testing
in text analysis serves as a springboard for future research.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15793" title="Abstract">arXiv:2310.15793</a> [<a href="/pdf/2310.15793" title="Download PDF">pdf</a>, <a href="/format/2310.15793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving generalization in large language models by learning prefix  subspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Falissard%2C+L">Louis Falissard</a>, 
<a href="/search/cs?searchtype=author&query=Guigue%2C+V">Vincent Guigue</a>, 
<a href="/search/cs?searchtype=author&query=Soulier%2C+L">Laure Soulier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This article focuses on large language models (LLMs) fine-tuning in the
scarce data regime (also known as the "few-shot" learning setting). We propose
a method to increase the generalization capabilities of LLMs based on neural
network subspaces. This optimization method, recently introduced in computer
vision, aims to improve model generalization by identifying wider local optima
through the joint optimization of an entire simplex of models in parameter
space. Its adaptation to massive, pretrained transformers, however, poses some
challenges. First, their considerable number of parameters makes it difficult
to train several models jointly, and second, their deterministic parameter
initialization schemes make them unfit for the subspace method as originally
proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT)
methods, however, are perfectly compatible with this original approach, and
propose to learn entire simplex of continuous prefixes. We test our method on a
variant of the GLUE benchmark adapted to the few-shot learning setting, and
show that both our contributions jointly lead to a gain in average performances
compared to sota methods. The implementation can be found at the following
link: https://github.com/Liloulou/prefix_subspace
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15794" title="Abstract">arXiv:2310.15794</a> [<a href="/pdf/2310.15794" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Derivative-based Flexible Integration Algorithm for Power  Electronic Systems Simulation Considering Nonlinear Components
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+B">Bochen Shi</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+Z">Zhujun Yu</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+J">Jialin Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Z">Zhengming Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Simulation is an efficient tool in the design and control of power electronic
systems. However, quick and accurate simulation of them is still challenging,
especially when the system contains a large number of switches and state
variables. Conventional general-purpose integration algorithms assume
nonlinearity within systems but face inefficiency in handling the piecewise
characteristics of power electronic switches. While some specialized algorithms
can adapt to the piecewise characteristics, most of these methods require
systems to be piecewise linear. In this article, a numerical derivative-based
flexible integration algorithm is proposed. This algorithm can adapt to the
piecewise characteristic caused by switches and have no difficulty when
nonlinear non-switching components are present in the circuit. This algorithm
consists of a recursive numerical scheme that obtains high-order time
derivatives of nonlinear components and a decoupling strategy that further
increases computational efficiency. The proposed method is applied to solve a
motor derive system and a large-scale power conversion system (PCS) to verify
its accuracy and efficiency by comparing experimental waveforms and simulated
results given by commercial software. Our proposed method demonstrates
several-fold acceleration compared to multiple commonly used algorithms in
Simulink.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15797" title="Abstract">arXiv:2310.15797</a> [<a href="/pdf/2310.15797" title="Download PDF">pdf</a>, <a href="/format/2310.15797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Entity Quantization for Parameter-Efficient Compositional  Knowledge Graph Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Quan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Licheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhendong Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Representation Learning on Knowledge Graphs (KGs) is essential for downstream
tasks. The dominant approach, KG Embedding (KGE), represents entities with
independent vectors and faces the scalability challenge. Recent studies propose
an alternative way for parameter efficiency, which represents entities by
composing entity-corresponding codewords matched from predefined small-scale
codebooks. We refer to the process of obtaining corresponding codewords of each
entity as entity quantization, for which previous works have designed
complicated strategies. Surprisingly, this paper shows that simple random
entity quantization can achieve similar results to current strategies. We
analyze this phenomenon and reveal that entity codes, the quantization outcomes
for expressing entities, have higher entropy at the code level and Jaccard
distance at the codeword level under random entity quantization. Therefore,
different entities become more easily distinguished, facilitating effective KG
representation. The above results show that current quantization strategies are
not critical for KG representation, and there is still room for improvement in
entity distinguishability beyond current strategies. The code to reproduce our
results is available at https://github.com/JiaangL/RandomQuantization.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15799" title="Abstract">arXiv:2310.15799</a> [<a href="/pdf/2310.15799" title="Download PDF">pdf</a>, <a href="/format/2310.15799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DALE: Generative Data Augmentation for Low-Resource Legal NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sreyan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Evuru%2C+C+K">Chandra Kiran Evuru</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sonal Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Ramaneswaran%2C+S">S Ramaneswaran</a>, 
<a href="/search/cs?searchtype=author&query=Sakshi%2C+S">S Sakshi</a>, 
<a href="/search/cs?searchtype=author&query=Tyagi%2C+U">Utkarsh Tyagi</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference. Code: <a href="https://github.com/Sreyan88/DALE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present DALE, a novel and effective generative Data Augmentation framework
for low-resource LEgal NLP. DALE addresses the challenges existing frameworks
pose in generating effective data augmentations of legal documents - legal
language, with its specialized vocabulary and complex semantics, morphology,
and syntax, does not benefit from data augmentations that merely rephrase the
source sentence. To address this, DALE, built on an Encoder-Decoder Language
Model, is pre-trained on a novel unsupervised text denoising objective based on
selective masking - our masking strategy exploits the domain-specific language
characteristics of templatized legal documents to mask collocated spans of
text. Denoising these spans helps DALE acquire knowledge about legal concepts,
principles, and language usage. Consequently, it develops the ability to
generate coherent and diverse augmentations with novel contexts. Finally, DALE
performs conditional generation to generate synthetic augmentations for
low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13
datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our
baselines, including LLMs, qualitatively and quantitatively, with improvements
of 1%-50%.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15800" title="Abstract">arXiv:2310.15800</a> [<a href="/pdf/2310.15800" title="Download PDF">pdf</a>, <a href="/format/2310.15800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Access for Conjunctive Queries with Negation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Capelli%2C+F">Florent Capelli</a>, 
<a href="/search/cs?searchtype=author&query=Irwin%2C+O">Oliver Irwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Given a conjunctive query $Q$ and a database $\mathbf{D}$, a direct access to
the answers of $Q$ over $\mathbf{D}$ is the operation of returning, given an
index $j$, the $j^{\mathsf{th}}$ answer for some order on its answers. While
this problem is $\#\mathsf{P}$-hard in general with respect to combined
complexity, many conjunctive queries have an underlying structure that allows
for a direct access to their answers for some lexicographical ordering that
takes polylogarithmic time in the size of the database after a polynomial time
precomputation. Previous work has precisely characterised the tractable classes
and given fine-grained lower bounds on the precomputation time needed depending
on the structure of the query.
<br />In this paper, we generalise these tractability results to the case of signed
conjunctive queries, that is, conjunctive queries that may contain negative
atoms. Our technique is based on a class of circuits that can represent
relational data. We first show that this class supports tractable direct access
after a polynomial time preprocessing. We then give bounds on the size of the
circuit needed to represent the answer set of signed conjunctive queries
depending on their structure. Both results combined together allow us to prove
the tractability of direct access for a large class of conjunctive queries. On
the one hand, we recover the known tractable classes from the literature in the
case of positive conjunctive queries. On the other hand, we generalise and
unify known tractability results about negative conjunctive queries -- that is,
queries having only negated atoms. In particular, we show that the class of
$\beta$-acyclic negative conjunctive queries and the class of bounded nest set
width negative conjunctive queries admit tractable direct access.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15801" title="Abstract">arXiv:2310.15801</a> [<a href="/pdf/2310.15801" title="Download PDF">pdf</a>, <a href="/format/2310.15801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A High-Performance and Low-Complexity 5G LDPC Decoder: Algorithm and  Implementation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuqing Ren</a>, 
<a href="/search/cs?searchtype=author&query=Harb%2C+H">Hassan Harb</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yifei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Balatsoukas-Stimming%2C+A">Alexios Balatsoukas-Stimming</a>, 
<a href="/search/cs?searchtype=author&query=Burg%2C+A">Andreas Burg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Hardware Architecture (cs.AR); Signal Processing (eess.SP)

</div>
<p class="mathjax">5G New Radio (NR) has stringent demands on both performance and complexity
for the design of low-density parity-check (LDPC) decoding algorithms and
corresponding VLSI implementations. Furthermore, decoders must fully support
the wide range of all 5G NR blocklengths and code rates, which is a significant
challenge. In this paper, we present a high-performance and low-complexity LDPC
decoder, tailor-made to fulfill the 5G requirements. First, to close the gap
between belief propagation (BP) decoding and its approximations in hardware, we
propose an extension of adjusted min-sum decoding, called generalized adjusted
min-sum (GA-MS) decoding. This decoding algorithm flexibly truncates the
incoming messages at the check node level and carefully approximates the
non-linear functions of BP decoding to balance the error-rate and hardware
complexity. Numerical results demonstrate that the proposed fixed-point GAMS
has only a minor gap of 0.1 dB compared to floating-point BP under various
scenarios of 5G standard specifications. Secondly, we present a fully
reconfigurable 5G NR LDPC decoder implementation based on GA-MS decoding. Given
that memory occupies a substantial portion of the decoder area, we adopt
multiple data compression and approximation techniques to reduce 42.2% of the
memory overhead. The corresponding 28nm FD-SOI ASIC decoder has a core area of
1.823 mm2 and operates at 895 MHz. It is compatible with all 5G NR LDPC codes
and achieves a peak throughput of 24.42 Gbps and a maximum area efficiency of
13.40 Gbps/mm2 at 4 decoding iterations.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15805" title="Abstract">arXiv:2310.15805</a> [<a href="/pdf/2310.15805" title="Download PDF">pdf</a>, <a href="/format/2310.15805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equilibrium: Optimization of Ceph Cluster Storage by Size-Aware Shard  Balancing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jelten%2C+J">Jonas Jelten</a>, 
<a href="/search/cs?searchtype=author&query=Wollek%2C+A">Alessandro Wollek</a>, 
<a href="/search/cs?searchtype=author&query=Frank%2C+D">David Frank</a>, 
<a href="/search/cs?searchtype=author&query=Lasser%2C+T">Tobias Lasser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> source code: <a href="https://github.com/TheJJ/ceph-balancer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Worldwide, storage demands and costs are increasing. As a consequence of
fault tolerance, storage device heterogenity, and data center specific
constraints, optimal storage capacity utilization cannot be achieved with the
integrated balancing algorithm of the distributed storage cluster system Ceph.
This work presents Equilibrium, a device utilization size-aware shard balancing
algorithm. With extensive experiments we demonstrate that our proposed
algorithm balances near optimally on real-world clusters with strong available
storage capacity improvements while reducing the amount of needed data
movement.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15808" title="Abstract">arXiv:2310.15808</a> [<a href="/pdf/2310.15808" title="Download PDF">pdf</a>, <a href="/format/2310.15808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting the Performance of Satellite Network Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raman%2C+A">Aravindh Raman</a>, 
<a href="/search/cs?searchtype=author&query=Varvello%2C+M">Matteo Varvello</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Hyunseok Chang</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+N">Nishanth Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+Y">Yasir Zaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at International Conference on emerging Networking EXperiments and Technologies (CoNEXT 2023). Please cite the CoNEXT version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The rapid growth of satellite network operators (SNOs) has revolutionized
broadband communications, enabling global connectivity and bridging the digital
divide. As these networks expand, it is important to evaluate their performance
and efficiency. This paper presents the first comprehensive study of SNOs. We
take an opportunistic approach and devise a methodology which allows to
identify public network measurements performed via SNOs. We apply this
methodology to both M-Lab and RIPE public datasets which allowed us to
characterize low level performance and footprint of up to 18 SNOs operating in
different orbits. Finally, we identify and recruit paid testers on three
popular SNOs (Starlink, HughesNet, and ViaSat) to evaluate the performance of
popular applications like web browsing and video streaming.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15813" title="Abstract">arXiv:2310.15813</a> [<a href="/pdf/2310.15813" title="Download PDF">pdf</a>, <a href="/ps/2310.15813" title="Download PostScript">ps</a>, <a href="/format/2310.15813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Risks and Challenges of National Electronic Identity  (NeID) System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edu%2C+J">Jide Edu</a>, 
<a href="/search/cs?searchtype=author&query=Hooper%2C+M">Mark Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>, 
<a href="/search/cs?searchtype=author&query=Crowcroft%2C+J">Jon Crowcroft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Many countries have embraced national electronic identification (NeID)
systems, recognising their potential to foster a fair, transparent, and
well-governed society by ensuring the secure verification of citizens'
identities. The inclusive nature of NeID empowers people to exercise their
rights while holding them accountable for fulfilling their obligations.
Nevertheless, the development and implementation of these complex
identity-verification systems have raised concerns regarding security, privacy,
and exclusion. In this study, we discuss the different categories of NeID risk
and explore the successful deployment of these systems, while examining how the
specific risks and other challenges posed by this technology are addressed.
Based on the review of the different NeID systems and the efforts made to
mitigate the unique risks and challenges presented within each deployment, we
highlighted the best practices for mitigating risk, including implementing
strong security measures, conducting regular risk assessments, and involving
stakeholders in the design and implementation of the system.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15815" title="Abstract">arXiv:2310.15815</a> [<a href="/pdf/2310.15815" title="Download PDF">pdf</a>, <a href="/format/2310.15815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Good Better Best: Self-Motivated Imitation Learning for noisy  Demonstrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+Y">Yong Heng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Leiji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">MingZhong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Imitation Learning (IL) aims to discover a policy by minimizing the
discrepancy between the agent's behavior and expert demonstrations. However, IL
is susceptible to limitations imposed by noisy demonstrations from non-expert
behaviors, presenting a significant challenge due to the lack of supplementary
information to assess their expertise. In this paper, we introduce
Self-Motivated Imitation LEarning (SMILE), a method capable of progressively
filtering out demonstrations collected by policies deemed inferior to the
current policy, eliminating the need for additional information. We utilize the
forward and reverse processes of Diffusion Models to emulate the shift in
demonstration expertise from low to high and vice versa, thereby extracting the
noise information that diffuses expertise. Then, the noise information is
leveraged to predict the diffusion steps between the current policy and
demonstrators, which we theoretically demonstrate its equivalence to their
expertise gap. We further explain in detail how the predicted diffusion steps
are applied to filter out noisy demonstrations in a self-motivated manner and
provide its theoretical grounds. Through empirical evaluations on MuJoCo tasks,
we demonstrate that our method is proficient in learning the expert policy
amidst noisy demonstrations, and effectively filters out demonstrations with
expertise inferior to the current policy.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15817" title="Abstract">arXiv:2310.15817</a> [<a href="/pdf/2310.15817" title="Download PDF">pdf</a>, <a href="/ps/2310.15817" title="Download PostScript">ps</a>, <a href="/format/2310.15817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discriminator Guidance for Autoregressive Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kelvinius%2C+F+E">Filip Ekstr&#xf6;m Kelvinius</a>, 
<a href="/search/cs?searchtype=author&query=Lindsten%2C+F">Fredrik Lindsten</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce discriminator guidance in the setting of Autoregressive
Diffusion Models. The use of a discriminator to guide a diffusion process has
previously been used for continuous diffusion models, and in this work we
derive ways of using a discriminator together with a pretrained generative
model in the discrete case. First, we show that using an optimal discriminator
will correct the pretrained model and enable exact sampling from the underlying
data distribution. Second, to account for the realistic scenario of using a
sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which
iteratively takes the predictions from the discrimiator into account during the
generation process. We test these approaches on the task of generating
molecular graphs and show how the discriminator improves the generative
performance over using only the pretrained model.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15819" title="Abstract">arXiv:2310.15819</a> [<a href="/pdf/2310.15819" title="Download PDF">pdf</a>, <a href="/format/2310.15819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Language Models Exhibit Social Identity Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tiancheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kyrychenko%2C+Y">Yara Kyrychenko</a>, 
<a href="/search/cs?searchtype=author&query=Rathje%2C+S">Steve Rathje</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Linden%2C+S">Sander van der Linden</a>, 
<a href="/search/cs?searchtype=author&query=Roozenbeek%2C+J">Jon Roozenbeek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> supplementary material, data, and code see <a href="https://osf.io/9ht32/?view_only=f0ab4b23325f4c31ad3e12a7353b55f5">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The surge in popularity of large language models has given rise to concerns
about biases that these models could learn from humans. In this study, we
investigate whether ingroup solidarity and outgroup hostility, fundamental
social biases known from social science, are present in 51 large language
models. We find that almost all foundational language models and some
instruction fine-tuned models exhibit clear ingroup-positive and
outgroup-negative biases when prompted to complete sentences (e.g., "We
are..."). A comparison of LLM-generated sentences with human-written sentences
on the internet reveals that these models exhibit similar level, if not
greater, levels of bias than human text. To investigate where these biases stem
from, we experimentally varied the amount of ingroup-positive or
outgroup-negative sentences the model was exposed to during fine-tuning in the
context of the United States Democrat-Republican divide. Doing so resulted in
the models exhibiting a marked increase in ingroup solidarity and an even
greater increase in outgroup hostility. Furthermore, removing either
ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning
data leads to a significant reduction in both ingroup solidarity and outgroup
hostility, suggesting that biases can be reduced by removing biased training
data. Our findings suggest that modern language models exhibit fundamental
social identity biases and that such biases can be mitigated by curating
training data. Our results have practical implications for creating less biased
large-language models and further underscore the need for more research into
user interactions with LLMs to prevent potential bias reinforcement in humans.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15823" title="Abstract">arXiv:2310.15823</a> [<a href="/pdf/2310.15823" title="Download PDF">pdf</a>, <a href="/format/2310.15823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To  Word--Definition Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=ElBakry%2C+A">Ahmed ElBakry</a>, 
<a href="/search/cs?searchtype=author&query=Gabr%2C+M">Mohamed Gabr</a>, 
<a href="/search/cs?searchtype=author&query=ElNokrashy%2C+M">Muhammad ElNokrashy</a>, 
<a href="/search/cs?searchtype=author&query=AlKhamissi%2C+B">Badr AlKhamissi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ArabicNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A Reverse Dictionary is a tool enabling users to discover a word based on its
provided definition, meaning, or description. Such a technique proves valuable
in various scenarios, aiding language learners who possess a description of a
word without its identity, and benefiting writers seeking precise terminology.
These scenarios often encapsulate what is referred to as the
"Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning
solution for the Arabic Reverse Dictionary shared task. This task focuses on
deriving a vector representation of an Arabic word from its accompanying
description. The shared task encompasses two distinct subtasks: the first
involves an Arabic definition as input, while the second employs an English
definition. For the first subtask, our approach relies on an ensemble of
finetuned Arabic BERT-based models, predicting the word embedding for a given
definition. The final representation is obtained through averaging the output
embeddings from each model within the ensemble. In contrast, the most effective
solution for the second subtask involves translating the English test
definitions into Arabic and applying them to the finetuned models originally
trained for the first subtask. This straightforward method achieves the highest
score across both subtasks.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15825" title="Abstract">arXiv:2310.15825</a> [<a href="/pdf/2310.15825" title="Download PDF">pdf</a>, <a href="/format/2310.15825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spline-Based Collocation Method for Stokes and Navier-Stokes equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+J">Jinsil Lee</a>, 
<a href="/search/math?searchtype=author&query=Yoon%2C+J">Jungho Yoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In the paper, we propose a collocation method based on multivariate
polynomial splines over triangulation or tetrahedralization for solving Stokes
and Navier-Stokes equations. We start with a detailed explanation of the method
for the Stokes equation and then extend the study to the Navier-Stokes
equations. We shall show that the numerical solution can approximate the exact
PDE solution very well over several domains. Then we present several numerical
experimental results to demonstrate the performance of the method over the 2D
and 3D settings. Also, we apply the IPBM method to our method to find the
solution over several curved domains effectively. In addition, we present a
comparison with the existing multivariate spline methods in \cite{AL02} and
several existing methods to show that the new method produces a similar and
sometimes more accurate approximation in a more efficient fashion.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15826" title="Abstract">arXiv:2310.15826</a> [<a href="/pdf/2310.15826" title="Download PDF">pdf</a>, <a href="/format/2310.15826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One or Two Things We know about Concept Drift -- A Survey on Monitoring  Evolving Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hinder%2C+F">Fabian Hinder</a>, 
<a href="/search/cs?searchtype=author&query=Vaquet%2C+V">Valerie Vaquet</a>, 
<a href="/search/cs?searchtype=author&query=Hammer%2C+B">Barbara Hammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The world surrounding us is subject to constant change. These changes,
frequently described as concept drift, influence many industrial and technical
processes. As they can lead to malfunctions and other anomalous behavior, which
may be safety-critical in many scenarios, detecting and analyzing concept drift
is crucial. In this paper, we provide a literature review focusing on concept
drift in unsupervised data streams. While many surveys focus on supervised data
streams, so far, there is no work reviewing the unsupervised setting. However,
this setting is of particular relevance for monitoring and anomaly detection
which are directly applicable to many tasks and challenges in engineering. This
survey provides a taxonomy of existing work on drift detection. Besides, it
covers the current state of research on drift localization in a systematic way.
In addition to providing a systematic literature review, this work provides
precise mathematical definitions of the considered problems and contains
standardized experiments on parametric artificial datasets allowing for a
direct comparison of different strategies for detection and localization.
Thereby, the suitability of different schemes can be analyzed systematically
and guidelines for their usage in real-world scenarios can be provided.
Finally, there is a section on the emerging topic of explaining concept drift.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15827" title="Abstract">arXiv:2310.15827</a> [<a href="/pdf/2310.15827" title="Download PDF">pdf</a>, <a href="/format/2310.15827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D  ResUNet: Contribution to the SEG.A Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wodzinski%2C+M">Marek Wodzinski</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+H">Henning M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI 2023 - SEG.A Challenge Contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic aorta segmentation from 3-D medical volumes is an important yet
difficult task. Several factors make the problem challenging, e.g. the
possibility of aortic dissection or the difficulty with segmenting and
annotating the small branches. This work presents a contribution by the MedGIFT
team to the SEG.A challenge organized during the MICCAI 2023 conference. We
propose a fully automated algorithm based on deep encoder-decoder architecture.
The main assumption behind our work is that data preprocessing and augmentation
are much more important than the deep architecture, especially in low data
regimes. Therefore, the solution is based on a variant of traditional
convolutional U-Net. The proposed solution achieved a Dice score above 0.9 for
all testing cases with the highest stability among all participants. The method
scored 1st, 4th, and 3rd in terms of the clinical evaluation, quantitative
results, and volumetric meshing quality, respectively. We freely release the
source code, pretrained model, and provide access to the algorithm on the
Grand-Challenge platform.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15828" title="Abstract">arXiv:2310.15828</a> [<a href="/pdf/2310.15828" title="Download PDF">pdf</a>, <a href="/ps/2310.15828" title="Download PostScript">ps</a>, <a href="/format/2310.15828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Negative Imaginary Control Using Hybrid Integrator-Gain Systems:  Application to MEMS Nanopositioner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+K">Kanghong Shi</a>, 
<a href="/search/eess?searchtype=author&query=Nikooienejad%2C+N">Nastaran Nikooienejad</a>, 
<a href="/search/eess?searchtype=author&query=Petersen%2C+I+R">Ian R. Petersen</a>, 
<a href="/search/eess?searchtype=author&query=Moheimani%2C+S+O+R">S. O. Reza Moheimani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures. Accepted for publication as a Full Paper in the IEEE Transactions on Control Systems Technology (TCST)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper, we propose a new approach to address the control problem for
negative imaginary (NI) systems by using hybrid integrator-gain systems (HIGS).
We investigate the single HIGS of its original form and its two variations,
including a multi-HIGS and the serial cascade of two HIGS. A single HIGS is
shown to be a nonlinear negative imaginary system, and so is the multi-HIGS and
the cascade of two HIGS. We show that these three types of HIGS can be used as
controllers to asymptotically stabilize linear NI systems. The results of this
paper are then illustrated in a real-world experiment where a 2-DOF
microelectromechanical system nanopositioner is stabilized by a multi-HIGS.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15829" title="Abstract">arXiv:2310.15829</a> [<a href="/pdf/2310.15829" title="Download PDF">pdf</a>, <a href="/format/2310.15829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unnatural language processing: How do language models handle  machine-generated prompts?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kervadec%2C+C">Corentin Kervadec</a>, 
<a href="/search/cs?searchtype=author&query=Franzon%2C+F">Francesca Franzon</a>, 
<a href="/search/cs?searchtype=author&query=Baroni%2C+M">Marco Baroni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023 Camera-Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language model prompt optimization research has shown that semantically and
grammatically well-formed manually crafted prompts are routinely outperformed
by automatically generated token sequences with no apparent meaning or
syntactic structure, including sequences of vectors from a model's embedding
space. We use machine-generated prompts to probe how models respond to input
that is not composed of natural language expressions. We study the behavior of
models of different sizes in multiple semantic tasks in response to both
continuous and discrete machine-generated prompts, and compare it to the
behavior in response to human-generated natural-language prompts. Even when
producing a similar output, machine-generated and human prompts trigger
different response patterns through the network processing pathways, including
different perplexities, different attention and output entropy distributions,
and different unit activation profiles. We provide preliminary insight into the
nature of the units activated by different prompt types, suggesting that only
natural language prompts recruit a genuinely linguistic circuit.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15830" title="Abstract">arXiv:2310.15830</a> [<a href="/pdf/2310.15830" title="Download PDF">pdf</a>, <a href="/format/2310.15830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Localization of Small Leakages in Water Distribution Networks using  Concept Drift Explanation Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaquet%2C+V">Valerie Vaquet</a>, 
<a href="/search/cs?searchtype=author&query=Hinder%2C+F">Fabian Hinder</a>, 
<a href="/search/cs?searchtype=author&query=Lammers%2C+K">Kathrin Lammers</a>, 
<a href="/search/cs?searchtype=author&query=Vaquet%2C+J">Jonas Vaquet</a>, 
<a href="/search/cs?searchtype=author&query=Hammer%2C+B">Barbara Hammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Facing climate change the already limited availability of drinking water will
decrease in the future rendering drinking water an increasingly scarce
resource. Considerable amounts of it are lost through leakages in water
transportation and distribution networks. Leakage detection and localization
are challenging problems due to the complex interactions and changing demands
in water distribution networks. Especially small leakages are hard to pinpoint
yet their localization is vital to avoid water loss over long periods of time.
While there exist different approaches to solving the tasks of leakage
detection and localization, they are relying on various information about the
system, e.g. real-time demand measurements and the precise network topology,
which is an unrealistic assumption in many real-world scenarios. In contrast,
this work attempts leakage localization using pressure measurements only. For
this purpose, first, leakages in the water distribution network are modeled
employing Bayesian networks, and the system dynamics are analyzed. We then show
how the problem is connected to and can be considered through the lens of
concept drift. In particular, we argue that model-based explanations of concept
drift are a promising tool for localizing leakages given limited information
about the network. The methodology is experimentally evaluated using realistic
benchmark scenarios.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15836" title="Abstract">arXiv:2310.15836</a> [<a href="/pdf/2310.15836" title="Download PDF">pdf</a>, <a href="/format/2310.15836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diffusion Weighted Graph Framework for New Intent Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenkai Shi</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+W">Wenbin An</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+F">Feng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinghua Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">QianYing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Ping Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">New Intent Discovery (NID) aims to recognize both new and known intents from
unlabeled data with the aid of limited labeled data containing only known
intents. Without considering structure relationships between samples, previous
methods generate noisy supervisory signals which cannot strike a balance
between quantity and quality, hindering the formation of new intent clusters
and effective transfer of the pre-training knowledge. To mitigate this
limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to
capture both semantic similarities and structure relationships inherent in
data, enabling more sufficient and reliable supervisory signals. Specifically,
for each sample, we diffuse neighborhood relationships along semantic paths
guided by the nearest neighbors for multiple hops to characterize its local
structure discriminately. Then, we sample its positive keys and weigh them
based on semantic similarities and local structures for contrastive learning.
During inference, we further propose Graph Smoothing Filter (GSF) to explicitly
utilize the structure relationships to filter high-frequency noise embodied in
semantically ambiguous samples on the cluster boundary. Extensive experiments
show that our method outperforms state-of-the-art models on all evaluation
metrics across multiple benchmark datasets. Code and data are available at
https://github.com/yibai-shi/DWGF.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15846" title="Abstract">arXiv:2310.15846</a> [<a href="/pdf/2310.15846" title="Download PDF">pdf</a>, <a href="/format/2310.15846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Spatial-Temporal Triangulation for Bearing-Only Cooperative  Motion Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C+L">C. L. Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+Y+Z">Y. Z. Mi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H+Q">H. Q. Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H+B">H. B. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">F. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J+H">J. H. Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z+Y">Z.Y. Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S+Y">S. Y. Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Vision-based cooperative motion estimation is an important problem for many
multi-robot systems such as cooperative aerial target pursuit. This problem can
be formulated as bearing-only cooperative motion estimation, where the visual
measurement is modeled as a bearing vector pointing from the camera to the
target. The conventional approaches for bearing-only cooperative estimation are
mainly based on the framework distributed Kalman filtering (DKF). In this
paper, we propose a new optimal bearing-only cooperative estimation algorithm,
named spatial-temporal triangulation, based on the method of distributed
recursive least squares, which provides a more flexible framework for designing
distributed estimators than DKF. The design of the algorithm fully incorporates
all the available information and the specific triangulation geometric
constraint. As a result, the algorithm has superior estimation performance than
the state-of-the-art DKF algorithms in terms of both accuracy and convergence
speed as verified by numerical simulation. We rigorously prove the exponential
convergence of the proposed algorithm. Moreover, to verify the effectiveness of
the proposed algorithm under practical challenging conditions, we develop a
vision-based cooperative aerial target pursuit system, which is the first of
such fully autonomous systems so far to the best of our knowledge.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15847" title="Abstract">arXiv:2310.15847</a> [<a href="/pdf/2310.15847" title="Download PDF">pdf</a>, <a href="/format/2310.15847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Method for Analysing Racial Bias: Collection of Person Level  References
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kocyigit%2C+M+Y">Muhammed Yusuf Kocyigit</a>, 
<a href="/search/cs?searchtype=author&query=Andy%2C+A">Anietie Andy</a>, 
<a href="/search/cs?searchtype=author&query=Wijaya%2C+D">Derry Wijaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main paper is 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Long term exposure to biased content in literature or media can significantly
influence people's perceptions of reality, leading to the development of
implicit biases that are difficult to detect and address (Gerbner 1998). In
this study, we propose a novel method to analyze the differences in
representation between two groups and use it examine the representation of
African Americans and White Americans in books between 1850 to 2000 with the
Google Books dataset (Goldberg and Orwant 2013). By developing better tools to
understand differences in representation, we aim to contribute to the ongoing
efforts to recognize and mitigate biases. To improve upon the more common
phrase based (men, women, white, black, etc) methods to differentiate context
(Tripodi et al. 2019, Lucy; Tadimeti, and Bamman 2022), we propose collecting a
comprehensive list of historically significant figures and using their names to
select relevant context. This novel approach offers a more accurate and nuanced
method for detecting implicit biases through reducing the risk of selection
bias. We create group representations for each decade and analyze them in an
aligned semantic space (Hamilton, Leskovec, and Jurafsky 2016). We further
support our results by assessing the time adjusted toxicity (Bassignana,
Basile, and Patti 2018) in the context for each group and identifying the
semantic axes (Lucy, Tadimeti, and Bamman 2022) that exhibit the most
significant differences between the groups across decades. We support our
method by showing that our proposed method can capture known socio political
changes accurately and our findings indicate that while the relative number of
African American names mentioned in books have increased over time, the context
surrounding them remains more toxic than white Americans.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15848" title="Abstract">arXiv:2310.15848</a> [<a href="/pdf/2310.15848" title="Download PDF">pdf</a>, <a href="/format/2310.15848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Responsible Machine Learning Datasets with Fairness, Privacy, and  Regulatory Norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Surbhi Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Thakral%2C+K">Kartik Thakral</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Richa Singh</a>, 
<a href="/search/cs?searchtype=author&query=Vatsa%2C+M">Mayank Vatsa</a>, 
<a href="/search/cs?searchtype=author&query=Glaser%2C+T">Tamar Glaser</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+C+C">Cristian Canton Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Hassner%2C+T">Tal Hassner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Artificial Intelligence (AI) has made its way into various scientific fields,
providing astonishing improvements over existing algorithms for a wide variety
of tasks. In recent years, there have been severe concerns over the
trustworthiness of AI technologies. The scientific community has focused on the
development of trustworthy AI algorithms. However, machine and deep learning
algorithms, popular in the AI community today, depend heavily on the data used
during their development. These learning algorithms identify patterns in the
data, learning the behavioral objective. Any flaws in the data have the
potential to translate directly into algorithms. In this study, we discuss the
importance of Responsible Machine Learning Datasets and propose a framework to
evaluate the datasets through a responsible rubric. While existing work focuses
on the post-hoc evaluation of algorithms for their trustworthiness, we provide
a framework that considers the data component separately to understand its role
in the algorithm. We discuss responsible datasets through the lens of fairness,
privacy, and regulatory compliance and provide recommendations for constructing
future datasets. After surveying over 100 datasets, we use 60 datasets for
analysis and demonstrate that none of these datasets is immune to issues of
fairness, privacy preservation, and regulatory compliance. We provide
modifications to the ``datasheets for datasets" with important additions for
improved dataset documentation. With governments around the world regularizing
data protection laws, the method for the creation of datasets in the scientific
community requires revision. We believe this study is timely and relevant in
today's era of AI.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15849" title="Abstract">arXiv:2310.15849</a> [<a href="/pdf/2310.15849" title="Download PDF">pdf</a>, <a href="/format/2310.15849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Resilient Framework for 5G-Edge-Connected UAVs based on Switching  Edge-MPC and Onboard-PID Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Damigos%2C+G">Gerasimos Damigos</a>, 
<a href="/search/cs?searchtype=author&query=Seisa%2C+A+S">Achilleas Santi Seisa</a>, 
<a href="/search/cs?searchtype=author&query=Satpute%2C+S+G">Sumeet Gajanan Satpute</a>, 
<a href="/search/cs?searchtype=author&query=Lindgren%2C+T">Tore Lindgren</a>, 
<a href="/search/cs?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, isie2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE 32nd International Symposium on Industrial Electronics
  (ISIE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In recent years, the need for resources for handling processes with high
computational complexity for mobile robots is becoming increasingly urgent.
More specifically, robots need to autonomously operate in a robust and
continuous manner, while keeping high performance, a need that led to the
utilization of edge computing to offload many computationally demanding and
time-critical robotic procedures. However, safe mechanisms should be
implemented to handle situations when it is not possible to use the offloaded
procedures, such as if the communication is challenged or the edge cluster is
not available. To this end, this article presents a switching strategy for
safety, redundancy, and optimized behavior through an edge computing-based
Model Predictive Controller (MPC) and a low-level onboard-PID controller for
edge-connected Unmanned Aerial Vehicles (UAVs). The switching strategy is based
on the communication Key Performance Indicators (KPIs) over 5G to decide
whether the UAV should be controlled by the edge-based or have a safe fallback
based on the onboard controller.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15851" title="Abstract">arXiv:2310.15851</a> [<a href="/pdf/2310.15851" title="Download PDF">pdf</a>, <a href="/format/2310.15851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Guard: Empower the LLM to Safeguard Itself
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zezhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fangkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Pu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The jailbreak attack can bypass the safety measures of a Large Language Model
(LLM), generating harmful content. This misuse of LLM has led to negative
societal consequences. Currently, there are two main approaches to address
jailbreak attacks: safety training and safeguards. Safety training focuses on
further training LLM to enhance its safety. On the other hand, safeguards
involve implementing external models or filters to prevent harmful outputs.
However, safety training has constraints in its ability to adapt to new attack
types and often leads to a drop in model performance. Safeguards have proven to
be of limited help. To tackle these issues, we propose a novel approach called
Self-Guard, which combines the strengths of both safety methods. Self-Guard
includes two stages. In the first stage, we enhance the model's ability to
assess harmful content, and in the second stage, we instruct the model to
consistently perform harmful content detection on its own responses. The
experiment has demonstrated that Self-Guard is robust against jailbreak
attacks. In the bad case analysis, we find that LLM occasionally provides
harmless responses to harmful queries. Additionally, we evaluated the general
capabilities of the LLM before and after safety training, providing evidence
that Self-Guard does not result in the LLM's performance degradation. In
sensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM
but also can even mitigate this issue.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15852" title="Abstract">arXiv:2310.15852</a> [<a href="/pdf/2310.15852" title="Download PDF">pdf</a>, <a href="/format/2310.15852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Artificial French Data to Understand the Emergence of Gender Bias  in Transformer Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conti%2C+L">Lina Conti</a>, 
<a href="/search/cs?searchtype=author&query=Wisniewski%2C+G">Guillaume Wisniewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP'23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Numerous studies have demonstrated the ability of neural language models to
learn various linguistic properties without direct supervision. This work takes
an initial step towards exploring the less researched topic of how neural
models discover linguistic properties of words, such as gender, as well as the
rules governing their usage. We propose to use an artificial corpus generated
by a PCFG based on French to precisely control the gender distribution in the
training data and determine under which conditions a model correctly captures
gender information or, on the contrary, appears gender-biased.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15858" title="Abstract">arXiv:2310.15858</a> [<a href="/pdf/2310.15858" title="Download PDF">pdf</a>, <a href="/ps/2310.15858" title="Download PostScript">ps</a>, <a href="/format/2310.15858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology-aware Debiased Self-supervised Graph Learning for  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lei Han</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hui Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Z">Zhicheng Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages,8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recommendation, graph-based Collaborative Filtering (CF) methods mitigate
the data sparsity by introducing Graph Contrastive Learning (GCL). However, the
random negative sampling strategy in these GCL-based CF models neglects the
semantic structure of users (items), which not only introduces false negatives
(negatives that are similar to anchor user (item)) but also ignores the
potential positive samples. To tackle the above issues, we propose
Topology-aware Debiased Self-supervised Graph Learning (TDSGL) for
recommendation, which constructs contrastive pairs according to the semantic
similarity between users (items). Specifically, since the original user-item
interaction data commendably reflects the purchasing intent of users and
certain characteristics of items, we calculate the semantic similarity between
users (items) on interaction data. Then, given a user (item), we construct its
negative pairs by selecting users (items) which embed different semantic
structures to ensure the semantic difference between the given user (item) and
its negatives. Moreover, for a user (item), we design a feature extraction
module that converts other semantically similar users (items) into an auxiliary
positive sample to acquire a more informative representation. Experimental
results show that the proposed model outperforms the state-of-the-art models
significantly on three public datasets. Our model implementation codes are
available at https://github.com/malajikuai/TDSGL.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15862" title="Abstract">arXiv:2310.15862</a> [<a href="/pdf/2310.15862" title="Download PDF">pdf</a>, <a href="/format/2310.15862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is a humorous robot more trustworthy?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sienkiewicz%2C+B">Barbara Sienkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Indurkhya%2C+B">Bipin Indurkhya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICSR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">As more and more social robots are being used for collaborative activities
with humans, it is crucial to investigate mechanisms to facilitate trust in the
human-robot interaction. One such mechanism is humour: it has been shown to
increase creativity and productivity in human-human interaction, which has an
indirect influence on trust. In this study, we investigate if humour can
increase trust in human-robot interaction. We conducted a between-subjects
experiment with 40 participants to see if the participants are more likely to
accept the robot's suggestion in the Three-card Monte game, as a trust check
task. Though we were unable to find a significant effect of humour, we discuss
the effect of possible confounding variables, and also report some interesting
qualitative observations from our study: for instance, the participants
interacted effectively with the robot as a team member, regardless of the
humour or no-humour condition.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15863" title="Abstract">arXiv:2310.15863</a> [<a href="/pdf/2310.15863" title="Download PDF">pdf</a>, <a href="/format/2310.15863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metric Clustering and MST with Strong and Weak Distance Oracles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bateni%2C+M">MohammadHossein Bateni</a>, 
<a href="/search/cs?searchtype=author&query=Dharangutte%2C+P">Prathamesh Dharangutte</a>, 
<a href="/search/cs?searchtype=author&query=Jayaram%2C+R">Rajesh Jayaram</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study optimization problems in a metric space $(\mathcal{X},d)$ where we
can compute distances in two ways: via a ''strong'' oracle that returns exact
distances $d(x,y)$, and a ''weak'' oracle that returns distances
$\tilde{d}(x,y)$ which may be arbitrarily corrupted with some probability. This
model captures the increasingly common trade-off between employing both an
expensive similarity model (e.g. a large-scale embedding model), and a less
accurate but cheaper model. Hence, the goal is to make as few queries to the
strong oracle as possible. We consider both so-called ''point queries'', where
the strong oracle is queried on a set of points $S \subset \mathcal{X} $ and
returns $d(x,y)$ for all $x,y \in S$, and ''edge queries'' where it is queried
for individual distances $d(x,y)$.
<br />Our main contributions are optimal algorithms and lower bounds for clustering
and Minimum Spanning Tree (MST) in this model. For $k$-centers, $k$-median, and
$k$-means, we give constant factor approximation algorithms with only
$\tilde{O}(k)$ strong oracle point queries, and prove that $\Omega(k)$ queries
are required for any bounded approximation. For edge queries, our upper and
lower bounds are both $\tilde{\Theta}(k^2)$. Surprisingly, for the MST problem
we give a $O(\sqrt{\log n})$ approximation algorithm using no strong oracle
queries at all, and a matching $\Omega(\sqrt{\log n})$ lower bound. We
empirically evaluate our algorithms, and show that their quality is comparable
to that of the baseline algorithms that are given all true distances, but while
querying the strong oracle on only a small fraction ($&lt;1\%$) of points.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15865" title="Abstract">arXiv:2310.15865</a> [<a href="/pdf/2310.15865" title="Download PDF">pdf</a>, <a href="/format/2310.15865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Causality-Aware Graph Neural Networks to Predict Temporal  Centralities in Dynamic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heeg%2C+F">Franziska Heeg</a>, 
<a href="/search/cs?searchtype=author&query=Scholtes%2C+I">Ingo Scholtes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Node centralities play a pivotal role in network science, social network
analysis, and recommender systems. In temporal data, static path-based
centralities like closeness or betweenness can give misleading results about
the true importance of nodes in a temporal graph. To address this issue,
temporal generalizations of betweenness and closeness have been defined that
are based on the shortest time-respecting paths between pairs of nodes.
However, a major issue of those generalizations is that the calculation of such
paths is computationally expensive. Addressing this issue, we study the
application of De Bruijn Graph Neural Networks (DBGNN), a causality-aware graph
neural network architecture, to predict temporal path-based centralities in
time series data. We experimentally evaluate our approach in 13 temporal graphs
from biological and social systems and show that it considerably improves the
prediction of both betweenness and closeness centrality compared to a static
Graph Convolutional Neural Network.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15870" title="Abstract">arXiv:2310.15870</a> [<a href="/pdf/2310.15870" title="Download PDF">pdf</a>, <a href="/format/2310.15870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybridized Formulations of Flux Reconstruction Schemes for  Advection-Diffusion Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pereira%2C+C+A">Carlos A. Pereira</a>, 
<a href="/search/math?searchtype=author&query=Vermeire%2C+B+C">Brian C. Vermeire</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present the hybridization of flux reconstruction methods for
advection-diffusion problems. Hybridization introduces a new variable into the
problem so that it can be reduced via static condensation. This allows the
solution of implicit discretizations to be done more efficiently. We derive an
energy statement from a stability analysis considering a range of correction
functions on hybridized and embedded flux reconstruction schemes. Then, we
establish connections to standard formulations. We devise a post-processing
scheme that leverages existing flux reconstruction operators to enhance
accuracy for diffusion-dominated problems. Results show that the implicit
convergence of these methods for advection-diffusion problems can result in
performance benefits of over an order of magnitude. In addition, we observe
that the superconvergence property of hybridized methods can be extended to the
family of FR schemes for a range of correction functions.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15872" title="Abstract">arXiv:2310.15872</a> [<a href="/pdf/2310.15872" title="Download PDF">pdf</a>, <a href="/format/2310.15872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhengqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fan-Keng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Boning%2C+D+S">Duane S. Boning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
<p class="mathjax">In this paper, we exploit a fundamental principle of analog electronic
circuitry, Kirchhoff's current law, to introduce a unique class of neural
network models that we refer to as KirchhoffNet. KirchhoffNet establishes close
connections with message passing neural networks and continuous-depth networks.
We demonstrate that even in the absence of any traditional layers (such as
convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test
accuracy on the MNIST dataset, comparable with state of the art (SOTA) results.
What makes KirchhoffNet more intriguing is its potential in the realm of
hardware. Contemporary deep neural networks are conventionally deployed on
GPUs. In contrast, KirchhoffNet can be physically realized by an analog
electronic circuit. Moreover, we justify that irrespective of the number of
parameters within a KirchhoffNet, its forward calculation can always be
completed within 1/f seconds, with f representing the hardware's clock
frequency. This characteristic introduces a promising technology for
implementing ultra-large-scale neural networks.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15875" title="Abstract">arXiv:2310.15875</a> [<a href="/pdf/2310.15875" title="Download PDF">pdf</a>, <a href="/format/2310.15875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Modeling and Analysis of Transmission Error in Harmonic  Drive Systems: Nonlinear Dynamics, Error Modeling, and Compensation  Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Ju Wu</a>, 
<a href="/search/cs?searchtype=author&query=Schuchert%2C+P+L">Philippe Louis Schuchert</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+A">Alireza Karimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">Harmonic drive systems (HDS) are high-precision robotic transmissions
featuring compact size and high gear ratios. However, issues like kinematic
transmission errors hamper their precision performance. This article focuses on
data-driven modeling and analysis of an HDS to improve kinematic error
compensation. The background introduces HDS mechanics, nonlinear attributes,
and modeling approaches from literature. The HDS dynamics are derived using
Lagrange equations. Experiments under aggressive conditions provide training
data exhibiting deterministic patterns. Various linear and nonlinear models
have been developed. The best-performing model, based on a nonlinear neural
network, achieves over 98\% accuracy for one-step predictions on both the
training and validation data sets. A phenomenological model separates the
kinematic error into a periodic pure part and flexible part. Apart from
implementation of estimated transmission error injection compensation, novel
compensation mechanisms policies for the kinematic error are analyzed and
proposed, including nonlinear model predictive control and frequency
loop-shaping. The feedback loop is analyzed to select the controller for
vibration mitigation. Main contributions include the nonlinear dynamics
derivation, data-driven nonlinear modeling of flexible kinematic errors,
repeatable experiment design, and proposed novel compensation mechanism and
policies. Future work involves using physics-informed neural networks,
sensitivity analysis, full life-cycle monitoring, and extracting physical laws
directly from data.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15883" title="Abstract">arXiv:2310.15883</a> [<a href="/pdf/2310.15883" title="Download PDF">pdf</a>, <a href="/format/2310.15883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attitude Takeover Control for Noncooperative Space Targets Based on  Gaussian Processes with Online Model Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yuhan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+P">Pengyu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+C">Chang-Hun Lee</a>, 
<a href="/search/eess?searchtype=author&query=T%C3%B3th%2C+R">Roland T&#xf3;th</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 14 figures. Submitted to in IEEE Transactions on Aerospace and Electronic Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">One major challenge for autonomous attitude takeover control for on-orbit
servicing of spacecraft is that an accurate dynamic motion model of the
combined vehicles is highly nonlinear, complex and often costly to identify
online, which makes traditional model-based control impractical for this task.
To address this issue, a recursive online sparse Gaussian Process (GP)-based
learning strategy for attitude takeover control of noncooperative targets with
maneuverability is proposed, where the unknown dynamics are online compensated
based on the learnt GP model in a semi-feedforward manner. The method enables
the continuous use of on-orbit data to successively improve the learnt model
during online operation and has reduced computational load compared to standard
GP regression. Next to the GP-based feedforward, a feedback controller is
proposed that varies its gains based on the predicted model confidence,
ensuring robustness of the overall scheme. Moreover, rigorous theoretical
proofs of Lyapunov stability and boundedness guarantees of the proposed
method-driven closed-loop system are provided in the probabilistic sense. A
simulation study based on a high-fidelity simulator is used to show the
effectiveness of the proposed strategy and demonstrate its high performance.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15887" title="Abstract">arXiv:2310.15887</a> [<a href="/pdf/2310.15887" title="Download PDF">pdf</a>, <a href="/format/2310.15887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaptiX -- A Transitional XR Framework for Development and Evaluation of  Shared Control Applications in Assistive Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pascher%2C+M">Max Pascher</a>, 
<a href="/search/cs?searchtype=author&query=Goldau%2C+F+F">Felix Ferdinand Goldau</a>, 
<a href="/search/cs?searchtype=author&query=Kronhardt%2C+K">Kirill Kronhardt</a>, 
<a href="/search/cs?searchtype=author&query=Frese%2C+U">Udo Frese</a>, 
<a href="/search/cs?searchtype=author&query=Gerken%2C+J">Jens Gerken</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted submission at The 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Software Engineering (cs.SE)

</div>
<p class="mathjax">With the ongoing efforts to empower people with mobility impairments and the
increase in technological acceptance by the general public, assistive
technologies, such as collaborative robotic arms, are gaining popularity. Yet,
their widespread success is limited by usability issues, specifically the
disparity between user input and software control along the autonomy continuum.
To address this, shared control concepts provide opportunities to combine the
targeted increase of user autonomy with a certain level of computer assistance.
This paper presents the free and open-source AdaptiX XR framework for
developing and evaluating shared control applications in a high-resolution
simulation environment. The initial framework consists of a simulated robotic
arm with an example scenario in Virtual Reality (VR), multiple standard control
interfaces, and a specialized recording/replay system. AdaptiX can easily be
extended for specific research needs, allowing Human-Robot Interaction (HRI)
researchers to rapidly design and test novel interaction methods, intervention
strategies, and multi-modal feedback techniques, without requiring an actual
physical robotic arm during the early phases of ideation, prototyping, and
evaluation. Also, a Robot Operating System (ROS) integration enables the
controlling of a real robotic arm in a PhysicalTwin approach without any
simulation-reality gap. Here, we review the capabilities and limitations of
AdaptiX in detail and present three bodies of research based on the framework.
AdaptiX can be accessed at https://adaptix.robot-research.de.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15888" title="Abstract">arXiv:2310.15888</a> [<a href="/pdf/2310.15888" title="Download PDF">pdf</a>, <a href="/format/2310.15888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State Sequences Prediction via Fourier Transform for Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Mingxuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Y">Yufei Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wengang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Feng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While deep reinforcement learning (RL) has been demonstrated effective in
solving complex control tasks, sample efficiency remains a key challenge due to
the large amounts of data required for remarkable performance. Existing
research explores the application of representation learning for data-efficient
RL, e.g., learning predictive representations by predicting long-term future
states. However, many existing methods do not fully exploit the structural
information inherent in sequential state signals, which can potentially improve
the quality of long-term decision-making but is difficult to discern in the
time domain. To tackle this problem, we propose State Sequences Prediction via
Fourier Transform (SPF), a novel method that exploits the frequency domain of
state sequences to extract the underlying patterns in time series data for
learning expressive representations efficiently. Specifically, we theoretically
analyze the existence of structural information in state sequences, which is
closely related to policy performance and signal regularity, and then propose
to predict the Fourier transform of infinite-step future state sequences to
extract such information. One of the appealing features of SPF is that it is
simple to implement while not requiring storage of infinite-step future states
as prediction targets. Experiments demonstrate that the proposed method
outperforms several state-of-the-art algorithms in terms of both sample
efficiency and performance.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15890" title="Abstract">arXiv:2310.15890</a> [<a href="/pdf/2310.15890" title="Download PDF">pdf</a>, <a href="/format/2310.15890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-feature Contrastive Loss for Decentralized Deep Learning on  Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aketi%2C+S+A">Sai Aparna Aketi</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures, 11 tables. arXiv admin note: text overlap with <a href="/abs/2305.04792">arXiv:2305.04792</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The current state-of-the-art decentralized learning algorithms mostly assume
the data distribution to be Independent and Identically Distributed (IID).
However, in practical scenarios, the distributed datasets can have
significantly heterogeneous data distributions across the agents. In this work,
we present a novel approach for decentralized learning on heterogeneous data,
where data-free knowledge distillation through contrastive loss on
cross-features is utilized to improve performance. Cross-features for a pair of
neighboring agents are the features (i.e., last hidden layer activations)
obtained from the data of an agent with respect to the model parameters of the
other agent. We demonstrate the effectiveness of the proposed technique through
an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10,
CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network
topologies. Our experiments show that the proposed method achieves superior
performance (0.2-4% improvement in test accuracy) compared to other existing
techniques for decentralized learning on heterogeneous data.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15896" title="Abstract">arXiv:2310.15896</a> [<a href="/pdf/2310.15896" title="Download PDF">pdf</a>, <a href="/format/2310.15896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs  with Multi-turn Health Conversations Polished by ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yirong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+X">Xiaofen Xing</a>, 
<a href="/search/cs?searchtype=author&query=zheng%2C+h">huimin zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhipei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+K">Kai Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sihang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jieling Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangmin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large language models (LLMs) have performed well in providing general and
extensive health suggestions in single-turn conversations, exemplified by
systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the
limited information provided by users during single turn results in inadequate
personalization and targeting of the generated suggestions, which requires
users to independently select the useful part. It is mainly caused by the
missing ability to engage in multi-turn questioning. In real-world medical
consultations, doctors usually employ a series of iterative inquiries to
comprehend the patient's condition thoroughly, enabling them to provide
effective and personalized suggestions subsequently, which can be defined as
chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose
BianQue, a ChatGLM-based LLM finetuned with the self-constructed health
conversation dataset BianQueCorpus that is consist of multiple turns of
questioning and health suggestions polished by ChatGPT. Experimental results
demonstrate that the proposed BianQue can simultaneously balance the
capabilities of both questioning and health suggestions, which will help
promote the research and application of LLMs in the field of proactive health.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15902" title="Abstract">arXiv:2310.15902</a> [<a href="/pdf/2310.15902" title="Download PDF">pdf</a>, <a href="/format/2310.15902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delaunay Bifiltrations of Functions on Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso%2C+%C3%81+J">&#xc1;ngel Javier Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Kerber%2C+M">Michael Kerber</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+T">Tung Lam</a>, 
<a href="/search/cs?searchtype=author&query=Lesnick%2C+M">Michael Lesnick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 7 figures, 8 tables. To appear in the proceedings of SODA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Algebraic Topology (math.AT)

</div>
<p class="mathjax">The Delaunay filtration $\mathcal{D}_{\bullet}(X)$ of a point cloud $X\subset
\mathbb{R}^d$ is a central tool of computational topology. Its use is justified
by the topological equivalence of $\mathcal{D}_{\bullet}(X)$ and the offset
(i.e., union-of-balls) filtration of $X$. Given a function $\gamma: X \to
\mathbb{R}$, we introduce a Delaunay bifiltration
$\mathcal{DC}_{\bullet}(\gamma)$ that satisfies an analogous topological
equivalence, ensuring that $\mathcal{DC}_{\bullet}(\gamma)$ topologically
encodes the offset filtrations of all sublevel sets of $\gamma$, as well as the
topological relations between them. $\mathcal{DC}_{\bullet}(\gamma)$ is of size
$O(|X|^{\lceil\frac{d+1}{2}\rceil})$, which for $d$ odd matches the worst-case
size of $\mathcal{D}_{\bullet}(X)$. Adapting the Bowyer-Watson algorithm for
computing Delaunay triangulations, we give a simple, practical algorithm to
compute $\mathcal{DC}_{\bullet}(\gamma)$ in time $O(|X|^{\lceil
\frac{d}{2}\rceil +1})$. Our implementation, based on CGAL, computes
$\mathcal{DC}_{\bullet}(\gamma)$ with modest overhead compared to computing
$\mathcal{D}_{\bullet}(X)$, and handles tens of thousands of points in
$\mathbb{R}^3$ within seconds.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15903" title="Abstract">arXiv:2310.15903</a> [<a href="/pdf/2310.15903" title="Download PDF">pdf</a>, <a href="/ps/2310.15903" title="Download PostScript">ps</a>, <a href="/format/2310.15903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Collapse in Multi-label Learning with Pick-all-label Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yutong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study deep neural networks for the multi-label classification (MLab) task
through the lens of neural collapse (NC). Previous works have been restricted
to the multi-class classification setting and discovered a prevalent NC
phenomenon comprising of the following properties for the last-layer features:
(i) the variability of features within every class collapses to zero, (ii) the
set of feature means form an equi-angular tight frame (ETF), and (iii) the last
layer classifiers collapse to the feature mean upon some scaling. We generalize
the study to multi-label learning, and prove for the first time that a
generalized NC phenomenon holds with the "pick-all-label'' formulation. Under
the natural analog of the unconstrained feature model (UFM), we establish that
the only global classifier of the pick-all-label cross entropy loss display the
same ETF geometry which further collapse to multiplicity-1 feature class means.
Besides, we discover a combinatorial property in generalized NC which is unique
for multi-label learning that we call ``tag-wise average'' property, where the
feature class-means of samples with multiple labels are scaled average of the
feature class-means of single label tags. Theoretically, we establish global
optimality result for the pick-all-label cross-entropy risk for the UFM.
Additionally, We also provide empirical evidence to support our investigation
into training deep neural networks on multi-label datasets, resulting in
improved training efficiency.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15904" title="Abstract">arXiv:2310.15904</a> [<a href="/pdf/2310.15904" title="Download PDF">pdf</a>, <a href="/format/2310.15904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Stochastic Parrots have Feelings Too? Improving Neural Detection of  Synthetic Text via Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cowap%2C+A">Alan Cowap</a>, 
<a href="/search/cs?searchtype=author&query=Graham%2C+Y">Yvette Graham</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+J">Jennifer Foster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EMNLP 2023 (long paper). Camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent developments in generative AI have shone a spotlight on
high-performance synthetic text generation technologies. The now wide
availability and ease of use of such models highlights the urgent need to
provide equally powerful technologies capable of identifying synthetic text.
With this in mind, we draw inspiration from psychological studies which suggest
that people can be driven by emotion and encode emotion in the text they
compose. We hypothesize that pretrained language models (PLMs) have an
affective deficit because they lack such an emotional driver when generating
text and consequently may generate synthetic text which has affective
incoherence i.e. lacking the kind of emotional coherence present in
human-authored text. We subsequently develop an emotionally aware detector by
fine-tuning a PLM on emotion. Experiment results indicate that our
emotionally-aware detector achieves improvements across a range of synthetic
text generators, various sized models, datasets, and domains. Finally, we
compare our emotionally-aware synthetic text detector to ChatGPT in the task of
identification of its own output and show substantial gains, reinforcing the
potential of emotion as a signal to identify synthetic text. Code, models, and
datasets are available at https: //github.com/alanagiasi/emoPLMsynth
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15905" title="Abstract">arXiv:2310.15905</a> [<a href="/pdf/2310.15905" title="Download PDF">pdf</a>, <a href="/format/2310.15905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Probing All You Need? Indicator Tasks as an Alternative to Probing  Embedding Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levy%2C+T">Tal Levy</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+O">Omer Goldman</a>, 
<a href="/search/cs?searchtype=author&query=Tsarfaty%2C+R">Reut Tsarfaty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The ability to identify and control different kinds of linguistic information
encoded in vector representations of words has many use cases, especially for
explainability and bias removal. This is usually done via a set of simple
classification tasks, termed probes, to evaluate the information encoded in the
embedding space. However, the involvement of a trainable classifier leads to
entanglement between the probe's results and the classifier's nature. As a
result, contemporary works on probing include tasks that do not involve
training of auxiliary models. In this work we introduce the term indicator
tasks for non-trainable tasks which are used to query embedding spaces for the
existence of certain properties, and claim that this kind of tasks may point to
a direction opposite to probes, and that this contradiction complicates the
decision on whether a property exists in an embedding space. We demonstrate our
claims with two test cases, one dealing with gender debiasing and another with
the erasure of morphological information from embedding spaces. We show that
the application of a suitable indicator provides a more accurate picture of the
information captured and removed compared to probes. We thus conclude that
indicator tasks should be implemented and taken into consideration when
eliciting information from embedded representations.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15907" title="Abstract">arXiv:2310.15907</a> [<a href="/pdf/2310.15907" title="Download PDF">pdf</a>, <a href="/format/2310.15907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiCROM: Linear-Subspace Continuous Reduced Order Modeling with Neural  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yue Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P+Y">Peter Yichen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhecheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chiaramonte%2C+M+M">Maurizio M. Chiaramonte</a>, 
<a href="/search/cs?searchtype=author&query=Carlberg%2C+K">Kevin Carlberg</a>, 
<a href="/search/cs?searchtype=author&query=Grinspun%2C+E">Eitan Grinspun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Linear reduced-order modeling (ROM) simplifies complex simulations by
approximating the behavior of a system using a simplified kinematic
representation. Typically, ROM is trained on input simulations created with a
specific spatial discretization, and then serves to accelerate simulations with
the same discretization. This discretization-dependence is restrictive.
<br />Becoming independent of a specific discretization would provide flexibility
to mix and match mesh resolutions, connectivity, and type (tetrahedral,
hexahedral) in training data; to accelerate simulations with novel
discretizations unseen during training; and to accelerate adaptive simulations
that temporally or parametrically change the discretization.
<br />We present a flexible, discretization-independent approach to reduced-order
modeling. Like traditional ROM, we represent the configuration as a linear
combination of displacement fields. Unlike traditional ROM, our displacement
fields are continuous maps from every point on the reference domain to a
corresponding displacement vector; these maps are represented as implicit
neural fields.
<br />With linear continuous ROM (LiCROM), our training set can include multiple
geometries undergoing multiple loading conditions, independent of their
discretization. This opens the door to novel applications of reduced order
modeling. We can now accelerate simulations that modify the geometry at
runtime, for instance via cutting, hole punching, and even swapping the entire
mesh. We can also accelerate simulations of geometries unseen during training.
We demonstrate one-shot generalization, training on a single geometry and
subsequently simulating various unseen geometries.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15910" title="Abstract">arXiv:2310.15910</a> [<a href="/pdf/2310.15910" title="Download PDF">pdf</a>, <a href="/format/2310.15910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Mechanisms for Factual Recall in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qinan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Merullo%2C+J">Jack Merullo</a>, 
<a href="/search/cs?searchtype=author&query=Pavlick%2C+E">Ellie Pavlick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language Models (LMs) often must integrate facts they memorized in
pretraining with new information that appears in a given context. These two
sources can disagree, causing competition within the model, and it is unclear
how an LM will resolve the conflict. On a dataset that queries for knowledge of
world capitals, we investigate both distributional and mechanistic determinants
of LM behavior in such situations. Specifically, we measure the proportion of
the time an LM will use a counterfactual prefix (e.g., "The capital of Poland
is London") to overwrite what it learned in pretraining ("Warsaw"). On Pythia
and GPT2, the training frequency of both the query country ("Poland") and the
in-context city ("London") highly affect the models' likelihood of using the
counterfactual. We then use head attribution to identify individual attention
heads that either promote the memorized answer or the in-context answer in the
logits. By scaling up or down the value vector of these heads, we can control
the likelihood of using the in-context answer on new data. This method can
increase the rate of generating the in-context answer to 88\% of the time
simply by scaling a single head at runtime. Our work contributes to a body of
evidence showing that we can often localize model behaviors to specific
components and provides a proof of concept for how future methods might control
model behavior dynamically at runtime.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15911" title="Abstract">arXiv:2310.15911</a> [<a href="/pdf/2310.15911" title="Download PDF">pdf</a>, <a href="/format/2310.15911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beam Design and Signal Enhancement in RIS-Aided Multi-User Communication  Systems: A Maximization-Minimization Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xiong%2C+R">Rujing Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+J">Jialong Lu</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+K">Ke Yin</a>, 
<a href="/search/eess?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+R+C">Robert Caiming Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Most beam design schemes in RIS-aided multi-user systems suffer from
imbalanced signal power gains and computation complexity. This paper tackles
the crucial beam design issue by focusing on received power optimization.
Concretely, we leverage the passive characteristics of RIS, modeling the
reflecting signals and articulating a comprehensive max-min optimization
framework. To efficiently address the beamforming issue, we propose the
Moreau-Yosida approximation (MA) algorithm, which pursues the optimal
beamforming designs based on arbitrary utility functions in the user's received
power. The comprehensive numerical simulations and prototype experiments
substantiate the effectiveness of our proposed algorithm.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15912" title="Abstract">arXiv:2310.15912</a> [<a href="/pdf/2310.15912" title="Download PDF">pdf</a>, <a href="/format/2310.15912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Climate Change Impact on Agricultural Land Suitability: An Interpretable  Machine Learning-Based Eurasia Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shevchenko%2C+V">Valeriy Shevchenko</a>, 
<a href="/search/cs?searchtype=author&query=Taniushkina%2C+D">Daria Taniushkina</a>, 
<a href="/search/cs?searchtype=author&query=Lukashevich%2C+A">Aleksander Lukashevich</a>, 
<a href="/search/cs?searchtype=author&query=Bulkin%2C+A">Aleksandr Bulkin</a>, 
<a href="/search/cs?searchtype=author&query=Grinis%2C+R">Roland Grinis</a>, 
<a href="/search/cs?searchtype=author&query=Kovalev%2C+K">Kirill Kovalev</a>, 
<a href="/search/cs?searchtype=author&query=Narozhnaia%2C+V">Veronika Narozhnaia</a>, 
<a href="/search/cs?searchtype=author&query=Sotiriadi%2C+N">Nazar Sotiriadi</a>, 
<a href="/search/cs?searchtype=author&query=Krenke%2C+A">Alexander Krenke</a>, 
<a href="/search/cs?searchtype=author&query=Maximov%2C+Y">Yury Maximov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The United Nations has identified improving food security and reducing hunger
as essential components of its sustainable development goals. As of 2021,
approximately 828 million people worldwide are experiencing hunger and
malnutrition, with numerous fatalities reported. Climate change significantly
impacts agricultural land suitability, potentially leading to severe food
shortages and subsequent social and political conflicts. To address this
pressing issue, we have developed a machine learning-based approach to predict
the risk of substantial land suitability degradation and changes in irrigation
patterns. Our study focuses on Central Eurasia, a region burdened with economic
and social challenges.
<br />This study represents a pioneering effort in utilizing machine learning
methods to assess the impact of climate change on agricultural land suitability
under various carbon emissions scenarios. Through comprehensive feature
importance analysis, we unveil specific climate and terrain characteristics
that exert influence on land suitability. Our approach achieves remarkable
accuracy, offering policymakers invaluable insights to facilitate informed
decisions aimed at averting a humanitarian crisis, including strategies such as
the provision of additional water and fertilizers. This research underscores
the tremendous potential of machine learning in addressing global challenges,
with a particular emphasis on mitigating hunger and malnutrition.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15913" title="Abstract">arXiv:2310.15913</a> [<a href="/pdf/2310.15913" title="Download PDF">pdf</a>, <a href="/format/2310.15913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigate Domain Shift by Primary-Auxiliary Objectives Association for  Generalizing Person ReID
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qilei Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shaogang Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While deep learning has significantly improved ReID model accuracy under the
independent and identical distribution (IID) assumption, it has also become
clear that such models degrade notably when applied to an unseen novel domain
due to unpredictable/unknown domain shift. Contemporary domain generalization
(DG) ReID models struggle in learning domain-invariant representation solely
through training on an instance classification objective. We consider that a
deep learning model is heavily influenced and therefore biased towards
domain-specific characteristics, e.g., background clutter, scale and viewpoint
variations, limiting the generalizability of the learned model, and hypothesize
that the pedestrians are domain invariant owning they share the same structural
characteristics. To enable the ReID model to be less domain-specific from these
pure pedestrians, we introduce a method that guides model learning of the
primary ReID instance classification objective by a concurrent auxiliary
learning objective on weakly labeled pedestrian saliency detection. To solve
the problem of conflicting optimization criteria in the model parameter space
between the two learning objectives, we introduce a Primary-Auxiliary
Objectives Association (PAOA) mechanism to calibrate the loss gradients of the
auxiliary task towards the primary learning task gradients. Benefiting from the
harmonious multitask learning design, our model can be extended with the recent
test-time diagram to form the PAOA+, which performs on-the-fly optimization
against the auxiliary objective in order to maximize the model's generative
capacity in the test target domain. Experiments demonstrate the superiority of
the proposed PAOA model.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15915" title="Abstract">arXiv:2310.15915</a> [<a href="/pdf/2310.15915" title="Download PDF">pdf</a>, <a href="/format/2310.15915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Pure Demand Operational Semantics With Applications to Program  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+S">Scott Smith</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Robert Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">This paper develops a novel minimal-state operational semantics for
higher-order functional languages which uses only the call stack and two source
program points as the complete state information: there is no environment, no
substitution, no continuation, etc. We prove this form of operational semantics
is equivalent to standard presentations.
<br />We then show how this approach can open the door to potential new
applications: we define a program analysis as a direct finitization of this
operational semantics. The program analysis that naturally emerges has a number
of novel and interesting properties compared to standard program analyses for
higher-order programs: for example, it can infer recurrences, and does not need
value widening. We both give a formal definition of the analysis and describe
our current implementation.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15916" title="Abstract">arXiv:2310.15916</a> [<a href="/pdf/2310.15916" title="Download PDF">pdf</a>, <a href="/format/2310.15916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning Creates Task Vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hendel%2C+R">Roee Hendel</a>, 
<a href="/search/cs?searchtype=author&query=Geva%2C+M">Mor Geva</a>, 
<a href="/search/cs?searchtype=author&query=Globerson%2C+A">Amir Globerson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning (ICL) in Large Language Models (LLMs) has emerged as a
powerful new learning paradigm. However, its underlying mechanism is still not
well understood. In particular, it is challenging to map it to the "standard"
machine learning framework, where one uses a training set $S$ to find a
best-fitting function $f(x)$ in some hypothesis class. Here we make progress on
this problem by showing that the functions learned by ICL often have a very
simple structure: they correspond to the transformer LLM whose only inputs are
the query $x$ and a single "task vector" calculated from the training set.
Thus, ICL can be seen as compressing $S$ into a single task vector
$\boldsymbol{\theta}(S)$ and then using this task vector to modulate the
transformer to produce the output. We support the above claim via comprehensive
experiments across a range of models and tasks.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15921" title="Abstract">arXiv:2310.15921</a> [<a href="/pdf/2310.15921" title="Download PDF">pdf</a>, <a href="/format/2310.15921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Learning-based Sentence Encoders Implicitly Weight  Informative Words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurita%2C+H">Hiroto Kurita</a>, 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+G">Goro Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Yokoi%2C+S">Sho Yokoi</a>, 
<a href="/search/cs?searchtype=author&query=Inui%2C+K">Kentaro Inui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures, accepted to EMNLP 2023 Findings (short paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The performance of sentence encoders can be significantly improved through
the simple practice of fine-tuning using contrastive loss. A natural question
arises: what characteristics do models acquire during contrastive learning?
This paper theoretically and experimentally shows that contrastive-based
sentence encoders implicitly weight words based on information-theoretic
quantities; that is, more informative words receive greater weight, while
others receive less. The theory states that, in the lower bound of the optimal
value of the contrastive learning objective, the norm of word embedding
reflects the information gain associated with the distribution of surrounding
words. We also conduct comprehensive experiments using various models, multiple
datasets, two methods to measure the implicit weighting of models (Integrated
Gradients and SHAP), and two information-theoretic quantities (information gain
and self-information). The results provide empirical evidence that contrastive
fine-tuning emphasizes informative words.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15928" title="Abstract">arXiv:2310.15928</a> [<a href="/pdf/2310.15928" title="Download PDF">pdf</a>, <a href="/format/2310.15928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AO-Grasp: Articulated Object Grasp Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morlans%2C+C+P">Carlota Par&#xe9;s Morlans</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Claire Chen</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Y">Yijia Weng</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+M">Michelle Yi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuying Huang</a>, 
<a href="/search/cs?searchtype=author&query=Heppert%2C+N">Nick Heppert</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Linqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Guibas%2C+L">Leonidas Guibas</a>, 
<a href="/search/cs?searchtype=author&query=Bohg%2C+J">Jeannette Bohg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://stanford-iprl-lab.github.io/ao-grasp">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We introduce AO-Grasp, a grasp proposal method that generates stable and
actionable 6 degree-of-freedom grasps for articulated objects. Our generated
grasps enable robots to interact with articulated objects, such as opening and
closing cabinets and appliances. Given a segmented partial point cloud of a
single articulated object, AO-Grasp predicts the best grasp points on the
object with a novel Actionable Grasp Point Predictor model and then finds
corresponding grasp orientations for each point by leveraging a
state-of-the-art rigid object grasping method. We train AO-Grasp on our new
AO-Grasp Dataset, which contains 48K actionable parallel-jaw grasps on
synthetic articulated objects. In simulation, AO-Grasp achieves higher grasp
success rates than existing rigid object grasping and articulated object
interaction baselines on both train and test categories. Additionally, we
evaluate AO-Grasp on 120 realworld scenes of objects with varied geometries,
articulation axes, and joint states, where AO-Grasp produces successful grasps
on 67.5% of scenes, while the baseline only produces successful grasps on 33.3%
of scenes.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15929" title="Abstract">arXiv:2310.15929</a> [<a href="/pdf/2310.15929" title="Download PDF">pdf</a>, <a href="/format/2310.15929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> E-Sparse: Boosting the Large Language Model Inference through  Entropy-based N:M Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Lin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jianchen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Zhanhui Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Traditional pruning methods are known to be challenging to work in Large
Language Models (LLMs) for Generative AI because of their unaffordable training
process and large computational demands. For the first time, we introduce the
information entropy of hidden state features into a pruning metric design,
namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse
employs the information richness to leverage the channel importance, and
further incorporates several novel techniques to put it into effect: (1) it
introduces information entropy to enhance the significance of parameter weights
and input feature norms as a novel pruning metric, and performs N:M sparsity
without modifying the remaining weights. (2) it designs global naive shuffle
and local block shuffle to quickly optimize the information distribution and
adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is
implemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere
GPUs. Extensive experiments on the LLaMA family and OPT models show that
E-Sparse can significantly speed up the model inference over the dense model
(up to 1.53X) and obtain significant memory saving (up to 43.52%), with
acceptable accuracy loss.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15930" title="Abstract">arXiv:2310.15930</a> [<a href="/pdf/2310.15930" title="Download PDF">pdf</a>, <a href="/format/2310.15930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CDSD: Chinese Dysarthria Speech Database
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mengyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Ming Gao</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+X">Xinchen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiru Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jun Du</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+D">Dengfeng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Su-Jing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We present the Chinese Dysarthria Speech Database (CDSD) as a valuable
resource for dysarthria research. This database comprises speech data from 24
participants with dysarthria. Among these participants, one recorded an
additional 10 hours of speech data, while each recorded one hour, resulting in
34 hours of speech material. To accommodate participants with varying cognitive
levels, our text pool primarily consists of content from the AISHELL-1 dataset
and speeches by primary and secondary school students. When participants read
these texts, they must use a mobile device or the ZOOM F8n multi-track field
recorder to record their speeches. In this paper, we elucidate the data
collection and annotation processes and present an approach for establishing a
baseline for dysarthric speech recognition. Furthermore, we conducted a
speaker-dependent dysarthric speech recognition experiment using an additional
10 hours of speech data from one of our participants. Our research findings
indicate that, through extensive data-driven model training, fine-tuning
limited quantities of specific individual data yields commendable results in
speaker-dependent dysarthric speech recognition. However, we observe
significant variations in recognition results among different dysarthric
speakers. These insights provide valuable reference points for
speaker-dependent dysarthric speech recognition.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15931" title="Abstract">arXiv:2310.15931</a> [<a href="/pdf/2310.15931" title="Download PDF">pdf</a>, <a href="/format/2310.15931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GO-FEAP: Global Optimal UAV Planner Using Frontier-Omission-Aware  Exploration and Altitude-Stratified Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiye Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenshuai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+L">Licong Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiasong Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages,29 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Autonomous exploration is a fundamental problem for various applications of
unmanned aerial vehicles(UAVs). Existing methods, however, are demonstrated to
static local optima and two-dimensional exploration. To address these
challenges, this paper introduces GO-FEAP (Global Optimal UAV Planner Using
Frontier-Omission-Aware Exploration and Altitude-Stratified Planning), aiming
to achieve efficient and complete three-dimensional exploration.
Frontier-Omission-Aware Exploration module presented in this work takes into
account multiple pivotal factors, encompassing frontier distance, nearby
frontier count, frontier duration, and frontier categorization, for a
comprehensive assessment of frontier importance. Furthermore, to tackle
scenarios with substantial vertical variations, we introduce the
Altitude-Stratified Planning strategy, which stratifies the three-dimensional
space based on altitude, conducting global-local planning for each stratum. The
objective of global planning is to identify the most optimal frontier for
exploration, followed by viewpoint selection and local path optimization based
on frontier type, ultimately generating dynamically feasible three-dimensional
spatial exploration trajectories. We present extensive benchmark and real-world
tests, in which our method completes the exploration tasks with unprecedented
completeness compared to state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15932" title="Abstract">arXiv:2310.15932</a> [<a href="/pdf/2310.15932" title="Download PDF">pdf</a>, <a href="/format/2310.15932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Robust Mean Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kane%2C+D+M">Daniel M. Kane</a>, 
<a href="/search/cs?searchtype=author&query=Diakonikolas%2C+I">Ilias Diakonikolas</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Hanshen Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sihan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in SODA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the problem of high-dimensional robust mean estimation in an online
setting. Specifically, we consider a scenario where $n$ sensors are measuring
some common, ongoing phenomenon. At each time step $t=1,2,\ldots,T$, the
$i^{th}$ sensor reports its readings $x^{(i)}_t$ for that time step. The
algorithm must then commit to its estimate $\mu_t$ for the true mean value of
the process at time $t$. We assume that most of the sensors observe independent
samples from some common distribution $X$, but an $\epsilon$-fraction of them
may instead behave maliciously. The algorithm wishes to compute a good
approximation $\mu$ to the true mean $\mu^\ast := \mathbf{E}[X]$. We note that
if the algorithm is allowed to wait until time $T$ to report its estimate, this
reduces to the well-studied problem of robust mean estimation. However, the
requirement that our algorithm produces partial estimates as the data is coming
in substantially complicates the situation.
<br />We prove two main results about online robust mean estimation in this model.
First, if the uncorrupted samples satisfy the standard condition of
$(\epsilon,\delta)$-stability, we give an efficient online algorithm that
outputs estimates $\mu_t$, $t \in [T],$ such that with high probability it
holds that $\|\mu-\mu^\ast\|_2 = O(\delta \log(T))$, where $\mu = (\mu_t)_{t
\in [T]}$. We note that this error bound is nearly competitive with the best
offline algorithms, which would achieve $\ell_2$-error of $O(\delta)$. Our
second main result shows that with additional assumptions on the input (most
notably that $X$ is a product distribution) there are inefficient algorithms
whose error does not depend on $T$ at all.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15933" title="Abstract">arXiv:2310.15933</a> [<a href="/pdf/2310.15933" title="Download PDF">pdf</a>, <a href="/format/2310.15933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling and Contribution of Flexible Heating Systems for Transmission  Grid Congestion Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kr%C3%B6ger%2C+D">David Kr&#xf6;ger</a>, 
<a href="/search/eess?searchtype=author&query=Teodosic%2C+M">Milijana Teodosic</a>, 
<a href="/search/eess?searchtype=author&query=Rehtanz%2C+C">Christian Rehtanz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">The large-scale integration of flexible heating systems in the European
electricity market leads to a substantial increase of transportation
requirements and consecutively grid congestions in the continental transmission
grid. Novel model formulations for the grid-aware operation of both individual
small-scale heat pumps and large-scale power-to-heat (PtH) units located in
district heating networks are presented. The functionality of the models and
the contribution of flexible heating systems for transmission grid congestion
management is evaluated by running simulations for the target year 2035 for the
German transmission grid. The findings show a decrease in annual conventional
redispatch volumes and renewable energy sources (RES) curtailment resulting in
cost savings of approximately 6 % through the integration of flexible heating
systems in the grid congestion management scheme. The analysis suggests that
especially large-scale PtH units in combination with thermal energy storages
can contribute significantly to the alleviation of grid congestion and foster
RES integration.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15934" title="Abstract">arXiv:2310.15934</a> [<a href="/pdf/2310.15934" title="Download PDF">pdf</a>, <a href="/format/2310.15934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redactable Signature Schemes and Zero-knowledge Proofs: A comparative  examination for applications in Decentralized Digital Identity Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumara%2C+B">Bryan Kumara</a>, 
<a href="/search/cs?searchtype=author&query=Hooper%2C+M">Mark Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Maple%2C+C">Carsten Maple</a>, 
<a href="/search/cs?searchtype=author&query=Hobson%2C+T">Timothy Hobson</a>, 
<a href="/search/cs?searchtype=author&query=Crowcroft%2C+J">Jon Crowcroft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, Trustworthy digital identity international conference 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Redactable Signature Schemes and Zero-Knowledge Proofs are two radically
different approaches to enable privacy. This paper analyses their merits and
drawbacks when applied to decentralized identity system. Redactable Signatures,
though competitively quick and compact, are not as expressive as zero-knowledge
proofs and do not provide the same level of privacy. On the other hand,
zero-knowledge proofs can be much faster but some protocols require a trusted
set-up. We conclude that given the benefits and drawbacks, redactable
signatures are more appropriate at an earlier stage and zero-knowledge proofs
are more appropriate at a later stage for decentralized identity systems
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15935" title="Abstract">arXiv:2310.15935</a> [<a href="/pdf/2310.15935" title="Download PDF">pdf</a>, <a href="/format/2310.15935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mediator Interpretation and Faster Learning Algorithms for Linear  Correlated Equilibria in General Extensive-Form Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">A recent paper by Farina &amp; Pipis (2023) established the existence of
uncoupled no-linear-swap regret dynamics with polynomial-time iterations in
extensive-form games. The equilibrium points reached by these dynamics, known
as linear correlated equilibria, are currently the tightest known relaxation of
correlated equilibrium that can be learned in polynomial time in any finite
extensive-form game. However, their properties remain vastly unexplored, and
their computation is onerous. In this paper, we provide several contributions
shedding light on the fundamental nature of linear-swap regret. First, we show
a connection between linear deviations and a generalization of communication
deviations in which the player can make queries to a "mediator" who replies
with action recommendations, and, critically, the player is not constrained to
match the timing of the game as would be the case for communication deviations.
We coin this latter set the untimed communication (UTC) deviations. We show
that the UTC deviations coincide precisely with the linear deviations, and
therefore that any player minimizing UTC regret also minimizes linear-swap
regret. We then leverage this connection to develop state-of-the-art no-regret
algorithms for computing linear correlated equilibria, both in theory and in
practice. In theory, our algorithms achieve polynomially better per-iteration
runtimes; in practice, our algorithms represent the state of the art by several
orders of magnitude.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15937" title="Abstract">arXiv:2310.15937</a> [<a href="/pdf/2310.15937" title="Download PDF">pdf</a>, <a href="/format/2310.15937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Behavioral Perspective on Models of Linear Dynamical Networks with  Manifest Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+S">Shengling Shi</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Z">Zhiyong Sun</a>, 
<a href="/search/eess?searchtype=author&query=De+Schutter%2C+B">Bart De Schutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Networks of dynamical systems play an important role in various domains and
have motivated many studies on control and analysis of linear dynamical
networks. For linear network models considered in these studies, it is
typically pre-determined what signal channels are inputs and what are outputs.
These models do not capture the practical need to incorporate different
experimental situations, where different selections of input and output
channels are applied to the same network. Moreover, a unified view on different
network models is lacking. This work makes an initial step towards addressing
the above issues by taking a behavioral perspective, where input and output
channels are not pre-determined. The focus of this work is on behavioral
network models with only external variables. Novel dual graphical
representations, called system graphs and signal graphs, are introduced for
behavioral networks. Moreover, connections between behavioral network models
and structural vector autoregressive models are established. Besides their
connection in graphical representations, it is shown that the regularity of
interconnections is an essential assumption when choosing a structural vector
autoregressive model.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15938" title="Abstract">arXiv:2310.15938</a> [<a href="/pdf/2310.15938" title="Download PDF">pdf</a>, <a href="/format/2310.15938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ABKD: Graph Neural Network Compression with Attention-Based Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahluwalia%2C+A">Anshul Ahluwalia</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+R">Rohit Das</a>, 
<a href="/search/cs?searchtype=author&query=Behnam%2C+P">Payman Behnam</a>, 
<a href="/search/cs?searchtype=author&query=Khare%2C+A">Alind Khare</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>, 
<a href="/search/cs?searchtype=author&query=Tumanov%2C+A">Alexey Tumanov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have proven to be quite versatile for a variety
of applications, including recommendation systems, fake news detection, drug
discovery, and even computer vision. Due to the expanding size of
graph-structured data, GNN models have also increased in complexity, leading to
substantial latency issues. This is primarily attributed to the irregular
structure of graph data and its access pattern into memory. The natural
solution to reduce latency is to compress large GNNs into small GNNs. One way
to do this is via knowledge distillation (KD). However, most KD approaches for
GNNs only consider the outputs of the last layers and do not consider the
outputs of the intermediate layers of the GNNs; these layers may contain
important inductive biases indicated by the graph structure. To address this
shortcoming, we propose a novel KD approach to GNN compression that we call
Attention-Based Knowledge Distillation (ABKD). ABKD is a KD approach that uses
attention to identify important intermediate teacher-student layer pairs and
focuses on aligning their outputs. ABKD enables higher compression of GNNs with
a smaller accuracy dropoff compared to existing KD approaches. On average, we
achieve a 1.79% increase in accuracy with a 32.3x compression ratio on
OGBN-Mag, a large graph dataset, compared to state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15940" title="Abstract">arXiv:2310.15940</a> [<a href="/pdf/2310.15940" title="Download PDF">pdf</a>, <a href="/format/2310.15940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Behaviors with the Successor Features Keyboard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carvalho%2C+W">Wilka Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Saraiva%2C+A">Andre Saraiva</a>, 
<a href="/search/cs?searchtype=author&query=Filos%2C+A">Angelos Filos</a>, 
<a href="/search/cs?searchtype=author&query=Lampinen%2C+A+K">Andrew Kyle Lampinen</a>, 
<a href="/search/cs?searchtype=author&query=Matthey%2C+L">Loic Matthey</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+R+L">Richard L. Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Satinder Singh</a>, 
<a href="/search/cs?searchtype=author&query=Rezende%2C+D+J">Danilo J. Rezende</a>, 
<a href="/search/cs?searchtype=author&query=Zoran%2C+D">Daniel Zoran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Option Keyboard (OK) was recently proposed as a method for transferring
behavioral knowledge across tasks. OK transfers knowledge by adaptively
combining subsets of known behaviors using Successor Features (SFs) and
Generalized Policy Improvement (GPI). However, it relies on hand-designed
state-features and task encodings which are cumbersome to design for every new
environment. In this work, we propose the "Successor Features Keyboard" (SFK),
which enables transfer with discovered state-features and task encodings. To
enable discovery, we propose the "Categorical Successor Feature Approximator"
(CSFA), a novel learning algorithm for estimating SFs while jointly discovering
state-features and task encodings. With SFK and CSFA, we achieve the first
demonstration of transfer with SFs in a challenging 3D environment where all
the necessary representations are discovered. We first compare CSFA against
other methods for approximating SFs and show that only CSFA discovers
representations compatible with SF&amp;GPI at this scale. We then compare SFK
against transfer learning baselines and show that it transfers most quickly to
long-horizon tasks.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15941" title="Abstract">arXiv:2310.15941</a> [<a href="/pdf/2310.15941" title="Download PDF">pdf</a>, <a href="/format/2310.15941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> This is not a Dataset: A Large Negation Benchmark to Challenge Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ferrero%2C+I">Iker Garc&#xed;a-Ferrero</a>, 
<a href="/search/cs?searchtype=author&query=Altuna%2C+B">Bego&#xf1;a Altuna</a>, 
<a href="/search/cs?searchtype=author&query=%C3%81lvez%2C+J">Javier &#xc1;lvez</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Dios%2C+I">Itziar Gonzalez-Dios</a>, 
<a href="/search/cs?searchtype=author&query=Rigau%2C+G">German Rigau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Although large language models (LLMs) have apparently acquired a certain
level of grammatical knowledge and the ability to make generalizations, they
fail to interpret negation, a crucial step in Natural Language Processing. We
try to clarify the reasons for the sub-optimal performance of LLMs
understanding negation. We introduce a large semi-automatically generated
dataset of circa 400,000 descriptive sentences about commonsense knowledge that
can be true or false in which negation is present in about 2/3 of the corpus in
different forms. We have used our dataset with the largest available open LLMs
in a zero-shot approach to grasp their generalization and inference capability
and we have also fine-tuned some of the models to assess whether the
understanding of negation can be trained. Our findings show that, while LLMs
are proficient at classifying affirmative sentences, they struggle with
negative sentences and lack a deep understanding of negation, often relying on
superficial cues. Although fine-tuning the models on negative sentences
improves their performance, the lack of generalization in handling negation is
persistent, highlighting the ongoing challenges of LLMs regarding negation
understanding and generalization. The dataset and code are publicly available.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15943" title="Abstract">arXiv:2310.15943</a> [<a href="/pdf/2310.15943" title="Download PDF">pdf</a>, <a href="/format/2310.15943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Roadmap of Emerging Trends Discovery in Hydrology: A Topic Modeling  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korkut%2C+S+O">Sila Ovgu Korkut</a>, 
<a href="/search/cs?searchtype=author&query=Kaymak%2C+O+O">Oznur Oztunc Kaymak</a>, 
<a href="/search/cs?searchtype=author&query=Onan%2C+A">Aytug Onan</a>, 
<a href="/search/cs?searchtype=author&query=Ulker%2C+E">Erman Ulker</a>, 
<a href="/search/cs?searchtype=author&query=Yalcin%2C+F">Femin Yalcin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures, 3 Tables. This work was supported by the Center of Scientific Research Projects of the Izmir Katip Celebi University [Grant Number: 2022-GAP-MUMF-0029]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In the new global era, determining trends can play an important role in
guiding researchers, scientists, and agencies. The main faced challenge is to
track the emerging topics among the stacked publications. Therefore, any study
done to propose the trend topics in a field to foresee upcoming subjects is
crucial. In the current study, the trend topics in the field of "Hydrology"
have been attempted to evaluate. To do so, the model is composed of three key
components: a gathering of data, preprocessing of the article's significant
features, and determining trend topics. Various topic models including Latent
Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), and Latent
Semantic Analysis (LSA) have been implemented. Comparing the obtained results
with respect to the $C_V$ coherence score, in 2022, the topics of "Climate
change", "River basin", "Water management", "Natural hazards/erosion", and
"Hydrologic cycle" have been obtained. According to a further analysis, it is
shown that these topics keep their impact on the field in 2023, as well.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15946" title="Abstract">arXiv:2310.15946</a> [<a href="/pdf/2310.15946" title="Download PDF">pdf</a>, <a href="/format/2310.15946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShARc: Shape and Appearance Recognition for Person Identification  In-the-wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haidong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wanrong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhaoheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Nevatia%2C+R">Ram Nevatia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Identifying individuals in unconstrained video settings is a valuable yet
challenging task in biometric analysis due to variations in appearances,
environments, degradations, and occlusions. In this paper, we present ShARc, a
multimodal approach for video-based person identification in uncontrolled
environments that emphasizes 3-D body shape, pose, and appearance. We introduce
two encoders: a Pose and Shape Encoder (PSE) and an Aggregated Appearance
Encoder (AAE). PSE encodes the body shape via binarized silhouettes, skeleton
motions, and 3-D body shape, while AAE provides two levels of temporal
appearance feature aggregation: attention-based feature aggregation and
averaging aggregation. For attention-based feature aggregation, we employ
spatial and temporal attention to focus on key areas for person distinction.
For averaging aggregation, we introduce a novel flattening layer after
averaging to extract more distinguishable information and reduce overfitting of
attention. We utilize centroid feature averaging for gallery registration. We
demonstrate significant improvements over existing state-of-the-art methods on
public datasets, including CCVID, MEVID, and BRIAR.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15948" title="Abstract">arXiv:2310.15948</a> [<a href="/pdf/2310.15948" title="Download PDF">pdf</a>, <a href="/format/2310.15948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-driven Scene Synthesis using Multi-conditional Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vuong%2C+A">An Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+M+N">Minh Nhat Vu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+T">Toan Tien Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baoru Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dzung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+T">Thieu Vo</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene synthesis is a challenging problem with several industrial
applications. Recently, substantial efforts have been directed to synthesize
the scene using human motions, room layouts, or spatial graphs as the input.
However, few studies have addressed this problem from multiple modalities,
especially combining text prompts. In this paper, we propose a language-driven
scene synthesis task, which is a new task that integrates text prompts, human
motion, and existing objects for scene synthesis. Unlike other single-condition
synthesis tasks, our problem involves multiple conditions and requires a
strategy for processing and encoding them into a unified space. To address the
challenge, we present a multi-conditional diffusion model, which differs from
the implicit unification approach of other diffusion literature by explicitly
predicting the guiding points for the original data distribution. We
demonstrate that our approach is theoretically supportive. The intensive
experiment results illustrate that our method outperforms state-of-the-art
benchmarks and enables natural scene editing applications. The source code and
dataset can be accessed at https://lang-scene-synth.github.io/.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15950" title="Abstract">arXiv:2310.15950</a> [<a href="/pdf/2310.15950" title="Download PDF">pdf</a>, <a href="/format/2310.15950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Learning with Large Language Models for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xubin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Lixin Su</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Suqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recommender systems have seen significant advancements with the influence of
deep learning and graph neural networks, particularly in capturing complex
user-item relationships. However, these graph-based recommenders heavily depend
on ID-based data, potentially disregarding valuable textual information
associated with users and items, resulting in less informative learned
representations. Moreover, the utilization of implicit feedback data introduces
potential noise and bias, posing challenges for the effectiveness of user
preference learning. While the integration of large language models (LLMs) into
traditional ID-based recommenders has gained attention, challenges such as
scalability issues, limitations in text-only reliance, and prompt input
constraints need to be addressed for effective implementation in practical
recommender systems. To address these challenges, we propose a model-agnostic
framework RLMRec that aims to enhance existing recommenders with LLM-empowered
representation learning. It proposes a recommendation paradigm that integrates
representation learning with LLMs to capture intricate semantic aspects of user
behaviors and preferences. RLMRec incorporates auxiliary textual signals,
develops a user/item profiling paradigm empowered by LLMs, and aligns the
semantic space of LLMs with the representation space of collaborative
relational signals through a cross-view alignment framework. This work further
establish a theoretical foundation demonstrating that incorporating textual
signals through mutual information maximization enhances the quality of
representations. In our evaluation, we integrate RLMRec with state-of-the-art
recommender models, while also analyzing its efficiency and robustness to noise
data. Our implementation codes are available at
https://github.com/HKUDS/RLMRec.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15951" title="Abstract">arXiv:2310.15951</a> [<a href="/pdf/2310.15951" title="Download PDF">pdf</a>, <a href="/format/2310.15951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Distance Nearest Neighbor Condensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gottlieb%2C+L">Lee-Ad Gottlieb</a>, 
<a href="/search/cs?searchtype=author&query=Sharabi%2C+T">Timor Sharabi</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+R">Roi Weiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The problem of nearest neighbor condensing has enjoyed a long history of
study, both in its theoretical and practical aspects. In this paper, we
introduce the problem of weighted distance nearest neighbor condensing, where
one assigns weights to each point of the condensed set, and then new points are
labeled based on their weighted distance nearest neighbor in the condensed set.
<br />We study the theoretical properties of this new model, and show that it can
produce dramatically better condensing than the standard nearest neighbor rule,
yet is characterized by generalization bounds almost identical to the latter.
We then suggest a condensing heuristic for our new problem. We demonstrate
Bayes consistency for this heuristic, and also show promising empirical
results.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15952" title="Abstract">arXiv:2310.15952</a> [<a href="/pdf/2310.15952" title="Download PDF">pdf</a>, <a href="/format/2310.15952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Robustness and Reliability in Medical Image Classification  with Latent-Guided Diffusion and Nested-Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xing Shen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hengguan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nichyporuk%2C+B">Brennan Nichyporuk</a>, 
<a href="/search/cs?searchtype=author&query=Arbel%2C+T">Tal Arbel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">While deep learning models have achieved remarkable success across a range of
medical image analysis tasks, deployment of these models in real clinical
contexts requires that they be robust to variability in the acquired images.
While many methods apply predefined transformations to augment the training
data to enhance test-time robustness, these transformations may not ensure the
model's robustness to the diverse variability seen in patient images. In this
paper, we introduce a novel three-stage approach based on transformers coupled
with conditional diffusion models, with the goal of improving model robustness
to the kinds of imaging variability commonly encountered in practice without
the need for pre-determined data augmentation strategies. To this end, multiple
image encoders first learn hierarchical feature representations to build
discriminative latent spaces. Next, a reverse diffusion process, guided by the
latent code, acts on an informative prior and proposes prediction candidates in
a generative manner. Finally, several prediction candidates are aggregated in a
bi-level aggregation protocol to produce the final output. Through extensive
experiments on medical imaging benchmark datasets, we show that our method
improves upon state-of-the-art methods in terms of robustness and confidence
calibration. Additionally, we introduce a strategy to quantify the prediction
uncertainty at the instance level, increasing their trustworthiness to
clinicians using them in clinical practice.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15955" title="Abstract">arXiv:2310.15955</a> [<a href="/pdf/2310.15955" title="Download PDF">pdf</a>, <a href="/format/2310.15955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoupled DETR: Spatially Disentangling Localization and Classification  for Improved End-to-End Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Manyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+G">Guanglu Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ICCV2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The introduction of DETR represents a new paradigm for object detection.
However, its decoder conducts classification and box localization using shared
queries and cross-attention layers, leading to suboptimal results. We observe
that different regions of interest in the visual feature map are suitable for
performing query classification and box localization tasks, even for the same
object. Salient regions provide vital information for classification, while the
boundaries around them are more favorable for box regression. Unfortunately,
such spatial misalignment between these two tasks greatly hinders DETR's
training. Therefore, in this work, we focus on decoupling localization and
classification tasks in DETR. To achieve this, we introduce a new design scheme
called spatially decoupled DETR (SD-DETR), which includes a task-aware query
generation module and a disentangled feature learning process. We elaborately
design the task-aware query initialization process and divide the
cross-attention block in the decoder to allow the task-aware queries to match
different visual regions. Meanwhile, we also observe that the prediction
misalignment problem for high classification confidence and precise
localization exists, so we propose an alignment loss to further guide the
spatially decoupled DETR training. Through extensive experiments, we
demonstrate that our approach achieves a significant improvement in MSCOCO
datasets compared to previous work. For instance, we improve the performance of
Conditional DETR by 4.5 AP. By spatially disentangling the two tasks, our
method overcomes the misalignment problem and greatly improves the performance
of DETR for object detection.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15958" title="Abstract">arXiv:2310.15958</a> [<a href="/pdf/2310.15958" title="Download PDF">pdf</a>, <a href="/format/2310.15958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Review of Unscented Kalman Filter Design for Agricultural  Anaerobic Digestion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hellmann%2C+S">Simon Hellmann</a>, 
<a href="/search/eess?searchtype=author&query=Wilms%2C+T">Terrance Wilms</a>, 
<a href="/search/eess?searchtype=author&query=Streif%2C+S">Stefan Streif</a>, 
<a href="/search/eess?searchtype=author&query=Weinrich%2C+S">S&#xf6;ren Weinrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Model equations in Abstract are in preliminary state and will be added soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Dynamic operation of biological processes, such as anaerobic digestion (AD),
requires reliable process monitoring to guarantee stable operating conditions
at all times. Unscented Kalman filters (UKF) are an established tool for
nonlinear state estimation, and there exist numerous variants of UKF
implementations, treating state constraints, improvements of numerical
performance and different noise scenarios. So far, however, a unified
comparison of proposed methods emphasizing the algorithmic details is lacking.
The present study thus examines multiple unconstrained and constrained UKF
variants, addresses aspects crucial for direct implementation and applies them
to a simplified AD model. The constrained UKF considering additive noise
delivered the most accurate state estimations. The long run time of the
underlying optimization could be vastly reduced through pre-calculated
gradients and Hessian of the associated cost function, as well as by
reformulation of the cost function as a quadratic program. However,
unconstrained UKF variants showed lower run times and competitive estimation
accuracy. This study provides useful advice to practitioners working with
nonlinear Kalman filters by paying close attention to algorithmic details and
modifications crucial for successful implementation.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15959" title="Abstract">arXiv:2310.15959</a> [<a href="/pdf/2310.15959" title="Download PDF">pdf</a>, <a href="/format/2310.15959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoteChat: A Dataset of Synthetic Doctor-Patient Conversations  Conditioned on Clinical Notes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junda Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zonghai Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhichao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huixue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rumeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yucheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The detailed clinical records drafted by doctors after each patient's visit
are crucial for medical practitioners and researchers. Automating the creation
of these notes with language models can reduce the workload of doctors.
However, training such models can be difficult due to the limited public
availability of conversations between patients and doctors. In this paper, we
introduce NoteChat, a cooperative multi-agent framework leveraging Large
Language Models (LLMs) for generating synthetic doctor-patient conversations
conditioned on clinical notes. NoteChat consists of Planning, Roleplay, and
Polish modules. We provide a comprehensive automatic and human evaluation of
NoteChat, comparing it with state-of-the-art models, including OpenAI's ChatGPT
and GPT-4. Results demonstrate that NoteChat facilitates high-quality synthetic
doctor-patient conversations, underscoring the untapped potential of LLMs in
healthcare. This work represents the first instance of multiple LLMs
cooperating to complete a doctor-patient conversation conditioned on clinical
notes, offering promising avenues for the intersection of AI and healthcare
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15960" title="Abstract">arXiv:2310.15960</a> [<a href="/pdf/2310.15960" title="Download PDF">pdf</a>, <a href="/format/2310.15960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LGR-MPC: A user-friendly software based on Legendre-Gauss-Radau pseudo  spectral method for solving Model Predictive Control problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bayat%2C+S">Saeid Bayat</a>, 
<a href="/search/eess?searchtype=author&query=Allison%2C+J+T">James T.Allison</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Active components, such as actuators, constitute a fundamental aspect of
engineering systems, affording the freedom to shape system behavior as desired.
However, this capability necessitates energy consumption, primarily in the form
of electricity. Thus, a trade-off emerges between energy usage and desired
outcomes. While open-loop optimal control methods strive for efficiency,
practical implementation is hampered by disturbances and model discrepancies,
underscoring the need for closed-loop controllers. The Proportional-
Integral-Derivative (PID) controller is widely favored in industry due to its
simplicity, despite sub-optimal responses in many cases. To bridge this gap,
Model Predictive Control (MPC) offers a solution, yet its complexity limits its
broad applicability. This paper introduces user-friendly Python-based MPC
software, enabling easy access to MPC. The effectiveness of this software is
demonstrated through multiple examples, including those with a known analytical
solution.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15961" title="Abstract">arXiv:2310.15961</a> [<a href="/pdf/2310.15961" title="Download PDF">pdf</a>, <a href="/format/2310.15961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture of Tokens: Efficient LLMs through Cross-Example Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antoniak%2C+S">Szymon Antoniak</a>, 
<a href="/search/cs?searchtype=author&query=Jaszczur%2C+S">Sebastian Jaszczur</a>, 
<a href="/search/cs?searchtype=author&query=Krutul%2C+M">Micha&#x142; Krutul</a>, 
<a href="/search/cs?searchtype=author&query=Pi%C3%B3ro%2C+M">Maciej Pi&#xf3;ro</a>, 
<a href="/search/cs?searchtype=author&query=Krajewski%2C+J">Jakub Krajewski</a>, 
<a href="/search/cs?searchtype=author&query=Ludziejewski%2C+J">Jan Ludziejewski</a>, 
<a href="/search/cs?searchtype=author&query=Odrzyg%C3%B3%C5%BAd%C5%BA%2C+T">Tomasz Odrzyg&#xf3;&#x17a;d&#x17a;</a>, 
<a href="/search/cs?searchtype=author&query=Cygan%2C+M">Marek Cygan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the promise of Mixture of Experts (MoE) models in increasing
parameter counts of Transformer models while maintaining training and inference
costs, their application carries notable drawbacks. The key strategy of these
models is to, for each processed token, activate at most a few experts -
subsets of an extensive feed-forward layer. But this approach is not without
its challenges. The operation of matching experts and tokens is discrete, which
makes MoE models prone to issues like training instability and uneven expert
utilization. Existing techniques designed to address these concerns, such as
auxiliary losses or balance-aware matching, result either in lower model
performance or are more difficult to train. In response to these issues, we
propose Mixture of Tokens, a fully-differentiable model that retains the
benefits of MoE architectures while avoiding the aforementioned difficulties.
Rather than routing tokens to experts, this approach mixes tokens from
different examples prior to feeding them to experts, enabling the model to
learn from all token-expert combinations. Importantly, this mixing can be
disabled to avoid mixing of different sequences during inference. Crucially,
this method is fully compatible with both masked and causal Large Language
Model training and inference.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15970" title="Abstract">arXiv:2310.15970</a> [<a href="/pdf/2310.15970" title="Download PDF">pdf</a>, <a href="/format/2310.15970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accented Speech Recognition With Accent-specific Codebooks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prabhu%2C+D">Darshan Prabhu</a> (1), 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+P">Preethi Jyothi</a> (1), 
<a href="/search/cs?searchtype=author&query=Ganapathy%2C+S">Sriram Ganapathy</a> (2), 
<a href="/search/cs?searchtype=author&query=Unni%2C+V">Vinit Unni</a> (1) ((1) Indian Institute of Technology Bombay, Mumbai, India, (2) Indian Institute of Science, Bangalore, India)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference (Long Paper)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Speech accents pose a significant challenge to state-of-the-art automatic
speech recognition (ASR) systems. Degradation in performance across
underrepresented accents is a severe deterrent to the inclusive adoption of
ASR. In this work, we propose a novel accent adaptation approach for end-to-end
ASR systems using cross-attention with a trainable set of codebooks. These
learnable codebooks capture accent-specific information and are integrated
within the ASR encoder layers. The model is trained on accented English speech,
while the test data also contained accents which were not seen during training.
On the Mozilla Common Voice multi-accented dataset, we show that our proposed
approach yields significant performance gains not only on the seen English
accents (up to $37\%$ relative improvement in word error rate) but also on the
unseen accents (up to $5\%$ relative improvement in WER). Further, we
illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We
also compare the performance with other approaches based on accent adversarial
training.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15971" title="Abstract">arXiv:2310.15971</a> [<a href="/pdf/2310.15971" title="Download PDF">pdf</a>, <a href="/format/2310.15971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Issue Management in Runtime Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tamanna%2C+S+B">Salma Begum Tamanna</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+G">Gias Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Longyu Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CASCON 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Modern programming languages like Java require runtime systems to support the
implementation and deployment of software applications in diverse computing
platforms and operating systems. These runtime systems are normally developed
in GitHub-hosted repositories based on close collaboration between large
software companies (e.g., IBM, Microsoft) and OSS developers. However, despite
their popularity and broad usage; to the best of our knowledge, these
repositories have never been studied. We report an empirical study of around
118K issues from 34 runtime system repos in GitHub. We found that issues
regarding enhancement, test failure and bug are mostly posted on runtime system
repositories and solution related discussion are mostly present on issue
discussion. 82.69% issues in the runtime system repositories have been resolved
and 0.69% issues are ignored; median of issue close rate, ignore rate and
addressing time in these repositories are 76.1%, 2.2% and 58 days respectively.
82.65% issues are tagged with labels while only 28.30% issues have designated
assignees and 90.65% issues contain at least one comment; also presence of
these features in an issue report can affect issue closure. Based on the
findings, we offer six recommendat
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15972" title="Abstract">arXiv:2310.15972</a> [<a href="/pdf/2310.15972" title="Download PDF">pdf</a>, <a href="/format/2310.15972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Method for Realizing Contractions of Access Structures in  Cloud Storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shuai Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L+F">Liang Feng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Services Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In single-cloud storage, ciphertext-policy attribute-based encryption
(CP-ABE) allows one to encrypt any data under an access structure to a cloud
server, specifying what attributes are required to decrypt. In multi-cloud
storage, a secret sharing scheme (SSS) allows one to split any data into
multiple shares, one to a single server, and specify which subset of the
servers are able to recover the data. It is an interesting problem to remove
some attributes/servers but still enable the remaining attributes/servers in
every authorized set to recover the data. The problem is related to the
contraction problem of access structures for SSSs. In this paper, we propose a
method that can efficiently transform a given SSS for an access structure to
SSSs for contractions of the access structure. We show its applications in
solving the attribute removal problem in the CP-ABE based single-cloud storage
and the data relocating problem in multi-cloud storage. Our method results in
solutions that require either less server storage or even no additional server
storage.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15975" title="Abstract">arXiv:2310.15975</a> [<a href="/pdf/2310.15975" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven Traffic Simulation: A Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Di Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Meixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuesong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinhai Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Autonomous vehicles (AVs) have the potential to significantly revolutionize
society by providing a secure and efficient mode of transportation. Recent
years have witnessed notable advance-ments in autonomous driving perception and
prediction, but the challenge of validating the performance of AVs remains
largely unresolved. Data-driven microscopic traffic simulation has be-come an
important tool for autonomous driving testing due to 1) availability of
high-fidelity traffic data; 2) its advantages of ena-bling large-scale testing
and scenario reproducibility; and 3) its potential in reactive and realistic
traffic simulation. However, a comprehensive review of this topic is currently
lacking. This pa-per aims to fill this gap by summarizing relevant studies. The
primary objective of this paper is to review current research ef-forts and
provide a futuristic perspective that will benefit future developments in the
field. It introduces the general issues of data-driven traffic simulation and
outlines key concepts and terms. After overviewing traffic simulation, various
datasets and evalua-tion metrics commonly used are reviewed. The paper then
offers a comprehensive evaluation of imitation learning, reinforcement
learning, generative and deep learning methods, summarizing each and analyzing
their advantages and disadvantages in detail. Moreover, it evaluates the
state-of-the-art, existing challenges, and future research directions.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15976" title="Abstract">arXiv:2310.15976</a> [<a href="/pdf/2310.15976" title="Download PDF">pdf</a>, <a href="/format/2310.15976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhishuai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Pan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">signSGD is popular in nonconvex optimization due to its communication
efficiency. Yet, existing analyses of signSGD rely on assuming that data are
sampled with replacement in each iteration, contradicting the practical
implementation where data are randomly reshuffled and sequentially fed into the
algorithm. We bridge this gap by proving the first convergence result of
signSGD with random reshuffling (SignRR) for nonconvex optimization. Given the
dataset size $n$, the number of epochs of data passes $T$, and the variance
bound of a stochastic gradient $\sigma^2$, we show that SignRR has the same
convergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD
\citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, which
leverage variance-reduced gradients and momentum updates respectively, both
converging at $O(\log(nT)/\sqrt{nT})$. In contrast with the analysis of
signSGD, our results do not require an extremely large batch size in each
iteration to be of the same order as the total number of iterations
\citep{bernstein2018signsgd} or the signs of stochastic and true gradients
match element-wise with a minimum probability of 1/2
\citep{safaryan2021stochastic}. We also extend our algorithms to cases where
data are distributed across different machines, yielding dist-SignRVR and
dist-SignRVM, both converging at $O(\log(n_0T)/\sqrt{n_0T})$, where $n_0$ is
the dataset size of a single machine. We back up our theoretical findings
through experiments on simulated and real-world problems, verifying that
randomly reshuffled sign methods match or surpass existing baselines.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15977" title="Abstract">arXiv:2310.15977</a> [<a href="/pdf/2310.15977" title="Download PDF">pdf</a>, <a href="/format/2310.15977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Conspiracy Money Machine: Uncovering Telegram&#x27;s Conspiracy Channels  and their Profit Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imperati%2C+V">Vincenzo Imperati</a>, 
<a href="/search/cs?searchtype=author&query=La+Morgia%2C+M">Massimo La Morgia</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+A">Alessandro Mei</a>, 
<a href="/search/cs?searchtype=author&query=Mongardini%2C+A+M">Alberto Maria Mongardini</a>, 
<a href="/search/cs?searchtype=author&query=Sassi%2C+F">Francesco Sassi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In recent years, major social media platforms have implemented increasingly
strict moderation policies, resulting in bans and restrictions on conspiracy
theory-related content. To circumvent these restrictions, conspiracy theorists
are turning to alternatives, such as Telegram, where they can express and
spread their views with fewer limitations. Telegram offers channels -- virtual
rooms where only administrators can broadcast messages -- and a more permissive
content policy. These features have created the perfect breeding ground for a
complex ecosystem of conspiracy channels.
<br />In this paper, we illuminate this ecosystem. First, we propose an approach to
detect conspiracy channels. Then, we discover that conspiracy channels can be
clustered into four distinct communities comprising over 17,000 channels.
Finally, we uncover the "Conspiracy Money Machine," revealing how most
conspiracy channels actively seek to profit from their subscribers. We find
conspiracy theorists leverage e-commerce platforms to sell questionable
products or lucratively promote them through affiliate links. Moreover, we
observe that conspiracy channels use donation and crowdfunding platforms to
raise funds for their campaigns. We determine that this business involves
hundreds of donors and generates a turnover of over $90 million.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15978" title="Abstract">arXiv:2310.15978</a> [<a href="/pdf/2310.15978" title="Download PDF">pdf</a>, <a href="/format/2310.15978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Deep Learning for Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cini%2C+A">Andrea Cini</a>, 
<a href="/search/cs?searchtype=author&query=Marisca%2C+I">Ivan Marisca</a>, 
<a href="/search/cs?searchtype=author&query=Zambon%2C+D">Daniele Zambon</a>, 
<a href="/search/cs?searchtype=author&query=Alippi%2C+C">Cesare Alippi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph-based deep learning methods have become popular tools to process
collections of correlated time series. Differently from traditional
multivariate forecasting methods, neural graph-based predictors take advantage
of pairwise relationships by conditioning forecasts on a (possibly dynamic)
graph spanning the time series collection. The conditioning can take the form
of an architectural inductive bias on the neural forecasting architecture,
resulting in a family of deep learning models called spatiotemporal graph
neural networks. Such relational inductive biases enable the training of global
forecasting models on large time-series collections, while at the same time
localizing predictions w.r.t. each element in the set (i.e., graph nodes) by
accounting for local correlations among them (i.e., graph edges). Indeed,
recent theoretical and practical advances in graph neural networks and deep
learning for time series forecasting make the adoption of such processing
frameworks appealing and timely. However, most of the studies in the literature
focus on proposing variations of existing neural architectures by taking
advantage of modern deep learning practices, while foundational and
methodological aspects have not been subject to systematic investigation. To
fill the gap, this paper aims to introduce a comprehensive methodological
framework that formalizes the forecasting problem and provides design
principles for graph-based predictive models and methods to assess their
performance. At the same time, together with an overview of the field, we
provide design guidelines, recommendations, and best practices, as well as an
in-depth discussion of open challenges and future research directions.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15984" title="Abstract">arXiv:2310.15984</a> [<a href="/pdf/2310.15984" title="Download PDF">pdf</a>, <a href="/format/2310.15984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-Aware Video Quality Assessment for Dynamic Digital Human
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+X">Xiongkuo Min</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Dynamic Digital Humans (DDHs) are 3D digital models that are animated using
predefined motions and are inevitably bothered by noise/shift during the
generation process and compression distortion during the transmission process,
which needs to be perceptually evaluated. Usually, DDHs are displayed as 2D
rendered animation videos and it is natural to adapt video quality assessment
(VQA) methods to DDH quality assessment (DDH-QA) tasks. However, the VQA
methods are highly dependent on viewpoints and less sensitive to geometry-based
distortions. Therefore, in this paper, we propose a novel no-reference (NR)
geometry-aware video quality assessment method for DDH-QA challenge. Geometry
characteristics are described by the statistical parameters estimated from the
DDHs' geometry attribute distributions. Spatial and temporal features are
acquired from the rendered videos. Finally, all kinds of features are
integrated and regressed into quality values. Experimental results show that
the proposed method achieves state-of-the-art performance on the DDH-QA
database.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15985" title="Abstract">arXiv:2310.15985</a> [<a href="/pdf/2310.15985" title="Download PDF">pdf</a>, <a href="/format/2310.15985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+X">Xin Xing</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhexiao Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Stylianou%2C+A">Abby Stylianou</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+S">Srikumar Sastry</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+L">Liyu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Jacobs%2C+N">Nathan Jacobs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper presents a novel approach to Single-Positive Multi-label Learning.
In general multi-label learning, a model learns to predict multiple labels or
categories for a single input image. This is in contrast with standard
multi-class image classification, where the task is predicting a single label
from many possible labels for an image. Single-Positive Multi-label Learning
(SPML) specifically considers learning to predict multiple labels when there is
only a single annotation per image in the training data. Multi-label learning
is in many ways a more realistic task than single-label learning as real-world
data often involves instances belonging to multiple categories simultaneously;
however, most common computer vision datasets predominantly contain single
labels due to the inherent complexity and cost of collecting multiple high
quality annotations for each instance. We propose a novel approach called
Vision-Language Pseudo-Labeling (VLPL), which uses a vision-language model to
suggest strong positive and negative pseudo-labels, and outperforms the current
SOTA methods by 5.5% on Pascal VOC, 18.4% on MS-COCO, 15.2% on NUS-WIDE, and
8.4% on CUB-Birds. Our code and data are available at
https://github.com/mvrl/VLPL.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15987" title="Abstract">arXiv:2310.15987</a> [<a href="/pdf/2310.15987" title="Download PDF">pdf</a>, <a href="/format/2310.15987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting In-Context Learning of Translations in GPTs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raunak%2C+V">Vikas Raunak</a>, 
<a href="/search/cs?searchtype=author&query=Awadalla%2C+H+H">Hany Hassan Awadalla</a>, 
<a href="/search/cs?searchtype=author&query=Menezes%2C+A">Arul Menezes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings (+ Minor Updates over Camera-Ready)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Most of the recent work in leveraging Large Language Models (LLMs) such as
GPT-3 for Machine Translation (MT) has focused on selecting the few-shot
samples for prompting. In this work, we try to better understand the role of
demonstration attributes for the in-context learning of translations through
perturbations of high-quality, in-domain demonstrations. We find that
asymmetric perturbation of the source-target mappings yield vastly different
results. We show that the perturbation of the source side has surprisingly
little impact, while target perturbation can drastically reduce translation
quality, suggesting that it is the output text distribution that provides the
most important learning signal during in-context learning of translations. We
propose a method named Zero-Shot-Context to add this signal automatically in
Zero-Shot prompting. We demonstrate that it improves upon the zero-shot
translation performance of GPT-3, even making it competitive with few-shot
prompted translations.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15988" title="Abstract">arXiv:2310.15988</a> [<a href="/pdf/2310.15988" title="Download PDF">pdf</a>, <a href="/format/2310.15988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FabricCRDT: A Conflict-Free Replicated Datatypes Approach to  Permissioned Blockchains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasirifard%2C+P">Pezhman Nasirifard</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+R">Ruben Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Jacobsen%2C+H">Hans-Arno Jacobsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the 20th International Middleware Conference (Middleware '19). ACM 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">With the increased adaption of blockchain technologies, permissioned
blockchains such as Hyperledger Fabric provide a robust ecosystem for
developing production-grade decentralized applications. However, the additional
latency between executing and committing transactions, due to Fabric's
three-phase transaction lifecycle of Execute-Order-Validate (EOV), is a
potential scalability bottleneck. The added latency increases the probability
of concurrent updates on the same keys by different transactions, leading to
transaction failures caused by Fabric's concurrency control mechanism. The
transaction failures increase the application development complexity and
decrease Fabric's throughput. Conflict-free Replicated Datatypes (CRDTs)
provide a solution for merging and resolving conflicts in the presence of
concurrent updates. In this work, we introduce FabricCRDT, an approach for
integrating CRDTs to Fabric. Our evaluations show that in general, FabricCRDT
offers higher throughput of successful transactions than Fabric, while
successfully committing and merging all conflicting transactions without any
failures.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15990" title="Abstract">arXiv:2310.15990</a> [<a href="/pdf/2310.15990" title="Download PDF">pdf</a>, <a href="/format/2310.15990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Iterative Approach to Data-Driven Inference for Decoding Oscillator  Network Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shicheng Li</a>, 
<a href="/search/eess?searchtype=author&query=Singhal%2C+B">Bharat Singhal</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jr-Shin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 Figures, conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In complex networks, interactions between multiple agents give rise to an
array of intricate global dynamics, ranging from synchronization to cluster
formations. Decoding the connectivity structure as well as the types of
interactions from measurement data is the first step toward understanding these
intriguing behaviors. In this paper, we present a bilinear optimization
framework to infer both the connectivity and interaction functions of
oscillator networks with the identical class of coupling functions. We then
propose an iterative algorithm to solve the resulting bilinear problem and
illustrate its convergence. We validate our approach on both simulated and
noisy experimental datasets, where we demonstrate its effectiveness compared
with existing approaches.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15991" title="Abstract">arXiv:2310.15991</a> [<a href="/pdf/2310.15991" title="Download PDF">pdf</a>, <a href="/format/2310.15991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> White-box Compiler Fuzzing Empowered by Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yinlin Deng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+R">Runyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiayi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jabbarvand%2C+R">Reyhaneh Jabbarvand</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingming Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
<p class="mathjax">Compiler correctness is crucial, as miscompilation falsifying the program
behaviors can lead to serious consequences. In the literature, fuzzing has been
extensively studied to uncover compiler defects. However, compiler fuzzing
remains challenging: Existing arts focus on black- and grey-box fuzzing, which
generates tests without sufficient understanding of internal compiler
behaviors. As such, they often fail to construct programs to exercise
conditions of intricate optimizations. Meanwhile, traditional white-box
techniques are computationally inapplicable to the giant codebase of compilers.
Recent advances demonstrate that Large Language Models (LLMs) excel in code
generation/understanding tasks and have achieved state-of-the-art performance
in black-box fuzzing. Nonetheless, prompting LLMs with compiler source-code
information remains a missing piece of research in compiler testing.
<br />To this end, we propose WhiteFox, the first white-box compiler fuzzer using
LLMs with source-code information to test compiler optimization. WhiteFox
adopts a dual-model framework: (i) an analysis LLM examines the low-level
optimization source code and produces requirements on the high-level test
programs that can trigger the optimization; (ii) a generation LLM produces test
programs based on the summarized requirements. Additionally,
optimization-triggering tests are used as feedback to further enhance the test
generation on the fly. Our evaluation on four popular compilers shows that
WhiteFox can generate high-quality tests to exercise deep optimizations
requiring intricate conditions, practicing up to 80 more optimizations than
state-of-the-art fuzzers. To date, WhiteFox has found in total 96 bugs, with 80
confirmed as previously unknown and 51 already fixed. Beyond compiler testing,
WhiteFox can also be adapted for white-box fuzzing of other complex, real-world
software systems in general.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15999" title="Abstract">arXiv:2310.15999</a> [<a href="/pdf/2310.15999" title="Download PDF">pdf</a>, <a href="/format/2310.15999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transitivity Recovering Decompositions: Interpretable and Robust  Fine-Grained Relationships
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+A">Abhra Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Mancini%2C+M">Massimiliano Mancini</a>, 
<a href="/search/cs?searchtype=author&query=Akata%2C+Z">Zeynep Akata</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Anjan Dutta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neural Information Processing Systems (NeurIPS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in fine-grained representation learning leverage
local-to-global (emergent) relationships for achieving state-of-the-art
results. The relational representations relied upon by such methods, however,
are abstract. We aim to deconstruct this abstraction by expressing them as
interpretable graphs over image views. We begin by theoretically showing that
abstract relational representations are nothing but a way of recovering
transitive relationships among local views. Based on this, we design
Transitivity Recovering Decompositions (TRD), a graph-space search algorithm
that identifies interpretable equivalents of abstract emergent relationships at
both instance and class levels, and with no post-hoc computations. We
additionally show that TRD is provably robust to noisy views, with empirical
evidence also supporting this finding. The latter allows TRD to perform at par
or even better than the state-of-the-art, while being fully interpretable.
Implementation is available at https://github.com/abhrac/trd.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16002" title="Abstract">arXiv:2310.16002</a> [<a href="/pdf/2310.16002" title="Download PDF">pdf</a>, <a href="/format/2310.16002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating View Conditions for Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jinbin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+A">Aosong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+T">Tian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaicheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the field of image processing, applying intricate semantic modifications
within existing images remains an enduring challenge. This paper introduces a
pioneering framework that integrates viewpoint information to enhance the
control of image editing tasks. By surveying existing object editing
methodologies, we distill three essential criteria, consistency,
controllability, and harmony, that should be met for an image editing method.
In contrast to previous approaches, our method takes the lead in satisfying all
three requirements for addressing the challenge of image synthesis. Through
comprehensive experiments, encompassing both quantitative assessments and
qualitative comparisons with contemporary state-of-the-art methods, we present
compelling evidence of our framework's superior performance across multiple
dimensions. This work establishes a promising avenue for advancing image
synthesis techniques and empowering precise object modifications while
preserving the visual coherence of the entire composition.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16003" title="Abstract">arXiv:2310.16003</a> [<a href="/pdf/2310.16003" title="Download PDF">pdf</a>, <a href="/format/2310.16003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CVPR 2023 Text Guided Video Editing Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J+Z">Jay Zhangjie Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiuyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+D">Difei Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jinbin Bai</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Aishani Singh</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+X">Xiaoyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Youzeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zuwei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuanxi Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Rui He</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Feng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junhua Hu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hanyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>, 
<a href="/search/cs?searchtype=author&query=Iandola%2C+F">Forrest Iandola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://sites.google.com/view/loveucvpr23/track4">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Humans watch more than a billion hours of video per day. Most of this video
was edited manually, which is a tedious process. However, AI-enabled
video-generation and video-editing is on the rise. Building on text-to-image
models like Stable Diffusion and Imagen, generative AI has improved
dramatically on video tasks. But it's hard to evaluate progress in these video
tasks because there is no standard benchmark. So, we propose a new dataset for
text-guided video editing (TGVE), and we run a competition at CVPR to evaluate
models on our TGVE dataset. In this paper we present a retrospective on the
competition and describe the winning method. The competition dataset is
available at https://sites.google.com/view/loveucvpr23/track4.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16005" title="Abstract">arXiv:2310.16005</a> [<a href="/pdf/2310.16005" title="Download PDF">pdf</a>, <a href="/format/2310.16005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLFMF: Data Sets for Machine Learning for Mathematical Formalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bauer%2C+A">Andrej Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Petkovi%C4%87%2C+M">Matej Petkovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Todorovski%2C+L">Ljup&#x10d;o Todorovski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce MLFMF, a collection of data sets for benchmarking recommendation
systems used to support formalization of mathematics with proof assistants.
These systems help humans identify which previous entries (theorems,
constructions, datatypes, and postulates) are relevant in proving a new theorem
or carrying out a new construction. Each data set is derived from a library of
formalized mathematics written in proof assistants Agda or Lean. The collection
includes the largest Lean~4 library Mathlib, and some of the largest Agda
libraries: the standard library, the library of univalent mathematics
Agda-unimath, and the TypeTopology library. Each data set represents the
corresponding library in two ways: as a heterogeneous network, and as a list of
s-expressions representing the syntax trees of all the entries in the library.
The network contains the (modular) structure of the library and the references
between entries, while the s-expressions give complete and easily parsed
information about every entry. We report baseline results using standard graph
and word embeddings, tree ensembles, and instance-based learning algorithms.
The MLFMF data sets provide solid benchmarking support for further
investigation of the numerous machine learning approaches to formalized
mathematics. The methodology used to extract the networks and the s-expressions
readily applies to other libraries, and is applicable to other proof
assistants. With more than $250\,000$ entries in total, this is currently the
largest collection of formalized mathematical knowledge in machine learnable
format.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16014" title="Abstract">arXiv:2310.16014</a> [<a href="/pdf/2310.16014" title="Download PDF">pdf</a>, <a href="/format/2310.16014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-in-the-Loop Task and Motion Planning for Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandlekar%2C+A">Ajay Mandlekar</a>, 
<a href="/search/cs?searchtype=author&query=Garrett%2C+C">Caelan Garrett</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Danfei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+D">Dieter Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Robot Learning (CoRL) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Imitation learning from human demonstrations can teach robots complex
manipulation skills, but is time-consuming and labor intensive. In contrast,
Task and Motion Planning (TAMP) systems are automated and excel at solving
long-horizon tasks, but they are difficult to apply to contact-rich tasks. In
this paper, we present Human-in-the-Loop Task and Motion Planning (HITL-TAMP),
a novel system that leverages the benefits of both approaches. The system
employs a TAMP-gated control mechanism, which selectively gives and takes
control to and from a human teleoperator. This enables the human teleoperator
to manage a fleet of robots, maximizing data collection efficiency. The
collected human data is then combined with an imitation learning framework to
train a TAMP-gated policy, leading to superior performance compared to training
on full task demonstrations. We compared HITL-TAMP to a conventional
teleoperation system -- users gathered more than 3x the number of demos given
the same time budget. Furthermore, proficient agents (75\%+ success) could be
trained from just 10 minutes of non-expert teleoperation data. Finally, we
collected 2.1K demos with HITL-TAMP across 12 contact-rich, long-horizon tasks
and show that the system often produces near-perfect agents. Videos and
additional results at https://hitltamp.github.io .
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16020" title="Abstract">arXiv:2310.16020</a> [<a href="/pdf/2310.16020" title="Download PDF">pdf</a>, <a href="/format/2310.16020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConvBKI: Real-Time Probabilistic Semantic Mapping Network with  Quantifiable Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilson%2C+J">Joey Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yuewei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Friesen%2C+J">Joshua Friesen</a>, 
<a href="/search/cs?searchtype=author&query=Ewen%2C+P">Parker Ewen</a>, 
<a href="/search/cs?searchtype=author&query=Capodieci%2C+A">Andrew Capodieci</a>, 
<a href="/search/cs?searchtype=author&query=Jayakumar%2C+P">Paramsothy Jayakumar</a>, 
<a href="/search/cs?searchtype=author&query=Barton%2C+K">Kira Barton</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2209.10663">arXiv:2209.10663</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we develop a modular neural network for real-time semantic
mapping in uncertain environments, which explicitly updates per-voxel
probabilistic distributions within a neural network layer. Our approach
combines the reliability of classical probabilistic algorithms with the
performance and efficiency of modern neural networks. Although robotic
perception is often divided between modern differentiable methods and classical
explicit methods, a union of both is necessary for real-time and trustworthy
performance. We introduce a novel Convolutional Bayesian Kernel Inference
(ConvBKI) layer which incorporates semantic segmentation predictions online
into a 3D map through a depthwise convolution layer by leveraging conjugate
priors. We compare ConvBKI against state-of-the-art deep learning approaches
and probabilistic algorithms for mapping to evaluate reliability and
performance. We also create a Robot Operating System (ROS) package of ConvBKI
and test it on real-world perceptually challenging off-road driving data.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16022" title="Abstract">arXiv:2310.16022</a> [<a href="/pdf/2310.16022" title="Download PDF">pdf</a>, <a href="/ps/2310.16022" title="Download PostScript">ps</a>, <a href="/format/2310.16022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Colorful and Robust Measure for FDFAs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fisman%2C+D">Dana Fisman</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+E">Emmanuel Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Zimerman%2C+O">Oded Zimerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">We define a measure on families of DFAs (FDFAs) that we show to be robust in
the sense that two FDFAs for the same language are guaranteed to agree on this
measure. This measure tightly relates to the Wagner-Hierarchy (that defines the
complexity of omega regular languages). Inspired by the recently introduced
natural colors of infinite words, we define natural colors for finite words
(prefixes of periods of infinite words). From this semantic definition we
derive the Colorful FDFA a novel canonical model for $\omega$-regular languages
that also assigns correct colors for finite and infinite words. From the
colorful FDFA, for languages that can be recognized by deterministic B\"uchi or
coB\"uchi automata, we generate a canonical DBA or DCA termed the Black $\&amp;$
White Automaton, thus complementing the recent result on canonical good for
games coB\"uchi automata for coB\"uchi languages.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16027" title="Abstract">arXiv:2310.16027</a> [<a href="/pdf/2310.16027" title="Download PDF">pdf</a>, <a href="/format/2310.16027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimewarpVAE: Simultaneous Time-Warping and Representation Learning of  Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rhodes%2C+T">Travers Rhodes</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D+D">Daniel D. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Human demonstrations of trajectories are an important source of training data
for many machine learning problems. However, the difficulty of collecting human
demonstration data for complex tasks makes learning efficient representations
of those trajectories challenging. For many problems, such as for handwriting
or for quasistatic dexterous manipulation, the exact timings of the
trajectories should be factored from their spatial path characteristics. In
this work, we propose TimewarpVAE, a fully differentiable manifold-learning
algorithm that incorporates Dynamic Time Warping (DTW) to simultaneously learn
both timing variations and latent factors of spatial variation. We show how the
TimewarpVAE algorithm learns appropriate time alignments and meaningful
representations of spatial variations in small handwriting and fork
manipulation datasets. Our results have lower spatial reconstruction test error
than baseline approaches and the learned low-dimensional representations can be
used to efficiently generate semantically meaningful novel trajectories.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16028" title="Abstract">arXiv:2310.16028</a> [<a href="/pdf/2310.16028" title="Download PDF">pdf</a>, <a href="/format/2310.16028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Algorithms can Transformers Learn? A Study in Length Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hattie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bradley%2C+A">Arwen Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Littwin%2C+E">Etai Littwin</a>, 
<a href="/search/cs?searchtype=author&query=Razin%2C+N">Noam Razin</a>, 
<a href="/search/cs?searchtype=author&query=Saremi%2C+O">Omid Saremi</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+J">Josh Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+S">Samy Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Nakkiran%2C+P">Preetum Nakkiran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Large language models exhibit surprising emergent generalization properties,
yet also struggle on many simple reasoning tasks such as arithmetic and parity.
This raises the question of if and when Transformer models can learn the true
algorithm for solving a task. We study the scope of Transformers' abilities in
the specific setting of length generalization on algorithmic tasks. Here, we
propose a unifying framework to understand when and how Transformers can
exhibit strong length generalization on a given task. Specifically, we leverage
RASP (Weiss et al., 2021) -- a programming language designed for the
computational model of a Transformer -- and introduce the RASP-Generalization
Conjecture: Transformers tend to length generalize on a task if the task can be
solved by a short RASP program which works for all input lengths. This simple
conjecture remarkably captures most known instances of length generalization on
algorithmic tasks. Moreover, we leverage our insights to drastically improve
generalization performance on traditionally hard tasks (such as parity and
addition). On the theoretical side, we give a simple example where the
"min-degree-interpolator" model of learning from Abbe et al. (2023) does not
correctly predict Transformers' out-of-distribution behavior, but our
conjecture does. Overall, our work provides a novel perspective on the
mechanisms of compositional generalization and the algorithmic capabilities of
Transformers.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16029" title="Abstract">arXiv:2310.16029</a> [<a href="/pdf/2310.16029" title="Download PDF">pdf</a>, <a href="/format/2310.16029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finetuning Offline World Models in the Real World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yunhai Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hansen%2C+N">Nicklas Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Ziyan Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Rajagopalan%2C+C">Chandramouli Rajagopalan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoRL 2023 Oral; Project website: <a href="https://yunhaifeng.com/FOWM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">Reinforcement Learning (RL) is notoriously data-inefficient, which makes
training on a real robot difficult. While model-based RL algorithms (world
models) improve data-efficiency to some extent, they still require hours or
days of interaction to learn skills. Recently, offline RL has been proposed as
a framework for training RL policies on pre-existing datasets without any
online interaction. However, constraining an algorithm to a fixed dataset
induces a state-action distribution shift between training and inference, and
limits its applicability to new tasks. In this work, we seek to get the best of
both worlds: we consider the problem of pretraining a world model with offline
data collected on a real robot, and then finetuning the model on online data
collected by planning with the learned model. To mitigate extrapolation errors
during online interaction, we propose to regularize the planner at test-time by
balancing estimated returns and (epistemic) model uncertainty. We evaluate our
method on a variety of visuo-motor control tasks in simulation and on a real
robot, and find that our method enables few-shot finetuning to seen and unseen
tasks even when offline data is limited. Videos, code, and data are available
at https://yunhaifeng.com/FOWM .
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16033" title="Abstract">arXiv:2310.16033</a> [<a href="/pdf/2310.16033" title="Download PDF">pdf</a>, <a href="/format/2310.16033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Cropping Improves Zero-Shot Question Answering of Multimodal  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiarui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khayatkhoei%2C+M">Mahyar Khayatkhoei</a>, 
<a href="/search/cs?searchtype=author&query=Chhikara%2C+P">Prateek Chhikara</a>, 
<a href="/search/cs?searchtype=author&query=Ilievski%2C+F">Filip Ilievski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Multimodal Large Language Models (LLMs) have recently achieved promising
zero-shot accuracy on visual question answering (VQA) -- a fundamental task
affecting various downstream applications and domains. Given the great
potential for the broad use of these models, it is important to investigate
their limitations in dealing with different image and question properties. In
this work, we investigate whether multimodal LLMs can perceive small details as
well as large details in images. In particular, we show that their zero-shot
accuracy in answering visual questions is very sensitive to the size of the
visual subject of the question, declining up to $46\%$ with size. Furthermore,
we show that this effect is causal by observing that human visual cropping can
significantly mitigate their sensitivity to size. Inspired by the usefulness of
human cropping, we then propose three automatic visual cropping methods as
inference time mechanisms to improve the zero-shot performance of multimodal
LLMs. We study their effectiveness on four popular VQA datasets, and a subset
of the VQAv2 dataset tailored towards fine visual details. Our findings suggest
that multimodal LLMs should be used with caution in detail-sensitive VQA
applications, and that visual cropping is a promising direction to improve
their zero-shot performance. Our code and data are publicly available.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16035" title="Abstract">arXiv:2310.16035</a> [<a href="/pdf/2310.16035" title="Download PDF">pdf</a>, <a href="/format/2310.16035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s Left? Concept Grounding with Logic-Enhanced Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsu%2C+J">Joy Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jiayuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. First two authors contributed equally. Project page: <a href="https://web.stanford.edu/~joycj/projects/left_neurips_2023">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent works such as VisProg and ViperGPT have smartly composed foundation
models for visual reasoning-using large language models (LLMs) to produce
programs that can be executed by pre-trained vision-language models. However,
they operate in limited domains, such as 2D images, not fully exploiting the
generalization of language: abstract concepts like "left" can also be grounded
in 3D, temporal, and action data, as in moving to your left. This limited
generalization stems from these inference-only methods' inability to learn or
adapt pre-trained models to a new domain. We propose the Logic-Enhanced
Foundation Model (LEFT), a unified framework that learns to ground and reason
with concepts across domains with a differentiable, domain-independent,
first-order logic-based program executor. LEFT has an LLM interpreter that
outputs a program represented in a general, logic-based reasoning language,
which is shared across all domains and tasks. LEFT's executor then executes the
program with trainable domain-specific grounding modules. We show that LEFT
flexibly learns concepts in four domains: 2D images, 3D scenes, human motions,
and robotic manipulation. It exhibits strong reasoning ability in a wide
variety of tasks, including those that are complex and not seen during
training, and can be easily applied to new domains.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16040" title="Abstract">arXiv:2310.16040</a> [<a href="/pdf/2310.16040" title="Download PDF">pdf</a>, <a href="/format/2310.16040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruct and Extract: Instruction Tuning for On-Demand Information  Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yizhu Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+M">Ming Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sha Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruining Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+S">Siru Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models with instruction-following capabilities open the door
to a wider group of users. However, when it comes to information extraction - a
classic task in natural language processing - most task-specific systems cannot
align well with long-tail ad hoc extraction use cases for non-expert users. To
address this, we propose a novel paradigm, termed On-Demand Information
Extraction, to fulfill the personalized demands of real-world users. Our task
aims to follow the instructions to extract the desired content from the
associated text and present it in a structured tabular format. The table
headers can either be user-specified or inferred contextually by the model. To
facilitate research in this emerging area, we present a benchmark named
InstructIE, inclusive of both automatically generated training data, as well as
the human-annotated test set. Building on InstructIE, we further develop an
On-Demand Information Extractor, ODIE. Comprehensive evaluations on our
benchmark reveal that ODIE substantially outperforms the existing open-source
models of similar size. Our code and dataset are released on
https://github.com/yzjiao/On-Demand-IE.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16042" title="Abstract">arXiv:2310.16042</a> [<a href="/pdf/2310.16042" title="Download PDF">pdf</a>, <a href="/format/2310.16042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WebWISE: Web Interface Control and Sequential Exploration with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+H">Heyi Tao</a>, 
<a href="/search/cs?searchtype=author&query=V%2C+S+T">Sethuraman T V</a>, 
<a href="/search/cs?searchtype=author&query=Shlapentokh-Rothman%2C+M">Michal Shlapentokh-Rothman</a>, 
<a href="/search/cs?searchtype=author&query=Hoiem%2C+D">Derek Hoiem</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The paper investigates using a Large Language Model (LLM) to automatically
perform web software tasks using click, scroll, and text input operations.
Previous approaches, such as reinforcement learning (RL) or imitation learning,
are inefficient to train and task-specific. Our method uses filtered Document
Object Model (DOM) elements as observations and performs tasks step-by-step,
sequentially generating small programs based on the current observations. We
use in-context learning, either benefiting from a single manually provided
example, or an automatically generated example based on a successful zero-shot
trial. We evaluate the proposed method on the MiniWob++ benchmark. With only
one in-context example, our WebWISE method achieves similar or better
performance than other methods that require many demonstrations or trials.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16044" title="Abstract">arXiv:2310.16044</a> [<a href="/pdf/2310.16044" title="Download PDF">pdf</a>, <a href="/format/2310.16044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Z">Zhengfei Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunzhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong-Xing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Agarwala%2C+S">Samir Agarwala</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shangzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equality to this work. NeurIPS 2023 Datasets and Benchmarks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering
Benchmark. Recent advances in inverse rendering have enabled a wide range of
real-world applications in 3D content generation, moving rapidly from research
and commercial use cases to consumer devices. While the results continue to
improve, there is no real-world benchmark that can quantitatively assess and
compare the performance of various inverse rendering methods. Existing
real-world datasets typically only consist of the shape and multi-view images
of objects, which are not sufficient for evaluating the quality of material
recovery and object relighting. Methods capable of recovering material and
lighting often resort to synthetic data for quantitative evaluation, which on
the other hand does not guarantee generalization to complex real-world
environments. We introduce a new dataset of real-world objects captured under a
variety of natural scenes with ground-truth 3D scans, multi-view images, and
environment lighting. Using this dataset, we establish the first comprehensive
real-world evaluation benchmark for object inverse rendering tasks from
in-the-wild scenes, and compare the performance of various existing methods.
All data, code, and models can be accessed at https://stanfordorb.github.io/.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16045" title="Abstract">arXiv:2310.16045</a> [<a href="/pdf/2310.16045" title="Download PDF">pdf</a>, <a href="/format/2310.16045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Woodpecker: Hallucination Correction for Multimodal Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Shukang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chaoyou Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sirui Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+D">Dianbo Sui</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yunhang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Ke Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures. Code Website: <a href="https://github.com/BradyFU/Woodpecker">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Hallucination is a big shadow hanging over the rapidly evolving Multimodal
Large Language Models (MLLMs), referring to the phenomenon that the generated
text is inconsistent with the image content. In order to mitigate
hallucinations, existing studies mainly resort to an instruction-tuning manner
that requires retraining the models with specific data. In this paper, we pave
a different way, introducing a training-free method named Woodpecker. Like a
woodpecker heals trees, it picks out and corrects hallucinations from the
generated text. Concretely, Woodpecker consists of five stages: key concept
extraction, question formulation, visual knowledge validation, visual claim
generation, and hallucination correction. Implemented in a post-remedy manner,
Woodpecker can easily serve different MLLMs, while being interpretable by
accessing intermediate outputs of the five stages. We evaluate Woodpecker both
quantitatively and qualitatively and show the huge potential of this new
paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement
in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released
at https://github.com/BradyFU/Woodpecker.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16046" title="Abstract">arXiv:2310.16046</a> [<a href="/pdf/2310.16046" title="Download PDF">pdf</a>, <a href="/format/2310.16046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified, Scalable Framework for Neural Population Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azabou%2C+M">Mehdi Azabou</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+V">Vinam Arora</a>, 
<a href="/search/cs?searchtype=author&query=Ganesh%2C+V">Venkataramana Ganesh</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Ximeng Mao</a>, 
<a href="/search/cs?searchtype=author&query=Nachimuthu%2C+S">Santosh Nachimuthu</a>, 
<a href="/search/cs?searchtype=author&query=Mendelson%2C+M+J">Michael J. Mendelson</a>, 
<a href="/search/cs?searchtype=author&query=Richards%2C+B">Blake Richards</a>, 
<a href="/search/cs?searchtype=author&query=Perich%2C+M+G">Matthew G. Perich</a>, 
<a href="/search/cs?searchtype=author&query=Lajoie%2C+G">Guillaume Lajoie</a>, 
<a href="/search/cs?searchtype=author&query=Dyer%2C+E+L">Eva L. Dyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Our ability to use deep learning approaches to decipher neural activity would
likely benefit from greater scale, in terms of both model size and datasets.
However, the integration of many neural recordings into one unified model is
challenging, as each recording contains the activity of different neurons from
different individual animals. In this paper, we introduce a training framework
and architecture designed to model the population dynamics of neural activity
across diverse, large-scale neural recordings. Our method first tokenizes
individual spikes within the dataset to build an efficient representation of
neural events that captures the fine temporal structure of neural activity. We
then employ cross-attention and a PerceiverIO backbone to further construct a
latent tokenization of neural population activities. Utilizing this
architecture and training framework, we construct a large-scale multi-session
model trained on large datasets from seven nonhuman primates, spanning over 158
different sessions of recording from over 27,373 neural units and over 100
hours of recordings. In a number of different tasks, we demonstrate that our
pretrained model can be rapidly adapted to new, unseen sessions with
unspecified neuron correspondence, enabling few-shot performance with minimal
labels. This work presents a powerful new approach for building deep learning
tools to analyze neural data and stakes out a clear path to training at scale.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16047" title="Abstract">arXiv:2310.16047</a> [<a href="/pdf/2310.16047" title="Download PDF">pdf</a>, <a href="/format/2310.16047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Posterior Sampling to Meaningful Diversity in Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Noa Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Manor%2C+H">Hila Manor</a>, 
<a href="/search/cs?searchtype=author&query=Bahat%2C+Y">Yuval Bahat</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and examples are available in <a href="https://noa-cohen.github.io/MeaningfulDiversityInIR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Image restoration problems are typically ill-posed in the sense that each
degraded image can be restored in infinitely many valid ways. To accommodate
this, many works generate a diverse set of outputs by attempting to randomly
sample from the posterior distribution of natural images given the degraded
input. Here we argue that this strategy is commonly of limited practical value
because of the heavy tail of the posterior distribution. Consider for example
inpainting a missing region of the sky in an image. Since there is a high
probability that the missing region contains no object but clouds, any set of
samples from the posterior would be entirely dominated by (practically
identical) completions of sky. However, arguably, presenting users with only
one clear sky completion, along with several alternative solutions such as
airships, birds, and balloons, would better outline the set of possibilities.
In this paper, we initiate the study of meaningfully diverse image restoration.
We explore several post-processing approaches that can be combined with any
diverse image restoration method to yield semantically meaningful diversity.
Moreover, we propose a practical approach for allowing diffusion based image
restoration methods to generate meaningfully diverse outputs, while incurring
only negligent computational overhead. We conduct extensive user studies to
analyze the proposed techniques, and find the strategy of reducing similarity
between outputs to be significantly favorable over posterior sampling. Code and
examples are available in https://noa-cohen.github.io/MeaningfulDiversityInIR
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16048" title="Abstract">arXiv:2310.16048</a> [<a href="/pdf/2310.16048" title="Download PDF">pdf</a>, <a href="/ps/2310.16048" title="Download PostScript">ps</a>, <a href="/format/2310.16048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Alignment and Social Choice: Fundamental Limitations and Policy  Implications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+A">Abhilash Mishra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Aligning AI agents to human intentions and values is a key bottleneck in
building safe and deployable AI applications. But whose values should AI agents
be aligned with? Reinforcement learning with human feedback (RLHF) has emerged
as the key framework for AI alignment. RLHF uses feedback from human
reinforcers to fine-tune outputs; all widely deployed large language models
(LLMs) use RLHF to align their outputs to human values. It is critical to
understand the limitations of RLHF and consider policy challenges arising from
these limitations. In this paper, we investigate a specific challenge in
building RLHF systems that respect democratic norms. Building on impossibility
results in social choice theory, we show that, under fairly broad assumptions,
there is no unique voting protocol to universally align AI systems using RLHF
through democratic processes. Further, we show that aligning AI agents with the
values of all individuals will always violate certain private ethical
preferences of an individual user i.e., universal AI alignment using RLHF is
impossible. We discuss policy implications for the governance of AI systems
built using RLHF: first, the need for mandating transparent voting rules to
hold model builders accountable. Second, the need for model builders to focus
on developing AI agents that are narrowly aligned to specific user groups.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16049" title="Abstract">arXiv:2310.16049</a> [<a href="/pdf/2310.16049" title="Download PDF">pdf</a>, <a href="/format/2310.16049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MuSR: Testing the Limits of Chain-of-thought with Multistep Soft  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sprague%2C+Z">Zayne Sprague</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Bostrom%2C+K">Kaj Bostrom</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+S">Swarat Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Durrett%2C+G">Greg Durrett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LLMs) equipped with techniques like
chain-of-thought prompting have demonstrated impressive capabilities, they
still fall short in their ability to reason robustly in complex settings.
However, evaluating LLM reasoning is challenging because system capabilities
continue to grow while benchmark datasets for tasks like logical deduction have
remained static. We introduce MuSR, a dataset for evaluating language models on
multistep soft reasoning tasks specified in a natural language narrative. This
dataset has two crucial features. First, it is created through a novel
neurosymbolic synthetic-to-natural generation algorithm, enabling the
construction of complex reasoning instances that challenge GPT-4 (e.g., murder
mysteries roughly 1000 words in length) and which can be scaled further as more
capable LLMs are released. Second, our dataset instances are free text
narratives corresponding to real-world domains of reasoning; this makes it
simultaneously much more challenging than other synthetically-crafted
benchmarks while remaining realistic and tractable for human annotators to
solve with high accuracy. We evaluate a range of LLMs and prompting techniques
on this dataset and characterize the gaps that remain for techniques like
chain-of-thought to perform robust reasoning.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16050" title="Abstract">arXiv:2310.16050</a> [<a href="/pdf/2310.16050" title="Download PDF">pdf</a>, <a href="/format/2310.16050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EquivAct: SIM(3)-Equivariant Visuomotor Policies beyond Rigid Object  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingyun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Congyue Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jimmy Wu</a>, 
<a href="/search/cs?searchtype=author&query=Antonova%2C+R">Rika Antonova</a>, 
<a href="/search/cs?searchtype=author&query=Guibas%2C+L">Leonidas Guibas</a>, 
<a href="/search/cs?searchtype=author&query=Bohg%2C+J">Jeannette Bohg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">If a robot masters folding a kitchen towel, we would also expect it to master
folding a beach towel. However, existing works for policy learning that rely on
data set augmentations are still limited in achieving this level of
generalization. Our insight is to add equivariance to both the visual object
representation and policy architecture. We propose EquivAct which utilizes
SIM(3)-equivariant network structures that guarantee generalization across all
possible object translations, 3D rotations, and scales by construction.
Training of EquivAct is done in two phases. We first pre-train a
SIM(3)-equivariant visual representation on simulated scene point clouds. Then,
we learn a SIM(3)-equivariant visuomotor policy on top of the pre-trained
visual representation using a small amount of source task demonstrations. We
demonstrate that after training, the learned policy directly transfers to
objects that substantially differ in scale, position and orientation from the
source demonstrations. In simulation, we evaluate our method in three
manipulation tasks involving deformable and articulated objects thereby going
beyond the typical rigid object manipulation tasks that prior works considered.
We show that our method outperforms prior works that do not use equivariant
architectures or do not use our contrastive pre-training procedure. We also
show quantitative and qualitative experiments on three real robot tasks, where
the robot watches twenty demonstrations of a tabletop task and transfers
zero-shot to a mobile manipulation task in a much larger setup. Project
website: https://equivact.github.io
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16052" title="Abstract">arXiv:2310.16052</a> [<a href="/pdf/2310.16052" title="Download PDF">pdf</a>, <a href="/format/2310.16052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic Data as Validation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qixin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zongwei Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study leverages synthetic data as a validation set to reduce overfitting
and ease the selection of the best model in AI development. While synthetic
data have been used for augmenting the training set, we find that synthetic
data can also significantly diversify the validation set, offering marked
advantages in domains like healthcare, where data are typically limited,
sensitive, and from out-domain sources (i.e., hospitals). In this study, we
illustrate the effectiveness of synthetic data for early cancer detection in
computed tomography (CT) volumes, where synthetic tumors are generated and
superimposed onto healthy organs, thereby creating an extensive dataset for
rigorous validation. Using synthetic data as validation can improve AI
robustness in both in-domain and out-domain test sets. Furthermore, we
establish a new continual learning framework that continuously trains AI models
on a stream of out-domain data with synthetic tumors. The AI model trained and
validated in dynamically expanding synthetic data can consistently outperform
models trained and validated exclusively on real-world data. Specifically, the
DSC score for liver tumor segmentation improves from 26.7% (95% CI:
22.6%-30.9%) to 34.5% (30.8%-38.2%) when evaluated on an in-domain dataset and
from 31.1% (26.0%-36.2%) to 35.4% (32.1%-38.7%) on an out-domain dataset.
Importantly, the performance gain is particularly significant in identifying
very tiny liver tumors (radius &lt; 5mm) in CT volumes, with Sensitivity improving
from 33.1% to 55.4% on an in-domain dataset and 33.9% to 52.3% on an out-domain
dataset, justifying the efficacy in early detection of cancer. The application
of synthetic data, from both training and validation perspectives, underlines a
promising avenue to enhance AI robustness when dealing with data from varying
domains.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Wed, 25 Oct 23</h3>
<dl>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15092" title="Abstract">arXiv:2310.15092</a> (cross-list from quant-ph) [<a href="/pdf/2310.15092" title="Download PDF">pdf</a>, <a href="/ps/2310.15092" title="Download PostScript">ps</a>, <a href="/format/2310.15092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dihedral Quantum Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Borello%2C+M">Martino Borello</a>, 
<a href="/search/quant-ph?searchtype=author&query=Horlemann%2C+A">Anna-Lena Horlemann</a>, 
<a href="/search/quant-ph?searchtype=author&query=Islam%2C+H">Habibul Islam</a>, 
<a href="/search/quant-ph?searchtype=author&query=Willenborg%2C+N">Nadja Willenborg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
<p class="mathjax">We study dihedral quantum codes of short block length, a large class of
quantum CSS codes obtained by the lifted product construction. We present code
construction and give a formula for the code dimension, depending on the two
classical codes on which the CSS code is based on. We also give a lower bound
on the code distance. Finally we construct an example of short dihedral quantum
codes, improving parameters of previously known quantum codes.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15177" title="Abstract">arXiv:2310.15177</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.15177" title="Download PDF">pdf</a>, <a href="/format/2310.15177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian  Learning and Free Energy Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Ororbia%2C+A">Alexander Ororbia</a>, 
<a href="/search/q-bio?searchtype=author&query=Kelly%2C+M+A">Mary Alexandria Kelly</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted draft to 2023 AAAI Fall Symposium on Integration of Cognitive Architectures and Generative Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Over the last few years, large neural generative models, capable of
synthesizing intricate sequences of words or producing complex image patterns,
have recently emerged as a popular representation of what has come to be known
as "generative artificial intelligence" (generative AI). Beyond opening the
door to new opportunities as well as challenges for the domain of statistical
machine learning, the rising popularity of generative AI brings with it
interesting questions for Cognitive Science, which seeks to discover the nature
of the processes that underpin minds and brains as well as to understand how
such functionality might be acquired and instantiated in biological (or
artificial) substrate. With this goal in mind, we argue that a promising
long-term pathway lies in the crafting of cognitive architectures, a
long-standing tradition of the field, cast fundamentally in terms of
neuro-mimetic generative building blocks. Concretely, we discuss the COGnitive
Neural GENerative system, which is an architecture that casts the Common Model
of Cognition in terms of Hebbian adaptation operating in service of optimizing
a variational free energy functional.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15179" title="Abstract">arXiv:2310.15179</a> (cross-list from physics.ao-ph) [<a href="/pdf/2310.15179" title="Download PDF">pdf</a>, <a href="/format/2310.15179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Uncertainty in Sea-level Rise Prediction: A  Spatial-variability-aware Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ghosh%2C+S">Subhankar Ghosh</a>, 
<a href="/search/physics?searchtype=author&query=An%2C+S">Shuai An</a>, 
<a href="/search/physics?searchtype=author&query=Sharma%2C+A">Arun Sharma</a>, 
<a href="/search/physics?searchtype=author&query=Gupta%2C+J">Jayant Gupta</a>, 
<a href="/search/physics?searchtype=author&query=Shekhar%2C+S">Shashi Shekhar</a>, 
<a href="/search/physics?searchtype=author&query=Subramanian%2C+A">Aneesh Subramanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, I-GUIDE 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Dynamical Systems (math.DS); Other Statistics (stat.OT)

</div>
<p class="mathjax">Given multi-model ensemble climate projections, the goal is to accurately and
reliably predict future sea-level rise while lowering the uncertainty. This
problem is important because sea-level rise affects millions of people in
coastal communities and beyond due to climate change's impacts on polar ice
sheets and the ocean. This problem is challenging due to spatial variability
and unknowns such as possible tipping points (e.g., collapse of Greenland or
West Antarctic ice-shelf), climate feedback loops (e.g., clouds, permafrost
thawing), future policy decisions, and human actions. Most existing climate
modeling approaches use the same set of weights globally, during either
regression or deep learning to combine different climate projections. Such
approaches are inadequate when different regions require different weighting
schemes for accurate and reliable sea-level rise predictions. This paper
proposes a zonal regression model which addresses spatial variability and model
inter-dependency. Experimental results show more reliable predictions using the
weights learned via this approach on a regional scale.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15194" title="Abstract">arXiv:2310.15194</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.15194" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do the resting EEG preprocessing states affect the outcomes of  postprocessing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Hu%2C+S">Shiang Hu</a>, 
<a href="/search/q-bio?searchtype=author&query=Ruan%2C+J">Jie Ruan</a>, 
<a href="/search/q-bio?searchtype=author&query=Hou%2C+J">Juan Hou</a>, 
<a href="/search/q-bio?searchtype=author&query=Valdes-Sosa%2C+P+A">Pedro Antonio Valdes-Sosa</a>, 
<a href="/search/q-bio?searchtype=author&query=Lv%2C+Z">Zhao Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Human-Computer Interaction (cs.HC); Signal Processing (eess.SP); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Plenty of artifact removal tools and pipelines have been developed to correct
the EEG recordings and discover the values below the waveforms. Without visual
inspection from the experts, it is susceptible to derive improper preprocessing
states, like the insufficient preprocessed EEG (IPE), and the excessive
preprocessed EEG (EPE). However, little is known about the impacts of IPE or
EPE on the postprocessing in the frequency, spatial and temporal domains,
particularly as to the spectra and the functional connectivity (FC) analysis.
Here, the clean EEG (CE) was synthesized as the ground truth based on the
New-York head model and the multivariate autoregressive model. Later, the IPE
and the EPE were simulated by injecting the Gaussian noise and losing the brain
activities, respectively. Then, the impacts on postprocessing were quantified
by the deviation caused by the IPE or EPE from the CE as to the 4 temporal
statistics, the multichannel power, the cross spectra, the dispersion of source
imaging, and the properties of scalp EEG network. Lastly, the association
analysis was performed between the PaLOSi metric and the varying trends of
postprocessing with the evolution of preprocessing states. This study shed
light on how the postprocessing outcomes are affected by the preprocessing
states and PaLOSi may be a potential effective quality metric.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15201" title="Abstract">arXiv:2310.15201</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.15201" title="Download PDF">pdf</a>, <a href="/format/2310.15201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Skin Microbiome Model with AMP interactions and Analysis of  Quasi-Stability vs Stability in Population Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Greugny%2C+E+T">El&#xe9;a Thibault Greugny</a> (Lifeware), 
<a href="/search/q-bio?searchtype=author&query=Fages%2C+F">Fran&#xe7;ois Fages</a> (Lifeware), 
<a href="/search/q-bio?searchtype=author&query=Radulescu%2C+O">Ovidiu Radulescu</a> (UM), 
<a href="/search/q-bio?searchtype=author&query=Szmolyan%2C+P">Peter Szmolyan</a>, 
<a href="/search/q-bio?searchtype=author&query=Stamatas%2C+G">Georgios Stamatas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2206.10221">arXiv:2206.10221</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Tissues and Organs (q-bio.TO)

</div>
<p class="mathjax">The skin microbiome plays an important role in the maintenance of a healthy
skin. It is an ecosystem, composed of several species, competing for resources
and interacting with the skin cells. Imbalance in the cutaneous microbiome,
also called dysbiosis, has been correlated with several skin conditions,
including acne and atopic dermatitis. Generally, dysbiosis is linked to
colonization of the skin by a population of opportunistic pathogenic bacteria.
Treatments consisting in non-specific elimination of cutaneous microflora have
shown conflicting results. In this article, we introduce a mathematical model
based on ordinary differential equations, with 2 types of bacteria populations
(skin commensals and opportunistic pathogens) and including the production of
antimicrobial peptides to study the mechanisms driving the dominance of one
population over the other. By using published experimental data, assumed to
correspond to the observation of stable states in our model, we reduce the
number of parameters of the model from 13 to 5. We then use a formal
specification in quantitative temporal logic to calibrate our model by global
parameter optimization and perform sensitivity analyses. On the time scale of 2
days of the experiments, the model predicts that certain changes of the
environment, like the elevation of skin surface pH, create favorable conditions
for the emergence and colonization of the skin by the opportunistic pathogen
population, while the production of human AMPs has non-linear effect on the
balance between pathogens and commensals. Surprisingly, simulations on longer
time scales reveal that the equilibrium reached around 2 days can in fact be a
quasi-stable state followed by the reaching of a reversed stable state after 12
days or more. We analyse the conditions of quasi-stability observed in this
model using tropical algebraic methods, and show their non-generic character in
contrast to slow-fast systems. These conditions are then generalized to a large
class of population dynamics models over any number of species.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15202" title="Abstract">arXiv:2310.15202</a> (cross-list from q-bio.GN) [<a href="/pdf/2310.15202" title="Download PDF">pdf</a>, <a href="/format/2310.15202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Transcription Factor Binding Sites using Transformer based  Capsule Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Ghosh%2C+N">Nimisha Ghosh</a>, 
<a href="/search/q-bio?searchtype=author&query=Santoni%2C+D">Daniele Santoni</a>, 
<a href="/search/q-bio?searchtype=author&query=Saha%2C+I">Indrajit Saha</a>, 
<a href="/search/q-bio?searchtype=author&query=Felici%2C+G">Giovanni Felici</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prediction of binding sites for transcription factors is important to
understand how they regulate gene expression and how this regulation can be
modulated for therapeutic purposes. Although in the past few years there are
significant works addressing this issue, there is still space for improvement.
In this regard, a transformer based capsule network viz. DNABERT-Cap is
proposed in this work to predict transcription factor binding sites mining
ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with
large number of genomic DNA sequences, empowered with a capsule layer
responsible for the final prediction. The proposed model builds a predictor for
transcription factor binding sites using the joint optimisation of features
encompassing both bidirectional encoder and capsule layer, along with
convolutional and bidirectional long-short term memory layers. To evaluate the
efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of
five cell lines viz. A549, GM12878, Hep-G2, H1-hESC and Hela, available in the
ENCODE repository. The results show that the average area under the receiver
operating characteristic curve score exceeds 0.91 for all such five cell lines.
DNABERT-Cap is also compared with existing state-of-the-art deep learning based
predictors viz. DeepARC, DeepTF, CNN-Zeng and DeepBind, and is seen to
outperform them.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15211" title="Abstract">arXiv:2310.15211</a> (cross-list from q-bio.QM) [<a href="/pdf/2310.15211" title="Download PDF">pdf</a>, <a href="/format/2310.15211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Path Importance for Effective Alzheimer&#x27;s Disease Drug  Repurposing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Xiang%2C+S">Shunian Xiang</a>, 
<a href="/search/q-bio?searchtype=author&query=Lawrence%2C+P+J">Patrick J. Lawrence</a>, 
<a href="/search/q-bio?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/q-bio?searchtype=author&query=PhD%2C+C+C">ChienWei Chiang PhD</a>, 
<a href="/search/q-bio?searchtype=author&query=PhD%2C+D+K">Dokyoon Kim PhD</a>, 
<a href="/search/q-bio?searchtype=author&query=PhD%2C+L+S">Li Shen PhD</a>, 
<a href="/search/q-bio?searchtype=author&query=Ning%2C+X">Xia Ning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 2 tables, 1 supplementary figure, 5 supplementary tables, Preprint of an article accepted for publication in Pacific Symposium on Biocomputing \c{opyright} 2023 World Scientific Publishing Co., Singapore, <a href="http://psb.stanford.edu/">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Molecular Networks (q-bio.MN)

</div>
<p class="mathjax">Recently, drug repurposing has emerged as an effective and resource-efficient
paradigm for AD drug discovery. Among various methods for drug repurposing,
network-based methods have shown promising results as they are capable of
leveraging complex networks that integrate multiple interaction types, such as
protein-protein interactions, to more effectively identify candidate drugs.
However, existing approaches typically assume paths of the same length in the
network have equal importance in identifying the therapeutic effect of drugs.
Other domains have found that same length paths do not necessarily have the
same importance. Thus, relying on this assumption may be deleterious to drug
repurposing attempts. In this work, we propose MPI (Modeling Path Importance),
a novel network-based method for AD drug repurposing. MPI is unique in that it
prioritizes important paths via learned node embeddings, which can effectively
capture a network's rich structural information. Thus, leveraging learned
embeddings allows MPI to effectively differentiate the importance among paths.
We evaluate MPI against a commonly used baseline method that identifies anti-AD
drug candidates primarily based on the shortest paths between drugs and AD in
the network. We observe that among the top-50 ranked drugs, MPI prioritizes
20.0% more drugs with anti-AD evidence compared to the baseline. Finally, Cox
proportional-hazard models produced from insurance claims data aid us in
identifying the use of etodolac, nicotine, and BBB-crossing ACE-INHs as having
a reduced risk of AD, suggesting such drugs may be viable candidates for
repurposing and should be explored further in future studies.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15233" title="Abstract">arXiv:2310.15233</a> (cross-list from gr-qc) [<a href="/pdf/2310.15233" title="Download PDF">pdf</a>, <a href="/format/2310.15233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new approach to template banks of gravitational waves with higher  harmonics: reducing matched-filtering cost by over an order of magnitude
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/gr-qc?searchtype=author&query=Wadekar%2C+D">Digvijay Wadekar</a>, 
<a href="/search/gr-qc?searchtype=author&query=Venumadhav%2C+T">Tejaswi Venumadhav</a>, 
<a href="/search/gr-qc?searchtype=author&query=Mehta%2C+A+K">Ajit Kumar Mehta</a>, 
<a href="/search/gr-qc?searchtype=author&query=Roulet%2C+J">Javier Roulet</a>, 
<a href="/search/gr-qc?searchtype=author&query=Olsen%2C+S">Seth Olsen</a>, 
<a href="/search/gr-qc?searchtype=author&query=Mushkin%2C+J">Jonathan Mushkin</a>, 
<a href="/search/gr-qc?searchtype=author&query=Zackay%2C+B">Barak Zackay</a>, 
<a href="/search/gr-qc?searchtype=author&query=Zaldarriaga%2C+M">Matias Zaldarriaga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12+2 pages, 7+1 figures. The template bank described here will be publicly available at <a href="https://github.com/JayWadekar/GW_higher_harmonics_search">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Relativity and Quantum Cosmology (gr-qc)</span>; High Energy Astrophysical Phenomena (astro-ph.HE); Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Searches for gravitational wave events use models, or templates, for the
signals of interest. The templates used in current searches in the
LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$
of the signals, and omit sub-dominant higher-order modes (HM) such as
$(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence,
these searches could lose sensitivity to black hole mergers in interesting
parts of parameter space, such as systems with high-masses and asymmetric mass
ratios. We develop a new strategy to include HM in template banks that exploits
the natural connection between the modes. We use a combination of
post-Newtonian formulae and machine learning tools to model aligned-spin
$(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of
these modes can be individually filtered against the data to yield separate
timeseries of signal-to-noise ratios (SNR), which can be combined in a
relatively inexpensive way to marginalize over extrinsic parameters of the
signals. This leads to a HM search pipeline whose matched-filtering cost is
just $\approx 3\times$ that of a quadrupole-only search (in contrast to being
$\approx\! 100 \times$, as in previously proposed HM search methods). Our
method is effectual and is generally applicable for template banks constructed
with either stochastic or geometric placement techniques. Additionally, we
discuss compression of $(2,2)$-only geometric-placement template banks using
machine learning algorithms.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15234" title="Abstract">arXiv:2310.15234</a> (cross-list from astro-ph.CO) [<a href="/pdf/2310.15234" title="Download PDF">pdf</a>, <a href="/format/2310.15234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Field-level simulation-based inference with galaxy catalogs: the impact  of systematic effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=de+Santi%2C+N+S+M">Natal&#xed; S. M. de Santi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Villaescusa-Navarro%2C+F">Francisco Villaescusa-Navarro</a>, 
<a href="/search/astro-ph?searchtype=author&query=Abramo%2C+L+R">L. Raul Abramo</a>, 
<a href="/search/astro-ph?searchtype=author&query=Shao%2C+H">Helen Shao</a>, 
<a href="/search/astro-ph?searchtype=author&query=Perez%2C+L+A">Lucia A. Perez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Castro%2C+T">Tiago Castro</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ni%2C+Y">Yueying Ni</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lovell%2C+C+C">Christopher C. Lovell</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hernandez-Martinez%2C+E">Elena Hernandez-Martinez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Marinacci%2C+F">Federico Marinacci</a>, 
<a href="/search/astro-ph?searchtype=author&query=Spergel%2C+D+N">David N. Spergel</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dolag%2C+K">Klaus Dolag</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hernquist%2C+L">Lars Hernquist</a>, 
<a href="/search/astro-ph?searchtype=author&query=Vogelsberger%2C+M">Mark Vogelsberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 11 figures. For the reference in the abstract (de Santi et al. 2023) see <a href="/abs/2302.14101">arXiv:2302.14101</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Astrophysics of Galaxies (astro-ph.GA); Machine Learning (cs.LG)

</div>
<p class="mathjax">It has been recently shown that a powerful way to constrain cosmological
parameters from galaxy redshift surveys is to train graph neural networks to
perform field-level likelihood-free inference without imposing cuts on scale.
In particular, de Santi et al. (2023) developed models that could accurately
infer the value of $\Omega_{\rm m}$ from catalogs that only contain the
positions and radial velocities of galaxies that are robust to uncertainties in
astrophysics and subgrid models. However, observations are affected by many
effects, including 1) masking, 2) uncertainties in peculiar velocities and
radial distances, and 3) different galaxy selections. Moreover, observations
only allow us to measure redshift, intertwining galaxies' radial positions and
velocities. In this paper we train and test our models on galaxy catalogs,
created from thousands of state-of-the-art hydrodynamic simulations run with
different codes from the CAMELS project, that incorporate these observational
effects. We find that, although the presence of these effects degrades the
precision and accuracy of the models, and increases the fraction of catalogs
where the model breaks down, the fraction of galaxy catalogs where the model
performs well is over 90 %, demonstrating the potential of these models to
constrain cosmological parameters even when applied to real data.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15256" title="Abstract">arXiv:2310.15256</a> (cross-list from astro-ph.CO) [<a href="/pdf/2310.15256" title="Download PDF">pdf</a>, <a href="/format/2310.15256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimBIG: Field-level Simulation-Based Inference of Galaxy Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Lemos%2C+P">Pablo Lemos</a>, 
<a href="/search/astro-ph?searchtype=author&query=Parker%2C+L">Liam Parker</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hahn%2C+C">ChangHoon Hahn</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ho%2C+S">Shirley Ho</a>, 
<a href="/search/astro-ph?searchtype=author&query=Eickenberg%2C+M">Michael Eickenberg</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hou%2C+J">Jiamin Hou</a>, 
<a href="/search/astro-ph?searchtype=author&query=Massara%2C+E">Elena Massara</a>, 
<a href="/search/astro-ph?searchtype=author&query=Modi%2C+C">Chirag Modi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dizgah%2C+A+M">Azadeh Moradinezhad Dizgah</a>, 
<a href="/search/astro-ph?searchtype=author&query=Blancard%2C+B+R">Bruno Regaldo-Saint Blancard</a>, 
<a href="/search/astro-ph?searchtype=author&query=Spergel%2C+D">David Spergel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures. A previous version of the paper was published in the ICML 2023 Workshop on Machine Learning for Astrophysics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present the first simulation-based inference (SBI) of cosmological
parameters from field-level analysis of galaxy clustering. Standard galaxy
clustering analyses rely on analyzing summary statistics, such as the power
spectrum, $P_\ell$, with analytic models based on perturbation theory.
Consequently, they do not fully exploit the non-linear and non-Gaussian
features of the galaxy distribution. To address these limitations, we use the
{\sc SimBIG} forward modelling framework to perform SBI using normalizing
flows. We apply SimBIG to a subset of the BOSS CMASS galaxy sample using a
convolutional neural network with stochastic weight averaging to perform
massive data compression of the galaxy field. We infer constraints on $\Omega_m
= 0.267^{+0.033}_{-0.029}$ and $\sigma_8=0.762^{+0.036}_{-0.035}$. While our
constraints on $\Omega_m$ are in-line with standard $P_\ell$ analyses, those on
$\sigma_8$ are $2.65\times$ tighter. Our analysis also provides constraints on
the Hubble constant $H_0=64.5 \pm 3.8 \ {\rm km / s / Mpc}$ from galaxy
clustering alone. This higher constraining power comes from additional
non-Gaussian cosmological information, inaccessible with $P_\ell$. We
demonstrate the robustness of our analysis by showcasing our ability to infer
unbiased cosmological constraints from a series of test simulations that are
constructed using different forward models than the one used in our training
dataset. This work not only presents competitive cosmological constraints but
also introduces novel methods for leveraging additional cosmological
information in upcoming galaxy surveys like DESI, PFS, and Euclid.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15263" title="Abstract">arXiv:2310.15263</a> (cross-list from q-bio.NC) [<a href="/pdf/2310.15263" title="Download PDF">pdf</a>, <a href="/format/2310.15263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-hot Generalized Linear Model for Switching Brain State Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Li%2C+C">Chengrui Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+S+H">Soon Ho Kim</a>, 
<a href="/search/q-bio?searchtype=author&query=Rodgers%2C+C">Chris Rodgers</a>, 
<a href="/search/q-bio?searchtype=author&query=Choi%2C+H">Hannah Choi</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+A">Anqi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Exposing meaningful and interpretable neural interactions is critical to
understanding neural circuits. Inferred neural interactions from neural signals
primarily reflect functional interactions. In a long experiment, subject
animals may experience different stages defined by the experiment, stimuli, or
behavioral states, and hence functional interactions can change over time. To
model dynamically changing functional interactions, prior work employs
state-switching generalized linear models with hidden Markov models (i.e.,
HMM-GLMs). However, we argue they lack biological plausibility, as functional
interactions are shaped and confined by the underlying anatomical connectome.
Here, we propose a novel prior-informed state-switching GLM. We introduce both
a Gaussian prior and a one-hot prior over the GLM in each state. The priors are
learnable. We will show that the learned prior should capture the
state-constant interaction, shedding light on the underlying anatomical
connectome and revealing more likely physical neuron interactions. The
state-dependent interaction modeled by each GLM offers traceability to capture
functional variations across multiple brain states. Our methods effectively
recover true interaction structures in simulated data, achieve the highest
predictive likelihood with real neural datasets, and render interaction
structures and hidden states more interpretable when applied to real neural
data.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15281" title="Abstract">arXiv:2310.15281</a> (cross-list from stat.ML) [<a href="/pdf/2310.15281" title="Download PDF">pdf</a>, <a href="/format/2310.15281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UncertaintyPlayground: A Fast and Simplified Python Library for  Uncertainty Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Azizi%2C+I">Ilia Azizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces UncertaintyPlayground, a Python library built on
PyTorch and GPyTorch for uncertainty estimation in supervised learning tasks.
The library offers fast training for Gaussian and multi-modal outcome
distributions through Sparse and Variational Gaussian Process Regressions
(SVGPRs) for normally distributed outcomes and Mixed Density Networks (MDN) for
mixed distributions. In addition to model training with various
hyperparameters, UncertaintyPlayground can visualize the prediction intervals
of one or more instances. Due to using tensor operations, the library can be
trained both on CPU and GPU and offers various PyTorch-specific techniques for
speed optimization. The library contains unit tests for each module and ensures
multi-platform continuous integration with GitHub Workflows (online
integration) and Tox (local integration). Finally, the code is documented with
Google-style docstrings and offers a documentation website created with MkDocs
and MkDocStrings.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15286" title="Abstract">arXiv:2310.15286</a> (cross-list from stat.ML) [<a href="/pdf/2310.15286" title="Download PDF">pdf</a>, <a href="/format/2310.15286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Doubly Robust Approach to Sparse Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kim%2C+W">Wonyoung Kim</a>, 
<a href="/search/stat?searchtype=author&query=Iyengar%2C+G">Garud Iyengar</a>, 
<a href="/search/stat?searchtype=author&query=Zeevi%2C+A">Assaf Zeevi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a new regret minimization algorithm for episodic sparse linear
Markov decision process (SMDP) where the state-transition distribution is a
linear function of observed features. The only previously known algorithm for
SMDP requires the knowledge of the sparsity parameter and oracle access to an
unknown policy. We overcome these limitations by combining the doubly robust
method that allows one to use feature vectors of \emph{all} actions with a
novel analysis technique that enables the algorithm to use data from all
periods in all episodes. The regret of the proposed algorithm is
$\tilde{O}(\sigma^{-1}_{\min} s_{\star} H \sqrt{N})$, where $\sigma_{\min}$
denotes the restrictive the minimum eigenvalue of the average Gram matrix of
feature vectors, $s_\star$ is the sparsity parameter, $H$ is the length of an
episode, and $N$ is the number of rounds. We provide a lower regret bound that
matches the upper bound up to logarithmic factors on a newly identified
subclass of SMDPs. Our numerical experiments support our theoretical results
and demonstrate the superior performance of our algorithm.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15303" title="Abstract">arXiv:2310.15303</a> (cross-list from quant-ph) [<a href="/pdf/2310.15303" title="Download PDF">pdf</a>, <a href="/format/2310.15303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Quantum Rapidly-Exploring Random Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lathrop%2C+P">Paul Lathrop</a>, 
<a href="/search/quant-ph?searchtype=author&query=Boardman%2C+B">Beth Boardman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mart%C3%ADnez%2C+S">Sonia Mart&#xed;nez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we present the Parallel Quantum Rapidly-Exploring Random Tree
(Pq-RRT) algorithm, a parallel version of the Quantum Rapidly-Exploring Random
Trees (q-RRT) algorithm. Parallel Quantum RRT is a parallel quantum algorithm
formulation of a sampling-based motion planner that uses Quantum Amplitude
Amplification to search databases of reachable states for addition to a tree.
In this work we investigate how parallel quantum devices can more efficiently
search a database, as the quantum measurement process involves the collapse of
the superposition to a base state, erasing probability information and
therefore the ability to efficiently find multiple solutions. Pq-RRT uses a
manager/parallel-quantum-workers formulation, inspired by traditional parallel
motion planning, to perform simultaneous quantum searches of a feasible state
database. We present results regarding likelihoods of multiple parallel units
finding any and all solutions contained with a shared database, with and
without reachability errors, allowing efficiency predictions to be made. We
offer simulations in dense obstacle environments showing efficiency,
density/heatmap, and speed comparisons for Pq-RRT against q-RRT, classical RRT,
and classical parallel RRT. We then present Quantum Database Annealing, a
database construction strategy for Pq-RRT and q-RRT that uses a temperature
construct to define database creation over time for balancing exploration and
exploitation.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15328" title="Abstract">arXiv:2310.15328</a> (cross-list from eess.IV) [<a href="/pdf/2310.15328" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepVox and SAVE-CT: a contrast- and dose-independent 3D deep learning  approach for thoracic aorta segmentation and aneurysm prediction using  computed tomography scans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=del-Valle%2C+M">Matheus del-Valle</a>, 
<a href="/search/eess?searchtype=author&query=de+Oliveira%2C+L+L">Lariza Laura de Oliveira</a>, 
<a href="/search/eess?searchtype=author&query=Vieira%2C+H+C">Henrique Cursino Vieira</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H+M+H">Henrique Min Ho Lee</a>, 
<a href="/search/eess?searchtype=author&query=Pinheiro%2C+L+L">Lucas Lembran&#xe7;a Pinheiro</a>, 
<a href="/search/eess?searchtype=author&query=Portugal%2C+M+F">Maria Fernanda Portugal</a>, 
<a href="/search/eess?searchtype=author&query=Miyoshi%2C+N+S+B">Newton Shydeo Brand&#xe3;o Miyoshi</a>, 
<a href="/search/eess?searchtype=author&query=Wolosker%2C+N">Nelson Wolosker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Thoracic aortic aneurysm (TAA) is a fatal disease which potentially leads to
dissection or rupture through progressive enlargement of the aorta. It is
usually asymptomatic and screening recommendation are limited. The
gold-standard evaluation is performed by computed tomography angiography (CTA)
and radiologists time-consuming assessment. Scans for other indications could
help on this screening, however if acquired without contrast enhancement or
with low dose protocol, it can make the clinical evaluation difficult, besides
increasing the scans quantity for the radiologists. In this study, it was
selected 587 unique CT scans including control and TAA patients, acquired with
low and standard dose protocols, with or without contrast enhancement. A novel
segmentation model, DeepVox, exhibited dice score coefficients of 0.932 and
0.897 for development and test sets, respectively, with faster training speed
in comparison to models reported in the literature. The novel TAA
classification model, SAVE-CT, presented accuracies of 0.930 and 0.922 for
development and test sets, respectively, using only the binary segmentation
mask from DeepVox as input, without hand-engineered features. These two models
together are a potential approach for TAA screening, as they can handle
variable number of slices as input, handling thoracic and thoracoabdominal
sequences, in a fully automated contrast- and dose-independent evaluation. This
may assist to decrease TAA mortality and prioritize the evaluation queue of
patients for radiologists.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15330" title="Abstract">arXiv:2310.15330</a> (cross-list from stat.ML) [<a href="/pdf/2310.15330" title="Download PDF">pdf</a>, <a href="/format/2310.15330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Federated Learning: A Federated Gradient EM Algorithm for  Heterogeneous Mixture Models with Robustness against Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/stat?searchtype=author&query=Weng%2C+H">Haolei Weng</a>, 
<a href="/search/stat?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While supervised federated learning approaches have enjoyed significant
success, the domain of unsupervised federated learning remains relatively
underexplored. In this paper, we introduce a novel federated gradient EM
algorithm designed for the unsupervised learning of mixture models with
heterogeneous mixture proportions across tasks. We begin with a comprehensive
finite-sample theory that holds for general mixture models, then apply this
general theory on Gaussian Mixture Models (GMMs) and Mixture of Regressions
(MoRs) to characterize the explicit estimation error of model parameters and
mixture proportions. Our proposed federated gradient EM algorithm demonstrates
several key advantages: adaptability to unknown task similarity, resilience
against adversarial attacks on a small fraction of data sources, protection of
local data privacy, and computational and communication efficiency.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15347" title="Abstract">arXiv:2310.15347</a> (cross-list from math.OC) [<a href="/pdf/2310.15347" title="Download PDF">pdf</a>, <a href="/ps/2310.15347" title="Download PostScript">ps</a>, <a href="/format/2310.15347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controller implementability: a data-driven approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Padoan%2C+A">Alberto Padoan</a>, 
<a href="/search/math?searchtype=author&query=Coulson%2C+J">Jeremy Coulson</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">We study the controller implementability problem, which seeks to determine if
a controller can make the closed-loop behavior of a given plant match that of a
desired reference behavior. We establish necessary and sufficient conditions
for controller implementability which only rely on raw data. Subsequently, we
consider the problem of constructing controllers directly from data. By
leveraging the concept of canonical controller, we provide a formula to
directly construct controllers that implement plant-compatible reference
behaviors using measurements of both reference and plant behaviors.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15354" title="Abstract">arXiv:2310.15354</a> (cross-list from math.OC) [<a href="/pdf/2310.15354" title="Download PDF">pdf</a>, <a href="/ps/2310.15354" title="Download PostScript">ps</a>, <a href="/format/2310.15354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven representations of conical, convex, and affine behaviors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Padoan%2C+A">Alberto Padoan</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>, 
<a href="/search/math?searchtype=author&query=Lygeros%2C+J">John Lygeros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The paper studies conical, convex, and affine models in the framework of
behavioral systems theory. We investigate basic properties of such behaviors
and address the problem of constructing models from measured data. We prove
that closed, shift-invariant, conical, convex, and affine models have the
intersection property, thereby enabling the definition of most powerful
unfalsified models based on infinite-horizon measurements. We then provide
necessary and sufficient conditions for representing conical, convex, and
affine finite-horizon behaviors using raw data matrices, expressing persistence
of excitation requirements in terms of non-negative rank conditions. The
applicability of our results is demonstrated by a numerical example arising in
population ecology.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15371" title="Abstract">arXiv:2310.15371</a> (cross-list from eess.IV) [<a href="/pdf/2310.15371" title="Download PDF">pdf</a>, <a href="/format/2310.15371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vicinal Feature Statistics Augmentation for Federated 3D Medical Volume  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yongsong Huang</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+W">Wanqing Xie</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Mingzhen Li</a>, 
<a href="/search/eess?searchtype=author&query=Cheng%2C+M">Mingmei Cheng</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+J">Jinzhou Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Weixiao Wang</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+J">Jane You</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiaofeng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28th biennial international conference on Information Processing in Medical Imaging (IPMI 2023): Oral Paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: Frangi, A., de Bruijne, M., Wassermann, D., Navab, N. (eds)
  Information Processing in Medical Imaging. IPMI 2023. Lecture Notes in
  Computer Science, vol 13939. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Federated learning (FL) enables multiple client medical institutes
collaboratively train a deep learning (DL) model with privacy protection.
However, the performance of FL can be constrained by the limited availability
of labeled data in small institutes and the heterogeneous (i.e., non-i.i.d.)
data distribution across institutes. Though data augmentation has been a proven
technique to boost the generalization capabilities of conventional centralized
DL as a "free lunch", its application in FL is largely underexplored. Notably,
constrained by costly labeling, 3D medical segmentation generally relies on
data augmentation. In this work, we aim to develop a vicinal feature-level data
augmentation (VFDA) scheme to efficiently alleviate the local feature shift and
facilitate collaborative training for privacy-aware FL segmentation. We take
both the inner- and inter-institute divergence into consideration, without the
need for cross-institute transfer of raw data or their mixup. Specifically, we
exploit the batch-wise feature statistics (e.g., mean and standard deviation)
in each institute to abstractly represent the discrepancy of data, and model
each feature statistic probabilistically via a Gaussian prototype, with the
mean corresponding to the original statistic and the variance quantifying the
augmentation scope. From the vicinal risk minimization perspective, novel
feature statistics can be drawn from the Gaussian distribution to fulfill
augmentation. The variance is explicitly derived by the data bias in each
individual institute and the underlying feature statistics characterized by all
participating institutes. The added-on VFDA consistently yielded marked
improvements over six advanced FL methods on both 3D brain tumor and cardiac
segmentation.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15375" title="Abstract">arXiv:2310.15375</a> (cross-list from stat.AP) [<a href="/pdf/2310.15375" title="Download PDF">pdf</a>, <a href="/format/2310.15375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Structured Matrix Approximation for Robustness to Incomplete  Biosequence Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Salahub%2C+C">Chris Salahub</a>, 
<a href="/search/stat?searchtype=author&query=Uhlmann%2C+J">Jeffrey Uhlmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We propose a general method for optimally approximating an arbitrary matrix
$\mathbf{M}$ by a structured matrix $\mathbf{T}$ (circulant, Toeplitz/Hankel,
etc.) and examine its use for estimating the spectra of genomic linkage
disequilibrium matrices. This application is prototypical of a variety of
genomic and proteomic problems that demand robustness to incomplete biosequence
information. We perform a simulation study and corroborative test of our method
using real genomic data from the Mouse Genome Database. The results confirm the
predicted utility of the method and provide strong evidence of its potential
value to a wide range of bioinformatics applications. Our optimal general
matrix approximation method is expected to be of independent interest to an
even broader range of applications in applied mathematics and engineering.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15387" title="Abstract">arXiv:2310.15387</a> (cross-list from stat.ML) [<a href="/pdf/2310.15387" title="Download PDF">pdf</a>, <a href="/ps/2310.15387" title="Download PostScript">ps</a>, <a href="/format/2310.15387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error analysis of generative adversarial network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hasan%2C+M">Mahmud Hasan</a>, 
<a href="/search/stat?searchtype=author&query=Sang%2C+H">Hailin Sang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The generative adversarial network (GAN) is an important model developed for
high-dimensional distribution learning in recent years. However, there is a
pressing need for a comprehensive method to understand its error convergence
rate. In this research, we focus on studying the error convergence rate of the
GAN model that is based on a class of functions encompassing the discriminator
and generator neural networks. These functions are VC type with bounded
envelope function under our assumptions, enabling the application of the
Talagrand inequality. By employing the Talagrand inequality and Borel-Cantelli
lemma, we establish a tight convergence rate for the error of GAN. This method
can also be applied on existing error estimations of GAN and yields improved
convergence rates. In particular, the error defined with the neural network
distance is a special case error in our definition.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15390" title="Abstract">arXiv:2310.15390</a> (cross-list from astro-ph.SR) [<a href="/pdf/2310.15390" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MEMPSEP III. A machine learning-oriented multivariate data set for  forecasting the Occurrence and Properties of Solar Energetic Particle Events  using a Multivariate Ensemble Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Moreland%2C+K">Kimberly Moreland</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dayeh%2C+M">Maher Dayeh</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bain%2C+H+M">Hazel M. Bain</a>, 
<a href="/search/astro-ph?searchtype=author&query=Chatterjee%2C+S">Subhamoy Chatterjee</a>, 
<a href="/search/astro-ph?searchtype=author&query=Munoz-Jaramillo%2C+A">Andres Munoz-Jaramillo</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hart%2C+S">Samuel Hart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Machine Learning (cs.LG); Space Physics (physics.space-ph)

</div>
<p class="mathjax">We introduce a new multivariate data set that utilizes multiple spacecraft
collecting in-situ and remote sensing heliospheric measurements shown to be
linked to physical processes responsible for generating solar energetic
particles (SEPs). Using the Geostationary Operational Environmental Satellites
(GOES) flare event list from Solar Cycle (SC) 23 and part of SC 24 (1998-2013),
we identify 252 solar events (flares) that produce SEPs and 17,542 events that
do not. For each identified event, we acquire the local plasma properties at 1
au, such as energetic proton and electron data, upstream solar wind conditions,
and the interplanetary magnetic field vector quantities using various
instruments onboard GOES and the Advanced Composition Explorer (ACE)
spacecraft. We also collect remote sensing data from instruments onboard the
Solar Dynamic Observatory (SDO), Solar and Heliospheric Observatory (SoHO), and
the Wind solar radio instrument WAVES. The data set is designed to allow for
variations of the inputs and feature sets for machine learning (ML) in
heliophysics and has a specific purpose for forecasting the occurrence of SEP
events and their subsequent properties. This paper describes a dataset created
from multiple publicly available observation sources that is validated,
cleaned, and carefully curated for our machine-learning pipeline. The dataset
has been used to drive the newly-developed Multivariate Ensemble of Models for
Probabilistic Forecast of Solar Energetic Particles (MEMPSEP; see MEMPSEP I
(Chatterjee et al., 2023) and MEMPSEP II (Dayeh et al., 2023) for associated
papers).
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15399" title="Abstract">arXiv:2310.15399</a> (cross-list from eess.AS) [<a href="/pdf/2310.15399" title="Download PDF">pdf</a>, <a href="/ps/2310.15399" title="Download PostScript">ps</a>, <a href="/format/2310.15399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GESI: Gammachirp Envelope Similarity Index for Predicting  Intelligibility of Simulated Hearing Loss Sounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yamamoto%2C+A">Ayako Yamamoto</a>, 
<a href="/search/eess?searchtype=author&query=Irino%2C+T">Toshio Irino</a>, 
<a href="/search/eess?searchtype=author&query=Miyazaki%2C+F">Fuki Miyazaki</a>, 
<a href="/search/eess?searchtype=author&query=Tamaru%2C+H">Honoka Tamaru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was submitted to Speech Communication on September 14, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">We proposed a new objective intelligibility measure (OIM), called the
Gammachirp Envelope Similarity Index (GESI), which can predict the speech
intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing
(NH) listeners. GESI is an intrusive method that computes the SI metric using
the gammachirp filterbank (GCFB), the modulation filterbank, and the extended
cosine similarity measure. GESI can accept the level asymmetry of the reference
and test sounds and reflect the HI listener's hearing level as it appears on
the audiogram. A unique feature of GESI is its ability to incorporate an
individual participant's listening condition into the SI prediction. We
conducted four SI experiments on male and female speech sounds in both
laboratory and crowdsourced remote environments. We then evaluated GESI and the
conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI, for their ability to predict
mean and individual SI values with and without the use of simulated HL sounds.
GESI outperformed the other OIMs in all evaluations. STOI, ESTOI, and MBSTOI
did not predict SI at all, even when using the simulated HL sounds. HASPI did
not predict the difference between the laboratory and remote experiments on
male speech sounds and the individual SI values. GESI may provide a first step
toward SI prediction for individual HI listeners whose HL is caused solely by
peripheral dysfunction.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15401" title="Abstract">arXiv:2310.15401</a> (cross-list from math.CO) [<a href="/pdf/2310.15401" title="Download PDF">pdf</a>, <a href="/ps/2310.15401" title="Download PostScript">ps</a>, <a href="/format/2310.15401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph decomposition via edge edits into a union of regular graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zeng%2C+T">Tony Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Suppose a finite, unweighted, combinatorial graph $G = (V,E)$ is the union of
several (degree-)regular graphs which are then additionally connected with a
few additional edges. $G$ will then have only a small number of vertices $v \in
V$ with the property that one of their neighbors $(v,w) \in E$ has a higher
degree $\mbox{deg}(w) &gt; \mbox{deg}(v)$. We prove the converse statement: if a
graph has few vertices having a neighbor with higher degree and satisfies a
mild regularity condition, then, via adding and removing a few edges, the graph
can be turned into a disjoint union of (distance-)regular graphs. The number of
edge operations depends on the maximum degree and number of vertices with a
higher degree neighbor but is independent of the size of $|V|$.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15402" title="Abstract">arXiv:2310.15402</a> (cross-list from eess.IV) [<a href="/pdf/2310.15402" title="Download PDF">pdf</a>, <a href="/format/2310.15402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards contrast-agnostic soft segmentation of the spinal cord
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=B%C3%A9dard%2C+S">Sandrine B&#xe9;dard</a>, 
<a href="/search/eess?searchtype=author&query=Enamundram%2C+N+K">Naga Karthik Enamundram</a>, 
<a href="/search/eess?searchtype=author&query=Tsagkas%2C+C">Charidimos Tsagkas</a>, 
<a href="/search/eess?searchtype=author&query=Pravat%C3%A0%2C+E">Emanuele Pravat&#xe0;</a>, 
<a href="/search/eess?searchtype=author&query=Granziera%2C+C">Cristina Granziera</a>, 
<a href="/search/eess?searchtype=author&query=Smith%2C+A">Andrew Smith</a>, 
<a href="/search/eess?searchtype=author&query=Weber%2C+K+A">Kenneth Arnold Weber II</a>, 
<a href="/search/eess?searchtype=author&query=Cohen-Adad%2C+J">Julien Cohen-Adad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Medical Image Analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Spinal cord segmentation is clinically relevant and is notably used to
compute spinal cord cross-sectional area (CSA) for the diagnosis and monitoring
of cord compression or neurodegenerative diseases such as multiple sclerosis.
While several semi and automatic methods exist, one key limitation remains: the
segmentation depends on the MRI contrast, resulting in different CSA across
contrasts. This is partly due to the varying appearance of the boundary between
the spinal cord and the cerebrospinal fluid that depends on the sequence and
acquisition parameters. This contrast-sensitive CSA adds variability in
multi-center studies where protocols can vary, reducing the sensitivity to
detect subtle atrophies. Moreover, existing methods enhance the CSA variability
by training one model per contrast, while also producing binary masks that do
not account for partial volume effects. In this work, we present a deep
learning-based method that produces soft segmentations of the spinal cord.
Using the Spine Generic Public Database of healthy participants
($\text{n}=267$; $\text{contrasts}=6$), we first generated participant-wise
soft ground truth (GT) by averaging the binary segmentations across all 6
contrasts. These soft GT, along with a regression-based loss function, were
then used to train a UNet model for spinal cord segmentation. We evaluated our
model against state-of-the-art methods and performed ablation studies involving
different GT mask types, loss functions, and contrast-specific models. Our
results show that using the soft average segmentations along with a regression
loss function reduces CSA variability ($p &lt; 0.05$, Wilcoxon signed-rank test).
The proposed spinal cord segmentation model generalizes better than the
state-of-the-art contrast-specific methods amongst unseen datasets, vendors,
contrasts, and pathologies (compression, lesions), while accounting for partial
volume effects.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15425" title="Abstract">arXiv:2310.15425</a> (cross-list from eess.AS) [<a href="/pdf/2310.15425" title="Download PDF">pdf</a>, <a href="/format/2310.15425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mason-Alberta Phonetic Segmenter: A forced alignment system based on  deep neural networks and interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kelley%2C+M+C">Matthew C. Kelley</a>, 
<a href="/search/eess?searchtype=author&query=Perry%2C+S+J">Scott James Perry</a>, 
<a href="/search/eess?searchtype=author&query=Tucker%2C+B+V">Benjamin V. Tucker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Forced alignment systems automatically determine boundaries between segments
in speech data, given an orthographic transcription. These tools are
commonplace in phonetics to facilitate the use of speech data that would be
infeasible to manually transcribe and segment. In the present paper, we
describe a new neural network-based forced alignment system, the Mason-Alberta
Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two
possible improvements we pursue for forced alignment systems. The first is
treating the acoustic model in a forced aligner as a tagging task, rather than
a classification task, motivated by the common understanding that segments in
speech are not truly discrete and commonly overlap. The second is an
interpolation technique to allow boundaries more precise than the common 10 ms
limit in modern forced alignment systems. We compare configurations of our
system to a state-of-the-art system, the Montreal Forced Aligner. The tagging
approach did not generally yield improved results over the Montreal Forced
Aligner. However, a system with the interpolation technique had a 27.92%
increase relative to the Montreal Forced Aligner in the amount of boundaries
within 10 ms of the target on the test set. We also reflect on the task and
training process for acoustic modeling in forced alignment, highlighting how
the output targets for these models do not match phoneticians' conception of
similarity between phones and that reconciliation of this tension may require
rethinking the task and output targets or how speech itself should be
segmented.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15440" title="Abstract">arXiv:2310.15440</a> (cross-list from stat.ML) [<a href="/pdf/2310.15440" title="Download PDF">pdf</a>, <a href="/format/2310.15440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Dynamics in Linear VAE: Posterior Collapse Threshold,  Superfluous Latent Space Pitfalls, and Speedup with KL Annealing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ichikawa%2C+Y">Yuma Ichikawa</a>, 
<a href="/search/stat?searchtype=author&query=Hukushima%2C+K">Koji Hukushima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">Variational autoencoders (VAEs) face a notorious problem wherein the
variational posterior often aligns closely with the prior, a phenomenon known
as posterior collapse, which hinders the quality of representation learning. To
mitigate this problem, an adjustable hyperparameter $\beta$ and a strategy for
annealing this parameter, called KL annealing, are proposed. This study
presents a theoretical analysis of the learning dynamics in a minimal VAE. It
is rigorously proved that the dynamics converge to a deterministic process
within the limit of large input dimensions, thereby enabling a detailed
dynamical analysis of the generalization error. Furthermore, the analysis shows
that the VAE initially learns entangled representations and gradually acquires
disentangled representations. A fixed-point analysis of the deterministic
process reveals that when $\beta$ exceeds a certain threshold, posterior
collapse becomes inevitable regardless of the learning period. Additionally,
the superfluous latent variables for the data-generative factors lead to
overfitting of the background noise; this adversely affects both generalization
and learning convergence. The analysis further unveiled that appropriately
tuned KL annealing can accelerate convergence.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15441" title="Abstract">arXiv:2310.15441</a> (cross-list from quant-ph) [<a href="/pdf/2310.15441" title="Download PDF">pdf</a>, <a href="/format/2310.15441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence rate of algorithms for solving linear equations by quantum  annealing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Shalgin%2C+V">V. Shalgin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tikhomirov%2C+S">S. Tikhomirov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Russian, 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Numerical Analysis (math.NA); Probability (math.PR)

</div>
<p class="mathjax">We consider various iterative algorithms for solving the linear equation
$ax=b$ using a quantum computer operating on the principle of quantum
annealing. Assuming that the computer's output is described by the Boltzmann
distribution, it is shown under which conditions the equation-solving
algorithms converge, and an estimate of their convergence rate is provided. The
application of this approach to algorithms using both an infinite number of
qubits and a small number of qubits is discussed.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15448" title="Abstract">arXiv:2310.15448</a> (cross-list from math.OC) [<a href="/pdf/2310.15448" title="Download PDF">pdf</a>, <a href="/format/2310.15448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An accelerated first-order regularized momentum descent ascent algorithm  for stochastic nonconvex-concave minimax problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+H">Huiling Zhang</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Z">Zi Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Stochastic nonconvex minimax problems have attracted wide attention in
machine learning, signal processing and many other fields in recent years. In
this paper, we propose an accelerated first-order regularized momentum descent
ascent algorithm (FORMDA) for solving stochastic nonconvex-concave minimax
problems. The iteration complexity of the algorithm is proved to be
$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$ to obtain an
$\varepsilon$-stationary point, which achieves the best-known complexity bound
for single-loop algorithms to solve the stochastic nonconvex-concave minimax
problems under the stationarity of the objective function.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15465" title="Abstract">arXiv:2310.15465</a> (cross-list from physics.soc-ph) [<a href="/pdf/2310.15465" title="Download PDF">pdf</a>, <a href="/ps/2310.15465" title="Download PostScript">ps</a>, <a href="/format/2310.15465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A universal meta-heuristic framework for influence maximization in  hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Xie%2C+M">Ming Xie</a>, 
<a href="/search/physics?searchtype=author&query=Zhan%2C+X">Xiu-Xiu Zhan</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+C">Chuang Liu</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+Z">Zi-Ke Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Influence maximization (IM) aims to select a small number of nodes that are
able to maximize their influence in a network and covers a wide range of
applications. Despite numerous attempts to provide effective solutions in
ordinary networks, higher-order interactions between entities in various
real-world systems are not usually taken into account. In this paper, we
propose a versatile meta-heuristic approach, hyper genetic algorithm (HGA), to
tackle the IM problem in hypergraphs, which is based on the concept of genetic
evolution. Systematic validations in synthetic and empirical hypergraphs under
both simple and complex contagion models indicate that HGA achieves universal
and plausible performance compared to baseline methods. We explore the cause of
the excellent performance of HGA through ablation studies and correlation
analysis. The findings show that the solution of HGA is distinct from that of
other prior methods. Moreover, a closer look at the local topological features
of the seed nodes acquired by different algorithms reveals that the selection
of seed nodes cannot be based on a single topological characteristic, but
should involve a combination of multiple topological features to address the IM
problem.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15478" title="Abstract">arXiv:2310.15478</a> (cross-list from math.OC) [<a href="/pdf/2310.15478" title="Download PDF">pdf</a>, <a href="/format/2310.15478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Train Your Neural Control Barrier Function: Learning Safety  Filters for Complex Input-Constrained Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=So%2C+O">Oswin So</a>, 
<a href="/search/math?searchtype=author&query=Serlin%2C+Z">Zachary Serlin</a>, 
<a href="/search/math?searchtype=author&query=Mann%2C+M">Makai Mann</a>, 
<a href="/search/math?searchtype=author&query=Gonzales%2C+J">Jake Gonzales</a>, 
<a href="/search/math?searchtype=author&query=Rutledge%2C+K">Kwesi Rutledge</a>, 
<a href="/search/math?searchtype=author&query=Roy%2C+N">Nicholas Roy</a>, 
<a href="/search/math?searchtype=author&query=Fan%2C+C">Chuchu Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024. Project page can be found at <a href="https://mit-realm.github.io/pncbf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Control barrier functions (CBFs) have become popular as a safety filter to
guarantee the safety of nonlinear dynamical systems for arbitrary inputs.
However, it is difficult to construct functions that satisfy the CBF
constraints for high relative degree systems with input constraints. To address
these challenges, recent work has explored learning CBFs using neural networks
via neural CBFs (NCBFs). However, such methods face difficulties when scaling
to higher dimensional systems under input constraints. In this work, we first
identify challenges that NCBFs face during training. Next, to address these
challenges, we propose policy neural CBFs (PNCBFs), a method of constructing
CBFs by learning the value function of a nominal policy, and show that the
value function of the maximum-over-time cost is a CBF. We demonstrate the
effectiveness of our method in simulation on a variety of systems ranging from
toy linear systems to an F-16 jet with a 16-dimensional state space. Finally,
we validate our approach on a two-agent quadcopter system on hardware under
tight input constraints.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15479" title="Abstract">arXiv:2310.15479</a> (cross-list from stat.ML) [<a href="/pdf/2310.15479" title="Download PDF">pdf</a>, <a href="/format/2310.15479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoDiff: combining Auto-encoder and Diffusion model for tabular data  synthesizing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Suh%2C+N">Namjoon Suh</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+X">Xiaofeng Lin</a>, 
<a href="/search/stat?searchtype=author&query=Hsieh%2C+D">Din-Yin Hsieh</a>, 
<a href="/search/stat?searchtype=author&query=Honarkhah%2C+M">Merhdad Honarkhah</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+G">Guang Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion model has become a main paradigm for synthetic data generation in
many subfields of modern machine learning, including computer vision, language
model, or speech synthesis. In this paper, we leverage the power of diffusion
model for generating synthetic tabular data. The heterogeneous features in
tabular data have been main obstacles in tabular data synthesis, and we tackle
this problem by employing the auto-encoder architecture. When compared with the
state-of-the-art tabular synthesizers, the resulting synthetic tables from our
model show nice statistical fidelities to the real data, and perform well in
downstream tasks for machine learning utilities. We conducted the experiments
over 15 publicly available datasets. Notably, our model adeptly captures the
correlations among features, which has been a long-standing challenge in
tabular data synthesis. Our code is available upon request and will be publicly
released if paper is accepted.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15502" title="Abstract">arXiv:2310.15502</a> (cross-list from math.CO) [<a href="/pdf/2310.15502" title="Download PDF">pdf</a>, <a href="/ps/2310.15502" title="Download PostScript">ps</a>, <a href="/format/2310.15502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebraic combinatorial optimization on the degree of determinants of  noncommutative symbolic matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hirai%2C+H">Hiroshi Hirai</a>, 
<a href="/search/math?searchtype=author&query=Iwamasa%2C+Y">Yuni Iwamasa</a>, 
<a href="/search/math?searchtype=author&query=Oki%2C+T">Taihei Oki</a>, 
<a href="/search/math?searchtype=author&query=Soma%2C+T">Tasuku Soma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We address the computation of the degrees of minors of a noncommutative
symbolic matrix of form \[
<br />A[c] := \sum_{k=1}^m A_k t^{c_k} x_k, \] where $A_k$ are matrices over a
field $\mathbb{K}$, $x_i$ are noncommutative variables, $c_k$ are integer
weights, and $t$ is a commuting variable specifying the degree. This problem
extends noncommutative Edmonds' problem (Ivanyos et al. 2017), and can
formulate various combinatorial optimization problems. Extending the study by
Hirai 2018, and Hirai, Ikeda 2022, we provide novel duality theorems and
polyhedral characterization for the maximum degrees of minors of $A[c]$ of all
sizes, and develop a strongly polynomial-time algorithm for computing them.
This algorithm is viewed as a unified algebraization of the classical Hungarian
method for bipartite matching and the weight-splitting algorithm for linear
matroid intersection. As applications, we provide polynomial-time algorithms
for weighted fractional linear matroid matching and linear optimization over
rank-2 Brascamp-Lieb polytopes.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15547" title="Abstract">arXiv:2310.15547</a> (cross-list from math.AP) [<a href="/pdf/2310.15547" title="Download PDF">pdf</a>, <a href="/format/2310.15547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean-square Exponential Stabilization of Mixed-autonomy Traffic PDE  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yihuai Zhang</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+H">Huan Yu</a>, 
<a href="/search/math?searchtype=author&query=Auriol%2C+J">Jean Auriol</a>, 
<a href="/search/math?searchtype=author&query=Pereira%2C+M">Mike Pereira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">Control of mixed-autonomy traffic where Human-driven Vehicles (HVs) and
Autonomous Vehicles (AVs) coexist on the road have gained increasing attention
over the recent decades. This paper addresses the boundary stabilization
problem for mixed traffic on freeways. The traffic dynamics are described by
uncertain coupled hyperbolic partial differential equations (PDEs) with Markov
jumping parameters, which aim to address the distinctive driving strategies
between AVs and HVs. Considering the spacing policies of AVs vary in the mixed
traffic, the stochastic impact area of AVs is governed by a continuous Markov
chain. The interactions between HVs and AVs such as overtaking or lane changing
are mainly induced by the impact areas. Using backstepping design, we develop a
full-state feedback boundary control law to stabilize the deterministic system
(nominal system). Applying Lyapunov analysis, we demonstrate that the nominal
backstepping control law is able to stabilize the traffic system with Markov
jumping parameters, provided the nominal parameters are sufficiently close to
the stochastic ones on average. The mean-square exponential stability
conditions are derived, and the results are validated by numerical simulations.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15549" title="Abstract">arXiv:2310.15549</a> (cross-list from math.OC) [<a href="/pdf/2310.15549" title="Download PDF">pdf</a>, <a href="/format/2310.15549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic Regularization in Tensor Optimization: Towards a Lifted  Approach in Matrix Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ma%2C+Z">Ziye Ma</a>, 
<a href="/search/math?searchtype=author&query=Lavaei%2C+J">Javad Lavaei</a>, 
<a href="/search/math?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS23 Poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Gradient descent (GD) is crucial for generalization in machine learning
models, as it induces implicit regularization, promoting compact
representations. In this work, we examine the role of GD in inducing implicit
regularization for tensor optimization, particularly within the context of the
lifted matrix sensing framework. This framework has been recently proposed to
address the non-convex matrix sensing problem by transforming spurious
solutions into strict saddles when optimizing over symmetric, rank-1 tensors.
We show that, with sufficiently small initialization scale, GD applied to this
lifted problem results in approximate rank-1 tensors and critical points with
escape directions. Our findings underscore the significance of the tensor
parametrization of matrix sensing, in combination with first-order methods, in
achieving global optimality in such problems.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15550" title="Abstract">arXiv:2310.15550</a> (cross-list from eess.IV) [<a href="/pdf/2310.15550" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PET Synthesis via Self-supervised Adaptive Residual Estimation  Generative Adversarial Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xue%2C+Y">Yuxin Xue</a>, 
<a href="/search/eess?searchtype=author&query=Bi%2C+L">Lei Bi</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+Y">Yige Peng</a>, 
<a href="/search/eess?searchtype=author&query=Fulham%2C+M">Michael Fulham</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+D+D">David Dagan Feng</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Jinman Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Positron emission tomography (PET) is a widely used, highly sensitive
molecular imaging in clinical diagnosis. There is interest in reducing the
radiation exposure from PET but also maintaining adequate image quality. Recent
methods using convolutional neural networks (CNNs) to generate synthesized
high-quality PET images from low-dose counterparts have been reported to be
state-of-the-art for low-to-high image recovery methods. However, these methods
are prone to exhibiting discrepancies in texture and structure between
synthesized and real images. Furthermore, the distribution shift between
low-dose PET and standard PET has not been fully investigated. To address these
issues, we developed a self-supervised adaptive residual estimation generative
adversarial network (SS-AEGAN). We introduce (1) An adaptive residual
estimation mapping mechanism, AE-Net, designed to dynamically rectify the
preliminary synthesized PET images by taking the residual map between the
low-dose PET and synthesized output as the input, and (2) A self-supervised
pre-training strategy to enhance the feature representation of the coarse
generator. Our experiments with a public benchmark dataset of total-body PET
images show that SS-AEGAN consistently outperformed the state-of-the-art
synthesis methods with various dose reduction factors.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15559" title="Abstract">arXiv:2310.15559</a> (cross-list from math.OC) [<a href="/pdf/2310.15559" title="Download PDF">pdf</a>, <a href="/ps/2310.15559" title="Download PostScript">ps</a>, <a href="/format/2310.15559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Oja&#x27;s Algorithm to the Multiplicative Weights Update Method with  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garber%2C+D">Dan Garber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Oja's algorithm is a well known online algorithm studied mainly in the
context of stochastic principal component analysis. We make a simple
observation, yet to the best of our knowledge a novel one, that when applied to
a any (not necessarily stochastic) sequence of symmetric matrices which share
common eigenvectors, the regret of Oja's algorithm could be directly bounded in
terms of the regret of the well known multiplicative weights update method for
the problem of prediction with expert advice. Several applications to
optimization with quadratic forms over the unit sphere in $\reals^n$ are
discussed.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15627" title="Abstract">arXiv:2310.15627</a> (cross-list from stat.ML) [<a href="/pdf/2310.15627" title="Download PDF">pdf</a>, <a href="/format/2310.15627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual directed acyclic graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Thompson%2C+R">Ryan Thompson</a>, 
<a href="/search/stat?searchtype=author&query=Bonilla%2C+E+V">Edwin V. Bonilla</a>, 
<a href="/search/stat?searchtype=author&query=Kohn%2C+R">Robert Kohn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Estimating the structure of directed acyclic graphs (DAGs) from observational
data remains a significant challenge in machine learning. Most research in this
area concentrates on learning a single DAG for the entire population. This
paper considers an alternative setting where the graph structure varies across
individuals based on available "contextual" features. We tackle this contextual
DAG problem via a neural network that maps the contextual features to a DAG,
represented as a weighted adjacency matrix. The neural network is equipped with
a novel projection layer that ensures the output matrices are sparse and
satisfy a recently developed characterization of acyclicity. We devise a
scalable computational framework for learning contextual DAGs and provide a
convergence guarantee and an analytical gradient for backpropagating through
the projection layer. Our experiments suggest that the new approach can recover
the true context-specific graph where existing approaches fail.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15663" title="Abstract">arXiv:2310.15663</a> (cross-list from eess.AS) [<a href="/pdf/2310.15663" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FOLEY-VAE: Generaci&#xf3;n de efectos de audio para cine con inteligencia  artificial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=C%C3%A1mara%2C+M">Mateo C&#xe1;mara</a>, 
<a href="/search/eess?searchtype=author&query=Blanco%2C+J+L">Jos&#xe9; Luis Blanco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, in Spanish, Tecniac\'ustica
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In this research, we present an interface based on Variational Autoencoders
trained with a wide range of natural sounds for the innovative creation of
Foley effects. The model can transfer new sound features to prerecorded audio
or microphone-captured speech in real time. In addition, it allows interactive
modification of latent variables, facilitating precise and customized artistic
adjustments. Taking as a starting point our previous study on Variational
Autoencoders presented at this same congress last year, we analyzed an existing
implementation: RAVE [1]. This model has been specifically trained for audio
effects production. Various audio effects have been successfully generated,
ranging from electromagnetic, science fiction, and water sounds, among others
published with this work. This innovative approach has been the basis for the
artistic creation of the first Spanish short film with sound effects assisted
by artificial intelligence. This milestone illustrates palpably the
transformative potential of this technology in the film industry, opening the
door to new possibilities for sound creation and the improvement of artistic
quality in film productions.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15709" title="Abstract">arXiv:2310.15709</a> (cross-list from stat.ML) [<a href="/pdf/2310.15709" title="Download PDF">pdf</a>, <a href="/format/2310.15709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Representation Learning Made Identifiable by Grouping of  Observational Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Morioka%2C+H">Hiroshi Morioka</a>, 
<a href="/search/stat?searchtype=author&query=Hyv%C3%A4rinen%2C+A">Aapo Hyv&#xe4;rinen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A topic of great current interest is Causal Representation Learning (CRL),
whose goal is to learn a causal model for hidden features in a data-driven
manner. Unfortunately, CRL is severely ill-posed since it is a combination of
the two notoriously ill-posed problems of representation learning and causal
discovery. Yet, finding practical identifiability conditions that guarantee a
unique solution is crucial for its practical applicability. Most approaches so
far have been based on assumptions on the latent causal mechanisms, such as
temporal causality, or existence of supervision or interventions; these can be
too restrictive in actual applications. Here, we show identifiability based on
novel, weak constraints, which requires no temporal structure, intervention,
nor weak supervision. The approach is based assuming the observational mixing
exhibits a suitable grouping of the observational variables. We also propose a
novel self-supervised estimation framework consistent with the model, prove its
statistical consistency, and experimentally show its superior CRL performances
compared to the state-of-the-art baselines. We further demonstrate its
robustness against latent confounders and causal cycles.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15744" title="Abstract">arXiv:2310.15744</a> (cross-list from stat.ML) [<a href="/pdf/2310.15744" title="Download PDF">pdf</a>, <a href="/format/2310.15744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Single Cell RNA Sequencing with Topological Nonnegative Matrix  Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hozumi%2C+Y">Yuta Hozumi</a>, 
<a href="/search/stat?searchtype=author&query=Wei%2C+G">Guo-Wei Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Algebraic Topology (math.AT)

</div>
<p class="mathjax">Single-cell RNA sequencing (scRNA-seq) is a relatively new technology that
has stimulated enormous interest in statistics, data science, and computational
biology due to the high dimensionality, complexity, and large scale associated
with scRNA-seq data. Nonnegative matrix factorization (NMF) offers a unique
approach due to its meta-gene interpretation of resulting low-dimensional
components. However, NMF approaches suffer from the lack of multiscale
analysis. This work introduces two persistent Laplacian regularized NMF
methods, namely, topological NMF (TNMF) and robust topological NMF (rTNMF). By
employing a total of 12 datasets, we demonstrate that the proposed TNMF and
rTNMF significantly outperform all other NMF-based methods. We have also
utilized TNMF and rTNMF for the visualization of popular Uniform Manifold
Approximation and Projection (UMAP) and t-distributed stochastic neighbor
embedding (t-SNE).
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15767" title="Abstract">arXiv:2310.15767</a> (cross-list from eess.IV) [<a href="/pdf/2310.15767" title="Download PDF">pdf</a>, <a href="/ps/2310.15767" title="Download PostScript">ps</a>, <a href="/format/2310.15767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Q">Quanwei Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jianan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiling Liu</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+Y">Yanni Dong</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+Z">Zhihan Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">High-resolution (HR) magnetic resonance imaging (MRI) is crucial for
enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent
limitation of MRI resolution restricts its widespread applicability. Deep
learning-based image super-resolution (SR) methods exhibit promise in improving
MRI resolution without additional cost. However, these methods frequently
require a substantial number of HR MRI images for training, which can be
challenging to acquire. In this paper, we propose an unpaired MRI SR approach
that employs self-supervised contrastive learning to enhance SR performance
with limited training data. Our approach leverages both authentic HR images and
synthetically generated SR images to construct positive and negative sample
pairs, thus facilitating the learning of discriminative features. Empirical
results presented in this study underscore significant enhancements in the peak
signal-to-noise ratio and structural similarity index, even when a paucity of
HR images is available. These findings accentuate the potential of our approach
in addressing the challenge of limited training data, thereby contributing to
the advancement of high-resolution MRI in clinical applications.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15786" title="Abstract">arXiv:2310.15786</a> (cross-list from stat.ML) [<a href="/pdf/2310.15786" title="Download PDF">pdf</a>, <a href="/format/2310.15786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amortised Inference in Neural Networks for Small-Scale Probabilistic  Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ashman%2C+M">Matthew Ashman</a>, 
<a href="/search/stat?searchtype=author&query=Rochussen%2C+T">Tommy Rochussen</a>, 
<a href="/search/stat?searchtype=author&query=Weller%2C+A">Adrian Weller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The global inducing point variational approximation for BNNs is based on
using a set of inducing inputs to construct a series of conditional
distributions that accurately approximate the conditionals of the true
posterior distribution. Our key insight is that these inducing inputs can be
replaced by the actual data, such that the variational distribution consists of
a set of approximate likelihoods for each datapoint. This structure lends
itself to amortised inference, in which the parameters of each approximate
likelihood are obtained by passing each datapoint through a meta-model known as
the inference network. By training this inference network across related
datasets, we can meta-learn Bayesian inference over task-specific BNNs.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15816" title="Abstract">arXiv:2310.15816</a> (cross-list from math.DS) [<a href="/pdf/2310.15816" title="Download PDF">pdf</a>, <a href="/format/2310.15816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear dimensionality reduction then and now: AIMs for dissipative  PDEs in the ML era
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Koronaki%2C+E+D">Eleni D. Koronaki</a>, 
<a href="/search/math?searchtype=author&query=Evangelou%2C+N">Nikolaos Evangelou</a>, 
<a href="/search/math?searchtype=author&query=Martin-Linares%2C+C+P">Cristina P. Martin-Linares</a>, 
<a href="/search/math?searchtype=author&query=Titi%2C+E+S">Edriss S. Titi</a>, 
<a href="/search/math?searchtype=author&query=Kevrekidis%2C+I+G">Ioannis G. Kevrekidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study presents a collection of purely data-driven workflows for
constructing reduced-order models (ROMs) for distributed dynamical systems. The
ROMs we focus on, are data-assisted models inspired by, and templated upon, the
theory of Approximate Inertial Manifolds (AIMs); the particular motivation is
the so-called post-processing Galerkin method of Garcia-Archilla, Novo and
Titi. Its applicability can be extended: the need for accurate truncated
Galerkin projections and for deriving closed-formed corrections can be
circumvented using machine learning tools. When the right latent variables are
not a priori known, we illustrate how autoencoders as well as Diffusion Maps (a
manifold learning scheme) can be used to discover good sets of latent variables
and test their explainability. The proposed methodology can express the ROMs in
terms of (a) theoretical (Fourier coefficients), (b) linear data-driven (POD
modes) and/or (c) nonlinear data-driven (Diffusion Maps) coordinates. Both
Black-Box and (theoretically-informed and data-corrected) Gray-Box models are
described; the necessity for the latter arises when truncated Galerkin
projections are so inaccurate as to not be amenable to post-processing. We use
the Chafee-Infante reaction-diffusion and the Kuramoto-Sivashinsky dissipative
partial differential equations to illustrate and successfully test the overall
framework.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15850" title="Abstract">arXiv:2310.15850</a> (cross-list from physics.med-ph) [<a href="/pdf/2310.15850" title="Download PDF">pdf</a>, <a href="/format/2310.15850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Posterior Estimation for Dynamic PET imaging using Conditional  Variational Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Liu%2C+X">Xiaofeng Liu</a>, 
<a href="/search/physics?searchtype=author&query=Marin%2C+T">Thibault Marin</a>, 
<a href="/search/physics?searchtype=author&query=Amal%2C+T">Tiss Amal</a>, 
<a href="/search/physics?searchtype=author&query=Woo%2C+J">Jonghye Woo</a>, 
<a href="/search/physics?searchtype=author&query=Fakhri%2C+G+E">Georges El Fakhri</a>, 
<a href="/search/physics?searchtype=author&query=Ouyang%2C+J">Jinsong Ouyang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published on IEEE NSS&amp;MIC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">This work aims efficiently estimating the posterior distribution of kinetic
parameters for dynamic positron emission tomography (PET) imaging given a
measurement of time of activity curve. Considering the inherent information
loss from parametric imaging to measurement space with the forward kinetic
model, the inverse mapping is ambiguous. The conventional (but expensive)
solution can be the Markov Chain Monte Carlo (MCMC) sampling, which is known to
produce unbiased asymptotical estimation. We propose a deep-learning-based
framework for efficient posterior estimation. Specifically, we counteract the
information loss in the forward process by introducing latent variables. Then,
we use a conditional variational autoencoder (CVAE) and optimize its evidence
lower bound. The well-trained decoder is able to infer the posterior with a
given measurement and the sampled latent variables following a simple
multivariate Gaussian distribution. We validate our CVAE-based method using
unbiased MCMC as the reference for low-dimensional data (a single brain region)
with the simplified reference tissue model.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15853" title="Abstract">arXiv:2310.15853</a> (cross-list from stat.ML) [<a href="/pdf/2310.15853" title="Download PDF">pdf</a>, <a href="/format/2310.15853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Event Time Prediction by Learning to Partition the Event Time  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hickey%2C+J">Jimmy Hickey</a>, 
<a href="/search/stat?searchtype=author&query=Henao%2C+R">Ricardo Henao</a>, 
<a href="/search/stat?searchtype=author&query=Wojdyla%2C+D">Daniel Wojdyla</a>, 
<a href="/search/stat?searchtype=author&query=Pencina%2C+M">Michael Pencina</a>, 
<a href="/search/stat?searchtype=author&query=Engelhard%2C+M+M">Matthew M. Engelhard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently developed survival analysis methods improve upon existing approaches
by predicting the probability of event occurrence in each of a number
pre-specified (discrete) time intervals. By avoiding placing strong parametric
assumptions on the event density, this approach tends to improve prediction
performance, particularly when data are plentiful. However, in clinical
settings with limited available data, it is often preferable to judiciously
partition the event time space into a limited number of intervals well suited
to the prediction task at hand. In this work, we develop a method to learn from
data a set of cut points defining such a partition. We show that in two
simulated datasets, we are able to recover intervals that match the underlying
generative model. We then demonstrate improved prediction performance on three
real-world observational datasets, including a large, newly harmonized stroke
risk prediction dataset. Finally, we argue that our approach facilitates
clinical decision-making by suggesting time intervals that are most appropriate
for each task, in the sense that they facilitate more accurate risk prediction.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15856" title="Abstract">arXiv:2310.15856</a> (cross-list from math.CO) [<a href="/pdf/2310.15856" title="Download PDF">pdf</a>, <a href="/ps/2310.15856" title="Download PostScript">ps</a>, <a href="/format/2310.15856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A criterion for determining whether multiple shells support a $t$-design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Awada%2C+M">Madoka Awada</a>, 
<a href="/search/math?searchtype=author&query=Ishikawa%2C+R">Reina Ishikawa</a>, 
<a href="/search/math?searchtype=author&query=Miezaki%2C+T">Tsuyoshi Miezaki</a>, 
<a href="/search/math?searchtype=author&query=Tanaka%2C+Y">Yuuho Tanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages. arXiv admin note: substantial text overlap with <a href="/abs/2309.03206">arXiv:2309.03206</a>, <a href="/abs/2305.03285">arXiv:2305.03285</a>, <a href="/abs/2310.14281">arXiv:2310.14281</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT); Group Theory (math.GR); Number Theory (math.NT)

</div>
<p class="mathjax">In this paper, we provide a criterion for determining whether multiple shells
support a $t$-design. We construct as a corollary an infinite series of
$2$-designs using power residue codes.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15884" title="Abstract">arXiv:2310.15884</a> (cross-list from stat.CO) [<a href="/pdf/2310.15884" title="Download PDF">pdf</a>, <a href="/ps/2310.15884" title="Download PostScript">ps</a>, <a href="/format/2310.15884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiently generating inverse-Wishart matrices and their Cholesky  factors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Axen%2C+S+D">Seth D. Axen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper presents a new algorithm for generating random inverse-Wishart
matrices that directly generates the Cholesky factor of the matrix without
computing the factorization. Whenever parameterized in terms of a precision
matrix $\Omega=\Sigma^{-1}$, or its Cholesky factor, instead of a covariance
matrix $\Sigma$, the new algorithm is more efficient than the current standard
algorithm.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15898" title="Abstract">arXiv:2310.15898</a> (cross-list from eess.IV) [<a href="/pdf/2310.15898" title="Download PDF">pdf</a>, <a href="/format/2310.15898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLO-Angio: An Algorithm for Coronary Anatomy Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tom Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+H">Hui Lin</a>, 
<a href="/search/eess?searchtype=author&query=Katsaggelos%2C+A+K">Aggelos K. Katsaggelos</a>, 
<a href="/search/eess?searchtype=author&query=Kline%2C+A">Adrienne Kline</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MICCAI Conference ARCADE Grand Challenge, YOLO, Computer Vision,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Coronary angiography remains the gold standard for diagnosis of coronary
artery disease, the most common cause of death worldwide. While this procedure
is performed more than 2 million times annually, there remain few methods for
fast and accurate automated measurement of disease and localization of coronary
anatomy. Here, we present our solution to the Automatic Region-based Coronary
Artery Disease diagnostics using X-ray angiography images (ARCADE) challenge
held at MICCAI 2023. For the artery segmentation task, our three-stage approach
combines preprocessing and feature selection by classical computer vision to
enhance vessel contrast, followed by an ensemble model based on YOLOv8 to
propose possible vessel candidates by generating a vessel map. A final
segmentation is based on a logic-based approach to reconstruct the coronary
tree in a graph-based sorting method. Our entry to the ARCADE challenge placed
3rd overall. Using the official metric for evaluation, we achieved an F1 score
of 0.422 and 0.4289 on the validation and hold-out sets respectively.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15966" title="Abstract">arXiv:2310.15966</a> (cross-list from hep-th) [<a href="/pdf/2310.15966" title="Download PDF">pdf</a>, <a href="/format/2310.15966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing and Machine Learning Calabi-Yau Five-folds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-th?searchtype=author&query=Alawadhi%2C+R">R. Alawadhi</a>, 
<a href="/search/hep-th?searchtype=author&query=Angella%2C+D">D. Angella</a>, 
<a href="/search/hep-th?searchtype=author&query=Leonardo%2C+A">A. Leonardo</a>, 
<a href="/search/hep-th?searchtype=author&query=Gherardini%2C+T+S">T. Schettini Gherardini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 8 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Theory (hep-th)</span>; Machine Learning (cs.LG); Algebraic Geometry (math.AG)

</div>
<p class="mathjax">We construct all possible complete intersection Calabi-Yau five-folds in a
product of four or less complex projective spaces, with up to four constraints.
We obtain $27068$ spaces, which are not related by permutations of rows and
columns of the configuration matrix, and determine the Euler number for all of
them. Excluding the $3909$ product manifolds among those, we calculate the
cohomological data for $12433$ cases, i.e. $53.7 \%$ of the non-product spaces,
obtaining $2375$ different Hodge diamonds. The dataset containing all the above
information is available at
https://www.dropbox.com/scl/fo/z7ii5idt6qxu36e0b8azq/h?rlkey=0qfhx3tykytduobpld510gsfy&amp;dl=0
. The distributions of the invariants are presented, and a comparison with the
lower-dimensional analogues is discussed. Supervised machine learning is
performed on the cohomological data, via classifier and regressor (both fully
connected and convolutional) neural networks. We find that $h^{1,1}$ can be
learnt very efficiently, with very high $R^2$ score and an accuracy of $96\%$,
i.e. $96 \%$ of the predictions exactly match the correct values. For
$h^{1,4},h^{2,3}, \eta$, we also find very high $R^2$ scores, but the accuracy
is lower, due to the large ranges of possible values.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15974" title="Abstract">arXiv:2310.15974</a> (cross-list from stat.ML) [<a href="/pdf/2310.15974" title="Download PDF">pdf</a>, <a href="/ps/2310.15974" title="Download PostScript">ps</a>, <a href="/format/2310.15974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimax Forward and Backward Learning of Evolving Tasks with Performance  Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=%C3%81lvarez%2C+V">Ver&#xf3;nica &#xc1;lvarez</a>, 
<a href="/search/stat?searchtype=author&query=Mazuelas%2C+S">Santiago Mazuelas</a>, 
<a href="/search/stat?searchtype=author&query=Lozano%2C+J+A">Jose A. Lozano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">For a sequence of classification tasks that arrive over time, it is common
that tasks are evolving in the sense that consecutive tasks often have a higher
similarity. The incremental learning of a growing sequence of tasks holds
promise to enable accurate classification even with few samples per task by
leveraging information from all the tasks in the sequence (forward and backward
learning). However, existing techniques developed for continual learning and
concept drift adaptation are either designed for tasks with time-independent
similarities or only aim to learn the last task in the sequence. This paper
presents incremental minimax risk classifiers (IMRCs) that effectively exploit
forward and backward learning and account for evolving tasks. In addition, we
analytically characterize the performance improvement provided by forward and
backward learning in terms of the tasks' expected quadratic change and the
number of tasks. The experimental evaluation shows that IMRCs can result in a
significant performance improvement, especially for reduced sample sizes.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Wed, 25 Oct 23</h3>
<dl>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1904.07381" title="Abstract">arXiv:1904.07381</a> (replaced) [<a href="/pdf/1904.07381" title="Download PDF">pdf</a>, <a href="/ps/1904.07381" title="Download PostScript">ps</a>, <a href="/format/1904.07381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation Algorithms for Distributionally Robust Stochastic  Optimization with Black-Box Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Linhares%2C+A">Andre Linhares</a>, 
<a href="/search/cs?searchtype=author&query=Swamy%2C+C">Chaitanya Swamy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.07541" title="Abstract">arXiv:2001.07541</a> (replaced) [<a href="/pdf/2001.07541" title="Download PDF">pdf</a>, <a href="/ps/2001.07541" title="Download PostScript">ps</a>, <a href="/format/2001.07541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provenance for the Description Logic ELHr
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bourgaux%2C+C">Camille Bourgaux</a>, 
<a href="/search/cs?searchtype=author&query=Ozaki%2C+A">Ana Ozaki</a>, 
<a href="/search/cs?searchtype=author&query=Pe%C3%B1aloza%2C+R">Rafael Pe&#xf1;aloza</a>, 
<a href="/search/cs?searchtype=author&query=Predoiu%2C+L">Livia Predoiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the long version of an IJCAI 2020 paper (23 pages) - v3 fixes glitches in proof of lemma 27 and in claim 30
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2007.01777" title="Abstract">arXiv:2007.01777</a> (replaced) [<a href="/pdf/2007.01777" title="Download PDF">pdf</a>, <a href="/format/2007.01777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Text Classification Via Prototype Trajectories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Dat Hong</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+S+S">Stephen S. Baek</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.02120" title="Abstract">arXiv:2104.02120</a> (replaced) [<a href="/pdf/2104.02120" title="Download PDF">pdf</a>, <a href="/format/2104.02120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear model reduction for slow-fast stochastic systems near unknown  invariant manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+F+X+-">Felix X.-F. Ye</a>, 
<a href="/search/stat?searchtype=author&query=Yang%2C+S">Sichen Yang</a>, 
<a href="/search/stat?searchtype=author&query=Maggioni%2C+M">Mauro Maggioni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.03412" title="Abstract">arXiv:2106.03412</a> (replaced) [<a href="/pdf/2106.03412" title="Download PDF">pdf</a>, <a href="/format/2106.03412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolution learning in deep convolutional networks using scale-space  theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pintea%2C+S+L">Silvia L.Pintea</a>, 
<a href="/search/cs?searchtype=author&query=Tomen%2C+N">Nergis Tomen</a>, 
<a href="/search/cs?searchtype=author&query=Goes%2C+S+F">Stanley F. Goes</a>, 
<a href="/search/cs?searchtype=author&query=Loog%2C+M">Marco Loog</a>, 
<a href="/search/cs?searchtype=author&query=van+Gemert%2C+J+C">Jan C. van Gemert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint accepted by IEEE Transactions on Image Processing, 2021 (TIP). Link to final published article: <a href="https://ieeexplore.ieee.org/abstract/document/9552550">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Image Processing, vol. 30, pp. 8342-8353,
  2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.06483" title="Abstract">arXiv:2106.06483</a> (replaced) [<a href="/pdf/2106.06483" title="Download PDF">pdf</a>, <a href="/ps/2106.06483" title="Download PostScript">ps</a>, <a href="/format/2106.06483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Costless Model Selection in Contextual Bandits: A Bias-Variance  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+S+K">Sanath Kumar Krishnamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Propp%2C+A+M">Adrienne Margaret Propp</a>, 
<a href="/search/cs?searchtype=author&query=Athey%2C+S">Susan Athey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.12831" title="Abstract">arXiv:2110.12831</a> (replaced) [<a href="/pdf/2110.12831" title="Download PDF">pdf</a>, <a href="/format/2110.12831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental implementation of an emission-aware prosumer with online  flexibility quantification and provision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cai%2C+H">Hanmin Cai</a>, 
<a href="/search/eess?searchtype=author&query=Heer%2C+P">Philipp Heer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.01956" title="Abstract">arXiv:2112.01956</a> (replaced) [<a href="/pdf/2112.01956" title="Download PDF">pdf</a>, <a href="/format/2112.01956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Valid and Diverse Mutations of Real-World Media Data for DNN  Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuanyuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Q">Qi Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.08022" title="Abstract">arXiv:2201.08022</a> (replaced) [<a href="/pdf/2201.08022" title="Download PDF">pdf</a>, <a href="/format/2201.08022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEAM: High-Efficiency Approximate Multiplier Optimization for Deep  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Su Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jingbo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jide Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingli Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, accepted by 2022 IEEE International Symposium on Circuits and Systems (ISCAS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11808" title="Abstract">arXiv:2201.11808</a> (replaced) [<a href="/pdf/2201.11808" title="Download PDF">pdf</a>, <a href="/format/2201.11808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAP: An Attention-Based Module for Concept Based Self-Interpretation and  Knowledge Injection in Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Modegh%2C+R+G">Rassa Ghavami Modegh</a>, 
<a href="/search/cs?searchtype=author&query=Salimi%2C+A">Ahmad Salimi</a>, 
<a href="/search/cs?searchtype=author&query=Dizaji%2C+A">Alireza Dizaji</a>, 
<a href="/search/cs?searchtype=author&query=Rabiee%2C+H+R">Hamid R. Rabiee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.10739" title="Abstract">arXiv:2202.10739</a> (replaced) [<a href="/pdf/2202.10739" title="Download PDF">pdf</a>, <a href="/format/2202.10739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamashita%2C+M">Michiharu Yamashita</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J+T">Jia Tracy Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Thanh Tran</a>, 
<a href="/search/cs?searchtype=author&query=Ekhtiari%2C+H">Hamoon Ekhtiari</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongwon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE DSAA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.05481" title="Abstract">arXiv:2203.05481</a> (replaced) [<a href="/pdf/2203.05481" title="Download PDF">pdf</a>, <a href="/format/2203.05481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Adaptive Composition in Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Whitehouse%2C+J">Justin Whitehouse</a>, 
<a href="/search/cs?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+R">Ryan Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.08546" title="Abstract">arXiv:2203.08546</a> (replaced) [<a href="/pdf/2203.08546" title="Download PDF">pdf</a>, <a href="/format/2203.08546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subgame-perfect Equilibria in Mean-payoff Games (journal version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brice%2C+L">L&#xe9;onard Brice</a>, 
<a href="/search/cs?searchtype=author&query=van+den+Bogaard%2C+M">Marie van den Bogaard</a>, 
<a href="/search/cs?searchtype=author&query=Raskin%2C+J">Jean-Fran&#xe7;ois Raskin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2101.10685">arXiv:2101.10685</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.06863" title="Abstract">arXiv:2204.06863</a> (replaced) [<a href="/pdf/2204.06863" title="Download PDF">pdf</a>, <a href="/format/2204.06863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ULF: Unsupervised Labeling Function Correction using Cross-Validation  for Weak Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sedova%2C+A">Anastasiia Sedova</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+B">Benjamin Roth</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.14376" title="Abstract">arXiv:2206.14376</a> (replaced) [<a href="/pdf/2206.14376" title="Download PDF">pdf</a>, <a href="/ps/2206.14376" title="Download PostScript">ps</a>, <a href="/format/2206.14376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on the Tuza constant $c_k$ for small $k$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lu%2C+Y">Yun-Shan Lu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+H">Hung-Lung Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, 2 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Discrete Mathematics 347 (2024), Article 113756
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.15316" title="Abstract">arXiv:2206.15316</a> (replaced) [<a href="/pdf/2206.15316" title="Download PDF">pdf</a>, <a href="/format/2206.15316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomaly Detection in Echocardiograms with Dynamic Variational Trajectory  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryser%2C+A">Alain Ryser</a>, 
<a href="/search/cs?searchtype=author&query=Manduchi%2C+L">Laura Manduchi</a>, 
<a href="/search/cs?searchtype=author&query=Laumer%2C+F">Fabian Laumer</a>, 
<a href="/search/cs?searchtype=author&query=Michel%2C+H">Holger Michel</a>, 
<a href="/search/cs?searchtype=author&query=Wellmann%2C+S">Sven Wellmann</a>, 
<a href="/search/cs?searchtype=author&query=Vogt%2C+J+E">Julia E. Vogt</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 7th Machine Learning for Healthcare Conference,
  PMLR 182:425-458, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12653" title="Abstract">arXiv:2207.12653</a> (replaced) [<a href="/pdf/2207.12653" title="Download PDF">pdf</a>, <a href="/format/2207.12653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incremental Measurement of Structural Entropy for Dynamic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Runze Yang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Angsheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13559" title="Abstract">arXiv:2207.13559</a> (replaced) [<a href="/pdf/2207.13559" title="Download PDF">pdf</a>, <a href="/format/2207.13559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Encoding of Near-Zero Correlation for an AES Implementation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungkwang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeong-Nyeo Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages, 25 figures, submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07365" title="Abstract">arXiv:2208.07365</a> (replaced) [<a href="/pdf/2208.07365" title="Download PDF">pdf</a>, <a href="/format/2208.07365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Video Domain Adaptation for Action Recognition: A  Disentanglement Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+P">Pengfei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingdong Kong</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xinghua Qu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yi Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiqiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xiang Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023; 20 pages, 9 figures, 10 tables; Code at <a href="https://github.com/ldkong1205/TranSVAE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09708" title="Abstract">arXiv:2208.09708</a> (replaced) [<a href="/pdf/2208.09708" title="Download PDF">pdf</a>, <a href="/format/2208.09708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two  Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R+H">Rui Heng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+V">Vanessa Courville</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+C">Chao Xing</a>, 
<a href="/search/cs?searchtype=author&query=Nia%2C+V+P">Vahid Partovi Nia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.00232" title="Abstract">arXiv:2209.00232</a> (replaced) [<a href="/pdf/2209.00232" title="Download PDF">pdf</a>, <a href="/format/2209.00232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Gromov-Wasserstein Embedding for Capsule Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shamsolmoali%2C+P">Pourya Shamsolmoali</a>, 
<a href="/search/cs?searchtype=author&query=Zareapoor%2C+M">Masoumeh Zareapoor</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Swagatam Das</a>, 
<a href="/search/cs?searchtype=author&query=Granger%2C+E">Eric Granger</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+S">Salvador Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.01646" title="Abstract">arXiv:2209.01646</a> (replaced) [<a href="/pdf/2209.01646" title="Download PDF">pdf</a>, <a href="/format/2209.01646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented  Inference for Unlabeled Entity Problem in NER
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shuzheng Si</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shuang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiaxing Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+B">Baobao Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> COLING 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.05401" title="Abstract">arXiv:2209.05401</a> (replaced) [<a href="/pdf/2209.05401" title="Download PDF">pdf</a>, <a href="/format/2209.05401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MaXM: Towards Multilingual Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Changpinyo%2C+S">Soravit Changpinyo</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+L">Linting Xue</a>, 
<a href="/search/cs?searchtype=author&query=Yarom%2C+M">Michal Yarom</a>, 
<a href="/search/cs?searchtype=author&query=Thapliyal%2C+A+V">Ashish V. Thapliyal</a>, 
<a href="/search/cs?searchtype=author&query=Szpektor%2C+I">Idan Szpektor</a>, 
<a href="/search/cs?searchtype=author&query=Amelot%2C+J">Julien Amelot</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Soricut%2C+R">Radu Soricut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings). <a href="https://github.com/google-research-datasets/maxm">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10888" title="Abstract">arXiv:2209.10888</a> (replaced) [<a href="/pdf/2209.10888" title="Download PDF">pdf</a>, <a href="/format/2209.10888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amortized Variational Inference: A Systematic Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+A">Ankush Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Sanjana Jain</a>, 
<a href="/search/cs?searchtype=author&query=Watchareeruetai%2C+U">Ukrit Watchareeruetai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the Journal of Artificial Intelligence Research (JAIR)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J.Artif.Intell.Res.Vol.78(2023)167-215
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11740" title="Abstract">arXiv:2209.11740</a> (replaced) [<a href="/pdf/2209.11740" title="Download PDF">pdf</a>, <a href="/format/2209.11740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Shift Invariance of Max Pooling Feature Maps in Convolutional  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leterme%2C+H">Hubert Leterme</a> (UGA, LJK), 
<a href="/search/cs?searchtype=author&query=Polisano%2C+K">K&#xe9;vin Polisano</a> (UGA, LJK), 
<a href="/search/cs?searchtype=author&query=Perrier%2C+V">Val&#xe9;rie Perrier</a> (Grenoble INP, LJK), 
<a href="/search/cs?searchtype=author&query=Alahari%2C+K">Karteek Alahari</a> (LJK)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01125" title="Abstract">arXiv:2210.01125</a> (replaced) [<a href="/pdf/2210.01125" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral2Spectral: Image-spectral Similarity Assisted Spectral CT Deep  Reconstruction without Reference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+X">Xiaodong Guo</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Longhui Li</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+P">Peng He</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+P">Peng Feng</a>, 
<a href="/search/eess?searchtype=author&query=Chang%2C+D">Dingyue Chang</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+H">Hengyong Yu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+W">Weiwen Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TCI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01477" title="Abstract">arXiv:2210.01477</a> (replaced) [<a href="/pdf/2210.01477" title="Download PDF">pdf</a>, <a href="/format/2210.01477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OrderlessChain: Do Permissioned Blockchains Need Total Global Order of  Transactions?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasirifard%2C+P">Pezhman Nasirifard</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+R">Ruben Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Jacobsen%2C+H">Hans-Arno Jacobsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03116" title="Abstract">arXiv:2210.03116</a> (replaced) [<a href="/pdf/2210.03116" title="Download PDF">pdf</a>, <a href="/format/2210.03116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content-Based Search for Deep Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+D">Daohan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng-Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kumari%2C+N">Nupur Kumari</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+R">Rohan Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Mia Tang</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun-Yan Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our project page is hosted at <a href="https://generative-intelligence-lab.github.io/modelverse/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01450" title="Abstract">arXiv:2211.01450</a> (replaced) [<a href="/pdf/2211.01450" title="Download PDF">pdf</a>, <a href="/ps/2211.01450" title="Download PostScript">ps</a>, <a href="/format/2211.01450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An analog of the Edwards model for Jacobians of genus 2 curves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Flynn%2C+E+V">E. Victor Flynn</a>, 
<a href="/search/math?searchtype=author&query=Khuri-Makdisi%2C+K">Kamal Khuri-Makdisi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, with two supplemental ancillary files of maple code. v3: extensive revisions, mainly to sections 2 and 5. In particular, section 2 was completely rewritten to use the language of algebraic theta functions; this resulted in a longer exposition
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Symbolic Computation (cs.SC); Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05295" title="Abstract">arXiv:2211.05295</a> (replaced) [<a href="/pdf/2211.05295" title="Download PDF">pdf</a>, <a href="/format/2211.05295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harmonizing output imbalance for defect segmentation on  extremely-imbalanced photovoltaic module cells images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+J">Jianye Yi</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+X">Xiaopin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weixiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zongze Wu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuanlong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengguang Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 16 figures, 3 appendixes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07303" title="Abstract">arXiv:2211.07303</a> (replaced) [<a href="/pdf/2211.07303" title="Download PDF">pdf</a>, <a href="/format/2211.07303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Federated Minimax Optimization with Lower complexities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Feihu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to AISTATS-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07896" title="Abstract">arXiv:2211.07896</a> (replaced) [<a href="/pdf/2211.07896" title="Download PDF">pdf</a>, <a href="/ps/2211.07896" title="Download PostScript">ps</a>, <a href="/format/2211.07896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Probabilistic Proof of the nCPA to CCA Bound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morris%2C+B">Ben Morris</a>, 
<a href="/search/cs?searchtype=author&query=Oberschelp%2C+H">Hans Oberschelp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.13572" title="Abstract">arXiv:2211.13572</a> (replaced) [<a href="/pdf/2211.13572" title="Download PDF">pdf</a>, <a href="/format/2211.13572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Based Object 6D-Pose Estimation during Non-Prehensile  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zisong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Papallas%2C+R">Rafael Papallas</a>, 
<a href="/search/cs?searchtype=author&query=Dogar%2C+M">Mehmet Dogar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.02945" title="Abstract">arXiv:2212.02945</a> (replaced) [<a href="/pdf/2212.02945" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfeits on Darknet Markets: A measurement between Jan-2014 and  Sep-2015
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soldner%2C+F">Felix Soldner</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+B">Bennett Kleinberg</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+S+D">Shane D Johnson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is a pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06808" title="Abstract">arXiv:2212.06808</a> (replaced) [<a href="/pdf/2212.06808" title="Download PDF">pdf</a>, <a href="/format/2212.06808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentive-Aware Models of Financial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jalan%2C+A">Akhil Jalan</a>, 
<a href="/search/math?searchtype=author&query=Chakrabarti%2C+D">Deepayan Chakrabarti</a>, 
<a href="/search/math?searchtype=author&query=Sarkar%2C+P">Purnamrita Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07275" title="Abstract">arXiv:2212.07275</a> (replaced) [<a href="/pdf/2212.07275" title="Download PDF">pdf</a>, <a href="/format/2212.07275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhoMoH: Implicit Photorealistic 3D Models of Human Heads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zanfir%2C+M">Mihai Zanfir</a>, 
<a href="/search/cs?searchtype=author&query=Alldieck%2C+T">Thiemo Alldieck</a>, 
<a href="/search/cs?searchtype=author&query=Sminchisescu%2C+C">Cristian Sminchisescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at the International Conference on 3D Vision 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10465" title="Abstract">arXiv:2212.10465</a> (replaced) [<a href="/pdf/2212.10465" title="Download PDF">pdf</a>, <a href="/format/2212.10465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SODA: Million-scale Dialogue Distillation with Social Commonsense  Contextualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+P">Peter West</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Ximing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Bras%2C+R+L">Ronan Le Bras</a>, 
<a href="/search/cs?searchtype=author&query=Alikhani%2C+M">Malihe Alikhani</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gunhee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023. Dataset, model, and code can be found at <a href="https://hyunw.kim/sodaverse">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10548" title="Abstract">arXiv:2212.10548</a> (replaced) [<a href="/pdf/2212.10548" title="Download PDF">pdf</a>, <a href="/format/2212.10548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T-Projection: High Quality Annotation Projection for Sequence Labeling  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ferrero%2C+I">Iker Garc&#xed;a-Ferrero</a>, 
<a href="/search/cs?searchtype=author&query=Agerri%2C+R">Rodrigo Agerri</a>, 
<a href="/search/cs?searchtype=author&query=Rigau%2C+G">German Rigau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of the EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14098" title="Abstract">arXiv:2212.14098</a> (replaced) [<a href="/pdf/2212.14098" title="Download PDF">pdf</a>, <a href="/format/2212.14098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally Unitarily Invariantizable NEPv and Convergence Analysis of SCF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lu%2C+D">Ding Lu</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+R">Ren-Cang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08269" title="Abstract">arXiv:2301.08269</a> (replaced) [<a href="/pdf/2301.08269" title="Download PDF">pdf</a>, <a href="/format/2301.08269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FENDI: Toward High-Fidelity Entanglement Distribution in the Quantum  Internet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Huayue Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhouyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Ruozhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaojian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fangtong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+G">Guoliang Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently under review at IEEE/ACM Transactions on Networking
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.08721" title="Abstract">arXiv:2301.08721</a> (replaced) [<a href="/pdf/2301.08721" title="Download PDF">pdf</a>, <a href="/format/2301.08721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch Prompting: Efficient Inference with Large Language Model APIs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhoujun Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kasai%2C+J">Jungo Kasai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Industry Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.10956" title="Abstract">arXiv:2301.10956</a> (replaced) [<a href="/pdf/2301.10956" title="Download PDF">pdf</a>, <a href="/format/2301.10956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks can Recover the Hidden Features Solely from the  Graph Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+R">Ryoma Sato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11757" title="Abstract">arXiv:2301.11757</a> (replaced) [<a href="/pdf/2301.11757" title="Download PDF">pdf</a>, <a href="/format/2301.11757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mo&#xfb;sai: Text-to-Music Generation with Long-Context Latent Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+F">Flavio Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Kamal%2C+O">Ojasv Kamal</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhijing Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13060" title="Abstract">arXiv:2301.13060</a> (replaced) [<a href="/pdf/2301.13060" title="Download PDF">pdf</a>, <a href="/format/2301.13060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-One Laws of Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adam-Day%2C+S">Sam Adam-Day</a>, 
<a href="/search/cs?searchtype=author&query=Iliant%2C+T+M">Theodor Mihai Iliant</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+%C4%B0+%C4%B0">&#x130;smail &#x130;lkan Ceylan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS '23 camera-ready version; 10 pages + references + 10 pages appendices, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00617" title="Abstract">arXiv:2302.00617</a> (replaced) [<a href="/pdf/2302.00617" title="Download PDF">pdf</a>, <a href="/format/2302.00617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Large-scale Neural Fields via Context Pruned Meta-Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tack%2C+J">Jihoon Tack</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Subin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sihyun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaeho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>, 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+J+R">Jonathan Richard Schwarz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference proceeding for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01735" title="Abstract">arXiv:2302.01735</a> (replaced) [<a href="/pdf/2302.01735" title="Download PDF">pdf</a>, <a href="/format/2302.01735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Semi-Supervised Medical Image Segmentation: A  Variance-Reduction Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+C">Chenyu You</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Weicheng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+Y">Yifei Min</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fenglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Clifton%2C+D+A">David A. Clifton</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S+K">S Kevin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Staib%2C+L+H">Lawrence Hamilton Staib</a>, 
<a href="/search/cs?searchtype=author&query=Duncan%2C+J+S">James S Duncan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Advances in Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02854" title="Abstract">arXiv:2302.02854</a> (replaced) [<a href="/pdf/2302.02854" title="Download PDF">pdf</a>, <a href="/format/2302.02854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NA-SODINN: a deep learning algorithm for exoplanet image detection based  on residual noise regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Cantero%2C+C">Carles Cantero</a>, 
<a href="/search/astro-ph?searchtype=author&query=Absil%2C+O">Olivier Absil</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dahlqvist%2C+C">Carl-Henrik Dahlqvist</a>, 
<a href="/search/astro-ph?searchtype=author&query=Van+Droogenbroeck%2C+M">Marc Van Droogenbroeck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A&amp;A in press
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03169" title="Abstract">arXiv:2302.03169</a> (replaced) [<a href="/pdf/2302.03169" title="Download PDF">pdf</a>, <a href="/format/2302.03169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Selection for Language Models via Importance Resampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+S+M">Sang Michael Xie</a>, 
<a href="/search/cs?searchtype=author&query=Santurkar%2C+S">Shibani Santurkar</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+T">Tengyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04032" title="Abstract">arXiv:2302.04032</a> (replaced) [<a href="/pdf/2302.04032" title="Download PDF">pdf</a>, <a href="/format/2302.04032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Performance Analysis of Deep Perceptual Loss Networks:  Breaking Transfer Learning Conventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pihlgren%2C+G+G">Gustav Grund Pihlgren</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaidou%2C+K">Konstantina Nikolaidou</a>, 
<a href="/search/cs?searchtype=author&query=Chhipa%2C+P+C">Prakash Chandra Chhipa</a>, 
<a href="/search/cs?searchtype=author&query=Abid%2C+N">Nosheen Abid</a>, 
<a href="/search/cs?searchtype=author&query=Saini%2C+R">Rajkumar Saini</a>, 
<a href="/search/cs?searchtype=author&query=Sandin%2C+F">Fredrik Sandin</a>, 
<a href="/search/cs?searchtype=author&query=Liwicki%2C+M">Marcus Liwicki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04451" title="Abstract">arXiv:2302.04451</a> (replaced) [<a href="/pdf/2302.04451" title="Download PDF">pdf</a>, <a href="/format/2302.04451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on  Graph Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+H">Haotian Ju</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Aneesh Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H+R">Hongyang R. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages. Appeared in AISTATS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06434" title="Abstract">arXiv:2302.06434</a> (replaced) [<a href="/pdf/2302.06434" title="Download PDF">pdf</a>, <a href="/format/2302.06434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Graph Laplacian Estimation by Proximal Newton
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medvedovsky%2C+Y">Yakov Medvedovsky</a>, 
<a href="/search/cs?searchtype=author&query=Treister%2C+E">Eran Treister</a>, 
<a href="/search/cs?searchtype=author&query=Routtenberg%2C+T">Tirza Routtenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07294" title="Abstract">arXiv:2302.07294</a> (replaced) [<a href="/pdf/2302.07294" title="Download PDF">pdf</a>, <a href="/format/2302.07294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derandomized Novelty Detection with FDR Control via Conformal E-values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bashari%2C+M">Meshi Bashari</a>, 
<a href="/search/cs?searchtype=author&query=Epstein%2C+A">Amir Epstein</a>, 
<a href="/search/cs?searchtype=author&query=Romano%2C+Y">Yaniv Romano</a>, 
<a href="/search/cs?searchtype=author&query=Sesia%2C+M">Matteo Sesia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.12250" title="Abstract">arXiv:2302.12250</a> (replaced) [<a href="/pdf/2302.12250" title="Download PDF">pdf</a>, <a href="/format/2302.12250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase diagram of early training dynamics in deep neural networks: effect  of the learning rate, depth, and width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kalra%2C+D+S">Dayal Singh Kalra</a>, 
<a href="/search/cs?searchtype=author&query=Barkeshli%2C+M">Maissam Barkeshli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023 (camera-ready version): Additional results added for cross-entropy loss and effect on network output at initialization; 10+32 pages, 8+35 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13657" title="Abstract">arXiv:2302.13657</a> (replaced) [<a href="/pdf/2302.13657" title="Download PDF">pdf</a>, <a href="/ps/2302.13657" title="Download PostScript">ps</a>, <a href="/format/2302.13657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functors on relational structures which admit both left and right  adjoints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dalmau%2C+V">V&#xed;ctor Dalmau</a>, 
<a href="/search/math?searchtype=author&query=Krokhin%2C+A">Andrei Krokhin</a>, 
<a href="/search/math?searchtype=author&query=Opr%C5%A1al%2C+J">Jakub Opr&#x161;al</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.14229" title="Abstract">arXiv:2302.14229</a> (replaced) [<a href="/pdf/2302.14229" title="Download PDF">pdf</a>, <a href="/format/2302.14229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Cross-Lingual Summarization via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yunlong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+B">Beiqi Zou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhixu Li</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jianfeng Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Both first authors contributed equally. Technical Report, 12 pages. Accepted to the 4th New Frontiers in Summarization Workshop (NewSumm@EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02863" title="Abstract">arXiv:2303.02863</a> (replaced) [<a href="/pdf/2303.02863" title="Download PDF">pdf</a>, <a href="/ps/2303.02863" title="Download PostScript">ps</a>, <a href="/format/2303.02863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on the Proposed Law for Improving the Transparency of Political  Advertising in the European Union
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruohonen%2C+J">Jukka Ruohonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A continuously updated working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03387" title="Abstract">arXiv:2303.03387</a> (replaced) [<a href="/pdf/2303.03387" title="Download PDF">pdf</a>, <a href="/format/2303.03387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a  Context Synergized Hyperbolic Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sreyan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+M">Manan Suri</a>, 
<a href="/search/cs?searchtype=author&query=Chiniya%2C+P">Purva Chiniya</a>, 
<a href="/search/cs?searchtype=author&query=Tyagi%2C+U">Utkarsh Tyagi</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sonal Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Main Conference. Code: <a href="https://github.com/Sreyan88/CoSyn">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04048" title="Abstract">arXiv:2303.04048</a> (replaced) [<a href="/pdf/2303.04048" title="Download PDF">pdf</a>, <a href="/format/2303.04048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT a Good NLG Evaluator? A Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yunlong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fandong Meng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zengkui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haoxiang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhixu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jianfeng Qu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Both first authors contributed equally. Technical Report, 11 pages. Accepted to the 4th New Frontiers in Summarization Workshop (NewSumm@EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04931" title="Abstract">arXiv:2303.04931</a> (replaced) [<a href="/pdf/2303.04931" title="Download PDF">pdf</a>, <a href="/ps/2303.04931" title="Download PostScript">ps</a>, <a href="/format/2303.04931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Observer-Based Key Agreement Scheme for Remotely Controlled Mobile  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Naseri%2C+A+M">Amir Mohammad Naseri</a>, 
<a href="/search/eess?searchtype=author&query=Lucia%2C+W">Walter Lucia</a>, 
<a href="/search/eess?searchtype=author&query=Youssef%2C+A">Amr Youssef</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This preprint has been submitted to the 2023 IFAC World Congress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Cryptography and Security (cs.CR); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05351" title="Abstract">arXiv:2303.05351</a> (replaced) [<a href="/pdf/2303.05351" title="Download PDF">pdf</a>, <a href="/format/2303.05351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent-based Deep Reinforcement Learning for Multi-agent Informative  Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianze Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuhong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Sartoretti%2C+G">Guillaume Sartoretti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> \c{opyright} 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06073" title="Abstract">arXiv:2303.06073</a> (replaced) [<a href="/pdf/2303.06073" title="Download PDF">pdf</a>, <a href="/format/2303.06073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I Tag, You Tag, Everybody Tags!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+H">Hazem Ibrahim</a>, 
<a href="/search/cs?searchtype=author&query=Asim%2C+R">Rohail Asim</a>, 
<a href="/search/cs?searchtype=author&query=Varvello%2C+M">Matteo Varvello</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+Y">Yasir Zaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06273" title="Abstract">arXiv:2303.06273</a> (replaced) [<a href="/pdf/2303.06273" title="Download PDF">pdf</a>, <a href="/format/2303.06273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency Analysis of ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+M+E">Myeongjun Erik Jang</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09752" title="Abstract">arXiv:2303.09752</a> (replaced) [<a href="/pdf/2303.09752" title="Download PDF">pdf</a>, <a href="/format/2303.09752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLT5: Faster Long-Range Transformers with Conditional Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ainslie%2C+J">Joshua Ainslie</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+T">Tao Lei</a>, 
<a href="/search/cs?searchtype=author&query=de+Jong%2C+M">Michiel de Jong</a>, 
<a href="/search/cs?searchtype=author&query=Onta%C3%B1%C3%B3n%2C+S">Santiago Onta&#xf1;&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Brahma%2C+S">Siddhartha Brahma</a>, 
<a href="/search/cs?searchtype=author&query=Zemlyanskiy%2C+Y">Yury Zemlyanskiy</a>, 
<a href="/search/cs?searchtype=author&query=Uthus%2C+D">David Uthus</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Mandy Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lee-Thorp%2C+J">James Lee-Thorp</a>, 
<a href="/search/cs?searchtype=author&query=Tay%2C+Y">Yi Tay</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Yun-Hsuan Sung</a>, 
<a href="/search/cs?searchtype=author&query=Sanghai%2C+S">Sumit Sanghai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10076" title="Abstract">arXiv:2303.10076</a> (replaced) [<a href="/pdf/2303.10076" title="Download PDF">pdf</a>, <a href="/format/2303.10076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Framework for 3D Occupancy Estimation in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+W">Wanshui Gan</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+N">Ningkai Mo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yokoya%2C+N">Naoto Yokoya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12484" title="Abstract">arXiv:2303.12484</a> (replaced) [<a href="/pdf/2303.12484" title="Download PDF">pdf</a>, <a href="/format/2303.12484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-Efficient Deep Learning in Medical Image Analysis: Challenges and  Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Cheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhengrui Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Luyang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13988" title="Abstract">arXiv:2303.13988</a> (replaced) [<a href="/pdf/2303.13988" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Psychology: Investigating Emergent Capabilities and Behavior in  Large Language Models Using Psychological Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hagendorff%2C+T">Thilo Hagendorff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15206" title="Abstract">arXiv:2303.15206</a> (replaced) [<a href="/pdf/2303.15206" title="Download PDF">pdf</a>, <a href="/format/2303.15206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods  for Front-Facing Views
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hanxue Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hanji%2C+P">Param Hanji</a>, 
<a href="/search/cs?searchtype=author&query=Banterle%2C+F">Francesco Banterle</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Mantiuk%2C+R">Rafal Mantiuk</a>, 
<a href="/search/cs?searchtype=author&query=Oztireli%2C+C">Cengiz Oztireli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17971" title="Abstract">arXiv:2303.17971</a> (replaced) [<a href="/pdf/2303.17971" title="Download PDF">pdf</a>, <a href="/format/2303.17971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule Enforcing Through Ordering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sychrovsk%C3%BD%2C+D">David Sychrovsk&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+S">Sameer Desai</a>, 
<a href="/search/cs?searchtype=author&query=Loebl%2C+M">Martin Loebl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 14th Conference on Decision and Game Theory for Security (GameSec-23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00133" title="Abstract">arXiv:2304.00133</a> (replaced) [<a href="/pdf/2304.00133" title="Download PDF">pdf</a>, <a href="/format/2304.00133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate  Decision Stumps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatzimparmpas%2C+A">Angelos Chatzimparmpas</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+R+M">Rafael M. Martins</a>, 
<a href="/search/cs?searchtype=author&query=Telea%2C+A+C">Alexandru C. Telea</a>, 
<a href="/search/cs?searchtype=author&query=Kerren%2C+A">Andreas Kerren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript is currently under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02210" title="Abstract">arXiv:2304.02210</a> (replaced) [<a href="/pdf/2304.02210" title="Download PDF">pdf</a>, <a href="/format/2304.02210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Document-Level Machine Translation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+C">Chenyang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tianbo Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhirui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang are equal contributors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02539" title="Abstract">arXiv:2304.02539</a> (replaced) [<a href="/pdf/2304.02539" title="Download PDF">pdf</a>, <a href="/format/2304.02539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-annotator Deep Learning: A Probabilistic Framework for  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herde%2C+M">Marek Herde</a>, 
<a href="/search/cs?searchtype=author&query=Huseljic%2C+D">Denis Huseljic</a>, 
<a href="/search/cs?searchtype=author&query=Sick%2C+B">Bernhard Sick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Transactions on Machine Learning Research, see <a href="https://openreview.net/forum?id=MgdoxzImlK">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03699" title="Abstract">arXiv:2304.03699</a> (replaced) [<a href="/pdf/2304.03699" title="Download PDF">pdf</a>, <a href="/format/2304.03699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sorta Solving the OPF by Not Solving the OPF: DAE Control Theory and the  Price of Realtime Regulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nadeem%2C+M">Muhammad Nadeem</a>, 
<a href="/search/eess?searchtype=author&query=Taha%2C+A+F">Ahmad F. Taha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06479" title="Abstract">arXiv:2304.06479</a> (replaced) [<a href="/pdf/2304.06479" title="Download PDF">pdf</a>, <a href="/format/2304.06479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Search Approaches to Sampling-Based Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lathrop%2C+P">Paul Lathrop</a>, 
<a href="/search/quant-ph?searchtype=author&query=Boardman%2C+B">Beth Boardman</a>, 
<a href="/search/quant-ph?searchtype=author&query=Mart%C3%ADnez%2C+S">Sonia Mart&#xed;nez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, vol. 11, pp. 89506-89519, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06646" title="Abstract">arXiv:2304.06646</a> (replaced) [<a href="/pdf/2304.06646" title="Download PDF">pdf</a>, <a href="/ps/2304.06646" title="Download PostScript">ps</a>, <a href="/format/2304.06646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterising Modal Formulas with Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cate%2C+B+t">Balder ten Cate</a>, 
<a href="/search/cs?searchtype=author&query=Koudijs%2C+R">Raoul Koudijs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Expanded version of material from Raoul Koudijs's MSc thesis (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08130" title="Abstract">arXiv:2304.08130</a> (replaced) [<a href="/pdf/2304.08130" title="Download PDF">pdf</a>, <a href="/format/2304.08130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Few-Shot Class-Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+S">Songsong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lusi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+H">Hang Ran</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xin Ning</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+P">Prayag Tiwari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09848" title="Abstract">arXiv:2304.09848</a> (replaced) [<a href="/pdf/2304.09848" title="Download PDF">pdf</a>, <a href="/format/2304.09848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Verifiability in Generative Search Engines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+N+F">Nelson F. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 12 figures; to appear in Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11407" title="Abstract">arXiv:2304.11407</a> (replaced) [<a href="/pdf/2304.11407" title="Download PDF">pdf</a>, <a href="/format/2304.11407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Virtual Tube Planning and Control for Swarm Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+P">Pengda Mao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+R">Rao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+Q">Quan Quan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11582" title="Abstract">arXiv:2304.11582</a> (replaced) [<a href="/pdf/2304.11582" title="Download PDF">pdf</a>, <a href="/format/2304.11582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffTraj: Generating GPS Trajectory with Diffusion Probabilistic Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuanshao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yongchao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J+J+Q">James J.Q. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12668" title="Abstract">arXiv:2304.12668</a> (replaced) [<a href="/pdf/2304.12668" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social media in the Global South: A Network Dataset of the Malian  Twittersphere
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schroeder%2C+D+T">Daniel Thilo Schroeder</a>, 
<a href="/search/cs?searchtype=author&query=de+Bruijn%2C+M">Mirjam de Bruijn</a>, 
<a href="/search/cs?searchtype=author&query=Bruls%2C+L">Luca Bruls</a>, 
<a href="/search/cs?searchtype=author&query=Moges%2C+M+A">Mulatu Alemayehu Moges</a>, 
<a href="/search/cs?searchtype=author&query=Badji%2C+S+D">Samba Dialimpa Badji</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+N">No&#xeb;mie Fritz</a>, 
<a href="/search/cs?searchtype=author&query=Cisse%2C+M+G">Modibo Galy Cisse</a>, 
<a href="/search/cs?searchtype=author&query=Langguth%2C+J">Johannes Langguth</a>, 
<a href="/search/cs?searchtype=author&query=Mutsvairo%2C+B">Bruce Mutsvairo</a>, 
<a href="/search/cs?searchtype=author&query=Orgeret%2C+K+S">Kristin Skare Orgeret</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13534" title="Abstract">arXiv:2304.13534</a> (replaced) [<a href="/pdf/2304.13534" title="Download PDF">pdf</a>, <a href="/format/2304.13534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mean-field games laboratory for generative modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+B+J">Benjamin J. Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Katsoulakis%2C+M+A">Markos A. Katsoulakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 56 pages, 10 figures. Version 5 has a slightly modified version of the normalizing flow and improved introduction and conclusions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14232" title="Abstract">arXiv:2304.14232</a> (replaced) [<a href="/pdf/2304.14232" title="Download PDF">pdf</a>, <a href="/format/2304.14232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of Reconfigurable Intelligent Surfaces for Wireless  Communication: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xiong%2C+R">Rujing Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jianan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+F">Fuhai Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhengyu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+X">Xiang Ren</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Junshuo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+J">Jialong Lu</a>, 
<a href="/search/eess?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/eess?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+R+C">Robert Caiming Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00586" title="Abstract">arXiv:2305.00586</a> (replaced) [<a href="/pdf/2305.00586" title="Download PDF">pdf</a>, <a href="/format/2305.00586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How does GPT-2 compute greater-than?: Interpreting mathematical  abilities in a pre-trained language model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanna%2C+M">Michael Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+O">Ollie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Variengien%2C+A">Alexandre Variengien</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Camera Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01278" title="Abstract">arXiv:2305.01278</a> (replaced) [<a href="/pdf/2305.01278" title="Download PDF">pdf</a>, <a href="/format/2305.01278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VPGTrans: Transfer Visual Prompt Generator across LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Ao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+H">Hao Fei</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+W">Wei Ji</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Website: <a href="https://vpgtrans.github.io">this https URL</a> Code: <a href="https://github.com/VPGTrans/VPGTrans">this https URL</a> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03635" title="Abstract">arXiv:2305.03635</a> (replaced) [<a href="/pdf/2305.03635" title="Download PDF">pdf</a>, <a href="/ps/2305.03635" title="Download PostScript">ps</a>, <a href="/format/2305.03635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On MSR Subspace Families of Lines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ihringer%2C+F">Ferdinand Ihringer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages; various editing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT); Group Theory (math.GR)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06659" title="Abstract">arXiv:2305.06659</a> (replaced) [<a href="/pdf/2305.06659" title="Download PDF">pdf</a>, <a href="/format/2305.06659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Algorithms for Bounded Weighted Edit Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cassis%2C+A">Alejandro Cassis</a>, 
<a href="/search/cs?searchtype=author&query=Kociumaka%2C+T">Tomasz Kociumaka</a>, 
<a href="/search/cs?searchtype=author&query=Wellnitz%2C+P">Philip Wellnitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Shortened abstract for arXiv. Accepted to FOCS'23. Version 2: minor fixes and aesthetic changes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07644" title="Abstract">arXiv:2305.07644</a> (replaced) [<a href="/pdf/2305.07644" title="Download PDF">pdf</a>, <a href="/format/2305.07644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beware of diffusion models for synthesizing medical images -- A  comparison with GANs in terms of memorizing brain MRI and chest x-ray images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Akbar%2C+M+U">Muhammad Usman Akbar</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+W">Wuhao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Eklund%2C+A">Anders Eklund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 Pages, 6 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08078" title="Abstract">arXiv:2305.08078</a> (replaced) [<a href="/pdf/2305.08078" title="Download PDF">pdf</a>, <a href="/format/2305.08078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised Domain Adaptation for Recognizing Retinal Diseases from  Wide-Field Fundus Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wei%2C+Q">Qijie Wei</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+J">Jingyuan Yang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jinrui Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+J">Jianchun Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+X">Xinyu Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+S">Sheng Yang</a>, 
<a href="/search/eess?searchtype=author&query=Manivannan%2C+N">Niranchana Manivannan</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Youxin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Ding%2C+D">Dayong Ding</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jing Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xirong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by BIBM2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08546" title="Abstract">arXiv:2305.08546</a> (replaced) [<a href="/pdf/2305.08546" title="Download PDF">pdf</a>, <a href="/format/2305.08546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Visual Saliency Explanations of Face Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuhang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zewei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+T">Touradj Ebrahimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09836" title="Abstract">arXiv:2305.09836</a> (replaced) [<a href="/pdf/2305.09836" title="Download PDF">pdf</a>, <a href="/format/2305.09836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Minimalist Approach to Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarasov%2C+D">Denis Tarasov</a>, 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+V">Vladislav Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+S">Sergey Kolesnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source code: <a href="https://github.com/DT6A/ReBRAC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09857" title="Abstract">arXiv:2305.09857</a> (replaced) [<a href="/pdf/2305.09857" title="Download PDF">pdf</a>, <a href="/format/2305.09857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoEdIT: Text Editing by Task-Specific Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raheja%2C+V">Vipul Raheja</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+R">Ryan Koo</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Findings). 18 pages, 13 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10387" title="Abstract">arXiv:2305.10387</a> (replaced) [<a href="/pdf/2305.10387" title="Download PDF">pdf</a>, <a href="/format/2305.10387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elaborative Simplification as Implicit Questions Under Discussion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yating Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sheffield%2C+W">William Sheffield</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Equal contribution by Yating Wu and William Sheffield. This the EMNLP 2023 Main camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12001" title="Abstract">arXiv:2305.12001</a> (replaced) [<a href="/pdf/2305.12001" title="Download PDF">pdf</a>, <a href="/format/2305.12001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OPT-R: Exploring the Role of Explanations in Finetuning and Prompting  for Reasoning Skills of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlKhamissi%2C+B">Badr AlKhamissi</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+S">Siddharth Verma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Ping Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhijing Jin</a>, 
<a href="/search/cs?searchtype=author&query=Celikyilmaz%2C+A">Asli Celikyilmaz</a>, 
<a href="/search/cs?searchtype=author&query=Diab%2C+M">Mona Diab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE) at ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12920" title="Abstract">arXiv:2305.12920</a> (replaced) [<a href="/pdf/2305.12920" title="Download PDF">pdf</a>, <a href="/format/2305.12920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and  Why?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pramanick%2C+A">Aniket Pramanick</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yufang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12962" title="Abstract">arXiv:2305.12962</a> (replaced) [<a href="/pdf/2305.12962" title="Download PDF">pdf</a>, <a href="/format/2305.12962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling ChatGPT for Explainable Automated Student Answer Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuxiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+D">David West</a>, 
<a href="/search/cs?searchtype=author&query=Aloisi%2C+C">Cesare Aloisi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13034" title="Abstract">arXiv:2305.13034</a> (replaced) [<a href="/pdf/2305.13034" title="Download PDF">pdf</a>, <a href="/format/2305.13034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearest Neighbor Machine Translation is Meta-Optimizer on Output  Projection Layer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Ruize Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhirui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yichao Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13035" title="Abstract">arXiv:2305.13035</a> (replaced) [<a href="/pdf/2305.13035" title="Download PDF">pdf</a>, <a href="/format/2305.13035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alabdulmohsin%2C+I">Ibrahim Alabdulmohsin</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaohua Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+A">Alexander Kolesnikov</a>, 
<a href="/search/cs?searchtype=author&query=Beyer%2C+L">Lucas Beyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, 9 tables. Version 2: Layout fixes
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13040" title="Abstract">arXiv:2305.13040</a> (replaced) [<a href="/pdf/2305.13040" title="Download PDF">pdf</a>, <a href="/format/2305.13040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented  Dialogue Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Shuzheng Si</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wentao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Haoyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuchuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Ting-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yinpei Dai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hangyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13245" title="Abstract">arXiv:2305.13245</a> (replaced) [<a href="/pdf/2305.13245" title="Download PDF">pdf</a>, <a href="/format/2305.13245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GQA: Training Generalized Multi-Query Transformer Models from Multi-Head  Checkpoints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ainslie%2C+J">Joshua Ainslie</a>, 
<a href="/search/cs?searchtype=author&query=Lee-Thorp%2C+J">James Lee-Thorp</a>, 
<a href="/search/cs?searchtype=author&query=de+Jong%2C+M">Michiel de Jong</a>, 
<a href="/search/cs?searchtype=author&query=Zemlyanskiy%2C+Y">Yury Zemlyanskiy</a>, 
<a href="/search/cs?searchtype=author&query=Lebr%C3%B3n%2C+F">Federico Lebr&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Sanghai%2C+S">Sumit Sanghai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13316" title="Abstract">arXiv:2305.13316</a> (replaced) [<a href="/pdf/2305.13316" title="Download PDF">pdf</a>, <a href="/format/2305.13316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KineticNet: Deep learning a transferable kinetic energy functional for  orbital-free density functional theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Remme%2C+R">Roman Remme</a>, 
<a href="/search/physics?searchtype=author&query=Kaczun%2C+T">Tobias Kaczun</a>, 
<a href="/search/physics?searchtype=author&query=Scheurer%2C+M">Maximilian Scheurer</a>, 
<a href="/search/physics?searchtype=author&query=Dreuw%2C+A">Andreas Dreuw</a>, 
<a href="/search/physics?searchtype=author&query=Hamprecht%2C+F+A">Fred A. Hamprecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article may be downloaded for personal use only. Any other use requires prior permission of the author and AIP Publishing. This article appeared in The Journal of Chemical Physics 159, 144113 (2023) and may be found at <a href="https://doi.org/10.1063/5.0158275">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Journal of Chemical Physics 159, 144113 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13627" title="Abstract">arXiv:2305.13627</a> (replaced) [<a href="/pdf/2305.13627" title="Download PDF">pdf</a>, <a href="/format/2305.13627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructAlign: High-and-Low Resource Language Alignment via Continual  Crosslingual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cahyawijaya%2C+S">Samuel Cahyawijaya</a>, 
<a href="/search/cs?searchtype=author&query=Lovenia%2C+H">Holy Lovenia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tiezheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+W">Willy Chung</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+P">Pascale Fung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13658" title="Abstract">arXiv:2305.13658</a> (replaced) [<a href="/pdf/2305.13658" title="Download PDF">pdf</a>, <a href="/format/2305.13658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Compositional Data Augmentation in Typologically Diverse  Morphological Inflection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Samir%2C+F">Farhan Samir</a>, 
<a href="/search/cs?searchtype=author&query=Silfverberg%2C+M">Miikka Silfverberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13751" title="Abstract">arXiv:2305.13751</a> (replaced) [<a href="/pdf/2305.13751" title="Download PDF">pdf</a>, <a href="/format/2305.13751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges in Context-Aware Neural Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Linghao Jin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jacqueline He</a>, 
<a href="/search/cs?searchtype=author&query=May%2C+J">Jonathan May</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuezhe Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13829" title="Abstract">arXiv:2305.13829</a> (replaced) [<a href="/pdf/2305.13829" title="Download PDF">pdf</a>, <a href="/format/2305.13829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Mistakes via Cooperative Study Assistant for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Danqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13981" title="Abstract">arXiv:2305.13981</a> (replaced) [<a href="/pdf/2305.13981" title="Download PDF">pdf</a>, <a href="/format/2305.13981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preserving Knowledge Invariance: Rethinking Robustness Evaluation of  Open Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuchun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaozhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+K">Kaisheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jifan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiuding Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Lei Hou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13998" title="Abstract">arXiv:2305.13998</a> (replaced) [<a href="/pdf/2305.13998" title="Download PDF">pdf</a>, <a href="/format/2305.13998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and  Mixed Variables Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saves%2C+P">Paul Saves</a>, 
<a href="/search/cs?searchtype=author&query=Lafage%2C+R">Remi Lafage</a>, 
<a href="/search/cs?searchtype=author&query=Bartoli%2C+N">Nathalie Bartoli</a>, 
<a href="/search/cs?searchtype=author&query=Diouane%2C+Y">Youssef Diouane</a>, 
<a href="/search/cs?searchtype=author&query=Bussemaker%2C+J">Jasper Bussemaker</a>, 
<a href="/search/cs?searchtype=author&query=Lefebvre%2C+T">Thierry Lefebvre</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J+T">John T. Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Morlier%2C+J">Joseph Morlier</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+J+R+R+A">Joaquim R. R. A. Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> version 3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13999" title="Abstract">arXiv:2305.13999</a> (replaced) [<a href="/pdf/2305.13999" title="Download PDF">pdf</a>, <a href="/format/2305.13999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards A Unified View of Sparse Feed-Forward Network in Pretraining  Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z+L">Zeyu Leo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dettmers%2C+T">Tim Dettmers</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X+V">Xi Victoria Lin</a>, 
<a href="/search/cs?searchtype=author&query=Stoyanov%2C+V">Veselin Stoyanov</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xian Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14280" title="Abstract">arXiv:2305.14280</a> (replaced) [<a href="/pdf/2305.14280" title="Download PDF">pdf</a>, <a href="/format/2305.14280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual Pixel Representations for Translation and Effective  Cross-lingual Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salesky%2C+E">Elizabeth Salesky</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+N">Neha Verma</a>, 
<a href="/search/cs?searchtype=author&query=Koehn%2C+P">Philipp Koehn</a>, 
<a href="/search/cs?searchtype=author&query=Post%2C+M">Matt Post</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14616" title="Abstract">arXiv:2305.14616</a> (replaced) [<a href="/pdf/2305.14616" title="Download PDF">pdf</a>, <a href="/format/2305.14616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Affordance and Situated Meaning in Image Captions: A  Multimodal Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Er Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P+A">Po-Ya Angela Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+H">Hsin-Yu Chou</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+Y">Yu-Hsiang Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+S">Shu-Kai Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14843" title="Abstract">arXiv:2305.14843</a> (replaced) [<a href="/pdf/2305.14843" title="Download PDF">pdf</a>, <a href="/format/2305.14843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-learning For Vision-and-language Cross-lingual Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hanxu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Keller%2C+F">Frank Keller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MRL2023 (co-located with EMNLP2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14878" title="Abstract">arXiv:2305.14878</a> (replaced) [<a href="/pdf/2305.14878" title="Download PDF">pdf</a>, <a href="/format/2305.14878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging GPT-4 for Automatic Translation Post-Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raunak%2C+V">Vikas Raunak</a>, 
<a href="/search/cs?searchtype=author&query=Sharaf%2C+A">Amr Sharaf</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Awadallah%2C+H+H">Hany Hassan Awadallah</a>, 
<a href="/search/cs?searchtype=author&query=Menezes%2C+A">Arul Menezes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14879" title="Abstract">arXiv:2305.14879</a> (replaced) [<a href="/pdf/2305.14879" title="Download PDF">pdf</a>, <a href="/format/2305.14879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ByteSized32: A Corpus and Challenge Task for Generating Task-Specific  World Models Expressed as Text Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruoyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Todd%2C+G">Graham Todd</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+E">Eric Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Ziang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%B4t%C3%A9%2C+M">Marc-Alexandre C&#xf4;t&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+P">Peter Jansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14934" title="Abstract">arXiv:2305.14934</a> (replaced) [<a href="/pdf/2305.14934" title="Download PDF">pdf</a>, <a href="/format/2305.14934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRACE: Discriminator-Guided Chain-of-Thought Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalifa%2C+M">Muhammad Khalifa</a>, 
<a href="/search/cs?searchtype=author&query=Logeswaran%2C+L">Lajanugen Logeswaran</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Moontae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Honglak Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14975" title="Abstract">arXiv:2305.14975</a> (replaced) [<a href="/pdf/2305.14975" title="Download PDF">pdf</a>, <a href="/format/2305.14975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence  Scores from Language Models Fine-Tuned with Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+K">Katherine Tian</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Allan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Archit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Rafailov%2C+R">Rafael Rafailov</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C+D">Christopher D. Manning</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14989" title="Abstract">arXiv:2305.14989</a> (replaced) [<a href="/pdf/2305.14989" title="Download PDF">pdf</a>, <a href="/format/2305.14989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dolphin: A Challenging and Diverse Benchmark for Arabic NLG
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Elmadany%2C+A">AbdelRahim Elmadany</a>, 
<a href="/search/cs?searchtype=author&query=El-Shangiti%2C+A">Ahmed El-Shangiti</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14996" title="Abstract">arXiv:2305.14996</a> (replaced) [<a href="/pdf/2305.14996" title="Download PDF">pdf</a>, <a href="/format/2305.14996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The ACL OCL Corpus: Advancing Open Science in Computational Linguistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rohatgi%2C+S">Shaurya Rohatgi</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yanxia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Aw%2C+B">Benjamin Aw</a>, 
<a href="/search/cs?searchtype=author&query=Unnithan%2C+N">Niranjana Unnithan</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+M">Min-Yen Kan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Digital Libraries (cs.DL)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15023" title="Abstract">arXiv:2305.15023</a> (replaced) [<a href="/pdf/2305.15023" title="Download PDF">pdf</a>, <a href="/format/2305.15023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+G">Gen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+T">Tianhe Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shengxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongrong Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15025" title="Abstract">arXiv:2305.15025</a> (replaced) [<a href="/pdf/2305.15025" title="Download PDF">pdf</a>, <a href="/format/2305.15025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dior-CVAE: Pre-trained Language Models and Diffusion Priors for  Variational Dialog Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T+T">Thy Thy Tran</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15068" title="Abstract">arXiv:2305.15068</a> (replaced) [<a href="/pdf/2305.15068" title="Download PDF">pdf</a>, <a href="/format/2305.15068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks  for Exploring Theory of Mind
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaomeng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Lingyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qihui Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoNLL 2023 camera ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15077" title="Abstract">arXiv:2305.15077</a> (replaced) [<a href="/pdf/2305.15077" title="Download PDF">pdf</a>, <a href="/format/2305.15077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Learning of Sentence Embeddings from Scratch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junlei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Emnlp 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15110" title="Abstract">arXiv:2305.15110</a> (replaced) [<a href="/pdf/2305.15110" title="Download PDF">pdf</a>, <a href="/format/2305.15110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intersection of Longest Cycle and Largest Bond in 3-Connected Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ren%2C+E">Emily Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 19 figures. Paper presented at the 54th Southeastern International Conference on Combinatorics, Graph Theory and Computing (March 6-10, 2023); submitted on May 9, 2023 to the conference proceedings book series publication titled "Springer Proceedings in Mathematics and Statistics" (PROMS). Paper abstract also on <a href="https://www.math.fau.edu/combinatorics/abstracts/ren54.pdf">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15901" title="Abstract">arXiv:2305.15901</a> (replaced) [<a href="/pdf/2305.15901" title="Download PDF">pdf</a>, <a href="/format/2305.15901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent Optimal Transport with Empirical Conditional Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manupriya%2C+P">Piyushi Manupriya</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+R+K">Rachit Keerti Das</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sayantan Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Jagarlapudi%2C+S+N">Saketha Nath Jagarlapudi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16339" title="Abstract">arXiv:2305.16339</a> (replaced) [<a href="/pdf/2305.16339" title="Download PDF">pdf</a>, <a href="/format/2305.16339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Trust ChatGPT when Your Question is not in English: A Study of  Multilingual Abilities and Types of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Senyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Hauer%2C+B">Bradley Hauer</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+N">Ning Shi</a>, 
<a href="/search/cs?searchtype=author&query=Kondrak%2C+G">Grzegorz Kondrak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17297" title="Abstract">arXiv:2305.17297</a> (replaced) [<a href="/pdf/2305.17297" title="Download PDF">pdf</a>, <a href="/format/2305.17297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Low-Rank Data Under Distribution Shift: Double Descent and  Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kausik%2C+C">Chinmaya Kausik</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+K">Kashvi Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Sonthalia%2C+R">Rishi Sonthalia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Complete overhaul of presentation, many new results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02602" title="Abstract">arXiv:2306.02602</a> (replaced) [<a href="/pdf/2306.02602" title="Download PDF">pdf</a>, <a href="/format/2306.02602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReContrast: Domain-Specific Anomaly Detection via Contrastive  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jia Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shuai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+L">Lize Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weihang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huiqi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03552" title="Abstract">arXiv:2306.03552</a> (replaced) [<a href="/pdf/2306.03552" title="Download PDF">pdf</a>, <a href="/format/2306.03552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State Regularized Policy Optimization on Data with Dynamics Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhenghai Xue</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Q">Qingpeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+D">Dong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bo An</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04712" title="Abstract">arXiv:2306.04712</a> (replaced) [<a href="/pdf/2306.04712" title="Download PDF">pdf</a>, <a href="/format/2306.04712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentiable Earth Mover&#x27;s Distance for Data Compression at the  High-Luminosity LHC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Shenoy%2C+R">Rohan Shenoy</a>, 
<a href="/search/hep-ex?searchtype=author&query=Duarte%2C+J">Javier Duarte</a>, 
<a href="/search/hep-ex?searchtype=author&query=Herwig%2C+C">Christian Herwig</a>, 
<a href="/search/hep-ex?searchtype=author&query=Hirschauer%2C+J">James Hirschauer</a>, 
<a href="/search/hep-ex?searchtype=author&query=Noonan%2C+D">Daniel Noonan</a>, 
<a href="/search/hep-ex?searchtype=author&query=Pierini%2C+M">Maurizio Pierini</a>, 
<a href="/search/hep-ex?searchtype=author&query=Tran%2C+N">Nhan Tran</a>, 
<a href="/search/hep-ex?searchtype=author&query=Suarez%2C+C+M">Cristina Mantilla Suarez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures, submitted to Machine Learning: Science and Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Machine Learning (cs.LG); Instrumentation and Detectors (physics.ins-det)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05131" title="Abstract">arXiv:2306.05131</a> (replaced) [<a href="/pdf/2306.05131" title="Download PDF">pdf</a>, <a href="/format/2306.05131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Prediction for Federated Uncertainty Quantification Under  Label Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Plassier%2C+V">Vincent Plassier</a>, 
<a href="/search/stat?searchtype=author&query=Makni%2C+M">Mehdi Makni</a>, 
<a href="/search/stat?searchtype=author&query=Rubashevskii%2C+A">Aleksandr Rubashevskii</a>, 
<a href="/search/stat?searchtype=author&query=Moulines%2C+E">Eric Moulines</a>, 
<a href="/search/stat?searchtype=author&query=Panov%2C+M">Maxim Panov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06721" title="Abstract">arXiv:2306.06721</a> (replaced) [<a href="/pdf/2306.06721" title="Download PDF">pdf</a>, <a href="/format/2306.06721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Conditional Independence Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kalemaj%2C+I">Iden Kalemaj</a>, 
<a href="/search/stat?searchtype=author&query=Kasiviswanathan%2C+S+P">Shiva Prasad Kasiviswanathan</a>, 
<a href="/search/stat?searchtype=author&query=Ramdas%2C+A">Aaditya Ramdas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09101" title="Abstract">arXiv:2306.09101</a> (replaced) [<a href="/pdf/2306.09101" title="Download PDF">pdf</a>, <a href="/format/2306.09101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-aided Wireless Image Transmission with Channel Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haotian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yulin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ozfatura%2C+E">Emre Ozfatura</a>, 
<a href="/search/cs?searchtype=author&query=Mikolajczyk%2C+K">Krystian Mikolajczyk</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnd%C3%BCz%2C+D">Deniz G&#xfc;nd&#xfc;z</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09347" title="Abstract">arXiv:2306.09347</a> (replaced) [<a href="/pdf/2306.09347" title="Download PDF">pdf</a>, <a href="/format/2306.09347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segment Any Point Cloud Sequences by Distilling Vision Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Youquan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingdong Kong</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+J">Jun Cen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Runnan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liang Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 (Spotlight); 37 pages, 16 figures, 15 tables; Code at <a href="https://github.com/youquanl/Segment-Any-Point-Cloud">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10912" title="Abstract">arXiv:2306.10912</a> (replaced) [<a href="/pdf/2306.10912" title="Download PDF">pdf</a>, <a href="/format/2306.10912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jamming Detection in Low-BER Mobile Indoor Scenarios via Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sciancalepore%2C+S">Savio Sciancalepore</a>, 
<a href="/search/cs?searchtype=author&query=Kusters%2C+F">Fabrice Kusters</a>, 
<a href="/search/cs?searchtype=author&query=Abdelhadi%2C+N+K">Nada Khaled Abdelhadi</a>, 
<a href="/search/cs?searchtype=author&query=Oligeri%2C+G">Gabriele Oligeri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures; Submitted and under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10933" title="Abstract">arXiv:2306.10933</a> (replaced) [<a href="/pdf/2306.10933" title="Download PDF">pdf</a>, <a href="/format/2306.10933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Open-World Recommendation with Knowledge Augmentation from Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xi%2C+Y">Yunjia Xi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jieming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11768" title="Abstract">arXiv:2306.11768</a> (replaced) [<a href="/pdf/2306.11768" title="Download PDF">pdf</a>, <a href="/format/2306.11768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Survey in Geometric Deep Learning for Structure-based Drug  Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Zaixi Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Yan%2C+J">Jiaxian Yan</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+E">Enhong Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Zitnik%2C+M">Marinka Zitnik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14846" title="Abstract">arXiv:2306.14846</a> (replaced) [<a href="/pdf/2306.14846" title="Download PDF">pdf</a>, <a href="/format/2306.14846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViNT: A Foundation Model for Visual Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhruv Shah</a>, 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A">Ajay Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Dashora%2C+N">Nitish Dashora</a>, 
<a href="/search/cs?searchtype=author&query=Stachowicz%2C+K">Kyle Stachowicz</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+K">Kevin Black</a>, 
<a href="/search/cs?searchtype=author&query=Hirose%2C+N">Noriaki Hirose</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for oral presentation at CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15359" title="Abstract">arXiv:2306.15359</a> (replaced) [<a href="/pdf/2306.15359" title="Download PDF">pdf</a>, <a href="/ps/2306.15359" title="Download PostScript">ps</a>, <a href="/format/2306.15359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix equation representation of the convolution equation and its  unique solvability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Satake%2C+Y">Yuki Satake</a>, 
<a href="/search/math?searchtype=author&query=Sogabe%2C+T">Tomohiro Sogabe</a>, 
<a href="/search/math?searchtype=author&query=Kemmochi%2C+T">Tomoya Kemmochi</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+S">Shao-Liang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16163" title="Abstract">arXiv:2306.16163</a> (replaced) [<a href="/pdf/2306.16163" title="Download PDF">pdf</a>, <a href="/format/2306.16163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Turbo: Effective Caching in Differentially-Private Databases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kostopoulou%2C+K">Kelly Kostopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Tholoniat%2C+P">Pierre Tholoniat</a>, 
<a href="/search/cs?searchtype=author&query=Cidon%2C+A">Asaf Cidon</a>, 
<a href="/search/cs?searchtype=author&query=Geambasu%2C+R">Roxana Geambasu</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A9cuyer%2C+M">Mathias L&#xe9;cuyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of a paper presented at the 29th ACM Symposium on Operating Systems Principles (SOSP '23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16635" title="Abstract">arXiv:2306.16635</a> (replaced) [<a href="/pdf/2306.16635" title="Download PDF">pdf</a>, <a href="/format/2306.16635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Fairness in Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+Y">Yan Ju</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+S">Shan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G+H">George H. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02680" title="Abstract">arXiv:2307.02680</a> (replaced) [<a href="/pdf/2307.02680" title="Download PDF">pdf</a>, <a href="/format/2307.02680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulating Cardiac Fluid Dynamics in the Human Heart
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Davey%2C+M">Marshall Davey</a>, 
<a href="/search/q-bio?searchtype=author&query=Puelz%2C+C">Charles Puelz</a>, 
<a href="/search/q-bio?searchtype=author&query=Rossi%2C+S">Simone Rossi</a>, 
<a href="/search/q-bio?searchtype=author&query=Smith%2C+M+A">Margaret Anne Smith</a>, 
<a href="/search/q-bio?searchtype=author&query=Wells%2C+D+R">David R. Wells</a>, 
<a href="/search/q-bio?searchtype=author&query=Sturgeon%2C+G">Greg Sturgeon</a>, 
<a href="/search/q-bio?searchtype=author&query=Segars%2C+W+P">W. Paul Segars</a>, 
<a href="/search/q-bio?searchtype=author&query=Vavalle%2C+J+P">John P. Vavalle</a>, 
<a href="/search/q-bio?searchtype=author&query=Peskin%2C+C+S">Charles S. Peskin</a>, 
<a href="/search/q-bio?searchtype=author&query=Griffith%2C+B+E">Boyce E. Griffith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Tissues and Organs (q-bio.TO)</span>; Numerical Analysis (math.NA); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03217" title="Abstract">arXiv:2307.03217</a> (replaced) [<a href="/pdf/2307.03217" title="Download PDF">pdf</a>, <a href="/format/2307.03217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantification of Uncertainty with Adversarial Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schweighofer%2C+K">Kajetan Schweighofer</a>, 
<a href="/search/cs?searchtype=author&query=Aichberger%2C+L">Lukas Aichberger</a>, 
<a href="/search/cs?searchtype=author&query=Ielanskyi%2C+M">Mykyta Ielanskyi</a>, 
<a href="/search/cs?searchtype=author&query=Klambauer%2C+G">G&#xfc;nter Klambauer</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03838" title="Abstract">arXiv:2307.03838</a> (replaced) [<a href="/pdf/2307.03838" title="Download PDF">pdf</a>, <a href="/format/2307.03838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RADAR: Robust AI-Text Detection via Adversarial Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaomeng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T">Tsung-Yi Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023. Project page and demos: <a href="https://radar.vizhub.ai">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07227" title="Abstract">arXiv:2307.07227</a> (replaced) [<a href="/pdf/2307.07227" title="Download PDF">pdf</a>, <a href="/ps/2307.07227" title="Download PostScript">ps</a>, <a href="/format/2307.07227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Short-Packet Communications via UAV-Enabled Mobile Relaying:  Joint Resource Optimization and 3D Trajectory Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mamaghani%2C+M+T">Milad Tatar Mamaghani</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiangyun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Swindlehurst%2C+A+L">A. Lee Swindlehurst</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08603" title="Abstract">arXiv:2307.08603</a> (replaced) [<a href="/pdf/2307.08603" title="Download PDF">pdf</a>, <a href="/format/2307.08603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing hyperparameter dependence by external timescale tailoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Jaurigue%2C+L+C">Lina C. Jaurigue</a>, 
<a href="/search/physics?searchtype=author&query=L%C3%BCdge%2C+K">Kathy L&#xfc;dge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09302" title="Abstract">arXiv:2307.09302</a> (replaced) [<a href="/pdf/2307.09302" title="Download PDF">pdf</a>, <a href="/format/2307.09302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal prediction under ambiguous ground truth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stutz%2C+D">David Stutz</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+A+G">Abhijit Guha Roy</a>, 
<a href="/search/cs?searchtype=author&query=Matejovicova%2C+T">Tatiana Matejovicova</a>, 
<a href="/search/cs?searchtype=author&query=Strachan%2C+P">Patricia Strachan</a>, 
<a href="/search/cs?searchtype=author&query=Cemgil%2C+A+T">Ali Taylan Cemgil</a>, 
<a href="/search/cs?searchtype=author&query=Doucet%2C+A">Arnaud Doucet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10878" title="Abstract">arXiv:2307.10878</a> (replaced) [<a href="/pdf/2307.10878" title="Download PDF">pdf</a>, <a href="/ps/2307.10878" title="Download PostScript">ps</a>, <a href="/format/2307.10878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mempool Privacy: An Economic Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rondelet%2C+A">Antoine Rondelet</a>, 
<a href="/search/cs?searchtype=author&query=Kilbourn%2C+Q">Quintus Kilbourn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11030" title="Abstract">arXiv:2307.11030</a> (replaced) [<a href="/pdf/2307.11030" title="Download PDF">pdf</a>, <a href="/format/2307.11030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cluster-aware Semi-supervised Learning: Relational Knowledge  Distillation Provably Learns Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dong%2C+Y">Yijun Dong</a>, 
<a href="/search/stat?searchtype=author&query=Miller%2C+K">Kevin Miller</a>, 
<a href="/search/stat?searchtype=author&query=Lei%2C+Q">Qi Lei</a>, 
<a href="/search/stat?searchtype=author&query=Ward%2C+R">Rachel Ward</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12388" title="Abstract">arXiv:2307.12388</a> (replaced) [<a href="/pdf/2307.12388" title="Download PDF">pdf</a>, <a href="/format/2307.12388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-aware Grounded Action Transformation towards Sim-to-Real  Transfer for Traffic Signal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da%2C+L">Longchao Da</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Hao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Romir Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hua Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures. This paper is accepted by IEEE-CDC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13854" title="Abstract">arXiv:2307.13854</a> (replaced) [<a href="/pdf/2307.13854" title="Download PDF">pdf</a>, <a href="/format/2307.13854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WebArena: A Realistic Web Environment for Building Autonomous Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shuyan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F+F">Frank F. Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+R">Robert Lo</a>, 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+A">Abishek Sridhar</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xianyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+T">Tianyue Ou</a>, 
<a href="/search/cs?searchtype=author&query=Bisk%2C+Y">Yonatan Bisk</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code, data, environment reproduction resources, and video demonstrations are publicly available at <a href="https://webarena.dev/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13885" title="Abstract">arXiv:2307.13885</a> (replaced) [<a href="/pdf/2307.13885" title="Download PDF">pdf</a>, <a href="/format/2307.13885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Estimation of Average-Case Robustness for Multi-Class  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tessa Han</a>, 
<a href="/search/cs?searchtype=author&query=Srinivas%2C+S">Suraj Srinivas</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14105" title="Abstract">arXiv:2307.14105</a> (replaced) [<a href="/pdf/2307.14105" title="Download PDF">pdf</a>, <a href="/ps/2307.14105" title="Download PostScript">ps</a>, <a href="/format/2307.14105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Robot Vision for Distant Object Change Detection: A Lightweight  Training Simulator Inspired by Multi-Armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Terashima%2C+K">Kouki Terashima</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+K">Kanji Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Yamamoto%2C+R">Ryogo Yamamoto</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J+T+Y">Jonathan Tay Yu Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14387" title="Abstract">arXiv:2307.14387</a> (replaced) [<a href="/pdf/2307.14387" title="Download PDF">pdf</a>, <a href="/format/2307.14387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coupled-Space Attacks against Random-Walk-based Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yuni Lai</a>, 
<a href="/search/cs?searchtype=author&query=Waniek%2C+M">Marcin Waniek</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liying Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jingwen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yulin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Michalak%2C+T+P">Tomasz P. Michalak</a>, 
<a href="/search/cs?searchtype=author&query=Rahwan%2C+T">Talal Rahwan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kai Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14491" title="Abstract">arXiv:2307.14491</a> (replaced) [<a href="/pdf/2307.14491" title="Download PDF">pdf</a>, <a href="/format/2307.14491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for Modality-Agnostic Deepfakes Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Cai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jiahe Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Jiao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Y">Yesheng Chai</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+S">Shan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jizhong Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14619" title="Abstract">arXiv:2307.14619</a> (replaced) [<a href="/pdf/2307.14619" title="Download PDF">pdf</a>, <a href="/format/2307.14619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level  Stability and High-Level Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Block%2C+A">Adam Block</a>, 
<a href="/search/cs?searchtype=author&query=Jadbabaie%2C+A">Ali Jadbabaie</a>, 
<a href="/search/cs?searchtype=author&query=Pfrommer%2C+D">Daniel Pfrommer</a>, 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated figures, minor notational change for readability
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14642" title="Abstract">arXiv:2307.14642</a> (replaced) [<a href="/pdf/2307.14642" title="Download PDF">pdf</a>, <a href="/ps/2307.14642" title="Download PostScript">ps</a>, <a href="/format/2307.14642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Convergence of Black-Box Variational Inference: Should We Stick  the Landing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kim%2C+K">Kyurae Kim</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+Y">Yian Ma</a>, 
<a href="/search/stat?searchtype=author&query=Gardner%2C+J+R">Jacob R. Gardner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15317" title="Abstract">arXiv:2307.15317</a> (replaced) [<a href="/pdf/2307.15317" title="Download PDF">pdf</a>, <a href="/format/2307.15317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable  Kendall&#x27;s Rank Correlation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaipeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huishuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16335" title="Abstract">arXiv:2307.16335</a> (replaced) [<a href="/pdf/2307.16335" title="Download PDF">pdf</a>, <a href="/format/2307.16335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Approximate Bayesian Optimization Algorithms with Two Mixers and  Uncertainty Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kim%2C+J+E">Jungin E. Kim</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wang%2C+Y">Yan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00664" title="Abstract">arXiv:2308.00664</a> (replaced) [<a href="/pdf/2308.00664" title="Download PDF">pdf</a>, <a href="/format/2308.00664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyDe: A Hybrid PCM/FeFET/SRAM Device-search for Optimizing Area and  Energy-efficiencies in Analog IMC Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Abhiroop Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Moitra%2C+A">Abhishek Moitra</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Priyadarshini Panda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Journal on Emerging and Selected Topics in Circuits and
  Systems (JETCAS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01249" title="Abstract">arXiv:2308.01249</a> (replaced) [<a href="/pdf/2308.01249" title="Download PDF">pdf</a>, <a href="/ps/2308.01249" title="Download PostScript">ps</a>, <a href="/format/2308.01249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spatially Coupled LDPC Coding Scheme with Scalable Decoders for Space  Division Multiplexing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Schmalen%2C+L">Laurent Schmalen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages plus comments, 3 figures, European Conference on Optical Communication (ECOC) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02019" title="Abstract">arXiv:2308.02019</a> (replaced) [<a href="/pdf/2308.02019" title="Download PDF">pdf</a>, <a href="/format/2308.02019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Baby Llama: knowledge distillation from an ensemble of teachers trained  on a small dataset with no performance penalty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Timiryasov%2C+I">Inar Timiryasov</a>, 
<a href="/search/cs?searchtype=author&query=Tastet%2C+J">Jean-Loup Tastet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures, 4 tables, submitted to the BabyLM Challenge and accepted as archival full paper (CoNLL--CMCL 2023 Shared Task), checkpoint available at <a href="https://huggingface.co/timinar/baby-llama-58m">this https URL</a>, training code available at <a href="https://github.com/timinar/BabyLlama">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02490" title="Abstract">arXiv:2308.02490</a> (replaced) [<a href="/pdf/2308.02490" title="Download PDF">pdf</a>, <a href="/format/2308.02490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Weihao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhengyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinchao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijuan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add results of GPT-4V. Code, data and leaderboard: <a href="https://github.com/yuweihao/MM-Vet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03051" title="Abstract">arXiv:2308.03051</a> (replaced) [<a href="/pdf/2308.03051" title="Download PDF">pdf</a>, <a href="/format/2308.03051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten  Arabic Varieties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadaoui%2C+K">Karima Kadaoui</a>, 
<a href="/search/cs?searchtype=author&query=Magdy%2C+S+M">Samar M. Magdy</a>, 
<a href="/search/cs?searchtype=author&query=Waheed%2C+A">Abdul Waheed</a>, 
<a href="/search/cs?searchtype=author&query=Khondaker%2C+M+T+I">Md Tawkat Islam Khondaker</a>, 
<a href="/search/cs?searchtype=author&query=El-Shangiti%2C+A+O">Ahmed Oumar El-Shangiti</a>, 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ArabicNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04600" title="Abstract">arXiv:2308.04600</a> (replaced) [<a href="/pdf/2308.04600" title="Download PDF">pdf</a>, <a href="/format/2308.04600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model of models -- Part 1
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Komarovsky%2C+S">Shimon Komarovsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2301.13556">arXiv:2301.13556</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO); Symbolic Computation (cs.SC)

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05379" title="Abstract">arXiv:2308.05379</a> (replaced) [<a href="/pdf/2308.05379" title="Download PDF">pdf</a>, <a href="/format/2308.05379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Semantics: Learning a Behavior Augmented Relevance Model with  Self-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by CIKM2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07308" title="Abstract">arXiv:2308.07308</a> (replaced) [<a href="/pdf/2308.07308" title="Download PDF">pdf</a>, <a href="/format/2308.07308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phute%2C+M">Mansi Phute</a>, 
<a href="/search/cs?searchtype=author&query=Helbling%2C+A">Alec Helbling</a>, 
<a href="/search/cs?searchtype=author&query=Hull%2C+M">Matthew Hull</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">ShengYun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Szyller%2C+S">Sebastian Szyller</a>, 
<a href="/search/cs?searchtype=author&query=Cornelius%2C+C">Cory Cornelius</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09312" title="Abstract">arXiv:2308.09312</a> (replaced) [<a href="/pdf/2308.09312" title="Download PDF">pdf</a>, <a href="/format/2308.09312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Path Signatures for Seizure Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Haderlein%2C+J+F">Jonas F. Haderlein</a>, 
<a href="/search/stat?searchtype=author&query=Peterson%2C+A+D+H">Andre D. H. Peterson</a>, 
<a href="/search/stat?searchtype=author&query=Eskikand%2C+P+Z">Parvin Zarei Eskikand</a>, 
<a href="/search/stat?searchtype=author&query=Cook%2C+M+J">Mark J. Cook</a>, 
<a href="/search/stat?searchtype=author&query=Burkitt%2C+A+N">Anthony N. Burkitt</a>, 
<a href="/search/stat?searchtype=author&query=Mareels%2C+I+M+Y">Iven M. Y. Mareels</a>, 
<a href="/search/stat?searchtype=author&query=Grayden%2C+D+B">David B. Grayden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10238" title="Abstract">arXiv:2308.10238</a> (replaced) [<a href="/pdf/2308.10238" title="Download PDF">pdf</a>, <a href="/format/2308.10238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thompson Sampling for Real-Valued Combinatorial Pure Exploration of  Multi-Armed Bandit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakamura%2C+S">Shintaro Nakamura</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10637" title="Abstract">arXiv:2308.10637</a> (replaced) [<a href="/pdf/2308.10637" title="Download PDF">pdf</a>, <a href="/ps/2308.10637" title="Download PostScript">ps</a>, <a href="/format/2308.10637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metro Access Network with Convergence of Coherent and Analog RoF Data  Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delmade%2C+A">Amol Delmade</a>, 
<a href="/search/cs?searchtype=author&query=Slyne%2C+F">Frank Slyne</a>, 
<a href="/search/cs?searchtype=author&query=Browning%2C+C">Colm Browning</a>, 
<a href="/search/cs?searchtype=author&query=Barry%2C+D+K+L">Daniel Kilper Liam Barry</a>, 
<a href="/search/cs?searchtype=author&query=Ruffini%2C+M">Marco Ruffini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10817" title="Abstract">arXiv:2308.10817</a> (replaced) [<a href="/pdf/2308.10817" title="Download PDF">pdf</a>, <a href="/ps/2308.10817" title="Download PostScript">ps</a>, <a href="/format/2308.10817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the impossibility of discovering a formula for primes using AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolpakov%2C+A">Alexander Kolpakov</a>, 
<a href="/search/cs?searchtype=author&query=Rocke%2C+A">Aidan Rocke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12947" title="Abstract">arXiv:2308.12947</a> (replaced) [<a href="/pdf/2308.12947" title="Download PDF">pdf</a>, <a href="/ps/2308.12947" title="Download PostScript">ps</a>, <a href="/format/2308.12947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting Distinct Elements Under Person-Level Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knop%2C+A">Alexander Knop</a>, 
<a href="/search/cs?searchtype=author&query=Steinke%2C+T">Thomas Steinke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14080" title="Abstract">arXiv:2308.14080</a> (replaced) [<a href="/pdf/2308.14080" title="Download PDF">pdf</a>, <a href="/format/2308.14080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Global R-linear Convergence of Nesterov&#x27;s Accelerated Gradient  Method with Unknown Strongly Convex Parameter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bao%2C+C">Chenglong Bao</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jiahong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15512" title="Abstract">arXiv:2308.15512</a> (replaced) [<a href="/pdf/2308.15512" title="Download PDF">pdf</a>, <a href="/format/2308.15512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shatter and Gather: Learning Referring Image Segmentation with Text  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongwon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+N">Namyup Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+C">Cuiling Lan</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+S">Suha Kwak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023, Project page: <a href="https://southflame.github.io/sag/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02373" title="Abstract">arXiv:2309.02373</a> (replaced) [<a href="/pdf/2309.02373" title="Download PDF">pdf</a>, <a href="/format/2309.02373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style  Models with Limited Resources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nawrot%2C+P">Piotr Nawrot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at 3rd Workshop for Natural Language Processing Open Source Software
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06067" title="Abstract">arXiv:2309.06067</a> (replaced) [<a href="/pdf/2309.06067" title="Download PDF">pdf</a>, <a href="/ps/2309.06067" title="Download PostScript">ps</a>, <a href="/format/2309.06067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch Implicit Neural Representation for MRI Parallel Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yusheng Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jianan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiling Liu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+Z">Zhihan Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06132" title="Abstract">arXiv:2309.06132</a> (replaced) [<a href="/pdf/2309.06132" title="Download PDF">pdf</a>, <a href="/format/2309.06132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring vagueness and subjectivity in texts: from symbolic to neural  VAGO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Icard%2C+B">Benjamin Icard</a>, 
<a href="/search/cs?searchtype=author&query=Claveau%2C+V">Vincent Claveau</a>, 
<a href="/search/cs?searchtype=author&query=Atemezing%2C+G">Ghislain Atemezing</a>, 
<a href="/search/cs?searchtype=author&query=%C3%89gr%C3%A9%2C+P">Paul &#xc9;gr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper to appear in the Proceedings of the 2023 IEEE International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06782" title="Abstract">arXiv:2309.06782</a> (replaced) [<a href="/pdf/2309.06782" title="Download PDF">pdf</a>, <a href="/format/2309.06782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable neural network models and terascale datasets for particle-flow  reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Pata%2C+J">Joosep Pata</a>, 
<a href="/search/physics?searchtype=author&query=Wulff%2C+E">Eric Wulff</a>, 
<a href="/search/physics?searchtype=author&query=Mokhtar%2C+F">Farouk Mokhtar</a>, 
<a href="/search/physics?searchtype=author&query=Southwick%2C+D">David Southwick</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+M">Mengke Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Girone%2C+M">Maria Girone</a>, 
<a href="/search/physics?searchtype=author&query=Duarte%2C+J">Javier Duarte</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Analysis, Statistics and Probability (physics.data-an)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); Instrumentation and Detectors (physics.ins-det); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07430" title="Abstract">arXiv:2309.07430</a> (replaced) [<a href="/pdf/2309.07430" title="Download PDF">pdf</a>, <a href="/format/2309.07430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clinical Text Summarization: Adapting Large Language Models Can  Outperform Human Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Veen%2C+D">Dave Van Veen</a>, 
<a href="/search/cs?searchtype=author&query=Van+Uden%2C+C">Cara Van Uden</a>, 
<a href="/search/cs?searchtype=author&query=Blankemeier%2C+L">Louis Blankemeier</a>, 
<a href="/search/cs?searchtype=author&query=Delbrouck%2C+J">Jean-Benoit Delbrouck</a>, 
<a href="/search/cs?searchtype=author&query=Aali%2C+A">Asad Aali</a>, 
<a href="/search/cs?searchtype=author&query=Bluethgen%2C+C">Christian Bluethgen</a>, 
<a href="/search/cs?searchtype=author&query=Pareek%2C+A">Anuj Pareek</a>, 
<a href="/search/cs?searchtype=author&query=Polacin%2C+M">Malgorzata Polacin</a>, 
<a href="/search/cs?searchtype=author&query=Reis%2C+E+P">Eduardo Pontes Reis</a>, 
<a href="/search/cs?searchtype=author&query=Seehofnerova%2C+A">Anna Seehofnerova</a>, 
<a href="/search/cs?searchtype=author&query=Rohatgi%2C+N">Nidhi Rohatgi</a>, 
<a href="/search/cs?searchtype=author&query=Hosamani%2C+P">Poonam Hosamani</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+W">William Collins</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+N">Neera Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Langlotz%2C+C+P">Curtis P. Langlotz</a>, 
<a href="/search/cs?searchtype=author&query=Hom%2C+J">Jason Hom</a>, 
<a href="/search/cs?searchtype=author&query=Gatidis%2C+S">Sergios Gatidis</a>, 
<a href="/search/cs?searchtype=author&query=Pauly%2C+J">John Pauly</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+A+S">Akshay S. Chaudhari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 24 figures. Compared to the original, newer versions include minor edits and supplementary additional experiments that reinforce the initial findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08030" title="Abstract">arXiv:2309.08030</a> (replaced) [<a href="/pdf/2309.08030" title="Download PDF">pdf</a>, <a href="/format/2309.08030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised  Features for Audio-Visual Speech Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chou%2C+J">Ju-Chieh Chou</a>, 
<a href="/search/eess?searchtype=author&query=Chien%2C+C">Chung-Ming Chien</a>, 
<a href="/search/eess?searchtype=author&query=Livescu%2C+K">Karen Livescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09404" title="Abstract">arXiv:2309.09404</a> (replaced) [<a href="/pdf/2309.09404" title="Download PDF">pdf</a>, <a href="/format/2309.09404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promoting Research Collaboration with Open Data Driven Team  Recommendation in Response to Call for Proposals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valluru%2C+S+L">Siva Likitha Valluru</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+B">Biplav Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Paladi%2C+S+T">Sai Teja Paladi</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Siwen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+S">Sriraam Natarajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 3 tables, Accepted to The Thirty-Sixth Annual Conference on Innovative Applications of Artificial Intelligence (IAAI/AAAI-24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09736" title="Abstract">arXiv:2309.09736</a> (replaced) [<a href="/pdf/2309.09736" title="Download PDF">pdf</a>, <a href="/format/2309.09736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimization Case Study for solving a Transport Robot Scheduling  Problem on Quantum-Hybrid and Quantum-Inspired Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Leib%2C+D">Dominik Leib</a>, 
<a href="/search/quant-ph?searchtype=author&query=Seidel%2C+T">Tobias Seidel</a>, 
<a href="/search/quant-ph?searchtype=author&query=J%C3%A4ger%2C+S">Sven J&#xe4;ger</a>, 
<a href="/search/quant-ph?searchtype=author&query=Heese%2C+R">Raoul Heese</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jones%2C+C+I">Caitlin Isobel Jones</a>, 
<a href="/search/quant-ph?searchtype=author&query=Awasthi%2C+A">Abhishek Awasthi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Niederle%2C+A">Astrid Niederle</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bortz%2C+M">Michael Bortz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10011" title="Abstract">arXiv:2309.10011</a> (replaced) [<a href="/pdf/2309.10011" title="Download PDF">pdf</a>, <a href="/format/2309.10011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instant Photorealistic Style Transfer: A Lightweight and Adaptive  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+E">Enyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+A">Andrew Feng</a>, 
<a href="/search/cs?searchtype=author&query=Easley%2C+S+J">Scott John Easley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (reference excluded), 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10617" title="Abstract">arXiv:2309.10617</a> (replaced) [<a href="/pdf/2309.10617" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Debris Mass Estimation Model for Autonomous Underwater  Vehicle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+M+S">Mohana Sri S</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S">Swethaa S</a>, 
<a href="/search/cs?searchtype=author&query=Y%2C+A+B+S">Aouthithiye Barathwaj SR Y</a>, 
<a href="/search/cs?searchtype=author&query=CS%2C+S+G">Sai Ganesh CS</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12671" title="Abstract">arXiv:2309.12671</a> (replaced) [<a href="/pdf/2309.12671" title="Download PDF">pdf</a>, <a href="/format/2309.12671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Fine-tune the Model: Unified Model Shift and Model Bias Policy  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junqiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongtu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chen Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12775" title="Abstract">arXiv:2309.12775</a> (replaced) [<a href="/pdf/2309.12775" title="Download PDF">pdf</a>, <a href="/format/2309.12775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Change Driven Generative Semantic Communication Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wanting Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zehui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yanli Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12871" title="Abstract">arXiv:2309.12871</a> (replaced) [<a href="/pdf/2309.12871" title="Download PDF">pdf</a>, <a href="/format/2309.12871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnglE-optimized Text Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xianming Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> update results and add non-STS transfer tasks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13135" title="Abstract">arXiv:2309.13135</a> (replaced) [<a href="/pdf/2309.13135" title="Download PDF">pdf</a>, <a href="/format/2309.13135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Response to Treatment with Deep Learning and Pharmacokinetic  Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Potosnak%2C+W">Willa Potosnak</a>, 
<a href="/search/cs?searchtype=author&query=Challu%2C+C">Cristian Challu</a>, 
<a href="/search/cs?searchtype=author&query=Olivares%2C+K+G">Kin G. Olivares</a>, 
<a href="/search/cs?searchtype=author&query=Dubrawski%2C+A">Artur Dubrawski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13230" title="Abstract">arXiv:2309.13230</a> (replaced) [<a href="/pdf/2309.13230" title="Download PDF">pdf</a>, <a href="/format/2309.13230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unify word-level and span-level tasks: NJUNLP&#x27;s Participation for the  WMT2023 Quality Estimation Shared Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xiang Geng</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhejian Lai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+S">Shimin Tao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13505" title="Abstract">arXiv:2309.13505</a> (replaced) [<a href="/pdf/2309.13505" title="Download PDF">pdf</a>, <a href="/format/2309.13505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rewrite Caption Semantics: Bridging Semantic Gaps for  Language-Supervised Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yun Xing</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jian Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+A">Aoran Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jiahao Nie</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+S">Shao Ling</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023. Code is available at <a href="https://github.com/xing0047/rewrite">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13837" title="Abstract">arXiv:2309.13837</a> (replaced) [<a href="/pdf/2309.13837" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backorder Prediction in Inventory Management: Classification Techniques  and Cost Considerations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sukanya Kundu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14057" title="Abstract">arXiv:2309.14057</a> (replaced) [<a href="/e-print/2309.14057" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Semantic Segmentation by Knowledge Graph Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our description in Chapter 3, Section 3.2 of the paper is too repetitive with the paper "Object detection meets knowledge graphs". There is an error in the description of formula (5) in Section 3.3. And a detailed reasoning process is required for formula (5). Therefore, we wish to request a retraction of the paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14321" title="Abstract">arXiv:2309.14321</a> (replaced) [<a href="/pdf/2309.14321" title="Download PDF">pdf</a>, <a href="/format/2309.14321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lifelong Robot Learning with Human Assisted Language Planners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parakh%2C+M">Meenal Parakh</a>, 
<a href="/search/cs?searchtype=author&query=Fong%2C+A">Alisha Fong</a>, 
<a href="/search/cs?searchtype=author&query=Simeonov%2C+A">Anthony Simeonov</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Abhishek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Pulkit Agrawal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14518" title="Abstract">arXiv:2309.14518</a> (replaced) [<a href="/pdf/2309.14518" title="Download PDF">pdf</a>, <a href="/format/2309.14518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detach-ROCKET: Sequential feature selection for time series  classification with random convolutional kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uribarri%2C+G">Gonzalo Uribarri</a>, 
<a href="/search/cs?searchtype=author&query=Barone%2C+F">Federico Barone</a>, 
<a href="/search/cs?searchtype=author&query=Ansuini%2C+A">Alessio Ansuini</a>, 
<a href="/search/cs?searchtype=author&query=Frans%C3%A9n%2C+E">Erik Frans&#xe9;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14538" title="Abstract">arXiv:2309.14538</a> (replaced) [<a href="/pdf/2309.14538" title="Download PDF">pdf</a>, <a href="/format/2309.14538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Scene Graph Representation for Surgical Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holm%2C+F">Felix Holm</a>, 
<a href="/search/cs?searchtype=author&query=Ghazaei%2C+G">Ghazal Ghazaei</a>, 
<a href="/search/cs?searchtype=author&query=Czempiel%2C+T">Tobias Czempiel</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zsoy%2C+E">Ege &#xd6;zsoy</a>, 
<a href="/search/cs?searchtype=author&query=Saur%2C+S">Stefan Saur</a>, 
<a href="/search/cs?searchtype=author&query=Navab%2C+N">Nassir Navab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14931" title="Abstract">arXiv:2309.14931</a> (replaced) [<a href="/pdf/2309.14931" title="Download PDF">pdf</a>, <a href="/format/2309.14931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interaction-Aware Sampling-Based MPC with Learned Local Goal Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jansma%2C+W">Walter Jansma</a>, 
<a href="/search/cs?searchtype=author&query=Trevisan%2C+E">Elia Trevisan</a>, 
<a href="/search/cs?searchtype=author&query=Serra-G%C3%B3mez%2C+%C3%81">&#xc1;lvaro Serra-G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the 2023 IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15842" title="Abstract">arXiv:2309.15842</a> (replaced) [<a href="/pdf/2309.15842" title="Download PDF">pdf</a>, <a href="/format/2309.15842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting the Signal-Leak Bias in Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Everaert%2C+M+N">Martin Nicolas Everaert</a>, 
<a href="/search/cs?searchtype=author&query=Fitsios%2C+A">Athanasios Fitsios</a>, 
<a href="/search/cs?searchtype=author&query=Bocchio%2C+M">Marco Bocchio</a>, 
<a href="/search/cs?searchtype=author&query=Arpa%2C+S">Sami Arpa</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%BCsstrunk%2C+S">Sabine S&#xfc;sstrunk</a>, 
<a href="/search/cs?searchtype=author&query=Achanta%2C+R">Radhakrishna Achanta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> corrected the author names in reference [24]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16679" title="Abstract">arXiv:2309.16679</a> (replaced) [<a href="/pdf/2309.16679" title="Download PDF">pdf</a>, <a href="/format/2309.16679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Deep Learning and Online Source Sentiment for Financial  Portfolio Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Nousi%2C+P">Paraskevi Nousi</a>, 
<a href="/search/q-fin?searchtype=author&query=Avramelou%2C+L">Loukia Avramelou</a>, 
<a href="/search/q-fin?searchtype=author&query=Rodinos%2C+G">Georgios Rodinos</a>, 
<a href="/search/q-fin?searchtype=author&query=Tzelepi%2C+M">Maria Tzelepi</a>, 
<a href="/search/q-fin?searchtype=author&query=Manousis%2C+T">Theodoros Manousis</a>, 
<a href="/search/q-fin?searchtype=author&query=Tsampazis%2C+K">Konstantinos Tsampazis</a>, 
<a href="/search/q-fin?searchtype=author&query=Stefanidis%2C+K">Kyriakos Stefanidis</a>, 
<a href="/search/q-fin?searchtype=author&query=Spanos%2C+D">Dimitris Spanos</a>, 
<a href="/search/q-fin?searchtype=author&query=Kirtas%2C+M">Manos Kirtas</a>, 
<a href="/search/q-fin?searchtype=author&query=Tosidis%2C+P">Pavlos Tosidis</a>, 
<a href="/search/q-fin?searchtype=author&query=Tsantekidis%2C+A">Avraam Tsantekidis</a>, 
<a href="/search/q-fin?searchtype=author&query=Passalis%2C+N">Nikolaos Passalis</a>, 
<a href="/search/q-fin?searchtype=author&query=Tefas%2C+A">Anastasios Tefas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16730" title="Abstract">arXiv:2309.16730</a> (replaced) [<a href="/pdf/2309.16730" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable machine learning-based prediction model for diabetic  nephropathy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jing-Mei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jun-Tang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+G">Guo-Wei Zong</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhong-Ze Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+L">Lang Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00013" title="Abstract">arXiv:2310.00013</a> (replaced) [<a href="/pdf/2310.00013" title="Download PDF">pdf</a>, <a href="/format/2310.00013" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Communications in Collaborative Perception with Domain  Alignment for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Senkang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhengru Fang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+H">Haonan An</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guowen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianhao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuguang Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01128" title="Abstract">arXiv:2310.01128</a> (replaced) [<a href="/pdf/2310.01128" title="Download PDF">pdf</a>, <a href="/format/2310.01128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling Voice and Content with Self-Supervision for Speaker  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tianchi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+K+A">Kong Aik Lee</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qiongqiong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (main track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01320" title="Abstract">arXiv:2310.01320</a> (replaced) [<a href="/pdf/2310.01320" title="Download PDF">pdf</a>, <a href="/format/2310.01320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avalon&#x27;s Game of Thoughts: Battle Against Deception through Recursive  Contemplation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zilong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Siyuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qisen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+A">Andrew Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaofei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shiji Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02971" title="Abstract">arXiv:2310.02971</a> (replaced) [<a href="/pdf/2310.02971" title="Download PDF">pdf</a>, <a href="/format/2310.02971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Ming-Hsin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+Y">Yun-Ping Lin</a>, 
<a href="/search/eess?searchtype=author&query=Hsu%2C+J+N">Jing Neng Hsu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+P+K">Paul Kuo-Ming Huang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+C">Chien-yu Huang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shang-Wen Li</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03128" title="Abstract">arXiv:2310.03128</a> (replaced) [<a href="/pdf/2310.03128" title="Download PDF">pdf</a>, <a href="/format/2310.03128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaTool Benchmark for Large Language Models: Deciding Whether to Use  Tools and Which to Use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiawen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chenrui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qihui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yixin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03275" title="Abstract">arXiv:2310.03275</a> (replaced) [<a href="/pdf/2310.03275" title="Download PDF">pdf</a>, <a href="/format/2310.03275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power Optimization in Multi-IRS Aided Delay-Constrained IoVT Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chong%2C+B">Baolin Chong</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+H">Hancheng Lu</a>, 
<a href="/search/eess?searchtype=author&query=Qin%2C+L">Langtian Qin</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chenwu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jiasen Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+C+W">Chang Wen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03368" title="Abstract">arXiv:2310.03368</a> (replaced) [<a href="/pdf/2310.03368" title="Download PDF">pdf</a>, <a href="/format/2310.03368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Hallucinations in Chinese Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mozhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junliang He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mianqiu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhangyue Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03388" title="Abstract">arXiv:2310.03388</a> (replaced) [<a href="/pdf/2310.03388" title="Download PDF">pdf</a>, <a href="/format/2310.03388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenPatch: a 3D patchwork for Out-Of-Distribution detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rabino%2C+P">Paolo Rabino</a>, 
<a href="/search/cs?searchtype=author&query=Alliegro%2C+A">Antonio Alliegro</a>, 
<a href="/search/cs?searchtype=author&query=Borlino%2C+F+C">Francesco Cappio Borlino</a>, 
<a href="/search/cs?searchtype=author&query=Tommasi%2C+T">Tatiana Tommasi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05030" title="Abstract">arXiv:2310.05030</a> (replaced) [<a href="/pdf/2310.05030" title="Download PDF">pdf</a>, <a href="/format/2310.05030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as  You May Think -- Introducing AI Detectability Index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+M">Megha Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Tonmoy%2C+S+M+T+I">S.M Towhidul Islam Tonmoy</a>, 
<a href="/search/cs?searchtype=author&query=Zaman%2C+S+M+M">S M Mehedi Zaman</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Krish Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Barman%2C+N+R">Niyar R Barman</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+C">Chandan Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+S">Shreya Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+T">Tanay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+A+P">Amit P. Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Amitava Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05374" title="Abstract">arXiv:2310.05374</a> (replaced) [<a href="/pdf/2310.05374" title="Download PDF">pdf</a>, <a href="/format/2310.05374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving End-to-End Speech Processing by Efficient Text Data  Utilization with Latent Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianqiao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nianzu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+Y+T">Yu Ting Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures, 8 tables, Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05388" title="Abstract">arXiv:2310.05388</a> (replaced) [<a href="/pdf/2310.05388" title="Download PDF">pdf</a>, <a href="/format/2310.05388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GROVE: A Retrieval-augmented Complex Story Generation Framework with A  Forest of Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhihua Wen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhiliang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yanqi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05462" title="Abstract">arXiv:2310.05462</a> (replaced) [<a href="/pdf/2310.05462" title="Download PDF">pdf</a>, <a href="/format/2310.05462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaFuse: Adaptive Medical Image Fusion Based on Spatial-Frequential  Cross Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+X">Xianming Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lihui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zeyu Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Ying Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xingyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yue-min Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06484" title="Abstract">arXiv:2310.06484</a> (replaced) [<a href="/pdf/2310.06484" title="Download PDF">pdf</a>, <a href="/format/2310.06484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory efficient location recommendation through proximity-aware  representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xuan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mingqing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+R">Rui Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hui Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06498" title="Abstract">arXiv:2310.06498</a> (replaced) [<a href="/pdf/2310.06498" title="Download PDF">pdf</a>, <a href="/format/2310.06498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Benchmark and Reverse Validation Method for Passage-level  Hallucination Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shiping Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Renliang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08644" title="Abstract">arXiv:2310.08644</a> (replaced) [<a href="/pdf/2310.08644" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of  Geoscientific Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuan-Heng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H+V">Hoshin V. Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages and 7 figures in the main text. 10 figures, and 10 tables in the supplementary materials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09141" title="Abstract">arXiv:2310.09141</a> (replaced) [<a href="/pdf/2310.09141" title="Download PDF">pdf</a>, <a href="/ps/2310.09141" title="Download PostScript">ps</a>, <a href="/format/2310.09141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PuoBERTa: Training and evaluation of a curated language model for  Setswana
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marivate%2C+V">Vukosi Marivate</a>, 
<a href="/search/cs?searchtype=author&query=Mots%27Oehli%2C+M">Moseli Mots&#x27;Oehli</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+V">Valencia Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Lastrucci%2C+R">Richard Lastrucci</a>, 
<a href="/search/cs?searchtype=author&query=Dzingirai%2C+I">Isheanesu Dzingirai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for SACAIR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09168" title="Abstract">arXiv:2310.09168</a> (replaced) [<a href="/pdf/2310.09168" title="Download PDF">pdf</a>, <a href="/format/2310.09168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through  Active Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fanqi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 (Main Conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09519" title="Abstract">arXiv:2310.09519</a> (replaced) [<a href="/pdf/2310.09519" title="Download PDF">pdf</a>, <a href="/ps/2310.09519" title="Download PostScript">ps</a>, <a href="/format/2310.09519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crowd Modeling and Control via Cooperative Adaptive Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zirui Wan</a>, 
<a href="/search/cs?searchtype=author&query=Sanei%2C+S">Saeid Sanei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been submitted to 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09647" title="Abstract">arXiv:2310.09647</a> (replaced) [<a href="/pdf/2310.09647" title="Download PDF">pdf</a>, <a href="/format/2310.09647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point-DynRF: Point-based Dynamic Radiance Fields from a Monocular Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Byeongjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C">Changick Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09706" title="Abstract">arXiv:2310.09706</a> (replaced) [<a href="/pdf/2310.09706" title="Download PDF">pdf</a>, <a href="/format/2310.09706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaptSSR: Pre-training User Model with Augmentation-Adaptive  Self-Supervised Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuren Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chao Song</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+M">Min Hou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuqing Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhihao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zaixi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S+L">Sanshi Lei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10180" title="Abstract">arXiv:2310.10180</a> (replaced) [<a href="/pdf/2310.10180" title="Download PDF">pdf</a>, <a href="/format/2310.10180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lin Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qingxing Cao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yinya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xiaodan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP 2023. Code is available at <a href="https://github.com/menik1126/TRIGO">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11258" title="Abstract">arXiv:2310.11258</a> (replaced) [<a href="/pdf/2310.11258" title="Download PDF">pdf</a>, <a href="/format/2310.11258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Weak Supervision To Generate Indonesian Conservation Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fransiska%2C+M">Mega Fransiska</a>, 
<a href="/search/cs?searchtype=author&query=Pitaloka%2C+D">Diah Pitaloka</a>, 
<a href="/search/cs?searchtype=author&query=Saripudin">Saripudin</a>, 
<a href="/search/cs?searchtype=author&query=Putra%2C+S">Satrio Putra</a>, 
<a href="/search/cs?searchtype=author&query=Sutawika%2C+L">Lintang Sutawika</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11368" title="Abstract">arXiv:2310.11368</a> (replaced) [<a href="/pdf/2310.11368" title="Download PDF">pdf</a>, <a href="/format/2310.11368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VECHR: A Dataset for Explainable and Robust Classification of  Vulnerability Type in the European Court of Human Rights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Staufer%2C+L">Leon Staufer</a>, 
<a href="/search/cs?searchtype=author&query=Santosh%2C+T+Y+S+S">T.Y.S.S Santosh</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Heri%2C+C">Corina Heri</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11398" title="Abstract">arXiv:2310.11398</a> (replaced) [<a href="/pdf/2310.11398" title="Download PDF">pdf</a>, <a href="/format/2310.11398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism  with Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated the formulas in Section 3.2 "Detailed Methodology" and revised Section 2 "Background" for clarity and accuracy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11479" title="Abstract">arXiv:2310.11479</a> (replaced) [<a href="/pdf/2310.11479" title="Download PDF">pdf</a>, <a href="/format/2310.11479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Temperature of Bayesian Graph Neural Networks for Conformal  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cha%2C+S">Seohyeon Cha</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Honggu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Joonhyuk Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11862" title="Abstract">arXiv:2310.11862</a> (replaced) [<a href="/pdf/2310.11862" title="Download PDF">pdf</a>, <a href="/format/2310.11862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Generate Parameters of ConvNets for Unseen Image Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Kaituo Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoren Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11874" title="Abstract">arXiv:2310.11874</a> (replaced) [<a href="/pdf/2310.11874" title="Download PDF">pdf</a>, <a href="/ps/2310.11874" title="Download PostScript">ps</a>, <a href="/format/2310.11874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some derivations among Logarithmic Space Bounded Counting Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janaki%2C+V">V. Janaki</a>, 
<a href="/search/cs?searchtype=author&query=Madhan%2C+S">S. Madhan</a>, 
<a href="/search/cs?searchtype=author&query=Vijayaraghavan%2C+T+C">T. C. Vijayaraghavan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11878" title="Abstract">arXiv:2310.11878</a> (replaced) [<a href="/pdf/2310.11878" title="Download PDF">pdf</a>, <a href="/format/2310.11878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Dissonance to Insights: Dissecting Disagreements in Rationale  Construction for Case Outcome Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Santosh%2C+T+Y+S+S">T.Y.S.S Santosh</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Risini%2C+I">Isabella Risini</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11978" title="Abstract">arXiv:2310.11978</a> (replaced) [<a href="/pdf/2310.11978" title="Download PDF">pdf</a>, <a href="/format/2310.11978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can bin-wise scaling improve consistency and adaptivity of prediction  uncertainty for machine learning regression ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Pernot%2C+P">Pascal Pernot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version corrects an error in the estimation of the Sx scores for the test set, affecting Fig. 2 and Tables I-III of the initial version. The main points of the discussion and the conclusions are unchanged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12046" title="Abstract">arXiv:2310.12046</a> (replaced) [<a href="/pdf/2310.12046" title="Download PDF">pdf</a>, <a href="/format/2310.12046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of ML-Based Surrogates in Bayesian Approaches to Inverse  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ersin%2C+P">Pelin Ersin</a>, 
<a href="/search/stat?searchtype=author&query=Hayes%2C+E">Emma Hayes</a>, 
<a href="/search/stat?searchtype=author&query=Matthews%2C+P">Peter Matthews</a>, 
<a href="/search/stat?searchtype=author&query=Mohapatra%2C+P">Paramjyoti Mohapatra</a>, 
<a href="/search/stat?searchtype=author&query=Negrini%2C+E">Elisa Negrini</a>, 
<a href="/search/stat?searchtype=author&query=Schulz%2C+K">Karl Schulz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, submitted to NeurIPS Workshop on Machine Learning for Physical Sciences
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12172" title="Abstract">arXiv:2310.12172</a> (replaced) [<a href="/pdf/2310.12172" title="Download PDF">pdf</a>, <a href="/format/2310.12172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overview of ImageArg-2023: The First Shared Task in Multimodal Argument  Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhexiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Elaraby%2C+M">Mohamed Elaraby</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Litman%2C+D">Diane Litman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In The 10th Argument Mining Workshop, held in conjunction with The Conference on Empirical Methods in Natural Language Processing (EMNLP), December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12214" title="Abstract">arXiv:2310.12214</a> (replaced) [<a href="/pdf/2310.12214" title="Download PDF">pdf</a>, <a href="/format/2310.12214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PrivInfer: Privacy-Preserving Inference for Black-box Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+M">Meng Tong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kejiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12952" title="Abstract">arXiv:2310.12952</a> (replaced) [<a href="/pdf/2310.12952" title="Download PDF">pdf</a>, <a href="/format/2310.12952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity  Metrics For Science And Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pasarkar%2C+A+P">Amey P. Pasarkar</a>, 
<a href="/search/cs?searchtype=author&query=Dieng%2C+A+B">Adji Bousso Dieng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code for evaluating diversity using the Vendi scores can be found at <a href="https://github.com/vertaix/Vendi-Score.">this https URL</a> Code for using the scores within Vendi Sampling can be found at <a href="https://github.com/vertaix/Vendi-Sampling">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Chemical Physics (physics.chem-ph); Populations and Evolution (q-bio.PE)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13121" title="Abstract">arXiv:2310.13121</a> (replaced) [<a href="/pdf/2310.13121" title="Download PDF">pdf</a>, <a href="/format/2310.13121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Addition in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quirke%2C+P">Philip Quirke</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures, submitted to ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13123" title="Abstract">arXiv:2310.13123</a> (replaced) [<a href="/pdf/2310.13123" title="Download PDF">pdf</a>, <a href="/format/2310.13123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fuel Consumption Prediction for a Passenger Ferry using Machine Learning  and In-service Data: A Comparative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agand%2C+P">Pedram Agand</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+A">Allison Kennedy</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+T">Trevor Harris</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+C">Chanwoo Bae</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+E+J">Edward J Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures, 7 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ocean Engineering 284 (2023): 115271
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13129" title="Abstract">arXiv:2310.13129</a> (replaced) [<a href="/pdf/2310.13129" title="Download PDF">pdf</a>, <a href="/format/2310.13129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning-based Intelligent Traffic Signal Controls  with Optimized CO2 emissions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Agand%2C+P">Pedram Agand</a>, 
<a href="/search/eess?searchtype=author&query=Iskrov%2C+A">Alexey Iskrov</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Mo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, 1 table. International Conference on Intelligent Robots and Systems. IEEE/RSJ, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13245" title="Abstract">arXiv:2310.13245</a> (replaced) [<a href="/pdf/2310.13245" title="Download PDF">pdf</a>, <a href="/format/2310.13245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneous Shape Tracking of Multiple Deformable Linear Objects with  Global-Local Topology Preservation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jingyi Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Dinkel%2C+H">Holly Dinkel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, 3 figures, presented at the 3rd Workshop on Representing and Manipulating Deformable Objects at the IEEE International Conference on Robotics and Automation. Video presentation [<a href="https://youtu.be/hfiqwMxitqA">this https URL</a>]. 3rd Workshop on Representing and Manipulating Deformable Objects [<a href="https://deformable-workshop.github.io/icra2023/">this https URL</a>]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13469" title="Abstract">arXiv:2310.13469</a> (replaced) [<a href="/pdf/2310.13469" title="Download PDF">pdf</a>, <a href="/format/2310.13469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Language Model to Clean Your Noisy Translation Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bolding%2C+Q">Quinten Bolding</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Denis%2C+B+J">Brandon James Denis</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023, Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13519" title="Abstract">arXiv:2310.13519</a> (replaced) [<a href="/pdf/2310.13519" title="Download PDF">pdf</a>, <a href="/format/2310.13519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The performance of the DK (Discrete Kirchhoff) and DKM (Discrete  Kirchhoff-Mindlin) shell element classes in the upper bound  finite-element-based limit analysis and in the sequential  finite-element-based limit analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=%C5%A0tembera%2C+V">V&#xed;t&#x11b;zslav &#x160;tembera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13548" title="Abstract">arXiv:2310.13548</a> (replaced) [<a href="/pdf/2310.13548" title="Download PDF">pdf</a>, <a href="/format/2310.13548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Sycophancy in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mrinank Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+M">Meg Tong</a>, 
<a href="/search/cs?searchtype=author&query=Korbak%2C+T">Tomasz Korbak</a>, 
<a href="/search/cs?searchtype=author&query=Duvenaud%2C+D">David Duvenaud</a>, 
<a href="/search/cs?searchtype=author&query=Askell%2C+A">Amanda Askell</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Newton Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Durmus%2C+E">Esin Durmus</a>, 
<a href="/search/cs?searchtype=author&query=Hatfield-Dodds%2C+Z">Zac Hatfield-Dodds</a>, 
<a href="/search/cs?searchtype=author&query=Johnston%2C+S+R">Scott R. Johnston</a>, 
<a href="/search/cs?searchtype=author&query=Kravec%2C+S">Shauna Kravec</a>, 
<a href="/search/cs?searchtype=author&query=Maxwell%2C+T">Timothy Maxwell</a>, 
<a href="/search/cs?searchtype=author&query=McCandlish%2C+S">Sam McCandlish</a>, 
<a href="/search/cs?searchtype=author&query=Ndousse%2C+K">Kamal Ndousse</a>, 
<a href="/search/cs?searchtype=author&query=Rausch%2C+O">Oliver Rausch</a>, 
<a href="/search/cs?searchtype=author&query=Schiefer%2C+N">Nicholas Schiefer</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Da Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miranda Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13570" title="Abstract">arXiv:2310.13570</a> (replaced) [<a href="/pdf/2310.13570" title="Download PDF">pdf</a>, <a href="/format/2310.13570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Baseline for Knowledge-Based Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xenos%2C+A">Alexandros Xenos</a>, 
<a href="/search/cs?searchtype=author&query=Stafylakis%2C+T">Themos Stafylakis</a>, 
<a href="/search/cs?searchtype=author&query=Patras%2C+I">Ioannis Patras</a>, 
<a href="/search/cs?searchtype=author&query=Tzimiropoulos%2C+G">Georgios Tzimiropoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 (camera-ready version)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13573" title="Abstract">arXiv:2310.13573</a> (replaced) [<a href="/pdf/2310.13573" title="Download PDF">pdf</a>, <a href="/format/2310.13573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Generalization with Adaptive Style Techniques for Fingerprint  Liveness Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kexin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yule%2C+A">Adam Yule</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiajun Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1st Place in LivDet2023 Fingerprint Representation Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13639" title="Abstract">arXiv:2310.13639</a> (replaced) [<a href="/pdf/2310.13639" title="Download PDF">pdf</a>, <a href="/format/2310.13639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Preference Learning: Learning from Human Feedback without RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hejna%2C+J">Joey Hejna</a>, 
<a href="/search/cs?searchtype=author&query=Rafailov%2C+R">Rafael Rafailov</a>, 
<a href="/search/cs?searchtype=author&query=Sikchi%2C+H">Harshit Sikchi</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Niekum%2C+S">Scott Niekum</a>, 
<a href="/search/cs?searchtype=author&query=Knox%2C+W+B">W. Bradley Knox</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code released at <a href="https://github.com/jhejna/cpl.">this https URL</a> Edited 10/23 only to fix typo in the title
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13896" title="Abstract">arXiv:2310.13896</a> (replaced) [<a href="/pdf/2310.13896" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPTutor: an open-source AI pair programming tool alternative to Copilot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Eason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ray Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Justa Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Damien Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+P">Pierce Hung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14017" title="Abstract">arXiv:2310.14017</a> (replaced) [<a href="/pdf/2310.14017" title="Download PDF">pdf</a>, <a href="/format/2310.14017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrast Everything: A Hierarchical Contrastive Framework for Medical  Time-Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yihe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yu Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haishuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeruIPS 2023; 24pages (13 pages main paper + 11 pages supplementary materials)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeruIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14085" title="Abstract">arXiv:2310.14085</a> (replaced) [<a href="/pdf/2310.14085" title="Download PDF">pdf</a>, <a href="/ps/2310.14085" title="Download PostScript">ps</a>, <a href="/format/2310.14085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and  Exp-Concave Games with Gradient Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tianyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhengyuan Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Operations Research; 47 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14168" title="Abstract">arXiv:2310.14168</a> (replaced) [<a href="/pdf/2310.14168" title="Download PDF">pdf</a>, <a href="/format/2310.14168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomized Forward Mode of Automatic Differentiation for Optimization  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shukla%2C+K">Khemraj Shukla</a>, 
<a href="/search/math?searchtype=author&query=Shin%2C+Y">Yeonjong Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 Pages, 8 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14230" title="Abstract">arXiv:2310.14230</a> (replaced) [<a href="/pdf/2310.14230" title="Download PDF">pdf</a>, <a href="/format/2310.14230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comprehensive survey on deep active learning and its applications in  medical image analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiuye Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiman Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Manning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhijian Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper List on Github: <a href="https://github.com/LightersWang/Awesome-Active-Learning-for-Medical-Image-Analysis">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14270" title="Abstract">arXiv:2310.14270</a> (replaced) [<a href="/pdf/2310.14270" title="Download PDF">pdf</a>, <a href="/format/2310.14270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion-Based Adversarial Purification for Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bai%2C+Y">Yibo Bai</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiao-Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14468" title="Abstract">arXiv:2310.14468</a> (replaced) [<a href="/pdf/2310.14468" title="Download PDF">pdf</a>, <a href="/format/2310.14468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Implicit Differentiation for Learning Problems in Optimal  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Ming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Molloy%2C+T">Timothy Molloy</a>, 
<a href="/search/cs?searchtype=author&query=Gould%2C+S">Stephen Gould</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14512" title="Abstract">arXiv:2310.14512</a> (replaced) [<a href="/pdf/2310.14512" title="Download PDF">pdf</a>, <a href="/format/2310.14512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CorefPrompt: Prompt-based Event Coreference Resolution by Measuring  Event Type and Argument Compatibilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Sheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiaoming Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14523" title="Abstract">arXiv:2310.14523</a> (replaced) [<a href="/pdf/2310.14523" title="Download PDF">pdf</a>, <a href="/format/2310.14523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Word-Level Auto-Completion in Computer-Aided Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lemao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Guoping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhirui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14545" title="Abstract">arXiv:2310.14545</a> (replaced) [<a href="/pdf/2310.14545" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing ChatGPT for thematic analysis: Are we ready?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+V+V">V Vien Lee</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Lubbe%2C+S+C+C">Stephanie C. C. van der Lubbe</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+L+H">Lay Hoon Goh</a>, 
<a href="/search/cs?searchtype=author&query=Valderas%2C+J+M">Jose M. Valderas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures, 3 tables, 1 textbox
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14654" title="Abstract">arXiv:2310.14654</a> (replaced) [<a href="/pdf/2310.14654" title="Download PDF">pdf</a>, <a href="/ps/2310.14654" title="Download PostScript">ps</a>, <a href="/format/2310.14654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab,  IIT Madras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%2C+N">Nithya R</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+M">Malavika S</a>, 
<a href="/search/cs?searchtype=author&query=F%2C+J">Jordan F</a>, 
<a href="/search/cs?searchtype=author&query=Gangwar%2C+A">Arjun Gangwar</a>, 
<a href="/search/cs?searchtype=author&query=J%2C+M+N">Metilda N J</a>, 
<a href="/search/cs?searchtype=author&query=Umesh%2C+S">S Umesh</a>, 
<a href="/search/cs?searchtype=author&query=Sarab%2C+R">Rithik Sarab</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+A+K">Akhilesh Kumar Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Divakaran%2C+G">Govind Divakaran</a>, 
<a href="/search/cs?searchtype=author&query=K%2C+S+V">Samudra Vijaya K</a>, 
<a href="/search/cs?searchtype=author&query=Gangashetty%2C+S+V">Suryakanth V Gangashetty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 pages, About SPRING-INX Data
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14724" title="Abstract">arXiv:2310.14724</a> (replaced) [<a href="/pdf/2310.14724" title="Download PDF">pdf</a>, <a href="/format/2310.14724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on LLM-generated Text Detection: Necessity, Methods, and Future  Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junchao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+R">Runzhe Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yulin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D+F">Derek F. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+L+S">Lidia S. Chao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14747" title="Abstract">arXiv:2310.14747</a> (replaced) [<a href="/pdf/2310.14747" title="Download PDF">pdf</a>, <a href="/format/2310.14747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCC-KD: Multi-CoT Consistent Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongzhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ENMLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14892" title="Abstract">arXiv:2310.14892</a> (replaced) [<a href="/pdf/2310.14892" title="Download PDF">pdf</a>, <a href="/format/2310.14892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time  Controllable Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+T">Tianqi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Quan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jingxuan Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongdong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhendong Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as an EMNLP 2023 main paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14925" title="Abstract">arXiv:2310.14925</a> (replaced) [<a href="/pdf/2310.14925" title="Download PDF">pdf</a>, <a href="/format/2310.14925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Causal Discovery for Robotics Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castri%2C+L">Luca Castri</a>, 
<a href="/search/cs?searchtype=author&query=Mghames%2C+S">Sariah Mghames</a>, 
<a href="/search/cs?searchtype=author&query=Bellotto%2C+N">Nicola Bellotto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at 5th Italian Conference on Robotics and Intelligent Machines (I-RIM 3D 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14948" title="Abstract">arXiv:2310.14948</a> (replaced) [<a href="/pdf/2310.14948" title="Download PDF">pdf</a>, <a href="/format/2310.14948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Graph Convolutional Networks: Towards a generalized  framework for complex geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chenaud%2C+M">Marien Chenaud</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+J">Jos&#xe9; Alves</a>, 
<a href="/search/cs?searchtype=author&query=Magoul%C3%A8s%2C+F">Fr&#xe9;d&#xe9;ric Magoul&#xe8;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Civil-Comp Conferences, Volume 5, Paper 4.2, Civil-Comp Press,
  Edinburgh, United Kingdom, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mathematical Physics (math-ph); Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15008" title="Abstract">arXiv:2310.15008</a> (replaced) [<a href="/pdf/2310.15008" title="Download PDF">pdf</a>, <a href="/format/2310.15008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wonder3D: Single Image to 3D using Cross-Domain Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xiaoxiao Long</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuan-Chen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Cheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhiyang Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Song-Hai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Habermann%2C+M">Marc Habermann</a>, 
<a href="/search/cs?searchtype=author&query=Theobalt%2C+C">Christian Theobalt</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://www.xxlong.site/Wonder3D/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15019" title="Abstract">arXiv:2310.15019</a> (replaced) [<a href="/pdf/2310.15019" title="Download PDF">pdf</a>, <a href="/format/2310.15019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta learning with language models: Challenges and opportunities in the  classification of imbalanced text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vassilev%2C+A">Apostol Vassilev</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Honglan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M">Munawar Hasan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, including 5 figures, 12 tables, 1 appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15047" title="Abstract">arXiv:2310.15047</a> (replaced) [<a href="/pdf/2310.15047" title="Download PDF">pdf</a>, <a href="/format/2310.15047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta- (out-of-context) learning in neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krasheninnikov%2C+D">Dmitrii Krasheninnikov</a>, 
<a href="/search/cs?searchtype=author&query=Krasheninnikov%2C+E">Egor Krasheninnikov</a>, 
<a href="/search/cs?searchtype=author&query=Mlodozeniec%2C+B">Bruno Mlodozeniec</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+D">David Krueger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15057" title="Abstract">arXiv:2310.15057</a> (replaced) [<a href="/pdf/2310.15057" title="Download PDF">pdf</a>, <a href="/format/2310.15057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shareable Driving Style Learning and Analysis with a Hierarchical Latent  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenshuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaokun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lijun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+J">Junqiang Xi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15106" title="Abstract">arXiv:2310.15106</a> (replaced) [<a href="/pdf/2310.15106" title="Download PDF">pdf</a>, <a href="/format/2310.15106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytical Performance Bounds for Radio Map Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romero%2C+D">Daniel Romero</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+T+N">Tien Ngoc Ha</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+R">Raju Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Franceschetti%2C+M">Massimo Franceschetti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15168" title="Abstract">arXiv:2310.15168</a> (replaced) [<a href="/pdf/2310.15168" title="Download PDF">pdf</a>, <a href="/format/2310.15168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ghost on the Shell: An Expressive Representation of General 3D Shapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Xiu%2C+Y">Yuliang Xiu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Paull%2C+L">Liam Paull</a>, 
<a href="/search/cs?searchtype=author&query=Black%2C+M+J">Michael J. Black</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report (26 pages, 16 figures, Project Page: <a href="https://gshell3d.github.io/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item365">Cross-lists</a></li>
<li><a href="#item416">Replacements</a></li>
</ul>
<small>[ total of 675 entries:  <b>1-675</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2310">2310</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
