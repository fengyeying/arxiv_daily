<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Tue 31 Oct 23  to  Wed  1 Nov 23, announced Thu,  2 Nov 23</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item263">Cross-lists</a></li>
<li><a href="#item302">Replacements</a></li>
</ul>
<small>[ total of 510 entries:  <b>1-510</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Thu,  2 Nov 23</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00047" title="Abstract">arXiv:2311.00047</a> [<a href="/pdf/2311.00047" title="Download PDF">pdf</a>, <a href="/format/2311.00047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounding Visual Illusions in Language: Do Vision-Language Models  Perceive Illusions Like Humans?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jiayi Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuchen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+R">Rui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Joyce Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision-Language Models (VLMs) are trained on vast amounts of data captured by
humans emulating our understanding of the world. However, known as visual
illusions, human's perception of reality isn't always faithful to the physical
world. This raises a key question: do VLMs have the similar kind of illusions
as humans do, or do they faithfully learn to represent reality? To investigate
this question, we build a dataset containing five types of visual illusions and
formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our
findings have shown that although the overall alignment is low, larger models
are closer to human perception and more susceptible to visual illusions. Our
dataset and initial findings will promote a better understanding of visual
illusions in humans and machines and provide a stepping stone for future
computational models that can better align humans and machines in perceiving
and communicating about the shared visual world. The code and data are
available at https://github.com/vl-illusion/dataset.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00048" title="Abstract">arXiv:2311.00048</a> [<a href="/pdf/2311.00048" title="Download PDF">pdf</a>, <a href="/format/2311.00048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+P">Peijie Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+P">Pan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yalin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sotiras%2C+A">Aristeidis Sotiras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multiple Instance Learning (MIL) has been widely used in weakly supervised
whole slide image (WSI) classification. Typical MIL methods include a feature
embedding part that embeds the instances into features via a pre-trained
feature extractor and the MIL aggregator that combines instance embeddings into
predictions. The current focus has been directed toward improving these parts
by refining the feature embeddings through self-supervised pre-training and
modeling the correlations between instances separately. In this paper, we
proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the
same time by leveraging sparse dictionary learning. The sparse dictionary
learning captures the similarities of instances by expressing them as a sparse
linear combination of atoms in an over-complete dictionary. In addition,
imposing sparsity help enhance the instance feature embeddings by suppressing
irrelevant instances while retaining the most relevant ones. To make the
conventional sparse coding algorithm compatible with deep learning, we unrolled
it into an SC module by leveraging deep unrolling. The proposed SC module can
be incorporated into any existing MIL framework in a plug-and-play manner with
an acceptable computation cost. The experimental results on multiple datasets
demonstrated that the proposed SC module could substantially boost the
performance of state-of-the-art MIL methods. The codes are available at
\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00049" title="Abstract">arXiv:2311.00049</a> [<a href="/pdf/2311.00049" title="Download PDF">pdf</a>, <a href="/format/2311.00049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Kolmogorov neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ismayilova%2C+A">Aysu Ismayilova</a>, 
<a href="/search/cs?searchtype=author&query=Ismailov%2C+V">Vugar Ismailov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 1 figure; this article uses material from <a href="/abs/2012.03016">arXiv:2012.03016</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Functional Analysis (math.FA); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we show that the Kolmogorov two hidden layer neural network
model with a continuous, discontinuous bounded or unbounded activation function
in the second hidden layer can precisely represent continuous, discontinuous
bounded and all unbounded multivariate functions, respectively.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00055" title="Abstract">arXiv:2311.00055</a> [<a href="/pdf/2311.00055" title="Download PDF">pdf</a>, <a href="/format/2311.00055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-Free Generalization on Heterogeneous Tabular Data via  Meta-Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Han-Jia Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qi-Le Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+D">De-Chuan Zhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Tabular data is prevalent across various machine learning domains. Yet, the
inherent heterogeneities in attribute and class spaces across different tabular
datasets hinder the effective sharing of knowledge, limiting a tabular model to
benefit from other datasets. In this paper, we propose Tabular data
Pre-Training via Meta-representation (TabPTM), which allows one tabular model
pre-training on a set of heterogeneous datasets. Then, this pre-trained model
can be directly applied to unseen datasets that have diverse attributes and
classes without additional training. Specifically, TabPTM represents an
instance through its distance to a fixed number of prototypes, thereby
standardizing heterogeneous tabular datasets. A deep neural network is then
trained to associate these meta-representations with dataset-specific
classification confidences, endowing TabPTM with the ability of training-free
generalization. Experiments validate that TabPTM achieves promising performance
in new datasets, even under few-shot scenarios.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00056" title="Abstract">arXiv:2311.00056</a> [<a href="/pdf/2311.00056" title="Download PDF">pdf</a>, <a href="/format/2311.00056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity and Diffusion: Observations on Synthetic Image Distributions  with Stable Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marwood%2C+D">David Marwood</a>, 
<a href="/search/cs?searchtype=author&query=Baluja%2C+S">Shumeet Baluja</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+Y">Yair Alon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent progress in text-to-image (TTI) systems, such as StableDiffusion,
Imagen, and DALL-E 2, have made it possible to create realistic images with
simple text prompts. It is tempting to use these systems to eliminate the
manual task of obtaining natural images for training a new machine learning
classifier. However, in all of the experiments performed to date, classifiers
trained solely with synthetic images perform poorly at inference, despite the
images used for training appearing realistic. Examining this apparent
incongruity in detail gives insight into the limitations of the underlying
image generation processes. Through the lens of diversity in image creation
vs.accuracy of what is created, we dissect the differences in semantic
mismatches in what is modeled in synthetic vs. natural images. This will
elucidate the roles of the image-languag emodel, CLIP, and the image generation
model, diffusion. We find four issues that limit the usefulness of TTI systems
for this task: ambiguity, adherence to prompt, lack of diversity, and inability
to represent the underlying concept. We further present surprising insights
into the geometry of CLIP embeddings.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00059" title="Abstract">arXiv:2311.00059</a> [<a href="/pdf/2311.00059" title="Download PDF">pdf</a>, <a href="/format/2311.00059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Generative AI Paradox: &quot;What It Can Create, It May Not Understand&quot;
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=West%2C+P">Peter West</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Ximing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Dziri%2C+N">Nouha Dziri</a>, 
<a href="/search/cs?searchtype=author&query=Brahman%2C+F">Faeze Brahman</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+J+D">Jena D. Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fisher%2C+J">Jillian Fisher</a>, 
<a href="/search/cs?searchtype=author&query=Ravichander%2C+A">Abhilasha Ravichander</a>, 
<a href="/search/cs?searchtype=author&query=Chandu%2C+K">Khyathi Chandu</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+B">Benjamin Newman</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+P+W">Pang Wei Koh</a>, 
<a href="/search/cs?searchtype=author&query=Ettinger%2C+A">Allyson Ettinger</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The recent wave of generative AI has sparked unprecedented global attention,
with both excitement and concern over potentially superhuman levels of
artificial intelligence: models now take only seconds to produce outputs that
would challenge or exceed the capabilities even of expert humans. At the same
time, models still show basic errors in understanding that would not be
expected even in non-expert humans. This presents us with an apparent paradox:
how do we reconcile seemingly superhuman capabilities with the persistence of
errors that few humans would make? In this work, we posit that this tension
reflects a divergence in the configuration of intelligence in today's
generative models relative to intelligence in humans. Specifically, we propose
and test the Generative AI Paradox hypothesis: generative models, having been
trained directly to reproduce expert-like outputs, acquire generative
capabilities that are not contingent upon -- and can therefore exceed -- their
ability to understand those same types of outputs. This contrasts with humans,
for whom basic understanding almost always precedes the ability to generate
expert-level outputs. We test this hypothesis through controlled experiments
analyzing generation vs. understanding in generative models, across both
language and image modalities. Our results show that although models can
outperform humans in generation, they consistently fall short of human
capabilities in measures of understanding, as well as weaker correlation
between generation and understanding performance, and more brittleness to
adversarial inputs. Our findings support the hypothesis that models' generative
capability may not be contingent upon understanding capability, and call for
caution in interpreting artificial intelligence by analogy to human
intelligence.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00063" title="Abstract">arXiv:2311.00063</a> [<a href="/pdf/2311.00063" title="Download PDF">pdf</a>, <a href="/format/2311.00063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe multi-agent motion planning under uncertainty for drones using  filtered reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Safaoui%2C+S">Sleiman Safaoui</a>, 
<a href="/search/cs?searchtype=author&query=Vinod%2C+A+P">Abraham P. Vinod</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarty%2C+A">Ankush Chakrabarty</a>, 
<a href="/search/cs?searchtype=author&query=Quirynen%2C+R">Rien Quirynen</a>, 
<a href="/search/cs?searchtype=author&query=Yoshikawa%2C+N">Nobuyuki Yoshikawa</a>, 
<a href="/search/cs?searchtype=author&query=Di+Cairano%2C+S">Stefano Di Cairano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
<p class="mathjax">We consider the problem of safe multi-agent motion planning for drones in
uncertain, cluttered workspaces. For this problem, we present a tractable
motion planner that builds upon the strengths of reinforcement learning and
constrained-control-based trajectory planning. First, we use single-agent
reinforcement learning to learn motion plans from data that reach the target
but may not be collision-free. Next, we use a convex optimization, chance
constraints, and set-based methods for constrained control to ensure safety,
despite the uncertainty in the workspace, agent motion, and sensing. The
proposed approach can handle state and control constraints on the agents, and
enforce collision avoidance among themselves and with static obstacles in the
workspace with high probability. The proposed approach yields a safe, real-time
implementable, multi-agent motion planner that is simpler to train than methods
based solely on learning. Numerical simulations and experiments show the
efficacy of the approach.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00066" title="Abstract">arXiv:2311.00066</a> [<a href="/pdf/2311.00066" title="Download PDF">pdf</a>, <a href="/ps/2311.00066" title="Download PostScript">ps</a>, <a href="/format/2311.00066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Mobile Application Privacy: A Quantitative Framework for  Privacy Measurement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marono%2C+J">Joao Marono</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+C">Catarina Silva</a>, 
<a href="/search/cs?searchtype=author&query=Barraca%2C+J+P">Joao P. Barraca</a>, 
<a href="/search/cs?searchtype=author&query=Cunha%2C+V">Vitor Cunha</a>, 
<a href="/search/cs?searchtype=author&query=Salvador%2C+P">Paulo Salvador</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The proliferation of mobile applications and the subsequent sharing of
personal data with service and application providers have given rise to
substantial privacy concerns. Application marketplaces have introduced
mechanisms to conform to regulations and provide individuals with control over
their data. However, a notable absence persists regarding clear indications,
labels or scores elucidating the privacy implications of these applications. In
response to this challenge, this paper introduces a privacy quantification
framework. The purpose of this framework is to systematically evaluate the
level of privacy risk when using particular Android applications. The main goal
is to provide individuals with qualitative labels to make informed decisions
about their privacy. This work aims to contribute to a digital environment that
prioritizes privacy, promotes informed decision-making, and endorses the
privacy-preserving design principles incorporation.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00067" title="Abstract">arXiv:2311.00067</a> [<a href="/pdf/2311.00067" title="Download PDF">pdf</a>, <a href="/format/2311.00067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Control of Euler-Lagrange Systems under Time-varying State  Constraints without a Priori Bounded Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sankaranarayanan%2C+V+N">Viswa Narayanan Sankaranarayanan</a>, 
<a href="/search/eess?searchtype=author&query=Satpute%2C+S+G">Sumeet Gajanan Satpute</a>, 
<a href="/search/eess?searchtype=author&query=Roy%2C+S">Spandan Roy</a>, 
<a href="/search/eess?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Preprints of the 22nd IFAC World Congress, Yokohama, Japan, July
  9-14, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">In this article, a novel adaptive controller is designed for Euler-Lagrangian
systems under predefined time-varying state constraints. The proposed
controller could achieve this objective without a priori knowledge of system
parameters and, crucially, of state-dependent uncertainties. The closed-loop
stability is verified using the Lyapunov method, while the overall efficacy of
the proposed scheme is verified using a simulated robotic arm compared to the
state of the art.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00068" title="Abstract">arXiv:2311.00068</a> [<a href="/pdf/2311.00068" title="Download PDF">pdf</a>, <a href="/format/2311.00068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> View Classification and Object Detection in Cardiac Ultrasound to  Localize Valves via Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gungor%2C+D+G">Derya Gol Gungor</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+B">Bimba Rao</a>, 
<a href="/search/cs?searchtype=author&query=Wolverton%2C+C">Cynthia Wolverton</a>, 
<a href="/search/cs?searchtype=author&query=Guracar%2C+I">Ismayil Guracar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 11 figures, Submitted to CVPR 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Echocardiography provides an important tool for clinicians to observe the
function of the heart in real time, at low cost, and without harmful radiation.
Automated localization and classification of heart valves enables automatic
extraction of quantities associated with heart mechanical function and related
blood flow measurements. We propose a machine learning pipeline that uses deep
neural networks for separate classification and localization steps. As the
first step in the pipeline, we apply view classification to echocardiograms
with ten unique anatomic views of the heart. In the second step, we apply deep
learning-based object detection to both localize and identify the valves. Image
segmentation based object detection in echocardiography has been shown in many
earlier studies but, to the best of our knowledge, this is the first study that
predicts the bounding boxes around the valves along with classification from 2D
ultrasound images with the help of deep neural networks. Our object detection
experiments applied to the Apical views suggest that it is possible to localize
and identify multiple valves precisely.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00073" title="Abstract">arXiv:2311.00073</a> [<a href="/pdf/2311.00073" title="Download PDF">pdf</a>, <a href="/format/2311.00073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLOv8-Based Visual Detection of Road Hazards: Potholes, Sewer Covers,  and Manholes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khare%2C+O+M">Om M. Khare</a>, 
<a href="/search/cs?searchtype=author&query=Gandhi%2C+S">Shubham Gandhi</a>, 
<a href="/search/cs?searchtype=author&query=Rahalkar%2C+A+M">Aditya M. Rahalkar</a>, 
<a href="/search/cs?searchtype=author&query=Mane%2C+S">Sunil Mane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE-PuneCon-2023 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Effective detection of road hazards plays a pivotal role in road
infrastructure maintenance and ensuring road safety. This research paper
provides a comprehensive evaluation of YOLOv8, an object detection model, in
the context of detecting road hazards such as potholes, Sewer Covers, and Man
Holes. A comparative analysis with previous iterations, YOLOv5 and YOLOv7, is
conducted, emphasizing the importance of computational efficiency in various
applications. The paper delves into the architecture of YOLOv8 and explores
image preprocessing techniques aimed at enhancing detection accuracy across
diverse conditions, including variations in lighting, road types, hazard sizes,
and types. Furthermore, hyperparameter tuning experiments are performed to
optimize model performance through adjustments in learning rates, batch sizes,
anchor box sizes, and augmentation strategies. Model evaluation is based on
Mean Average Precision (mAP), a widely accepted metric for object detection
performance. The research assesses the robustness and generalization
capabilities of the models through mAP scores calculated across the diverse
test scenarios, underlining the significance of YOLOv8 in road hazard detection
and infrastructure maintenance.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00074" title="Abstract">arXiv:2311.00074</a> [<a href="/pdf/2311.00074" title="Download PDF">pdf</a>, <a href="/format/2311.00074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Software Developers&#x27; Challenges for Android Permissions in  Stack Overflow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oishwee%2C+S+J">Sahrima Jannat Oishwee</a>, 
<a href="/search/cs?searchtype=author&query=Stakhanova%2C+N">Natalia Stakhanova</a>, 
<a href="/search/cs?searchtype=author&query=Codabux%2C+Z">Zadia Codabux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The Android permission system is a set of controls to regulate access to
sensitive data and platform resources (e.g., camera). The fast evolving nature
of Android permissions, coupled with inadequate documentation, results in
numerous challenges for third-party developers. This study investigates the
permission-related challenges developers face and the solutions provided to
resolve them on the crowdsourcing platform Stack Overflow. We conducted
qualitative and quantitative analyses on 3,327 permission-related questions and
3,271 corresponding answers. Our study found that most questions are related to
non-evolving SDK permissions that remain constant across various Android
versions, which emphasizes the lack of documentation. We classify developers'
challenges into several categories: Documentation-Related, Problems with
Dependencies, Debugging, Conceptual Understanding, and Implementation Issues.
We further divided these categories into 12 subcategories, nine
sub-subcategories, and nine sub-sub-subcategories. Our analysis shows that
developers infrequently identify the restriction type or protection level of
permissions, and when they do, their descriptions often contradict Google's
official documentation. Our study indicates the need for clear, consistent
documentation to guide the use of permissions and reduce developer
misunderstanding leading to potential misuse of Android permission. These
insights from this study can inform strategies and guidelines for permission
issues. Future studies should explore the effectiveness of Stack Overflow
solutions to form best practices and develop tools to address these problems.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00077" title="Abstract">arXiv:2311.00077</a> [<a href="/pdf/2311.00077" title="Download PDF">pdf</a>, <a href="/format/2311.00077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;s conjecture for binary completely reachable automata: an approach  and its limitations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casas%2C+D">David Casas</a>, 
<a href="/search/cs?searchtype=author&query=Volkov%2C+M+V">Mikhail V. Volkov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">A deterministic finite automaton in which every non-empty set of states
occurs as the image of the whole state set under the action of a suitable input
word is called completely reachable. It was conjectured that in each completely
reachable automaton with $n$ states, every set of $k&gt;0$ states is the image of
a word of length at most $n(n-k)$. We confirm the conjecture for completely
reachable automata with two input letters satisfying certain restrictions on
the action of the letters.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00079" title="Abstract">arXiv:2311.00079</a> [<a href="/pdf/2311.00079" title="Download PDF">pdf</a>, <a href="/format/2311.00079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spuriosity Rankings for Free: A Simple Framework for Last Layer  Retraining Based on Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azizmalayeri%2C+M">Mohammad Azizmalayeri</a>, 
<a href="/search/cs?searchtype=author&query=Abbasi%2C+R">Reza Abbasi</a>, 
<a href="/search/cs?searchtype=author&query=rezaie%2C+A+H+H+M">Amir Hosein Haji Mohammad rezaie</a>, 
<a href="/search/cs?searchtype=author&query=Zohrabi%2C+R">Reihaneh Zohrabi</a>, 
<a href="/search/cs?searchtype=author&query=Amiri%2C+M">Mahdi Amiri</a>, 
<a href="/search/cs?searchtype=author&query=Manzuri%2C+M+T">Mohammad Taghi Manzuri</a>, 
<a href="/search/cs?searchtype=author&query=Rohban%2C+M+H">Mohammad Hossein Rohban</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICML 2023 Workshop on Spurious Correlations, Invariance, and Stability (SCIS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural networks have exhibited remarkable performance in various
domains. However, the reliance of these models on spurious features has raised
concerns about their reliability. A promising solution to this problem is
last-layer retraining, which involves retraining the linear classifier head on
a small subset of data without spurious cues. Nevertheless, selecting this
subset requires human supervision, which reduces its scalability. Moreover,
spurious cues may still exist in the selected subset. As a solution to this
problem, we propose a novel ranking framework that leverages an open vocabulary
object detection technique to identify images without spurious cues. More
specifically, we use the object detector as a measure to score the presence of
the target object in the images. Next, the images are sorted based on this
score, and the last-layer of the model is retrained on a subset of the data
with the highest scores. Our experiments on the ImageNet-1k dataset demonstrate
the effectiveness of this ranking framework in sorting images based on
spuriousness and using them for last-layer retraining.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00081" title="Abstract">arXiv:2311.00081</a> [<a href="/pdf/2311.00081" title="Download PDF">pdf</a>, <a href="/format/2311.00081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolution Quadrature for the quasilinear subdiffusion equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fern%C3%A1ndez%2C+M+L">Maria L&#xf3;pez Fern&#xe1;ndez</a>, 
<a href="/search/math?searchtype=author&query=P%C5%82ociniczak%2C+%C5%81">&#x141;ukasz P&#x142;ociniczak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We construct a Convolution Quadrature (CQ) scheme for the quasilinear
subdiffusion equation and supply it with the fast and oblivious implementation.
In particular we find a condition for the CQ to be admissible and discretize
the spatial part of the equation with the Finite Element Method. We prove the
unconditional stability and convergence of the scheme and find a bound on the
error. As a passing result, we also obtain a discrete Gronwall inequality for
the CQ, which is a crucial ingredient of our convergence proof based on the
energy method. The paper is concluded with numerical examples verifying
convergence and computation time reduction when using fast and oblivious
quadrature.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00094" title="Abstract">arXiv:2311.00094</a> [<a href="/pdf/2311.00094" title="Download PDF">pdf</a>, <a href="/format/2311.00094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressive Modeling Is Insufficient for Offline RL: A Tractable  Inference Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuejie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Anji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Van+den+Broeck%2C+G">Guy Van den Broeck</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yitao Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A popular paradigm for offline Reinforcement Learning (RL) tasks is to first
fit the offline trajectories to a sequence model, and then prompt the model for
actions that lead to high expected return. While a common consensus is that
more expressive sequence models imply better performance, this paper highlights
that tractability, the ability to exactly and efficiently answer various
probabilistic queries, plays an equally important role. Specifically, due to
the fundamental stochasticity from the offline data-collection policies and the
environment dynamics, highly non-trivial conditional/constrained generation is
required to elicit rewarding actions. While it is still possible to approximate
such queries, we observe that such crude estimates significantly undermine the
benefits brought by expressive sequence models. To overcome this problem, this
paper proposes Trifle (Tractable Inference for Offline RL), which leverages
modern Tractable Probabilistic Models (TPMs) to bridge the gap between good
sequence models and high expected returns at evaluation time. Empirically,
Trifle achieves the most state-of-the-art scores in 9 Gym-MuJoCo benchmarks
against strong baselines. Further, owing to its tractability, Trifle
significantly outperforms prior approaches in stochastic environments and safe
RL tasks (e.g. with action constraints) with minimum algorithmic modifications.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00096" title="Abstract">arXiv:2311.00096</a> [<a href="/pdf/2311.00096" title="Download PDF">pdf</a>, <a href="/format/2311.00096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandit-Driven Batch Selection for Robust Learning under Label Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lisicki%2C+M">Michal Lisicki</a>, 
<a href="/search/cs?searchtype=author&query=Nica%2C+M">Mihai Nica</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+G+W">Graham W. Taylor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WANT@NeurIPS 2023 &amp; OPT@NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a novel approach for batch selection in Stochastic Gradient
Descent (SGD) training, leveraging combinatorial bandit algorithms. Our
methodology focuses on optimizing the learning process in the presence of label
noise, a prevalent issue in real-world datasets. Experimental evaluations on
the CIFAR-10 dataset reveal that our approach consistently outperforms existing
methods across various levels of label corruption. Importantly, we achieve this
superior performance without incurring the computational overhead commonly
associated with auxiliary neural network models. This work presents a balanced
trade-off between computational efficiency and model efficacy, offering a
scalable solution for complex machine learning applications.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00097" title="Abstract">arXiv:2311.00097</a> [<a href="/pdf/2311.00097" title="Download PDF">pdf</a>, <a href="/format/2311.00097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cocoon: Static Information Flow Control in Rust
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barach%2C+A">Ada Barach</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+M">Maxwell Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Beardsley%2C+V">Vincent Beardsley</a>, 
<a href="/search/cs?searchtype=author&query=Bambeck%2C+J">Jacob Bambeck</a>, 
<a href="/search/cs?searchtype=author&query=Bond%2C+M+D">Michael D. Bond</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiqiang Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Information flow control (IFC) ensures confidentiality by preventing secret
values from affecting non-secret values. Existing language-level IFC approaches
modify the language and use non-standard compilation tools, impose run-time
overhead, or report false leaks, all of which hinder adoption. This paper
presents Cocoon, a Rust library for static type-based IFC that uses the
unmodified Rust language and compiler. The key insight of Cocoon lies in
leveraging Rust's type system and procedural macros to establish an effect
system that allows applications to safely compute arbitrary functions on secret
data. We integrated Cocoon into two popular Rust programs, the Spotify TUI
client and Mozilla's Servo browser engine, to protect a secret value in each
program. The results show that applications can be retrofitted to use Cocoon
with limited modifications, at least to protect a single value, with negligible
or nonexistent impacts on run-time and compile-time performance.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00101" title="Abstract">arXiv:2311.00101</a> [<a href="/pdf/2311.00101" title="Download PDF">pdf</a>, <a href="/format/2311.00101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overcoming membrane locking in quadratic NURBS-based discretizations of  linear Kirchhoff-Love shells: CAS elements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casquero%2C+H">Hugo Casquero</a>, 
<a href="/search/cs?searchtype=author&query=Mathews%2C+K+D">Kyle Dakota Mathews</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Quadratic NURBS-based discretizations of the Galerkin method suffer from
membrane locking when applied to Kirchhoff-Love shell formulations. Membrane
locking causes not only smaller displacements than expected, but also
large-amplitude spurious oscillations of the membrane forces.
Continuous-assumed-strain (CAS) elements have been recently introduced to
remove membrane locking in quadratic NURBS-based discretizations of linear
plane curved Kirchhoff rods (Casquero et al., CMAME, 2022). In this work, we
generalize CAS elements to vanquish membrane locking in quadratic NURBS-based
discretizations of linear Kirchhoff-Love shells. CAS elements bilinearly
interpolate the membrane strains at the four corners of each element. Thus, the
assumed strains have C0 continuity across element boundaries. To the best of
the authors' knowledge, CAS elements are the first assumed-strain treatment to
effectively overcome membrane locking in quadratic NURBS-based discretizations
of Kirchhoff-Love shells while satisfying the following important
characteristics for computational efficiency: (1) No additional degrees of
freedom are added, (2) No additional systems of algebraic equations need to be
solved, (3) No matrix multiplications or matrix inversions are needed to obtain
the stiffness matrix, and (4) The nonzero pattern of the stiffness matrix is
preserved. The benchmark problems show that CAS elements, using either 2x2 or
3x3 Gauss-Legendre quadrature points per element, are an effective locking
treatment since this element type results in more accurate displacements for
coarse meshes and excises the spurious oscillations of the membrane forces. The
benchmark problems also show that CAS elements outperform state-of-the-art
element types based on Lagrange polynomials equipped with either assumed-strain
or reduced-integration locking treatments.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00109" title="Abstract">arXiv:2311.00109</a> [<a href="/pdf/2311.00109" title="Download PDF">pdf</a>, <a href="/format/2311.00109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairWASP: Fast and Optimal Fair Wasserstein Pre-processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zikai Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Dalmasso%2C+N">Niccol&#xf2; Dalmasso</a>, 
<a href="/search/cs?searchtype=author&query=Mishler%2C+A">Alan Mishler</a>, 
<a href="/search/cs?searchtype=author&query=Potluru%2C+V+K">Vamsi K. Potluru</a>, 
<a href="/search/cs?searchtype=author&query=Balch%2C+T">Tucker Balch</a>, 
<a href="/search/cs?searchtype=author&query=Veloso%2C+M">Manuela Veloso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent years have seen a surge of machine learning approaches aimed at
reducing disparities in model outputs across different subgroups. In many
settings, training data may be used in multiple downstream applications by
different users, which means it may be most effective to intervene on the
training data itself. In this work, we present FairWASP, a novel pre-processing
approach designed to reduce disparities in classification datasets without
modifying the original data. FairWASP returns sample-level weights such that
the reweighted dataset minimizes the Wasserstein distance to the original
dataset while satisfying (an empirical version of) demographic parity, a
popular fairness criterion. We show theoretically that integer weights are
optimal, which means our method can be equivalently understood as duplicating
or eliminating samples. FairWASP can therefore be used to construct datasets
which can be fed into any classification method, not just methods which accept
sample weights. Our work is based on reformulating the pre-processing task as a
large-scale mixed-integer program (MIP), for which we propose a highly
efficient algorithm based on the cutting plane method. Experiments on synthetic
datasets demonstrate that our proposed optimization algorithm significantly
outperforms state-of-the-art commercial solvers in solving both the MIP and its
linear program relaxation. Further experiments highlight the competitive
performance of FairWASP in reducing disparities while preserving accuracy in
downstream classification settings.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00112" title="Abstract">arXiv:2311.00112</a> [<a href="/pdf/2311.00112" title="Download PDF">pdf</a>, <a href="/format/2311.00112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Optimization-based Control for Whole-body Loco-manipulation  of Heavy Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rigo%2C+A">Alberto Rigo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Muqun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S+K">Satyandra K. Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quan Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In recent years, the field of legged robotics has seen growing interest in
enhancing the capabilities of these robots through the integration of
articulated robotic arms. However, achieving successful loco-manipulation,
especially involving interaction with heavy objects, is far from
straightforward, as object manipulation can introduce substantial disturbances
that impact the robot's locomotion. This paper presents a novel framework for
legged loco-manipulation that considers whole-body coordination through a
hierarchical optimization-based control framework. First, an online
manipulation planner computes the manipulation forces and manipulated object
task-based reference trajectory. Then, pose optimization aligns the robot's
trajectory with kinematic constraints. The resultant robot reference trajectory
is executed via a linear MPC controller incorporating the desired manipulation
forces into its prediction model. Our approach has been validated in simulation
and hardware experiments, highlighting the necessity of whole-body optimization
compared to the baseline locomotion MPC when interacting with heavy objects.
Experimental results with Unitree Aliengo, equipped with a custom-made robotic
arm, showcase its ability to successfully lift and carry an 8kg payload and
manipulate doors.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00115" title="Abstract">arXiv:2311.00115</a> [<a href="/pdf/2311.00115" title="Download PDF">pdf</a>, <a href="/format/2311.00115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EXTRACT: Explainable Transparent Control of Bias in Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaozhen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lewis%2C+M">Martha Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Cristianini%2C+N">Nello Cristianini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Aequitas 2023: Workshop on Fairness and Bias in AI | co-located with ECAI 2023, Krak\'ow, Poland
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Knowledge Graphs are a widely used method to represent relations between
entities in various AI applications, and Graph Embedding has rapidly become a
standard technique to represent Knowledge Graphs in such a way as to facilitate
inferences and decisions. As this representation is obtained from behavioural
data, and is not in a form readable by humans, there is a concern that it might
incorporate unintended information that could lead to biases. We propose
EXTRACT: a suite of Explainable and Transparent methods to ConTrol bias in
knowledge graph embeddings, so as to assess and decrease the implicit presence
of protected information. Our method uses Canonical Correlation Analysis (CCA)
to investigate the presence, extent and origins of information leaks during
training, then decomposes embeddings into a sum of their private attributes by
solving a linear system. Our experiments, performed on the MovieLens1M dataset,
show that a range of personal attributes can be inferred from a user's viewing
behaviour and preferences, including gender, age, and occupation. Further
experiments, performed on the KG20C citation dataset, show that the information
about the conference in which a paper was published can be inferred from the
citation network of that article. We propose four transparent methods to
maintain the capability of the embedding to make the intended predictions
without retaining unwanted information. A trade-off between these two goals is
observed.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00116" title="Abstract">arXiv:2311.00116</a> [<a href="/pdf/2311.00116" title="Download PDF">pdf</a>, <a href="/format/2311.00116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BERTwich: Extending BERT&#x27;s Capabilities to Model Dialectal and Noisy  Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Aarohi Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+D">David Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Findings of the ACL: EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Real-world NLP applications often deal with nonstandard text (e.g.,
dialectal, informal, or misspelled text). However, language models like BERT
deteriorate in the face of dialect variation or noise. How do we push BERT's
modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it
is designed for specializing a model to a task and does not seem to bring about
the deeper, more pervasive changes needed to adapt a model to nonstandard
language. In this paper, we introduce the novel idea of sandwiching BERT's
encoder stack between additional encoder layers trained to perform masked
language modeling on noisy text. We find that our approach, paired with recent
work on including character-level noise in fine-tuning data, can promote
zero-shot transfer to dialectal text, as well as reduce the distance in the
embedding space between words and their noisy counterparts.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00117" title="Abstract">arXiv:2311.00117</a> [<a href="/pdf/2311.00117" title="Download PDF">pdf</a>, <a href="/format/2311.00117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gade%2C+P">Pranav Gade</a>, 
<a href="/search/cs?searchtype=author&query=Lermen%2C+S">Simon Lermen</a>, 
<a href="/search/cs?searchtype=author&query=Rogers-Smith%2C+C">Charlie Rogers-Smith</a>, 
<a href="/search/cs?searchtype=author&query=Ladish%2C+J">Jeffrey Ladish</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Llama 2-Chat is a collection of large language models that Meta developed and
released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output
harmful content, we hypothesize that public access to model weights enables bad
actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's
capabilities for malicious purposes. We demonstrate that it is possible to
effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than
$200, while retaining its general capabilities. Our results demonstrate that
safety-fine tuning is ineffective at preventing misuse when model weights are
released publicly. Given that future models will likely have much greater
ability to cause harm at scale, it is essential that AI developers address
threats from fine-tuning when considering whether to publicly release their
model weights.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00118" title="Abstract">arXiv:2311.00118</a> [<a href="/pdf/2311.00118" title="Download PDF">pdf</a>, <a href="/format/2311.00118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extracting the Multiscale Causal Backbone of Brain Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Acunto%2C+G">Gabriele D&#x27;Acunto</a>, 
<a href="/search/cs?searchtype=author&query=Bonchi%2C+F">Francesco Bonchi</a>, 
<a href="/search/cs?searchtype=author&query=De+Francisci+Morales%2C+G">Gianmarco De Francisci Morales</a>, 
<a href="/search/cs?searchtype=author&query=Petri%2C+G">Giovanni Petri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neurons and Cognition (q-bio.NC); Applications (stat.AP); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">The bulk of the research effort on brain connectivity revolves around
statistical associations among brain regions, which do not directly relate to
the causal mechanisms governing brain dynamics. Here we propose the multiscale
causal backbone (MCB) of brain dynamics shared by a set of individuals across
multiple temporal scales, and devise a principled methodology to extract it.
<br />Our approach leverages recent advances in multiscale causal structure
learning and optimizes the trade-off between the model fitting and its
complexity. Empirical assessment on synthetic data shows the superiority of our
methodology over a baseline based on canonical functional connectivity
networks. When applied to resting-state fMRI data, we find sparse MCBs for both
the left and right brain hemispheres. Thanks to its multiscale nature, our
approach shows that at low-frequency bands, causal dynamics are driven by brain
regions associated with high-level cognitive functions; at higher frequencies
instead, nodes related to sensory processing play a crucial role. Finally, our
analysis of individual multiscale causal structures confirms the existence of a
causal fingerprint of brain connectivity, thus supporting from a causal
perspective the existing extensive research in brain connectivity
fingerprinting.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00126" title="Abstract">arXiv:2311.00126</a> [<a href="/pdf/2311.00126" title="Download PDF">pdf</a>, <a href="/format/2311.00126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Time-Optimal Trajectory Planning for Connected and Automated  Vehicles in Mixed-Traffic Merging Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Le%2C+V">Viet-Anh Le</a>, 
<a href="/search/eess?searchtype=author&query=Chalaki%2C+B">Behdad Chalaki</a>, 
<a href="/search/eess?searchtype=author&query=Tzortzoglou%2C+F+N">Filippos N. Tzortzoglou</a>, 
<a href="/search/eess?searchtype=author&query=Malikopoulos%2C+A+A">Andreas A. Malikopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> first submission 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Addressing safe and efficient interaction between connected and automated
vehicles (CAVs) and human-driven vehicles in a mixed-traffic environment has
attracted considerable attention. In this paper, we develop a framework for
stochastic time-optimal trajectory planning for coordinating multiple CAVs in
mixed-traffic merging scenarios. We present a data-driven model, combining
Newell's car-following model with Bayesian linear regression, for efficiently
learning the driving behavior of human drivers online. Using the prediction
model and uncertainty quantification, a stochastic time-optimal control problem
is formulated to find robust trajectories for CAVs. We also integrate a
replanning mechanism that determines when deriving new trajectories for CAVs is
needed based on the accuracy of the Bayesian linear regression predictions.
Finally, we demonstrate the performance of our proposed framework using a
realistic simulation environment.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00128" title="Abstract">arXiv:2311.00128</a> [<a href="/pdf/2311.00128" title="Download PDF">pdf</a>, <a href="/format/2311.00128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the effect of curriculum learning with developmental data for grammar  acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Opper%2C+M">Mattia Opper</a>, 
<a href="/search/cs?searchtype=author&query=Morrison%2C+J">J. Morrison</a>, 
<a href="/search/cs?searchtype=author&query=Siddharth%2C+N">N. Siddharth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CoNLL-CMCL Shared Task BabyLM Challenge 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This work explores the degree to which grammar acquisition is driven by
language `simplicity' and the source modality (speech vs. text) of data. Using
BabyBERTa as a probe, we find that grammar acquisition is largely driven by
exposure to speech data, and in particular through exposure to two of the
BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this
finding by examining various ways of presenting input data to our model. First,
we assess the impact of various sequence-level complexity based curricula. We
then examine the impact of learning over `blocks' -- covering spans of text
that are balanced for the number of tokens in each of the source corpora
(rather than number of lines). Finally, we explore curricula that vary the
degree to which the model is exposed to different corpora. In all cases, we
find that over-exposure to AO-Childes and Open Subtitles significantly drives
performance. We verify these findings through a comparable control dataset in
which exposure to these corpora, and speech more generally, is limited by
design. Our findings indicate that it is not the proportion of tokens occupied
by high-utility data that aids acquisition, but rather the proportion of
training steps assigned to such data. We hope this encourages future research
into the use of more developmentally plausible linguistic data (which tends to
be more scarce) to augment general purpose pre-training regimes.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00134" title="Abstract">arXiv:2311.00134</a> [<a href="/pdf/2311.00134" title="Download PDF">pdf</a>, <a href="/format/2311.00134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Depth Prediction and Semantic Segmentation with Multi-View SAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shvets%2C+M">Mykhailo Shvets</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongxu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Niethammer%2C+M">Marc Niethammer</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+R">Roni Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Berg%2C+A+C">Alexander C. Berg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in the 2024 IEEE/CVF Winter Conference on Applications of Computer Vision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-task approaches to joint depth and segmentation prediction are
well-studied for monocular images. Yet, predictions from a single-view are
inherently limited, while multiple views are available in many robotics
applications. On the other end of the spectrum, video-based and full 3D methods
require numerous frames to perform reconstruction and segmentation. With this
work we propose a Multi-View Stereo (MVS) technique for depth prediction that
benefits from rich semantic features of the Segment Anything Model (SAM). This
enhanced depth prediction, in turn, serves as a prompt to our Transformer-based
semantic segmentation decoder. We report the mutual benefit that both tasks
enjoy in our quantitative and qualitative studies on the ScanNet dataset. Our
approach consistently outperforms single-task MVS and segmentation models,
along with multi-task monocular methods.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00143" title="Abstract">arXiv:2311.00143</a> [<a href="/pdf/2311.00143" title="Download PDF">pdf</a>, <a href="/format/2311.00143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Stage Classifier for Campaign Negativity Detection using Axis  Embeddings: A Case Study on Tweets of Political Users during 2021  Presidential Election in Iran
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajabi%2C+F">Fatemeh Rajabi</a>, 
<a href="/search/cs?searchtype=author&query=Mohades%2C+A">Ali Mohades</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In elections around the world, the candidates may turn their campaigns toward
negativity due to the prospect of failure and time pressure. In the digital
age, social media platforms such as Twitter are rich sources of political
discourse. Therefore, despite the large amount of data that is published on
Twitter, the automatic system for campaign negativity detection can play an
essential role in understanding the strategy of candidates and parties in their
campaigns. In this paper, we propose a hybrid model for detecting campaign
negativity consisting of a two-stage classifier that combines the strengths of
two machine learning models. Here, we have collected Persian tweets from 50
political users, including candidates and government officials. Then we
annotated 5,100 of them that were published during the year before the 2021
presidential election in Iran. In the proposed model, first, the required
datasets of two classifiers based on the cosine similarity of tweet embeddings
with axis embeddings (which are the average of embedding in positive and
negative classes of tweets) from the training set (85\%) are made, and then
these datasets are considered the training set of the two classifiers in the
hybrid model. Finally, our best model (RF-RF) was able to achieve 79\% for the
macro F1 score and 82\% for the weighted F1 score. By running the best model on
the rest of the tweets of 50 political users that were published one year
before the election and with the help of statistical models, we find that the
publication of a tweet by a candidate has nothing to do with the negativity of
that tweet, and the presence of the names of political persons and political
organizations in the tweet is directly related to its negativity.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00144" title="Abstract">arXiv:2311.00144</a> [<a href="/pdf/2311.00144" title="Download PDF">pdf</a>, <a href="/format/2311.00144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backdoor Threats from Compromised Foundation Models to Federated  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Songhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by FL@FM-NeurIPS 23 (International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023). The corresponding author is Jiaqi Wang (jqwang@psu.edu)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Federated learning (FL) represents a novel paradigm to machine learning,
addressing critical issues related to data privacy and security, yet suffering
from data insufficiency and imbalance. The emergence of foundation models (FMs)
provides a promising solution to the problems with FL. For instance, FMs could
serve as teacher models or good starting points for FL. However, the
integration of FM in FL presents a new challenge, exposing the FL systems to
potential threats. This paper investigates the robustness of FL incorporating
FMs by assessing their susceptibility to backdoor attacks. Contrary to classic
backdoor attacks against FL, the proposed attack (1) does not require the
attacker fully involved in the FL process; (2) poses a significant risk in
practical FL scenarios; (3) is able to evade existing robust FL frameworks/ FL
backdoor defenses; (4) underscores the researches on the robustness of FL
systems integrated with FMs. The effectiveness of the proposed attack is
demonstrated by extensive experiments with various well-known models and
benchmark datasets encompassing both text and image classification domains.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00152" title="Abstract">arXiv:2311.00152</a> [<a href="/pdf/2311.00152" title="Download PDF">pdf</a>, <a href="/format/2311.00152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing a Tool to Automate Extensions to Support a Flexible Extension  Policy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+J">Jordan Schwartz</a>, 
<a href="/search/cs?searchtype=author&query=Bohannan%2C+M">Madison Bohannan</a>, 
<a href="/search/cs?searchtype=author&query=Yim%2C+J">Jacob Yim</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuerou Tang</a>, 
<a href="/search/cs?searchtype=author&query=Benedicto%2C+D">Dana Benedicto</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Charisse Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+A">Armando Fox</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lisa Yan</a>, 
<a href="/search/cs?searchtype=author&query=Norouzi%2C+N">Narges Norouzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In this work, we present the development of an automated extension tool to
assist educators and increase the success and well-being of students by
implementing flexible extension policies. Flexible extension policies
materialize in many ways, yet there are similarities in students' interactions
with them; students tend to request multi-day long extensions repeatedly. In
courses with hundreds or potentially thousands of students, providing a system
to support this extension request demand is not possible given most currently
available resources and limited staff. As such, a tool is necessary to help
automate flexible extension processes. The development of this tool should
reduce staff load while increasing individualized student support, which can be
used in varying ways for different extension policies. Our research questions
are: RQ1: Does the extension tool reduce barriers and stigma around asking for
assistance? RQ2: Does the tool lessen the wait time between requesting and
receiving an extension, and how does the tool improve students' learning
experience in the course? These questions will help inform us about how an
automated tool for flexible extensions helps support growing course sizes and
students who may not otherwise receive the support they need for their success
and well-being in the course.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00153" title="Abstract">arXiv:2311.00153</a> [<a href="/pdf/2311.00153" title="Download PDF">pdf</a>, <a href="/format/2311.00153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards A Natural Language Interface for Flexible Multi-Agent Task  Assignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brawer%2C+J">Jake Brawer</a>, 
<a href="/search/cs?searchtype=author&query=Bishop%2C+K">Kayleigh Bishop</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+B">Bradley Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Roncone%2C+A">Alessandro Roncone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Task assignment and scheduling algorithms are powerful tools for autonomously
coordinating large teams of robotic or AI agents. However, the decisions these
system make often rely on components designed by domain experts, which can be
difficult for non-technical end-users to understand or modify to their own
ends. In this paper we propose a preliminary design for a flexible natural
language interface for a task assignment system. The goal of our approach is
both to grant users more control over a task assignment system's decision
process, as well as render these decisions more transparent. Users can direct
the task assignment system via natural language commands, which are applied as
constraints to a mixed-integer linear program (MILP) using a large language
model (LLM). Additionally, our proposed system can alert users to potential
issues with their commands, and engage them in a corrective dialogue in order
to find a viable solution. We conclude with a description of our planned
user-evaluation in the simulated environment Overcooked and describe next steps
towards developing a flexible and transparent task allocation system.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00156" title="Abstract">arXiv:2311.00156</a> [<a href="/pdf/2311.00156" title="Download PDF">pdf</a>, <a href="/format/2311.00156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Cloudonomics of Efficient I/O for Data-Intensive  Analytics Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+C">Chunxu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+B">Bin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beinan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shouwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Ziyue Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mingmin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhongting Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Databases (cs.DB)

</div>
<p class="mathjax">This paper explores a prevailing trend in the industry: migrating
data-intensive analytics applications from on-premises to cloud-native
environments. We find that the unique cost models associated with cloud-based
storage necessitate a more nuanced understanding of optimizing performance.
Specifically, based on traces collected from Uber's Presto fleet in production,
we argue that common I/O optimizations, such as table scan and filter, and
broadcast join, may lead to unexpected costs when naively applied in the cloud.
This is because traditional I/O optimizations mainly focus on improving
throughput or latency in on-premises settings, without taking into account the
monetary costs associated with storage API calls. In cloud environments, these
costs can be significant, potentially involving billions of API calls per day
just for Presto workloads at Uber scale. Presented as a case study, this paper
serves as a starting point for further research to design efficient I/O
strategies specifically tailored for data-intensive applications in cloud
settings.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00157" title="Abstract">arXiv:2311.00157</a> [<a href="/pdf/2311.00157" title="Download PDF">pdf</a>, <a href="/format/2311.00157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score Normalization for a Faster Diffusion Exponential Integrator  Sampler
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Guoxuan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Danier%2C+D">Duolikun Danier</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Ayan Das</a>, 
<a href="/search/cs?searchtype=author&query=Fotiadis%2C+S">Stathi Fotiadis</a>, 
<a href="/search/cs?searchtype=author&query=Nabiei%2C+F">Farhang Nabiei</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+U">Ushnish Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Bernacchia%2C+A">Alberto Bernacchia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recently, zhang et al have proposed the Diffusion Exponential Integrator
Sampler (DEIS) for fast generation of samples from Diffusion Models. It
leverages the semi-linear nature of the probability flow ordinary differential
equation (ODE) in order to greatly reduce integration error and improve
generation quality at low numbers of function evaluations (NFEs). Key to this
approach is the score function reparameterisation, which reduces the
integration error incurred from using a fixed score function estimate over each
integration step. The original authors use the default parameterisation used by
models trained for noise prediction -- multiply the score by the standard
deviation of the conditional forward noising distribution. We find that
although the mean absolute value of this score parameterisation is close to
constant for a large portion of the reverse sampling process, it changes
rapidly at the end of sampling. As a simple fix, we propose to instead
reparameterise the score (at inference) by dividing it by the average absolute
value of previous score estimates at that time step collected from offline high
NFE generations. We find that our score normalisation (DEIS-SN) consistently
improves FID compared to vanilla DEIS, showing an FID improvement from 6.44 to
5.57 at 10 NFEs for our CIFAR-10 experiments. Our code is available at
https://github.com/mtkresearch/Diffusion-DEIS-SN.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00159" title="Abstract">arXiv:2311.00159</a> [<a href="/pdf/2311.00159" title="Download PDF">pdf</a>, <a href="/format/2311.00159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Longer Fixations, More Computation: Gaze-Guided Recurrent Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Jiajing Wan</a>, 
<a href="/search/cs?searchtype=author&query=Kritikos%2C+I">Ioannis Kritikos</a>, 
<a href="/search/cs?searchtype=author&query=Hollenstein%2C+N">Nora Hollenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Humans read texts at a varying pace, while machine learning models treat each
token in the same way in terms of a computational process. Therefore, we ask,
does it help to make models act more like humans? In this paper, we convert
this intuition into a set of novel models with fixation-guided parallel RNNs or
layers and conduct various experiments on language modeling and sentiment
analysis tasks to test their effectiveness, thus providing empirical validation
for this intuition. Our proposed models achieve good performance on the
language modeling task, considerably surpassing the baseline model. In
addition, we find that, interestingly, the fixation duration predicted by
neural networks bears some resemblance to humans' fixation. Without any
explicit guidance, the model makes similar choices to humans. We also
investigate the reasons for the differences between them, which explain why
"model fixations" are often more suitable than human fixations, when used to
guide language models.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00161" title="Abstract">arXiv:2311.00161</a> [<a href="/pdf/2311.00161" title="Download PDF">pdf</a>, <a href="/format/2311.00161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Denouncing Hate: Strategies for Countering Implied Biases and  Stereotypes in Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mun%2C+J">Jimin Mun</a>, 
<a href="/search/cs?searchtype=author&query=Allaway%2C+E">Emily Allaway</a>, 
<a href="/search/cs?searchtype=author&query=Yerukola%2C+A">Akhila Yerukola</a>, 
<a href="/search/cs?searchtype=author&query=Vianna%2C+L">Laura Vianna</a>, 
<a href="/search/cs?searchtype=author&query=Leslie%2C+S">Sarah-Jane Leslie</a>, 
<a href="/search/cs?searchtype=author&query=Sap%2C+M">Maarten Sap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings, 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Counterspeech, i.e., responses to counteract potential harms of hateful
speech, has become an increasingly popular solution to address online hate
speech without censorship. However, properly countering hateful language
requires countering and dispelling the underlying inaccurate stereotypes
implied by such language. In this work, we draw from psychology and philosophy
literature to craft six psychologically inspired strategies to challenge the
underlying stereotypical implications of hateful language. We first examine the
convincingness of each of these strategies through a user study, and then
compare their usages in both human- and machine-generated counterspeech
datasets. Our results show that human-written counterspeech uses countering
strategies that are more specific to the implied stereotype (e.g., counter
examples to the stereotype, external factors about the stereotype's origins),
whereas machine-generated counterspeech uses less specific strategies (e.g.,
generally denouncing the hatefulness of speech). Furthermore, machine-generated
counterspeech often employs strategies that humans deem less convincing
compared to human-produced counterspeech. Our findings point to the importance
of accounting for the underlying stereotypical implications of speech when
generating counterspeech and for better machine reasoning about
anti-stereotypical examples.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00164" title="Abstract">arXiv:2311.00164</a> [<a href="/pdf/2311.00164" title="Download PDF">pdf</a>, <a href="/format/2311.00164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations  for Accident Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nippani%2C+A">Abhinav Nippani</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+H">Haotian Ju</a>, 
<a href="/search/cs?searchtype=author&query=Koutsopoulos%2C+H+N">Haris N. Koutsopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H+R">Hongyang R. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages. To appear in NeurIPS 2023 (D&amp;B)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of traffic accident analysis on a road network based
on road network connections and traffic volume. Previous works have designed
various deep-learning methods using historical records to predict traffic
accident occurrences. However, there is a lack of consensus on how accurate
existing methods are, and a fundamental issue is the lack of public accident
datasets for comprehensive evaluations. This paper constructs a large-scale,
unified dataset of traffic accident records from official reports of various
states in the US, totaling 9 million records, accompanied by road networks and
traffic volume reports. Using this new dataset, we evaluate existing
deep-learning methods for predicting the occurrence of accidents on road
networks. Our main finding is that graph neural networks such as GraphSAGE can
accurately predict the number of accidents on roads with less than 22% mean
absolute error (relative to the actual count) and whether an accident will
occur or not with over 87% AUROC, averaged over states. We achieve these
results by using multitask learning to account for cross-state variabilities
(e.g., availability of accident labels) and transfer learning to combine
traffic volume with accident prediction. Ablation studies highlight the
importance of road graph-structural features, amongst other features. Lastly,
we discuss the implications of the analysis and develop a package for easily
using our new dataset.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00167" title="Abstract">arXiv:2311.00167</a> [<a href="/pdf/2311.00167" title="Download PDF">pdf</a>, <a href="/format/2311.00167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-task Deep Convolutional Network to Predict Sea Ice Concentration  and Drift in the Arctic Ocean
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koo%2C+Y">Younghyun Koo</a>, 
<a href="/search/cs?searchtype=author&query=Rahnemoonfar%2C+M">Maryam Rahnemoonfar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic
Ocean is of great significance as the Arctic environment has been changed by
the recent warming climate. Given that physical sea ice models require high
computational costs with complex parameterization, deep learning techniques can
effectively replace the physical model and improve the performance of sea ice
prediction. This study proposes a novel multi-task fully conventional network
architecture named hierarchical information-sharing U-net (HIS-Unet) to predict
daily SIC and SID. Instead of learning SIC and SID separately at each branch,
we allow the SIC and SID layers to share their information and assist each
other's prediction through the weighting attention modules (WAMs).
Consequently, our HIS-Unet outperforms other statistical approaches, sea ice
physical models, and neural networks without such information-sharing units.
The improvement of HIS-Unet is obvious both for SIC and SID prediction when and
where sea ice conditions change seasonally, which implies that the information
sharing through WAMs allows the model to learn the sudden changes of SIC and
SID. The weight values of the WAMs imply that SIC information plays a more
critical role in SID prediction, compared to that of SID information in SIC
prediction, and information sharing is more active in sea ice edges (seasonal
sea ice) than in the central Arctic (multi-year sea ice).
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00168" title="Abstract">arXiv:2311.00168</a> [<a href="/pdf/2311.00168" title="Download PDF">pdf</a>, <a href="/format/2311.00168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from  Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lambert%2C+N">Nathan Lambert</a>, 
<a href="/search/cs?searchtype=author&query=Calandra%2C+R">Roberto Calandra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) easier to prompt and more
capable in complex settings. RLHF at its core is providing a new toolkit to
optimize LLMs other than next-token prediction, enabling the integration of
qualitative training goals. The attempted match between user preferences and
downstream performance, which happens in a learned reward model, results in an
optimization landscape where training and evaluation metrics can appear
correlated. The apparent correlation can lead to unexpected behaviors and
stories of "too much RLHF." In RLHF, challenges emerge because the following
sub-modules are not consistent with each other: the reward model training, the
policy model training, and the policy model evaluation. This mismatch results
in models that sometimes avoid user requests for false safety flags, are
difficult to steer to an intended characteristic, or always answer in a
specific style. As chat model evaluation becomes increasingly nuanced, the
reliance on a perceived link between reward model score and downstream
performance drives the objective mismatch issue. In this paper, we illustrate
the cause of this issue, reviewing relevant literature from model-based
reinforcement learning, and discuss relevant solutions to encourage further
research. By solving objective mismatch in RLHF, the LLMs of the future will be
more precisely aligned to user instructions for both safety and helpfulness.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00172" title="Abstract">arXiv:2311.00172</a> [<a href="/pdf/2311.00172" title="Download PDF">pdf</a>, <a href="/format/2311.00172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Safety Classifier for Large Language Models: Adversarial Prompt  Shield
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinhwa Kim</a>, 
<a href="/search/cs?searchtype=author&query=Derakhshan%2C+A">Ali Derakhshan</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+I+G">Ian G. Harris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models' safety remains a critical concern due to their
vulnerability to adversarial attacks, which can prompt these systems to produce
harmful responses. In the heart of these systems lies a safety classifier, a
computational model trained to discern and mitigate potentially harmful,
offensive, or unethical outputs. However, contemporary safety classifiers,
despite their potential, often fail when exposed to inputs infused with
adversarial noise. In response, our study introduces the Adversarial Prompt
Shield (APS), a lightweight model that excels in detection accuracy and
demonstrates resilience against adversarial prompts. Additionally, we propose
novel strategies for autonomously generating adversarial training datasets,
named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are
designed to fortify the safety classifier's robustness, and we investigate the
consequences of incorporating adversarial examples into the training process.
Through evaluations involving Large Language Models, we demonstrate that our
classifier has the potential to decrease the attack success rate resulting from
adversarial attacks by up to 60%. This advancement paves the way for the next
generation of more reliable and resilient conversational agents.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00176" title="Abstract">arXiv:2311.00176</a> [<a href="/pdf/2311.00176" title="Download PDF">pdf</a>, <a href="/format/2311.00176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChipNeMo: Domain-Adapted LLMs for Chip Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ene%2C+T">Teo Ene</a>, 
<a href="/search/cs?searchtype=author&query=Kirby%2C+R">Robert Kirby</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Chris Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Pinckney%2C+N">Nathaniel Pinckney</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+R">Rongjian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Alben%2C+J">Jonah Alben</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+H">Himyanshu Anand</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Sanmitra Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Bayraktaroglu%2C+I">Ismet Bayraktaroglu</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskaran%2C+B">Bonita Bhaskaran</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+A">Arjun Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Clay%2C+S">Sharon Clay</a>, 
<a href="/search/cs?searchtype=author&query=Dally%2C+B">Bill Dally</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+L">Laura Dang</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+P">Parikshit Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Dhodhi%2C+S">Siddhanth Dhodhi</a>, 
<a href="/search/cs?searchtype=author&query=Halepete%2C+S">Sameer Halepete</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+E">Eric Hill</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiashang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Sumit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Khailany%2C+B">Brucek Khailany</a>, 
<a href="/search/cs?searchtype=author&query=Kunal%2C+K">Kishor Kunal</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaowei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Oberman%2C+S">Stuart Oberman</a>, 
<a href="/search/cs?searchtype=author&query=Omar%2C+S">Sujeet Omar</a>, 
<a href="/search/cs?searchtype=author&query=Pratty%2C+S">Sreedhar Pratty</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Ambar Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhengjiang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hanfei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Suthar%2C+P+P">Pratik P Suthar</a>, 
<a href="/search/cs?searchtype=author&query=Tej%2C+V">Varun Tej</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaizhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Haoxing Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">ChipNeMo aims to explore the applications of large language models (LLMs) for
industrial chip design. Instead of directly deploying off-the-shelf commercial
or open-source LLMs, we instead adopt the following domain adaptation
techniques: custom tokenizers, domain-adaptive continued pretraining,
supervised fine-tuning (SFT) with domain-specific instructions, and
domain-adapted retrieval models. We evaluate these methods on three selected
LLM applications for chip design: an engineering assistant chatbot, EDA script
generation, and bug summarization and analysis. Our results show that these
domain adaptation techniques enable significant LLM performance improvements
over general-purpose base models across the three evaluated applications,
enabling up to 5x model size reduction with similar or better performance on a
range of design tasks. Our findings also indicate that there's still room for
improvement between our current results and ideal outcomes. We believe that
further investigation of domain-adapted LLM approaches will help close this gap
in the future.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00177" title="Abstract">arXiv:2311.00177</a> [<a href="/pdf/2311.00177" title="Download PDF">pdf</a>, <a href="/format/2311.00177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Students&#x27; Perspective on AI Code Completion: Benefits and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takerngsaksiri%2C+W">Wannita Takerngsaksiri</a>, 
<a href="/search/cs?searchtype=author&query=Warusavitarne%2C+C">Cleshan Warusavitarne</a>, 
<a href="/search/cs?searchtype=author&query=Yaacoub%2C+C">Christian Yaacoub</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+M+H+K">Matthew Hee Keng Hou</a>, 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review at IEEE Software
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">AI Code Completion (e.g., GitHub's Copilot, Amazon CodeWhisperer) has
revolutionized the way in which computer science students interact with
programming languages. However, these tools are not available for free public
use, preventing us from conducting our research. In addition, AI code
completion has been studied from developers' perspective, not students'
perspective who represent the future generation of our digital world. In this
article, we investigated the benefits, challenges, and expectations of AI code
completion from students' perspectives and introduced AutoAurora, an AI code
completion tool integrated into the Visual Studio Code Extension as a research
instrument. Through an interview study with ten participants, we found that AI
code completion enhanced students' productivity and efficiency by providing
correct syntax suggestions, offering alternative solutions, and functioning as
a coding tutor. However, the over-reliance on AI code completion may lead to a
surface-level understanding of programming concepts, diminishing
problem-solving skills and restricting creativity. In the future, AI code
completion must be explainable to facilitate the learning of coding concepts.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00180" title="Abstract">arXiv:2311.00180</a> [<a href="/pdf/2311.00180" title="Download PDF">pdf</a>, <a href="/format/2311.00180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-centric Video Representation for Long-term Action Anticipation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Ce Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Changcheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+N">Nakul Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kwonjoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Chiho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an accepted WACV 2024 paper. Our code is available at <a href="https://github.com/brown-palm/ObjectPrompt">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper focuses on building object-centric representations for long-term
action anticipation in videos. Our key motivation is that objects provide
important cues to recognize and predict human-object interactions, especially
when the predictions are longer term, as an observed "background" object could
be used by the human actor in the future. We observe that existing object-based
video recognition frameworks either assume the existence of in-domain
supervised object detectors or follow a fully weakly-supervised pipeline to
infer object locations from action labels. We propose to build object-centric
video representations by leveraging visual-language pretrained models. This is
achieved by "object prompts", an approach to extract task-specific
object-centric representations from general-purpose pretrained models without
finetuning. To recognize and predict human-object interactions, we use a
Transformer-based neural architecture which allows the "retrieval" of relevant
objects for action anticipation at various time scales. We conduct extensive
evaluations on the Ego4D, 50Salads, and EGTEA Gaze+ benchmarks. Both
quantitative and qualitative results confirm the effectiveness of our proposed
method.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00182" title="Abstract">arXiv:2311.00182</a> [<a href="/pdf/2311.00182" title="Download PDF">pdf</a>, <a href="/ps/2311.00182" title="Download PostScript">ps</a>, <a href="/format/2311.00182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Max-Cut on Sparse Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwartzman%2C+G">Gregory Schwartzman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We bound the smoothed running time of the FLIP algorithm for local Max-Cut as
a function of $\alpha$, the arboricity of the input graph. We show that, with
high probability, the following holds (where $n$ is the number of nodes and
$\phi$ is the smoothing parameter):
<br />1) When $\alpha = O(\sqrt{\log n})$ FLIP terminates in $\phi poly(n)$
iterations. Previous to our results the only graph families for which FLIP was
known to achieve a smoothed polynomial running time were complete graphs and
graphs with logarithmic maximum degree.
<br />2) For arbitrary values of $\alpha$ we get a running time of $\phi
n^{O(\frac{\alpha}{\log n} + \log \alpha)}$. This improves over the best known
running time for general graphs of $\phi n^{O(\sqrt{ \log n })}$ for $\alpha =
o(\log^{1.5} n)$. Specifically, when $\alpha = O(\log n)$ we get a
significantly faster running time of $\phi n^{O(\log \log n)}$.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00187" title="Abstract">arXiv:2311.00187</a> [<a href="/pdf/2311.00187" title="Download PDF">pdf</a>, <a href="/format/2311.00187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decodable and Sample Invariant Continuous Object Encoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+D">Dehao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ferm%C3%BCller%2C+C">Cornelia Ferm&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Aloimonos%2C+Y">Yiannis Aloimonos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a
continuous object (e.g. a function), HDFE produces an explicit vector
representation of the given object, invariant to the sample distribution and
density. Sample distribution and density invariance enables HDFE to
consistently encode continuous objects regardless of their sampling, and
therefore allows neural networks to receive continuous objects as inputs for
machine learning tasks, such as classification and regression. Besides, HDFE
does not require any training and is proved to map the object into an organized
embedding space, which facilitates the training of the downstream tasks. In
addition, the encoding is decodable, which enables neural networks to regress
continuous objects by regressing their encodings. Therefore, HDFE serves as an
interface for processing continuous objects.
<br />We apply HDFE to function-to-function mapping, where vanilla HDFE achieves
competitive performance as the state-of-the-art algorithm. We apply HDFE to
point cloud surface normal estimation, where a simple replacement from PointNet
to HDFE leads to immediate 12% and 15% error reductions in two benchmarks. In
addition, by integrating HDFE into the PointNet-based SOTA network, we improve
the SOTA baseline by 2.5% and 1.7% in the same benchmarks.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00189" title="Abstract">arXiv:2311.00189</a> [<a href="/pdf/2311.00189" title="Download PDF">pdf</a>, <a href="/format/2311.00189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajialigol%2C+D">Daniel Hajialigol</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text classification aims to effectively categorize documents into pre-defined
categories. Traditional methods for text classification often rely on large
amounts of manually annotated training data, making the process time-consuming
and labor-intensive. To address this issue, recent studies have focused on
weakly-supervised and extremely weakly-supervised settings, which require
minimal or no human annotation, respectively. In previous methods of weakly
supervised text classification, pseudo-training data is generated by assigning
pseudo-labels to documents based on their alignment (e.g., keyword matching)
with specific classes. However, these methods ignore the importance of
incorporating the explanations of the generated pseudo-labels, or saliency of
individual words, as additional guidance during the text classification
training process. To address this limitation, we propose XAI-CLASS, a novel
explanation-enhanced extremely weakly-supervised text classification method
that incorporates word saliency prediction as an auxiliary task. XAI-CLASS
begins by employing a multi-round question-answering process to generate
pseudo-training data that promotes the mutual enhancement of class labels and
corresponding explanation word generation. This pseudo-training data is then
used to train a multi-task framework that simultaneously learns both text
classification and word saliency prediction. Extensive experiments on several
weakly-supervised text classification datasets show that XAI-CLASS outperforms
other weakly-supervised text classification methods significantly. Moreover,
experiments demonstrate that XAI-CLASS enhances both model performance and
explainability.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00192" title="Abstract">arXiv:2311.00192</a> [<a href="/pdf/2311.00192" title="Download PDF">pdf</a>, <a href="/format/2311.00192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brown%2C+K">Kyle Brown</a>, 
<a href="/search/cs?searchtype=author&query=Asmar%2C+D+M">Dylan M. Asmar</a>, 
<a href="/search/cs?searchtype=author&query=Schwager%2C+M">Mac Schwager</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Repository: <a href="https://github.com/sisl/ConstructionBots.jl.">this https URL</a> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Mobile autonomous robots have the potential to revolutionize manufacturing
processes. However, employing large robot fleets in manufacturing requires
addressing challenges including collision-free movement in a shared workspace,
effective multi-robot collaboration to manipulate and transport large payloads,
complex task allocation due to coupled manufacturing processes, and spatial
planning for parallel assembly and transportation of nested subassemblies. We
propose a full algorithmic stack for large-scale multi-robot assembly planning
that addresses these challenges and can synthesize construction plans for
complex assemblies with thousands of parts in a matter of minutes. Our approach
takes in a CAD-like product specification and automatically plans a full-stack
assembly procedure for a group of robots to manufacture the product. We propose
an algorithmic stack that comprises: (i) an iterative radial layout
optimization procedure to define a global staging layout for the manufacturing
facility, (ii) a graph-repair mixed-integer program formulation and a modified
greedy task allocation algorithm to optimally allocate robots and robot
sub-teams to assembly and transport tasks, (iii) a geometric heuristic and a
hill-climbing algorithm to plan collaborative carrying configurations of robot
sub-teams, and (iv) a distributed control policy that enables robots to execute
the assembly motion plan collision-free. We also present an open-source
multi-robot manufacturing simulator implemented in Julia as a resource to the
research community, to test our algorithms and to facilitate multi-robot
manufacturing research more broadly. Our empirical results demonstrate the
scalability and effectiveness of our approach by generating plans to
manufacture a LEGO model of a Saturn V launch vehicle with 1845 parts, 306
subassemblies, and 250 robots in under three minutes on a standard laptop
computer.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00197" title="Abstract">arXiv:2311.00197</a> [<a href="/pdf/2311.00197" title="Download PDF">pdf</a>, <a href="/format/2311.00197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design, Modeling, and Control of a Low-Cost and Rapid Response  Soft-Growing Manipulator for Orchard Operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorosh%2C+R">Ryan Dorosh</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+J">Justin Allen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zixuan He</a>, 
<a href="/search/cs?searchtype=author&query=Ninatanta%2C+C">Christopher Ninatanta</a>, 
<a href="/search/cs?searchtype=author&query=Coleman%2C+J">Jack Coleman</a>, 
<a href="/search/cs?searchtype=author&query=Spieker%2C+J">Jack Spieker</a>, 
<a href="/search/cs?searchtype=author&query=Tuck%2C+E">Ethan Tuck</a>, 
<a href="/search/cs?searchtype=author&query=Kurtz%2C+J">Jordan Kurtz</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Whiting%2C+M+D">Matthew D. Whiting</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jiecai Luo</a>, 
<a href="/search/cs?searchtype=author&query=Karkee%2C+M">Manoj Karkee</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Ming Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Intelligent Robots and Systems (IROS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Tree fruit growers around the world are facing labor shortages for critical
operations, including harvest and pruning. There is a great interest in
developing robotic solutions for these labor-intensive tasks, but current
efforts have been prohibitively costly, slow, or require a reconfiguration of
the orchard in order to function. In this paper, we introduce an alternative
approach to robotics using a novel and low-cost soft-growing robotic platform.
Our platform features the ability to extend up to 1.2 m linearly at a maximum
speed of 0.27 m/s. The soft-growing robotic arm can operate with a terminal
payload of up to 1.4 kg (4.4 N), more than sufficient for carrying an apple.
This platform decouples linear and steering motions to simplify path planning
and the controller design for targeting. We anticipate our platform being
relatively simple to maintain compared to rigid robotic arms. Herein we also
describe and experimentally verify the platform's kinematic model, including
the prediction of the relationship between the steering angle and the angular
positions of the three steering motors. Information from the model enables the
position controller to guide the end effector to the targeted positions faster
and with higher stability than without this information. Overall, our research
show promise for using soft-growing robotic platforms in orchard operations.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00199" title="Abstract">arXiv:2311.00199</a> [<a href="/pdf/2311.00199" title="Download PDF">pdf</a>, <a href="/format/2311.00199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the alternating randomized block Kaczmarz method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+N">Nian-Ci Wu</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/math?searchtype=author&query=Tian%2C+Z">Zhaolu Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The block Kaczmarz method and its variants are designed for solving the
over-determined linear system. They involve iteratively projecting the current
point onto the solution space of a subset of constraints. In this work, by
alternately dealing with two subproblems (i.e., linear system with multiple
right-hand sides) using the block Kaczmarz method, we propose the {\it
Alternating Randomized Block Kaczmarz} (ARBK) method to solve the linear matrix
equation $AXB=F$, which incorporates a randomized index selection scheme to
determine the subset of constraints. The convergence analysis reveals that the
ARBK method has a linear convergence rate bounded by an explicit expression.
Several numerical studies have been conducted to validate the theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00201" title="Abstract">arXiv:2311.00201</a> [<a href="/pdf/2311.00201" title="Download PDF">pdf</a>, <a href="/ps/2311.00201" title="Download PostScript">ps</a>, <a href="/format/2311.00201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Natural Policy Gradient Methods for Multi-task Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cen%2C+S">Shicong Cen</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yuting Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated reinforcement learning (RL) enables collaborative decision making
of multiple distributed agents without sharing local data trajectories. In this
work, we consider a multi-task setting, in which each agent has its own private
reward function corresponding to different tasks, while sharing the same
transition kernel of the environment. Focusing on infinite-horizon tabular
Markov decision processes, the goal is to learn a globally optimal policy that
maximizes the sum of the discounted total rewards of all the agents in a
decentralized manner, where each agent only communicates with its neighbors
over some prescribed graph topology. We develop federated vanilla and
entropy-regularized natural policy gradient (NPG) methods under softmax
parameterization, where gradient tracking is applied to the global Q-function
to mitigate the impact of imperfect information sharing. We establish
non-asymptotic global convergence guarantees under exact policy evaluation,
which are nearly independent of the size of the state-action space and
illuminate the impacts of network size and connectivity. To the best of our
knowledge, this is the first time that global convergence is established for
federated multi-task RL using policy optimization. Moreover, the convergence
behavior of the proposed algorithms is robust against inexactness of policy
evaluation.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00203" title="Abstract">arXiv:2311.00203</a> [<a href="/pdf/2311.00203" title="Download PDF">pdf</a>, <a href="/format/2311.00203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling subjectivity (by Mimicking Annotator Annotation) in toxic  comment identification across diverse communities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Senjuti Dutta</a> (1), 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Sid Mittal</a> (2), 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sherol Chen</a> (2), 
<a href="/search/cs?searchtype=author&query=Ramachandran%2C+D">Deepak Ramachandran</a> (2), 
<a href="/search/cs?searchtype=author&query=Rajakumar%2C+R">Ravi Rajakumar</a> (2), 
<a href="/search/cs?searchtype=author&query=Kivlichan%2C+I">Ian Kivlichan</a> (2), 
<a href="/search/cs?searchtype=author&query=Mak%2C+S">Sunny Mak</a> (2), 
<a href="/search/cs?searchtype=author&query=Butryna%2C+A">Alena Butryna</a> (2), 
<a href="/search/cs?searchtype=author&query=Paritosh%2C+P">Praveen Paritosh</a> (2) ((1) University of Tennessee, Knoxville, (2) Google LLC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The prevalence and impact of toxic discussions online have made content
moderation crucial.Automated systems can play a vital role in identifying
toxicity, and reducing the reliance on human moderation.Nevertheless,
identifying toxic comments for diverse communities continues to present
challenges that are addressed in this paper.The two-part goal of this study is
to(1)identify intuitive variances from annotator disagreement using
quantitative analysis and (2)model the subjectivity of these viewpoints.To
achieve our goal, we published a new
dataset\footnote{\url{https://github.com/XXX}} with expert annotators'
annotations and used two other public datasets to identify the subjectivity of
toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's
ability to mimic diverse viewpoints on toxicity by varying size of the training
data and utilizing same set of annotators as the test set used during model
training and a separate set of annotators as the test set.We conclude that
subjectivity is evident across all annotator groups, demonstrating the
shortcomings of majority-rule voting. Moving forward, subjective annotations
should serve as ground truth labels for training models for domains like
toxicity in diverse communities.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00204" title="Abstract">arXiv:2311.00204</a> [<a href="/pdf/2311.00204" title="Download PDF">pdf</a>, <a href="/format/2311.00204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Training and Fine-tuning for Domain-Specific Language Models  in Medical Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yining Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models exhibit promising general capabilities but often lack
specialized knowledge for domain-specific tasks. Developing domain experts from
a base model enables a range of applications without prohibitive training
costs. This work demonstrates a method using continuous training and
instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese
medical domain. We first conduct continuous training on 1B tokens from Chinese
medical references to teach relevant vocabulary and knowledge. The models are
then fine-tuned on 54K examples sourced from the Chinese National Medical
Licensing Examination. Experiments on Chinese medical data confirm the
effectiveness of this approach, producing a model comparable to GPT-3.5-turbo
while using way less computational resource. The resulting domain-specific
model could be useful for various Chinese medical applications. More broadly,
this provides a template for domain-specific training of large language models
in areas where pre-trained models lack the required expertise, such as law,
science, and engineering.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00206" title="Abstract">arXiv:2311.00206</a> [<a href="/pdf/2311.00206" title="Download PDF">pdf</a>, <a href="/format/2311.00206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT-Powered Hierarchical Comparisons for Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhiyuan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yiyang Su</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2023 Poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The zero-shot open-vocabulary challenge in image classification is tackled by
pretrained vision-language models like CLIP, which benefit from incorporating
class-specific knowledge from large language models (LLMs) like ChatGPT.
However, biases in CLIP lead to similar descriptions for distinct but related
classes, prompting our novel image classification framework via hierarchical
comparisons: using LLMs to recursively group classes into hierarchies and
classifying images by comparing image-text embeddings at each hierarchy level,
resulting in an intuitive, effective, and explainable approach.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00207" title="Abstract">arXiv:2311.00207</a> [<a href="/pdf/2311.00207" title="Download PDF">pdf</a>, <a href="/format/2311.00207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based  Wireless Communication Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jung-Woo Chang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>, 
<a href="/search/cs?searchtype=author&query=Heydaribeni%2C+N">Nasimeh Heydaribeni</a>, 
<a href="/search/cs?searchtype=author&query=Hidano%2C+S">Seira Hidano</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Koushanfar%2C+F">Farinaz Koushanfar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine Learning (ML) has been instrumental in enabling joint transceiver
optimization by merging all physical layer blocks of the end-to-end wireless
communication systems. Although there have been a number of adversarial attacks
on ML-based wireless systems, the existing methods do not provide a
comprehensive view including multi-modality of the source data, common physical
layer components, and wireless domain constraints. This paper proposes Magmaw,
the first black-box attack methodology capable of generating universal
adversarial perturbations for any multimodal signal transmitted over a wireless
channel. We further introduce new objectives for adversarial attacks on
ML-based downstream applications. The resilience of the attack to the existing
widely used defense methods of adversarial training and perturbation signal
subtraction is experimentally verified. For proof-of-concept evaluation, we
build a real-time wireless attack platform using a software-defined radio
system. Experimental results demonstrate that Magmaw causes significant
performance degradation even in the presence of the defense mechanisms.
Surprisingly, Magmaw is also effective against encrypted communication channels
and conventional communications.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00208" title="Abstract">arXiv:2311.00208</a> [<a href="/pdf/2311.00208" title="Download PDF">pdf</a>, <a href="/format/2311.00208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers as Recognizers of Formal Languages: A Survey on  Expressivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Strobl%2C+L">Lena Strobl</a>, 
<a href="/search/cs?searchtype=author&query=Merrill%2C+W">William Merrill</a>, 
<a href="/search/cs?searchtype=author&query=Weiss%2C+G">Gail Weiss</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+D">David Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Angluin%2C+D">Dana Angluin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">As transformers have gained prominence in natural language processing, some
researchers have investigated theoretically what problems they can and cannot
solve, by treating problems as formal languages. Exploring questions such as
this will help to compare transformers with other models, and transformer
variants with one another, for various tasks. Work in this subarea has made
considerable progress in recent years. Here, we undertake a comprehensive
survey of this work, documenting the diverse assumptions that underlie
different results and providing a unified framework for harmonizing seemingly
contradictory findings.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00211" title="Abstract">arXiv:2311.00211</a> [<a href="/pdf/2311.00211" title="Download PDF">pdf</a>, <a href="/ps/2311.00211" title="Download PostScript">ps</a>, <a href="/format/2311.00211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anachronic Tertiary Studies in Software Engineering: An Exploratory  Quaternary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neto%2C+V+V+G">Valdemar Vicente Graciano Neto</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+C+L">C&#xe9;lia La&#xed;s Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Kamei%2C+F+K">Fernando Kenji Kamei</a>, 
<a href="/search/cs?searchtype=author&query=de+Oliveira%2C+J+L">Juliano Lopes de Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=de+Lima%2C+E+A">Eliomar Ara&#xfa;jo de Lima</a>, 
<a href="/search/cs?searchtype=author&query=Kassab%2C+M">Mohamad Kassab</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+R">Roberto Oliveira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, not peer-reviewed yet
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Systematic literature reviews tentativelydescribe the state of the art in a
given research area. However, the continuous publication of new primary and
secondary studies following the release of a tertiary study can make the
communication of results not integrally representative in regards to the
advances achieved by that time. Consequently, using such a study as a reference
within specific bodies of knowledge may introduce imprecision, both in terms of
its subareas and with respect to new methodologies, languages, and tools. Thus,
a review of tertiary studies (what could be understood as a quaternary study)
could contribute to show the representativeness of the reported findings in
comparison to the state of the art and also to compile a set of perceptions
that could not be previously achieved. In that direction, the main contribution
of this paper is presenting the findings from an analysis of 34 software
engineering tertiary studies published between 2009 and 2021. The results
indicate that over 60% of the studies demonstrate varying degrees of
anachronism due to the publication of primary and secondary studies following
the publication of the tertiary study or even due to a time elapse between its
conduction and its publication.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00212" title="Abstract">arXiv:2311.00212</a> [<a href="/pdf/2311.00212" title="Download PDF">pdf</a>, <a href="/format/2311.00212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework to Enforce, Discover, and Promote Symmetry in  Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Otto%2C+S+E">Samuel E. Otto</a>, 
<a href="/search/cs?searchtype=author&query=Zolman%2C+N">Nicholas Zolman</a>, 
<a href="/search/cs?searchtype=author&query=Kutz%2C+J+N">J. Nathan Kutz</a>, 
<a href="/search/cs?searchtype=author&query=Brunton%2C+S+L">Steven L. Brunton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Differential Geometry (math.DG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Symmetry is present throughout nature and continues to play an increasingly
central role in physics and machine learning. Fundamental symmetries, such as
Poincar\'{e} invariance, allow physical laws discovered in laboratories on
Earth to be extrapolated to the farthest reaches of the universe. Symmetry is
essential to achieving this extrapolatory power in machine learning
applications. For example, translation invariance in image classification
allows models with fewer parameters, such as convolutional neural networks, to
be trained on smaller data sets and achieve state-of-the-art performance. In
this paper, we provide a unifying theoretical and methodological framework for
incorporating symmetry into machine learning models in three ways: 1. enforcing
known symmetry when training a model; 2. discovering unknown symmetries of a
given model or data set; and 3. promoting symmetry during training by learning
a model that breaks symmetries within a user-specified group of candidates when
there is sufficient evidence in the data. We show that these tasks can be cast
within a common mathematical framework whose central object is the Lie
derivative associated with fiber-linear Lie group actions on vector bundles. We
extend and unify several existing results by showing that enforcing and
discovering symmetry are linear-algebraic tasks that are dual with respect to
the bilinear structure of the Lie derivative. We also propose a novel way to
promote symmetry by introducing a class of convex regularization functions
based on the Lie derivative and nuclear norm relaxation to penalize symmetry
breaking during training of machine learning models. We explain how these ideas
can be applied to a wide range of machine learning models including basis
function regression, dynamical systems discovery, multilayer perceptrons, and
neural networks acting on spatial fields such as images.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00213" title="Abstract">arXiv:2311.00213</a> [<a href="/pdf/2311.00213" title="Download PDF">pdf</a>, <a href="/format/2311.00213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistent Video-to-Video Transfer Using Synthetic Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jiaxin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tianjun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a novel and efficient approach for text-based video-to-video
editing that eliminates the need for resource-intensive per-video-per-model
finetuning. At the core of our approach is a synthetic paired video dataset
tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's
image transfer via editing instruction, we adapt this paradigm to the video
domain. Extending the Prompt-to-Prompt to videos, we efficiently generate
paired samples, each with an input video and its edited counterpart. Alongside
this, we introduce the Long Video Sampling Correction during sampling, ensuring
consistent long videos across batches. Our method surpasses current methods
like Tune-A-Video, heralding substantial progress in text-based video-to-video
editing and suggesting exciting avenues for further exploration and deployment.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00214" title="Abstract">arXiv:2311.00214</a> [<a href="/pdf/2311.00214" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WinNet:time series forecasting with a window-enhanced period extracting  and interacting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ou%2C+W">Wenjie Ou</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dongyue Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhishuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yi Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, Transformer-based methods have significantly improved
state-of-the-art time series forecasting results, but they suffer from high
computational costs and the inability to capture the long and short periodicity
of time series. We present a highly accurate and simply structured CNN-based
model for long-term time series forecasting tasks, called WinNet, including (i)
Inter-Intra Period Encoder (I2PE) to transform 1D sequence into 2D tensor with
long and short periodicity according to the predefined periodic window, (ii)
Two-Dimensional Period Decomposition (TDPD) to model period-trend and
oscillation terms, and (iii) Decomposition Correlation Block (DCB) to leverage
the correlations of the period-trend and oscillation terms to support the
prediction tasks by CNNs. Results on nine benchmark datasets show that the
WinNet can achieve SOTA performance and lower computational complexity over
CNN-, MLP-, Transformer-based approaches. The WinNet provides potential for the
CNN-based methods in the time series forecasting tasks, with perfect tradeoff
between performance and efficiency.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00217" title="Abstract">arXiv:2311.00217</a> [<a href="/pdf/2311.00217" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Capture Public Opinion about Global Warming?  An Empirical Assessment of Algorithmic Fidelity and Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">S. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+T+Q">T. Q. Peng</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+M+H">M. H. Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Rosenthal%2C+S+A">S. A. Rosenthal</a>, 
<a href="/search/cs?searchtype=author&query=Kotcher%2C+J+E">J. E. Kotcher</a>, 
<a href="/search/cs?searchtype=author&query=Maibach%2C+E+W">E. W. Maibach</a>, 
<a href="/search/cs?searchtype=author&query=Leiserowitz%2C+A">A. Leiserowitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated their potential in social
science research by emulating human perceptions and behaviors, a concept
referred to as algorithmic fidelity. This study assesses the algorithmic
fidelity and bias of LLMs by utilizing two nationally representative climate
change surveys. The LLMs were conditioned on demographics and/or psychological
covariates to simulate survey responses. The findings indicate that LLMs can
effectively capture presidential voting behaviors but encounter challenges in
accurately representing global warming perspectives when relevant covariates
are not included. GPT-4 exhibits improved performance when conditioned on both
demographics and covariates. However, disparities emerge in LLM estimations of
the views of certain groups, with LLMs tending to underestimate worry about
global warming among Black Americans. While highlighting the potential of LLMs
to aid social science research, these results underscore the importance of
meticulous conditioning, model selection, survey question format, and bias
assessment when employing LLMs for survey simulation. Further investigation
into prompt engineering and algorithm auditing is essential to harness the
power of LLMs while addressing their inherent limitations.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00223" title="Abstract">arXiv:2311.00223</a> [<a href="/pdf/2311.00223" title="Download PDF">pdf</a>, <a href="/format/2311.00223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is GPT Powerful Enough to Analyze the Emotions of Memes?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingjing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Joshua Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Grace Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+A">Allen Hong</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Feng Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Large Language Models (LLMs), representing a significant achievement in
artificial intelligence (AI) research, have demonstrated their ability in a
multitude of tasks. This project aims to explore the capabilities of GPT-3.5, a
leading example of LLMs, in processing the sentiment analysis of Internet
memes. Memes, which include both verbal and visual aspects, act as a powerful
yet complex tool for expressing ideas and sentiments, demanding an
understanding of societal norms and cultural contexts. Notably, the detection
and moderation of hateful memes pose a significant challenge due to their
implicit offensive nature. This project investigates GPT's proficiency in such
subjective tasks, revealing its strengths and potential limitations. The tasks
include the classification of meme sentiment, determination of humor type, and
detection of implicit hate in memes. The performance evaluation, using datasets
from SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative
understanding of GPT responses against human annotations. Despite GPT's
remarkable progress, our findings underscore the challenges faced by these
models in handling subjective tasks, which are rooted in their inherent
limitations including contextual understanding, interpretation of implicit
meanings, and data biases. This research contributes to the broader discourse
on the applicability of AI in handling complex, context-dependent tasks, and
offers valuable insights for future advancements.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00224" title="Abstract">arXiv:2311.00224</a> [<a href="/pdf/2311.00224" title="Download PDF">pdf</a>, <a href="/format/2311.00224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain decomposition-based coupling of physics-informed neural networks  via the Schwarz alternating method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Snyder%2C+W">Will Snyder</a>, 
<a href="/search/math?searchtype=author&query=Tezaur%2C+I">Irina Tezaur</a>, 
<a href="/search/math?searchtype=author&query=Wentland%2C+C">Christopher Wentland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI); Mathematical Physics (math-ph)

</div>
<p class="mathjax">Physics-informed neural networks (PINNs) are appealing data-driven tools for
solving and inferring solutions to nonlinear partial differential equations
(PDEs). Unlike traditional neural networks (NNs), which train only on solution
data, a PINN incorporates a PDE's residual into its loss function and trains to
minimize the said residual at a set of collocation points in the solution
domain. This paper explores the use of the Schwarz alternating method as a
means to couple PINNs with each other and with conventional numerical models
(i.e., full order models, or FOMs, obtained via the finite element, finite
difference or finite volume methods) following a decomposition of the physical
domain. It is well-known that training a PINN can be difficult when the PDE
solution has steep gradients. We investigate herein the use of domain
decomposition and the Schwarz alternating method as a means to accelerate the
PINN training phase. Within this context, we explore different approaches for
imposing Dirichlet boundary conditions within each subdomain PINN: weakly
through the loss and/or strongly through a solution transformation. As a
numerical example, we consider the one-dimensional steady state
advection-diffusion equation in the advection-dominated (high Peclet) regime.
Our results suggest that the convergence of the Schwarz method is strongly
linked to the choice of boundary condition implementation within the PINNs
being coupled. Surprisingly, strong enforcement of the Schwarz boundary
conditions does not always lead to a faster convergence of the method. While it
is not clear from our preliminary study that the PINN-PINN coupling via the
Schwarz alternating method accelerates PINN convergence in the
advection-dominated regime, it reveals that PINN training can be improved
substantially for Peclet numbers as high as 1e6 by performing a PINN-FOM
coupling.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00227" title="Abstract">arXiv:2311.00227</a> [<a href="/pdf/2311.00227" title="Download PDF">pdf</a>, <a href="/format/2311.00227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StableFDG: Style and Attention Based Learning for Federated Domain  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jungwuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jinho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jaekyun Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023, 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional federated learning (FL) algorithms operate under the assumption
that the data distributions at training (source domains) and testing (target
domain) are the same. The fact that domain shifts often occur in practice
necessitates equipping FL methods with a domain generalization (DG) capability.
However, existing DG algorithms face fundamental challenges in FL setups due to
the lack of samples/domains in each client's local dataset. In this paper, we
propose StableFDG, a style and attention based learning strategy for
accomplishing federated domain generalization, introducing two key
contributions. The first is style-based learning, which enables each client to
explore novel styles beyond the original source domains in its local dataset,
improving domain diversity based on the proposed style sharing, shifting, and
exploration strategies. Our second contribution is an attention-based feature
highlighter, which captures the similarities between the features of data
samples in the same class, and emphasizes the important/common characteristics
to better learn the domain-invariant characteristics of each class in data-poor
FL scenarios. Experimental results show that StableFDG outperforms existing
baselines on various DG benchmark datasets, demonstrating its efficacy.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00230" title="Abstract">arXiv:2311.00230</a> [<a href="/pdf/2311.00230" title="Download PDF">pdf</a>, <a href="/format/2311.00230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision  Model and Feature Mixing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gaoshuang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaofei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenglong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Luying Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+W">Wenjian Gan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+M">Mingbo Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Utilizing visual place recognition (VPR) technology to ascertain the
geographical location of publicly available images is a pressing issue for
real-world VPR applications. Although most current VPR methods achieve
favorable results under ideal conditions, their performance in complex
environments, characterized by lighting variations, seasonal changes, and
occlusions caused by moving objects, is generally unsatisfactory. In this
study, we utilize the DINOv2 model as the backbone network for trimming and
fine-tuning to extract robust image features. We propose a novel VPR
architecture called DINO-Mix, which combines a foundational vision model with
feature aggregation. This architecture relies on the powerful image feature
extraction capabilities of foundational vision models. We employ an
MLP-Mixer-based mix module to aggregate image features, resulting in globally
robust and generalizable descriptors that enable high-precision VPR. We
experimentally demonstrate that the proposed DINO-Mix architecture
significantly outperforms current state-of-the-art (SOTA) methods. In test sets
having lighting variations, seasonal changes, and occlusions (Tokyo24/7,
Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1
accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA
methods, our architecture exhibited an average accuracy improvement of 5.14%.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00231" title="Abstract">arXiv:2311.00231</a> [<a href="/pdf/2311.00231" title="Download PDF">pdf</a>, <a href="/format/2311.00231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DistDNAS: Search Efficient Feature Interactions within 2 Hours
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tunhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wei Wen</a>, 
<a href="/search/cs?searchtype=author&query=Fedorov%2C+I">Igor Fedorov</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Buyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+F">Fangqiu Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen-Yen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yiping Han</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+F">Feng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hai Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiran Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Search efficiency and serving efficiency are two major axes in building
feature interactions and expediting the model development process in
recommender systems. On large-scale benchmarks, searching for the optimal
feature interaction design requires extensive cost due to the sequential
workflow on the large volume of data. In addition, fusing interactions of
various sources, orders, and mathematical operations introduces potential
conflicts and additional redundancy toward recommender models, leading to
sub-optimal trade-offs in performance and serving cost. In this paper, we
present DistDNAS as a neat solution to brew swift and efficient feature
interaction design. DistDNAS proposes a supernet to incorporate interaction
modules of varying orders and types as a search space. To optimize search
efficiency, DistDNAS distributes the search and aggregates the choice of
optimal interaction modules on varying data dates, achieving over 25x speed-up
and reducing search cost from 2 days to 2 hours. To optimize serving
efficiency, DistDNAS introduces a differentiable cost-aware loss to penalize
the selection of redundant interaction modules, enhancing the efficiency of
discovered feature interactions in serving. We extensively evaluate the best
models crafted by DistDNAS on a 1TB Criteo Terabyte dataset. Experimental
evaluations demonstrate 0.001 AUC improvement and 60% FLOPs saving over current
state-of-the-art CTR models.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00232" title="Abstract">arXiv:2311.00232</a> [<a href="/pdf/2311.00232" title="Download PDF">pdf</a>, <a href="/format/2311.00232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanically-Inflatable Bio-Inspired Locomotion for Robotic Pipeline  Inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atalla%2C+M+A">Mostafa A. Atalla</a>, 
<a href="/search/cs?searchtype=author&query=van+Gelder%2C+S+P">Sebastiaan P. van Gelder</a>, 
<a href="/search/cs?searchtype=author&query=Trauzettel%2C+F">Fabian Trauzettel</a>, 
<a href="/search/cs?searchtype=author&query=Breedveld%2C+P">Paul Breedveld</a>, 
<a href="/search/cs?searchtype=author&query=Wiertlewski%2C+M">Micha&#xeb;l Wiertlewski</a>, 
<a href="/search/cs?searchtype=author&query=Sakes%2C+A">Aim&#xe9;e Sakes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Pipelines, vital for fluid transport, pose an important yet challenging
inspection task, particularly in small, flexible biological systems, that
robots have yet to master. In this study, we explored the development of an
innovative robot inspired by the ovipositor of parasitic wasps to navigate and
inspect pipelines. The robot features a flexible locomotion system that adapts
to different tube sizes and shapes through a mechanical inflation technique.
The flexible locomotion system employs a reciprocating motion, in which groups
of three sliders extend and retract in a cyclic fashion. In a
proof-of-principle experiment, the robot locomotion efficiency demonstrated
positive linear correlation (r=0.6434) with the diameter ratio (ratio of robot
diameter to tube diameter). The robot showcased a remarkable ability to
traverse tubes of different sizes, shapes and payloads with an average of (70%)
locomotion efficiency across all testing conditions, at varying diameter ratios
(0.7-1.5). Furthermore, the mechanical inflation mechanism displayed
substantial load-carrying capacity, producing considerable holding force of (13
N), equivalent to carrying a payload of approximately (5.8 Kg) inclusive the
robot weight. This novel soft robotic system shows promise for inspection and
navigation within tubular confined spaces, particularly in scenarios requiring
adaptability to different tube shapes, sizes, and load-carrying capacities.
This novel design serves as a foundation for a new class of pipeline inspection
robots that exhibit versatility across various pipeline environments,
potentially including biological systems.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00233" title="Abstract">arXiv:2311.00233</a> [<a href="/pdf/2311.00233" title="Download PDF">pdf</a>, <a href="/format/2311.00233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distort, Distract, Decode: Instruction-Tuned Model Can Refine its  Response from Noisy Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonkee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gihun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While instruction-tuned language models have demonstrated impressive
zero-shot generalization, these models often struggle to generate accurate
responses when faced with instructions that fall outside their training set.
This paper presents Instructive Decoding (ID), a simple yet effective approach
that augments the efficacy of instruction-tuned models. Specifically, ID
adjusts the logits for next-token prediction in a contrastive manner, utilizing
predictions generated from a manipulated version of the original instruction,
referred to as a noisy instruction. This noisy instruction aims to elicit
responses that could diverge from the intended instruction yet remain
plausible. We conduct experiments across a spectrum of such noisy instructions,
ranging from those that insert semantic noise via random words to others like
'opposite' that elicit the deviated responses. Our approach achieves
considerable performance gains across various instruction-tuned models and
tasks without necessitating any additional parameter updates. Notably,
utilizing 'opposite' as the noisy instruction in ID, which exhibits the maximum
divergence from the original instruction, consistently produces the most
significant performance gains across multiple models and tasks.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00236" title="Abstract">arXiv:2311.00236</a> [<a href="/pdf/2311.00236" title="Download PDF">pdf</a>, <a href="/format/2311.00236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Objectives and Key Results in Software Teams: Challenges, Opportunities  and Impact on Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Butler%2C+J">Jenna Butler</a>, 
<a href="/search/cs?searchtype=author&query=Zimmermann%2C+T">Thomas Zimmermann</a>, 
<a href="/search/cs?searchtype=author&query=Bird%2C+C">Christian Bird</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Building software, like building almost anything, requires people to
understand a common goal and work together towards it. In large software
companies, a VP or Director will have an idea or goal and it is often the job
of middle management to distill that lofty, general idea into manageable,
finite units of work. How do organizations do this hard work of setting and
measuring progress towards goals? To understand this question, we undertook a
mixed methods approach to studying goal setting, management dissemination of
goals, goal tracking and ultimately software delivery at a large multi-national
software company.
<br />Semi-structured interviews with 47 participants were analyzed and used to
develop a survey which was deployed to a multi-national team of over 4,000
engineers. The 512 responses were analyzed using thematic analysis, linear
regressions and hypothesis testing, and found that tracking, measuring and
setting goals is hard work, regardless of tools used. Middle management seems
to be a critical component of the translation of lofty goals to actionable work
items. In addition, attitudes and beliefs of engineers are critical to the
success of any goal setting framework. Based on this research, we make
recommendations on how to improve the goal setting and OKR process in software
organizations: invest in the data pipeline, increase transparency, improve
communication, promote learning communities, and a structured roll out of OKRs.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00237" title="Abstract">arXiv:2311.00237</a> [<a href="/pdf/2311.00237" title="Download PDF">pdf</a>, <a href="/format/2311.00237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mystery and Fascination of LLMs: A Comprehensive Survey on the  Interpretation and Analysis of Emergent Abilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuxiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yanzheng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hanqi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding emergent abilities, such as in-context learning (ICL) and
chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost
importance. This importance stems not only from the better utilization of these
capabilities across various tasks, but also from the proactive identification
and mitigation of potential risks, including concerns of truthfulness, bias,
and toxicity, that may arise alongside these capabilities. In this paper, we
present a thorough survey on the interpretation and analysis of emergent
abilities of LLMs. First, we provide a concise introduction to the background
and definition of emergent abilities. Then, we give an overview of advancements
from two perspectives: 1) a macro perspective, emphasizing studies on the
mechanistic interpretability and delving into the mathematical foundations
behind emergent abilities; and 2) a micro-perspective, concerning studies that
focus on empirical interpretability by examining factors associated with these
abilities. We conclude by highlighting the challenges encountered and
suggesting potential avenues for future research. We believe that our work
establishes the basis for further exploration into the interpretation of
emergent abilities.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00240" title="Abstract">arXiv:2311.00240</a> [<a href="/pdf/2311.00240" title="Download PDF">pdf</a>, <a href="/format/2311.00240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intell-dragonfly: A Cybersecurity Attack Surface Generation Engine Based  On Artificial Intelligence-generated Content Technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xingchen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Q">Qin Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the rapid development of the Internet, cyber security issues have become
increasingly prominent. Traditional cyber security defense methods are limited
in the face of ever-changing threats, so it is critical to seek innovative
attack surface generation methods. This study proposes Intell-dragonfly, a
cyber security attack surface generation engine based on artificial
intelligence generation technology, to meet the challenges of cyber security.
Based on ChatGPT technology, this paper designs an automated attack surface
generation process, which can generate diversified and personalized attack
scenarios, targets, elements and schemes. Through experiments in a real network
environment, the effect of the engine is verified and compared with traditional
methods, which improves the authenticity and applicability of the attack
surface. The experimental results show that the ChatGPT-based method has
significant advantages in the accuracy, diversity and operability of attack
surface generation. Furthermore, we explore the strengths and limitations of
the engine and discuss its potential applications in the field of cyber
security. This research provides a novel approach to the field of cyber
security that is expected to have a positive impact on defense and prevention
of cyberthreats.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00241" title="Abstract">arXiv:2311.00241</a> [<a href="/pdf/2311.00241" title="Download PDF">pdf</a>, <a href="/format/2311.00241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 1DFormer: Learning 1D Landmark Representations via Transformer for  Facial Landmark Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Shi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huan%2C+S">Shijie Huan</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shangfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinshui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Bing Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baocai Yin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Cong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, heatmap regression methods based on 1D landmark representations
have shown prominent performance on locating facial landmarks. However,
previous methods ignored to make deep explorations on the good potentials of 1D
landmark representations for sequential and structural modeling of multiple
landmarks to track facial landmarks. To address this limitation, we propose a
Transformer architecture, namely 1DFormer, which learns informative 1D landmark
representations by capturing the dynamic and the geometric patterns of
landmarks via token communications in both temporal and spatial dimensions for
facial landmark tracking. For temporal modeling, we propose a recurrent token
mixing mechanism, an axis-landmark-positional embedding mechanism, as well as a
confidence-enhanced multi-head attention mechanism to adaptively and robustly
embed long-term landmark dynamics into their 1D representations; for structure
modeling, we design intra-group and inter-group structure modeling mechanisms
to encode the component-level as well as global-level facial structure patterns
as a refinement for the 1D representations of landmarks through token
communications in the spatial dimension via 1D convolutional layers.
Experimental results on the 300VW and the TF databases show that 1DFormer
successfully models the long-range sequential patterns as well as the inherent
facial structures to learn informative 1D representations of landmark
sequences, and achieves state-of-the-art performance on facial landmark
tracking.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00246" title="Abstract">arXiv:2311.00246</a> [<a href="/pdf/2311.00246" title="Download PDF">pdf</a>, <a href="/ps/2311.00246" title="Download PostScript">ps</a>, <a href="/format/2311.00246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAUNE-Net: A Residual and Attention-Driven Underwater Image Enhancement  Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wangzhen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chenghao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Runze Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jingchao Cao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yutao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Underwater image enhancement (UIE) poses challenges due to distinctive
properties of the underwater environment, including low contrast, high
turbidity, visual blurriness, and color distortion. In recent years, the
application of deep learning has quietly revolutionized various areas of
scientific research, including UIE. However, existing deep learning-based UIE
methods generally suffer from issues of weak robustness and limited
adaptability. In this paper, inspired by residual and attention mechanisms, we
propose a more reliable and reasonable UIE network called RAUNE-Net by
employing residual learning of high-level features at the network's bottle-neck
and two aspects of attention manipulations in the down-sampling procedure.
Furthermore, we collect and create two datasets specifically designed for
evaluating UIE methods, which contains different types of underwater
distortions and degradations. The experimental validation demonstrates that our
method obtains promising objective performance and consistent visual results
across various real-world underwater images compared to other eight UIE
methods. Our example code and datasets are publicly available at
https://github.com/fansuregrin/RAUNE-Net.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00252" title="Abstract">arXiv:2311.00252</a> [<a href="/pdf/2311.00252" title="Download PDF">pdf</a>, <a href="/format/2311.00252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Neural Topological Mapping for Multi-Agent Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingchen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Haibing Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huazhong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This paper investigates the multi-agent cooperative exploration problem,
which requires multiple agents to explore an unseen environment via sensory
signals in a limited time. A popular approach to exploration tasks is to
combine active mapping with planning. Metric maps capture the details of the
spatial representation, but are with high communication traffic and may vary
significantly between scenarios, resulting in inferior generalization.
Topological maps are a promising alternative as they consist only of nodes and
edges with abstract but essential information and are less influenced by the
scene structures. However, most existing topology-based exploration tasks
utilize classical methods for planning, which are time-consuming and
sub-optimal due to their handcrafted design. Deep reinforcement learning (DRL)
has shown great potential for learning (near) optimal policies through fast
end-to-end inference. In this paper, we propose Multi-Agent Neural Topological
Mapping (MANTM) to improve exploration efficiency and generalization for
multi-agent exploration tasks. MANTM mainly comprises a Topological Mapper and
a novel RL-based Hierarchical Topological Planner (HTP). The Topological Mapper
employs a visual encoder and distance-based heuristics to construct a graph
containing main nodes and their corresponding ghost nodes. The HTP leverages
graph neural networks to capture correlations between agents and graph nodes in
a coarse-to-fine manner for effective global goal selection. Extensive
experiments conducted in a physically-realistic simulator, Habitat, demonstrate
that MANTM reduces the steps by at least 26.40% over planning-based baselines
and by at least 7.63% over RL-based competitors in unseen scenarios.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00253" title="Abstract">arXiv:2311.00253</a> [<a href="/pdf/2311.00253" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational multiphase micro-periporomechanics for dynamic shear  banding and fracturing of unsaturated porous media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hossein">Hossein</a>, 
<a href="/search/math?searchtype=author&query=Pashazad">Pashazad</a>, 
<a href="/search/math?searchtype=author&query=Xiaoyu">Xiaoyu</a>, 
<a href="/search/math?searchtype=author&query=Song">Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Dynamic shearing banding and fracturing in unsaturated porous media is a
significant problem in engineering and science. This article proposes a
multiphase micro-periporomechanics (uPPM) paradigm for modeling dynamic shear
banding and fracturing in unsaturated porous media. Periporomechanics (PPM) is
a nonlocal reformulation of classical poromechanics to model continuous and
discontinuous deformation/fracture and fluid flow in porous media through a
single framework. In PPM, a multiphase porous material is postulated as a
collection of a finite number of mixed material points. The length scale in PPM
that dictates the nonlocal interaction between material points is a
mathematical object that lacks a direct physical meaning. As a novelty, in the
coupled uPPM, a microstructure-based material length scale is incorporated by
considering micro-rotations of the solid skeleton following the Cosserat
continuum theory for solids. As a new contribution, we reformulate the
second-order work for detecting material instability and the energy-based crack
criterion and J-integral for modeling fracturing in the uPPM paradigm. The
stabilized Cosserat PPM correspondence principle that mitigates the multiphase
zero-energy mode instability is augmented to include unsaturated fluid flow. We
have numerically implemented the novel uPPM paradigm through a dual-way
fractional-step algorithm in time and a hybrid Lagrangian-Eulerian meshfree
method in space. Numerical examples are presented to demonstrate the robustness
and efficacy of the proposed uPPM paradigm for modeling shear banding and
fracturing in unsaturated porous media.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00256" title="Abstract">arXiv:2311.00256</a> [<a href="/pdf/2311.00256" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How is Software Reuse Discussed in Stack Overflow?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=AlOmara%2C+E+A">Eman Abdullah AlOmara</a>, 
<a href="/search/cs?searchtype=author&query=Peruma%2C+A">Anthony Peruma</a>, 
<a href="/search/cs?searchtype=author&query=Mkaouer%2C+M+W">Mohamed Wiem Mkaouer</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+C">Christian Newman</a>, 
<a href="/search/cs?searchtype=author&query=Ouni%2C+A">Ali Ouni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 Conference on Systems Engineering Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software reuse is a crucial external quality attribute targeted by
open-source and commercial projects. Despite that software reuse has
experienced an increased adoption throughout the years, little is known about
what aspects of code reuse developers discuss. In this paper, we present an
empirical study of 1,409 posts to better understand the challenges developers
face when reusing code. Our findings show that 'visual studio' is the top
occurring bigrams for question posts, and there are frequent design patterns
utilized by developers for the purpose of reuse. We envision our findings
enabling researchers to develop guidelines to be utilized to foster software
reuse.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00257" title="Abstract">arXiv:2311.00257</a> [<a href="/pdf/2311.00257" title="Download PDF">pdf</a>, <a href="/format/2311.00257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AMSP: Super-Scaling LLM Training via Advanced Model States Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qiaoling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qinghao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhisheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoteng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yonggang Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated impressive performance across
various downstream tasks. When training these models, there is a growing
inclination to process more tokens on larger training scales but with
relatively smaller model sizes. Zero Redundancy Optimizer (ZeRO), although
effective in conventional training environments, grapples with scaling
challenges when confronted with this emerging paradigm. To this end, we propose
a novel LLM training framework AMSP, which undertakes a granular partitioning
of model states, encompassing parameters ($P$), gradient ($G$), and optimizer
states ($OS$). Specifically, AMSP(1) builds a unified partitioning space,
enabling independent partitioning strategies for $P$, $G$, and $OS$; (2)
incorporates a scale-aware partitioner to autonomously search for optimal
partitioning strategies: (3) designs a dedicated communication optimizer to
ensure proficient management of data placement discrepancies arising from
diverse partitioning strategies. Our evaluations show that AMSP achieves up to
90.3% scaling efficiency across 1024 GPUs.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00258" title="Abstract">arXiv:2311.00258</a> [<a href="/pdf/2311.00258" title="Download PDF">pdf</a>, <a href="/format/2311.00258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noisy Exemplars Make Large Language Models More Robust: A  Domain-Agnostic Behavioral Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hongyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Saparov%2C+A">Abulhair Saparov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in prompt engineering enable large language models (LLMs) to
solve multi-hop logical reasoning problems with impressive accuracy. However,
there is little existing work investigating the robustness of LLMs with
few-shot prompting techniques. Therefore, we introduce a systematic approach to
test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic
perturbations. We include perturbations at multiple levels of abstractions
(e.g. lexical perturbations such as typos, and semantic perturbations such as
the inclusion of intermediate reasoning steps in the questions) to conduct
behavioral analysis on the LLMs. Throughout our experiments, we find that
models are more sensitive to certain perturbations such as replacing words with
their synonyms. We also demonstrate that increasing the proportion of perturbed
exemplars in the prompts improves the robustness of few-shot prompting methods.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00259" title="Abstract">arXiv:2311.00259</a> [<a href="/pdf/2311.00259" title="Download PDF">pdf</a>, <a href="/format/2311.00259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solutions to Elliptic and Parabolic Problems via Finite Difference Based  Unsupervised Small Linear Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Celaya%2C+A">Adrian Celaya</a>, 
<a href="/search/cs?searchtype=author&query=Kirk%2C+K">Keegan Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Fuentes%2C+D">David Fuentes</a>, 
<a href="/search/cs?searchtype=author&query=Riviere%2C+B">Beatrice Riviere</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to CMA, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Numerical Analysis (math.NA)

</div>
<p class="mathjax">In recent years, there has been a growing interest in leveraging deep
learning and neural networks to address scientific problems, particularly in
solving partial differential equations (PDEs). However, current neural
network-based PDE solvers often rely on extensive training data or labeled
input-output pairs, making them prone to challenges in generalizing to
out-of-distribution examples. To mitigate the generalization gap encountered by
conventional neural network-based methods in estimating PDE solutions, we
formulate a fully unsupervised approach, requiring no training data, to
estimate finite difference solutions for PDEs directly via small convolutional
neural networks. Our proposed algorithms demonstrate a comparable accuracy to
the true solution for several selected elliptic and parabolic problems compared
to the finite difference method.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00260" title="Abstract">arXiv:2311.00260</a> [<a href="/pdf/2311.00260" title="Download PDF">pdf</a>, <a href="/ps/2311.00260" title="Download PostScript">ps</a>, <a href="/format/2311.00260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentivized Collaboration in Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+L">Lee Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Han Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In collaborative active learning, where multiple agents try to learn labels
from a common hypothesis, we introduce an innovative framework for incentivized
collaboration. Here, rational agents aim to obtain labels for their data sets
while keeping label complexity at a minimum. We focus on designing (strict)
individually rational (IR) collaboration protocols, ensuring that agents cannot
reduce their expected label complexity by acting individually. We first show
that given any optimal active learning algorithm, the collaboration protocol
that runs the algorithm as is over the entire data is already IR. However,
computing the optimal algorithm is NP-hard. We therefore provide collaboration
protocols that achieve (strict) IR and are comparable with the best known
tractable approximation algorithm in terms of label complexity.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00262" title="Abstract">arXiv:2311.00262</a> [<a href="/pdf/2311.00262" title="Download PDF">pdf</a>, <a href="/format/2311.00262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug-and-Play Policy Planner for Large Language Model Powered Dialogue  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Proactive dialogues serve as a practical yet challenging dialogue problem in
the era of large language models (LLMs), where the dialogue policy planning is
the key to improving the proactivity of LLMs. Most existing studies enable the
dialogue policy planning of LLMs using various prompting schemes or iteratively
enhance this capability in handling the given case with verbal AI feedback.
However, these approaches are either bounded by the policy planning capability
of the frozen LLMs or hard to be transferred to new cases. In this work, we
introduce a new dialogue policy planning paradigm to strategize LLMs for
proactive dialogue problems with a tunable language model plug-in as a
plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a
novel training framework to facilitate supervised fine-tuning over available
human-annotated data as well as reinforcement learning from goal-oriented AI
feedback with dynamic interaction data collected by the LLM-based self-play
simulation. In this manner, the LLM-powered dialogue agent can not only be
generalized to different cases after the training, but also be applicable to
different applications by just substituting the learned plug-in. In addition,
we propose to evaluate the policy planning capability of dialogue systems under
the interactive setting. Experimental results demonstrate that PPDPP
consistently and substantially outperforms existing approaches on three
different proactive dialogue applications, including negotiation, emotional
support, and tutoring dialogues.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00263" title="Abstract">arXiv:2311.00263</a> [<a href="/pdf/2311.00263" title="Download PDF">pdf</a>, <a href="/format/2311.00263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The bottleneck and ceiling effects in quantized tracking control of  heterogeneous multi-agent systems under DoS attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Feng%2C+S">Shuai Feng</a>, 
<a href="/search/eess?searchtype=author&query=Ran%2C+M">Maopeng Ran</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+B">Baoyong Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+L">Lihua Xie</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+S">Shengyuan Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we investigate tracking control of heterogeneous multi-agent
systems under Denial-of-Service (DoS) attacks and state quantization. Dynamic
quantized mechanisms are designed for inter-follower communication and
leader-follower communication. Zooming-in and out factors, and data rates of
both mechanisms for preventing quantizer saturation are provided. Our results
show that by tuning the inter-follower quantized controller, one cannot improve
the resilience beyond a level determined by the data rate of leader-follower
quantized communication, i.e., the ceiling effect. Otherwise, overflow of
followers' state quantizer can occur. On the other hand, if one selects a
"large" data rate for leader-follower quantized communication, then the
inter-follower quantized communication determines the resilience, and further
increasing the data rate for leader-follower quantized communication cannot
improve the resilience, i.e., the bottleneck effect. Simulation examples are
provided to justify the results of our paper.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00267" title="Abstract">arXiv:2311.00267</a> [<a href="/pdf/2311.00267" title="Download PDF">pdf</a>, <a href="/format/2311.00267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Decision Transformer via Hierarchical Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chenjun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hebin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Decision Transformer (DT) is an innovative algorithm leveraging recent
advances of the transformer architecture in reinforcement learning (RL).
However, a notable limitation of DT is its reliance on recalling trajectories
from datasets, losing the capability to seamlessly stitch sub-optimal
trajectories together. In this work we introduce a general sequence modeling
framework for studying sequential decision making through the lens of
Hierarchical RL. At the time of making decisions, a high-level policy first
proposes an ideal prompt for the current state, a low-level policy subsequently
generates an action conditioned on the given prompt. We show DT emerges as a
special case of this framework with certain choices of high-level and low-level
policies, and discuss the potential failure of these choices. Inspired by these
observations, we study how to jointly optimize the high-level and low-level
policies to enable the stitching ability, which further leads to the
development of new offline RL algorithms. Our empirical results clearly show
that the proposed algorithms significantly surpass DT on several control and
navigation benchmarks. We hope our contributions can inspire the integration of
transformer architectures within the field of RL.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00268" title="Abstract">arXiv:2311.00268</a> [<a href="/pdf/2311.00268" title="Download PDF">pdf</a>, <a href="/format/2311.00268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Syntactic Inductive Bias in Transformer Language Models: Especially  Helpful for Low-Resource Languages?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gessler%2C+L">Luke Gessler</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+N">Nathan Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CoNLL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">A line of work on Transformer-based language models such as BERT has
attempted to use syntactic inductive bias to enhance the pretraining process,
on the theory that building syntactic structure into the training process
should reduce the amount of data needed for training. But such methods are
often tested for high-resource languages such as English. In this work, we
investigate whether these methods can compensate for data sparseness in
low-resource languages, hypothesizing that they ought to be more effective for
low-resource languages. We experiment with five low-resource languages: Uyghur,
Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic
inductive bias methods produce uneven results in low-resource settings, and
provide surprisingly little benefit in most cases.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00270" title="Abstract">arXiv:2311.00270</a> [<a href="/pdf/2311.00270" title="Download PDF">pdf</a>, <a href="/format/2311.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> smart contract, security, vulnerabilities, attacks, defenses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiyuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoxuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liehuang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 6 figures, ACM Computing Surveys
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">With the increasing adoption of smart contracts, ensuring their security has
become a critical concern. Numerous vulnerabilities and attacks have been
identified and exploited, resulting in significant financial losses. In
response, researchers have developed various tools and techniques to identify
and prevent vulnerabilities in smart contracts. In this survey, we present a
systematic overview of the quality assurance of smart contracts, covering
vulnerabilities, attacks, defenses, and tool support. By classifying
vulnerabilities based on known attacks, we can identify patterns and common
weaknesses that need to be addressed. Moreover, in order to effectively protect
smart contracts, we have created a labeled dataset to evaluate various
vulnerability detection tools and compare their effectiveness.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00271" title="Abstract">arXiv:2311.00271</a> [<a href="/pdf/2311.00271" title="Download PDF">pdf</a>, <a href="/format/2311.00271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EdgeDis: Enabling Fast, Economical, and Reliable Data Dissemination for  Mobile Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qiang He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Bouguettaya%2C+A">Athman Bouguettaya</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yun Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Mobile edge computing (MEC) enables web data caching in close geographic
proximity to end users. Popular data can be cached on edge servers located less
than hundreds of meters away from end users. This ensures bounded latency
guarantees for various latency-sensitive web applications. However,
transmitting a large volume of data out of the cloud onto many
geographically-distributed web servers individually can be expensive. In
addition, web content dissemination may be interrupted by various intentional
and accidental events in the volatile MEC environment, which undermines
dissemination efficiency and subsequently incurs extra transmission costs. To
tackle the above challenges, we present a novel scheme named EdgeDis that
coordinates data dissemination by distributed consensus among those servers. We
analyze EdgeDis's validity theoretically and evaluate its performance
experimentally. Results demonstrate that compared with baseline and
state-of-the-art schemes, EdgeDis: 1) is 5.97x - 7.52x faster; 2) reduces
dissemination costs by 48.21% to 91.87%; and 3) reduces performance loss caused
by dissemination failures by up to 97.30% in time and 96.35% in costs.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00272" title="Abstract">arXiv:2311.00272</a> [<a href="/pdf/2311.00272" title="Download PDF">pdf</a>, <a href="/format/2311.00272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatCoder: Chat-based Refine Requirement Improves LLMs&#x27; Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zejun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Ge Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models have shown good performances in generating code to meet
human requirements. However, human requirements expressed in natural languages
can be vague, incomplete, and ambiguous, leading large language models to
misunderstand human requirements and make mistakes. Worse, it is difficult for
a human user to refine the requirement. To help human users refine their
requirements and improve large language models' code generation performances,
we propose ChatCoder: a method to refine the requirements via chatting with
large language models. We design a chat scheme in which the large language
models will guide the human users to refine their expression of requirements to
be more precise, unambiguous, and complete than before. Experiments show that
ChatCoder has improved existing large language models' performance by a large
margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs
fine-tuned via human response.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00273" title="Abstract">arXiv:2311.00273</a> [<a href="/pdf/2311.00273" title="Download PDF">pdf</a>, <a href="/format/2311.00273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoulChat: Improving LLMs&#x27; Empathy, Listening, and Comfort Abilities  through Fine-tuning with Multi-turn Empathy Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yirong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+X">Xiaofen Xing</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jingkai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Huimin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangmin Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appectped to Findings of EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have been widely applied in various fields due
to their excellent capability for memorizing knowledge and chain of thought
(CoT). When these language models are applied in the field of psychological
counseling, they often rush to provide universal advice. However, when users
seek psychological support, they need to gain empathy, trust, understanding and
comfort, rather than just reasonable advice. To this end, we constructed a
multi-turn empathetic conversation dataset of more than 2 million samples, in
which the input is the multi-turn conversation context, and the target is
empathetic responses that cover expressions such as questioning, comfort,
recognition, listening, trust, emotional support, etc. Experiments have shown
that the empathy ability of LLMs can be significantly enhanced when finetuning
by using multi-turn dialogue history and responses that are closer to the
expression of a psychological consultant.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00276" title="Abstract">arXiv:2311.00276</a> [<a href="/pdf/2311.00276" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiDAR-based SLAM for robotic mapping: state of the art and new frontiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiangdi Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+M">Miaolei He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In recent decades, the field of robotic mapping has witnessed widespread
research and development in LiDAR (Light Detection And Ranging)-based
simultaneous localization and mapping (SLAM) techniques. In this paper, we
review the state-of-the-art in LiDAR-based SLAM and explore the remaining
challenges that still require attention to satisfy the needs of contemporary
applications. A distinctive aspect of this study lies in its literature survey,
which specifically investigates the application of various types and
configurations of LiDAR, setting it apart from prior reviews. Furthermore,
several representative comparisons of LiDAR-based SLAM algorithms are
presented, which can serve as a point of reference. Finally, the paper
concludes with an insightful discussion on the emergence of new frontiers in
the domain of LiDAR-based SLAM.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00277" title="Abstract">arXiv:2311.00277</a> [<a href="/pdf/2311.00277" title="Download PDF">pdf</a>, <a href="/format/2311.00277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenForest: A data catalogue for machine learning in forest monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouaknine%2C+A">Arthur Ouaknine</a>, 
<a href="/search/cs?searchtype=author&query=Kattenborn%2C+T">Teja Kattenborn</a>, 
<a href="/search/cs?searchtype=author&query=Lalibert%C3%A9%2C+E">Etienne Lalibert&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Rolnick%2C+D">David Rolnick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 3 figures, 9 tables. Preprint under review. The OpenForest catalogue is available at $\href{<a href="https://github.com/RolnickLab/OpenForest.git">this https URL</a>}{\text{this repository}}$
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Forests play a crucial role in Earth's system processes and provide a suite
of social and economic ecosystem services, but are significantly impacted by
human activities, leading to a pronounced disruption of the equilibrium within
ecosystems. Advancing forest monitoring worldwide offers advantages in
mitigating human impacts and enhancing our comprehension of forest composition,
alongside the effects of climate change. While statistical modeling has
traditionally found applications in forest biology, recent strides in machine
learning and computer vision have reached important milestones using remote
sensing data, such as tree species identification, tree crown segmentation and
forest biomass assessments. For this, the significance of open access data
remains essential in enhancing such data-driven algorithms and methodologies.
Here, we provide a comprehensive and extensive overview of 86 open access
forest datasets across spatial scales, encompassing inventories, ground-based,
aerial-based, satellite-based recordings, and country or world maps. These
datasets are grouped in OpenForest, a dynamic catalogue open to contributions
that strives to reference all available open access forest datasets. Moreover,
in the context of these datasets, we aim to inspire research in machine
learning applied to forest biology by establishing connections between
contemporary topics, perspectives and challenges inherent in both domains. We
hope to encourage collaborations among scientists, fostering the sharing and
exploration of diverse datasets through the application of machine learning
methods for large-scale forest monitoring. OpenForest is available at this url:
https://github.com/RolnickLab/OpenForest
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00278" title="Abstract">arXiv:2311.00278</a> [<a href="/pdf/2311.00278" title="Download PDF">pdf</a>, <a href="/format/2311.00278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Scoring Using Image-Language Similarity for Few-Shot Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+M+J">Min Jae Jung</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S+D">Seung Dae Han</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joohee Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Few-shot object detection, which focuses on detecting novel objects with few
labels, is an emerging challenge in the community. Recent studies show that
adapting a pre-trained model or modified loss function can improve performance.
In this paper, we explore leveraging the power of Contrastive Language-Image
Pre-training (CLIP) and hard negative classification loss in low data setting.
Specifically, we propose Re-scoring using Image-language Similarity for
Few-shot object detection (RISF) which extends Faster R-CNN by introducing
Calibration Module using CLIP (CM-CLIP) and Background Negative Re-scale Loss
(BNRL). The former adapts CLIP, which performs zero-shot classification, to
re-score the classification scores of a detector using image-class
similarities, the latter is modified classification loss considering the
punishment for fake backgrounds as well as confusing categories on a
generalized few-shot object detection dataset. Extensive experiments on MS-COCO
and PASCAL VOC show that the proposed RISF substantially outperforms the
state-of-the-art approaches. The code will be available.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00279" title="Abstract">arXiv:2311.00279</a> [<a href="/pdf/2311.00279" title="Download PDF">pdf</a>, <a href="/format/2311.00279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Maximal Clique Enumeration via Graph Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Wen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Weiguo Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hong Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">As a fundamental task in graph data management, maximal clique enumeration
(MCE) has attracted extensive attention from both academic and industrial
communities due to its wide range of applications. However, MCE is very
challenging as the number of maximal cliques may grow exponentially with the
number of vertices. The state-of-the-art methods adopt a recursive paradigm to
enumerate maximal cliques exhaustively, suffering from a large amount of
redundant computation. In this paper, we propose a novel reduction-based
framework for MCE, namely RMCE, that aims to reduce the search space and
minimize unnecessary computations. The proposed framework RMCE incorporates
three kinds of powerful reduction techniques including global reduction,
dynamic reduction, and maximality check reduction. Global dynamic reduction
techniques effectively reduce the size of the input graph and dynamically
constructed subgraphs during the recursive subtasks, respectively. The
maximality check reduction minimizes the computation for ensuring maximality by
utilizing neighborhood dominance between visited vertices. Extensive
experiments on 18 real graphs demonstrate the effectiveness of our proposed
method. Notably, it achieves speedups up to 4.81x compared to state-of-the-art
approaches.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00280" title="Abstract">arXiv:2311.00280</a> [<a href="/pdf/2311.00280" title="Download PDF">pdf</a>, <a href="/format/2311.00280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RF-Enhanced Road Infrastructure for Intelligent Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suo%2C+D">Dajiang Suo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Heyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+R">Rahul Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zijin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shengxuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+O">Ou Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Valderas%2C+D">Daniel Valderas</a>, 
<a href="/search/cs?searchtype=author&query=Meli%C3%A0-Segu%C3%AD%2C+J">Joan Meli&#xe0;-Segu&#xed;</a>, 
<a href="/search/cs?searchtype=author&query=Abdel-Aty%2C+M">Mohamed Abdel-Aty</a>, 
<a href="/search/cs?searchtype=author&query=Sarma%2C+S+E">Sanjay E. Sarma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The EPC GEN 2 communication protocol for Ultra-high frequency Radio Frequency
Identification (RFID) has offered a promising avenue for advancing the
intelligence of transportation infrastructure. With the capability of linking
vehicles to RFID readers to crowdsource information from RFID tags on road
infrastructures, the RF-enhanced road infrastructure (REI) can potentially
transform data acquisition for urban transportation. Despite its potential, the
broader adoption of RFID technologies in building intelligent roads has been
limited by a deficiency in understanding how the GEN 2 protocol impacts system
performance under different transportation settings. This paper fills this
knowledge gap by presenting the system architecture and detailing the design
challenges associated with REI. Comprehensive real-world experiments are
conducted to assess REI's effectiveness across various urban contexts. The
results yield crucial insights into the optimal design of on-vehicle RFID
readers and on-road RFID tags, considering the constraints imposed by vehicle
dynamics, road geometries, and tag placements. With the optimized designs of
encoding schemes for reader-tag communication and on-vehicle antennas, REI is
able to fulfill the requirements of traffic sign inventory management and
environmental monitoring while falling short of catering to the demand for
high-speed navigation. In particular, the Miller 2 encoding scheme strikes the
best balance between reading performance (e.g., throughput) and noise tolerance
for the multipath effect. Additionally, we show that the on-vehicle antenna
should be oriented to maximize the available time for reading on-road tags,
although it may reduce the received power by the tags in the forward link.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00282" title="Abstract">arXiv:2311.00282</a> [<a href="/pdf/2311.00282" title="Download PDF">pdf</a>, <a href="/format/2311.00282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TLMCM Network for Medical Image Hierarchical Multi-Label Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Meng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Siyan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wenbin Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Medical Image Hierarchical Multi-Label Classification (MI-HMC) is of
paramount importance in modern healthcare, presenting two significant
challenges: data imbalance and \textit{hierarchy constraint}. Existing
solutions involve complex model architecture design or domain-specific
preprocessing, demanding considerable expertise or effort in implementation. To
address these limitations, this paper proposes Transfer Learning with Maximum
Constraint Module (TLMCM) network for the MI-HMC task. The TLMCM network offers
a novel approach to overcome the aforementioned challenges, outperforming
existing methods based on the Area Under the Average Precision and Recall
Curve($AU\overline{(PRC)}$) metric. In addition, this research proposes two
novel accuracy metrics, $EMR$ and $HammingAccuracy$, which have not been
extensively explored in the context of the MI-HMC task. Experimental results
demonstrate that the TLMCM network achieves high multi-label prediction
accuracy($80\%$-$90\%$) for MI-HMC tasks, making it a valuable contribution to
healthcare domain applications.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00284" title="Abstract">arXiv:2311.00284</a> [<a href="/pdf/2311.00284" title="Download PDF">pdf</a>, <a href="/format/2311.00284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-driven Engineering for Machine Learning Components: A Systematic  Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naveed%2C+H">Hira Naveed</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Chetan Arora</a>, 
<a href="/search/cs?searchtype=author&query=Khalajzadeh%2C+H">Hourieh Khalajzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Grundy%2C+J">John Grundy</a>, 
<a href="/search/cs?searchtype=author&query=Haggag%2C+O">Omar Haggag</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Context: Machine Learning (ML) has become widely adopted as a component in
many modern software applications. Due to the large volumes of data available,
organizations want to increasingly leverage their data to extract meaningful
insights and enhance business profitability. ML components enable predictive
capabilities, anomaly detection, recommendation, accurate image and text
processing, and informed decision-making. However, developing systems with ML
components is not trivial; it requires time, effort, knowledge, and expertise
in ML, data processing, and software engineering. There have been several
studies on the use of model-driven engineering (MDE) techniques to address
these challenges when developing traditional software and cyber-physical
systems. Recently, there has been a growing interest in applying MDE for
systems with ML components. Objective: The goal of this study is to further
explore the promising intersection of MDE with ML (MDE4ML) through a systematic
literature review (SLR). Through this SLR, we wanted to analyze existing
studies, including their motivations, MDE solutions, evaluation techniques, key
benefits and limitations. Results: We analyzed selected studies with respect to
several areas of interest and identified the following: 1) the key motivations
behind using MDE4ML; 2) a variety of MDE solutions applied, such as modeling
languages, model transformations, tool support, targeted ML aspects,
contributions and more; 3) the evaluation techniques and metrics used; and 4)
the limitations and directions for future work. We also discuss the gaps in
existing literature and provide recommendations for future research.
Conclusion: This SLR highlights current trends, gaps and future research
directions in the field of MDE4ML, benefiting both researchers and
practitioners
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00285" title="Abstract">arXiv:2311.00285</a> [<a href="/pdf/2311.00285" title="Download PDF">pdf</a>, <a href="/format/2311.00285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space  Detection Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Z">Zhenbang Du</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jiayu An</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jiahao Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dongrui Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Open Set Domain Adaptation (OSDA) aims to cope with the distribution and
label shifts between the source and target domains simultaneously, performing
accurate classification for known classes while identifying unknown class
samples in the target domain. Most existing OSDA approaches, depending on the
final image feature space of deep models, require manually-tuned thresholds,
and may easily misclassify unknown samples as known classes. Mixture-of-Expert
(MoE) could be a remedy. Within an MoE, different experts address different
input features, producing unique expert routing patterns for different classes
in a routing feature space. As a result, unknown class samples may also display
different expert routing patterns to known classes. This paper proposes
Dual-Space Detection, which exploits the inconsistencies between the image
feature space and the routing feature space to detect unknown class samples
without any threshold. Graph Router is further introduced to better make use of
the spatial information among image patches. Experiments on three different
datasets validated the effectiveness and superiority of our approach. The code
will come soon.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00286" title="Abstract">arXiv:2311.00286</a> [<a href="/pdf/2311.00286" title="Download PDF">pdf</a>, <a href="/format/2311.00286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JADE: A Linguistic-based Safety Evaluation Platform for LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xudong Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preprint work. Benchmark link: <a href="https://github.com/whitzard-ai/jade-db.">this https URL</a> Website link: <a href="https://whitzard-ai.github.io/jade.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we present \textit{JADE}, a targeted linguistic fuzzing
platform which strengthens the linguistic complexity of seed questions to
simultaneously and consistently break a wide range of widely-used LLMs
categorized in three groups: eight open-sourced Chinese, six commercial Chinese
and four commercial English LLMs. JADE generates three safety benchmarks for
the three groups of LLMs, which contain unsafe questions that are highly
threatening: the questions simultaneously trigger harmful generation of
multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$}
(please see the table below), while are still natural questions, fluent and
preserving the core unsafe semantics. We release the benchmark demos generated
for commercial English LLMs and open-sourced English LLMs in the following
link: https://github.com/whitzard-ai/jade-db. For readers who are interested in
evaluating on more questions generated by JADE, please contact us.
<br />\textit{JADE} is based on Noam Chomsky's seminal theory of
transformational-generative grammar. Given a seed question with unsafe
intention, \textit{JADE} invokes a sequence of generative and transformational
rules to increment the complexity of the syntactic structure of the original
question, until the safety guardrail is broken. Our key insight is: Due to the
complexity of human language, most of the current best LLMs can hardly
recognize the invariant evil from the infinite number of different syntactic
structures which form an unbound example space that can never be fully covered.
Technically, the generative/transformative rules are constructed by native
speakers of the languages, and, once developed, can be used to automatically
grow and transform the parse tree of a given question, until the guardrail is
broken. For more evaluation results and demo, please check our website:
https://whitzard-ai.github.io/jade.html.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00287" title="Abstract">arXiv:2311.00287</a> [<a href="/pdf/2311.00287" title="Download PDF">pdf</a>, <a href="/format/2311.00287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data  Generation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hejie Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+X">Xuan Kan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Wenqi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+J">Joyce Ho</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Carl Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Clinical natural language processing requires methods that can address
domain-specific challenges, such as complex medical terminology and clinical
contexts. Recently, large language models (LLMs) have shown promise in this
domain. Yet, their direct deployment can lead to privacy issues and are
constrained by resources. To address this challenge, we delve into synthetic
clinical text generation using LLMs for clinical NLP tasks. We propose an
innovative, resource-efficient approach, ClinGen, which infuses knowledge into
the process. Our model involves clinical knowledge extraction and
context-informed LLM prompting. Both clinical topics and writing styles are
drawn from external domain-specific knowledge graphs and LLMs to guide data
generation. Our extensive empirical study across 7 clinical NLP tasks and 16
datasets reveals that ClinGen consistently enhances performance across various
tasks, effectively aligning the distribution of real datasets and significantly
enriching the diversity of generated training instances. We will publish our
code and all the generated data in \url{https://github.com/ritaranx/ClinGen}.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00288" title="Abstract">arXiv:2311.00288</a> [<a href="/pdf/2311.00288" title="Download PDF">pdf</a>, <a href="/format/2311.00288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Instruction Tuning: Improving Cross-Task Generalization by  Training on Prompt Sensitive Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kung%2C+P">Po-Nien Kung</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Fan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Instruction tuning (IT) achieves impressive zero-shot generalization results
by training large language models (LLMs) on a massive amount of diverse tasks
with instructions. However, how to select new tasks to improve the performance
and generalizability of IT models remains an open question. Training on all
existing tasks is impractical due to prohibiting computation requirements, and
randomly selecting tasks can lead to suboptimal performance. In this work, we
propose active instruction tuning based on prompt uncertainty, a novel
framework to identify informative tasks, and then actively tune the models on
the selected tasks. We represent the informativeness of new tasks with the
disagreement of the current model outputs over perturbed prompts. Our
experiments on NIV2 and Self-Instruct datasets demonstrate that our method
consistently outperforms other baseline strategies for task selection,
achieving better out-of-distribution generalization with fewer training tasks.
Additionally, we introduce a task map that categorizes and diagnoses tasks
based on prompt uncertainty and prediction probability. We discover that
training on ambiguous (prompt-uncertain) tasks improves generalization while
training on difficult (prompt-certain and low-probability) tasks offers no
benefit, underscoring the importance of task selection for instruction tuning.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00290" title="Abstract">arXiv:2311.00290</a> [<a href="/pdf/2311.00290" title="Download PDF">pdf</a>, <a href="/format/2311.00290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference of CO2 flow patterns -- a feasibility study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gahlot%2C+A+P">Abhinav Prakash Gahlot</a>, 
<a href="/search/cs?searchtype=author&query=Erdinc%2C+H+T">Huseyin Tuna Erdinc</a>, 
<a href="/search/cs?searchtype=author&query=Orozco%2C+R">Rafael Orozco</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+F+J">Felix J. Herrmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2023 Workshop - Tackling Climate Change with Machine Learning (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Mathematical Physics (math-ph); Geophysics (physics.geo-ph)

</div>
<p class="mathjax">As the global deployment of carbon capture and sequestration (CCS) technology
intensifies in the fight against climate change, it becomes increasingly
imperative to establish robust monitoring and detection mechanisms for
potential underground CO2 leakage, particularly through pre-existing or induced
faults in the storage reservoir's seals. While techniques such as history
matching and time-lapse seismic monitoring of CO2 storage have been used
successfully in tracking the evolution of CO2 plumes in the subsurface, these
methods lack principled approaches to characterize uncertainties related to the
CO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is
essential for risk mitigation for the following reasons: (i) CO2 plume-induced
changes are small and seismic data is noisy; (ii) changes between regular and
irregular (e.g., caused by leakage) flow patterns are small; and (iii) the
reservoir properties that control the flow are strongly heterogeneous and
typically only available as distributions. To arrive at a formulation capable
of inferring flow patterns for regular and irregular flow from well and seismic
data, the performance of conditional normalizing flow will be analyzed on a
series of carefully designed numerical experiments. While the inferences
presented are preliminary in the context of an early CO2 leakage detection
system, the results do indicate that inferences with conditional normalizing
flows can produce high-fidelity estimates for CO2 plumes with or without
leakage. We are also confident that the inferred uncertainty is reasonable
because it correlates well with the observed errors. This uncertainty stems
from noise in the seismic data and from the lack of precise knowledge of the
reservoir's fluid flow properties.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00291" title="Abstract">arXiv:2311.00291</a> [<a href="/pdf/2311.00291" title="Download PDF">pdf</a>, <a href="/format/2311.00291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Representation Learning for Infrared and Visible Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lingfei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Hancock%2C+E+R">Edwin R. Hancock</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Infrared and visible image fusion aims to extract complementary features to
synthesize a single fused image. Many methods employ convolutional neural
networks (CNNs) to extract local features due to its translation invariance and
locality. However, CNNs fail to consider the image's non-local self-similarity
(NLss), though it can expand the receptive field by pooling operations, it
still inevitably leads to information loss. In addition, the transformer
structure extracts long-range dependence by considering the correlativity among
all image patches, leading to information redundancy of such transformer-based
methods. However, graph representation is more flexible than grid (CNN) or
sequence (transformer structure) representation to address irregular objects,
and graph can also construct the relationships among the spatially repeatable
details or texture with far-space distance. Therefore, to address the above
issues, it is significant to convert images into the graph space and thus adopt
graph convolutional networks (GCNs) to extract NLss. This is because the graph
can provide a fine structure to aggregate features and propagate information
across the nearest vertices without introducing redundant information.
Concretely, we implement a cascaded NLss extraction pattern to extract NLss of
intra- and inter-modal by exploring interactions of different image pixels in
intra- and inter-image positional distance. We commence by preforming GCNs on
each intra-modal to aggregate features and propagate information to extract
independent intra-modal NLss. Then, GCNs are performed on the concatenate
intra-modal NLss features of infrared and visible images, which can explore the
cross-domain NLss of inter-modal to reconstruct the fused image. Ablation
studies and extensive experiments illustrates the effectiveness and superiority
of the proposed method on three datasets.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00292" title="Abstract">arXiv:2311.00292</a> [<a href="/pdf/2311.00292" title="Download PDF">pdf</a>, <a href="/format/2311.00292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IBADR: an Iterative Bias-Aware Dataset Refinement Framework for  Debiasing NLU models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jinsong Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hua Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP2023 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As commonly-used methods for debiasing natural language understanding (NLU)
models, dataset refinement approaches heavily rely on manual data analysis, and
thus maybe unable to cover all the potential biased features. In this paper, we
propose IBADR, an Iterative Bias-Aware Dataset Refinement framework, which
debiases NLU models without predefining biased features. We maintain an
iteratively expanded sample pool. Specifically, at each iteration, we first
train a shallow model to quantify the bias degree of samples in the pool. Then,
we pair each sample with a bias indicator representing its bias degree, and use
these extended samples to train a sample generator. In this way, this generator
can effectively learn the correspondence relationship between bias indicators
and samples. Furthermore, we employ the generator to produce pseudo samples
with fewer biased features by feeding specific bias indicators. Finally, we
incorporate the generated pseudo samples into the pool. Experimental results
and in-depth analyses on two NLU tasks show that IBADR not only significantly
outperforms existing dataset refinement approaches, achieving SOTA, but also is
compatible with model-centric methods.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00296" title="Abstract">arXiv:2311.00296</a> [<a href="/pdf/2311.00296" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Representation Learning of Scientific Literature based on  Adaptive Feature and Graph Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongrui Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawen Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Meiyu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Zeli Guan</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhe Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Because most of the scientific literature data is unmarked, it makes semantic
representation learning based on unsupervised graph become crucial. At the same
time, in order to enrich the features of scientific literature, a learning
method of semantic representation of scientific literature based on adaptive
features and graph neural network is proposed. By introducing the adaptive
feature method, the features of scientific literature are considered globally
and locally. The graph attention mechanism is used to sum the features of
scientific literature with citation relationship, and give each scientific
literature different feature weights, so as to better express the correlation
between the features of different scientific literature. In addition, an
unsupervised graph neural network semantic representation learning method is
proposed. By comparing the mutual information between the positive and negative
local semantic representation of scientific literature and the global graph
semantic representation in the potential space, the graph neural network can
capture the local and global information, thus improving the learning ability
of the semantic representation of scientific literature. The experimental
results show that the proposed learning method of semantic representation of
scientific literature based on adaptive feature and graph neural network is
competitive on the basis of scientific literature classification, and has
achieved good results.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00298" title="Abstract">arXiv:2311.00298</a> [<a href="/pdf/2311.00298" title="Download PDF">pdf</a>, <a href="/format/2311.00298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of Frame Selection for Text-to-Video Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Mengxia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Min Cao</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziyin Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2023 EMNLP findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-video retrieval (TVR) aims to find the most relevant video in a large
video gallery given a query text. The intricate and abundant context of the
video challenges the performance and efficiency of TVR. To handle the
serialized video contexts, existing methods typically select a subset of frames
within a video to represent the video content for TVR. How to select the most
representative frames is a crucial issue, whereby the selected frames are
required to not only retain the semantic information of the video but also
promote retrieval efficiency by excluding temporally redundant frames. In this
paper, we make the first empirical study of frame selection for TVR. We
systemically classify existing frame selection methods into text-free and
text-guided ones, under which we detailedly analyze six different frame
selections in terms of effectiveness and efficiency. Among them, two frame
selections are first developed in this paper. According to the comprehensive
analysis on multiple TVR benchmarks, we empirically conclude that the TVR with
proper frame selections can significantly improve the retrieval efficiency
without sacrificing the retrieval performance.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00300" title="Abstract">arXiv:2311.00300</a> [<a href="/pdf/2311.00300" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entity Alignment Method of Science and Technology Patent based on Graph  Convolution Network and Information Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Runze Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawen Li</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yingxia Shao</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Zeli Guan</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zhe Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The entity alignment of science and technology patents aims to link the
equivalent entities in the knowledge graph of different science and technology
patent data sources. Most entity alignment methods only use graph neural
network to obtain the embedding of graph structure or use attribute text
description to obtain semantic representation, ignoring the process of
multi-information fusion in science and technology patents. In order to make
use of the graphic structure and auxiliary information such as the name,
description and attribute of the patent entity, this paper proposes an entity
alignment method based on the graph convolution network for science and
technology patent information fusion. Through the graph convolution network and
BERT model, the structure information and entity attribute information of the
science and technology patent knowledge graph are embedded and represented to
achieve multi-information fusion, thus improving the performance of entity
alignment. Experiments on three benchmark data sets show that the proposed
method Hit@K The evaluation indicators are better than the existing methods.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00301" title="Abstract">arXiv:2311.00301</a> [<a href="/pdf/2311.00301" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Syllable-Level Pronunciation Stress with A Self-Attention  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weiying%2C+W">Wang Weiying</a>, 
<a href="/search/cs?searchtype=author&query=Akinori%2C+N">Nakajima Akinori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> source codes available at <a href="https://github.com/wangweiying303/stress-detection-model">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">One precondition of effective oral communication is that words should be
pronounced clearly, especially for non-native speakers. Word stress is the key
to clear and correct English, and misplacement of syllable stress may lead to
misunderstandings. Thus, knowing the stress level is important for English
speakers and learners. This paper presents a self-attention model to identify
the stress level for each syllable of spoken English. Various prosodic and
categorical features, including the pitch level, intensity, duration and type
of the syllable and its nuclei (the vowel of the syllable), are explored. These
features are input to the self-attention model, and syllable-level stresses are
predicted. The simplest model yields an accuracy of over 88% and 93% on
different datasets, while more advanced models provide higher accuracy. Our
study suggests that the self-attention model can be promising in stress-level
detection. These models could be applied to various scenarios, such as online
meetings and English learning.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00304" title="Abstract">arXiv:2311.00304</a> [<a href="/pdf/2311.00304" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stacking an autoencoder for feature selection of zero-day threats
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tokmak%2C+M">Mahmut Tokmak</a>, 
<a href="/search/cs?searchtype=author&query=Nkongolo%2C+M">Mike Nkongolo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented at the International Conferences on Science and Technology Engineering Science and Technology, Budva, Montenegro
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Zero-day attack detection plays a critical role in mitigating risks,
protecting assets, and staying ahead in the evolving threat landscape. This
study explores the application of stacked autoencoder (SAE), a type of
artificial neural network, for feature selection and zero-day threat
classification using a Long Short-Term Memory (LSTM) scheme. The process
involves preprocessing the UGRansome dataset and training an unsupervised SAE
for feature extraction. Finetuning with supervised learning is then performed
to enhance the discriminative capabilities of this model. The learned weights
and activations of the autoencoder are analyzed to identify the most important
features for discriminating between zero-day threats and normal system
behavior. These selected features form a reduced feature set that enables
accurate classification. The results indicate that the SAE-LSTM performs well
across all three attack categories by showcasing high precision, recall, and F1
score values, emphasizing the model's strong predictive capabilities in
identifying various types of zero-day attacks. Additionally, the balanced
average scores of the SAE-LSTM suggest that the model generalizes effectively
and consistently across different attack categories.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00306" title="Abstract">arXiv:2311.00306</a> [<a href="/pdf/2311.00306" title="Download PDF">pdf</a>, <a href="/format/2311.00306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Explicit and Implicit Gender Bias through LLM Conditional Text  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangjue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Socially Responsible Language Modelling Research (SoLaR) 2023 at NeurIPS 2023; the first two authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) can generate biased and toxic responses. Yet
most prior work on LLM gender bias evaluation requires predefined
gender-related phrases or gender stereotypes, which are challenging to be
comprehensively collected and are limited to explicit bias evaluation. In
addition, we believe that instances devoid of gender-related language or
explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in
this work, we propose a conditional text generation mechanism without the need
for predefined gender phrases and stereotypes. This approach employs three
types of inputs generated through three distinct strategies to probe LLMs,
aiming to show evidence of explicit and implicit gender biases in LLMs. We also
utilize explicit and implicit evaluation metrics to evaluate gender bias in
LLMs under different strategies. Our experiments demonstrate that an increased
model size does not consistently lead to enhanced fairness and all tested LLMs
exhibit explicit and/or implicit gender bias, even when explicit gender
stereotypes are absent in the inputs.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00308" title="Abstract">arXiv:2311.00308</a> [<a href="/pdf/2311.00308" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Image to Language: A Critical Analysis of Visual Question Answering  (VQA) Approaches, Challenges, and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ishmam%2C+M+F">Md Farhan Ishmam</a>, 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+S+H">Md Sakib Hossain Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M.F. Mridha</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+N">Nilanjan Dey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The multimodal task of Visual Question Answering (VQA) encompassing elements
of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate
answers to questions on any visual input. Over time, the scope of VQA has
expanded from datasets focusing on an extensive collection of natural images to
datasets featuring synthetic images, video, 3D environments, and various other
visual inputs. The emergence of large pre-trained networks has shifted the
early VQA approaches relying on feature extraction and fusion schemes to vision
language pre-training (VLP) techniques. However, there is a lack of
comprehensive surveys that encompass both traditional VQA architectures and
contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of
VQA haven't been thoroughly explored, leaving room for potential open problems
to emerge. Our work presents a survey in the domain of VQA that delves into the
intricacies of VQA datasets and methods over the field's history, introduces a
detailed taxonomy to categorize the facets of VQA, and highlights the recent
trends, challenges, and scopes for improvement. We further generalize VQA to
multimodal question answering, explore tasks related to VQA, and present a set
of open problems for future investigation. The work aims to navigate both
beginners and experts by shedding light on the potential avenues of research
and expanding the boundaries of the field.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00310" title="Abstract">arXiv:2311.00310</a> [<a href="/pdf/2311.00310" title="Download PDF">pdf</a>, <a href="/format/2311.00310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Lexical Simplification with Context Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wada%2C+T">Takashi Wada</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+J+H">Jey Han Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages; accepted for the Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a new unsupervised lexical simplification method that uses only
monolingual data and pre-trained language models. Given a target word and its
context, our method generates substitutes based on the target context and also
additional contexts sampled from monolingual data. We conduct experiments in
English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that
our model substantially outperforms other unsupervised systems across all
languages. We also establish a new state-of-the-art by ensembling our model
with GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution
data set, achieving a state-of-the-art result.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00313" title="Abstract">arXiv:2311.00313</a> [<a href="/pdf/2311.00313" title="Download PDF">pdf</a>, <a href="/format/2311.00313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaze-based Learning from Demonstration In Surgical Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelaal%2C+A+E">A.E. Abdelaal</a>, 
<a href="/search/cs?searchtype=author&query=Zaman%2C+S+N">S.N. Zaman</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P+Y">P.Y Chen</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">T. Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Ingleton%2C+J">J. Ingleton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, Lots of Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Surgical robotics is a rising field in medical technology and advanced
robotics. Robot assisted surgery, or robotic surgery, allows surgeons to
perform complicated surgical tasks with more precision, automation, and
flexibility than is possible for traditional surgical approaches. The main type
of robot assisted surgery is minimally invasive surgery, which could be
automated and result in a faster healing time for the patient. The surgical
robot we are particularly interested in is the da Vinci surgical system, which
is developed and manufactured by Intuitive Surgical. In the current iteration
of the system, the endoscopic camera arm on the da Vinci robot has to be
manually controlled and calibrated by the surgeon during a surgical task, which
interrupts the flow of the operation. The main goal of this capstone project is
to automate the motion of the camera arm using a probabilistic model based on
surgeon eye gaze data and da Vinci robot kinematic data.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00314" title="Abstract">arXiv:2311.00314</a> [<a href="/pdf/2311.00314" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Topic Model and Model Pruning Based on Variational Autoencoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chengjie Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawen Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Meiyu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of 2023 Chinese Intelligent Automation Conference,
  2023: 51-60
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Topic modeling has emerged as a valuable tool for discovering patterns and
topics within large collections of documents. However, when cross-analysis
involves multiple parties, data privacy becomes a critical concern. Federated
topic modeling has been developed to address this issue, allowing multiple
parties to jointly train models while protecting pri-vacy. However, there are
communication and performance challenges in the federated sce-nario. In order
to solve the above problems, this paper proposes a method to establish a
federated topic model while ensuring the privacy of each node, and use neural
network model pruning to accelerate the model, where the client periodically
sends the model neu-ron cumulative gradients and model weights to the server,
and the server prunes the model. To address different requirements, two
different methods are proposed to determine the model pruning rate. The first
method involves slow pruning throughout the entire model training process,
which has limited acceleration effect on the model training process, but can
ensure that the pruned model achieves higher accuracy. This can significantly
reduce the model inference time during the inference process. The second
strategy is to quickly reach the target pruning rate in the early stage of
model training in order to accelerate the model training speed, and then
continue to train the model with a smaller model size after reaching the target
pruning rate. This approach may lose more useful information but can complete
the model training faster. Experimental results show that the federated topic
model pruning based on the variational autoencoder proposed in this paper can
greatly accelerate the model training speed while ensuring the model's
performance.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00317" title="Abstract">arXiv:2311.00317</a> [<a href="/pdf/2311.00317" title="Download PDF">pdf</a>, <a href="/format/2311.00317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation for Code Translation with Comparable Corpora and  Multiple References
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiqing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+A">Atharva Naik</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+D">Daniel Fried</a>, 
<a href="/search/cs?searchtype=author&query=Rose%2C+C">Carolyn Rose</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">One major challenge of translating code between programming languages is that
parallel training data is often limited. To overcome this challenge, we present
two data augmentation techniques, one that builds comparable corpora (i.e.,
code pairs with similar functionality), and another that augments existing
parallel data with multiple reference translations. Specifically, we build and
analyze multiple types of comparable corpora, including programs generated from
natural language documentation using a code generation model. Furthermore, to
reduce overfitting to a single reference translation, we automatically generate
additional translation references for available parallel data and filter the
translations by unit tests, which increases variation in target translations.
Experiments show that our data augmentation techniques significantly improve
CodeT5 for translation between Java, Python, and C++ by an average of 7.5%
Computational Accuracy (CA@1), which verifies the correctness of translations
by execution. The code is available at https://github.com/Veronicium/CMTrans.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00318" title="Abstract">arXiv:2311.00318</a> [<a href="/pdf/2311.00318" title="Download PDF">pdf</a>, <a href="/format/2311.00318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flooding Regularization for Stable Training of Generative Adversarial  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yahiro%2C+I">Iu Yahiro</a>, 
<a href="/search/cs?searchtype=author&query=Ishida%2C+T">Takashi Ishida</a>, 
<a href="/search/cs?searchtype=author&query=Yokoya%2C+N">Naoto Yokoya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14pages, 9 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Generative Adversarial Networks (GANs) have shown remarkable performance in
image generation. However, GAN training suffers from the problem of
instability. One of the main approaches to address this problem is to modify
the loss function, often using regularization terms in addition to changing the
type of adversarial losses. This paper focuses on directly regularizing the
adversarial loss function. We propose a method that applies flooding, an
overfitting suppression method in supervised learning, to GANs to directly
prevent the discriminator's loss from becoming excessively low. Flooding
requires tuning the flood level, but when applied to GANs, we propose that the
appropriate range of flood level settings is determined by the adversarial loss
function, supported by theoretical analysis of GANs using the binary cross
entropy loss. We experimentally verify that flooding stabilizes GAN training
and can be combined with other stabilization techniques. We also reveal that by
restricting the discriminator's loss to be no greater than flood level, the
training proceeds stably even when the flood level is somewhat high.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00320" title="Abstract">arXiv:2311.00320</a> [<a href="/pdf/2311.00320" title="Download PDF">pdf</a>, <a href="/format/2311.00320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veluri%2C+B">Bandhav Veluri</a>, 
<a href="/search/cs?searchtype=author&query=Itani%2C+M">Malek Itani</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J">Justin Chan</a>, 
<a href="/search/cs?searchtype=author&query=Yoshioka%2C+T">Takuya Yoshioka</a>, 
<a href="/search/cs?searchtype=author&query=Gollakota%2C+S">Shyamnath Gollakota</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Imagine being able to listen to the birds chirping in a park without hearing
the chatter from other hikers, or being able to block out traffic noise on a
busy street while still being able to hear emergency sirens and car honks. We
introduce semantic hearing, a novel capability for hearable devices that
enables them to, in real-time, focus on, or ignore, specific sounds from
real-world environments, while also preserving the spatial cues. To achieve
this, we make two technical contributions: 1) we present the first neural
network that can achieve binaural target sound extraction in the presence of
interfering sounds and background noise, and 2) we design a training
methodology that allows our system to generalize to real-world use. Results
show that our system can operate with 20 sound classes and that our
transformer-based network has a runtime of 6.56 ms on a connected smartphone.
In-the-wild evaluation with participants in previously unseen indoor and
outdoor scenarios shows that our proof-of-concept system can extract the target
sounds and generalize to preserve the spatial cues in its binaural output.
Project page with code: https://semantichearing.cs.washington.edu
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00321" title="Abstract">arXiv:2311.00321</a> [<a href="/pdf/2311.00321" title="Download PDF">pdf</a>, <a href="/format/2311.00321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yongjin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonkee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yujin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Namgyu Ho</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023; The first three authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the proliferation of social media, accurate detection of hate speech has
become critical to ensure safety online. To combat nuanced forms of hate
speech, it is important to identify and thoroughly explain hate speech to help
users understand its harmful effects. Recent benchmarks have attempted to
tackle this issue by training generative models on free-text annotations of
implications in hateful text. However, we find significant reasoning gaps in
the existing annotations schemes, which may hinder the supervision of detection
models. In this paper, we introduce a hate speech detection framework, HARE,
which harnesses the reasoning capabilities of large language models (LLMs) to
fill these gaps in explanations of hate speech, thus enabling effective
supervision of detection models. Experiments on SBIC and Implicit Hate
benchmarks show that our method, using model-generated data, consistently
outperforms baselines, using existing free-text human annotations. Analysis
demonstrates that our method enhances the explanation quality of trained models
and improves generalization to unseen datasets. Our code is available at
https://github.com/joonkeekim/hare-hate-speech.git.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00322" title="Abstract">arXiv:2311.00322</a> [<a href="/pdf/2311.00322" title="Download PDF">pdf</a>, <a href="/format/2311.00322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Graph Clustering via Meta Weighting for Noisy Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+H">Hyeonsoo Jo</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+F">Fanchen Bu</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CIKM '23: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">How can we find meaningful clusters in a graph robustly against noise edges?
Graph clustering (i.e., dividing nodes into groups of similar ones) is a
fundamental problem in graph analysis with applications in various fields.
Recent studies have demonstrated that graph neural network (GNN) based
approaches yield promising results for graph clustering. However, we observe
that their performance degenerates significantly on graphs with noise edges,
which are prevalent in practice. In this work, we propose MetaGC for robust
GNN-based graph clustering. MetaGC employs a decomposable clustering loss
function, which can be rephrased as a sum of losses over node pairs. We add a
learnable weight to each node pair, and MetaGC adaptively adjusts the weights
of node pairs using meta-weighting so that the weights of meaningful node pairs
increase and the weights of less-meaningful ones (e.g., noise edges) decrease.
We show empirically that MetaGC learns weights as intended and consequently
outperforms the state-of-the-art GNN-based competitors, even when they are
equipped with separate denoising schemes, on five real-world graphs under
varying levels of noise. Our code and datasets are available at
https://github.com/HyeonsooJo/MetaGC.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00327" title="Abstract">arXiv:2311.00327</a> [<a href="/pdf/2311.00327" title="Download PDF">pdf</a>, <a href="/format/2311.00327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-task Representation Learning for Pure Exploration in Bilinear  Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Subhojyoti Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiaomin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+J+P">Josiah P. Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Nowak%2C+R">Robert Nowak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study multi-task representation learning for the problem of pure
exploration in bilinear bandits. In bilinear bandits, an action takes the form
of a pair of arms from two different entity types and the reward is a bilinear
function of the known feature vectors of the arms. In the \textit{multi-task
bilinear bandit problem}, we aim to find optimal actions for multiple tasks
that share a common low-dimensional linear representation. The objective is to
leverage this characteristic to expedite the process of identifying the best
pair of arms for all tasks. We propose the algorithm GOBLIN that uses an
experimental design approach to optimize sample allocations for learning the
global representation as well as minimize the number of samples needed to
identify the optimal pair of arms in individual tasks. To the best of our
knowledge, this is the first study to give sample complexity analysis for pure
exploration in bilinear bandits with shared representation. Our results
demonstrate that by learning the shared representation across tasks, we achieve
significantly improved sample complexity compared to the traditional approach
of solving tasks independently.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00328" title="Abstract">arXiv:2311.00328</a> [<a href="/pdf/2311.00328" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fault-Tolerant Design Approach Based on Approximate Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+P">P Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Maskell%2C+D+L">D L Maskell</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Electronics, vol. 12, no. 18, Article #3819, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Triple Modular Redundancy (TMR) has been traditionally used to ensure
complete tolerance to a single fault or a faulty processing unit, where the
processing unit may be a circuit or a system. However, TMR incurs more than
200% overhead in terms of area and power compared to a single processing unit.
Hence, alternative redundancy approaches were proposed in the literature to
mitigate the design overheads associated with TMR, but they provide only
partial or moderate fault tolerance. This research presents a new
fault-tolerant design approach based on approximate computing called FAC that
has the same fault tolerance as TMR and achieves significant reductions in the
design metrics for physical implementation. FAC is suited for a plethora of
error-tolerant applications. Here, the performance of TMR and FAC has been
evaluated for a digital image processing application. The image processing
results obtained confirm the usefulness of FAC. When an example processing unit
was implemented using a 28-nm CMOS technology, FAC achieved a 15.3% reduction
in delay, a 19.5% reduction in area, and a 24.7% reduction in power compared to
TMR.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00330" title="Abstract">arXiv:2311.00330</a> [<a href="/pdf/2311.00330" title="Download PDF">pdf</a>, <a href="/format/2311.00330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Space Inference For Spatial Transcriptomics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">J. Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zaman%2C+S+N">S.N. Zaman</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P+Y">P.Y. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">D. Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">In order to understand the complexities of cellular biology, researchers are
interested in two important metrics: the genetic expression information of
cells and their spatial coordinates within a tissue sample. However,
state-of-the art methods, namely single-cell RNA sequencing and image based
spatial transcriptomics can only recover a subset of this information, either
full genetic expression with loss of spatial information, or spatial
information with loss of resolution in sequencing data. In this project, we
investigate a probabilistic machine learning method to obtain the full genetic
expression information for tissues samples while also preserving their spatial
coordinates. This is done through mapping both datasets to a joint latent space
representation with the use of variational machine learning methods. From here,
the full genetic and spatial information can be decoded and to give us greater
insights on the understanding of cellular processes and pathways.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00333" title="Abstract">arXiv:2311.00333</a> [<a href="/pdf/2311.00333" title="Download PDF">pdf</a>, <a href="/format/2311.00333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Caseformer: Pre-training for Legal Case Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Weihang Su</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yueyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yixiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haitao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Legal case retrieval aims to help legal workers find relevant cases related
to their cases at hand, which is important for the guarantee of fairness and
justice in legal judgments. While recent advances in neural retrieval methods
have significantly improved the performance of open-domain retrieval tasks
(e.g., Web search), their advantages have not been observed in legal case
retrieval due to their thirst for annotated data. As annotating large-scale
training data in legal domains is prohibitive due to the need for domain
expertise, traditional search techniques based on lexical matching such as
TF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval
systems. While previous studies have designed several pre-training methods for
IR models in open-domain tasks, these methods are usually suboptimal in legal
case retrieval because they cannot understand and capture the key knowledge and
data structures in the legal corpus. To this end, we propose a novel
pre-training framework named Caseformer that enables the pre-trained models to
learn legal knowledge and domain-specific relevance information in legal case
retrieval without any human-labeled data. Through three unsupervised learning
tasks, Caseformer is able to capture the special language, document structure,
and relevance patterns of legal case documents, making it a strong backbone for
downstream legal case retrieval tasks. Experimental results show that our model
has achieved state-of-the-art performance in both zero-shot and full-data
fine-tuning settings. Also, experiments on both Chinese and English legal
datasets demonstrate that the effectiveness of Caseformer is
language-independent in legal case retrieval.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00334" title="Abstract">arXiv:2311.00334</a> [<a href="/pdf/2311.00334" title="Download PDF">pdf</a>, <a href="/format/2311.00334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp;  Efficient Federated Learning Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stripelis%2C+D">Dimitris Stripelis</a>, 
<a href="/search/cs?searchtype=author&query=Anastasiou%2C+C">Chrysovalantis Anastasiou</a>, 
<a href="/search/cs?searchtype=author&query=Toral%2C+P">Patrick Toral</a>, 
<a href="/search/cs?searchtype=author&query=Asghar%2C+A">Armaghan Asghar</a>, 
<a href="/search/cs?searchtype=author&query=Ambite%2C+J+L">Jose Luis Ambite</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures, Accepted at DistributedML '23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">A Federated Learning (FL) system typically consists of two core processing
entities: the federation controller and the learners. The controller is
responsible for managing the execution of FL workflows across learners and the
learners for training and evaluating federated models over their private
datasets. While executing an FL workflow, the FL system has no control over the
computational resources or data of the participating learners. Still, it is
responsible for other operations, such as model aggregation, task dispatching,
and scheduling. These computationally heavy operations generally need to be
handled by the federation controller. Even though many FL systems have been
recently proposed to facilitate the development of FL workflows, most of these
systems overlook the scalability of the controller. To meet this need, we
designed and developed a novel FL system called MetisFL, where the federation
controller is the first-class citizen. MetisFL re-engineers all the operations
conducted by the federation controller to accelerate the training of
large-scale FL workflows. By quantitatively comparing MetisFL against other
state-of-the-art FL systems, we empirically demonstrate that MetisFL leads to a
10-fold wall-clock time execution boost across a wide range of challenging FL
workflows with increasing model sizes and federation sites.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00335" title="Abstract">arXiv:2311.00335</a> [<a href="/pdf/2311.00335" title="Download PDF">pdf</a>, <a href="/format/2311.00335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BGP Typo: A Longitudinal Study and Remedies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=David%2C+L">Liron David</a>, 
<a href="/search/cs?searchtype=author&query=Shavitt%2C+Y">Yuval Shavitt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">BGP is the protocol that keeps Internet connected. Operators use it by
announcing Address Prefixes (APs), namely IP address blocks, that they own or
that they agree to serve as transit for. BGP enables ISPs to devise complex
policies to control what AP announcements to accept (import policy), the route
selection, and what AP to announce and to whom (export policy). In addition,
BGP is also used to coarse traffic engineering for incoming traffic via the
prepend mechanism.
<br />However, there are no wide-spread good tools for managing BGP and much of the
complex configuration is done by home-brewed scripts or simply by manually
configuring router with bare-bone terminal interface. This process generates
many configuration mistakes.
<br />In this study, we examine typos that propagates in BGP announcements and can
be found in many of the public databases. We classify them and quantify their
presence, and surprisingly found tens of ASNs and hundreds of APs affected by
typos on any given time. In addition, we suggest a simple algorithm that can
detect (and clean) most of them with almost no false positives.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00339" title="Abstract">arXiv:2311.00339</a> [<a href="/pdf/2311.00339" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space Narrative: Generating Images and 3D Scenes of Chinese Garden from  Text using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi1%2C+J">Jiaxi Shi1</a>, 
<a href="/search/cs?searchtype=author&query=Hua1%2C+H">Hao Hua1</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, xArch symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The consistent mapping from poems to paintings is essential for the research
and restoration of traditional Chinese gardens. But the lack of firsthand
ma-terial is a great challenge to the reconstruction work. In this paper, we
pro-pose a method to generate garden paintings based on text descriptions using
deep learning method. Our image-text pair dataset consists of more than one
thousand Ming Dynasty Garden paintings and their inscriptions and post-scripts.
A latent text-to-image diffusion model learns the mapping from de-scriptive
texts to garden paintings of the Ming Dynasty, and then the text description of
Jichang Garden guides the model to generate new garden paintings. The cosine
similarity between the guide text and the generated image is the evaluation
criterion for the generated images. Our dataset is used to fine-tune the
pre-trained diffusion model using Low-Rank Adapta-tion of Large Language Models
(LoRA). We also transformed the generated images into a panorama and created a
free-roam scene in Unity 3D. Our post-trained model is capable of generating
garden images in the style of Ming Dynasty landscape paintings based on textual
descriptions. The gener-ated images are compatible with three-dimensional
presentation in Unity 3D.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00342" title="Abstract">arXiv:2311.00342</a> [<a href="/pdf/2311.00342" title="Download PDF">pdf</a>, <a href="/format/2311.00342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for  Multi-Subject Brain Activity Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xuelin Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+J">Jingyang Huo</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jianfeng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The exploration of brain activity and its decoding from fMRI data has been a
longstanding pursuit, driven by its potential applications in brain-computer
interfaces, medical diagnostics, and virtual reality. Previous approaches have
primarily focused on individual subject analysis, highlighting the need for a
more universal and adaptable framework, which is the core motivation behind our
work. In this work, we propose fMRI-PTE, an innovative auto-encoder approach
for fMRI pre-training, with a focus on addressing the challenges of varying
fMRI data dimensions due to individual brain differences. Our approach involves
transforming fMRI signals into unified 2D representations, ensuring consistency
in dimensions and preserving distinct brain activity patterns. We introduce a
novel learning strategy tailored for pre-training 2D fMRI images, enhancing the
quality of reconstruction. fMRI-PTE's adaptability with image generators
enables the generation of well-represented fMRI features, facilitating various
downstream tasks, including within-subject and cross-subject brain activity
decoding. Our contributions encompass introducing fMRI-PTE, innovative data
transformation, efficient training, a novel learning strategy, and the
universal applicability of our approach. Extensive experiments validate and
support our claims, offering a promising foundation for further research in
this domain.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00343" title="Abstract">arXiv:2311.00343</a> [<a href="/pdf/2311.00343" title="Download PDF">pdf</a>, <a href="/format/2311.00343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Head Orientation of Neurotypical and Autistic Individuals in  Triadic Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tepencelik%2C+O+N">Onur N. Tepencelik</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenchuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Cosman%2C+P+C">Pamela C. Cosman</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+S">Sujit Dey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a system that estimates people's body and head orientations using
low-resolution point cloud data from two LiDAR sensors. Our models make
accurate estimations in real-world conversation settings where the subject
moves naturally with varying head and body poses. The body orientation
estimation model uses ellipse fitting while the head orientation estimation
model is a pipeline of geometric feature extraction and an ensemble of neural
network regressors. Compared with other body and head orientation estimation
systems using RGB cameras, our proposed system uses LiDAR sensors to preserve
user privacy, while achieving comparable accuracy. Unlike other body/head
orientation estimation systems, our sensors do not require a specified
placement in front of the subject. Our models achieve a mean absolute
estimation error of 5.2 degrees for body orientation and 13.7 degrees for head
orientation. We use our models to quantify behavioral differences between
neurotypical and autistic individuals in triadic conversations. Tests of
significance show that people with autism spectrum disorder display
significantly different behavior compared to neurotypical individuals in terms
of distributing attention between participants in a conversation, suggesting
that the approach could be a component of a behavioral analysis or coaching
system.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00344" title="Abstract">arXiv:2311.00344</a> [<a href="/pdf/2311.00344" title="Download PDF">pdf</a>, <a href="/format/2311.00344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sigaud%2C+O">Olivier Sigaud</a>, 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+G">Gianluca Baldassarre</a>, 
<a href="/search/cs?searchtype=author&query=Colas%2C+C">Cedric Colas</a>, 
<a href="/search/cs?searchtype=author&query=Doncieux%2C+S">Stephane Doncieux</a>, 
<a href="/search/cs?searchtype=author&query=Duro%2C+R">Richard Duro</a>, 
<a href="/search/cs?searchtype=author&query=Perrin-Gilbert%2C+N">Nicolas Perrin-Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Santucci%2C+V">Vieri-Giuliano Santucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A lot of recent machine learning research papers have "Open-ended learning"
in their title. But very few of them attempt to define what they mean when
using the term. Even worse, when looking more closely there seems to be no
consensus on what distinguishes open-ended learning from related concepts such
as continual learning, lifelong learning or autotelic learning. In this paper,
we contribute to fixing this situation. After illustrating the genealogy of the
concept and more recent perspectives about what it truly means, we outline that
open-ended learning is generally conceived as a composite notion encompassing a
set of diverse properties. In contrast with these previous approaches, we
propose to isolate a key elementary property of open-ended processes, which is
to always produce novel elements from time to time over an infinite horizon.
From there, we build the notion of open-ended learning problems and focus in
particular on the subset of open-ended goal-conditioned reinforcement learning
problems, as this framework facilitates the definition of learning a growing
repertoire of skills. Finally, we highlight the work that remains to be
performed to fill the gap between our elementary definition and the more
involved notions of open-ended learning that developmental AI researchers may
have in mind.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00346" title="Abstract">arXiv:2311.00346</a> [<a href="/pdf/2311.00346" title="Download PDF">pdf</a>, <a href="/ps/2311.00346" title="Download PostScript">ps</a>, <a href="/format/2311.00346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarially Robust Distributed Count Tracking via Partial Differential  Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhongzheng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaoyi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zengfeng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the distributed tracking model, also known as distributed functional
monitoring. This model involves $k$ sites each receiving a stream of items and
communicating with the central server. The server's task is to track a function
of all items received thus far continuously, with minimum communication cost.
For count tracking, it is known that there is a $\sqrt{k}$ gap in communication
between deterministic and randomized algorithms. However, existing randomized
algorithms assume an "oblivious adversary" who constructs the entire input
streams before the algorithm starts. Here we consider adaptive adversaries who
can choose new items based on previous answers from the algorithm.
Deterministic algorithms are trivially robust to adaptive adversaries, while
randomized ones may not. Therefore, we investigate whether the $\sqrt{k}$
advantage of randomized algorithms is from randomness itself or the oblivious
adversary assumption. We provide an affirmative answer to this question by
giving a robust algorithm with optimal communication. Existing robustification
techniques do not yield optimal bounds due to the inherent challenges of the
distributed nature of the problem. To address this, we extend the differential
privacy framework by introducing "partial differential privacy" and proving a
new generalization theorem. This theorem may have broader applications beyond
robust count tracking, making it of independent interest.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00353" title="Abstract">arXiv:2311.00353</a> [<a href="/pdf/2311.00353" title="Download PDF">pdf</a>, <a href="/format/2311.00353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LatentWarp: Consistent Diffusion Latents for Zero-Shot Video-to-Video  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yuxiang Bao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+D">Di Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+G">Guoliang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Baochang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaiye Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+P">Pengfei Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Leveraging the generative ability of image diffusion models offers great
potential for zero-shot video-to-video translation. The key lies in how to
maintain temporal consistency across generated video frames by image diffusion
models. Previous methods typically adopt cross-frame attention, \emph{i.e.,}
sharing the \textit{key} and \textit{value} tokens across attentions of
different frames, to encourage the temporal consistency. However, in those
works, temporal inconsistency issue may not be thoroughly solved, rendering the
fidelity of generated videos limited.%The current state of the art cross-frame
attention method aims at maintaining fine-grained visual details across frames,
but it is still challenged by the temporal coherence problem. In this paper, we
find the bottleneck lies in the unconstrained query tokens and propose a new
zero-shot video-to-video translation framework, named \textit{LatentWarp}. Our
approach is simple: to constrain the query tokens to be temporally consistent,
we further incorporate a warping operation in the latent space to constrain the
query tokens. Specifically, based on the optical flow obtained from the
original video, we warp the generated latent features of last frame to align
with the current frame during the denoising process. As a result, the
corresponding regions across the adjacent frames can share closely-related
query tokens and attention outputs, which can further improve latent-level
consistency to enhance visual temporal coherence of generated videos. Extensive
experiment results demonstrate the superiority of \textit{LatentWarp} in
achieving video-to-video translation with temporal coherence.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00354" title="Abstract">arXiv:2311.00354</a> [<a href="/pdf/2311.00354" title="Download PDF">pdf</a>, <a href="/ps/2311.00354" title="Download PostScript">ps</a>, <a href="/format/2311.00354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Butson Hadamard matrices, bent sequences, and spherical codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Minjia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+D">Danni Lu</a>, 
<a href="/search/cs?searchtype=author&query=Armario%2C+A">Andr&#xe9;s Armario</a>, 
<a href="/search/cs?searchtype=author&query=Egan%2C+R">Ronan Egan</a>, 
<a href="/search/cs?searchtype=author&query=Ozbudak%2C+F">Ferruh Ozbudak</a>, 
<a href="/search/cs?searchtype=author&query=Sol%C3%A9%2C+P">Patrick Sol&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">We explore a notion of bent sequence attached to the data consisting of an
Hadamard matrix of order $n$ defined over the complex $q^{th}$ roots of unity,
an eigenvalue of that matrix, and a Galois automorphism from the cyclotomic
field of order $q.$ In particular we construct self-dual bent sequences for
various $q\le 60$ and lengths $n\le 21.$ Computational construction methods
comprise the resolution of polynomial systems by Groebner bases and eigenspace
computations. Infinite families can be constructed from regular Hadamard
matrices, Bush-type Hadamard matrices, and generalized Boolean bent
functions.As an application, we estimate the covering radius of the code
attached to that matrix over $\Z_q.$ We derive a lower bound on that quantity
for the Chinese Euclidean metric when bent sequences exist. We give the
Euclidean distance spectrum, and bound above the covering radius of an attached
spherical code, depending on its strength as a spherical design.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00356" title="Abstract">arXiv:2311.00356</a> [<a href="/pdf/2311.00356" title="Download PDF">pdf</a>, <a href="/format/2311.00356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QFree: A Universal Value Function Factorization for Multi-Agent  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rizhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huiping Li</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+D">Di Cui</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Demin Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Centralized training is widely utilized in the field of multi-agent
reinforcement learning (MARL) to assure the stability of training process. Once
a joint policy is obtained, it is critical to design a value function
factorization method to extract optimal decentralized policies for the agents,
which needs to satisfy the individual-global-max (IGM) principle. While
imposing additional limitations on the IGM function class can help to meet the
requirement, it comes at the cost of restricting its application to more
complex multi-agent environments. In this paper, we propose QFree, a universal
value function factorization method for MARL. We start by developing
mathematical equivalent conditions of the IGM principle based on the advantage
function, which ensures that the principle holds without any compromise,
removing the conservatism of conventional methods. We then establish a more
expressive mixing network architecture that can fulfill the equivalent
factorization. In particular, the novel loss function is developed by
considering the equivalent conditions as regularization term during policy
evaluation in the MARL algorithm. Finally, the effectiveness of the proposed
method is verified in a nonmonotonic matrix game scenario. Moreover, we show
that QFree achieves the state-of-the-art performance in a general-purpose
complex MARL benchmark environment, Starcraft Multi-Agent Challenge (SMAC).
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00358" title="Abstract">arXiv:2311.00358</a> [<a href="/pdf/2311.00358" title="Download PDF">pdf</a>, <a href="/format/2311.00358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Samples Selection for Contrastive Learning: Mining of  Potential Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hengkui Dong</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xianzhong Long</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Contrastive learning predicts whether two images belong to the same category
by training a model to make their feature representations as close or as far
away as possible. In this paper, we rethink how to mine samples in contrastive
learning, unlike other methods, our approach is more comprehensive, taking into
account both positive and negative samples, and mining potential samples from
two aspects: First, for positive samples, we consider both the augmented sample
views obtained by data augmentation and the mined sample views through data
mining. Then, we weight and combine them using both soft and hard weighting
strategies. Second, considering the existence of uninformative negative samples
and false negative samples in the negative samples, we analyze the negative
samples from the gradient perspective and finally mine negative samples that
are neither too hard nor too easy as potential negative samples, i.e., those
negative samples that are close to positive samples. The experiments show the
obvious advantages of our method compared with some traditional self-supervised
methods. Our method achieves 88.57%, 61.10%, and 36.69% top-1 accuracy on
CIFAR10, CIFAR100, and TinyImagenet, respectively.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00367" title="Abstract">arXiv:2311.00367</a> [<a href="/pdf/2311.00367" title="Download PDF">pdf</a>, <a href="/format/2311.00367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-based Logical Semantics Enhancement for Implicit Discourse  Relation Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenxu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jian%2C+P">Ping Jian</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Mu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted by the EMNLP 2023 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Implicit Discourse Relation Recognition (IDRR), which infers discourse
relations without the help of explicit connectives, is still a crucial and
challenging task for discourse parsing. Recent works tend to exploit the
hierarchical structure information from the annotated senses, which demonstrate
enhanced discourse relation representations can be obtained by integrating
sense hierarchy. Nevertheless, the performance and robustness for IDRR are
significantly constrained by the availability of annotated data. Fortunately,
there is a wealth of unannotated utterances with explicit connectives, that can
be utilized to acquire enriched discourse relation features. In light of such
motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE)
method for IDRR. Essentially, our method seamlessly injects knowledge relevant
to discourse relation into pre-trained language models through prompt-based
connective prediction. Furthermore, considering the prompt-based connective
prediction exhibits local dependencies due to the deficiency of masked language
model (MLM) in capturing global semantics, we design a novel self-supervised
learning objective based on mutual information maximization to derive enhanced
representations of logical semantics for IDRR. Experimental results on PDTB 2.0
and CoNLL16 datasets demonstrate that our method achieves outstanding and
consistent performance against the current state-of-the-art models.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00368" title="Abstract">arXiv:2311.00368</a> [<a href="/pdf/2311.00368" title="Download PDF">pdf</a>, <a href="/format/2311.00368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel  Max Series GPU
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zubair%2C+M">Mohammad Zubair</a>, 
<a href="/search/cs?searchtype=author&query=Bauinger%2C+C">Christoph Bauinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 1 Table, 19 Figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mathematical Software (cs.MS)

</div>
<p class="mathjax">In this paper, we focus on three sparse matrix operations that are relevant
for machine learning applications, namely, the sparse-dense matrix
multiplication (SPMM), the sampled dense-dense matrix multiplication (SDDMM),
and the composition of the SDDMM with SPMM, also termed as FusedMM. We develop
optimized implementations for SPMM, SDDMM, and FusedMM operations utilizing
Intel oneAPI's Explicit SIMD (ESIMD) SYCL extension API. In contrast to CUDA or
SYCL, the ESIMD API enables the writing of explicitly vectorized kernel code.
Sparse matrix algorithms implemented with the ESIMD API achieved performance
close to the peak of the targeted Intel Data Center GPU. We compare our
performance results to Intel's oneMKL library on Intel GPUs and to a recent
CUDA implementation for the sparse matrix operations on NVIDIA's V100 GPU and
demonstrate that our implementations for sparse matrix operations outperform
either.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00369" title="Abstract">arXiv:2311.00369</a> [<a href="/pdf/2311.00369" title="Download PDF">pdf</a>, <a href="/format/2311.00369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct System Identification of Dynamical Networks with Partial  Measurements: a Maximum Likelihood Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=da+Mata%2C+J+V+G">Jo&#xe3;o Victor Galv&#xe3;o da Mata</a>, 
<a href="/search/eess?searchtype=author&query=Hansson%2C+A">Anders Hansson</a>, 
<a href="/search/eess?searchtype=author&query=Andersen%2C+M+S">Martin S. Andersen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ECC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper introduces a novel direct approach to system identification of
dynamic networks with missing data based on maximum likelihood estimation.
Dynamic networks generally present a singular probability density function,
which poses a challenge in the estimation of their parameters. By leveraging
knowledge about the network's interconnections, we show that it is possible to
transform the problem into more tractable form by applying linear
transformations. This results in a nonsingular probability density function,
enabling the application of maximum likelihood estimation techniques. Our
preliminary numerical results suggest that when combined with global
optimization algorithms or a suitable initialization strategy, we are able to
obtain a good estimate the dynamic of the internal systems.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00371" title="Abstract">arXiv:2311.00371</a> [<a href="/pdf/2311.00371" title="Download PDF">pdf</a>, <a href="/format/2311.00371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Cooperative Trajectory Representations for Motion Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruan%2C+H">Hongzhi Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haibao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenxian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Siqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yingjuan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Z">Zaiqing Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Motion forecasting is an essential task for autonomous driving, and the
effective information utilization from infrastructure and other vehicles can
enhance motion forecasting capabilities. Existing research have primarily
focused on leveraging single-frame cooperative information to enhance the
limited perception capability of the ego vehicle, while underutilizing the
motion and interaction information of traffic participants observed from
cooperative devices. In this paper, we first propose the cooperative trajectory
representations learning paradigm. Specifically, we present V2X-Graph, the
first interpretable and end-to-end learning framework for cooperative motion
forecasting. V2X-Graph employs an interpretable graph to fully leverage the
cooperative motion and interaction contexts. Experimental results on the
vehicle-to-infrastructure (V2I) motion forecasting dataset, V2X-Seq,
demonstrate the effectiveness of V2X-Graph. To further evaluate on V2X
scenario, we construct the first real-world vehicle-to-everything (V2X) motion
forecasting dataset V2X-Traj, and the performance shows the advantage of our
method. We hope both V2X-Graph and V2X-Traj can facilitate the further
development of cooperative motion forecasting. Find project at
https://github.com/AIR-THU/V2X-Graph, find data at
https://github.com/AIR-THU/DAIR-V2X-Seq.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00372" title="Abstract">arXiv:2311.00372</a> [<a href="/pdf/2311.00372" title="Download PDF">pdf</a>, <a href="/ps/2311.00372" title="Download PostScript">ps</a>, <a href="/format/2311.00372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zeroth-Order Feedback-Based Optimization for Distributed Demand Response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jin%2C+R">Ruiyang Jin</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+Y">Yujie Tang</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+J">Jie Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Distributed demand response is a typical distributed optimization problem
that requires coordination among multiple agents to satisfy demand response
requirements. However, existing distributed algorithms for this problem still
face challenges such as unknown system models, nonconvexity, privacy issues,
etc. To address these challenges, we propose and analyze two distributed
algorithms, in which the agents do not share their information and instead
perform local updates using zeroth-order feedback information to estimate the
gradient of the global objective function. One algorithm applies to problems
with general convex and compact feasible sets but has higher oracle complexity
bounded by $O(d/\epsilon^2)$, while the other algorithm achieves lower
complexity bound $O(d/\epsilon)$ but is only applicable to problems with box
constraints. We conduct empirical experiments to validate their performance.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00373" title="Abstract">arXiv:2311.00373</a> [<a href="/pdf/2311.00373" title="Download PDF">pdf</a>, <a href="/format/2311.00373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architecture of Data Anomaly Detection-Enhanced Decentralized Expert  System for Early-Stage Alzheimer&#x27;s Disease Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behfar%2C+S+K">Stefan Kambiz Behfar</a>, 
<a href="/search/cs?searchtype=author&query=Behfar%2C+Q">Qumars Behfar</a>, 
<a href="/search/cs?searchtype=author&query=Hosseinpour%2C+M">Marzie Hosseinpour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Alzheimer's Disease is a global health challenge that requires early and
accurate detection to improve patient outcomes. Magnetic Resonance Imaging
(MRI) holds significant diagnostic potential, but its effective analysis
remains a formidable task. This study introduces a groundbreaking decentralized
expert system that cleverly combines blockchain technology with Artificial
Intelligence (AI) to integrate robust anomaly detection for patient-submitted
data.
<br />Traditional diagnostic methods often lead to delayed and imprecise
predictions, especially in the early stages of the disease. Centralized data
repositories struggle to manage the immense volumes of MRI data, and persistent
privacy concerns hinder collaborative efforts. Our innovative solution
harnesses decentralization to protect data integrity and patient privacy,
facilitated by blockchain technology. It not only emphasizes AI-driven MRI
analysis but also incorporates a sophisticated data anomaly detection
architecture. These mechanisms scrutinize patient-contributed data for various
issues, including data quality problems and atypical findings within MRI
images.
<br />Conducting an exhaustive check of MRI image correctness and quality directly
on the blockchain is impractical due to computational complexity and cost
constraints. Typically, such checks are performed off-chain, and the blockchain
securely records the results. This comprehensive approach empowers our
decentralized app to provide more precise early-stage Alzheimer's Disease
predictions. By merging the strengths of blockchain, AI, and anomaly detection,
our system represents a pioneering step towards revolutionizing disease
diagnostics.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00377" title="Abstract">arXiv:2311.00377</a> [<a href="/pdf/2311.00377" title="Download PDF">pdf</a>, <a href="/format/2311.00377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty quantification and out-of-distribution detection using  surjective normalizing flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dirmeier%2C+S">Simon Dirmeier</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Ye Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+Y">Yanan Xin</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Cruz%2C+F">Fernando Perez-Cruz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Reliable quantification of epistemic and aleatoric uncertainty is of crucial
importance in applications where models are trained in one environment but
applied to multiple different environments, often seen in real-world
applications for example, in climate science or mobility analysis. We propose a
simple approach using surjective normalizing flows to identify
out-of-distribution data sets in deep neural network models that can be
computed in a single forward pass. The method builds on recent developments in
deep uncertainty quantification and generative modeling with normalizing flows.
We apply our method to a synthetic data set that has been simulated using a
mechanistic model from the mobility literature and several data sets simulated
from interventional distributions induced by soft and atomic interventions on
that model, and demonstrate that our method can reliably discern
out-of-distribution data from in-distribution data. We compare the surjective
flow model to a Dirichlet process mixture model and a bijective flow and find
that the surjections are a crucial component to reliably distinguish
in-distribution from out-of-distribution data.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00382" title="Abstract">arXiv:2311.00382</a> [<a href="/pdf/2311.00382" title="Download PDF">pdf</a>, <a href="/ps/2311.00382" title="Download PostScript">ps</a>, <a href="/format/2311.00382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Will Code Remain a Relevant User Interface for End-User Programming with  Generative AI Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Advait Sarkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Advait Sarkar. 2023. "Will Code Remain a Relevant User Interface for End-User Programming with Generative AI Models?" In Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! '23)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 ACM SIGPLAN International Symposium on New
  Ideas, New Paradigms, and Reflections on Programming and Software (Onward!
  '23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">The research field of end-user programming has largely been concerned with
helping non-experts learn to code sufficiently well in order to achieve their
tasks. Generative AI stands to obviate this entirely by allowing users to
generate code from naturalistic language prompts. In this essay, we explore the
extent to which "traditional" programming languages remain relevant for
non-expert end-user programmers in a world with generative AI. We posit the
"generative shift hypothesis": that generative AI will create qualitative and
quantitative expansions in the traditional scope of end-user programming. We
outline some reasons that traditional programming languages may still be
relevant and useful for end-user programmers. We speculate whether each of
these reasons might be fundamental and enduring, or whether they may disappear
with further improvements and innovations in generative AI. Finally, we
articulate a set of implications for end-user programming research, including
the possibility of needing to revisit many well-established core concepts, such
as Ko's learning barriers and Blackwell's attention investment model.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00385" title="Abstract">arXiv:2311.00385</a> [<a href="/pdf/2311.00385" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MolecularWebXR: Multiuser discussions about chemistry and biology in  immersive and inclusive VR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+F+J+C">Fabio J. Cortes Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Frattini%2C+G">Gianfranco Frattini</a>, 
<a href="/search/cs?searchtype=author&query=Meireles%2C+F+T+P">Fernando Teixeira Pinto Meireles</a>, 
<a href="/search/cs?searchtype=author&query=Terrien%2C+D+A">Danae A. Terrien</a>, 
<a href="/search/cs?searchtype=author&query=Cruz-Leon%2C+S">Sergio Cruz-Leon</a>, 
<a href="/search/cs?searchtype=author&query=Peraro%2C+M+D">Matteo Dal Peraro</a>, 
<a href="/search/cs?searchtype=author&query=Schier%2C+E">Eva Schier</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+D+M">Diego M. Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Abriata%2C+L+A">Luciano A. Abriata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Graphics (cs.GR); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">MolecularWebXR is our new website for education, science communication and
scientific peer discussion in chemistry and biology built on WebXR. It
democratizes multi-user, inclusive virtual reality (VR) experiences that are
deeply immersive for users wearing high-end headsets, yet allow participation
by users with consumer devices such as smartphones, possibly inserted into
cardboard goggles for immersivity, or even computers or tablets. With no
installs as it is all web-served, MolecularWebXR enables multiple users to
simultaneously explore, communicate and discuss chemistry and biology concepts
in immersive 3D environments, manipulating objects with their bare hands,
either present in the same real space or scattered throughout the globe thanks
to built-in audio features. A series of preset rooms cover educational material
on chemistry and structural biology, and an empty room can be populated with
material prepared ad hoc using moleculARweb's VMD-based PDB2AR tool. We
verified ease of use and versatility by users aged 12-80 in entirely virtual
sessions or mixed real-virtual sessions at science outreach events, student
instruction, scientific collaborations, and conference lectures. MolecularWebXR
is available for free use without registration at https://molecularwebxr.org,
and a blog post version of this preprint with embedded videos is available at
https://go.epfl.ch/molecularwebxr-blog-post.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00386" title="Abstract">arXiv:2311.00386</a> [<a href="/pdf/2311.00386" title="Download PDF">pdf</a>, <a href="/format/2311.00386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Integration of Self-Sovereign Identity with TLS 1.3 Handshake to  Build Trust in IoT Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perugini%2C+L">Leonardo Perugini</a>, 
<a href="/search/cs?searchtype=author&query=Vesco%2C+A">Andrea Vesco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The centralized PKI is not a suitable solution to provide identities in
large-scale IoT systems. The main problem is the high cost of managing X.509
certificates throughout their lifecycle, from installation to regular updates
and revocation. The Self-Sovereign Identity (SSI) is a decentralised option
that reduces the need for human intervention, and therefore has the potential
to significantly reduce the complexity and cost associated to identity
management in large-scale IoT systems. However, to leverage the full potential
of SSI, the authentication of IoT nodes needs to be moved from the application
to the Transport Layer Security (TLS) level. This paper contributes to the
adoption of SSI in large-scale IoT systems by addressing, for the first time,
the extension of the original TLS 1.3 handshake to support two new SSI
authentication modes while maintaining the interoperability with nodes
implementing the original handshake protocol. The open source implementation of
the new TLS 1.3 handshake protocol in OpenSSL is used to experimentally prove
the feasibility of the approach.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00388" title="Abstract">arXiv:2311.00388</a> [<a href="/pdf/2311.00388" title="Download PDF">pdf</a>, <a href="/format/2311.00388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Automatic Sampling of User Behaviors for Sequential Recommender  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Mingyue Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiding Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Sequential recommender systems (SRS) have gained widespread popularity in
recommendation due to their ability to effectively capture dynamic user
preferences. One default setting in the current SRS is to uniformly consider
each historical behavior as a positive interaction. Actually, this setting has
the potential to yield sub-optimal performance, as each item makes a distinct
contribution to the user's interest. For example, purchased items should be
given more importance than clicked ones. Hence, we propose a general automatic
sampling framework, named AutoSAM, to non-uniformly treat historical behaviors.
Specifically, AutoSAM augments the standard sequential recommendation
architecture with an additional sampler layer to adaptively learn the skew
distribution of the raw input, and then sample informative sub-sets to build
more generalizable SRS. To overcome the challenges of non-differentiable
sampling actions and also introduce multiple decision factors for sampling, we
further introduce a novel reinforcement learning based method to guide the
training of the sampler. We theoretically design multi-objective sampling
rewards including Future Prediction and Sequence Perplexity, and then optimize
the whole framework in an end-to-end manner by combining the policy gradient.
We conduct extensive experiments on benchmark recommender models and four
real-world datasets. The experimental results demonstrate the effectiveness of
the proposed approach. We will make our code publicly available after the
acceptance.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00389" title="Abstract">arXiv:2311.00389</a> [<a href="/pdf/2311.00389" title="Download PDF">pdf</a>, <a href="/format/2311.00389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralGF: Unsupervised Point Normal Estimation by Learning Neural  Gradient Function
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Huifang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+K">Kanle Shi</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yue Gao</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yi Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu-Shen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhizhong Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Normal estimation for 3D point clouds is a fundamental task in 3D geometry
processing. The state-of-the-art methods rely on priors of fitting local
surfaces learned from normal supervision. However, normal supervision in
benchmarks comes from synthetic shapes and is usually not available from real
scans, thereby limiting the learned priors of these methods. In addition,
normal orientation consistency across shapes remains difficult to achieve
without a separate post-processing procedure. To resolve these issues, we
propose a novel method for estimating oriented normals directly from point
clouds without using ground truth normals as supervision. We achieve this by
introducing a new paradigm for learning neural gradient functions, which
encourages the neural network to fit the input point clouds and yield unit-norm
gradients at the points. Specifically, we introduce loss functions to
facilitate query points to iteratively reach the moving targets and aggregate
onto the approximated surface, thereby learning a global surface representation
of the data. Meanwhile, we incorporate gradients into the surface approximation
to measure the minimum signed deviation of queries, resulting in a consistent
gradient field associated with the surface. These techniques lead to our deep
unsupervised oriented normal estimator that is robust to noise, outliers and
density variations. Our excellent results on widely used benchmarks demonstrate
that our method can learn more accurate normals for both unoriented and
oriented normal estimation tasks than the latest methods. The source code and
pre-trained model are publicly available at https://github.com/LeoQLi/NeuralGF.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00390" title="Abstract">arXiv:2311.00390</a> [<a href="/pdf/2311.00390" title="Download PDF">pdf</a>, <a href="/format/2311.00390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Modular Pneumatic Soft Gripper Design for Aerial Grasping and Landing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheung%2C+H+C">Hiu Ching Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Ching-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bailun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chih-Yung Wen</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+H+K">Henry K. Chu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 13 figures, submitted to IEEE RoboSoft 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Aerial robots have garnered significant attention due to their potential
applications in various industries, such as inspection, search and rescue, and
drone delivery. However, the ability of these robots to effectively grasp and
land on objects or surfaces is often crucial for the successful completion of
missions. This paper presents a novel modular soft gripper design tailored
explicitly for aerial grasping and landing operations. The proposed modular
pneumatic soft gripper incorporates a feed-forward proportional controller to
regulate pressure, enabling compliant gripping capabilities. The modular
connectors of the soft fingers offer two configurations of the 4-finger soft
gripper, H-base and X-base, allowing adaptability to different target objects.
Furthermore, when deflated, the gripper can function as a soft landing gear,
reducing the weight and complexity of aerial manipulation control and enhancing
flight efficiency. We demonstrate the efficacy of indoor aerial grasping and
achieve a maximum payload of 217 g for the proposed soft aerial vehicle (SAV),
with the weight of the soft drone being 808 g.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00391" title="Abstract">arXiv:2311.00391</a> [<a href="/pdf/2311.00391" title="Download PDF">pdf</a>, <a href="/format/2311.00391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixation-based Self-calibration for Eye Tracking in VR Headsets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uramune%2C+R">Ryusei Uramune</a>, 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+S">Sei Ikeda</a>, 
<a href="/search/cs?searchtype=author&query=Ishizuka%2C+H">Hiroki Ishizuka</a>, 
<a href="/search/cs?searchtype=author&query=Oshiro%2C+O">Osamu Oshiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This study proposes a novel self-calibration method for eye tracking in a
virtual reality (VR) headset. The proposed method is based on the assumptions
that the user's viewpoint can freely move and that the points of regard (PoRs)
from different viewpoints are distributed within a small area on an object
surface during visual fixation. In the method, fixations are first detected
from the time-series data of uncalibrated gaze directions using an extension of
the I-VDT (velocity and dispersion threshold identification) algorithm to a
three-dimensional (3D) scene. Then, the calibration parameters are optimized by
minimizing the sum of a dispersion metrics of the PoRs. The proposed method can
potentially identify the optimal calibration parameters representing the
user-dependent offset from the optical axis to the visual axis without explicit
user calibration, image processing, or marker-substitute objects. For the gaze
data of 18 participants walking in two VR environments with many occlusions,
the proposed method achieved an accuracy of 2.1$^\circ$, which was
significantly lower than the average offset. Our method is the first
self-calibration method with an average error lower than 3$^\circ$ in 3D
environments. Further, the accuracy of the proposed method can be improved by
up to 1.2$^\circ$ by refining the fixation detection or optimization algorithm.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00393" title="Abstract">arXiv:2311.00393</a> [<a href="/pdf/2311.00393" title="Download PDF">pdf</a>, <a href="/format/2311.00393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting deep neural networks with symbolic knowledge: Towards  trustworthy and interpretable AI for education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hooshyar%2C+D">Danial Hooshyar</a>, 
<a href="/search/cs?searchtype=author&query=Azevedo%2C+R">Roger Azevedo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yeongwook Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Artificial neural networks (ANNs) have shown to be amongst the most important
artificial intelligence (AI) techniques in educational applications, providing
adaptive educational services. However, their educational potential is limited
in practice due to three major challenges: i) difficulty in incorporating
symbolic educational knowledge (e.g., causal relationships, and practitioners'
knowledge) in their development, ii) learning and reflecting biases, and iii)
lack of interpretability. Given the high-risk nature of education, the
integration of educational knowledge into ANNs becomes crucial for developing
AI applications that adhere to essential educational restrictions, and provide
interpretability over the predictions. This research argues that the
neural-symbolic family of AI has the potential to address the named challenges.
To this end, it adapts a neural-symbolic AI framework and accordingly develops
an approach called NSAI, that injects and extracts educational knowledge into
and from deep neural networks, for modelling learners computational thinking.
Our findings reveal that the NSAI approach has better generalizability compared
to deep neural networks trained merely on training data, as well as training
data augmented by SMOTE and autoencoder methods. More importantly, unlike the
other models, the NSAI approach prioritises robust representations that capture
causal relationships between input features and output labels, ensuring safety
in learning to avoid spurious correlations and control biases in training data.
Furthermore, the NSAI approach enables the extraction of rules from the learned
network, facilitating interpretation and reasoning about the path to
predictions, as well as refining the initial educational knowledge. These
findings imply that neural-symbolic AI can overcome the limitations of ANNs in
education, enabling trustworthy and interpretable applications.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00397" title="Abstract">arXiv:2311.00397</a> [<a href="/pdf/2311.00397" title="Download PDF">pdf</a>, <a href="/format/2311.00397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Omni-supervised Referring Expression Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minglang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+G">Gen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Guannan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+W">Weilin Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaoshuai Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Referring Expression Segmentation (RES) is an emerging task in computer
vision, which segments the target instances in images based on text
descriptions. However, its development is plagued by the expensive segmentation
labels. To address this issue, we propose a new learning task for RES called
Omni-supervised Referring Expression Segmentation (Omni-RES), which aims to
make full use of unlabeled, fully labeled and weakly labeled data, e.g.,
referring points or grounding boxes, for efficient RES training. To accomplish
this task, we also propose a novel yet strong baseline method for Omni-RES
based on the recently popular teacher-student learning, where where the weak
labels are not directly transformed into supervision signals but used as a
yardstick to select and refine high-quality pseudo-masks for teacher-student
learning. To validate the proposed Omni-RES method, we apply it to a set of
state-of-the-art RES models and conduct extensive experiments on a bunch of RES
datasets. The experimental results yield the obvious merits of Omni-RES than
the fully-supervised and semi-supervised training schemes. For instance, with
only 10% fully labeled data, Omni-RES can help the base model achieve 100%
fully supervised performance, and it also outperform the semi-supervised
alternative by a large margin, e.g., +14.93% on RefCOCO and +14.95% on
RefCOCO+, respectively. More importantly, Omni-RES also enable the use of
large-scale vision-langauges like Visual Genome to facilitate low-cost RES
training, and achieve new SOTA performance of RES, e.g., 80.66 on RefCOCO.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00399" title="Abstract">arXiv:2311.00399</a> [<a href="/pdf/2311.00399" title="Download PDF">pdf</a>, <a href="/format/2311.00399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Knowledge Injection for Radiology Report Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingqiu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jilan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Runtian Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mohan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuejie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+R">Rui Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaobo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Shang Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by BIBM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Automatic generation of radiology reports holds crucial clinical value, as it
can alleviate substantial workload on radiologists and remind less experienced
ones of potential anomalies. Despite the remarkable performance of various
image captioning methods in the natural image field, generating accurate
reports for medical images still faces challenges, i.e., disparities in visual
and textual data, and lack of accurate domain knowledge. To address these
issues, we propose an enhanced knowledge injection framework, which utilizes
two branches to extract different types of knowledge. The Weighted Concept
Knowledge (WCK) branch is responsible for introducing clinical medical concepts
weighted by TF-IDF scores. The Multimodal Retrieval Knowledge (MRK) branch
extracts triplets from similar reports, emphasizing crucial clinical
information related to entity positions and existence. By integrating this
finer-grained and well-structured knowledge with the current image, we are able
to leverage the multi-source knowledge gain to ultimately facilitate more
accurate report generation. Extensive experiments have been conducted on two
public benchmarks, demonstrating that our method achieves superior performance
over other state-of-the-art methods. Ablation studies further validate the
effectiveness of two extracted knowledge sources.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00400" title="Abstract">arXiv:2311.00400</a> [<a href="/pdf/2311.00400" title="Download PDF">pdf</a>, <a href="/format/2311.00400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Set Face Recognition with Maximal Entropy and Objectosphere Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vareto%2C+R+H">Rafael Henrique Vareto</a>, 
<a href="/search/cs?searchtype=author&query=Linghu%2C+Y">Yu Linghu</a>, 
<a href="/search/cs?searchtype=author&query=Boult%2C+T+E">Terrance E. Boult</a>, 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+W+R">William Robson Schwartz</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCnther%2C+M">Manuel G&#xfc;nther</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Image and Vision Computing 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Open-set face recognition characterizes a scenario where unknown individuals,
unseen during the training and enrollment stages, appear on operation time.
This work concentrates on watchlists, an open-set task that is expected to
operate at a low False Positive Identification Rate and generally includes only
a few enrollment samples per identity. We introduce a compact adapter network
that benefits from additional negative face images when combined with distinct
cost functions, such as Objectosphere Loss (OS) and the proposed Maximal
Entropy Loss (MEL). MEL modifies the traditional Cross-Entropy loss in favor of
increasing the entropy for negative samples and attaches a penalty to known
target classes in pursuance of gallery specialization. The proposed approach
adopts pre-trained deep neural networks (DNNs) for face recognition as feature
extractors. Then, the adapter network takes deep feature representations and
acts as a substitute for the output layer of the pre-trained DNN in exchange
for an agile domain adaptation. Promising results have been achieved following
open-set protocols for three different datasets: LFW, IJB-C, and UCCS as well
as state-of-the-art performance when supplementary negative data is properly
selected to fine-tune the adapter network.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00401" title="Abstract">arXiv:2311.00401</a> [<a href="/pdf/2311.00401" title="Download PDF">pdf</a>, <a href="/format/2311.00401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spatial-Temporal Transformer based Framework For Human Pose Assessment  And Correction in Education Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Libin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+H">Huiliang Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Human pose assessment and correction play a crucial role in applications
across various fields, including computer vision, robotics, sports analysis,
healthcare, and entertainment. In this paper, we propose a Spatial-Temporal
Transformer based Framework (STTF) for human pose assessment and correction in
education scenarios such as physical exercises and science experiment. The
framework comprising skeletal tracking, pose estimation, posture assessment,
and posture correction modules to educate students with professional,
quick-to-fix feedback. We also create a pose correction method to provide
corrective feedback in the form of visual aids. We test the framework with our
own dataset. It comprises (a) new recordings of five exercises, (b) existing
recordings found on the internet of the same exercises, and (c) corrective
feedback on the recordings by professional athletes and teachers. Results show
that our model can effectively measure and comment on the quality of students'
actions. The STTF leverages the power of transformer models to capture spatial
and temporal dependencies in human poses, enabling accurate assessment and
effective correction of students' movements.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00403" title="Abstract">arXiv:2311.00403</a> [<a href="/pdf/2311.00403" title="Download PDF">pdf</a>, <a href="/format/2311.00403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Preserving Time Discretization of Port-Hamiltonian Systems via  Discrete Gradient Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Schulze%2C+P">Philipp Schulze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We discuss structure-preserving time discretization for nonlinear
port-Hamiltonian systems with state-dependent mass matrix. Such systems occur,
for instance, in the context of structure-preserving nonlinear model order
reduction for port-Hamiltonian systems and, in this context,
structure-preserving time discretization is crucial for preserving some of the
properties of the time-continuous reduced-order model. For this purpose, we
introduce a new class of time discretization schemes which is based on
so-called discrete gradient pairs and leads to an exact power balance on the
time-discrete level. Moreover, for the special case of a pointwise symmetric
and positive definite mass matrix, we present an explicit construction of a
discrete gradient pair. Finally, we illustrate the theoretical findings by
means of a numerical example, where the time-continuous system is a nonlinear
reduced-order model for an advection-diffusion problem.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00405" title="Abstract">arXiv:2311.00405</a> [<a href="/pdf/2311.00405" title="Download PDF">pdf</a>, <a href="/format/2311.00405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Couples can be tractable: New algorithms and hardness results for the  Hospitals / Residents problem with Couples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cs%C3%A1ji%2C+G">Gergely Cs&#xe1;ji</a>, 
<a href="/search/cs?searchtype=author&query=Manlove%2C+D">David Manlove</a>, 
<a href="/search/cs?searchtype=author&query=McBride%2C+I">Iain McBride</a>, 
<a href="/search/cs?searchtype=author&query=Trimble%2C+J">James Trimble</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">In this paper we study the {\sc Hospitals / Residents problem with Couples}
({\sc hrc}), where a solution is a stable matching or a report that none
exists. We present a novel polynomial-time algorithm that can find a
near-feasible stable matching (adjusting the hospitals' capacities by at most
1) in an {\sc hrc} instance where the couples' preferences are sub-responsive
(i.e., if one member switches to a better hospital, than the couple also
improves) and sub-complete (i.e., each pair of hospitals that are individually
acceptable to both members are jointly acceptable for the couple) by reducing
it to an instance of the {\sc Stable Fixtures} problem. We also present a
polynomial-time algorithm for {\sc hrc} in a sub-responsive, sub-complete
instance that is a Dual Market, or where all couples are one of several
possible types. We show that our algorithm also implies the polynomial-time
solvability of a stable b-matching problem, where the underlying graph is a
multigraph with loops.
<br />We complement our algorithms with several hardness results. We show that {\sc
hrc} with sub-responsive and sub-complete couples is NP-hard, even with other
strong restrictions. We also show that {\sc hrc} with a Dual Market is NP-hard
under several simultaneous restrictions. Finally, we show that the problem of
finding a matching with the minimum number of blocking pairs in {\sc hrc} is
not approximable within $m^{1-\varepsilon}$, for any $\varepsilon&gt;0$, where $m$
is the total length of the hospitals' preference lists, unless P=NP, even if
each couple applies to only one pair of hospitals. Our polynomial-time
solvability results greatly expand the class of known tractable instances of
{\sc hrc} and provide additional evidence as to why long-standing entry-level
labour markets that allow couples such as the National Resident Matching
Program remain successful to this day.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00408" title="Abstract">arXiv:2311.00408</a> [<a href="/pdf/2311.00408" title="Download PDF">pdf</a>, <a href="/format/2311.00408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongxin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kexin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+S">Sourav Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+R+N">Raj Nath Patel</a>, 
<a href="/search/cs?searchtype=author&query=Glava%C5%A1%2C+G">Goran Glava&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent work has found that few-shot sentence classification based on
pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In
this work, we investigate strategies for domain-specialization in the context
of few-shot sentence classification with SEs. We first establish that
unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language
Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot
sentence classification by up to 8.4 points. However, applying DAPT on SEs, on
the one hand, disrupts the effects of their (general-domain) Sentence Embedding
Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of
a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient,
since the computationally expensive SEPT needs to be executed on top of a
DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples
SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be
inserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent's
effectiveness in extensive experiments on 17 different few-shot sentence
classification datasets. AdaSent matches or surpasses the performance of full
SEPT on DAPT-ed PLM, while substantially reducing the training costs. The code
for AdaSent is available.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00412" title="Abstract">arXiv:2311.00412</a> [<a href="/pdf/2311.00412" title="Download PDF">pdf</a>, <a href="/format/2311.00412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature-oriented Deep Learning Framework for Pulmonary Cone-beam CT  (CBCT) Enhancement with Multi-task Customized Perceptual Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiarui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Werxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hongfei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhi%2C+S">Shaohua Zhi</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jing Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+G">Ge Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages,7 figures,journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Cone-beam computed tomography (CBCT) is routinely collected during
image-guided radiation therapy (IGRT) to provide updated patient anatomy
information for cancer treatments. However, CBCT images often suffer from
streaking artifacts and noise caused by under-rate sampling projections and
low-dose exposure, resulting in low clarity and information loss. While recent
deep learning-based CBCT enhancement methods have shown promising results in
suppressing artifacts, they have limited performance on preserving anatomical
details since conventional pixel-to-pixel loss functions are incapable of
describing detailed anatomy. To address this issue, we propose a novel
feature-oriented deep learning framework that translates low-quality CBCT
images into high-quality CT-like imaging via a multi-task customized
feature-to-feature perceptual loss function. The framework comprises two main
components: a multi-task learning feature-selection network(MTFS-Net) for
customizing the perceptual loss function; and a CBCT-to-CT translation network
guided by feature-to-feature perceptual loss, which uses advanced generative
models such as U-Net, GAN and CycleGAN. Our experiments showed that the
proposed framework can generate synthesized CT (sCT) images for the lung that
achieved a high similarity to CT images, with an average SSIM index of 0.9869
and an average PSNR index of 39.9621. The sCT images also achieved visually
pleasing performance with effective artifacts suppression, noise reduction, and
distinctive anatomical details preservation. Our experiment results indicate
that the proposed framework outperforms the state-of-the-art models for
pulmonary CBCT enhancement. This framework holds great promise for generating
high-quality anatomical imaging from CBCT that is suitable for various clinical
applications.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00416" title="Abstract">arXiv:2311.00416</a> [<a href="/pdf/2311.00416" title="Download PDF">pdf</a>, <a href="/format/2311.00416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Human-AI Coordination via Preparatory Language-based  Convention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+C">Cong Guan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lichao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chunpeng Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lihe Li</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yunjia Tian</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Developing intelligent agents capable of seamless coordination with humans is
a critical step towards achieving artificial general intelligence. Existing
methods for human-AI coordination typically train an agent to coordinate with a
diverse set of policies or with human models fitted from real human data.
However, the massively diverse styles of human behavior present obstacles for
AI systems with constrained capacity, while high quality human data may not be
readily available in real-world scenarios. In this study, we observe that prior
to coordination, humans engage in communication to establish conventions that
specify individual roles and actions, making their coordination proceed in an
orderly manner. Building upon this observation, we propose employing the large
language model (LLM) to develop an action plan (or equivalently, a convention)
that effectively guides both human and AI. By inputting task requirements,
human preferences, the number of agents, and other pertinent information into
the LLM, it can generate a comprehensive convention that facilitates a clear
understanding of tasks and responsibilities for all parties involved.
Furthermore, we demonstrate that decomposing the convention formulation problem
into sub-problems with multiple new sessions being sequentially employed and
human feedback, will yield a more efficient coordination convention.
Experimental evaluations conducted in the Overcooked-AI environment, utilizing
a human proxy model, highlight the superior performance of our proposed method
compared to existing learning-based approaches. When coordinating with real
humans, our method achieves better alignment with human preferences and an
average performance improvement of 15% compared to the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00420" title="Abstract">arXiv:2311.00420</a> [<a href="/pdf/2311.00420" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A cost-benefit source-receptor framework for implementation of  Blue-Green flood risk management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iliadis%2C+C">Christos Iliadis</a>, 
<a href="/search/cs?searchtype=author&query=Glenis%2C+V">Vassilis Glenis</a>, 
<a href="/search/cs?searchtype=author&query=Kilsby%2C+C">Chris Kilsby</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper submitted for publication in the Journal of Hydrology (HYDROL52904). 23 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">As floods are a major and growing source of risk in urban areas, there is a
necessity to improve flood risk management frameworks and civil protection
through planning interventions that modify surface flow pathways and introduce
storage. Despite the complexity of densely urbanised areas, modern flood models
can represent urban features and flow characteristics to help researchers,
local authorities, and insurance companies to develop and improve efficient
flood risk frameworks to achieve resilience in cities. A cost-benefit driven
source-receptor flood risk framework is developed in this study to identify (1)
locations contributing to surface flooding (sources), (2) buildings and
locations at high flood risk (receptors), (3) the cost-benefit nexus between
the source and the receptor, and finally (4) ways to mitigate flooding at the
receptor by adding Blue-Green Infrastructure (BGI) in critical locations. The
analysis is based on five steps to identify the source and the receptor in a
study area based on the flood exposure of buildings, damages arising from
flooding and available green spaces with the best potential to add sustainable
and resilient solutions to reduce flooding. The framework was developed using
the detailed hydrodynamic model CityCAT in a case study of the city centre of
Newcastle upon Tyne, UK. The novelty of this analysis is that firstly, multiple
storm magnitudes (i.e. small and large floods) are used combined with a method
to locate the areas and the buildings at flood risk and a prioritized set of
best places to add interventions upstream and downstream. Secondly, planning
decisions are informed by considering the benefit from reduced damages to
properties and the cost to construct resilient BGI options rather than a
restricted hydraulic analysis considering only flood depths and storages in
isolation from real-world economics.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00422" title="Abstract">arXiv:2311.00422</a> [<a href="/pdf/2311.00422" title="Download PDF">pdf</a>, <a href="/format/2311.00422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Universal Atomic Composability: A Formal Model for Multi-Rollup  Environments on Ethereum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+D">Dipankar Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper introduces a comprehensive formal model to address the atomic
composability across multiple rollups on Ethereum. Drawing inspiration from
classical distributed system solutions and modern cryptographic techniques, the
proposed model integrates mechanisms like buffering, dependency handling,
concurrency control, and the revolutionary zero-knowledge proofs. Beyond the
mere proposal of the model, we delve into its practical implications, limits,
potential pitfalls, and corrective measures, ensuring that the system remains
resistant to manipulative or erroneous behaviors.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00423" title="Abstract">arXiv:2311.00423</a> [<a href="/pdf/2311.00423" title="Download PDF">pdf</a>, <a href="/format/2311.00423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMRec: Large Language Models with Graph Augmentation for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xubin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiabin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qinyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+L">Lixin Su</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Suqi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2024 Oral Presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The problem of data sparsity has long been a challenge in recommendation
systems, and previous studies have attempted to address this issue by
incorporating side information. However, this approach often introduces side
effects such as noise, availability issues, and low data quality, which in turn
hinder the accurate modeling of user preferences and adversely impact
recommendation performance. In light of the recent advancements in large
language models (LLMs), which possess extensive knowledge bases and strong
reasoning capabilities, we propose a novel framework called LLMRec that
enhances recommender systems by employing three simple yet effective LLM-based
graph augmentation strategies. Our approach leverages the rich content
available within online platforms (e.g., Netflix, MovieLens) to augment the
interaction graph in three ways: (i) reinforcing user-item interaction egde,
(ii) enhancing the understanding of item node attributes, and (iii) conducting
user node profiling, intuitively from the natural language perspective. By
employing these strategies, we address the challenges posed by sparse implicit
feedback and low-quality side information in recommenders. Besides, to ensure
the quality of the augmentation, we develop a denoised data robustification
mechanism that includes techniques of noisy implicit feedback pruning and
MAE-based feature enhancement that help refine the augmented data and improve
its reliability. Furthermore, we provide theoretical analysis to support the
effectiveness of LLMRec and clarify the benefits of our method in facilitating
model optimization. Experimental results on benchmark datasets demonstrate the
superiority of our LLM-based augmentation approach over state-of-the-art
techniques. To ensure reproducibility, we have made our code and augmented data
publicly available at: https://github.com/HKUDS/LLMRec.git
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00425" title="Abstract">arXiv:2311.00425</a> [<a href="/pdf/2311.00425" title="Download PDF">pdf</a>, <a href="/format/2311.00425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Implicit Field Editing Considering Object-environment Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhihong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zongji Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanben Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+W">Weinan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zehao Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lili Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junyi Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The 3D scene editing method based on neural implicit field has gained wide
attention. It has achieved excellent results in 3D editing tasks. However,
existing methods often blend the interaction between objects and scene
environment. The change of scene appearance like shadows is failed to be
displayed in the rendering view. In this paper, we propose an Object and Scene
environment Interaction aware (OSI-aware) system, which is a novel two-stream
neural rendering system considering object and scene environment interaction.
To obtain illuminating conditions from the mixture soup, the system
successfully separates the interaction between objects and scene environment by
intrinsic decomposition method. To study the corresponding changes to the scene
appearance from object-level editing tasks, we introduce a depth map guided
scene inpainting method and shadow rendering method by point matching strategy.
Extensive experiments demonstrate that our novel pipeline produce reasonable
appearance changes in scene editing tasks. It also achieve competitive
performance for the rendering quality in novel-view synthesis tasks.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00426" title="Abstract">arXiv:2311.00426</a> [<a href="/pdf/2311.00426" title="Download PDF">pdf</a>, <a href="/format/2311.00426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Generalization through Prioritization and Diversity in  Self-Imitation Reinforcement Learning over Procedural Environments with  Sparse Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andres%2C+A">Alain Andres</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+D">Daochen Zha</a>, 
<a href="/search/cs?searchtype=author&query=Del+Ser%2C+J">Javier Del Ser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Exploration poses a fundamental challenge in Reinforcement Learning (RL) with
sparse rewards, limiting an agent's ability to learn optimal decision-making
due to a lack of informative feedback signals. Self-Imitation Learning
(self-IL) has emerged as a promising approach for exploration, leveraging a
replay buffer to store and reproduce successful behaviors. However, traditional
self-IL methods, which rely on high-return transitions and assume singleton
environments, face challenges in generalization, especially in
procedurally-generated (PCG) environments. Therefore, new self-IL methods have
been proposed to rank which experiences to persist, but they replay transitions
uniformly regardless of their significance, and do not address the diversity of
the stored demonstrations. In this work, we propose tailored self-IL sampling
strategies by prioritizing transitions in different ways and extending
prioritization techniques to PCG environments. We also address diversity loss
through modifications to counteract the impact of generalization requirements
and bias introduced by prioritization techniques. Our experimental analysis,
conducted over three PCG sparse reward environments, including MiniGrid and
ProcGen, highlights the benefits of our proposed modifications, achieving a new
state-of-the-art performance in the MiniGrid-MultiRoom-N12-S10 environment.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00428" title="Abstract">arXiv:2311.00428</a> [<a href="/pdf/2311.00428" title="Download PDF">pdf</a>, <a href="/format/2311.00428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust  Multi-Exit Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ham%2C+S">Seokil Ham</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jungwuk Park</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">Jaekyun Moon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, accepted by 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While multi-exit neural networks are regarded as a promising solution for
making efficient inference via early exits, combating adversarial attacks
remains a challenging problem. In multi-exit networks, due to the high
dependency among different submodels, an adversarial example targeting a
specific exit not only degrades the performance of the target exit but also
reduces the performance of all other exits concurrently. This makes multi-exit
networks highly vulnerable to simple adversarial attacks. In this paper, we
propose NEO-KD, a knowledge-distillation-based adversarial training strategy
that tackles this fundamental challenge based on two key contributions. NEO-KD
first resorts to neighbor knowledge distillation to guide the output of the
adversarial examples to tend to the ensemble outputs of neighbor exits of clean
data. NEO-KD also employs exit-wise orthogonal knowledge distillation for
reducing adversarial transferability across different submodels. The result is
a significantly improved robustness against adversarial attacks. Experimental
results on various datasets/models show that our method achieves the best
adversarial accuracy with reduced computation budgets, compared to the
baselines relying on existing adversarial training or knowledge distillation
techniques for multi-exit networks.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00430" title="Abstract">arXiv:2311.00430</a> [<a href="/pdf/2311.00430" title="Download PDF">pdf</a>, <a href="/format/2311.00430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo  Labelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gandhi%2C+S">Sanchit Gandhi</a>, 
<a href="/search/cs?searchtype=author&query=von+Platen%2C+P">Patrick von Platen</a>, 
<a href="/search/cs?searchtype=author&query=Rush%2C+A+M">Alexander M. Rush</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 2 figures, 25 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">As the size of pre-trained speech recognition models increases, running these
large models in low-latency or resource-constrained environments becomes
challenging. In this work, we leverage pseudo-labelling to assemble a
large-scale open-source dataset which we use to distill the Whisper model into
a smaller variant, called Distil-Whisper. Using a simple word error rate (WER)
heuristic, we select only the highest quality pseudo-labels for training. The
distilled model is 5.8 times faster with 51% fewer parameters, while performing
to within 1% WER on out-of-distribution test data in a zero-shot transfer
setting. Distil-Whisper maintains the robustness of the Whisper model to
difficult acoustic conditions, while being less prone to hallucination errors
on long-form audio. Distil-Whisper is designed to be paired with Whisper for
speculative decoding, yielding a 2 times speed-up while mathematically ensuring
the same outputs as the original model. To facilitate further research in this
domain, we make our training code, inference code and models publicly
accessible.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00433" title="Abstract">arXiv:2311.00433</a> [<a href="/pdf/2311.00433" title="Download PDF">pdf</a>, <a href="/ps/2311.00433" title="Download PostScript">ps</a>, <a href="/format/2311.00433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized PI-control and Anti-windup in Resource Sharing Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Agner%2C+F">Felix Agner</a>, 
<a href="/search/eess?searchtype=author&query=Hansson%2C+J">Jonas Hansson</a>, 
<a href="/search/eess?searchtype=author&query=Kergus%2C+P">Pauline Kergus</a>, 
<a href="/search/eess?searchtype=author&query=Rantzer%2C+A">Anders Rantzer</a>, 
<a href="/search/eess?searchtype=author&query=Tarbouriech%2C+S">Sophie Tarbouriech</a>, 
<a href="/search/eess?searchtype=author&query=Zaccarian%2C+L">Luca Zaccarian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We consider control of multiple stable first-order systems which have a
control coupling described by an M-matrix. These agents are subject to
incremental sector-bounded nonlinearities. We show that such plants can be
globally asymptotically stabilized to a unique equilibrium using fully
decentralized proportional integral anti-windup-equipped controllers subject to
local tuning rules. In addition, we show that when the nonlinearities
correspond to the saturation function, the closed-loop asymptotically minimizes
a weighted 1-norm of the agents state mismatch. The control strategy is finally
compared to other state-of-the-art controllers on a numerical district heating
example.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00434" title="Abstract">arXiv:2311.00434</a> [<a href="/pdf/2311.00434" title="Download PDF">pdf</a>, <a href="/format/2311.00434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-based Background-Oriented Schlieren
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shiba%2C+S">Shintaro Shiba</a>, 
<a href="/search/cs?searchtype=author&query=Hamann%2C+F">Friedhelm Hamann</a>, 
<a href="/search/cs?searchtype=author&query=Aoki%2C+Y">Yoshimitsu Aoki</a>, 
<a href="/search/cs?searchtype=author&query=Gallego%2C+G">Guillermo Gallego</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE T-PAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Schlieren imaging is an optical technique to observe the flow of transparent
media, such as air or water, without any particle seeding. However,
conventional frame-based techniques require both high spatial and temporal
resolution cameras, which impose bright illumination and expensive computation
limitations. Event cameras offer potential advantages (high dynamic range, high
temporal resolution, and data efficiency) to overcome such limitations due to
their bio-inspired sensing principle. This paper presents a novel technique for
perceiving air convection using events and frames by providing the first
theoretical analysis that connects event data and schlieren. We formulate the
problem as a variational optimization one combining the linearized event
generation model with a physically-motivated parameterization that estimates
the temporal derivative of the air density. The experiments with accurately
aligned frame- and event camera data reveal that the proposed method enables
event cameras to obtain on par results with existing frame-based optical flow
techniques. Moreover, the proposed method works under dark conditions where
frame-based schlieren fails, and also enables slow-motion analysis by
leveraging the event camera's advantages. Our work pioneers and opens a new
stack of event camera applications, as we publish the source code as well as
the first schlieren dataset with high-quality frame and event data.
https://github.com/tub-rip/event_based_bos
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00436" title="Abstract">arXiv:2311.00436</a> [<a href="/pdf/2311.00436" title="Download PDF">pdf</a>, <a href="/format/2311.00436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Traffic Object Detection in Variable Illumination with  RGB-Event Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhanwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuke Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangmo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei-Yue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Traffic object detection under variable illumination is challenging due to
the information loss caused by the limited dynamic range of conventional
frame-based cameras. To address this issue, we introduce bio-inspired event
cameras and propose a novel Structure-aware Fusion Network (SFNet) that
extracts sharp and complete object structures from the event stream to
compensate for the lost information in images through cross-modality fusion,
enabling the network to obtain illumination-robust representations for traffic
object detection. Specifically, to mitigate the sparsity or blurriness issues
arising from diverse motion states of traffic objects in fixed-interval event
sampling methods, we propose the Reliable Structure Generation Network (RSGNet)
to generate Speed Invariant Frames (SIF), ensuring the integrity and sharpness
of object structures. Next, we design a novel Adaptive Feature Complement
Module (AFCM) which guides the adaptive fusion of two modality features to
compensate for the information loss in the images by perceiving the global
lightness distribution of the images, thereby generating illumination-robust
representations. Finally, considering the lack of large-scale and high-quality
annotations in the existing event-based object detection datasets, we build a
DSEC-Det dataset, which consists of 53 sequences with 63,931 images and more
than 208,000 labels for 8 classes. Extensive experimental results demonstrate
that our proposed SFNet can overcome the perceptual boundaries of conventional
cameras and outperform the frame-based method by 8.0% in mAP50 and 5.9% in
mAP50:95. Our code and dataset will be available at
https://github.com/YN-Yang/SFNet.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00437" title="Abstract">arXiv:2311.00437</a> [<a href="/pdf/2311.00437" title="Download PDF">pdf</a>, <a href="/format/2311.00437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untangling Graphs on Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Verdi%C3%A8re%2C+%C3%89+C">&#xc9;ric Colin de Verdi&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Despr%C3%A9%2C+V">Vincent Despr&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Dubois%2C+L">Lo&#xef;c Dubois</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages. 17 figures. To be presented at SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Consider a graph drawn on a surface (for example, the plane minus a finite
set of obstacle points), possibly with crossings. We provide an algorithm to
decide whether such a drawing can be untangled, namely, if one can slide the
vertices and edges of the graph on the surface (avoiding the obstacles) to
remove all crossings; in other words, whether the drawing is homotopic to an
embedding. While the problem boils down to planarity testing when the surface
is the sphere or the disk (or equivalently the plane without any obstacle), the
other cases have never been studied before, except when the input graph is a
cycle, in an abundant literature in topology and more recently by Despr\'e and
Lazarus [SoCG 2017, J. ACM 2019].
<br />Our algorithm runs in O(m + poly(g+b) n log n) time, where g &gt;= 0 and b &gt;= 0
are the genus and the number of boundary components of the input orientable
surface S, and n is the size of the input graph drawing, lying on some fixed
graph of size m cellularly embedded on S.
<br />We use various techniques from two-dimensional computational topology and
from the theory of hyperbolic surfaces. Most notably, we introduce reducing
triangulations, a novel discrete analog of hyperbolic surfaces in the spirit of
systems of quads by Lazarus and Rivaud [FOCS 2012] and Erickson and Whittlesey
[SODA 2013], which have the additional benefit that reduced paths are unique
and stable upon reversal; they are likely of independent interest. Tailored
data structures are needed to achieve certain homotopy tests efficiently on
these triangulations. As a key subroutine, we rely on an algorithm to test the
weak simplicity of a graph drawn on a surface by Akitaya, Fulek, and T\'oth
[SODA 2018, TALG 2019].
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00440" title="Abstract">arXiv:2311.00440</a> [<a href="/pdf/2311.00440" title="Download PDF">pdf</a>, <a href="/ps/2311.00440" title="Download PostScript">ps</a>, <a href="/format/2311.00440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum $k$- vs. $\ell$-colourings of graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+T">Tamio-Vesa Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=%C5%BDivn%C3%BD%2C+S">Stanislav &#x17d;ivn&#xfd;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We present polynomial-time SDP-based algorithms for the following problem:
For fixed $k \leq \ell$, given a real number $\epsilon&gt;0$ and a graph $G$ that
admits a $k$-colouring with a $\rho$-fraction of the edges coloured properly,
it returns an $\ell$-colouring of $G$ with an $(\alpha \rho -
\epsilon)$-fraction of the edges coloured properly in polynomial time in $G$
and $1 / \epsilon$. Our algorithms are based on the algorithms of Frieze and
Jerrum [Algorithmica'97] and of Karger, Motwani and Sudan [JACM'98].
<br />For $k = 2, \ell = 3$, our algorithm achieves an approximation ratio $\alpha
= 1$, which is the best possible. When $k$ is fixed and $\ell$ grows large, our
algorithm achieves an approximation ratio of $\alpha = 1 - o(1 / \ell)$. When
$k, \ell$ are both large, our algorithm achieves an approximation ratio of
$\alpha = 1 - 1 / \ell + 2 \ln \ell / k \ell - o(\ln \ell / k \ell) - O(1 /
k^2)$; if we fix $d = \ell - k$ and allow $k, \ell$ to grow large, this is
$\alpha = 1 - 1 / \ell + 2 \ln \ell / k \ell - o(\ln \ell / k \ell)$.
<br />By extending the results of Khot, Kindler, Mossel and O'Donnell [SICOMP'07]
to the promise setting, we show that for large $k$ and $\ell$, assuming the
Unique Games Conjecture, it is \NP-hard to achieve an approximation ratio
$\alpha$ greater than $1 - 1 / \ell + 2 \ln \ell / k \ell + o(\ln \ell / k
\ell)$, provided that $\ell$ is bounded by a function that is
$o(\exp(\sqrt[3]{k}))$. For the case where $d = \ell - k$ is fixed, this bound
matches the performance of our algorithm up to $o(\ln \ell / k \ell)$.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00441" title="Abstract">arXiv:2311.00441</a> [<a href="/pdf/2311.00441" title="Download PDF">pdf</a>, <a href="/format/2311.00441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Robustness for Vision Transformer with a Simple Dynamic  Scanning Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotyan%2C+S">Shashank Kotyan</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+D+V">Danilo Vasconcellos Vargas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Neurocomputing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision Transformer (ViT) has demonstrated promising performance in computer
vision tasks, comparable to state-of-the-art neural networks. Yet, this new
type of deep neural network architecture is vulnerable to adversarial attacks
limiting its capabilities in terms of robustness. This article presents a novel
contribution aimed at further improving the accuracy and robustness of ViT,
particularly in the face of adversarial attacks. We propose an augmentation
technique called `Dynamic Scanning Augmentation' that leverages dynamic input
sequences to adaptively focus on different patches, thereby maintaining
performance and robustness. Our detailed investigations reveal that this
adaptability to the input sequence induces significant changes in the attention
mechanism of ViT, even for the same image. We introduce four variations of
Dynamic Scanning Augmentation, outperforming ViT in terms of both robustness to
adversarial attacks and accuracy against natural images, with one variant
showing comparable results. By integrating our augmentation technique, we
observe a substantial increase in ViT's robustness, improving it from $17\%$ to
$92\%$ measured across different types of adversarial attacks. These findings,
together with other comprehensive tests, indicate that Dynamic Scanning
Augmentation enhances accuracy and robustness by promoting a more adaptive type
of attention. In conclusion, this work contributes to the ongoing research on
Vision Transformers by introducing Dynamic Scanning Augmentation as a technique
for improving the accuracy and robustness of ViT. The observed results
highlight the potential of this approach in advancing computer vision tasks and
merit further exploration in future studies.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00442" title="Abstract">arXiv:2311.00442</a> [<a href="/pdf/2311.00442" title="Download PDF">pdf</a>, <a href="/format/2311.00442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Virtual-Peripheral-in-the-Loop : A Hardware-in-the-Loop Strategy to  Bridge the VP/RTL Design-Gap
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadi-Pour%2C+S">Sallar Ahmadi-Pour</a>, 
<a href="/search/cs?searchtype=author&query=Pieper%2C+P">Pascal Pieper</a>, 
<a href="/search/cs?searchtype=author&query=Drechsler%2C+R">Rolf Drechsler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 Pages, 6 figures, 2 tables, 5 listings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Virtual Prototypes act as an executable specification model, offering a
unified behavior reference model for SW and HW engineers. However, between the
VP and the HW still exists a gap, as the step from an architectural level VP
implementation on the Transaction Level Modeling to the Register Transfer Layer
implementation is considerably big. Especially when a company wants to focus on
their Unique Selling-Point, the HW Design Space Exploration and acceptance
tests should start as early as possible. Traditionally, this can only start
once the rest of the System-on-Chip is also implemented in the RTL. As SoCs
consist of many common subsystems like processors, memories, and peripherals,
this may impact the time-to-market considerably. This is avoidable, however: In
this paper we propose a Hardware-in-the-Loop strategy that allows to bridge the
gap between the VP and RTL design that empowers engineers to focus on their USP
while leveraging an existing suite of TLM Intellectual Properties for the
common base-system components. We show how VPs and partial RTL implementations
of a SoC can be combined in a Hardware-in-the-Loop simulation environment
utilizing Field-Programmable Gate Arrays. The proposed approach allows early
DSE, validation, and verification of SoC subsystems, which bridges the TLM/RTL
gap. We evaluate our approach with a lightweight implementation of the proposed
protocol, and three case-studies with real-world peripherals and accelerators
on HW. Furthermore, we assess the capabilities of our approach and offer
practical considerations for engineers utilizing this HIL approach for SoC
design; and finally propose further extensions that can boost the approach for
specialized applications like high-performance accelerators and computation.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00444" title="Abstract">arXiv:2311.00444</a> [<a href="/pdf/2311.00444" title="Download PDF">pdf</a>, <a href="/format/2311.00444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Form follows Function: Text-to-Text Conditional Graph Generation based  on Functional Requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zachares%2C+P+A">Peter A. Zachares</a>, 
<a href="/search/cs?searchtype=author&query=Hovhannisyan%2C+V">Vahan Hovhannisyan</a>, 
<a href="/search/cs?searchtype=author&query=Mosca%2C+A">Alan Mosca</a>, 
<a href="/search/cs?searchtype=author&query=Gal%2C+Y">Yarin Gal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work focuses on the novel problem setting of generating graphs
conditioned on a description of the graph's functional requirements in a
downstream task. We pose the problem as a text-to-text generation problem and
focus on the approach of fine-tuning a pretrained large language model (LLM) to
generate graphs. We propose an inductive bias which incorporates information
about the structure of the graph into the LLM's generation process by
incorporating message passing layers into an LLM's architecture. To evaluate
our proposed method, we design a novel set of experiments using publicly
available and widely studied molecule and knowledge graph data sets. Results
suggest our proposed approach generates graphs which more closely meet the
requested functional requirements, outperforming baselines developed on similar
tasks by a statistically significant margin.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00445" title="Abstract">arXiv:2311.00445</a> [<a href="/pdf/2311.00445" title="Download PDF">pdf</a>, <a href="/format/2311.00445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Comparison of Syllogistic Reasoning in Humans and Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eisape%2C+T">Tiwalayo Eisape</a>, 
<a href="/search/cs?searchtype=author&query=Tessler%2C+M">MH Tessler</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+F">Fei Sha</a>, 
<a href="/search/cs?searchtype=author&query=van+Steenkiste%2C+S">Sjoerd van Steenkiste</a>, 
<a href="/search/cs?searchtype=author&query=Linzen%2C+T">Tal Linzen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">A central component of rational behavior is logical inference: the process of
determining which conclusions follow from a set of premises. Psychologists have
documented several ways in which humans' inferences deviate from the rules of
logic. Do language models, which are trained on text generated by humans,
replicate these biases, or are they able to overcome them? Focusing on the case
of syllogisms -- inferences from two simple premises, which have been studied
extensively in psychology -- we show that larger models are more logical than
smaller ones, and also more logical than humans. At the same time, even the
largest models make systematic errors, some of which mirror human reasoning
biases such as ordering effects and logical fallacies. Overall, we find that
language models mimic the human biases included in their training data, but are
able to overcome them in some cases.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00447" title="Abstract">arXiv:2311.00447</a> [<a href="/pdf/2311.00447" title="Download PDF">pdf</a>, <a href="/format/2311.00447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Opportunities of Green Computing: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">You Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiujing Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Maolin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+G">Gangwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huakang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yupeng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kehang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yongduo Sui</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Fengwei Jia</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zuoli Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tiannuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weibo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yunong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi Li</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+D">De Bao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+H">Hongrui Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinchi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=WEI%2C+Y">Ying WEI</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Hong Qian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kin%2C+W">Wai Kin</a> (Victor)
<a href="/search/cs?searchtype=author&query=Chan">Chan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yusen Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shiyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jining Yan</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+C">Chao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shuai Han</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wuxia Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guannan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiaodong Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 113 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Artificial Intelligence (AI) has achieved significant advancements in
technology and research with the development over several decades, and is
widely used in many areas including computing vision, natural language
processing, time-series analysis, speech synthesis, etc. During the age of deep
learning, especially with the arise of Large Language Models, a large majority
of researchers' attention is paid on pursuing new state-of-the-art (SOTA)
results, resulting in ever increasing of model size and computational
complexity. The needs for high computing power brings higher carbon emission
and undermines research fairness by preventing small or medium-sized research
institutions and companies with limited funding in participating in research.
To tackle the challenges of computing resources and environmental impact of AI,
Green Computing has become a hot research topic. In this survey, we give a
systematic overview of the technologies used in Green Computing. We propose the
framework of Green Computing and devide it into four key components: (1)
Measures of Greenness, (2) Energy-Efficient AI, (3) Energy-Efficient Computing
Systems and (4) AI Use Cases for Sustainability. For each components, we
discuss the research progress made and the commonly used techniques to optimize
the AI efficiency. We conclude that this new research direction has the
potential to address the conflicts between resource constraints and AI
development. We encourage more researchers to put attention on this direction
and make AI more environmental friendly.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00451" title="Abstract">arXiv:2311.00451</a> [<a href="/pdf/2311.00451" title="Download PDF">pdf</a>, <a href="/format/2311.00451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discourse Relations Classification and Cross-Framework Discourse  Relation Classification Through the Lens of Cognitive Dimensions: An  Empirical Investigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yingxue Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing discourse formalisms use different taxonomies of discourse
relations, which require expert knowledge to understand, posing a challenge for
annotation and automatic classification. We show that discourse relations can
be effectively captured by some simple cognitively inspired dimensions proposed
by Sanders et al.(2018). Our experiments on cross-framework discourse relation
classification (PDTB &amp; RST) demonstrate that it is possible to transfer
knowledge of discourse relations for one framework to another framework by
means of these dimensions, in spite of differences in discourse segmentation of
the two frameworks. This manifests the effectiveness of these dimensions in
characterizing discourse relations across frameworks. Ablation studies reveal
that different dimensions influence different types of discourse relations. The
patterns can be explained by the role of dimensions in characterizing and
distinguishing different relations. We also report our experimental results on
automatic prediction of these dimensions.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00452" title="Abstract">arXiv:2311.00452</a> [<a href="/pdf/2311.00452" title="Download PDF">pdf</a>, <a href="/format/2311.00452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hessian Eigenvectors and Principal Component Analysis of Neural Network  Weight Matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haink%2C+D">David Haink</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Master thesis: 60 pages, 35 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This study delves into the intricate dynamics of trained deep neural networks
and their relationships with network parameters. Trained networks predominantly
continue training in a single direction, known as the drift mode. This drift
mode can be explained by the quadratic potential model of the loss function,
suggesting a slow exponential decay towards the potential minima. We unveil a
correlation between Hessian eigenvectors and network weights. This
relationship, hinging on the magnitude of eigenvalues, allows us to discern
parameter directions within the network. Notably, the significance of these
directions relies on two defining attributes: the curvature of their potential
wells (indicated by the magnitude of Hessian eigenvalues) and their alignment
with the weight vectors. Our exploration extends to the decomposition of weight
matrices through singular value decomposition. This approach proves practical
in identifying critical directions within the Hessian, considering both their
magnitude and curvature. Furthermore, our examination showcases the
applicability of principal component analysis in approximating the Hessian,
with update parameters emerging as a superior choice over weights for this
purpose. Remarkably, our findings unveil a similarity between the largest
Hessian eigenvalues of individual layers and the entire network. Notably,
higher eigenvalues are concentrated more in deeper layers. Leveraging these
insights, we venture into addressing catastrophic forgetting, a challenge of
neural networks when learning new tasks while retaining knowledge from previous
ones. By applying our discoveries, we formulate an effective strategy to
mitigate catastrophic forgetting, offering a possible solution that can be
applied to networks of varying scales, including larger architectures.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00453" title="Abstract">arXiv:2311.00453</a> [<a href="/pdf/2311.00453" title="Download PDF">pdf</a>, <a href="/format/2311.00453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiangning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+G">Guanzhong Tian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haoyang He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wuhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yabiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yunsheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper considers zero-shot Anomaly Detection (AD), a valuable yet
under-studied task, which performs AD without any reference images of the test
objects. Specifically, we employ a language-guided strategy and propose a
simple-yet-effective architecture CLIP-AD, leveraging the superior zero-shot
classification capabilities of the large vision-language model CLIP. A natural
idea for anomaly segmentation is to directly calculate the similarity between
text/image features, but we observe opposite predictions and irrelevant
highlights in the results. Inspired by the phenomena, we introduce a Staged
Dual-Path model (SDP) that effectively uses features from various levels and
applies architecture and feature surgery to address these issues. Furthermore,
delving beyond surface phenomena, we identify the problem arising from
misalignment of text/image features in the joint embedding space. Thus, we
introduce a fine-tuning strategy by adding linear layers and construct an
extended model SDP+, further enhancing the performance. Abundant experiments
demonstrate the effectiveness of our approach, e.g., on VisA, SDP outperforms
SOTA by +1.0/+1.2 in classification/segmentation F1 scores, while SDP+ achieves
+1.9/+11.7 improvements.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00455" title="Abstract">arXiv:2311.00455</a> [<a href="/pdf/2311.00455" title="Download PDF">pdf</a>, <a href="/format/2311.00455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Progressive Recurrent Network for Shadow Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yonghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wengang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Hao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Single-image shadow removal is a significant task that is still unresolved.
Most existing deep learning-based approaches attempt to remove the shadow
directly, which can not deal with the shadow well. To handle this issue, we
consider removing the shadow in a coarse-to-fine fashion and propose a simple
but effective Progressive Recurrent Network (PRNet). The network aims to remove
the shadow progressively, enabing us to flexibly adjust the number of
iterations to strike a balance between performance and time. Our network
comprises two parts: shadow feature extraction and progressive shadow removal.
Specifically, the first part is a shallow ResNet which constructs the
representations of the input shadow image on its original size, preventing the
loss of high-frequency details caused by the downsampling operation. The second
part has two critical components: the re-integration module and the update
module. The proposed re-integration module can fully use the outputs of the
previous iteration, providing input for the update module for further shadow
removal. In this way, the proposed PRNet makes the whole process more concise
and only uses 29% network parameters than the best published method. Extensive
experiments on the three benchmarks, ISTD, ISTD+, and SRD, demonstrate that our
method can effectively remove shadows and achieve superior performance.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00457" title="Abstract">arXiv:2311.00457</a> [<a href="/pdf/2311.00457" title="Download PDF">pdf</a>, <a href="/format/2311.00457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single-view 3D Scene Reconstruction with High-fidelity Shape and Texture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Junfeng Ni</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyuan Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV 2024, project page: <a href="https://dali-jack.github.io/SSR/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Reconstructing detailed 3D scenes from single-view images remains a
challenging task due to limitations in existing approaches, which primarily
focus on geometric shape recovery, overlooking object appearances and fine
shape details. To address these challenges, we propose a novel framework for
simultaneous high-fidelity recovery of object shapes and textures from
single-view images. Our approach utilizes the proposed Single-view neural
implicit Shape and Radiance field (SSR) representations to leverage both
explicit 3D shape supervision and volume rendering of color, depth, and surface
normal images. To overcome shape-appearance ambiguity under partial
observations, we introduce a two-stage learning curriculum incorporating both
3D and 2D supervisions. A distinctive feature of our framework is its ability
to generate fine-grained textured meshes while seamlessly integrating rendering
capabilities into the single-view 3D reconstruction model. This integration
enables not only improved textured 3D object reconstruction by 27.7% and 11.6%
on the 3D-FRONT and Pix3D datasets, respectively, but also supports the
rendering of images from novel viewpoints. Beyond individual objects, our
approach facilitates composing object-level representations into flexible scene
representations, thereby enabling applications such as holistic scene
understanding and 3D scene editing. We conduct extensive experiments to
demonstrate the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00460" title="Abstract">arXiv:2311.00460</a> [<a href="/pdf/2311.00460" title="Download PDF">pdf</a>, <a href="/format/2311.00460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Budgeted Rejection Sampling for Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verine%2C+A">Alexandre Verine</a>, 
<a href="/search/cs?searchtype=author&query=Pydi%2C+M+S">Muni Sreenivas Pydi</a>, 
<a href="/search/cs?searchtype=author&query=Negrevergne%2C+B">Benjamin Negrevergne</a>, 
<a href="/search/cs?searchtype=author&query=Chevaleyre%2C+Y">Yann Chevaleyre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Rejection sampling methods have recently been proposed to improve the
performance of discriminator-based generative models. However, these methods
are only optimal under an unlimited sampling budget, and are usually applied to
a generator trained independently of the rejection procedure. We first propose
an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal
with respect to \textit{any} $f$-divergence between the true distribution and
the post-rejection distribution, for a given sampling budget. Second, we
propose an end-to-end method that incorporates the sampling scheme into the
training procedure to further enhance the model's overall performance. Through
experiments and supporting theory, we show that the proposed methods are
effective in significantly improving the quality and diversity of the samples.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00462" title="Abstract">arXiv:2311.00462</a> [<a href="/pdf/2311.00462" title="Download PDF">pdf</a>, <a href="/format/2311.00462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Heng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chongjie Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Multi-cellular robot design aims to create robots comprised of numerous cells
that can be efficiently controlled to perform diverse tasks. Previous research
has demonstrated the ability to generate robots for various tasks, but these
approaches often optimize robots directly in the vast design space, resulting
in robots with complicated morphologies that are hard to control. In response,
this paper presents a novel coarse-to-fine method for designing multi-cellular
robots. Initially, this strategy seeks optimal coarse-grained robots and
progressively refines them. To mitigate the challenge of determining the
precise refinement juncture during the coarse-to-fine transition, we introduce
the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies
robots of various granularity within a shared hyperbolic space and leverages a
refined Cross-Entropy Method for optimization. This framework enables our
method to autonomously identify areas of exploration in hyperbolic space and
concentrate on regions demonstrating promise. Finally, the extensive empirical
studies on various challenging tasks sourced from EvoGym show our approach's
superior efficiency and generalization capability.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00466" title="Abstract">arXiv:2311.00466</a> [<a href="/pdf/2311.00466" title="Download PDF">pdf</a>, <a href="/format/2311.00466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized covering in semi-ladder-free hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guillemot%2C+S">Sylvain Guillemot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">In this article, we study the parameterized complexity of the Set Cover
problem restricted to d-semi-ladder-free hypergraphs, a class defined by
Fabianski et al. [Proceedings of STACS 2019]. We observe that two algorithms
introduced by Langerman and Morin [Discrete \&amp; Computational Geometry 2005] in
the context of geometric covering problems can be adapted to this setting,
yielding simple FPT and kernelization algorithms for Set Cover in
d-semi-ladder-free hypergraphs. We complement our algorithmic results with a
compression lower bound for the problem, that proves the tightness of our
kernelization under standard complexity-theoretic assumptions.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00469" title="Abstract">arXiv:2311.00469</a> [<a href="/pdf/2311.00469" title="Download PDF">pdf</a>, <a href="/format/2311.00469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Conditioned Diffusion Models for Out-Of-Distribution Detection:  Application to Fetal Ultrasound Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+D">Divyanshu Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Pramit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Papageorghiou%2C+A+T">Aris T. Papageorghiou</a>, 
<a href="/search/cs?searchtype=author&query=Noble%2C+J+A">J.Alison Noble</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in MICCAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Out-of-distribution (OOD) detection is essential to improve the reliability
of machine learning models by detecting samples that do not belong to the
training distribution. Detecting OOD samples effectively in certain tasks can
pose a challenge because of the substantial heterogeneity within the
in-distribution (ID), and the high structural similarity between ID and OOD
classes. For instance, when detecting heart views in fetal ultrasound videos
there is a high structural similarity between the heart and other anatomies
such as the abdomen, and large in-distribution variance as a heart has 5
distinct views and structural variations within each view. To detect OOD
samples in this context, the resulting model should generalise to the
intra-anatomy variations while rejecting similar OOD samples. In this paper, we
introduce dual-conditioned diffusion models (DCDM) where we condition the model
on in-distribution class information and latent features of the input image for
reconstruction-based OOD detection. This constrains the generative manifold of
the model to generate images structurally and semantically similar to those
within the in-distribution. The proposed model outperforms reference methods
with a 12% improvement in accuracy, 22% higher precision, and an 8% better F1
score.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00474" title="Abstract">arXiv:2311.00474</a> [<a href="/pdf/2311.00474" title="Download PDF">pdf</a>, <a href="/ps/2311.00474" title="Download PostScript">ps</a>, <a href="/format/2311.00474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion models for probabilistic programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dirmeier%2C+S">Simon Dirmeier</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Cruz%2C+F">Fernando Perez-Cruz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose Diffusion Model Variational Inference (DMVI), a novel method for
automated approximate inference in probabilistic programming languages (PPLs).
DMVI utilizes diffusion models as variational approximations to the true
posterior distribution by deriving a novel bound to the marginal likelihood
objective used in Bayesian modelling. DMVI is easy to implement, allows
hassle-free inference in PPLs without the drawbacks of, e.g., variational
inference using normalizing flows, and does not make any constraints on the
underlying neural network model. We evaluate DMVI on a set of common Bayesian
models and show that its posterior inferences are in general more accurate than
those of contemporary methods used in PPLs while having a similar computational
cost and requiring less manual tuning.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00475" title="Abstract">arXiv:2311.00475</a> [<a href="/pdf/2311.00475" title="Download PDF">pdf</a>, <a href="/format/2311.00475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Style Locality for Controllable Generation with kNN Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nawezi%2C+G">Gilles Nawezi</a>, 
<a href="/search/cs?searchtype=author&query=Flek%2C+L">Lucie Flek</a>, 
<a href="/search/cs?searchtype=author&query=Welch%2C+C">Charles Welch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TamingLLM Workshop at SIGDIAL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent language models have been improved by the addition of external memory.
Nearest neighbor language models retrieve similar contexts to assist in word
prediction. The addition of locality levels allows a model to learn how to
weight neighbors based on their relative location to the current text in source
documents, and have been shown to further improve model performance. Nearest
neighbor models have been explored for controllable generation but have not
examined the use of locality levels. We present a novel approach for this
purpose and evaluate it using automatic and human evaluation on politeness,
formality, supportiveness, and toxicity textual data. We find that our model is
successfully able to control style and provides a better fluency-style
trade-off than previous work.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00476" title="Abstract">arXiv:2311.00476</a> [<a href="/pdf/2311.00476" title="Download PDF">pdf</a>, <a href="/format/2311.00476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group Distributionally Robust Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vilouras%2C+K">Konstantinos Vilouras</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+P">Pedro Sanchez</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neil%2C+A+Q">Alison Q. O&#x27;Neil</a>, 
<a href="/search/cs?searchtype=author&query=Tsaftaris%2C+S+A">Sotirios A. Tsaftaris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, MLMI workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge distillation enables fast and effective transfer of features
learned from a bigger model to a smaller one. However, distillation objectives
are susceptible to sub-population shifts, a common scenario in medical imaging
analysis which refers to groups/domains of data that are underrepresented in
the training set. For instance, training models on health data acquired from
multiple scanners or hospitals can yield subpar performance for minority
groups. In this paper, inspired by distributionally robust optimization (DRO)
techniques, we address this shortcoming by proposing a group-aware distillation
loss. During optimization, a set of weights is updated based on the per-group
losses at a given iteration. This way, our method can dynamically focus on
groups that have low performance during training. We empirically validate our
method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs)
and show consistent improvement in terms of worst-group accuracy.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00481" title="Abstract">arXiv:2311.00481</a> [<a href="/pdf/2311.00481" title="Download PDF">pdf</a>, <a href="/ps/2311.00481" title="Download PostScript">ps</a>, <a href="/format/2311.00481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed-Budget Best-Arm Identification in Sparse Linear Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yavas%2C+R+C">Recep Can Yavas</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+V+Y+F">Vincent Y. F. Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, Submitted to TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the best-arm identification problem in sparse linear bandits under
the fixed-budget setting. In sparse linear bandits, the unknown feature vector
$\theta^*$ may be of large dimension $d$, but only a few, say $s \ll d$ of
these features have non-zero values. We design a two-phase algorithm, Lasso and
Optimal-Design- (Lasso-OD) based linear best-arm identification. The first
phase of Lasso-OD leverages the sparsity of the feature vector by applying the
thresholded Lasso introduced by Zhou (2009), which estimates the support of
$\theta^*$ correctly with high probability using rewards from the selected arms
and a judicious choice of the design matrix. The second phase of Lasso-OD
applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated
support. We derive a non-asymptotic upper bound on the error probability of
Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization
parameter) and balancing the error probabilities of both phases. For fixed
sparsity $s$ and budget $T$, the exponent in the error probability of Lasso-OD
depends on $s$ but not on the dimension $d$, yielding a significant performance
improvement for sparse and high-dimensional linear bandits. Furthermore, we
show that Lasso-OD is almost minimax optimal in the exponent. Finally, we
provide numerical examples to demonstrate the significant performance
improvement over the existing algorithms for non-sparse linear bandits such as
OD-LinBAI, BayesGap, Peace, LinearExploration, and GSE.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00488" title="Abstract">arXiv:2311.00488</a> [<a href="/pdf/2311.00488" title="Download PDF">pdf</a>, <a href="/format/2311.00488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Optimization Targets for Contrast-Consistent Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fry%2C+H">Hugo Fry</a>, 
<a href="/search/cs?searchtype=author&query=Fallows%2C+S">Seamus Fallows</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+I">Ian Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+J">Jamie Wright</a>, 
<a href="/search/cs?searchtype=author&query=Schoots%2C+N">Nandi Schoots</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Socially Responsible Language Modelling Research (SoLaR) NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We investigate the optimization target of Contrast-Consistent Search (CCS),
which aims to recover the internal representations of truth of a large language
model. We present a new loss function that we call the Midpoint-Displacement
(MD) loss function. We demonstrate that for a certain hyper-parameter value
this MD loss function leads to a prober with very similar weights to CCS. We
further show that this hyper-parameter is not optimal and that with a better
hyper-parameter the MD loss function attains a higher test accuracy than CCS.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00489" title="Abstract">arXiv:2311.00489</a> [<a href="/pdf/2311.00489" title="Download PDF">pdf</a>, <a href="/format/2311.00489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Neural Networks for Automatic Speaker Recognition Do Not Learn  Supra-Segmental Temporal Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neururer%2C+D">Daniel Neururer</a>, 
<a href="/search/cs?searchtype=author&query=Dellow%2C+V">Volker Dellow</a>, 
<a href="/search/cs?searchtype=author&query=Stadelmann%2C+T">Thilo Stadelmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">While deep neural networks have shown impressive results in automatic speaker
recognition and related tasks, it is dissatisfactory how little is understood
about what exactly is responsible for these results. Part of the success has
been attributed in prior work to their capability to model supra-segmental
temporal information (SST), i.e., learn rhythmic-prosodic characteristics of
speech in addition to spectral features. In this paper, we (i) present and
apply a novel test to quantify to what extent the performance of
state-of-the-art neural networks for speaker recognition can be explained by
modeling SST; and (ii) present several means to force respective nets to focus
more on SST and evaluate their merits. We find that a variety of CNN- and
RNN-based neural network architectures for speaker recognition do not model SST
to any sufficient degree, even when forced. The results provide a highly
relevant basis for impactful future research into better exploitation of the
full speech signal and give insights into the inner workings of such networks,
enhancing explainability of deep learning for speech technologies.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00491" title="Abstract">arXiv:2311.00491</a> [<a href="/pdf/2311.00491" title="Download PDF">pdf</a>, <a href="/format/2311.00491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayes-enhanced Multi-view Attention Networks for Robust POI  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jiangnan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Senzhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiannong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Knowledge and Data Engineering, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">POI recommendation is practically important to facilitate various
Location-Based Social Network services, and has attracted rising research
attention recently. Existing works generally assume the available POI check-ins
reported by users are the ground-truth depiction of user behaviors. However, in
real application scenarios, the check-in data can be rather unreliable due to
both subjective and objective causes including positioning error and user
privacy concerns, leading to significant negative impacts on the performance of
the POI recommendation. To this end, we investigate a novel problem of robust
POI recommendation by considering the uncertainty factors of the user
check-ins, and proposes a Bayes-enhanced Multi-view Attention Network.
Specifically, we construct personal POI transition graph, the semantic-based
POI graph and distance-based POI graph to comprehensively model the
dependencies among the POIs. As the personal POI transition graph is usually
sparse and sensitive to noise, we design a Bayes-enhanced spatial dependency
learning module for data augmentation from the local view. A Bayesian posterior
guided graph augmentation approach is adopted to generate a new graph with
collaborative signals to increase the data diversity. Then both the original
and the augmented graphs are used for POI representation learning to counteract
the data uncertainty issue. Next, the POI representations of the three view
graphs are input into the proposed multi-view attention-based user preference
learning module. By incorporating the semantic and distance correlations of
POIs, the user preference can be effectively refined and finally robust
recommendation results are achieved. The results of extensive experiments show
that BayMAN significantly outperforms the state-of-the-art methods in POI
recommendation when the available check-ins are incomplete and noisy.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00500" title="Abstract">arXiv:2311.00500</a> [<a href="/pdf/2311.00500" title="Download PDF">pdf</a>, <a href="/format/2311.00500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intriguing Properties of Data Attribution on Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaosen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">Min Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Data attribution seeks to trace model outputs back to training data. With the
recent development of diffusion models, data attribution has become a desired
module to properly assign valuations for high-quality or copyrighted training
samples, ensuring that data contributors are fairly compensated or credited.
Several theoretically motivated methods have been proposed to implement data
attribution, in an effort to improve the trade-off between computational
scalability and effectiveness. In this work, we conduct extensive experiments
and ablation studies on attributing diffusion models, specifically focusing on
DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model
LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive
observations that theoretically unjustified design choices for attribution
empirically outperform previous baselines by a large margin, in terms of both
linear datamodeling score and counterfactual evaluation. Our work presents a
significantly more efficient approach for attributing diffusion models, while
the unexpected findings suggest that at least in non-convex settings,
constructions guided by theoretical assumptions may lead to inferior
attribution performance. The code is available at
https://github.com/sail-sg/D-TRAK.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00502" title="Abstract">arXiv:2311.00502</a> [<a href="/pdf/2311.00502" title="Download PDF">pdf</a>, <a href="/format/2311.00502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient LLM Inference on CPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Haihao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Hanwen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+H">Hengyu Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS'2023 on Efficient Natural Language and Speech Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated remarkable performance and
tremendous potential across a wide range of tasks. However, deploying these
models has been challenging due to the astronomical amount of model parameters,
which requires a demand for large memory capacity and high memory bandwidth. In
this paper, we propose an effective approach that can make the deployment of
LLMs more efficiently. We support an automatic INT4 weight-only quantization
flow and design a special LLM runtime with highly-optimized kernels to
accelerate the LLM inference on CPUs. We demonstrate the general applicability
of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase
the extreme inference efficiency on CPUs. The code is publicly available at:
https://github.com/intel/intel-extension-for-transformers.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00506" title="Abstract">arXiv:2311.00506</a> [<a href="/pdf/2311.00506" title="Download PDF">pdf</a>, <a href="/format/2311.00506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experimental Validation of a Grid-Aware Optimal Control of Hybrid AC/DC  Microgrids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lambrichts%2C+W">Willem Lambrichts</a>, 
<a href="/search/eess?searchtype=author&query=Mace%2C+J">Jules Mace</a>, 
<a href="/search/eess?searchtype=author&query=Paolone%2C+M">Mario Paolone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, submitted to PSCC2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents the experimental validation of a grid-aware real-time
control method for hybrid AC/DC microgrids. The optimal control is leveraged by
the voltage sensitivity coefficients (SC) that are computed analytically using
the close-form expression proposed in the authors' previous work. The SCs are
based on the unified power flow model for hybrid AC/DC grids that accounts for
the AC grid, DC grid, and the Interfacing Converters (IC), which can operate in
different control modes, e.g. voltage or power control. The SCs are used to
express the grid constraints in the optimal control problem in a fully linear
way and, therefore, allow for second- to subsecond control actions. The
validation of the model is performed on the hybrid AC/DC grid, available at the
EPFL. The network consists of 18 AC nodes, 8 DC nodes, and 4 converters to
interface the AC and DC network. The network hosts multiple controllable and
uncontrollable resources. The SC-based optimal control is validated in a
generic experiment. It is shown that the real-time control is able to control
the ICs optimally to redirect power through the DC grid, to avoid grid
constraint violations while providing reactive power support to the upper layer
AC grid. Furthermore, the computational time of the optimal control is analysed
to validate its application in critical real-time applications.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00508" title="Abstract">arXiv:2311.00508</a> [<a href="/pdf/2311.00508" title="Download PDF">pdf</a>, <a href="/format/2311.00508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness Tests for Automatic Machine Translation Metrics with  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yichen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We investigate MT evaluation metric performance on adversarially-synthesized
texts, to shed light on metric robustness. We experiment with word- and
character-level attacks on three popular machine translation metrics:
BERTScore, BLEURT, and COMET. Our human experiments validate that automatic
metrics tend to overpenalize adversarially-degraded translations. We also
identify inconsistencies in BERTScore ratings, where it judges the original
sentence and the adversarially-degraded one as similar, while judging the
degraded translation as notably worse than the original with respect to the
reference. We identify patterns of brittleness that motivate more robust metric
development.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00513" title="Abstract">arXiv:2311.00513</a> [<a href="/pdf/2311.00513" title="Download PDF">pdf</a>, <a href="/format/2311.00513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rule-Based Error Classification for Analyzing Differences in Frequent  Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shirafuji%2C+A">Atsushi Shirafuji</a>, 
<a href="/search/cs?searchtype=author&query=Matsumoto%2C+T">Taku Matsumoto</a>, 
<a href="/search/cs?searchtype=author&query=Amin%2C+M+F+I">Md Faizul Ibne Amin</a>, 
<a href="/search/cs?searchtype=author&query=Watanobe%2C+Y">Yutaka Watanobe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, accepted to TALE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
<p class="mathjax">Finding and fixing errors is a time-consuming task not only for novice
programmers but also for expert programmers. Prior work has identified frequent
error patterns among various levels of programmers. However, the differences in
the tendencies between novices and experts have yet to be revealed. From the
knowledge of the frequent errors in each level of programmers, instructors will
be able to provide helpful advice for each level of learners. In this paper, we
propose a rule-based error classification tool to classify errors in code pairs
consisting of wrong and correct programs. We classify errors for 95,631 code
pairs and identify 3.47 errors on average, which are submitted by various
levels of programmers on an online judge system. The classified errors are used
to analyze the differences in frequent errors between novice and expert
programmers. The analyzed results show that, as for the same introductory
problems, errors made by novices are due to the lack of knowledge in
programming, and the mistakes are considered an essential part of the learning
process. On the other hand, errors made by experts are due to misunderstandings
caused by the carelessness of reading problems or the challenges of solving
problems differently than usual. The proposed tool can be used to create
error-labeled datasets and for further code-related educational research.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00514" title="Abstract">arXiv:2311.00514</a> [<a href="/pdf/2311.00514" title="Download PDF">pdf</a>, <a href="/format/2311.00514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Hard Is Squash? -- Towards Information Theoretic Analysis of Motor  Behavior in Squash
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anand%2C+K">Kavya Anand</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Pramit Saha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Fitts' law has been widely employed as a research method for analyzing tasks
within the domain of Human-Computer Interaction (HCI). However, its application
to non-computer tasks has remained limited. This study aims to extend the
application of Fitts' law to the realm of sports, specifically focusing on
squash. Squash is a high-intensity sport that requires quick movements and
precise shots. Our research investigates the effectiveness of utilizing Fitts'
law to evaluate the task difficulty and effort level associated with executing
and responding to various squash shots. By understanding the effort/information
rate required for each shot, we can determine which shots are more effective in
making the opponent work harder. Additionally, this knowledge can be valuable
for coaches in designing training programs. However, since Fitts' law was
primarily developed for human-computer interaction, we adapted it to fit the
squash scenario. This paper provides an overview of Fitts' law and its
relevance to sports, elucidates the motivation driving this investigation,
outlines the methodology employed to explore this novel avenue, and presents
the obtained results, concluding with key insights. We conducted experiments
with different shots and players, collecting data on shot speed, player
movement time, and distance traveled. Using this data, we formulated a modified
version of Fitts' law specifically for squash. The results provide insights
into the difficulty and effectiveness of various shots, offering valuable
information for both players and coaches in the sport of squash.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00517" title="Abstract">arXiv:2311.00517</a> [<a href="/pdf/2311.00517" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Cardiovascular Disease Prediction Through Comparative Analysis  of Machine Learning Models: A Case Study on Myocardial Infarction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miah%2C+J">Jonayet Miah</a>, 
<a href="/search/cs?searchtype=author&query=Ca%2C+D+M">Duc M Ca</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+M+A">Md Abu Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Lipu%2C+E+R">Ehsanur Rashid Lipu</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+F">Fuad Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Arafat%2C+S+M+Y">S M Yasir Arafat</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 15th International Conference on Innovations in Information
  Technology (IIT) - Track 2: Artificial Intelligence in Data Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Cardiovascular disease remains a leading cause of mortality in the
contemporary world. Its association with smoking, elevated blood pressure, and
cholesterol levels underscores the significance of these risk factors. This
study addresses the challenge of predicting myocardial illness, a formidable
task in medical research. Accurate predictions are pivotal for refining
healthcare strategies. This investigation conducts a comparative analysis of
six distinct machine learning models: Logistic Regression, Support Vector
Machine, Decision Tree, Bagging, XGBoost, and LightGBM. The attained outcomes
exhibit promise, with accuracy rates as follows: Logistic Regression (81.00%),
Support Vector Machine (75.01%), XGBoost (92.72%), LightGBM (90.60%), Decision
Tree (82.30%), and Bagging (83.01%). Notably, XGBoost emerges as the
top-performing model. These findings underscore its potential to enhance
predictive precision for coronary infarction. As the prevalence of
cardiovascular risk factors persists, incorporating advanced machine learning
techniques holds the potential to refine proactive medical interventions.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00519" title="Abstract">arXiv:2311.00519</a> [<a href="/pdf/2311.00519" title="Download PDF">pdf</a>, <a href="/format/2311.00519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Based Reconstruction For Time-series Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M+A">Maxwell A. Xu</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+A">Alexander Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hui Wei</a>, 
<a href="/search/cs?searchtype=author&query=Marlin%2C+B+M">Benjamin M. Marlin</a>, 
<a href="/search/cs?searchtype=author&query=Rehg%2C+J+M">James M. Rehg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The success of self-supervised contrastive learning hinges on identifying
positive data pairs that, when pushed together in embedding space, encode
useful information for subsequent downstream tasks. However, in time-series,
this is challenging because creating positive pairs via augmentations may break
the original semantic meaning. We hypothesize that if we can retrieve
information from one subsequence to successfully reconstruct another
subsequence, then they should form a positive pair. Harnessing this intuition,
we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR)
contrastive learning. First, we utilize a convolutional cross-attention
architecture to calculate the REBAR error between two different time-series.
Then, through validation experiments, we show that the REBAR error is a
predictor of mutual class membership, justifying its usage as a
positive/negative labeler. Finally, once integrated into a contrastive learning
framework, our REBAR method can learn an embedding that achieves
state-of-the-art performance on downstream tasks across various modalities.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00522" title="Abstract">arXiv:2311.00522</a> [<a href="/pdf/2311.00522" title="Download PDF">pdf</a>, <a href="/format/2311.00522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Rendering Strategies for Pixel Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lotz%2C+J+F">Jonas F. Lotz</a>, 
<a href="/search/cs?searchtype=author&query=Salesky%2C+E">Elizabeth Salesky</a>, 
<a href="/search/cs?searchtype=author&query=Rust%2C+P">Phillip Rust</a>, 
<a href="/search/cs?searchtype=author&query=Elliott%2C+D">Desmond Elliott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pixel-based language models process text rendered as images, which allows
them to handle any script, making them a promising approach to open vocabulary
language modelling. However, recent approaches use text renderers that produce
a large set of almost-equivalent input patches, which may prove sub-optimal for
downstream tasks, due to redundancy in the input representations. In this
paper, we investigate four approaches to rendering text in the PIXEL model
(Rust et al., 2023), and find that simple character bigram rendering brings
improved performance on sentence-level tasks without compromising performance
on token-level or multilingual tasks. This new rendering strategy also makes it
possible to train a more compact model with only 22M parameters that performs
on par with the original 86M parameter model. Our analyses show that character
bigram rendering leads to a consistently better model but with an anisotropic
patch embedding space, driven by a patch frequency bias, highlighting the
connections between image patch- and tokenization-based language models.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00523" title="Abstract">arXiv:2311.00523</a> [<a href="/pdf/2311.00523" title="Download PDF">pdf</a>, <a href="/format/2311.00523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning impartial policies for sequential counterfactual explanations  using Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panagiotou%2C+E">E. Panagiotou</a>, 
<a href="/search/cs?searchtype=author&query=Ntoutsi%2C+E">E. Ntoutsi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the ECML PKDD 2023 Workshop: Explainable Artificial Intelligence From Static to Dynamic
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the field of explainable Artificial Intelligence (XAI), sequential
counterfactual (SCF) examples are often used to alter the decision of a trained
classifier by implementing a sequence of modifications to the input instance.
Although certain test-time algorithms aim to optimize for each new instance
individually, recently Reinforcement Learning (RL) methods have been proposed
that seek to learn policies for discovering SCFs, thereby enhancing
scalability. As is typical in RL, the formulation of the RL problem, including
the specification of state space, actions, and rewards, can often be ambiguous.
In this work, we identify shortcomings in existing methods that can result in
policies with undesired properties, such as a bias towards specific actions. We
propose to use the output probabilities of the classifier to create a more
informative reward, to mitigate this effect.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00527" title="Abstract">arXiv:2311.00527</a> [<a href="/pdf/2311.00527" title="Download PDF">pdf</a>, <a href="/format/2311.00527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Leakage-based Method for Mitigation of Faulty Reconfigurable  Intelligent Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gholian%2C+N+M">N. Moghadas Gholian</a>, 
<a href="/search/cs?searchtype=author&query=Rossanese%2C+M">M. Rossanese</a>, 
<a href="/search/cs?searchtype=author&query=Mursia%2C+P">P. Mursia</a>, 
<a href="/search/cs?searchtype=author&query=Garcia-Saavedra%2C+A">A. Garcia-Saavedra</a>, 
<a href="/search/cs?searchtype=author&query=Asadi%2C+A">A. Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Sciancalepore%2C+V">V. Sciancalepore</a>, 
<a href="/search/cs?searchtype=author&query=Costa-P%C3%A9rez%2C+X">X. Costa-P&#xe9;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE GLOBECOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable Intelligent Surfaces (RISs) are expected to be massively
deployed in future beyond-5th generation wireless networks, thanks to their
ability to programmatically alter the propagation environment, inherent
low-cost and low-maintenance nature. Indeed, they are envisioned to be
implemented on the facades of buildings or on moving objects. However, such an
innovative characteristic may potentially turn into an involuntary negative
behavior that needs to be addressed: an undesired signal scattering. In
particular, RIS elements may be prone to experience failures due to lack of
proper maintenance or external environmental factors. While the resulting
Signal-to-Noise-Ratio (SNR) at the intended User Equipment (UE) may not be
significantly degraded, we demonstrate the potential risks in terms of unwanted
spreading of the transmit signal to non-intended UE. In this regard, we
consider the problem of mitigating such undesired effect by proposing two
simple yet effective algorithms, which are based on maximizing the
Signal-to-Leakage- and-Noise-Ratio (SLNR) over a predefined two-dimensional
(2D) area and are applicable in the case of perfect channel-state-information
(CSI) and partial CSI, respectively. Numerical and full-wave simulations
demonstrate the added gains compared to leakage-unaware and reference schemes.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00529" title="Abstract">arXiv:2311.00529</a> [<a href="/pdf/2311.00529" title="Download PDF">pdf</a>, <a href="/ps/2311.00529" title="Download PostScript">ps</a>, <a href="/format/2311.00529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework for the Error Analysis of Physics-Informed Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zeinhofer%2C+M">Marius Zeinhofer</a>, 
<a href="/search/math?searchtype=author&query=Masri%2C+R">Rami Masri</a>, 
<a href="/search/math?searchtype=author&query=Mardal%2C+K">Kent-Andr&#xe9; Mardal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We prove a priori and a posteriori error estimates, also known as the
generalization error in the machine learning community, for physics-informed
neural networks (PINNs) for linear PDEs. We analyze elliptic equations in
primal and mixed form, elasticity, parabolic, hyperbolic and Stokes equations;
and a PDE constrained optimization problem. For the analysis, we propose an
abstract framework in the common language of bilinear forms, and we show that
coercivity and continuity lead to error estimates. Our results give insight
into the potential of neural networks for high dimensional PDEs and into the
benefit of encoding constraints directly in the ansatz class. The provided
estimates are -- apart from the Poisson equation -- the first results of
best-approximation and a posteriori error-control type. Finally, utilizing
recent advances in PINN optimization, we present numerical examples that
illustrate the ability of the method to achieve accurate solutions.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00530" title="Abstract">arXiv:2311.00530</a> [<a href="/pdf/2311.00530" title="Download PDF">pdf</a>, <a href="/format/2311.00530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Development of LLMs for Embodied Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jinzhou Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Han Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rongtao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Li Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shibiao Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In recent years, the rapid advancement of Large Language Models (LLMs) such
as the Generative Pre-trained Transformer (GPT) has attracted increasing
attention due to their potential in a variety of practical applications. The
application of LLMs with Embodied Intelligence has emerged as a significant
area of focus. Among the myriad applications of LLMs, navigation tasks are
particularly noteworthy because they demand a deep understanding of the
environment and quick, accurate decision-making. LLMs can augment embodied
intelligence systems with sophisticated environmental perception and
decision-making support, leveraging their robust language and image-processing
capabilities. This article offers an exhaustive summary of the symbiosis
between LLMs and embodied intelligence with a focus on navigation. It reviews
state-of-the-art models, research methodologies, and assesses the advantages
and disadvantages of existing embodied navigation models and datasets. Finally,
the article elucidates the role of LLMs in embodied intelligence, based on
current research, and forecasts future directions in the field. A comprehensive
list of studies in this survey is available at
https://github.com/Rongtao-Xu/Awesome-LLM-EN
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00533" title="Abstract">arXiv:2311.00533</a> [<a href="/pdf/2311.00533" title="Download PDF">pdf</a>, <a href="/format/2311.00533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Eclipse Layout Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Domr%C3%B6s%2C+S">S&#xf6;ren Domr&#xf6;s</a>, 
<a href="/search/cs?searchtype=author&query=von+Hanxleden%2C+R">Reinhard von Hanxleden</a>, 
<a href="/search/cs?searchtype=author&query=Sp%C3%B6nemann%2C+M">Miro Sp&#xf6;nemann</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%BCegg%2C+U">Ulf R&#xfc;egg</a>, 
<a href="/search/cs?searchtype=author&query=Schulze%2C+C+D">Christoph Daniel Schulze</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The Eclipse Layout Kernel (ELK) is a collection of graph drawing algorithms
that supports compound graph layout and ports as explicit anchor points of
edges. It is available as open-source library under an EPL license. Since its
beginning, ELK has served both as a research vehicle for graph drawing
algorithms, and as a practical tool for solving real-world problems. ELK and
its transpiled JavaScript cousin elkjs are now included in numerous academic
and commercial projects.
<br />Most of the algorithms realized in ELK are described in a series of
publications. In this paper, the technical description concentrates on the key
features of the flag-ship algorithm ELK Layered, the algorithm architecture,
and usage. However, the main purpose of this paper is to give the broader view
that is typically left unpublished. Specifically, we review its history, give a
brief overview of technical papers, discuss lessons learned over the past
fifteen years, and present example usages. Finally, we reflect on potential
threats to open-source graph drawing libraries.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00534" title="Abstract">arXiv:2311.00534</a> [<a href="/pdf/2311.00534" title="Download PDF">pdf</a>, <a href="/format/2311.00534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error analysis for a finite element approximation of the steady  $p(\cdot)$-Navier-Stokes equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berselli%2C+L+C">Luigi C. Berselli</a>, 
<a href="/search/math?searchtype=author&query=Kaltenbach%2C+A">Alex Kaltenbach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we examine a finite element approximation of the steady
$p(\cdot)$-Navier-Stokes equations ($p(\cdot)$ is variable dependent) and prove
orders of convergence by assuming natural fractional regularity assumptions on
the velocity vector field and the kinematic pressure. Compared to previous
results, we treat the convective term and employ a more practicable
discretization of the power-law index $p(\cdot)$. Numerical experiments confirm
the quasi-optimality of the a priori error estimates (for the velocity) with
respect to fractional regularity assumptions on the velocity vector field and
the kinematic pressure.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00535" title="Abstract">arXiv:2311.00535</a> [<a href="/pdf/2311.00535" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Noise Control Portable Device Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+k">kai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuanyuan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">While our world is filled with its own natural sounds that we can't resist
enjoying, it is also chock-full of other sounds that can be irritating, this is
noise. Noise not only influences the working efficiency but also the human's
health. The problem of reducing noise is one of great importance and great
difficulty. The problem has been addressed in many ways over the years. The
current methods for noise reducing mostly rely on the materials and
transmission medium, which are only effective to some extent for the high
frequency noise. However, the effective reduction noise method especially for
low frequency noise is very limited.
<br />Here we come up with a noise reduction system consist of a sensor to detect
the noise in the environment. Then the noise will be sent to an electronic
control system to process the noise, which will generate a reverse phase
frequency signal to counteract the disturbance. Finally, the processed smaller
noise will be broadcasted by the speaker. Through this smart noise reduction
system, even the noise with low-frequency can be eliminated.
<br />The system is also integrated with sleep tracking and music player
applications. It can also remember and store settings for the same environment,
sense temperature, and smart control of home furniture, fire alarm, etc. This
smart system can transfer data easily by Wi-Fi or Bluetooth and controlled by
its APP.
<br />In this project, we will present a model of the above technology which can be
used in various environments to prevent noise pollution and provide a solution
to the people who have difficulties finding a peaceful and quiet environment
for sleep, work or study.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00541" title="Abstract">arXiv:2311.00541</a> [<a href="/pdf/2311.00541" title="Download PDF">pdf</a>, <a href="/format/2311.00541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Embedded Diachronic Sense Change Model with a Case Study from Ancient  Greek
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zafar%2C+S">Schyan Zafar</a>, 
<a href="/search/cs?searchtype=author&query=Nicholls%2C+G+K">Geoff K. Nicholls</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Word meanings change over time, and word senses evolve, emerge or die out in
the process. For ancient languages, where the corpora are often small, sparse
and noisy, modelling such changes accurately proves challenging, and
quantifying uncertainty in sense-change estimates consequently becomes
important. GASC and DiSC are existing generative models that have been used to
analyse sense change for target words from an ancient Greek text corpus, using
unsupervised learning without the help of any pre-training. These models
represent the senses of a given target word such as "kosmos" (meaning
decoration, order or world) as distributions over context words, and sense
prevalence as a distribution over senses. The models are fitted using MCMC
methods to measure temporal changes in these representations. In this paper, we
introduce EDiSC, an embedded version of DiSC, which combines word embeddings
with DiSC to provide superior model performance. We show empirically that EDiSC
offers improved predictive accuracy, ground-truth recovery and uncertainty
quantification, as well as better sampling efficiency and scalability
properties with MCMC methods. We also discuss the challenges of fitting these
models.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00545" title="Abstract">arXiv:2311.00545</a> [<a href="/pdf/2311.00545" title="Download PDF">pdf</a>, <a href="/format/2311.00545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric  Models and the MDL Principle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferr%C3%A9%2C+S">S&#xe9;bastien Ferr&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The Abstraction and Reasoning Corpus (ARC) is a challenging benchmark,
introduced to foster AI research towards human-level intelligence. It is a
collection of unique tasks about generating colored grids, specified by a few
examples only. In contrast to the transformation-based programs of existing
work, we introduce object-centric models that are in line with the natural
programs produced by humans. Our models can not only perform predictions, but
also provide joint descriptions for input/output pairs. The Minimum Description
Length (MDL) principle is used to efficiently search the large model space. A
diverse range of tasks are solved, and the learned models are similar to the
natural programs. We demonstrate the generality of our approach by applying it
to a different domain.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00548" title="Abstract">arXiv:2311.00548</a> [<a href="/pdf/2311.00548" title="Download PDF">pdf</a>, <a href="/format/2311.00548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual atlas-based segmentation of prostate MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranem%2C+A">Amin Ranem</a>, 
<a href="/search/cs?searchtype=author&query=Gon%C3%A1zlez%2C+C">Camila Gon&#xe1;zlez</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+D+P+d">Daniel Pinto dos Santos</a>, 
<a href="/search/cs?searchtype=author&query=Bucher%2C+A+M">Andreas Michael Bucher</a>, 
<a href="/search/cs?searchtype=author&query=Othman%2C+A+E">Ahmed Ezzat Othman</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+A">Anirban Mukhopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Continual learning (CL) methods designed for natural image classification
often fail to reach basic quality standards for medical image segmentation.
Atlas-based segmentation, a well-established approach in medical imaging,
incorporates domain knowledge on the region of interest, leading to
semantically coherent predictions. This is especially promising for CL, as it
allows us to leverage structural information and strike an optimal balance
between model rigidity and plasticity over time. When combined with
privacy-preserving prototypes, this process offers the advantages of
rehearsal-based CL without compromising patient privacy. We propose Atlas
Replay, an atlas-based segmentation approach that uses prototypes to generate
high-quality segmentation masks through image registration that maintain
consistency even as the training distribution changes. We explore how our
proposed method performs compared to state-of-the-art CL methods in terms of
knowledge transferability across seven publicly available prostate segmentation
datasets. Prostate segmentation plays a vital role in diagnosing prostate
cancer, however, it poses challenges due to substantial anatomical variations,
benign structural differences in older age groups, and fluctuating acquisition
parameters. Our results show that Atlas Replay is both robust and generalizes
well to yet-unseen domains while being able to maintain knowledge, unlike
end-to-end segmentation methods. Our code base is available under
https://github.com/MECLabTUDA/Atlas-Replay.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00551" title="Abstract">arXiv:2311.00551</a> [<a href="/pdf/2311.00551" title="Download PDF">pdf</a>, <a href="/format/2311.00551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalised DePIN Protocol: A Framework for Decentralized Physical  Infrastructure Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+D">Dipankar Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper introduces the Generalised DePIN (GDP) protocol, a comprehensive
framework for decentralized physical infrastructure networks. GDP establishes a
modular system, enabling tailored application across sectors like ridesharing
and power systems. Leveraging device onboarding, multi-sensor redundancy, and a
reward/penalty mechanism, GDP promotes genuine behavior and ensures
network-wide vigilance. Through continuous audits and updates, the protocol
remains dynamic, ensuring sustainable decentralized operations.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00554" title="Abstract">arXiv:2311.00554</a> [<a href="/pdf/2311.00554" title="Download PDF">pdf</a>, <a href="/format/2311.00554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A higher order numerical method for singularly perturbed elliptic  problems with characteristic boundary layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hegarty%2C+A+F">Alan F. Hegarty</a>, 
<a href="/search/math?searchtype=author&query=O%27Riordan%2C+E">Eugene O&#x27;Riordan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A Petrov-Galerkin finite element method is constructed for a singularly
perturbed elliptic problem in two space dimensions. The solution contains a
regular boundary layer and two characteristic boundary layers. Exponential
splines are used as test functions in one coordinate direction and are combined
with bilinear trial functions defined on a Shishkin mesh. The resulting
numerical method is shown to be a stable parameter-uniform numerical method
that achieves a higher order of convergence compared to upwinding on the same
mesh.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00556" title="Abstract">arXiv:2311.00556</a> [<a href="/pdf/2311.00556" title="Download PDF">pdf</a>, <a href="/format/2311.00556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jieming Cui</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Ziren Gong</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+B">Baoxiong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zilong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianzhu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yixin Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The challenge of replicating research results has posed a significant
impediment to the field of molecular biology. The advent of modern intelligent
systems has led to notable progress in various domains. Consequently, we
embarked on an investigation of intelligent monitoring systems as a means of
tackling the issue of the reproducibility crisis. Specifically, we first curate
a comprehensive multimodal dataset, named ProBio, as an initial step towards
this objective. This dataset comprises fine-grained hierarchical annotations
intended for the purpose of studying activity understanding in BioLab. Next, we
devise two challenging benchmarks, transparent solution tracking and multimodal
action recognition, to emphasize the unique characteristics and difficulties
associated with activity understanding in BioLab settings. Finally, we provide
a thorough experimental evaluation of contemporary video understanding models
and highlight their limitations in this specialized domain to identify
potential avenues for future research. We hope ProBio with associated
benchmarks may garner increased focus on modern AI techniques in the realm of
molecular biology.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00558" title="Abstract">arXiv:2311.00558</a> [<a href="/pdf/2311.00558" title="Download PDF">pdf</a>, <a href="/format/2311.00558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Exponential Lower Bound for Linear 3-Query Locally Correctable Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kothari%2C+P+K">Pravesh K. Kothari</a>, 
<a href="/search/cs?searchtype=author&query=Manohar%2C+P">Peter Manohar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We prove that the blocklength $n$ of a linear $3$-query locally correctable
code (LCC) $\mathcal{L} \colon {\mathbb F}^k \to {\mathbb F}^n$ with distance
$\delta$ must be at least $n \geq 2^{\Omega\left(\left(\frac{\delta^2
k}{(|{\mathbb F}|-1)^2}\right)^{1/8}\right)}$. In particular, the blocklength
of a linear $3$-query LCC with constant distance over any small field grows
exponentially with $k$. This improves on the best prior lower bound of $n \geq
\tilde{\Omega}(k^3)$ [AGKM23], which holds even for the weaker setting of
$3$-query locally decodable codes (LDCs), and comes close to matching the
best-known construction of $3$-query LCCs based on binary Reed-Muller codes,
which achieve $n \leq 2^{O(k^{1/2})}$. Because there is a $3$-query LDC with a
strictly subexponential blocklength [Yek08, Efr09], as a corollary we obtain
the first strong separation between $q$-query LCCs and LDCs for any constant $q
\geq 3$.
<br />Our proof is based on a new upgrade of the method of spectral refutations via
Kikuchi matrices developed in recent works [GKM22, HKM23, AGKM23] that reduces
establishing (non-)existence of combinatorial objects to proving
unsatisfiability of associated XOR instances. Our key conceptual idea is to
apply this method with XOR instances obtained via long-chain derivations, a
structured variant of low-width resolution for XOR formulas from proof
complexity [Gri01, Sch08].
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00559" title="Abstract">arXiv:2311.00559</a> [<a href="/pdf/2311.00559" title="Download PDF">pdf</a>, <a href="/ps/2311.00559" title="Download PostScript">ps</a>, <a href="/format/2311.00559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to optimize by multi-gradient for multi-objective optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linxi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinmin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Liping Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The development of artificial intelligence (AI) for science has led to the
emergence of learning-based research paradigms, necessitating a compelling
reevaluation of the design of multi-objective optimization (MOO) methods. The
new generation MOO methods should be rooted in automated learning rather than
manual design. In this paper, we introduce a new automatic learning paradigm
for optimizing MOO problems, and propose a multi-gradient learning to optimize
(ML2O) method, which automatically learns a generator (or mappings) from
multiple gradients to update directions. As a learning-based method, ML2O
acquires knowledge of local landscapes by leveraging information from the
current step and incorporates global experience extracted from historical
iteration trajectory data. By introducing a new guarding mechanism, we propose
a guarded multi-gradient learning to optimize (GML2O) method, and prove that
the iterative sequence generated by GML2O converges to a Pareto critical point.
The experimental results demonstrate that our learned optimizer outperforms
hand-designed competitors on training multi-task learning (MTL) neural network.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00560" title="Abstract">arXiv:2311.00560</a> [<a href="/pdf/2311.00560" title="Download PDF">pdf</a>, <a href="/format/2311.00560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Experimentation and Human Factors An Exploratory Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muhammad%2C+A+P">Amna Pir Muhammad</a>, 
<a href="/search/cs?searchtype=author&query=Knauss%2C+E">Eric Knauss</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4rgman%2C+J">Jonas B&#xe4;rgman</a>, 
<a href="/search/cs?searchtype=author&query=Knauss%2C+A">Alessia Knauss</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Conference Publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In todays rapidly evolving technological landscape, the success of tools and
systems relies heavily on their ability to meet the needs and expectations of
users. User-centered design approaches, with a focus on human factors, have
gained increasing attention as they prioritize the human element in the
development process. With the increasing complexity of software-based systems,
companies are adopting agile development methodologies and emphasizing
continuous software experimentation. However, there is limited knowledge on how
to effectively execute continuous experimentation with respect to human factors
within this context. This research paper presents an exploratory qualitative
study for integrating human factors in continuous experimentation, aiming to
uncover distinctive characteristics of human factors and continuous software
experiments, practical challenges for integrating human factors in continuous
software experiments, and best practices associated with the management of
continuous human factors experimentation.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00562" title="Abstract">arXiv:2311.00562</a> [<a href="/pdf/2311.00562" title="Download PDF">pdf</a>, <a href="/format/2311.00562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MNN: Mixed Nearest-Neighbors for Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Chen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xianzhong Long</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, source code and pretrained models are available <a href="https://github.com/pc-cp/MNN">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In contrastive self-supervised learning, positive samples are typically drawn
from the same image but in different augmented views, resulting in a relatively
limited source of positive samples. An effective way to alleviate this problem
is to incorporate the relationship between samples, which involves including
the top-k nearest neighbors of positive samples in the framework. However, the
problem of false neighbors (i.e., neighbors that do not belong to the same
category as the positive sample) is an objective but often overlooked challenge
due to the query of neighbor samples without human supervision. In this paper,
we present a simple Self-supervised learning framework called Mixed
Nearest-Neighbors for Self-Supervised Learning (MNN). MNN optimizes the
influence of neighbor samples on the semantics of positive samples through an
intuitive weighting approach and image mixture operations. The results of our
study demonstrate that MNN exhibits exceptional generalization performance and
training efficiency on four benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00565" title="Abstract">arXiv:2311.00565</a> [<a href="/pdf/2311.00565" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Visual Cues in the Intensive Care Unit and Association with  Patient Clinical Status
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nerella%2C+S">Subhash Nerella</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Ziyuan Guan</a>, 
<a href="/search/cs?searchtype=author&query=Davidson%2C+A">Andrea Davidson</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuanfang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Baslanti%2C+T">Tezcan Baslanti</a>, 
<a href="/search/cs?searchtype=author&query=Armfield%2C+B">Brooke Armfield</a>, 
<a href="/search/cs?searchtype=author&query=Tighe%2C+P">Patrick Tighe</a>, 
<a href="/search/cs?searchtype=author&query=Bihorac%2C+A">Azra Bihorac</a>, 
<a href="/search/cs?searchtype=author&query=Rashidi%2C+P">Parisa Rashidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Intensive Care Units (ICU) provide close supervision and continuous care to
patients with life-threatening conditions. However, continuous patient
assessment in the ICU is still limited due to time constraints and the workload
on healthcare providers. Existing patient assessments in the ICU such as pain
or mobility assessment are mostly sporadic and administered manually, thus
introducing the potential for human errors. Developing Artificial intelligence
(AI) tools that can augment human assessments in the ICU can be beneficial for
providing more objective and granular monitoring capabilities. For example,
capturing the variations in a patient's facial cues related to pain or
agitation can help in adjusting pain-related medications or detecting
agitation-inducing conditions such as delirium. Additionally, subtle changes in
visual cues during or prior to adverse clinical events could potentially aid in
continuous patient monitoring when combined with high-resolution physiological
signals and Electronic Health Record (EHR) data. In this paper, we examined the
association between visual cues and patient condition including acuity status,
acute brain dysfunction, and pain. We leveraged our AU-ICU dataset with 107,064
frames collected in the ICU annotated with facial action units (AUs) labels by
trained annotators. We developed a new "masked loss computation" technique that
addresses the data imbalance problem by maximizing data resource utilization.
We trained the model using our AU-ICU dataset in conjunction with three
external datasets to detect 18 AUs. The SWIN Transformer model achieved 0.57
mean F1-score and 0.89 mean accuracy on the test set. Additionally, we
performed AU inference on 634,054 frames to evaluate the association between
facial AUs and clinically important patient conditions such as acuity status,
acute brain dysfunction, and pain.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00566" title="Abstract">arXiv:2311.00566</a> [<a href="/pdf/2311.00566" title="Download PDF">pdf</a>, <a href="/format/2311.00566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CROMA: Remote Sensing Representations with Contrastive Radar-Optical  Masked Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fuller%2C+A">Anthony Fuller</a>, 
<a href="/search/cs?searchtype=author&query=Millard%2C+K">Koreen Millard</a>, 
<a href="/search/cs?searchtype=author&query=Green%2C+J+R">James R. Green</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Camera Ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A vital and rapidly growing application, remote sensing offers vast yet
sparsely labeled, spatially aligned multimodal data; this makes self-supervised
learning algorithms invaluable. We present CROMA: a framework that combines
contrastive and reconstruction self-supervised objectives to learn rich
unimodal and multimodal representations. Our method separately encodes
masked-out multispectral optical and synthetic aperture radar samples --
aligned in space and time -- and performs cross-modal contrastive learning.
Another encoder fuses these sensors, producing joint multimodal encodings that
are used to predict the masked patches via a lightweight decoder. We show that
these objectives are complementary when leveraged on spatially aligned
multimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our
cross- and self-attention matrices. These strategies improve representations
and allow our models to effectively extrapolate to images up to 17.6x larger at
test-time. CROMA outperforms the current SoTA multispectral model, evaluated
on: four classification benchmarks -- finetuning (avg. 1.8%), linear (avg.
2.4%) and nonlinear (avg. 1.4%) probing, kNN classification (avg. 3.5%), and
K-means clustering (avg. 8.4%); and three segmentation benchmarks (avg. 6.4%).
CROMA's rich, optionally multimodal representations can be widely leveraged
across remote sensing applications.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00571" title="Abstract">arXiv:2311.00571</a> [<a href="/pdf/2311.00571" title="Download PDF">pdf</a>, <a href="/format/2311.00571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation,  Generation and Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Ge Chen</a>, 
<a href="/search/cs?searchtype=author&query=Spiridonova%2C+I">Irina Spiridonova</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chunyuan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 22 figures, 30M PDF file size; Project Page: <a href="https://llava-vl.github.io/llava-interactive/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Multimedia (cs.MM)

</div>
<p class="mathjax">LLaVA-Interactive is a research prototype for multimodal human-AI
interaction. The system can have multi-turn dialogues with human users by
taking multimodal user inputs and generating multimodal responses. Importantly,
LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled
to align human intents in the interaction. The development of LLaVA-Interactive
is extremely cost-efficient as the system combines three multimodal skills of
pre-built AI models without additional model training: visual chat of LLaVA,
image segmentation from SEEM, as well as image generation and editing from
GLIGEN. A diverse set of application scenarios is presented to demonstrate the
promises of LLaVA-Interactive and to inspire future research in multimodal
interactive systems.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00578" title="Abstract">arXiv:2311.00578</a> [<a href="/pdf/2311.00578" title="Download PDF">pdf</a>, <a href="/format/2311.00578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer learning for improved generalizability in causal  physics-informed neural networks for beam simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+T">Taniya Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hongrui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nunez%2C+A">Alfredo Nunez</a>, 
<a href="/search/cs?searchtype=author&query=Dollevoet%2C+R">Rolf Dollevoet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper introduces a novel methodology for simulating the dynamics of
beams on elastic foundations. Specifically, Euler-Bernoulli and Timoshenko beam
models on the Winkler foundation are simulated using a transfer learning
approach within a causality-respecting physics-informed neural network (PINN)
framework. Conventional PINNs encounter challenges in handling large space-time
domains, even for problems with closed-form analytical solutions. A
causality-respecting PINN loss function is employed to overcome this
limitation, effectively capturing the underlying physics. However, it is
observed that the causality-respecting PINN lacks generalizability. We propose
using solutions to similar problems instead of training from scratch by
employing transfer learning while adhering to causality to accelerate
convergence and ensure accurate results across diverse scenarios. Numerical
experiments on the Euler-Bernoulli beam highlight the efficacy of the proposed
approach for various initial conditions, including those with noise in the
initial data. Furthermore, the potential of the proposed method is demonstrated
for the Timoshenko beam in an extended spatial and temporal domain. Several
comparisons suggest that the proposed method accurately captures the inherent
dynamics, outperforming the state-of-the-art physics-informed methods under
standard $L^2$-norm metric and accelerating convergence.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00579" title="Abstract">arXiv:2311.00579</a> [<a href="/pdf/2311.00579" title="Download PDF">pdf</a>, <a href="/format/2311.00579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based  Inference Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weerasena%2C+H">Hansika Weerasena</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+P">Prabhat Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Convolution Neural Networks (CNNs) are widely used in various domains. Recent
advances in dataflow-based CNN accelerators have enabled CNN inference in
resource-constrained edge devices. These dataflow accelerators utilize inherent
data reuse of convolution layers to process CNN models efficiently. Concealing
the architecture of CNN models is critical for privacy and security. This paper
evaluates memory-based side-channel information to recover CNN architectures
from dataflow-based CNN inference accelerators. The proposed attack exploits
spatial and temporal data reuse of the dataflow mapping on CNN accelerators and
architectural hints to recover the structure of CNN models. Experimental
results demonstrate that our proposed side-channel attack can recover the
structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00582" title="Abstract">arXiv:2311.00582</a> [<a href="/pdf/2311.00582" title="Download PDF">pdf</a>, <a href="/format/2311.00582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and  Value
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Young Wu</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+J">Jeremy McMahan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiding Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yudong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaojin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiaomin Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study the game modification problem, where a benevolent game designer or a
malevolent adversary modifies the reward function of a zero-sum Markov game so
that a target deterministic or stochastic policy profile becomes the unique
Markov perfect Nash equilibrium and has a value within a target range, in a way
that minimizes the modification cost. We characterize the set of policy
profiles that can be installed as the unique equilibrium of some game, and
establish sufficient and necessary conditions for successful installation. We
propose an efficient algorithm, which solves a convex optimization problem with
linear constraints and then performs random perturbation, to obtain a
modification plan with a near-optimal cost.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00586" title="Abstract">arXiv:2311.00586</a> [<a href="/pdf/2311.00586" title="Download PDF">pdf</a>, <a href="/format/2311.00586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAUMER: Patch Pausing Transformer for Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Courdier%2C+E">Evann Courdier</a>, 
<a href="/search/cs?searchtype=author&query=Sivaprasad%2C+P+T">Prabhu Teja Sivaprasad</a>, 
<a href="/search/cs?searchtype=author&query=Fleuret%2C+F">Fran&#xe7;ois Fleuret</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We study the problem of improving the efficiency of segmentation transformers
by using disparate amounts of computation for different parts of the image. Our
method, PAUMER, accomplishes this by pausing computation for patches that are
deemed to not need any more computation before the final decoder. We use the
entropy of predictions computed from intermediate activations as the pausing
criterion, and find this aligns well with semantics of the image. Our method
has a unique advantage that a single network trained with the proposed strategy
can be effortlessly adapted at inference to various run-time requirements by
modulating its pausing parameters. On two standard segmentation datasets,
Cityscapes and ADE20K, we show that our method operates with about a $50\%$
higher throughput with an mIoU drop of about $0.65\%$ and $4.6\%$ respectively.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00587" title="Abstract">arXiv:2311.00587</a> [<a href="/pdf/2311.00587" title="Download PDF">pdf</a>, <a href="/format/2311.00587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crosslingual Retrieval Augmented In-context Learning for Bangla
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoqian Li</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+E">Ercong Nie</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Sheng Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In The 1st Bangla Language Processing (BLP) Workshop, held in conjunction with The Conference on Empirical Methods in Natural Language Processing (EMNLP), December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The promise of Large Language Models (LLMs) in Natural Language Processing
has often been overshadowed by their limited performance in low-resource
languages such as Bangla. To address this, our paper presents a pioneering
approach that utilizes cross-lingual retrieval augmented in-context learning.
By strategically sourcing semantically similar prompts from high-resource
language, we enable multilingual pretrained language models (MPLMs), especially
the generative model BLOOMZ, to successfully boost performance on Bangla tasks.
Our extensive evaluation highlights that the cross-lingual retrieval augmented
prompts bring steady improvements to MPLMs over the zero-shot performance.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00588" title="Abstract">arXiv:2311.00588</a> [<a href="/pdf/2311.00588" title="Download PDF">pdf</a>, <a href="/format/2311.00588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Summarization with Normalizing Flows and Aggressive Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaotong Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents FlowSUM, a normalizing flows-based variational
encoder-decoder framework for Transformer-based summarization. Our approach
tackles two primary challenges in variational summarization: insufficient
semantic information in latent representations and posterior collapse during
training. To address these challenges, we employ normalizing flows to enable
flexible latent posterior modeling, and we propose a controlled alternate
aggressive training (CAAT) strategy with an improved gate mechanism.
Experimental results show that FlowSUM significantly enhances the quality of
generated summaries and unleashes the potential for knowledge distillation with
minimal impact on inference time. Furthermore, we investigate the issue of
posterior collapse in normalizing flows and analyze how the summary quality is
affected by the training strategy, gate initialization, and the type and number
of normalizing flows used, offering valuable insights for future research.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00591" title="Abstract">arXiv:2311.00591</a> [<a href="/pdf/2311.00591" title="Download PDF">pdf</a>, <a href="/format/2311.00591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coop: Memory is not a Commodity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shihan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peihong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jinhui Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Tensor rematerialization allows the training of deep neural networks (DNNs)
under limited memory budgets by checkpointing the models and recomputing the
evicted tensors as needed. However, the existing tensor rematerialization
techniques overlook the memory system in deep learning frameworks and
implicitly assume that free memory blocks at different addresses are identical.
Under this flawed assumption, discontiguous tensors are evicted, among which
some are not used to allocate the new tensor. This leads to severe memory
fragmentation and increases the cost of potential rematerializations. To
address this issue, we propose to evict tensors within a sliding window to
ensure all evictions are contiguous and are immediately used. Furthermore, we
proposed cheap tensor partitioning and recomputable in-place to further reduce
the rematerialization cost by optimizing the tensor allocation. We named our
method Coop as it is a co-optimization of tensor allocation and tensor
rematerialization. We evaluated Coop on eight representative DNNs. The
experimental results demonstrate that Coop achieves up to $2\times$ memory
saving and hugely reduces compute overhead, search latency, and memory
fragmentation compared to the state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00594" title="Abstract">arXiv:2311.00594</a> [<a href="/pdf/2311.00594" title="Download PDF">pdf</a>, <a href="/format/2311.00594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Variational Inference for Probabilistic Programs with  Stochastic Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reichelt%2C+T">Tim Reichelt</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+L">Luke Ong</a>, 
<a href="/search/cs?searchtype=author&query=Rainforth%2C+T">Tom Rainforth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Programming Languages (cs.PL)

</div>
<p class="mathjax">We introduce Support Decomposition Variational Inference (SDVI), a new
variational inference (VI) approach for probabilistic programs with stochastic
support. Existing approaches to this problem rely on designing a single global
variational guide on a variable-by-variable basis, while maintaining the
stochastic control flow of the original program. SDVI instead breaks the
program down into sub-programs with static support, before automatically
building separate sub-guides for each. This decomposition significantly aids in
the construction of suitable variational families, enabling, in turn,
substantial improvements in inference performance.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00599" title="Abstract">arXiv:2311.00599</a> [<a href="/pdf/2311.00599" title="Download PDF">pdf</a>, <a href="/format/2311.00599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure Learning with Adaptive Random Neighborhood Informed MCMC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Caron%2C+A">Alberto Caron</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xitong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Livingstone%2C+S">Samuel Livingstone</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+J">Jim Griffin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation (stat.CO)

</div>
<p class="mathjax">In this paper, we introduce a novel MCMC sampler, PARNI-DAG, for a
fully-Bayesian approach to the problem of structure learning under
observational data. Under the assumption of causal sufficiency, the algorithm
allows for approximate sampling directly from the posterior distribution on
Directed Acyclic Graphs (DAGs). PARNI-DAG performs efficient sampling of DAGs
via locally informed, adaptive random neighborhood proposal that results in
better mixing properties. In addition, to ensure better scalability with the
number of nodes, we couple PARNI-DAG with a pre-tuning procedure of the
sampler's parameters that exploits a skeleton graph derived through some
constraint-based or scoring-based algorithms. Thanks to these novel features,
PARNI-DAG quickly converges to high-probability regions and is less likely to
get stuck in local modes in the presence of high correlation between nodes in
high-dimensional settings. After introducing the technical novelties in
PARNI-DAG, we empirically demonstrate its mixing efficiency and accuracy in
learning DAG structures on a variety of experiments.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00603" title="Abstract">arXiv:2311.00603</a> [<a href="/pdf/2311.00603" title="Download PDF">pdf</a>, <a href="/format/2311.00603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Occluded Person Re-Identification with Deep Learning: A Survey and  Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+E">Enhao Ning</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changshuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhangc%2C+H">Huang Zhangc</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xin Ning</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+P">Prayag Tiwari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Person re-identification (Re-ID) technology plays an increasingly crucial
role in intelligent surveillance systems. Widespread occlusion significantly
impacts the performance of person Re-ID. Occluded person Re-ID refers to a
pedestrian matching method that deals with challenges such as pedestrian
information loss, noise interference, and perspective misalignment. It has
garnered extensive attention from researchers. Over the past few years, several
occlusion-solving person Re-ID methods have been proposed, tackling various
sub-problems arising from occlusion. However, there is a lack of comprehensive
studies that compare, summarize, and evaluate the potential of occluded person
Re-ID methods in detail. In this review, we start by providing a detailed
overview of the datasets and evaluation scheme used for occluded person Re-ID.
Next, we scientifically classify and analyze existing deep learning-based
occluded person Re-ID methods from various perspectives, summarizing them
concisely. Furthermore, we conduct a systematic comparison among these methods,
identify the state-of-the-art approaches, and present an outlook on the future
development of occluded person Re-ID.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00604" title="Abstract">arXiv:2311.00604</a> [<a href="/pdf/2311.00604" title="Download PDF">pdf</a>, <a href="/format/2311.00604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Review of Approximability Results for Traveling Salesman  Problems leveraging the TSP-T3CO Definition Scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saller%2C+S">Sophia Saller</a>, 
<a href="/search/cs?searchtype=author&query=Koehler%2C+J">Jana Koehler</a>, 
<a href="/search/cs?searchtype=author&query=Karrenbauer%2C+A">Andreas Karrenbauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The traveling salesman (or salesperson) problem, short TSP, is a problem of
strong interest to many researchers from mathematics, economics, and computer
science. Manifold TSP variants occur in nearly every scientific field and
application domain: engineering, physics, biology, life sciences, and
manufacturing just to name a few. Several thousand papers are published on
theoretical research or application-oriented results each year. This paper
provides the first systematic survey on the best currently known
approximability and inapproximability results for well-known TSP variants such
as the "standard" TSP, Path TSP, Bottleneck TSP, Maximum Scatter TSP,
Generalized TSP, Clustered TSP, Traveling Purchaser Problem, Profitable Tour
Problem, Quota TSP, Prize-Collecting TSP, Orienteering Problem, Time-dependent
TSP, TSP with Time Windows, and the Orienteering Problem with Time Windows. The
foundation of our survey is the definition scheme T3CO, which we propose as a
uniform, easy-to-use and extensible means for the formal and precise definition
of TSP variants. Applying T3CO to formally define the variant studied by a
paper reveals subtle differences within the same named variant and also brings
out the differences between the variants more clearly. We achieve the first
comprehensive, concise, and compact representation of approximability results
by using T3CO definitions. This makes it easier to understand the
approximability landscape and the assumptions under which certain results hold.
Open gaps become more evident and results can be compared more easily.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00610" title="Abstract">arXiv:2311.00610</a> [<a href="/pdf/2311.00610" title="Download PDF">pdf</a>, <a href="/format/2311.00610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularized Shannon sampling formulas related to the special affine  Fourier transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Filbir%2C+F">Frank Filbir</a>, 
<a href="/search/math?searchtype=author&query=Tasche%2C+M">Manfred Tasche</a>, 
<a href="/search/math?searchtype=author&query=Veselovska%2C+A">Anna Veselovska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Signal Processing (eess.SP); Complex Variables (math.CV)

</div>
<p class="mathjax">In this paper, we present new regularized Shannon sampling formulas related
to the special affine Fourier transform (SAFT). These sampling formulas use
localized sampling with special compactly supported window functions, namely
B-spline, sinh-type, and continuous Kaiser-Bessel window functions. In contrast
to the Shannon sampling series for SAFT, the regularized Shannon sampling
formulas for SAFT possesses an exponential decay of the approximation error and
are numerically robust in the presence of noise, if certain oversampling
condition is fulfilled. Several numerical experiments illustrate the
theoretical results.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00611" title="Abstract">arXiv:2311.00611</a> [<a href="/pdf/2311.00611" title="Download PDF">pdf</a>, <a href="/ps/2311.00611" title="Download PostScript">ps</a>, <a href="/format/2311.00611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Threshold Selection for Set Membership State Estimation with  Quantized Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Casini%2C+M">Marco Casini</a>, 
<a href="/search/eess?searchtype=author&query=Garulli%2C+A">Andrea Garulli</a>, 
<a href="/search/eess?searchtype=author&query=Vicino%2C+A">Antonio Vicino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">State estimation for discrete-time linear systems with quantized measurements
is addressed. By exploiting the set-theoretic nature of the information
provided by the quantizer, the problem is cast in the set membership estimation
setting. Motivated by the possibility of suitably tuning the quantizer
thresholds in sensor networks, the optimal design of adaptive quantizers is
formulated in terms of the minimization of the radius of information associated
to the state estimation problem. The optimal solution is derived for
first-order systems and the result is exploited to design adaptive quantizers
for generic systems, minimizing the size of the feasible output signal set.
Then, the minimum number of sensor thresholds for which the adaptive quantizers
guarantee asymptotic boundedness of the state estimation uncertainty is
established. Threshold adaptation mechanisms based on several types of outer
approximations of the feasible state set are also proposed. The effectiveness
of the designed adaptive quantizers is demonstrated on numerical tests
involving a specific case study and randomly generated systems, highlighting
the trade off between the resulting estimation uncertainty and the
computational burden required by recursive set approximations.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00612" title="Abstract">arXiv:2311.00612</a> [<a href="/pdf/2311.00612" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Collaborative Filtering-Based Two Stage Model with Item Dependency for  Course Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+E+L">Eric L. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+T">Tsung-Ting Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shou-De Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, 2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In 2017 IEEE DSAA, pp. 496-503. IEEE, 2017
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recommender systems have been studied for decades with numerous promising
models been proposed. Among them, Collaborative Filtering (CF) models are
arguably the most successful one due to its high accuracy in recommendation and
elimination of privacy-concerned personal meta-data from training. This paper
extends the usage of CF-based model to the task of course recommendation. We
point out several challenges in applying the existing CF-models to build a
course recommendation engine, including the lack of rating and meta-data, the
imbalance of course registration distribution, and the demand of course
dependency modeling. We then propose several ideas to address these challenges.
Eventually, we combine a two-stage CF model regularized by course dependency
with a graph-based recommender based on course-transition network, to achieve
AUC as high as 0.97 with a real-world dataset.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00613" title="Abstract">arXiv:2311.00613</a> [<a href="/pdf/2311.00613" title="Download PDF">pdf</a>, <a href="/format/2311.00613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controllable Music Production with Diffusion Models and Guidance  Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levy%2C+M">Mark Levy</a>, 
<a href="/search/cs?searchtype=author&query=Di+Giorgi%2C+B">Bruno Di Giorgi</a>, 
<a href="/search/cs?searchtype=author&query=Weers%2C+F">Floris Weers</a>, 
<a href="/search/cs?searchtype=author&query=Katharopoulos%2C+A">Angelos Katharopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Nickson%2C+T">Tom Nickson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We demonstrate how conditional generation from diffusion models can be used
to tackle a variety of realistic tasks in the production of music in 44.1kHz
stereo audio with sampling-time guidance. The scenarios we consider include
continuation, inpainting and regeneration of musical audio, the creation of
smooth transitions between two different music tracks, and the transfer of
desired stylistic characteristics to existing audio clips. We achieve this by
applying guidance at sampling time in a simple framework that supports both
reconstruction and classification losses, or any combination of the two. This
approach ensures that generated audio can match its surrounding context, or
conform to a class distribution or latent representation specified relative to
any suitable pre-trained classifier or embedding model.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00618" title="Abstract">arXiv:2311.00618</a> [<a href="/pdf/2311.00618" title="Download PDF">pdf</a>, <a href="/format/2311.00618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> De-Diffusion Makes Text a Strong Cross-Modal Interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenxi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Siyuan Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhishuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiahui Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report. Project page: <a href="https://dediffusion.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We demonstrate text as a strong cross-modal interface. Rather than relying on
deep embeddings to connect image and language as the interface representation,
our approach represents an image as text, from which we enjoy the
interpretability and flexibility inherent to natural language. We employ an
autoencoder that uses a pre-trained text-to-image diffusion model for decoding.
The encoder is trained to transform an input image into text, which is then fed
into the fixed text-to-image diffusion decoder to reconstruct the original
input -- a process we term De-Diffusion. Experiments validate both the
precision and comprehensiveness of De-Diffusion text representing images, such
that it can be readily ingested by off-the-shelf text-to-image tools and LLMs
for diverse multi-modal tasks. For example, a single De-Diffusion model can
generalize to provide transferable prompts for different text-to-image tools,
and also achieves a new state of the art on open-ended vision-language tasks by
simply prompting large language models with few-shot examples.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00619" title="Abstract">arXiv:2311.00619</a> [<a href="/pdf/2311.00619" title="Download PDF">pdf</a>, <a href="/format/2311.00619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loss Modeling for Multi-Annotator Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jinadu%2C+U">Uthman Jinadu</a>, 
<a href="/search/cs?searchtype=author&query=Annan%2C+J">Jesse Annan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+S">Shanshan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yi Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Accounting for the opinions of all annotators of a dataset is critical for
fairness. However, when annotating large datasets, individual annotators will
frequently provide thousands of ratings which can lead to fatigue.
Additionally, these annotation processes can occur over multiple days which can
lead to an inaccurate representation of an annotator's opinion over time. To
combat this, we propose to learn a more accurate representation of diverse
opinions by utilizing multitask learning in conjunction with loss-based label
correction. We show that using our novel formulation, we can cleanly separate
agreeing and disagreeing annotations. Furthermore, we demonstrate that this
modification can improve prediction performance in a single or multi-annotator
setting. Lastly, we show that this method remains robust to additional label
noise that is applied to subjective data.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00626" title="Abstract">arXiv:2311.00626</a> [<a href="/pdf/2311.00626" title="Download PDF">pdf</a>, <a href="/format/2311.00626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> nvblox: GPU-Accelerated Incremental Signed Distance Field Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Millane%2C+A">Alexander Millane</a>, 
<a href="/search/cs?searchtype=author&query=Oleynikova%2C+H">Helen Oleynikova</a>, 
<a href="/search/cs?searchtype=author&query=Wirbel%2C+E">Emilie Wirbel</a>, 
<a href="/search/cs?searchtype=author&query=Steiner%2C+R">Remo Steiner</a>, 
<a href="/search/cs?searchtype=author&query=Ramasamy%2C+V">Vikram Ramasamy</a>, 
<a href="/search/cs?searchtype=author&query=Tingdahl%2C+D">David Tingdahl</a>, 
<a href="/search/cs?searchtype=author&query=Siegwart%2C+R">Roland Siegwart</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Dense, volumetric maps are essential for safe robot navigation through
cluttered spaces, as well as interaction with the environment. For latency and
robustness, it is best if these can be computed on-board on
computationally-constrained hardware from camera or LiDAR-based sensors.
Previous works leave a gap between CPU-based systems for robotic mapping, which
due to computation constraints limit map resolution or scale, and GPU-based
reconstruction systems which omit features that are critical to robotic path
planning. We introduce a library, nvblox, that aims to fill this gap, by
GPU-accelerating robotic volumetric mapping, and which is optimized for
embedded GPUs. nvblox delivers a significant performance improvement over the
state of the art, achieving up to a 177x speed-up in surface reconstruction,
and up to a 31x improvement in distance field computation, and is available
open-source.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00629" title="Abstract">arXiv:2311.00629</a> [<a href="/pdf/2311.00629" title="Download PDF">pdf</a>, <a href="/format/2311.00629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal Translation from Reversing Petri Nets to Coloured Petri Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barylska%2C+K">Kamila Barylska</a>, 
<a href="/search/cs?searchtype=author&query=Gogolinska%2C+A">Anna Gogolinska</a>, 
<a href="/search/cs?searchtype=author&query=Mikulski%2C+L">Lukasz Mikulski</a>, 
<a href="/search/cs?searchtype=author&query=Philippou%2C+A">Anna Philippou</a>, 
<a href="/search/cs?searchtype=author&query=Piatkowski%2C+M">Marcin Piatkowski</a>, 
<a href="/search/cs?searchtype=author&query=Psara%2C+K">Kyriaki Psara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is planned to be published in a reputable journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Reversible computation is an emerging computing paradigm that allows any
sequence of operations to be executed in reverse order at any point during
computation. Its appeal lies in its potential for lowpower computation and its
relevance to a wide array of applications such as chemical reactions, quantum
computation, robotics, and distributed systems. Reversing Petri nets are a
recently-proposed extension of Petri nets that implements the three main forms
of reversibility, namely, backtracking, causal reversing, and
out-of-causal-order reversing. Their distinguishing feature is the use of named
tokens that can be combined together to form bonds. Named tokens along with a
history function, constitute the means of remembering past behaviour, thus,
enabling reversal. In recent work, we have proposed a structural translation
from a subclass of RPNs to the model of Coloured Petri Nets (CPNs), an
extension of traditional Petri nets where tokens carry data values. In this
paper, we extend the translation to handle RPNs with token multiplicity under
the individual-token interpretation, a model which allows multiple tokens of
the same type to exist in a system. To support the three types of
reversibility, tokens are associated with their causal history and, while
tokens of the same type are equally eligible to fire a transition when going
forward, when going backwards they are able to reverse only the transitions
they have previously fired. The new translation, in addition to lifting the
restriction on token uniqueness, presents a refined approach for transforming
RPNs to CPNs through a unifying approach that allows instantiating each of the
three types of reversibility. The paper also reports on a tool that implements
this translation, paving the way for automated translations and analysis of
reversible systems using CPN Tools.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00634" title="Abstract">arXiv:2311.00634</a> [<a href="/pdf/2311.00634" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bi-level Framework for Traffic Accident Duration Prediction:  Leveraging Weather and Road Condition Data within a Practical Optimum  Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukonna%2C+R+T">Rafat Tabassum Sukonna</a>, 
<a href="/search/cs?searchtype=author&query=Swapnil%2C+S+I">Soham Irtiza Swapnil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Due to the stochastic nature of events, predicting the duration of a traffic
incident presents a formidable challenge. Accurate duration estimation can
result in substantial advantages for commuters in selecting optimal routes and
for traffic management personnel in addressing non-recurring congestion issues.
In this study, we gathered accident duration, road conditions, and
meteorological data from a database of traffic accidents to check the
feasibility of a traffic accident duration pipeline without accident contextual
information data like accident severity and textual description. Multiple
machine learning models were employed to predict whether an accident's impact
on road traffic would be of a short-term or long-term nature, and then
utilizing a bimodal approach the precise duration of the incident's effect was
determined. Our binary classification random forest model distinguished between
short-term and long-term effects with an 83% accuracy rate, while the LightGBM
regression model outperformed other machine learning regression models with
Mean Average Error (MAE) values of 26.15 and 13.3 and RMSE values of 32.91 and
28.91 for short and long-term accident duration prediction, respectively. Using
the optimal classification and regression model identified in the preceding
section, we then construct an end-to-end pipeline to incorporate the entire
process. The results of both separate and combined approaches were comparable
with previous works, which shows the applicability of only using static
features for predicting traffic accident duration. The SHAP value analysis
identified weather conditions, wind chill and wind speed as the most
influential factors in determining the duration of an accident.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00635" title="Abstract">arXiv:2311.00635</a> [<a href="/pdf/2311.00635" title="Download PDF">pdf</a>, <a href="/format/2311.00635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GATSY: Graph Attention Network for Music Artist Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Francesco%2C+A+G">Andrea Giuseppe Di Francesco</a>, 
<a href="/search/cs?searchtype=author&query=Giampietro%2C+G">Giuliano Giampietro</a>, 
<a href="/search/cs?searchtype=author&query=Spinelli%2C+I">Indro Spinelli</a>, 
<a href="/search/cs?searchtype=author&query=Comminiello%2C+D">Danilo Comminiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, Submitted to MLSP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The artist similarity quest has become a crucial subject in social and
scientific contexts. Modern research solutions facilitate music discovery
according to user tastes. However, defining similarity among artists may
involve several aspects, even related to a subjective perspective, and it often
affects a recommendation. This paper presents GATSY, a recommendation system
built upon graph attention networks and driven by a clusterized embedding of
artists. The proposed framework takes advantage of a graph topology of the
input data to achieve outstanding performance results without relying heavily
on hand-crafted features. This flexibility allows us to introduce fictitious
artists in a music dataset, create bridges to previously unrelated artists, and
get recommendations conditioned by possibly heterogeneous sources. Experimental
results prove the effectiveness of the proposed method with respect to
state-of-the-art solutions.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00636" title="Abstract">arXiv:2311.00636</a> [<a href="/pdf/2311.00636" title="Download PDF">pdf</a>, <a href="/format/2311.00636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kronecker-Factored Approximate Curvature for Modern Neural Network  Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eschenhagen%2C+R">Runa Eschenhagen</a>, 
<a href="/search/cs?searchtype=author&query=Immer%2C+A">Alexander Immer</a>, 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+F">Frank Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The core components of many modern neural network architectures, such as
transformers, convolutional, or graph neural networks, can be expressed as
linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate
Curvature (K-FAC), a second-order optimisation method, has shown promise to
speed up neural network training and thereby reduce computational costs.
However, there is currently no framework to apply it to generic architectures,
specifically ones with linear weight-sharing layers. In this work, we identify
two different settings of linear weight-sharing layers which motivate two
flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they
are exact for deep linear networks with weight-sharing in their respective
setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we
leverage to speed up automatic hyperparameter selection via optimising the
marginal likelihood for a Wide ResNet. Finally, we observe little difference
between these two K-FAC variations when using them to train both a graph neural
network and a vision transformer. However, both variations are able to reach a
fixed validation metric target in $50$-$75\%$ of the number of steps of a
first-order reference run, which translates into a comparable improvement in
wall-clock time. This highlights the potential of applying K-FAC to modern
neural network architectures.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00637" title="Abstract">arXiv:2311.00637</a> [<a href="/pdf/2311.00637" title="Download PDF">pdf</a>, <a href="/format/2311.00637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of the Single Reference Coupled Cluster Method for Electronic  Structure Calculations: The Discrete Coupled Cluster Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hassan%2C+M">Muhammad Hassan</a>, 
<a href="/search/math?searchtype=author&query=Maday%2C+Y">Yvon Maday</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Yipeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 78 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Coupled cluster methods are widely regarded as the gold standard of
computational quantum chemistry as they are perceived to offer the best
compromise between computational cost and a high-accuracy resolution of the
ground state eigenvalue of the electronic Hamiltonian -- an unbounded,
self-adjoint operator acting on a Hilbert space of antisymmetric functions that
describes electronic properties of molecular systems. The present contribution
is the second in a series of two articles where we introduce a new numerical
analysis of the single-reference coupled cluster method based on the
invertibility of coupled cluster Fr\'echet derivative. In this contribution, we
study discretisations of the single-reference coupled cluster equations based
on a prior mean-field (Hartree-Fock) calculation. We show that under some
structural assumptions on the associated discretisation spaces and assuming
that the discretisation is fine enough, the discrete coupled cluster equations
are locally well-posed, and we derive a priori and residual-based a posteriori
error estimates for the discrete coupled cluster solutions. Preliminary
numerical experiments indicate that the structural assumptions that we impose
for our analysis can be expected to hold for several small molecules, and the
theoretical constants that appear in our error estimates are a significant
improvement over those obtained from earlier approaches.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00638" title="Abstract">arXiv:2311.00638</a> [<a href="/pdf/2311.00638" title="Download PDF">pdf</a>, <a href="/format/2311.00638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAIRLABEL: Correcting Bias in Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sengamedu%2C+S+H">Srinivasan H Sengamedu</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H">Hien Pham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICDM LegalAI Workshop 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICDM 2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">There are several algorithms for measuring fairness of ML models. A
fundamental assumption in these approaches is that the ground truth is fair or
unbiased. In real-world datasets, however, the ground truth often contains data
that is a result of historical and societal biases and discrimination. Models
trained on these datasets will inherit and propagate the biases to the model
outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases
in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across
groups while maintaining high accuracy in predictions. We propose metrics to
measure the quality of bias correction and validate FAIRLABEL on synthetic
datasets and show that the label correction is correct 86.7% of the time vs.
71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such
as UCI Adult, German Credit Risk, and Compas datasets and show that the
Disparate Impact Ratio increases by as much as 54.2%.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00642" title="Abstract">arXiv:2311.00642</a> [<a href="/pdf/2311.00642" title="Download PDF">pdf</a>, <a href="/format/2311.00642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Optimal $k$-Clustering in the Sliding Window Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Peilin Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Samson Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Clustering is an important technique for identifying structural information
in large-scale data analysis, where the underlying dataset may be too large to
store. In many applications, recent data can provide more accurate information
and thus older data past a certain time is expired. The sliding window model
captures these desired properties and thus there has been substantial interest
in clustering in the sliding window model.
<br />In this paper, we give the first algorithm that achieves near-optimal
$(1+\varepsilon)$-approximation to $(k,z)$-clustering in the sliding window
model, where $z$ is the exponent of the distance function in the cost. Our
algorithm uses
$\frac{k}{\min(\varepsilon^4,\varepsilon^{2+z})}\,\text{polylog}\frac{n\Delta}{\varepsilon}$
words of space when the points are from $[\Delta]^d$, thus significantly
improving on works by Braverman et. al. (SODA 2016), Borassi et. al. (NeurIPS
2021), and Epasto et. al. (SODA 2022).
<br />Along the way, we develop a data structure for clustering called an online
coreset, which outputs a coreset not only for the end of a stream, but also for
all prefixes of the stream. Our online coreset samples
$\frac{k}{\min(\varepsilon^4,\varepsilon^{2+z})}\,\text{polylog}\frac{n\Delta}{\varepsilon}$
points from the stream. We then show that any online coreset requires
$\Omega\left(\frac{k}{\varepsilon^2}\log n\right)$ samples, which shows a
separation from the problem of constructing an offline coreset, i.e.,
constructing online coresets is strictly harder. Our results also extend to
general metrics on $[\Delta]^d$ and are near-optimal in light of a
$\Omega\left(\frac{k}{\varepsilon^{2+z}}\right)$ lower bound for the size of an
offline coreset.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00646" title="Abstract">arXiv:2311.00646</a> [<a href="/pdf/2311.00646" title="Download PDF">pdf</a>, <a href="/format/2311.00646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Issues and Causes in WebAssembly Application  Development: A Mining-based Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waseem%2C+M">Muhammad Waseem</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+T">Teerath Das</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+A">Aakash Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Peng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Mikkonen%2C+T">Tommi Mikkonen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">WebAssembly (Wasm) is a binary instruction format designed for secure and
efficient execution within sandboxed environments - predominantly web apps and
browsers - to facilitate performance, security, and flexibility of web
programming languages. In recent years, Wasm has gained significant attention
from academic research community and industrial development projects to
engineer high-performance web applications. Despite the offered benefits,
developers encounter a multitude of issues rooted in Wasm (e.g., faults,
errors, failures) and are often unaware of their root-causes that impact the
development of web applications. Wasm developers require knowledge, documented
as empirically rooted guidelines, patterns, documents etc., that help them to
understand, analyse, and resolve the issues that currently lacks in existing
research and practice. To this end, we conducted an empirical study that mines
and documents practitioners' knowledge expressed as 385 issues from 12
open-source Wasm projects deployed on GitHub and 354 question-answer posts via
Stack Overflow. Our study led to the first-of-its-kind taxonomies of issues
faced by developers and their underlying causes in Wasm-based applications.
Issues faced by developers arise from 'Infrastructure, Integration and
Compatibility Aspects' (28.16%), 'Language Features and Documentation Errors'
(18.00%), along with 'Code Implementation and Build failures' (13.83%). The
results indicate that 'Syntactic and Semantic Errors' (25.77%), 'Configuration
and Compatibility Constraints' (20.1%), and 'Operational Limitations' (12.98%)
are the principal causes of these issues. The study provides a taxonomical
classification of issues and their causes, offering empirically derived
guidelines, that can inform researchers and developers to systematically
design, develop, and refactor Wasm-based applications.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00651" title="Abstract">arXiv:2311.00651</a> [<a href="/pdf/2311.00651" title="Download PDF">pdf</a>, <a href="/format/2311.00651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergence of Collective Open-Ended Exploration from Decentralized  Meta-Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bornemann%2C+R">Richard Bornemann</a>, 
<a href="/search/cs?searchtype=author&query=Hamon%2C+G">Gautier Hamon</a>, 
<a href="/search/cs?searchtype=author&query=Nisioti%2C+E">Eleni Nisioti</a>, 
<a href="/search/cs?searchtype=author&query=Moulin-Frier%2C+C">Cl&#xe9;ment Moulin-Frier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent works have proven that intricate cooperative behaviors can emerge in
agents trained using meta reinforcement learning on open ended task
distributions using self-play. While the results are impressive, we argue that
self-play and other centralized training techniques do not accurately reflect
how general collective exploration strategies emerge in the natural world:
through decentralized training and over an open-ended distribution of tasks. In
this work we therefore investigate the emergence of collective exploration
strategies, where several agents meta-learn independent recurrent policies on
an open ended distribution of tasks. To this end we introduce a novel
environment with an open ended procedurally generated task space which
dynamically combines multiple subtasks sampled from five diverse task types to
form a vast distribution of task trees. We show that decentralized agents
trained in our environment exhibit strong generalization abilities when
confronted with novel objects at test time. Additionally, despite never being
forced to cooperate during training the agents learn collective exploration
strategies which allow them to solve novel tasks never encountered during
training. We further find that the agents learned collective exploration
strategies extend to an open ended task setting, allowing them to solve task
trees of twice the depth compared to the ones seen during training. Our open
source code as well as videos of the agents can be found on our companion
website.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00653" title="Abstract">arXiv:2311.00653</a> [<a href="/pdf/2311.00653" title="Download PDF">pdf</a>, <a href="/format/2311.00653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating measures of replicability into literature search: Challenges  and opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chakravorti%2C+T">Tatiana Chakravorti</a>, 
<a href="/search/cs?searchtype=author&query=Carroll%2C+J">John Carroll</a>, 
<a href="/search/cs?searchtype=author&query=Rajtmajer%2C+S">Sarah Rajtmajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Challenges to reproducibility and replicability have gained widespread
attention over the past decade, driven by a number of large replication
projects with lukewarm success rates. A nascent work has emerged developing
algorithms to estimate, or predict, the replicability of published findings.
The current study explores ways in which AI-enabled signals of confidence in
research might be integrated into literature search. We interview 17 PhD
researchers about their current processes for literature search and ask them to
provide feedback on a prototype replicability estimation tool. Our findings
suggest that information about replicability can support researchers throughout
literature review and research design processes. However, explainability and
interpretability of system outputs is critical, and potential drawbacks of
AI-enabled confidence assessment need to be further studied before such tools
could be widely accepted and deployed. We discuss implications for the design
of technological tools to support scholarly activities and advance
reproducibility and replicability.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00658" title="Abstract">arXiv:2311.00658</a> [<a href="/pdf/2311.00658" title="Download PDF">pdf</a>, <a href="/format/2311.00658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit Morphological Knowledge Improves Pre-training of Language  Models for Hebrew
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gueta%2C+E">Eylon Gueta</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+O">Omer Goldman</a>, 
<a href="/search/cs?searchtype=author&query=Tsarfaty%2C+R">Reut Tsarfaty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Pre-trained language models (PLMs) have shown remarkable successes in
acquiring a wide range of linguistic knowledge, relying solely on
self-supervised training on text streams. Nevertheless, the effectiveness of
this language-agnostic approach has been frequently questioned for its
sub-optimal performance when applied to morphologically-rich languages (MRLs).
We investigate the hypothesis that incorporating explicit morphological
knowledge in the pre-training phase can improve the performance of PLMs for
MRLs. We propose various morphologically driven tokenization methods enabling
the model to leverage morphological cues beyond raw text. We pre-train multiple
language models utilizing the different methods and evaluate them on Hebrew, a
language with complex and highly ambiguous morphology. Our experiments show
that morphologically driven tokenization demonstrates improved results compared
to a standard language-agnostic tokenization, on a benchmark of both semantic
and morphologic tasks. These findings suggest that incorporating morphological
knowledge holds the potential for further improving PLMs for morphologically
rich languages.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00660" title="Abstract">arXiv:2311.00660</a> [<a href="/pdf/2311.00660" title="Download PDF">pdf</a>, <a href="/format/2311.00660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining  and Object Detection in Rain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shen Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Changjie Lu</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+S+G">Srinivasa G. Narasimhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Rain generation algorithms have the potential to improve the generalization
of deraining methods and scene understanding in rainy conditions. However, in
practice, they produce artifacts and distortions and struggle to control the
amount of rain generated due to a lack of proper constraints. In this paper, we
propose an unpaired image-to-image translation framework for generating
realistic rainy images. We first introduce a Triangular Probability Similarity
(TPS) constraint to guide the generated images toward clear and rainy images in
the discriminator manifold, thereby minimizing artifacts and distortions during
rain generation. Unlike conventional contrastive learning approaches, which
indiscriminately push negative samples away from the anchors, we propose a
Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing
force of negative samples based on the semantic similarity between the clear
and the rainy images and the feature similarity between the anchor and the
negative samples. Experiments demonstrate realistic rain generation with
minimal artifacts and distortions, which benefits image deraining and object
detection in rain. Furthermore, the method can be used to generate realistic
snowy and night images, underscoring its potential for broader applicability.
Code is available at https://github.com/ShenZheng2000/TPSeNCE.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00664" title="Abstract">arXiv:2311.00664</a> [<a href="/pdf/2311.00664" title="Download PDF">pdf</a>, <a href="/format/2311.00664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Space Translation via Semantic Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maiorca%2C+V">Valentino Maiorca</a>, 
<a href="/search/cs?searchtype=author&query=Moschella%2C+L">Luca Moschella</a>, 
<a href="/search/cs?searchtype=author&query=Norelli%2C+A">Antonio Norelli</a>, 
<a href="/search/cs?searchtype=author&query=Fumero%2C+M">Marco Fumero</a>, 
<a href="/search/cs?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>, 
<a href="/search/cs?searchtype=author&query=Rodol%C3%A0%2C+E">Emanuele Rodol&#xe0;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. 21 pages, 13 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While different neural models often exhibit latent spaces that are alike when
exposed to semantically related data, this intrinsic similarity is not always
immediately discernible. Towards a better understanding of this phenomenon, our
work shows how representations learned from these neural modules can be
translated between different pre-trained networks via simpler transformations
than previously thought. An advantage of this approach is the ability to
estimate these transformations using standard, well-understood algebraic
procedures that have closed-form solutions. Our method directly estimates a
transformation between two given latent spaces, thereby enabling effective
stitching of encoders and decoders without additional training. We extensively
validate the adaptability of this translation procedure in different
experimental settings: across various trainings, domains, architectures (e.g.,
ResNet, CNN, ViT), and in multiple downstream tasks (classification,
reconstruction). Notably, we show how it is possible to zero-shot stitch text
encoders and vision decoders, or vice-versa, yielding surprisingly good
classification performance in this multimodal setting.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00668" title="Abstract">arXiv:2311.00668</a> [<a href="/pdf/2311.00668" title="Download PDF">pdf</a>, <a href="/format/2311.00668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProcSim: Proxy-based Confidence for Robust Similarity Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barbany%2C+O">Oriol Barbany</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiaofan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Bastan%2C+M">Muhammet Bastan</a>, 
<a href="/search/cs?searchtype=author&query=Dhua%2C+A">Arnab Dhua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the algorithms track of WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep Metric Learning (DML) methods aim at learning an embedding space in
which distances are closely related to the inherent semantic similarity of the
inputs. Previous studies have shown that popular benchmark datasets often
contain numerous wrong labels, and DML methods are susceptible to them.
Intending to study the effect of realistic noise, we create an ontology of the
classes in a dataset and use it to simulate semantically coherent labeling
mistakes. To train robust DML models, we propose ProcSim, a simple framework
that assigns a confidence score to each sample using the normalized distance to
its class representative. The experimental results show that the proposed
method achieves state-of-the-art performance on the DML benchmark datasets
injected with uniform and the proposed semantically coherent noise.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00671" title="Abstract">arXiv:2311.00671</a> [<a href="/pdf/2311.00671" title="Download PDF">pdf</a>, <a href="/format/2311.00671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotion Detection for Misinformation: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Thompson%2C+P">Paul Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zeping Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the advent of social media, an increasing number of netizens are sharing
and reading posts and news online. However, the huge volumes of misinformation
(e.g., fake news and rumors) that flood the internet can adversely affect
people's lives, and have resulted in the emergence of rumor and fake news
detection as a hot research topic. The emotions and sentiments of netizens, as
expressed in social media posts and news, constitute important factors that can
help to distinguish fake news from genuine news and to understand the spread of
rumors. This article comprehensively reviews emotion-based methods for
misinformation detection. We begin by explaining the strong links between
emotions and misinformation. We subsequently provide a detailed analysis of a
range of misinformation detection methods that employ a variety of emotion,
sentiment and stance-based features, and describe their strengths and
weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based
misinformation detection based on large language models and suggest future
research directions, including data collection (multi-platform, multilingual),
annotation, benchmark, multimodality, and interpretability.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00676" title="Abstract">arXiv:2311.00676</a> [<a href="/pdf/2311.00676" title="Download PDF">pdf</a>, <a href="/format/2311.00676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Last-Iterate Convergence Properties of Regret-Matching Algorithms in  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Grand-Cl%C3%A9ment%2C+J">Julien Grand-Cl&#xe9;ment</a>, 
<a href="/search/cs?searchtype=author&query=Kroer%2C+C">Christian Kroer</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chung-Wei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Weiqiang Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Algorithms based on regret matching, specifically regret matching$^+$
(RM$^+$), and its variants are the most popular approaches for solving
large-scale two-player zero-sum games in practice. Unlike algorithms such as
optimistic gradient descent ascent, which have strong last-iterate and ergodic
convergence properties for zero-sum games, virtually nothing is known about the
last-iterate properties of regret-matching algorithms. Given the importance of
last-iterate convergence for numerical optimization reasons and relevance as
modeling real-word learning in games, in this paper, we study the last-iterate
convergence properties of various popular variants of RM$^+$. First, we show
numerically that several practical variants such as simultaneous RM$^+$,
alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate
convergence guarantees even on a simple $3\times 3$ game. We then prove that
recent variants of these algorithms based on a smoothing technique do enjoy
last-iterate convergence: we prove that extragradient RM$^{+}$ and smooth
Predictive RM$^+$ enjoy asymptotic last-iterate convergence (without a rate)
and $1/\sqrt{t}$ best-iterate convergence. Finally, we introduce restarted
variants of these algorithms, and show that they enjoy linear-rate last-iterate
convergence.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00681" title="Abstract">arXiv:2311.00681</a> [<a href="/pdf/2311.00681" title="Download PDF">pdf</a>, <a href="/format/2311.00681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Reliable Judges? A Study on the Factuality  Evaluation Capabilities of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xue-Yong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Laskar%2C+M+T+R">Md Tahmid Rahman Laskar</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Cheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=TN%2C+S+B">Shashi Bhushan TN</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by Generation, Evaluation &amp; Metrics (GEM) Workshop at EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In recent years, Large Language Models (LLMs) have gained immense attention
due to their notable emergent capabilities, surpassing those seen in earlier
language models. A particularly intriguing application of LLMs is their role as
evaluators for texts produced by various generative models.
<br />In this study, we delve into the potential of LLMs as reliable assessors of
factual consistency in summaries generated by text-generation models.
Initially, we introduce an innovative approach for factuality assessment using
LLMs. This entails employing a singular LLM for the entirety of the
question-answering-based factuality scoring process. Following this, we examine
the efficacy of various LLMs in direct factuality scoring, benchmarking them
against traditional measures and human annotations.
<br />Contrary to initial expectations, our results indicate a lack of significant
correlations between factuality metrics and human evaluations, specifically for
GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across
two factuality subcategories. These consistent findings across various factual
error categories suggest a fundamental limitation in the current LLMs'
capability to accurately gauge factuality.
<br />This version presents the information more concisely while maintaining the
main points and findings of the original text.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00684" title="Abstract">arXiv:2311.00684</a> [<a href="/pdf/2311.00684" title="Download PDF">pdf</a>, <a href="/format/2311.00684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention Alignment and Flexible Positional Embeddings Improve  Transformer Length Extrapolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+T">Ta-Chung Chi</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+T">Ting-Han Fan</a>, 
<a href="/search/cs?searchtype=author&query=Rudnicky%2C+A+I">Alexander I. Rudnicky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">An ideal length-extrapolatable Transformer language model can handle
sequences longer than the training length without any long sequence
fine-tuning. Such long-context utilization capability highly relies on a
flexible positional embedding design. Upon investigating the flexibility of
existing large pre-trained Transformer language models, we find that the T5
family deserves a closer look, as its positional embeddings capture rich and
flexible attention patterns. However, T5 suffers from the dispersed attention
issue: the longer the input sequence, the flatter the attention distribution.
To alleviate the issue, we propose two attention alignment strategies via
temperature scaling. Our findings improve the long-context utilization
capability of T5 on language modeling, retrieval, and multi-document question
answering without any fine-tuning, suggesting that a flexible positional
embedding design and attention alignment go a long way toward Transformer
length
extrapolation.\footnote{\url{https://github.com/chijames/Attention-Alignment-Transformer-Length-Extrapolation}}
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00686" title="Abstract">arXiv:2311.00686</a> [<a href="/pdf/2311.00686" title="Download PDF">pdf</a>, <a href="/ps/2311.00686" title="Download PostScript">ps</a>, <a href="/format/2311.00686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Little Giants: Exploring the Potential of Small LLMs as Evaluation  Metrics in Summarization in the Eval4NLP 2023 Shared Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotonya%2C+N">Neema Kotonya</a>, 
<a href="/search/cs?searchtype=author&query=Krishnasamy%2C+S">Saran Krishnasamy</a>, 
<a href="/search/cs?searchtype=author&query=Tetreault%2C+J">Joel Tetreault</a>, 
<a href="/search/cs?searchtype=author&query=Jaimes%2C+A">Alejandro Jaimes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Eval4NLP 2023 Shared Task
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper describes and analyzes our participation in the 2023 Eval4NLP
shared task, which focuses on assessing the effectiveness of prompt-based
techniques to empower Large Language Models to handle the task of quality
estimation, particularly in the context of evaluating machine translations and
summaries. We conducted systematic experiments with various prompting
techniques, including standard prompting, prompts informed by annotator
instructions, and innovative chain-of-thought prompting. In addition, we
integrated these approaches with zero-shot and one-shot learning methods to
maximize the efficacy of our evaluation procedures. Our work reveals that
combining these approaches using a "small", open source model (orca_mini_v3_7B)
yields competitive results.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00687" title="Abstract">arXiv:2311.00687</a> [<a href="/pdf/2311.00687" title="Download PDF">pdf</a>, <a href="/format/2311.00687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Interpersonal Communication by Simulating Audiences with  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ryan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yen%2C+H">Howard Yen</a>, 
<a href="/search/cs?searchtype=author&query=Marjieh%2C+R">Raja Marjieh</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages (main paper), 7 tables and figures (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">How do we communicate with others to achieve our goals? We use our prior
experience or advice from others, or construct a candidate utterance by
predicting how it will be received. However, our experiences are limited and
biased, and reasoning about potential outcomes can be difficult and cognitively
challenging. In this paper, we explore how we can leverage Large Language Model
(LLM) simulations to help us communicate better. We propose the
Explore-Generate-Simulate (EGS) framework, which takes as input any scenario
where an individual is communicating to an audience with a goal they want to
achieve. EGS (1) explores the solution space by producing a diverse set of
advice relevant to the scenario, (2) generates communication candidates
conditioned on subsets of the advice, and (3) simulates the reactions from
various audiences to determine both the best candidate and advice to use. We
evaluate the framework on eight scenarios spanning the ten fundamental
processes of interpersonal communication. For each scenario, we collect a
dataset of human evaluations across candidates and baselines, and showcase that
our framework's chosen candidate is preferred over popular generation
mechanisms including Chain-of-Thought. We also find that audience simulations
achieve reasonably high agreement with human raters across 5 of the 8
scenarios. Finally, we demonstrate the generality of our framework by applying
it to real-world scenarios described by users on web forums. Through
evaluations and demonstrations, we show that EGS enhances the effectiveness and
outcomes of goal-oriented communication across a variety of situations, thus
opening up new possibilities for the application of large language models in
revolutionizing communication and decision-making processes.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00689" title="Abstract">arXiv:2311.00689</a> [<a href="/pdf/2311.00689" title="Download PDF">pdf</a>, <a href="/format/2311.00689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaboration in Immersive Environments: Challenges and Solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doroudian%2C+S">Shahin Doroudian</a>, 
<a href="/search/cs?searchtype=author&query=Wartell%2C+Z">Zachary Wartell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00690" title="Abstract">arXiv:2311.00690</a> [<a href="/pdf/2311.00690" title="Download PDF">pdf</a>, <a href="/format/2311.00690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What User Behaviors Make the Differences During the Process of Visual  Analytics?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doroudian%2C+S">Shahin Doroudian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zekun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+A">Aidong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The understanding of visual analytics process can benefit visualization
researchers from multiple aspects, including improving visual designs and
developing advanced interaction functions. However, the log files of user
behaviors are still hard to analyze due to the complexity of sensemaking and
our lack of knowledge on the related user behaviors. This work presents a study
on a comprehensive data collection of user behaviors, and our analysis approach
with time-series classification methods. We have chosen a classical
visualization application, Covid-19 data analysis, with common analysis tasks
covering geo-spatial, time-series and multi-attributes. Our user study collects
user behaviors on a diverse set of visualization tasks with two comparable
systems, desktop and immersive visualizations. We summarize the classification
results with three time-series machine learning algorithms at two scales, and
explore the influences of behavior features. Our results reveal that user
behaviors can be distinguished during the process of visual analytics and there
is a potentially strong association between the physical behaviors of users and
the visualization tasks they perform. We also demonstrate the usage of our
models by interpreting open sessions of visual analytics, which provides an
automatic way to study sensemaking without tedious manual annotations.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00691" title="Abstract">arXiv:2311.00691</a> [<a href="/pdf/2311.00691" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Software Repositories and Machine Learning Research in Cyber Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vanamala%2C+M">Mounika Vanamala</a>, 
<a href="/search/cs?searchtype=author&query=Bryant%2C+K">Keith Bryant</a>, 
<a href="/search/cs?searchtype=author&query=Caravella%2C+A">Alex Caravella</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In today's rapidly evolving technological landscape and advanced software
development, the rise in cyber security attacks has become a pressing concern.
The integration of robust cyber security defenses has become essential across
all phases of software development. It holds particular significance in
identifying critical cyber security vulnerabilities at the initial stages of
the software development life cycle, notably during the requirement phase.
Through the utilization of cyber security repositories like The Common Attack
Pattern Enumeration and Classification (CAPEC) from MITRE and the Common
Vulnerabilities and Exposures (CVE) databases, attempts have been made to
leverage topic modeling and machine learning for the detection of these
early-stage vulnerabilities in the software requirements process. Past research
themes have returned successful outcomes in attempting to automate
vulnerability identification for software developers, employing a mixture of
unsupervised machine learning methodologies such as LDA and topic modeling.
Looking ahead, in our pursuit to improve automation and establish connections
between software requirements and vulnerabilities, our strategy entails
adopting a variety of supervised machine learning techniques. This array
encompasses Support Vector Machines (SVM), Na\"ive Bayes, random forest, neural
networking and eventually transitioning into deep learning for our
investigation. In the face of the escalating complexity of cyber security, the
question of whether machine learning can enhance the identification of
vulnerabilities in diverse software development scenarios is a paramount
consideration, offering crucial assistance to software developers in developing
secure software.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00693" title="Abstract">arXiv:2311.00693</a> [<a href="/pdf/2311.00693" title="Download PDF">pdf</a>, <a href="/format/2311.00693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Task-personalized Multimodal Few-shot Learning for Visually-rich  Document Entity Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bo Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Aidong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures; regular long paper, EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Findings of the Association for Computational Linguistics: EMNLP
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Visually-rich document entity retrieval (VDER), which extracts key
information (e.g. date, address) from document images like invoices and
receipts, has become an important topic in industrial NLP applications. The
emergence of new document types at a constant pace, each with its unique entity
types, presents a unique challenge: many documents contain unseen entity types
that occur only a couple of times. Addressing this challenge requires models to
have the ability of learning entities in a few-shot manner. However, prior
works for Few-shot VDER mainly address the problem at the document level with a
predefined global entity space, which doesn't account for the entity-level
few-shot scenario: target entity types are locally personalized by each task
and entity occurrences vary significantly among documents. To address this
unexplored scenario, this paper studies a novel entity-level few-shot VDER
task. The challenges lie in the uniqueness of the label space for each task and
the increased complexity of out-of-distribution (OOD) contents. To tackle this
novel task, we present a task-aware meta-learning based framework, with a
central focus on achieving effective task personalization that distinguishes
between in-task and out-of-task distribution. Specifically, we adopt a
hierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) to
achieve this goal. Furthermore, we introduce a new dataset, FewVEX, to boost
future research in the field of entity-level few-shot VDER. Experimental
results demonstrate our approaches significantly improve the robustness of
popular meta-learning baselines.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00694" title="Abstract">arXiv:2311.00694</a> [<a href="/pdf/2311.00694" title="Download PDF">pdf</a>, <a href="/format/2311.00694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Creative Mind: Language Model As Hierarchical Policy For  Improved Exploration on Challenging Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yunhao Fang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuanlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+T">Tongzhou Mu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Mingu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Pourreza%2C+R">Reza Pourreza</a>, 
<a href="/search/cs?searchtype=author&query=Memisevic%2C+R">Roland Memisevic</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hao Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved tremendous progress, yet they
still often struggle with challenging reasoning problems. Current approaches
address this challenge by sampling or searching detailed and low-level
reasoning chains. However, these methods are still limited in their exploration
capabilities, making it challenging for correct solutions to stand out in the
huge solution space. In this work, we unleash LLMs' creative potential for
exploring multiple diverse problem solving strategies by framing an LLM as a
hierarchical policy via in-context learning. This policy comprises of a
visionary leader that proposes multiple diverse high-level problem-solving
tactics as hints, accompanied by a follower that executes detailed
problem-solving processes following each of the high-level instruction. The
follower uses each of the leader's directives as a guide and samples multiple
reasoning chains to tackle the problem, generating a solution group for each
leader proposal. Additionally, we propose an effective and efficient
tournament-based approach to select among these explored solution groups to
reach the final answer. Our approach produces meaningful and inspiring hints,
enhances problem-solving strategy exploration, and improves the final answer
accuracy on challenging problems in the MATH dataset. Code will be released at
https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00696" title="Abstract">arXiv:2311.00696</a> [<a href="/pdf/2311.00696" title="Download PDF">pdf</a>, <a href="/format/2311.00696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision Support Framework for Home Health Caregiver Allocation: A Case  Study of HHC Agency in Tennessee, USA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharifnia%2C+S+M+E">Seyed Mohammad Ebrahim Sharifnia</a>, 
<a href="/search/cs?searchtype=author&query=Bagheri%2C+F">Faezeh Bagheri</a>, 
<a href="/search/cs?searchtype=author&query=Sawhney%2C+R">Rupy Sawhney</a>, 
<a href="/search/cs?searchtype=author&query=Kobza%2C+J+E">John E. Kobza</a>, 
<a href="/search/cs?searchtype=author&query=De+Anda%2C+E+M">Enrique Macias De Anda</a>, 
<a href="/search/cs?searchtype=author&query=Hajiaghaei-Keshteli%2C+M">Mostafa Hajiaghaei-Keshteli</a>, 
<a href="/search/cs?searchtype=author&query=Mirrielees%2C+M">Michael Mirrielees</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The document is written in the Elsevier LaTeX format
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Population aging is a global challenge, leading to increased demand for
healthcare and social services for the elderly. Home Health Care (HHC) emerges
as a vital solution, specifically designed to serve this population segment.
Given the surging demand for HHC, it's essential to coordinate and regulate
caregiver allocation efficiently. This is crucial for both budget-optimized
planning and ensuring the delivery of high-quality care. This research
addresses a key question faced by home health agencies (HHAs): "How can
caregiver allocation be optimized, especially when caregivers prefer
flexibility in their visiting sequences?". While earlier studies proposed rigid
visiting sequences, our study introduces a decision support framework that
allocates caregivers through a hybrid method that considers the flexibility in
visiting sequences and aims to reduce travel mileage, increase the number of
visits per planning period, and maintain the continuity of care - a critical
metric for patient satisfaction. Utilizing data from an HHA in Tennessee,
United States, our approach led to an impressive reduction in average travel
mileage (up to 42% depending on discipline) without imposing restrictions on
caregivers. Furthermore, the proposed framework is used for caregivers' supply
analysis to provide valuable insights into caregiver resource management.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00697" title="Abstract">arXiv:2311.00697</a> [<a href="/pdf/2311.00697" title="Download PDF">pdf</a>, <a href="/format/2311.00697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Single-Channel Speaker-Turn Aware Conversational Speech  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuluaga-Gomez%2C+J">Juan Zuluaga-Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhaocheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xing Niu</a>, 
<a href="/search/cs?searchtype=author&query=Paturi%2C+R">Rohit Paturi</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+S">Sundararajan Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+P">Prashant Mathur</a>, 
<a href="/search/cs?searchtype=author&query=Thompson%2C+B">Brian Thompson</a>, 
<a href="/search/cs?searchtype=author&query=Federico%2C+M">Marcello Federico</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP 2023. Code: <a href="https://github.com/amazon-science/stac-speech-translation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Conventional speech-to-text translation (ST) systems are trained on
single-speaker utterances, and they may not generalize to real-life scenarios
where the audio contains conversations by multiple speakers. In this paper, we
tackle single-channel multi-speaker conversational ST with an end-to-end and
multi-task training model, named Speaker-Turn Aware Conversational Speech
Translation, that combines automatic speech recognition, speech translation and
speaker turn detection using special tokens in a serialized labeling format. We
run experiments on the Fisher-CALLHOME corpus, which we adapted by merging the
two single-speaker channels into one multi-speaker channel, thus representing
the more realistic and challenging scenario with multi-speaker turns and
cross-talk. Experimental results across single- and multi-speaker conditions
and against conventional ST systems, show that our model outperforms the
reference systems on the multi-speaker condition, while attaining comparable
performance on the single-speaker condition. We release scripts for data
processing and model training.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Thu,  2 Nov 23</h3>
<dl>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00060" title="Abstract">arXiv:2311.00060</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2311.00060" title="Download PDF">pdf</a>, <a href="/format/2311.00060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensemble models outperform single model uncertainties and predictions  for operator-learning of hypersonic flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Leon%2C+V+J">Victor J. Leon</a>, 
<a href="/search/physics?searchtype=author&query=Ford%2C+N">Noah Ford</a>, 
<a href="/search/physics?searchtype=author&query=Mrema%2C+H">Honest Mrema</a>, 
<a href="/search/physics?searchtype=author&query=Gilbert%2C+J">Jeffrey Gilbert</a>, 
<a href="/search/physics?searchtype=author&query=New%2C+A">Alexander New</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">High-fidelity computational simulations and physical experiments of
hypersonic flows are resource intensive. Training scientific machine learning
(SciML) models on limited high-fidelity data offers one approach to rapidly
predict behaviors for situations that have not been seen before. However,
high-fidelity data is itself in limited quantity to validate all outputs of the
SciML model in unexplored input space. As such, an uncertainty-aware SciML
model is desired. The SciML model's output uncertainties could then be used to
assess the reliability and confidence of the model's predictions. In this
study, we extend a DeepONet using three different uncertainty quantification
mechanisms: mean-variance estimation, evidential uncertainty, and ensembling.
The uncertainty aware DeepONet models are trained and evaluated on the
hypersonic flow around a blunt cone object with data generated via
computational fluid dynamics over a wide range of Mach numbers and altitudes.
We find that ensembling outperforms the other two uncertainty models in terms
of minimizing error and calibrating uncertainty in both interpolative and
extrapolative regimes.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00084" title="Abstract">arXiv:2311.00084</a> (cross-list from stat.CO) [<a href="/pdf/2311.00084" title="Download PDF">pdf</a>, <a href="/format/2311.00084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoMoPy: Noise Modeling in Python
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Albrecht%2C+D">Dylan Albrecht</a>, 
<a href="/search/stat?searchtype=author&query=Jacobson%2C+N+T">N. Tobias Jacobson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 55 pages, 68 figures, citation paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Computational Engineering, Finance, and Science (cs.CE); Mathematical Software (cs.MS); Machine Learning (stat.ML)

</div>
<p class="mathjax">NoMoPy is a code for fitting, analyzing, and generating noise modeled as a
hidden Markov model (HMM) or, more generally, factorial hidden Markov model
(FHMM). This code, written in Python, implements approximate and exact
expectation maximization (EM) algorithms for performing the parameter
estimation process, model selection procedures via cross-validation, and
parameter confidence region estimation. Here, we describe in detail the
functionality implemented in NoMoPy and provide examples of its use and
performance on example problems.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00087" title="Abstract">arXiv:2311.00087</a> (cross-list from hep-ph) [<a href="/pdf/2311.00087" title="Download PDF">pdf</a>, <a href="/format/2311.00087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeking Truth and Beauty in Flavor Physics with Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Matchev%2C+K+T">Konstantin T. Matchev</a>, 
<a href="/search/hep-ph?searchtype=author&query=Matcheva%2C+K">Katia Matcheva</a>, 
<a href="/search/hep-ph?searchtype=author&query=Ramond%2C+P">Pierre Ramond</a>, 
<a href="/search/hep-ph?searchtype=author&query=Verner%2C+S">Sarunas Verner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures; Accepted by "AI for Science" NeurIPS 2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG); Mathematical Physics (math-ph)

</div>
<p class="mathjax">The discovery process of building new theoretical physics models involves the
dual aspect of both fitting to the existing experimental data and satisfying
abstract theorists' criteria like beauty, naturalness, etc. We design loss
functions for performing both of those tasks with machine learning techniques.
We use the Yukawa quark sector as a toy example to demonstrate that the
optimization of these loss functions results in true and beautiful models.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00099" title="Abstract">arXiv:2311.00099</a> (cross-list from math.OC) [<a href="/pdf/2311.00099" title="Download PDF">pdf</a>, <a href="/ps/2311.00099" title="Download PostScript">ps</a>, <a href="/format/2311.00099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Integer Convex Quadratic Programming is Fixed Parameter Tractable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Del+Pia%2C+A">Alberto Del Pia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We construct an algorithm for the accurate solution of mixed integer convex
quadratic programming, which is the problem of minimizing a convex quadratic
function over mixed integer points in a polyhedron. Our algorithm is fixed
parameter tractable with parameter the number of integer variables. In
particular, when the number of integer variables is fixed, the running time of
our algorithm is bounded by a polynomial of the size of the problem. To design
our algorithm, we prove a number of fundamental structural and algorithmic
results for mixed integer linear and quadratic programming.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00104" title="Abstract">arXiv:2311.00104</a> (cross-list from eess.SP) [<a href="/pdf/2311.00104" title="Download PDF">pdf</a>, <a href="/format/2311.00104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-functional OFDM Signal Design for Integrated Sensing,  Communications, and Power Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yumeng Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Aditya%2C+S">Sundar Aditya</a>, 
<a href="/search/eess?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The wireless domain is witnessing a flourishing of integrated systems, e.g.
(a) integrated sensing and communications, and (b) simultaneous wireless
information and power transfer, due to their potential to use resources
(spectrum, power) judiciously. Inspired by this trend, we investigate
integrated sensing, communications and powering (ISCAP), through the design of
a wideband OFDM signal to power a sensor while simultaneously performing
target-sensing and communication. To characterize the ISCAP performance region,
we assume symbols with non-zero mean asymmetric Gaussian distribution (i.e.,
the input distribution), and optimize its mean and variance at each subcarrier
to maximize the harvested power, subject to constraints on the achievable rate
(communications) and the average side-to-peak-lobe difference (sensing). The
resulting input distribution, through simulations, achieves a larger
performance region than that of (i) a symmetric complex Gaussian input
distribution with identical mean and variance for the real and imaginary parts,
(ii) a zero-mean symmetric complex Gaussian input distribution, and (iii) the
superposed power-splitting communication and sensing signal (the coexisting
solution). In particular, the optimized input distribution balances the three
functions by exhibiting the following features: (a) symbols in subcarriers with
strong communication channels have high variance to satisfy the rate
constraint, while the other symbols are dominated by the mean, forming a
relatively uniform sum of mean and variance across subcarriers for sensing; (b)
with looser communication and sensing constraints, large absolute means appear
on subcarriers with stronger powering channels for higher harvested power. As a
final note, the results highlight the great potential of the co-designed ISCAP
system for further efficiency enhancement.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00107" title="Abstract">arXiv:2311.00107</a> (cross-list from physics.geo-ph) [<a href="/pdf/2311.00107" title="Download PDF">pdf</a>, <a href="/format/2311.00107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Compressed Learning for 3D Seismic Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gelboim%2C+M">Maayan Gelboim</a>, 
<a href="/search/physics?searchtype=author&query=Adler%2C+A">Amir Adler</a>, 
<a href="/search/physics?searchtype=author&query=Sun%2C+Y">Yen Sun</a>, 
<a href="/search/physics?searchtype=author&query=Araya-Polo%2C+M">Mauricio Araya-Polo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at The International Meeting for Applied Geoscience &amp; Energy (IMAGE23)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider the problem of 3D seismic inversion from pre-stack data using a
very small number of seismic sources. The proposed solution is based on a
combination of compressed-sensing and machine learning frameworks, known as
compressed-learning. The solution jointly optimizes a dimensionality reduction
operator and a 3D inversion encoder-decoder implemented by a deep convolutional
neural network (DCNN). Dimensionality reduction is achieved by learning a
sparse binary sensing layer that selects a small subset of the available
sources, then the selected data is fed to a DCNN to complete the regression
task. The end-to-end learning process provides a reduction by an
order-of-magnitude in the number of seismic records used during training, while
preserving the 3D reconstruction quality comparable to that obtained by using
the entire dataset.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00123" title="Abstract">arXiv:2311.00123</a> (cross-list from math.OC) [<a href="/pdf/2311.00123" title="Download PDF">pdf</a>, <a href="/format/2311.00123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-Learning for Stochastic Control under General Information Structures  and Non-Markovian Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kara%2C+A+D">Ali Devran Kara</a>, 
<a href="/search/math?searchtype=author&query=Yuksel%2C+S">Serdar Yuksel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">As a primary contribution, we present a convergence theorem for stochastic
iterations, and in particular, Q-learning iterates, under a general, possibly
non-Markovian, stochastic environment. Our conditions for convergence involve
an ergodicity and a positivity criterion. We provide a precise characterization
on the limit of the iterates and conditions on the environment and
initializations for convergence. As our second contribution, we discuss the
implications and applications of this theorem to a variety of stochastic
control problems with non-Markovian environments involving (i) quantized
approximations of fully observed Markov Decision Processes (MDPs) with
continuous spaces (where quantization break down the Markovian structure), (ii)
quantized approximations of belief-MDP reduced partially observable MDPS
(POMDPs) with weak Feller continuity and a mild version of filter stability
(which requires the knowledge of the model by the controller), (iii) finite
window approximations of POMDPs under a uniform controlled filter stability
(which does not require the knowledge of the model), and (iv) for multi-agent
models where convergence of learning dynamics to a new class of equilibria,
subjective Q-learning equilibria, will be studied. In addition to the
convergence theorem, some implications of the theorem above are new to the
literature and others are interpreted as applications of the convergence
theorem. Some open problems are noted.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00136" title="Abstract">arXiv:2311.00136</a> (cross-list from q-bio.NC) [<a href="/pdf/2311.00136" title="Download PDF">pdf</a>, <a href="/format/2311.00136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuroformer: Multimodal and Multitask Generative Pretraining for Brain  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Antoniades%2C+A">Antonis Antoniades</a>, 
<a href="/search/q-bio?searchtype=author&query=Yu%2C+Y">Yiyi Yu</a>, 
<a href="/search/q-bio?searchtype=author&query=Canzano%2C+J">Joseph Canzano</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+W">William Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Smith%2C+S+L">Spencer LaVere Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages for main paper. 22 pages in total. 13 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">State-of-the-art systems neuroscience experiments yield large-scale
multimodal data, and these data sets require new tools for analysis. Inspired
by the success of large pretrained models in vision and language domains, we
reframe the analysis of large-scale, cellular-resolution neuronal spiking data
into an autoregressive spatiotemporal generation problem. Neuroformer is a
multimodal, multitask generative pretrained transformer (GPT) model that is
specifically designed to handle the intricacies of data in systems
neuroscience. It scales linearly with feature size, can process an arbitrary
number of modalities, and is adaptable to downstream tasks, such as predicting
behavior. We first trained Neuroformer on simulated datasets, and found that it
both accurately predicted simulated neuronal circuit activity, and also
intrinsically inferred the underlying neural circuit connectivity, including
direction. When pretrained to decode neural responses, the model predicted the
behavior of a mouse with only few-shot fine-tuning, suggesting that the model
begins learning how to do so directly from the neural representations
themselves, without any explicit supervision. We used an ablation study to show
that joint training on neuronal responses and behavior boosted performance,
highlighting the model's ability to associate behavioral and neural
representations in an unsupervised manner. These findings show that Neuroformer
can analyze neural datasets and their emergent properties, informing the
development of models and hypotheses associated with the brain.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00146" title="Abstract">arXiv:2311.00146</a> (cross-list from eess.AS) [<a href="/pdf/2311.00146" title="Download PDF">pdf</a>, <a href="/format/2311.00146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel  Multi-talker ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shao%2C+Y">Yiwen Shao</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+S">Shi-Xiong Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-channel multi-talker automatic speech recognition (ASR) presents
ongoing challenges within the speech community, particularly when confronted
with significant reverberation effects. In this study, we introduce a novel
approach involving the convolution of overlapping speech signals with the room
impulse response (RIR) corresponding to the target speaker's transmission to a
microphone array. This innovative technique yields a novel spatial feature
known as the RIR-SF. Through a comprehensive comparison with the previously
established state-of-the-art 3D spatial feature, both theoretical analysis and
experimental results substantiate the superiority of our proposed RIR-SF. We
demonstrate that the RIR-SF outperforms existing methods, leading to a
remarkable 21.3\% relative reduction in the Character Error Rate (CER) in
multi-channel multi-talker ASR systems. Importantly, this novel feature
exhibits robustness in the face of strong reverberation, surpassing the
limitations of previous approaches.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00149" title="Abstract">arXiv:2311.00149</a> (cross-list from math.OC) [<a href="/pdf/2311.00149" title="Download PDF">pdf</a>, <a href="/format/2311.00149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Knowledge Compilation Take on Binary Polynomial Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Capelli%2C+F">Florent Capelli</a>, 
<a href="/search/math?searchtype=author&query=Del+Pia%2C+A">Alberto Del Pia</a>, 
<a href="/search/math?searchtype=author&query=Di+Gregorio%2C+S">Silvia Di Gregorio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The Binary Polynomial Optimization (BPO) problem is defined as the problem of
maximizing a given polynomial function over all binary points. The main
contribution of this paper is to draw a novel connection between BPO and the
problem of finding the maximal assignment for a Boolean function with weights
on variables. This connection allows us to give a strongly polynomial algorithm
that solves BPO with a hypergraph that is either $\beta$-acyclic or with
bounded incidence treewidth. This result unifies and significantly extends the
known tractable classes of BPO. The generality of our technique allows us to
deal also with extensions of BPO, where we enforce extended cardinality
constraints on the set of binary points, and where we seek $k$ best feasible
solutions. We also extend our results to the significantly more general problem
where variables are replaced by literals. Preliminary computational results
show that the resulting algorithms can be significantly faster than current
state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00154" title="Abstract">arXiv:2311.00154</a> (cross-list from eess.IV) [<a href="/pdf/2311.00154" title="Download PDF">pdf</a>, <a href="/format/2311.00154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medi-CAT: Contrastive Adversarial Training for Medical Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Khan%2C+P+I">Pervaiz Iqbal Khan</a>, 
<a href="/search/eess?searchtype=author&query=Dengel%2C+A">Andreas Dengel</a>, 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+S">Sheraz Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">There are not many large medical image datasets available. For these
datasets, too small deep learning models can't learn useful features, so they
don't work well due to underfitting, and too big models tend to overfit the
limited data. As a result, there is a compromise between the two issues. This
paper proposes a training strategy Medi-CAT to overcome the underfitting and
overfitting phenomena in medical imaging datasets. Specifically, the proposed
training methodology employs large pre-trained vision transformers to overcome
underfitting and adversarial and contrastive learning techniques to prevent
overfitting. The proposed method is trained and evaluated on four medical image
classification datasets from the MedMNIST collection. Our experimental results
indicate that the proposed approach improves the accuracy up to 2% on three
benchmark datasets compared to well-known approaches, whereas it increases the
performance up to 4.1% over the baseline methods.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00181" title="Abstract">arXiv:2311.00181</a> (cross-list from math.OC) [<a href="/pdf/2311.00181" title="Download PDF">pdf</a>, <a href="/format/2311.00181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bhuyan%2C+N">Neelkamal Bhuyan</a>, 
<a href="/search/math?searchtype=author&query=Mukherjee%2C+D">Debankur Mukherjee</a>, 
<a href="/search/math?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">Convex function chasing (CFC) is an online optimization problem in which
during each round $t$, a player plays an action $x_t$ in response to a hitting
cost $f_t(x_t)$ and an additional cost of $c(x_t,x_{t-1})$ for switching
actions. We study the CFC problem in stochastic and adversarial environments,
giving algorithms that achieve performance guarantees simultaneously in both
settings. Specifically, we consider the squared $\ell_2$-norm switching costs
and a broad class of quadratic hitting costs for which the sequence of
minimizers either forms a martingale or is chosen adversarially. This is the
first work that studies the CFC problem using a stochastic framework. We
provide a characterization of the optimal stochastic online algorithm and,
drawing a comparison between the stochastic and adversarial scenarios, we
demonstrate that the adversarial-optimal algorithm exhibits suboptimal
performance in the stochastic context. Motivated by this, we provide a
best-of-both-worlds algorithm that obtains robust adversarial performance while
simultaneously achieving near-optimal stochastic performance.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00185" title="Abstract">arXiv:2311.00185</a> (cross-list from math.OC) [<a href="/pdf/2311.00185" title="Download PDF">pdf</a>, <a href="/ps/2311.00185" title="Download PostScript">ps</a>, <a href="/format/2311.00185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Branch-and-Bound versus Lift-and-Project Relaxations in Combinatorial  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cornu%C3%A9jols%2C+G">G&#xe9;rard Cornu&#xe9;jols</a>, 
<a href="/search/math?searchtype=author&query=Dubey%2C+Y">Yatharth Dubey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this paper, we consider a theoretical framework for comparing
branch-and-bound with classical lift-and-project hierarchies. We simplify our
analysis of streamlining the definition of branch-and-bound. We introduce
"skewed $k$-trees" which give a hierarchy of relaxations that is incomparable
to that of Sherali-Adams, and we show that it is much better for some
instances. We also give an example where lift-and-project does very well and
branch-and-bound does not. Finally, we study the set of branch-and-bound trees
of height at most $k$ and effectively "squeeze" their effectiveness between two
well-known lift-and-project procedures.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00186" title="Abstract">arXiv:2311.00186</a> (cross-list from astro-ph.IM) [<a href="/pdf/2311.00186" title="Download PDF">pdf</a>, <a href="/format/2311.00186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Restoration with Point Spread Function Regularization and Active  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Jia%2C+P">Peng Jia</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lv%2C+J">Jiameng Lv</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ning%2C+R">Runyu Ning</a>, 
<a href="/search/astro-ph?searchtype=author&query=Song%2C+Y">Yu Song</a>, 
<a href="/search/astro-ph?searchtype=author&query=Li%2C+N">Nan Li</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ji%2C+K">Kaifan Ji</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cui%2C+C">Chenzhou Cui</a>, 
<a href="/search/astro-ph?searchtype=author&query=Li%2C+S">Shanshan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the MNRAS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Astrophysics of Galaxies (astro-ph.GA); Solar and Stellar Astrophysics (astro-ph.SR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Large-scale astronomical surveys can capture numerous images of celestial
objects, including galaxies and nebulae. Analysing and processing these images
can reveal intricate internal structures of these objects, allowing researchers
to conduct comprehensive studies on their morphology, evolution, and physical
properties. However, varying noise levels and point spread functions can hamper
the accuracy and efficiency of information extraction from these images. To
mitigate these effects, we propose a novel image restoration algorithm that
connects a deep learning-based restoration algorithm with a high-fidelity
telescope simulator. During the training stage, the simulator generates images
with different levels of blur and noise to train the neural network based on
the quality of restored images. After training, the neural network can directly
restore images obtained by the telescope, as represented by the simulator. We
have tested the algorithm using real and simulated observation data and have
found that it effectively enhances fine structures in blurry images and
increases the quality of observation images. This algorithm can be applied to
large-scale sky survey data, such as data obtained by LSST, Euclid, and CSST,
to further improve the accuracy and efficiency of information extraction,
promoting advances in the field of astronomical research.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00196" title="Abstract">arXiv:2311.00196</a> (cross-list from physics.chem-ph) [<a href="/pdf/2311.00196" title="Download PDF">pdf</a>, <a href="/format/2311.00196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning for accuracy in density functional approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Voss%2C+J">Johannes Voss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning techniques have found their way into computational chemistry
as indispensable tools to accelerate atomistic simulations and materials
design. In addition, machine learning approaches hold the potential to boost
the predictive power of computationally efficient electronic structure methods,
such as density functional theory, to chemical accuracy and to correct for
fundamental errors in density functional approaches. Here, recent progress in
applying machine learning to improve the accuracy of density functional and
related approximations is reviewed. Promises and challenges in devising machine
learning models transferable between different chemistries and materials
classes are discussed with the help of examples applying promising models to
systems far outside their training sets.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00226" title="Abstract">arXiv:2311.00226</a> (cross-list from eess.SP) [<a href="/pdf/2311.00226" title="Download PDF">pdf</a>, <a href="/format/2311.00226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers are Efficient In-Context Estimators for Wireless  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rajagopalan%2C+V">Vicram Rajagopalan</a> (1), 
<a href="/search/eess?searchtype=author&query=Kunde%2C+V+T">Vishnu Teja Kunde</a> (2), 
<a href="/search/eess?searchtype=author&query=Valmeekam%2C+C+S+K">Chandra Shekhara Kaushik Valmeekam</a> (2), 
<a href="/search/eess?searchtype=author&query=Narayanan%2C+K">Krishna Narayanan</a> (2), 
<a href="/search/eess?searchtype=author&query=Shakkottai%2C+S">Srinivas Shakkottai</a> (2), 
<a href="/search/eess?searchtype=author&query=Kalathil%2C+D">Dileep Kalathil</a> (2), 
<a href="/search/eess?searchtype=author&query=Chamberland%2C+J">Jean-Francois Chamberland</a> (2) ((1) Department of Computer Science and Engineering, Texas A&amp;M University, (2) Department of Electrical and Computer Engineering, Texas A&amp;M University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures, 2 tables, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Pre-trained transformers can perform in-context learning, where they adapt to
a new task using only a small number of prompts without any explicit model
optimization. Inspired by this attribute, we propose a novel approach, called
in-context estimation, for the canonical communication problem of estimating
transmitted symbols from received symbols. A communication channel is
essentially a noisy function that maps transmitted symbols to received symbols,
and this function can be represented by an unknown parameter whose statistics
depend on an (also unknown) latent context. Conventional approaches ignore this
hierarchical structure and simply attempt to use known transmissions, called
pilots, to perform a least-squares estimate of the channel parameter, which is
then used to estimate successive, unknown transmitted symbols. We make the
basic connection that transformers show excellent contextual sequence
completion with a few prompts, and so they should be able to implicitly
determine the latent context from pilot symbols to perform end-to-end
in-context estimation of transmitted symbols. Furthermore, the transformer
should use information efficiently, i.e., it should utilize any pilots received
to attain the best possible symbol estimates. Through extensive simulations, we
show that in-context estimation not only significantly outperforms standard
approaches, but also achieves the same performance as an estimator with perfect
knowledge of the latent context within a few context examples. Thus, we make a
strong case that transformers are efficient in-context estimators in the
communication setting.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00235" title="Abstract">arXiv:2311.00235</a> (cross-list from stat.ML) [<a href="/pdf/2311.00235" title="Download PDF">pdf</a>, <a href="/ps/2311.00235" title="Download PostScript">ps</a>, <a href="/format/2311.00235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit biases in multitask and continual learning from a backward  error analysis perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dherin%2C+B">Benoit Dherin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Mathematics of Modern Machine Learning Workshop at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Using backward error analysis, we compute implicit training biases in
multitask and continual learning settings for neural networks trained with
stochastic gradient descent. In particular, we derive modified losses that are
implicitly minimized during training. They have three terms: the original loss,
accounting for convergence, an implicit flatness regularization term
proportional to the learning rate, and a last term, the conflict term, which
can theoretically be detrimental to both convergence and implicit
regularization. In multitask, the conflict term is a well-known quantity,
measuring the gradient alignment between the tasks, while in continual learning
the conflict term is a new quantity in deep learning optimization, although a
basic tool in differential geometry: The Lie bracket between the task
gradients.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00265" title="Abstract">arXiv:2311.00265</a> (cross-list from eess.IV) [<a href="/pdf/2311.00265" title="Download PDF">pdf</a>, <a href="/format/2311.00265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Latent Diffusion Model for 3D Medical Image to Image  Translation: Multi-modal Magnetic Resonance Imaging Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kim%2C+J">Jonghun Kim</a>, 
<a href="/search/eess?searchtype=author&query=Park%2C+H">Hyunjin Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, WACV 2024 Accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multi-modal images play a crucial role in comprehensive evaluations in
medical image analysis providing complementary information for identifying
clinically important biomarkers. However, in clinical practice, acquiring
multiple modalities can be challenging due to reasons such as scan cost,
limited scan time, and safety considerations. In this paper, we propose a model
based on the latent diffusion model (LDM) that leverages switchable blocks for
image-to-image translation in 3D medical images without patch cropping. The 3D
LDM combined with conditioning using the target modality allows generating
high-quality target modality in 3D overcoming the shortcoming of the missing
out-of-slice information in 2D generation methods. The switchable block, noted
as multiple switchable spatially adaptive normalization (MS-SPADE), dynamically
transforms source latents to the desired style of the target latents to help
with the diffusion process. The MS-SPADE block allows us to have one single
model to tackle many translation tasks of one source modality to various
targets removing the need for many translation models for different scenarios.
Our model exhibited successful image synthesis across different source-target
modality scenarios and surpassed other models in quantitative evaluations
tested on multi-modal brain magnetic resonance imaging datasets of four
different modalities and an independent IXI dataset. Our model demonstrated
successful image synthesis across various modalities even allowing for
one-to-many modality translations. Furthermore, it outperformed other
one-to-one translation models in quantitative evaluations.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00274" title="Abstract">arXiv:2311.00274</a> (cross-list from stat.ML) [<a href="/pdf/2311.00274" title="Download PDF">pdf</a>, <a href="/ps/2311.00274" title="Download PostScript">ps</a>, <a href="/format/2311.00274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Bounds for Label Noise Stochastic Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Huh%2C+J+E">Jung Eun Huh</a> (1), 
<a href="/search/stat?searchtype=author&query=Rebeschini%2C+P">Patrick Rebeschini</a> (1) ((1) University of Oxford)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">We develop generalization error bounds for stochastic gradient descent (SGD)
with label noise in non-convex settings under uniform dissipativity and
smoothness conditions. Under a suitable choice of semimetric, we establish a
contraction in Wasserstein distance of the label noise stochastic gradient flow
that depends polynomially on the parameter dimension $d$. Using the framework
of algorithmic stability, we derive time-independent generalisation error
bounds for the discretized algorithm with a constant learning rate. The error
bound we achieve scales polynomially with $d$ and with the rate of $n^{-2/3}$,
where $n$ is the sample size. This rate is better than the best-known rate of
$n^{-1/2}$ established for stochastic gradient Langevin dynamics (SGLD) --
which employs parameter-independent Gaussian noise -- under similar conditions.
Our analysis offers quantitative insights into the effect of label noise.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00332" title="Abstract">arXiv:2311.00332</a> (cross-list from q-bio.TO) [<a href="/pdf/2311.00332" title="Download PDF">pdf</a>, <a href="/format/2311.00332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDF4CHD: Generative Modeling of Cardiac Anatomies with Congenital Heart  Defects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Kong%2C+F">Fanwei Kong</a>, 
<a href="/search/q-bio?searchtype=author&query=Stocker%2C+S">Sascha Stocker</a>, 
<a href="/search/q-bio?searchtype=author&query=Choi%2C+P+S">Perry S. Choi</a>, 
<a href="/search/q-bio?searchtype=author&query=Ma%2C+M">Michael Ma</a>, 
<a href="/search/q-bio?searchtype=author&query=Ennis%2C+D+B">Daniel B. Ennis</a>, 
<a href="/search/q-bio?searchtype=author&query=Marsden%2C+A">Alison Marsden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Tissues and Organs (q-bio.TO)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Congenital heart disease (CHD) encompasses a spectrum of cardiovascular
structural abnormalities, often requiring customized treatment plans for
individual patients. Computational modeling and analysis of these unique
cardiac anatomies can improve diagnosis and treatment planning and may
ultimately lead to improved outcomes. Deep learning (DL) methods have
demonstrated the potential to enable efficient treatment planning by automating
cardiac segmentation and mesh construction for patients with normal cardiac
anatomies. However, CHDs are often rare, making it challenging to acquire
sufficiently large patient cohorts for training such DL models. Generative
modeling of cardiac anatomies has the potential to fill this gap via the
generation of virtual cohorts; however, prior approaches were largely designed
for normal anatomies and cannot readily capture the significant topological
variations seen in CHD patients. Therefore, we propose a type- and
shape-disentangled generative approach suitable to capture the wide spectrum of
cardiac anatomies observed in different CHD types and synthesize differently
shaped cardiac anatomies that preserve the unique topology for specific CHD
types. Our DL approach represents generic whole heart anatomies with CHD
type-specific abnormalities implicitly using signed distance fields (SDF) based
on CHD type diagnosis, which conveniently captures divergent anatomical
variations across different types and represents meaningful intermediate CHD
states. To capture the shape-specific variations, we then learn invertible
deformations to morph the learned CHD type-specific anatomies and reconstruct
patient-specific shapes. Our approach has the potential to augment the
image-segmentation pairs for rarer CHD types for cardiac segmentation and
generate cohorts of CHD cardiac meshes for computational simulation.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00341" title="Abstract">arXiv:2311.00341</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2311.00341" title="Download PDF">pdf</a>, <a href="/format/2311.00341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct  Air Capture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Sriram%2C+A">Anuroop Sriram</a>, 
<a href="/search/cond-mat?searchtype=author&query=Choi%2C+S">Sihoon Choi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Yu%2C+X">Xiaohan Yu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Brabson%2C+L+M">Logan M. Brabson</a>, 
<a href="/search/cond-mat?searchtype=author&query=Das%2C+A">Abhishek Das</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ulissi%2C+Z">Zachary Ulissi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Uyttendaele%2C+M">Matt Uyttendaele</a>, 
<a href="/search/cond-mat?searchtype=author&query=Medford%2C+A+J">Andrew J. Medford</a>, 
<a href="/search/cond-mat?searchtype=author&query=Sholl%2C+D+S">David S. Sholl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">New methods for carbon dioxide removal are urgently needed to combat global
climate change. Direct air capture (DAC) is an emerging technology to capture
carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) have
been widely studied as potentially customizable adsorbents for DAC. However,
discovering promising MOF sorbents for DAC is challenging because of the vast
chemical space to explore and the need to understand materials as functions of
humidity and temperature. We explore a computational approach benefiting from
recent innovations in machine learning (ML) and present a dataset named Open
DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT)
calculations on more than 8,800 MOF materials containing adsorbed CO2 and/or
H2O. ODAC23 is by far the largest dataset of MOF adsorption calculations at the
DFT level of accuracy currently available. In addition to probing properties of
adsorbed molecules, the dataset is a rich source of information on structural
relaxation of MOFs, which will be useful in many contexts beyond specific
applications for DAC. A large number of MOFs with promising properties for DAC
are identified directly in ODAC23. We also trained state-of-the-art ML models
on this dataset to approximate calculations at the DFT level. This open-source
dataset and our initial ML models will provide an important baseline for future
efforts to identify MOFs for a wide range of applications, including DAC.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00364" title="Abstract">arXiv:2311.00364</a> (cross-list from eess.AS) [<a href="/pdf/2311.00364" title="Download PDF">pdf</a>, <a href="/format/2311.00364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C2C: Cough to COVID-19 Detection in BHI 2023 Data Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chung%2C+W">Woo-Jin Chung</a>, 
<a href="/search/eess?searchtype=author&query=Kim%2C+M">Miseul Kim</a>, 
<a href="/search/eess?searchtype=author&query=Kang%2C+H">Hong-Goo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1st place winning paper from the BHI 2023 Data Challenge Competition: Sensor Informatics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD); Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">This report describes our submission to BHI 2023 Data Competition: Sensor
challenge. Our Audio Alchemists team designed an acoustic-based COVID-19
diagnosis system, Cough to COVID-19 (C2C), and won the 1st place in the
challenge. C2C involves three key contributions: pre-processing of input
signals, cough-related representation extraction leveraging Wav2vec2.0, and
data augmentation. Through experimental findings, we demonstrate C2C's
promising potential to enhance the diagnostic accuracy of COVID-19 via cough
signals. Our proposed model achieves a ROC-AUC value of 0.7810 in the context
of COVID-19 diagnosis. The implementation details and the python code can be
found in the following link:
https://github.com/Woo-jin-Chung/BHI_2023_challenge_Audio_Alchemists
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00410" title="Abstract">arXiv:2311.00410</a> (cross-list from physics.app-ph) [<a href="/pdf/2311.00410" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ground-State Probabilistic Logic with the Simplest Binary Energy  Landscape for Probabilistic Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=He%2C+Y">Yihan He</a>, 
<a href="/search/physics?searchtype=author&query=Luo%2C+S">Sheng Luo</a>, 
<a href="/search/physics?searchtype=author&query=Fang%2C+C">Chao Fang</a>, 
<a href="/search/physics?searchtype=author&query=Liang%2C+G">Gengchiau Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Physical Review Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applied Physics (physics.app-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">We investigate the ground-state probabilistic logic based on a binary energy
landscape (GSPL-BEL) model, implementing the many-body interactions within
Ising model cells. The GSPL-BEL model offers a simplified binary energy
landscape, enabling the conversion of traditional CMOS-based logic into a
probabilistic graphical representation based on desired truth tables.
Stochastic Ising cells, coupled with generic probabilistic devices exhibiting
sigmoidal electrical responses, serve as the building blocks of the GSPL-BEL.
Multi-body interactions are realized through cascaded CMOS-based XNOR gates and
a passive resistor network. Through circuit simulations of three-node,
four-node, and five-node systems, the functionality of the GSPL-BEL model is
verified in forward, reverse, and partial-reverse operating modes, and applied
to various arithmetic tasks. The many-body effect provides additional degrees
of freedom in describing the system's energy function, resulting in distinct
energy levels for valid and invalid states. This observation is supported by
the binarized probability distribution observed in the steady state of the
probabilistic circuits. Furthermore, compared to conventional combinatorial
logic circuits, the GSPL-BEL-based circuit design requires a minimal number of
probabilistic devices, as demonstrated in the invertible multiplier/integer
factorizer circuit. These findings highlight the potential of the GSPL-BEL
model for future high-performance logic circuit designs leveraging
probabilistic devices.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00418" title="Abstract">arXiv:2311.00418</a> (cross-list from eess.SP) [<a href="/pdf/2311.00418" title="Download PDF">pdf</a>, <a href="/format/2311.00418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Surface Empowered Integrated Sensing and Communication: From  Coexistence to Reciprocity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Meng%2C+K">Kaitao Meng</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/eess?searchtype=author&query=Masouros%2C+C">Christos Masouros</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+D">Deshi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, submitted to IEEE Journal for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Integrated sensing and communication (ISAC) has attracted growing interests
for sixth-generation (6G) and beyond wireless networks. The primary challenges
faced by highly efficient ISAC include limited sensing and communication (S&amp;C)
coverage, constrained integration gain between S&amp;C under weak channel
correlations, and unknown performance boundary. Intelligent
reflecting/refracting surfaces (IRSs) can effectively expand S&amp;C coverage and
control the degree of freedom of channels between the transmitters and
receivers, thereby realizing increasing integration gains. In this work, we
first delve into the fundamental characteristics of IRS-empowered ISAC and
innovative IRS-assisted sensing architectures. Then, we discuss various
objectives for IRS channel control and deployment optimization in ISAC systems.
Furthermore, the interplay between S&amp;C in different deployment strategies is
investigated and some promising directions for IRS enhanced ISAC are outlined.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00424" title="Abstract">arXiv:2311.00424</a> (cross-list from q-bio.PE) [<a href="/pdf/2311.00424" title="Download PDF">pdf</a>, <a href="/format/2311.00424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking capelin spawning migration -- Integrating environmental data  and Individual-based modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Alrabeei%2C+S">Salah Alrabeei</a>, 
<a href="/search/q-bio?searchtype=author&query=Subbey%2C+S">Sam Subbey</a>, 
<a href="/search/q-bio?searchtype=author&query=Rahman%2C+T">Talal Rahman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This paper presents a modeling framework for tracking the spawning migration
of the capelin, which is a fish species in the Barents Sea. The framework
combines an individual-based model (IBM) with artificial neural networks
(ANNs). The ANNs determine the direction of the fish's movement based on local
environmental information, while a genetic algorithm and fitness function
assess the suitability of the proposed directions. The framework's efficacy is
demonstrated by comparing the spatial distributions of modeled and empirical
potential spawners.
<br />The proposed model successfully replicates the southeastward movement of
capelin during their spawning migration, accurately capturing the distribution
of spawning fish over historical spawning sites along the eastern coast of
northern Norway.
<br />Furthermore, the paper compares three migration models: passive swimmers,
taxis movement based on temperature gradients, and restricted-area search,
along with our proposed approach. The results reveal that our approach
outperforms the other models in mimicking the migration pattern. Most spawning
stocks managed to reach the spawning sites, unlike the other models where water
currents played a significant role in pushing the fish away from the coast. The
temperature gradient detection model and restricted-area search model are found
to be inadequate for accurately simulating capelin spawning migration in the
Barents Sea due to complex oceanographic conditions.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00429" title="Abstract">arXiv:2311.00429</a> (cross-list from eess.IV) [<a href="/pdf/2311.00429" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crop Disease Classification using Support Vector Machines with Green  Chromatic Coordinate (GCC) and Attention based feature extraction for IoT  based Smart Agricultural Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jha%2C+S">Shashwat Jha</a>, 
<a href="/search/eess?searchtype=author&query=Luhach%2C+V">Vishvaditya Luhach</a>, 
<a href="/search/eess?searchtype=author&query=Gupta%2C+G+S">Gauri Shanker Gupta</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+B">Beependra Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Crops hold paramount significance as they serve as the primary provider of
energy, nutrition, and medicinal benefits for the human population. Plant
diseases, however, can negatively affect leaves during agricultural
cultivation, resulting in significant losses in crop output and economic value.
Therefore, it is crucial for farmers to identify crop diseases. However, this
method frequently necessitates hard work, a lot of planning, and in-depth
familiarity with plant pathogens. Given these numerous obstacles, it is
essential to provide solutions that can easily interface with mobile and IoT
devices so that our farmers can guarantee the best possible crop development.
Various machine learning (ML) as well as deep learning (DL) algorithms have
been created &amp; studied for the identification of plant disease detection,
yielding substantial and promising results. This article presents a novel
classification method that builds on prior work by utilising attention-based
feature extraction, RGB channel-based chromatic analysis, Support Vector
Machines (SVM) for improved performance, and the ability to integrate with
mobile applications and IoT devices after quantization of information. Several
disease classification algorithms were compared with the suggested model, and
it was discovered that, in terms of accuracy, Vision Transformer-based feature
extraction and additional Green Chromatic Coordinate feature with SVM
classification achieved an accuracy of (GCCViT-SVM) - 99.69%, whereas after
quantization for IoT device integration achieved an accuracy of - 97.41% while
almost reducing 4x in size. Our findings have profound implications because
they have the potential to transform how farmers identify crop illnesses with
precise and fast information, thereby preserving agricultural output and
ensuring food security.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00463" title="Abstract">arXiv:2311.00463</a> (cross-list from stat.ML) [<a href="/pdf/2311.00463" title="Download PDF">pdf</a>, <a href="/format/2311.00463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and Conjugate Gaussian Process Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Altamirano%2C+M">Matias Altamirano</a>, 
<a href="/search/stat?searchtype=author&query=Briol%2C+F">Fran&#xe7;ois-Xavier Briol</a>, 
<a href="/search/stat?searchtype=author&query=Knoblauch%2C+J">Jeremias Knoblauch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">To enable closed form conditioning, a common assumption in Gaussian process
(GP) regression is independent and identically distributed Gaussian observation
noise. This strong and simplistic assumption is often violated in practice,
which leads to unreliable inferences and uncertainty quantification.
Unfortunately, existing methods for robustifying GPs break closed-form
conditioning, which makes them less attractive to practitioners and
significantly more computationally expensive. In this paper, we demonstrate how
to perform provably robust and conjugate Gaussian process (RCGP) regression at
virtually no additional cost using generalised Bayesian inference. RCGP is
particularly versatile as it enables exact conjugate closed form updates in all
settings where standard GPs admit them. To demonstrate its strong empirical
performance, we deploy RCGP for problems ranging from Bayesian optimisation to
sparse variational Gaussian processes.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00465" title="Abstract">arXiv:2311.00465</a> (cross-list from math.OC) [<a href="/pdf/2311.00465" title="Download PDF">pdf</a>, <a href="/ps/2311.00465" title="Download PostScript">ps</a>, <a href="/format/2311.00465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous SGD on Graphs: a Unified Framework for Asynchronous  Decentralized and Federated Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Even%2C+M">Mathieu Even</a>, 
<a href="/search/math?searchtype=author&query=Koloskova%2C+A">Anastasia Koloskova</a>, 
<a href="/search/math?searchtype=author&query=Massouli%C3%A9%2C+L">Laurent Massouli&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Decentralized and asynchronous communications are two popular techniques to
speedup communication complexity of distributed machine learning, by
respectively removing the dependency over a central orchestrator and the need
for synchronization. Yet, combining these two techniques together still remains
a challenge. In this paper, we take a step in this direction and introduce
Asynchronous SGD on Graphs (AGRAF SGD) -- a general algorithmic framework that
covers asynchronous versions of many popular algorithms including SGD,
Decentralized SGD, Local SGD, FedBuff, thanks to its relaxed communication and
computation assumptions. We provide rates of convergence under much milder
assumptions than previous decentralized asynchronous works, while still
recovering or even improving over the best know results for all the algorithms
covered.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00483" title="Abstract">arXiv:2311.00483</a> (cross-list from eess.IV) [<a href="/pdf/2311.00483" title="Download PDF">pdf</a>, <a href="/format/2311.00483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DEFN: Dual-Encoder Fourier Group Harmonics Network for Three-Dimensional  Macular Hole Reconstruction with Stochastic Retinal Defect Augmentation and  Dynamic Weight Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+X">Xingru Huang</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Y">Yihao Guo</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+J">Jian Huang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhi Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+T">Tianyun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+K">Kunyan Cai</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+G">Gaopeng Huang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Wenhao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Z">Zhaoyang Xu</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+L">Liangqiong Qu</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+J">Ji Hu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+T">Tinyu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+S">Shaowei Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+C">Chenggang Yan</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Y">Yaoqi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+X">Xin Ye</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yaqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25pages,15figures,7tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The spatial and quantitative parameters of macular holes are vital for
diagnosis, surgical choices, and post-op monitoring. Macular hole diagnosis and
treatment rely heavily on spatial and quantitative data, yet the scarcity of
such data has impeded the progress of deep learning techniques for effective
segmentation and real-time 3D reconstruction. To address this challenge, we
assembled the world's largest macular hole dataset, Retinal OCTfor Macular Hole
Enhancement (ROME-3914), and a Comprehensive Archive for Retinal Segmentation
(CARS-30k), both expertly annotated. In addition, we developed an innovative 3D
segmentation network, the Dual-Encoder FuGH Network (DEFN), which integrates
three innovative modules: Fourier Group Harmonics (FuGH), Simplified 3D Spatial
Attention (S3DSA) and Harmonic Squeeze-and-Excitation Module (HSE). These three
modules synergistically filter noise, reduce computational complexity,
emphasize detailed features, and enhance the network's representation ability.
We also proposed a novel data augmentation method, Stochastic Retinal Defect
Injection (SRDI), and a network optimization strategy DynamicWeightCompose
(DWC), to further improve the performance of DEFN. Compared with 13 baselines,
our DEFN shows the best performance. We also offer precise 3D retinal
reconstruction and quantitative metrics, bringing revolutionary diagnostic and
therapeutic decision-making tools for ophthalmologists, and is expected to
completely reshape the diagnosis and treatment patterns of difficult-to-treat
macular degeneration. The source code is publicly available at:
https://github.com/IIPL-HangzhouDianUniversity/DEFN-Pytorch.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00537" title="Abstract">arXiv:2311.00537</a> (cross-list from cond-mat.soft) [<a href="/pdf/2311.00537" title="Download PDF">pdf</a>, <a href="/format/2311.00537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Without a Processor: Emergent Learning in a Nonlinear  Electronic Metamaterial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Dillavou%2C+S">Sam Dillavou</a>, 
<a href="/search/cond-mat?searchtype=author&query=Beyer%2C+B+D">Benjamin D Beyer</a>, 
<a href="/search/cond-mat?searchtype=author&query=Stern%2C+M">Menachem Stern</a>, 
<a href="/search/cond-mat?searchtype=author&query=Miskin%2C+M+Z">Marc Z Miskin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Liu%2C+A+J">Andrea J Liu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Durian%2C+D+J">Douglas J Durian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">Standard deep learning algorithms require differentiating large nonlinear
networks, a process that is slow and power-hungry. Electronic learning
metamaterials offer potentially fast, efficient, and fault-tolerant hardware
for analog machine learning, but existing implementations are linear, severely
limiting their capabilities. These systems differ significantly from artificial
neural networks as well as the brain, so the feasibility and utility of
incorporating nonlinear elements have not been explored. Here we introduce a
nonlinear learning metamaterial -- an analog electronic network made of
self-adjusting nonlinear resistive elements based on transistors. We
demonstrate that the system learns tasks unachievable in linear systems,
including XOR and nonlinear regression, without a computer. We find our
nonlinear learning metamaterial reduces modes of training error in order (mean,
slope, curvature), similar to spectral bias in artificial neural networks. The
circuitry is robust to damage, retrainable in seconds, and performs learned
tasks in microseconds while dissipating only picojoules of energy across each
transistor. This suggests enormous potential for fast, low-power computing in
edge systems like sensors, robotic controllers, and medical devices, as well as
manufacturability at scale for performing and studying emergent learning.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00564" title="Abstract">arXiv:2311.00564</a> (cross-list from stat.ML) [<a href="/pdf/2311.00564" title="Download PDF">pdf</a>, <a href="/format/2311.00564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Student-$t$ Processes with an Overall-local Scale Structure for  Modelling Non-stationary Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sha%2C+T">Taole Sha</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+M+M">Michael Minyi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Time-dependent data often exhibit characteristics, such as non-stationarity
and heavy-tailed errors, that would be inappropriate to model with the typical
assumptions used in popular models. Thus, more flexible approaches are required
to be able to accommodate such issues. To this end, we propose a Bayesian
mixture of student-$t$ processes with an overall-local scale structure for the
covariance. Moreover, we use a sequential Monte Carlo (SMC) sampler in order to
perform online inference as data arrive in real-time. We demonstrate the
superiority of our proposed approach compared to typical Gaussian process-based
models on real-world data sets in order to prove the necessity of using
mixtures of student-$t$ processes.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00567" title="Abstract">arXiv:2311.00567</a> (cross-list from eess.IV) [<a href="/pdf/2311.00567" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Deep Learning Method with Uncertainty Estimation for the  Pathological Classification of Renal Cell Carcinoma based on CT Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yao%2C+N">Ni Yao</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+H">Hang Hu</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+K">Kaicong Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Y">Yuan Guo</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Boya Li</a>, 
<a href="/search/eess?searchtype=author&query=Nan%2C+J">Jiaofen Nan</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yanting Li</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+C">Chuang Han</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+F">Fubao Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+W">Weihua Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Tian%2C+L">Li Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Medical Physics (physics.med-ph); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Objectives To develop and validate a deep learning-based diagnostic model
incorporating uncertainty estimation so as to facilitate radiologists in the
preoperative differentiation of the pathological subtypes of renal cell
carcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients,
pathologically proven RCC, were retrospectively collected from Center 1. By
using five-fold cross-validation, a deep learning model incorporating
uncertainty estimation was developed to classify RCC subtypes into clear cell
RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external
validation set of 78 patients from Center 2 further evaluated the model's
performance. Results In the five-fold cross-validation, the model's area under
the receiver operating characteristic curve (AUC) for the classification of
ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI:
0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external
validation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI:
0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC,
respectively. Conclusions The developed deep learning model demonstrated robust
performance in predicting the pathological subtypes of RCC, while the
incorporated uncertainty emphasized the importance of understanding model
confidence, which is crucial for assisting clinical decision-making for
patients with renal tumors. Clinical relevance statement Our deep learning
approach, integrated with uncertainty estimation, offers clinicians a dual
advantage: accurate RCC subtype predictions complemented by diagnostic
confidence references, promoting informed decision-making for patients with
RCC.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00577" title="Abstract">arXiv:2311.00577</a> (cross-list from stat.ML) [<a href="/pdf/2311.00577" title="Download PDF">pdf</a>, <a href="/format/2311.00577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Assignment to One of Many Treatment Arms via Regularized  and Clustered Joint Assignment Forests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ladhania%2C+R">Rahul Ladhania</a>, 
<a href="/search/stat?searchtype=author&query=Spiess%2C+J">Jann Spiess</a>, 
<a href="/search/stat?searchtype=author&query=Ungar%2C+L">Lyle Ungar</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+W">Wenbo Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME)

</div>
<p class="mathjax">We consider learning personalized assignments to one of many treatment arms
from a randomized controlled trial. Standard methods that estimate
heterogeneous treatment effects separately for each arm may perform poorly in
this case due to excess variance. We instead propose methods that pool
information across treatment arms: First, we consider a regularized
forest-based assignment algorithm based on greedy recursive partitioning that
shrinks effect estimates across arms. Second, we augment our algorithm by a
clustering scheme that combines treatment arms with consistently similar
outcomes. In a simulation study, we compare the performance of these approaches
to predicting arm-wise outcomes separately, and document gains of directly
optimizing the treatment assignment with regularization and clustering. In a
theoretical model, we illustrate how a high number of treatment arms makes
finding the best arm hard, while we can achieve sizable utility gains from
personalization by regularized optimization.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00580" title="Abstract">arXiv:2311.00580</a> (cross-list from stat.ML) [<a href="/pdf/2311.00580" title="Download PDF">pdf</a>, <a href="/ps/2311.00580" title="Download PostScript">ps</a>, <a href="/format/2311.00580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Tails for Normalising Flows, with Application to the Modelling  of Financial Return Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hickling%2C+T">Tennessee Hickling</a>, 
<a href="/search/stat?searchtype=author&query=Prangle%2C+D">Dennis Prangle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a transformation capable of altering the tail properties of a
distribution, motivated by extreme value theory, which can be used as a layer
in a normalizing flow to approximate multivariate heavy tailed distributions.
We apply this approach to model financial returns, capturing potentially
extreme shocks that arise in such data. The trained models can be used directly
to generate new synthetic sets of potentially extreme returns
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00656" title="Abstract">arXiv:2311.00656</a> (cross-list from eess.SP) [<a href="/pdf/2311.00656" title="Download PDF">pdf</a>, <a href="/format/2311.00656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Signal Estimation on the Graph Edges via Line Graph  Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yan%2C+Y">Yi Yan</a>, 
<a href="/search/eess?searchtype=author&query=Kuruoglu%2C+E+E">Ercan Engin Kuruoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose the Line Graph Normalized Least Mean Square (LGNLMS) algorithm for
online time-varying graph edge signals prediction. LGNLMS utilizes the Line
Graph to transform graph edge signals into the node of its edge-to-vertex dual.
This enables edge signals to be processed using established GSP concepts
without redefining them on graph edges.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00674" title="Abstract">arXiv:2311.00674</a> (cross-list from stat.ML) [<a href="/pdf/2311.00674" title="Download PDF">pdf</a>, <a href="/format/2311.00674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovering Linear Causal Models with Latent Variables via Cholesky  Factorization of Covariance Matrix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cai%2C+Y">Yunfeng Cai</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+X">Xu Li</a>, 
<a href="/search/stat?searchtype=author&query=Sun%2C+M">Minging Sun</a>, 
<a href="/search/stat?searchtype=author&query=Li%2C+P">Ping Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Discovering the causal relationship via recovering the directed acyclic graph
(DAG) structure from the observed data is a well-known challenging
combinatorial problem. When there are latent variables, the problem becomes
even more difficult. In this paper, we first propose a DAG structure recovering
algorithm, which is based on the Cholesky factorization of the covariance
matrix of the observed data. The algorithm is fast and easy to implement and
has theoretical grantees for exact recovery. On synthetic and real-world
datasets, the algorithm is significantly faster than previous methods and
achieves the state-of-the-art performance. Furthermore, under the equal error
variances assumption, we incorporate an optimization procedure into the
Cholesky factorization based algorithm to handle the DAG recovering problem
with latent variables. Numerical simulations show that the modified "Cholesky +
optimization" algorithm is able to recover the ground truth graph in most cases
and outperforms existing algorithms.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00678" title="Abstract">arXiv:2311.00678</a> (cross-list from math.OC) [<a href="/pdf/2311.00678" title="Download PDF">pdf</a>, <a href="/format/2311.00678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity of Single Loop Algorithms for Nonlinear Programming with  Stochastic Objective and Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alacaoglu%2C+A">Ahmet Alacaoglu</a>, 
<a href="/search/math?searchtype=author&query=Wright%2C+S+J">Stephen J. Wright</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We analyze the complexity of single-loop quadratic penalty and augmented
Lagrangian algorithms for solving nonconvex optimization problems with
functional equality constraints. We consider three cases, in all of which the
objective is stochastic and smooth, that is, an expectation over an unknown
distribution that is accessed by sampling. The nature of the equality
constraints differs among the three cases: deterministic and linear in the
first case, deterministic, smooth and nonlinear in the second case, and
stochastic, smooth and nonlinear in the third case. Variance reduction
techniques are used to improve the complexity. To find a point that satisfies
$\varepsilon$-approximate first-order conditions, we require
$\widetilde{O}(\varepsilon^{-3})$ complexity in the first case,
$\widetilde{O}(\varepsilon^{-4})$ in the second case, and
$\widetilde{O}(\varepsilon^{-5})$ in the third case. For the first and third
cases, they are the first algorithms of "single loop" type (that also use
$O(1)$ samples at each iteration) that still achieve the best-known complexity
guarantees.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00682" title="Abstract">arXiv:2311.00682</a> (cross-list from physics.ins-det) [<a href="/pdf/2311.00682" title="Download PDF">pdf</a>, <a href="/format/2311.00682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-Based Classification of Gamma Photon Interactions in  Room-Temperature Semiconductor Radiation Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chaudhuri%2C+S+K">Sandeep K. Chaudhuri</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Q">Qinyang Li</a>, 
<a href="/search/physics?searchtype=author&query=Mandal%2C+K+C">Krishna C. Mandal</a>, 
<a href="/search/physics?searchtype=author&query=Hu%2C+J">Jianjun Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Detectors (physics.ins-det)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Photon counting radiation detectors have become an integral part of medical
imaging modalities such as Positron Emission Tomography or Computed Tomography.
One of the most promising detectors is the wide bandgap room temperature
semiconductor detectors, which depends on the interaction gamma/x-ray photons
with the detector material involves Compton scattering which leads to multiple
interaction photon events (MIPEs) of a single photon. For semiconductor
detectors like CdZnTeSe (CZTS), which have a high overlap of detected energies
between Compton and photoelectric events, it is nearly impossible to
distinguish between Compton scattered events from photoelectric events using
conventional readout electronics or signal processing algorithms. Herein, we
report a deep learning classifier CoPhNet that distinguishes between Compton
scattering and photoelectric interactions of gamma/x-ray photons with CdZnTeSe
(CZTS) semiconductor detectors. Our CoPhNet model was trained using simulated
data to resemble actual CZTS detector pulses and validated using both simulated
and experimental data. These results demonstrated that our CoPhNet model can
achieve high classification accuracy over the simulated test set. It also holds
its performance robustness under operating parameter shifts such as
Signal-Noise-Ratio (SNR) and incident energy. Our work thus laid solid
foundation for developing next-generation high energy gamma-rays detectors for
better biomedical imaging.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00701" title="Abstract">arXiv:2311.00701</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2311.00701" title="Download PDF">pdf</a>, <a href="/format/2311.00701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Moving Discontinuous Galerkin Method with Interface Condition  Enforcement for Robust Simulations of High-Speed Viscous Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ching%2C+E+J">Eric J. Ching</a>, 
<a href="/search/physics?searchtype=author&query=Kercher%2C+A+D">Andrew D. Kercher</a>, 
<a href="/search/physics?searchtype=author&query=Corrigan%2C+A">Andrew Corrigan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The moving discontinuous Galerkin method with interface condition enforcement
(MDG-ICE) is a high-order, r-adaptive method that treats the grid as a variable
and weakly enforces the conservation law, constitutive law, and corresponding
interface conditions in order to implicitly fit high-gradient flow features. In
this paper, we introduce nonlinear solver strategies to more robustly and
efficiently compute high-speed viscous flows. Specifically, we incorporate an
anisotropic grid regularization based on the mesh-implied metric into the
nonlinear least-squares solver that inhibits grid motion in directions with
small element length scales. Furthermore, we develop an adaptive elementwise
regularization strategy that locally scales the regularization terms as needed
to maintain grid validity. We apply the proposed MDG-ICE formulation to test
cases involving viscous shocks and/or boundary layers, including Mach 17.6
hypersonic viscous flow over a circular cylinder and Mach 5 hypersonic viscous
flow over a sphere, which are very challenging test cases for conventional
numerical schemes on simplicial grids. Even without artificial dissipation, the
computed solutions are free from spurious oscillations and yield highly
symmetric surface heat-flux profiles.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Thu,  2 Nov 23</h3>
<dl>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1807.06323" title="Abstract">arXiv:1807.06323</a> (replaced) [<a href="/pdf/1807.06323" title="Download PDF">pdf</a>, <a href="/ps/1807.06323" title="Download PostScript">ps</a>, <a href="/format/1807.06323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-optimal Bootstrapping of Hitting Sets for Algebraic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+M">Mrinal Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Saptharishi%2C+R">Ramprasad Saptharishi</a>, 
<a href="/search/cs?searchtype=author&query=Tengse%2C+A">Anamay Tengse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in ToC. The final section now also derives a bootstrapping result from a different hypothesis: e.g. when the number of variables grows with the size
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1906.04112" title="Abstract">arXiv:1906.04112</a> (replaced) [<a href="/pdf/1906.04112" title="Download PDF">pdf</a>, <a href="/format/1906.04112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CUR Low Rank Approximation of a Matrix at Sublinear Cost
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pan%2C+V+Y">Victor Y. Pan</a>, 
<a href="/search/math?searchtype=author&query=Luan%2C+Q">Qi Luan</a>, 
<a href="/search/math?searchtype=author&query=Svadlenka%2C+J">John Svadlenka</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 5 figures, 5 tables. arXiv admin note: text overlap with <a href="/abs/1906.04929">arXiv:1906.04929</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.08689" title="Abstract">arXiv:1911.08689</a> (replaced) [<a href="/pdf/1911.08689" title="Download PDF">pdf</a>, <a href="/ps/1911.08689" title="Download PostScript">ps</a>, <a href="/format/1911.08689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-robust exploration in episodic reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lykouris%2C+T">Thodoris Lykouris</a>, 
<a href="/search/cs?searchtype=author&query=Simchowitz%2C+M">Max Simchowitz</a>, 
<a href="/search/cs?searchtype=author&query=Slivkins%2C+A">Aleksandrs Slivkins</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in Mathematics of Operations Research. Preliminary version was accepted for presentation at COLT'21
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.00473" title="Abstract">arXiv:2108.00473</a> (replaced) [<a href="/pdf/2108.00473" title="Download PDF">pdf</a>, <a href="/format/2108.00473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivative-free Alternating Projection Algorithms for General  Nonconvex-Concave Minimax Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xu%2C+Z">Zi Xu</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+Z">Ziqi Wang</a>, 
<a href="/search/math?searchtype=author&query=Shen%2C+J">Jingjing Shen</a>, 
<a href="/search/math?searchtype=author&query=Dai%2C+Y">Yuhong Dai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.12468" title="Abstract">arXiv:2110.12468</a> (replaced) [<a href="/pdf/2110.12468" title="Download PDF">pdf</a>, <a href="/format/2110.12468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> False Correlation Reduction for Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhihong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zuyue Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+C">Chenjia Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.09598" title="Abstract">arXiv:2201.09598</a> (replaced) [<a href="/pdf/2201.09598" title="Download PDF">pdf</a>, <a href="/format/2201.09598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Optimization Landscape of Dynamic Output Feedback Linear  Quadratic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Duan%2C+J">Jingliang Duan</a>, 
<a href="/search/math?searchtype=author&query=Cao%2C+W">Wenhan Cao</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+Y">Yang Zheng</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+L">Lin Zhao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Automatic Control (full paper), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.03163" title="Abstract">arXiv:2202.03163</a> (replaced) [<a href="/pdf/2202.03163" title="Download PDF">pdf</a>, <a href="/format/2202.03163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Patch-Based Stochastic Attention for Image Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cherel%2C+N">Nicolas Cherel</a>, 
<a href="/search/cs?searchtype=author&query=Almansa%2C+A">Andr&#xe9;s Almansa</a>, 
<a href="/search/cs?searchtype=author&query=Gousseau%2C+Y">Yann Gousseau</a>, 
<a href="/search/cs?searchtype=author&query=Newson%2C+A">Alasdair Newson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 12 figures. Accepted version for publication in Computer Vision and Image Understanding (CVIU)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.12429" title="Abstract">arXiv:2202.12429</a> (replaced) [<a href="/pdf/2202.12429" title="Download PDF">pdf</a>, <a href="/format/2202.12429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BagPipe: Accelerating Deep Recommendation Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Saurabh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Chengpo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Venkataraman%2C+S">Shivaram Venkataraman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.04554" title="Abstract">arXiv:2203.04554</a> (replaced) [<a href="/pdf/2203.04554" title="Download PDF">pdf</a>, <a href="/format/2203.04554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChiTransformer:Towards Reliable Stereo from Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qing Su</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shihao Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a main conference paper at CVPR 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.13741" title="Abstract">arXiv:2204.13741</a> (replaced) [<a href="/pdf/2204.13741" title="Download PDF">pdf</a>, <a href="/format/2204.13741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Arithmetic and Geometric Fusion of Beliefs for Distributed  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kayaalp%2C+M">Mert Kayaalp</a>, 
<a href="/search/eess?searchtype=author&query=Inan%2C+Y">Yunus Inan</a>, 
<a href="/search/eess?searchtype=author&query=Telatar%2C+E">Emre Telatar</a>, 
<a href="/search/eess?searchtype=author&query=Sayed%2C+A+H">Ali H. Sayed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Transactions on Automatic Control
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Multiagent Systems (cs.MA); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07593" title="Abstract">arXiv:2205.07593</a> (replaced) [<a href="/pdf/2205.07593" title="Download PDF">pdf</a>, <a href="/ps/2205.07593" title="Download PostScript">ps</a>, <a href="/format/2205.07593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A $(3+\varepsilon)$-Approximate Correlation Clustering Algorithm in  Dynamic Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cambus%2C+M">M&#xe9;lanie Cambus</a>, 
<a href="/search/cs?searchtype=author&query=Kuhn%2C+F">Fabian Kuhn</a>, 
<a href="/search/cs?searchtype=author&query=Lindy%2C+E">Etna Lindy</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+S">Shreyas Pai</a>, 
<a href="/search/cs?searchtype=author&query=Uitto%2C+J">Jara Uitto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.14590" title="Abstract">arXiv:2205.14590</a> (replaced) [<a href="/pdf/2205.14590" title="Download PDF">pdf</a>, <a href="/ps/2205.14590" title="Download PostScript">ps</a>, <a href="/format/2205.14590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Independent and Decentralized Learning in Markov Potential Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maheshwari%2C+C">Chinmay Maheshwari</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Manxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+D">Druv Pai</a>, 
<a href="/search/cs?searchtype=author&query=Sastry%2C+S">Shankar Sastry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05637" title="Abstract">arXiv:2206.05637</a> (replaced) [<a href="/pdf/2206.05637" title="Download PDF">pdf</a>, <a href="/ps/2206.05637" title="Download PostScript">ps</a>, <a href="/format/2206.05637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence and Stability of Coupled Belief--Strategy Learning Dynamics  in Continuous Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Manxi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Amin%2C+S">Saurabh Amin</a>, 
<a href="/search/cs?searchtype=author&query=Ozdaglar%2C+A">Asuman Ozdaglar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2109.00719">arXiv:2109.00719</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.06448" title="Abstract">arXiv:2206.06448</a> (replaced) [<a href="/pdf/2206.06448" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal  GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bergen%2C+R+V">Robert V. Bergen</a>, 
<a href="/search/eess?searchtype=author&query=Rajotte%2C+J">Jean-Francois Rajotte</a>, 
<a href="/search/eess?searchtype=author&query=Yousefirizi%2C+F">Fereshteh Yousefirizi</a>, 
<a href="/search/eess?searchtype=author&query=Rahmim%2C+A">Arman Rahmim</a>, 
<a href="/search/eess?searchtype=author&query=Ng%2C+R+T">Raymond T. Ng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2111.01866">arXiv:2111.01866</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04053" title="Abstract">arXiv:2207.04053</a> (replaced) [<a href="/pdf/2207.04053" title="Download PDF">pdf</a>, <a href="/format/2207.04053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Need and Applicability of Causality for Fair Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Binkyt%C4%97%2C+R">R&#x16b;ta Binkyt&#x117;</a>, 
<a href="/search/cs?searchtype=author&query=Grozdanovski%2C+L">Ljupcho Grozdanovski</a>, 
<a href="/search/cs?searchtype=author&query=Zhioua%2C+S">Sami Zhioua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.00563" title="Abstract">arXiv:2208.00563</a> (replaced) [<a href="/pdf/2208.00563" title="Download PDF">pdf</a>, <a href="/format/2208.00563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Fidelity in DNN Watermarking: A Study of Backdoor Watermarking for  Classification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+G">Guang Hua</a>, 
<a href="/search/cs?searchtype=author&query=Teoh%2C+A+B+J">Andrew Beng Jin Teoh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Pattern Recognition
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Pattern Recognition, Vol. 144, Dec. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05142" title="Abstract">arXiv:2208.05142</a> (replaced) [<a href="/pdf/2208.05142" title="Download PDF">pdf</a>, <a href="/format/2208.05142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug-and-Play Model-Agnostic Counterfactual Policy Synthesis for Deep  Reinforcement Learning based Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaocong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>, 
<a href="/search/cs?searchtype=author&query=Cripps%2C+S">Sally Cripps</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TNNLS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.10483" title="Abstract">arXiv:2208.10483</a> (replaced) [<a href="/pdf/2208.10483" title="Download PDF">pdf</a>, <a href="/format/2208.10483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prioritizing Samples in Reinforcement Learning with Reducible Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sujit%2C+S">Shivakanth Sujit</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+S">Somjit Nath</a>, 
<a href="/search/cs?searchtype=author&query=Braga%2C+P+H+M">Pedro H. M. Braga</a>, 
<a href="/search/cs?searchtype=author&query=Kahou%2C+S+E">Samira Ebrahimi Kahou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07238" title="Abstract">arXiv:2209.07238</a> (replaced) [<a href="/pdf/2209.07238" title="Download PDF">pdf</a>, <a href="/format/2209.07238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Properties of NAS under Activation and Skip Connection  Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhenyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fanghui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chrysos%2C+G+G">Grigorios G Chrysos</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in NeurIPS 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01327" title="Abstract">arXiv:2210.01327</a> (replaced) [<a href="/pdf/2210.01327" title="Download PDF">pdf</a>, <a href="/format/2210.01327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rainbow spanning trees in randomly coloured $G_{k-out}$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bal%2C+D">Deepak Bal</a>, 
<a href="/search/math?searchtype=author&query=Frieze%2C+A">Alan Frieze</a>, 
<a href="/search/math?searchtype=author&query=Pralat%2C+P">Pawel Pralat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02341" title="Abstract">arXiv:2210.02341</a> (replaced) [<a href="/pdf/2210.02341" title="Download PDF">pdf</a>, <a href="/format/2210.02341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distributed Block-Split Gibbs Sampler with Hypergraph Structure for  High-Dimensional Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Thouvenin%2C+P">Pierre-Antoine Thouvenin</a>, 
<a href="/search/stat?searchtype=author&query=Repetti%2C+A">Audrey Repetti</a>, 
<a href="/search/stat?searchtype=author&query=Chainais%2C+P">Pierre Chainais</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.02361" title="Abstract">arXiv:2210.02361</a> (replaced) [<a href="/pdf/2210.02361" title="Download PDF">pdf</a>, <a href="/format/2210.02361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Power of Duality: Response Time Analysis meets Integer Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deppert%2C+M+A">Max A. Deppert</a>, 
<a href="/search/cs?searchtype=author&query=Jansen%2C+K">Klaus Jansen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.08159" title="Abstract">arXiv:2210.08159</a> (replaced) [<a href="/pdf/2210.08159" title="Download PDF">pdf</a>, <a href="/format/2210.08159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamics-aware Adversarial Attack of Adaptive Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tao%2C+A">An Tao</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yueqi Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiwen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2112.09428">arXiv:2112.09428</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01156" title="Abstract">arXiv:2211.01156</a> (replaced) [<a href="/pdf/2211.01156" title="Download PDF">pdf</a>, <a href="/format/2211.01156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropic Neural Optimal Transport via Diffusion Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gushchin%2C+N">Nikita Gushchin</a>, 
<a href="/search/cs?searchtype=author&query=Kolesov%2C+A">Alexander Kolesov</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>, 
<a href="/search/cs?searchtype=author&query=Vetrov%2C+D">Dmitry Vetrov</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.02920" title="Abstract">arXiv:2211.02920</a> (replaced) [<a href="/pdf/2211.02920" title="Download PDF">pdf</a>, <a href="/format/2211.02920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GmGM: a Fast Multi-Axis Gaussian Graphical Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Andrew%2C+B">Bailey Andrew</a>, 
<a href="/search/stat?searchtype=author&query=Westhead%2C+D">David Westhead</a>, 
<a href="/search/stat?searchtype=author&query=Cutillo%2C+L">Luisa Cutillo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (33 additional in supplementary material), 19 figures, submitted to AIStats
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06665" title="Abstract">arXiv:2211.06665</a> (replaced) [<a href="/pdf/2211.06665" title="Download PDF">pdf</a>, <a href="/format/2211.06665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Explainable Reinforcement Learning: Concepts, Algorithms,  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qing%2C+Y">Yunpeng Qing</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shunyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jie Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huiqiong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10586" title="Abstract">arXiv:2211.10586</a> (replaced) [<a href="/pdf/2211.10586" title="Download PDF">pdf</a>, <a href="/format/2211.10586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Justin Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruochen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+S">Si Si</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cho-Jui Hsieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00302" title="Abstract">arXiv:2212.00302</a> (replaced) [<a href="/pdf/2212.00302" title="Download PDF">pdf</a>, <a href="/ps/2212.00302" title="Download PostScript">ps</a>, <a href="/format/2212.00302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An analysis of the Rayleigh-Ritz and refined Rayleigh-Ritz methods for  nonlinear eigenvalue problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jia%2C+Z">Zhongxiao Jia</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+Q">Qingqing Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.01485" title="Abstract">arXiv:2212.01485</a> (replaced) [<a href="/pdf/2212.01485" title="Download PDF">pdf</a>, <a href="/format/2212.01485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theory of Semantic Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yulin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Gunduz%2C+D">Deniz Gunduz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Keywords: Semantic communication, JSCC, semantic decoding, semantic encoding, large language models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.09699" title="Abstract">arXiv:2212.09699</a> (replaced) [<a href="/pdf/2212.09699" title="Download PDF">pdf</a>, <a href="/format/2212.09699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SegAugment: Maximizing the Utility of Speech Translation Data with  Segmentation-based Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsiamas%2C+I">Ioannis Tsiamas</a>, 
<a href="/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R">Jos&#xe9; A. R. Fonollosa</a>, 
<a href="/search/cs?searchtype=author&query=Costa-juss%C3%A0%2C+M+R">Marta R. Costa-juss&#xe0;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02780" title="Abstract">arXiv:2301.02780</a> (replaced) [<a href="/pdf/2301.02780" title="Download PDF">pdf</a>, <a href="/format/2301.02780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xurui Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yinghui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Radev%2C+D">Dragomir Radev</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhangming Niu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03270" title="Abstract">arXiv:2301.03270</a> (replaced) [<a href="/pdf/2301.03270" title="Download PDF">pdf</a>, <a href="/format/2301.03270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Learning-based Automated Program Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Quanjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chunrong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuxiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weisong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenyu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM Transactions on Software Engineering and Methodology 2023 (TOSEM'23), 69 pages, 6 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03713" title="Abstract">arXiv:2301.03713</a> (replaced) [<a href="/pdf/2301.03713" title="Download PDF">pdf</a>, <a href="/format/2301.03713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-contact Respiratory Anomaly Detection using Infrared Light-wave  Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Islam%2C+M+Z">Md Zobaer Islam</a>, 
<a href="/search/eess?searchtype=author&query=Martin%2C+B">Brenden Martin</a>, 
<a href="/search/eess?searchtype=author&query=Gotcher%2C+C">Carly Gotcher</a>, 
<a href="/search/eess?searchtype=author&query=Martinez%2C+T">Tyler Martinez</a>, 
<a href="/search/eess?searchtype=author&query=O%27Hara%2C+J+F">John F. O&#x27;Hara</a>, 
<a href="/search/eess?searchtype=author&query=Ekin%2C+S">Sabit Ekin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 19 figures including sub-figures and photos of authors, submitted to IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09702" title="Abstract">arXiv:2301.09702</a> (replaced) [<a href="/pdf/2301.09702" title="Download PDF">pdf</a>, <a href="/format/2301.09702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Illumination Variation Correction Using Image Synthesis For Unsupervised  Domain Adaptive Person Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guo%2C+J">Jiaqi Guo</a>, 
<a href="/search/eess?searchtype=author&query=Reibman%2C+A+R">Amy R. Reibman</a>, 
<a href="/search/eess?searchtype=author&query=Delp%2C+E+J">Edward J. Delp</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12141" title="Abstract">arXiv:2301.12141</a> (replaced) [<a href="/pdf/2301.12141" title="Download PDF">pdf</a>, <a href="/format/2301.12141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Decreases Editing Capability? Domain-Specific Hybrid Refinement for  Improved GAN Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Pu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dongxv Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoya Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tianrui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qing Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WACV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12389" title="Abstract">arXiv:2301.12389</a> (replaced) [<a href="/pdf/2301.12389" title="Download PDF">pdf</a>, <a href="/format/2301.12389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Learning Necessary and Sufficient Causal Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hengrui Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M">Michael Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Rui Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Advances in Neural Information Processing Systems 37 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applications (stat.AP); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12713" title="Abstract">arXiv:2301.12713</a> (replaced) [<a href="/pdf/2301.12713" title="Download PDF">pdf</a>, <a href="/format/2301.12713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delayed Stochastic Algorithms for Distributed Weakly Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gao%2C+W">Wenzhi Gao</a>, 
<a href="/search/math?searchtype=author&query=Deng%2C+Q">Qi Deng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13126" title="Abstract">arXiv:2301.13126</a> (replaced) [<a href="/pdf/2301.13126" title="Download PDF">pdf</a>, <a href="/format/2301.13126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niklaus%2C+J">Joel Niklaus</a>, 
<a href="/search/cs?searchtype=author&query=Matoshi%2C+V">Veton Matoshi</a>, 
<a href="/search/cs?searchtype=author&query=Rani%2C+P">Pooja Rani</a>, 
<a href="/search/cs?searchtype=author&query=Galassi%2C+A">Andrea Galassi</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%BCrmer%2C+M">Matthias St&#xfc;rmer</a>, 
<a href="/search/cs?searchtype=author&query=Chalkidis%2C+I">Ilias Chalkidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EMNLP Findings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00171" title="Abstract">arXiv:2302.00171</a> (replaced) [<a href="/pdf/2302.00171" title="Download PDF">pdf</a>, <a href="/format/2302.00171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Uncertainty Reduction for Safe and Efficient Interaction  Planning: A Shielding-Aware Dual Control Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haimin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Isele%2C+D">David Isele</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Sangjae Bae</a>, 
<a href="/search/cs?searchtype=author&query=Fisac%2C+J+F">Jaime F. Fisac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The International Journal of Robotics Research. arXiv admin note: text overlap with <a href="/abs/2202.07720">arXiv:2202.07720</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00482" title="Abstract">arXiv:2302.00482</a> (replaced) [<a href="/pdf/2302.00482" title="Download PDF">pdf</a>, <a href="/format/2302.00482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving and generalizing flow-based generative models with minibatch  optimal transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+A">Alexander Tong</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Huguet%2C+G">Guillaume Huguet</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanlei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rector-Brooks%2C+J">Jarrid Rector-Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Fatras%2C+K">Kilian Fatras</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+G">Guy Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An earlier version of this paper appeared in the New Frontiers in Learning, Control, and Dynamical Systems workshop at ICML 2023. Code: <a href="https://github.com/atong01/conditional-flow-matching">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00997" title="Abstract">arXiv:2302.00997</a> (replaced) [<a href="/pdf/2302.00997" title="Download PDF">pdf</a>, <a href="/format/2302.00997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Online Two-stage Stochastic Optimization: Near Optimal  Algorithms via Adversarial Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiashuo Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01477" title="Abstract">arXiv:2302.01477</a> (replaced) [<a href="/pdf/2302.01477" title="Download PDF">pdf</a>, <a href="/ps/2302.01477" title="Download PostScript">ps</a>, <a href="/format/2302.01477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Reduction-based Framework for Sequential Decision Making with Delayed  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yunchang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01923" title="Abstract">arXiv:2302.01923</a> (replaced) [<a href="/pdf/2302.01923" title="Download PDF">pdf</a>, <a href="/format/2302.01923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Traffic End-of-Queue Detection and Tracking in UAV Video
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Messenger%2C+R">Russ Messenger</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+Z">Md Zobaer Islam</a>, 
<a href="/search/cs?searchtype=author&query=Whitlock%2C+M">Matthew Whitlock</a>, 
<a href="/search/cs?searchtype=author&query=Spong%2C+E">Erik Spong</a>, 
<a href="/search/cs?searchtype=author&query=Morton%2C+N">Nate Morton</a>, 
<a href="/search/cs?searchtype=author&query=Claggett%2C+L">Layne Claggett</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+C">Chris Matthews</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+J">Jordan Fox</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+L">Leland Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+D+C">Dane C. Johnson</a>, 
<a href="/search/cs?searchtype=author&query=O%27Hara%2C+J+F">John F. O&#x27;Hara</a>, 
<a href="/search/cs?searchtype=author&query=Crick%2C+C+J">Christopher J. Crick</a>, 
<a href="/search/cs?searchtype=author&query=Jacob%2C+J+D">Jamey D. Jacob</a>, 
<a href="/search/cs?searchtype=author&query=Ekin%2C+S">Sabit Ekin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures excluding photos of authors, Published in International Journal of Intelligent Transportation Systems Research. Link to the published version: <a href="https://link.springer.com/article/10.1007/s13177-023-00374-0">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02713" title="Abstract">arXiv:2302.02713</a> (replaced) [<a href="/pdf/2302.02713" title="Download PDF">pdf</a>, <a href="/format/2302.02713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flat Seeking Bayesian Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Van-Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+T">Tung-Long Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hoang Phan</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+T">Thanh-Toan Do</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03693" title="Abstract">arXiv:2302.03693</a> (replaced) [<a href="/pdf/2302.03693" title="Download PDF">pdf</a>, <a href="/format/2302.03693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concept Algebra for (Score-Based) Text-Controlled Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=Negrea%2C+J">Jeffrey Negrea</a>, 
<a href="/search/cs?searchtype=author&query=Veitch%2C+V">Victor Veitch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03805" title="Abstract">arXiv:2302.03805</a> (replaced) [<a href="/pdf/2302.03805" title="Download PDF">pdf</a>, <a href="/ps/2302.03805" title="Download PostScript">ps</a>, <a href="/format/2302.03805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eliciting User Preferences for Personalized Multi-Objective Decision  Making through Comparative Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Han Shao</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+L">Lee Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Blum%2C+A">Avrim Blum</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+Y">Yishay Mansour</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Aadirupa Saha</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+M+R">Matthew R. Walter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04841" title="Abstract">arXiv:2302.04841</a> (replaced) [<a href="/pdf/2302.04841" title="Download PDF">pdf</a>, <a href="/format/2302.04841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is This Loss Informative? Faster Text-to-Image Customization by Tracking  Objective Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Voronov%2C+A">Anton Voronov</a>, 
<a href="/search/cs?searchtype=author&query=Khoroshikh%2C+M">Mikhail Khoroshikh</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>, 
<a href="/search/cs?searchtype=author&query=Ryabinin%2C+M">Max Ryabinin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Conference on Neural Information Processing Systems (NeurIPS) 2023. 20 pages, 15 figures. Code: <a href="https://github.com/yandex-research/DVAR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07253" title="Abstract">arXiv:2302.07253</a> (replaced) [<a href="/pdf/2302.07253" title="Download PDF">pdf</a>, <a href="/format/2302.07253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoover%2C+B">Benjamin Hoover</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuchen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+B">Bao Pham</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>, 
<a href="/search/cs?searchtype=author&query=Strobelt%2C+H">Hendrik Strobelt</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>, 
<a href="/search/cs?searchtype=author&query=Zaki%2C+M+J">Mohammed J. Zaki</a>, 
<a href="/search/cs?searchtype=author&query=Krotov%2C+D">Dmitry Krotov</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08023" title="Abstract">arXiv:2302.08023</a> (replaced) [<a href="/pdf/2302.08023" title="Download PDF">pdf</a>, <a href="/format/2302.08023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object-centric Learning with Cyclic Walks between Parts and Whole
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09425" title="Abstract">arXiv:2302.09425</a> (replaced) [<a href="/pdf/2302.09425" title="Download PDF">pdf</a>, <a href="/format/2302.09425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Neurodiversity-Inspired Solver for the Abstraction \&amp; Reasoning Corpus  (ARC) Using Visual Imagery and Program Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ainooson%2C+J">James Ainooson</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+D">Deepayan Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Michelson%2C+J+P">Joel P. Michelson</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kunda%2C+M">Maithilee Kunda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09933" title="Abstract">arXiv:2302.09933</a> (replaced) [<a href="/pdf/2302.09933" title="Download PDF">pdf</a>, <a href="/ps/2302.09933" title="Download PostScript">ps</a>, <a href="/format/2302.09933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mysterious and Manipulative Black Boxes: A Qualitative Analysis of  Perceptions on Recommender Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruohonen%2C+J">Jukka Ruohonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10890" title="Abstract">arXiv:2302.10890</a> (replaced) [<a href="/pdf/2302.10890" title="Download PDF">pdf</a>, <a href="/format/2302.10890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Interpretable Low-dimensional Representation via Physical  Symmetry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuanjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+D">Daniel Chin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yichen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+G">Gus Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13875" title="Abstract">arXiv:2302.13875</a> (replaced) [<a href="/pdf/2302.13875" title="Download PDF">pdf</a>, <a href="/format/2302.13875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Robustness and Uncertainty of Graph Models Under Structural  Distributional Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazhenov%2C+G">Gleb Bazhenov</a>, 
<a href="/search/cs?searchtype=author&query=Kuznedelev%2C+D">Denis Kuznedelev</a>, 
<a href="/search/cs?searchtype=author&query=Malinin%2C+A">Andrey Malinin</a>, 
<a href="/search/cs?searchtype=author&query=Babenko%2C+A">Artem Babenko</a>, 
<a href="/search/cs?searchtype=author&query=Prokhorenkova%2C+L">Liudmila Prokhorenkova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01353" title="Abstract">arXiv:2303.01353</a> (replaced) [<a href="/pdf/2303.01353" title="Download PDF">pdf</a>, <a href="/format/2303.01353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penalising the biases in norm regularisation enforces sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Boursier%2C+E">Etienne Boursier</a>, 
<a href="/search/stat?searchtype=author&query=Flammarion%2C+N">Nicolas Flammarion</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01772" title="Abstract">arXiv:2303.01772</a> (replaced) [<a href="/pdf/2303.01772" title="Download PDF">pdf</a>, <a href="/format/2303.01772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Energy Market Clearing and Bidding With Model-Based  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wolgast%2C+T">Thomas Wolgast</a>, 
<a href="/search/eess?searchtype=author&query=Nie%C3%9Fe%2C+A">Astrid Nie&#xdf;e</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02257" title="Abstract">arXiv:2303.02257</a> (replaced) [<a href="/pdf/2303.02257" title="Download PDF">pdf</a>, <a href="/format/2303.02257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Perception System for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+S">Siyuan Gou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenbin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02347" title="Abstract">arXiv:2303.02347</a> (replaced) [<a href="/pdf/2303.02347" title="Download PDF">pdf</a>, <a href="/format/2303.02347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaGrad: Adaptive Gradient Quantization with Hypernetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaixin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+A+H+X">Alina Hui Xiu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weisi Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02863" title="Abstract">arXiv:2303.02863</a> (replaced) [<a href="/pdf/2303.02863" title="Download PDF">pdf</a>, <a href="/ps/2303.02863" title="Download PostScript">ps</a>, <a href="/format/2303.02863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on the Proposed Law for Improving the Transparency of Political  Advertising in the European Union
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruohonen%2C+J">Jukka Ruohonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A continuously updated deliberative working paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03196" title="Abstract">arXiv:2303.03196</a> (replaced) [<a href="/pdf/2303.03196" title="Download PDF">pdf</a>, <a href="/format/2303.03196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Population-based Evaluation in Repeated Rock-Paper-Scissors as a  Benchmark for Multiagent Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Schultz%2C+J">John Schultz</a>, 
<a href="/search/cs?searchtype=author&query=Burch%2C+N">Neil Burch</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+M+O">Max Olan Smith</a>, 
<a href="/search/cs?searchtype=author&query=Hennes%2C+D">Daniel Hennes</a>, 
<a href="/search/cs?searchtype=author&query=Anthony%2C+T">Thomas Anthony</a>, 
<a href="/search/cs?searchtype=author&query=Perolat%2C+J">Julien Perolat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 8 figures, Accepted at TMLR October 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03227" title="Abstract">arXiv:2303.03227</a> (replaced) [<a href="/pdf/2303.03227" title="Download PDF">pdf</a>, <a href="/format/2303.03227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Hybrid Networks: an interplay between quantum and classical  neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kordzanganeh%2C+M">Mo Kordzanganeh</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kosichkina%2C+D">Daria Kosichkina</a>, 
<a href="/search/quant-ph?searchtype=author&query=Melnikov%2C+A">Alexey Melnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Intell. Comput. 2, 0028 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06530" title="Abstract">arXiv:2303.06530</a> (replaced) [<a href="/pdf/2303.06530" title="Download PDF">pdf</a>, <a href="/format/2303.06530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Batch Normalization Great in Federated Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+J">Jike Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong-You Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chao%2C+W">Wei-Lun Chao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08050" title="Abstract">arXiv:2303.08050</a> (replaced) [<a href="/pdf/2303.08050" title="Download PDF">pdf</a>, <a href="/format/2303.08050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subjective and Objective Quality Assessment for in-the-Wild Computer  Graphics Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhichao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+X">Xiongkuo Min</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11340" title="Abstract">arXiv:2303.11340</a> (replaced) [<a href="/pdf/2303.11340" title="Download PDF">pdf</a>, <a href="/format/2303.11340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HDformer: A Higher Dimensional Transformer for Diabetes Detection  Utilizing Long Range Vascular Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lan%2C+E">Ella Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12040" title="Abstract">arXiv:2303.12040</a> (replaced) [<a href="/pdf/2303.12040" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Roots and Requirements for Collaborative AIs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stefik%2C+M">Mark Stefik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15422" title="Abstract">arXiv:2303.15422</a> (replaced) [<a href="/pdf/2303.15422" title="Download PDF">pdf</a>, <a href="/format/2303.15422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase  Extraction and Generation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Da Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18059" title="Abstract">arXiv:2303.18059</a> (replaced) [<a href="/pdf/2303.18059" title="Download PDF">pdf</a>, <a href="/format/2303.18059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inferring networks from time series: a neural approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaskin%2C+T">Thomas Gaskin</a>, 
<a href="/search/cs?searchtype=author&query=Pavliotis%2C+G+A">Grigorios A. Pavliotis</a>, 
<a href="/search/cs?searchtype=author&query=Girolami%2C+M">Mark Girolami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.18211" title="Abstract">arXiv:2303.18211</a> (replaced) [<a href="/pdf/2303.18211" title="Download PDF">pdf</a>, <a href="/format/2303.18211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive  Noise Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Reisach%2C+A+G">Alexander G. Reisach</a>, 
<a href="/search/stat?searchtype=author&query=Tami%2C+M">Myriam Tami</a>, 
<a href="/search/stat?searchtype=author&query=Seiler%2C+C">Christof Seiler</a>, 
<a href="/search/stat?searchtype=author&query=Chambaz%2C+A">Antoine Chambaz</a>, 
<a href="/search/stat?searchtype=author&query=Weichwald%2C+S">Sebastian Weichwald</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted at the 2023 Conference on Neural Information Processing Systems (NeurIPS 2023); cf. <a href="https://openreview.net/forum?id=nrbR2F29vU">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00922" title="Abstract">arXiv:2304.00922</a> (replaced) [<a href="/pdf/2304.00922" title="Download PDF">pdf</a>, <a href="/ps/2304.00922" title="Download PostScript">ps</a>, <a href="/format/2304.00922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $L_{\infty}$ norm minimization for nowhere-zero integer eigenvectors of  the block graphs of Steiner triple systems and Johnson graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bespalov%2C+E+A">E.A. Bespalov</a>, 
<a href="/search/math?searchtype=author&query=Mogilnykh%2C+I+Y">I.Yu. Mogilnykh</a>, 
<a href="/search/math?searchtype=author&query=Vorob%27ev%2C+K+V">K.V. Vorob&#x27;ev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Siberian Electronic Mathematical Reports
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05197" title="Abstract">arXiv:2304.05197</a> (replaced) [<a href="/pdf/2304.05197" title="Download PDF">pdf</a>, <a href="/ps/2304.05197" title="Download PostScript">ps</a>, <a href="/format/2304.05197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-step Jailbreaking Privacy Attacks on ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Dadi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingshi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanpu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Findings of EMNLP 2023. Updated with results on open-source LLMs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05589" title="Abstract">arXiv:2304.05589</a> (replaced) [<a href="/pdf/2304.05589" title="Download PDF">pdf</a>, <a href="/format/2304.05589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Structure From Corruption for Unsupervised Image  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Leong%2C+O">Oscar Leong</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+A+F">Angela F. Gao</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+H">He Sun</a>, 
<a href="/search/eess?searchtype=author&query=Bouman%2C+K+L">Katherine L. Bouman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of <a href="/abs/2303.12217">arXiv:2303.12217</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09242" title="Abstract">arXiv:2304.09242</a> (replaced) [<a href="/pdf/2304.09242" title="Download PDF">pdf</a>, <a href="/format/2304.09242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Analyzing Cross-correlators using Price&#x27;s Theorem and  Piecewise-Linear Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhili Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabartty%2C+S">Shantanu Chakrabartty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10001" title="Abstract">arXiv:2304.10001</a> (replaced) [<a href="/pdf/2304.10001" title="Download PDF">pdf</a>, <a href="/format/2304.10001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Detection of Baby Cry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+W">Weijun Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11839" title="Abstract">arXiv:2304.11839</a> (replaced) [<a href="/pdf/2304.11839" title="Download PDF">pdf</a>, <a href="/ps/2304.11839" title="Download PostScript">ps</a>, <a href="/format/2304.11839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Energy Distribution Based Hyperparameter Determination for  Stochastic Simulated Annealing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Onizawa%2C+N">Naoya Onizawa</a>, 
<a href="/search/cs?searchtype=author&query=Kuroki%2C+K">Kyo Kuroki</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+D">Duckgyu Shin</a>, 
<a href="/search/cs?searchtype=author&query=Hanyu%2C+T">Takahiro Hanyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13787" title="Abstract">arXiv:2304.13787</a> (replaced) [<a href="/pdf/2304.13787" title="Download PDF">pdf</a>, <a href="/format/2304.13787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surrogate Assisted Generation of Human-Robot Interaction Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+V">Varun Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Nemlekar%2C+H">Heramb Nemlekar</a>, 
<a href="/search/cs?searchtype=author&query=Fontaine%2C+M+C">Matthew C. Fontaine</a>, 
<a href="/search/cs?searchtype=author&query=Tjanaka%2C+B">Bryon Tjanaka</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hejia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+Y">Ya-Chuan Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaidis%2C+S">Stefanos Nikolaidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages; 12 figures; 3 tables; Accepted for oral presentation at CoRL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05276" title="Abstract">arXiv:2305.05276</a> (replaced) [<a href="/pdf/2305.05276" title="Download PDF">pdf</a>, <a href="/format/2305.05276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Discovery from Subsampled Time Series with Proxy Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mingzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lingjing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07539" title="Abstract">arXiv:2305.07539</a> (replaced) [<a href="/pdf/2305.07539" title="Download PDF">pdf</a>, <a href="/ps/2305.07539" title="Download PostScript">ps</a>, <a href="/format/2305.07539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling recovery in $L_2$ and other norms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Krieg%2C+D">David Krieg</a>, 
<a href="/search/math?searchtype=author&query=Pozharska%2C+K">Kateryna Pozharska</a>, 
<a href="/search/math?searchtype=author&query=Ullrich%2C+M">Mario Ullrich</a>, 
<a href="/search/math?searchtype=author&query=Ullrich%2C+T">Tino Ullrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The title has changed slightly. Some results from earlier versions are shifted to another publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07984" title="Abstract">arXiv:2305.07984</a> (replaced) [<a href="/pdf/2305.07984" title="Download PDF">pdf</a>, <a href="/format/2305.07984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative  Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Deqing Fu</a>, 
<a href="/search/cs?searchtype=author&query=Godbole%2C+A">Ameya Godbole</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08229" title="Abstract">arXiv:2305.08229</a> (replaced) [<a href="/pdf/2305.08229" title="Download PDF">pdf</a>, <a href="/format/2305.08229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid 3D Eddy Detection Technique Based on Sea Surface Height and  Velocity Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Weiping Hua</a>, 
<a href="/search/cs?searchtype=author&query=Bemis%2C+K">Karen Bemis</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dujuan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Ozer%2C+S">Sedat Ozer</a>, 
<a href="/search/cs?searchtype=author&query=Silver%2C+D">Deborah Silver</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 14 figures. Accepted by EnvirVis 2023. Project Link: <a href="https://github.com/VizlabRutgers/Hybrid-Eddy-detection">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08503" title="Abstract">arXiv:2305.08503</a> (replaced) [<a href="/pdf/2305.08503" title="Download PDF">pdf</a>, <a href="/format/2305.08503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chenhui Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Liying Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+X">Xuan-Phi Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+L">Lidong Bing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Findings of EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10657" title="Abstract">arXiv:2305.10657</a> (replaced) [<a href="/pdf/2305.10657" title="Download PDF">pdf</a>, <a href="/format/2305.10657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PTQD: Accurate Post-Training Quantization for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yefei He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Luping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Weijia Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+B">Bohan Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12171" title="Abstract">arXiv:2305.12171</a> (replaced) [<a href="/pdf/2305.12171" title="Download PDF">pdf</a>, <a href="/format/2305.12171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Co-Policy for Synergistic Human-Robot Collaborative Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ng%2C+E">Eley Ng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+M">Monroe Kennedy III</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Robotics and Automation Letters (RA-L), 2023. 8 pages, 7 figures, 3 tables. Supplementary material at <a href="https://sites.google.com/view/diffusion-co-policy-hrc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12661" title="Abstract">arXiv:2305.12661</a> (replaced) [<a href="/pdf/2305.12661" title="Download PDF">pdf</a>, <a href="/format/2305.12661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic-guided spatial relation and object co-occurrence modeling for  indoor scene recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chuanxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hanbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yibin Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12685" title="Abstract">arXiv:2305.12685</a> (replaced) [<a href="/pdf/2305.12685" title="Download PDF">pdf</a>, <a href="/format/2305.12685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoised Self-Augmented Learning for Social Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianle Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, published by IJCAI2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14585" title="Abstract">arXiv:2305.14585</a> (replaced) [<a href="/pdf/2305.14585" title="Download PDF">pdf</a>, <a href="/format/2305.14585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful and Efficient Explanations for Neural Networks via Neural  Tangent Kernel Surrogate Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Engel%2C+A">Andrew Engel</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Frank%2C+N+S">Natalie S. Frank</a>, 
<a href="/search/cs?searchtype=author&query=Dumitriu%2C+I">Ioana Dumitriu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>, 
<a href="/search/cs?searchtype=author&query=Sarwate%2C+A">Anand Sarwate</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Tony Chiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 3 tables Updated 11/1/2023 added hyperlinks to github repo, <a href="https://github.com/pnnl/projection_ntk.">this https URL</a> Updated 10/9/2023: Errata re. trNTK implementation for poisoned data experiments. Changes do not change our conclusions. Github repository will be posted soon. Updated 10/4/2023: significant changes for ICLR2024 submission. Github repository will be live soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14596" title="Abstract">arXiv:2305.14596</a> (replaced) [<a href="/pdf/2305.14596" title="Download PDF">pdf</a>, <a href="/format/2305.14596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing Probability Mass on Answer Choices Does Not Always Improve  Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiegreffe%2C+S">Sarah Wiegreffe</a>, 
<a href="/search/cs?searchtype=author&query=Finlayson%2C+M">Matthew Finlayson</a>, 
<a href="/search/cs?searchtype=author&query=Tafjord%2C+O">Oyvind Tafjord</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashish Sabharwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16381" title="Abstract">arXiv:2305.16381</a> (replaced) [<a href="/pdf/2305.16381" title="Download PDF">pdf</a>, <a href="/format/2305.16381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Ying Fan</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+O">Olivia Watkins</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuqing Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+M">Moonkyung Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Boutilier%2C+C">Craig Boutilier</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Ghavamzadeh%2C+M">Mohammad Ghavamzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kangwook Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kimin Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16698" title="Abstract">arXiv:2305.16698</a> (replaced) [<a href="/pdf/2305.16698" title="Download PDF">pdf</a>, <a href="/format/2305.16698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detect Any Shadow: Segment Anything for Video Shadow Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yonghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wengang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yunyao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Houqiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17037" title="Abstract">arXiv:2305.17037</a> (replaced) [<a href="/pdf/2305.17037" title="Download PDF">pdf</a>, <a href="/format/2305.17037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributionally Robust Linear Quadratic Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ta%C5%9Fkesen%2C+B">Bahar Ta&#x15f;kesen</a>, 
<a href="/search/math?searchtype=author&query=Iancu%2C+D+A">Dan A. Iancu</a>, 
<a href="/search/math?searchtype=author&query=Ko%C3%A7yi%C4%9Fit%2C+%C3%87">&#xc7;a&#x11f;&#x131;l Ko&#xe7;yi&#x11f;it</a>, 
<a href="/search/math?searchtype=author&query=Kuhn%2C+D">Daniel Kuhn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17552" title="Abstract">arXiv:2305.17552</a> (replaced) [<a href="/pdf/2305.17552" title="Download PDF">pdf</a>, <a href="/format/2305.17552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Nonstochastic Model-Free Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghai%2C+U">Udaya Ghai</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Arushi Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Wenhan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Karan Singh</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready version for NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18910" title="Abstract">arXiv:2305.18910</a> (replaced) [<a href="/pdf/2305.18910" title="Download PDF">pdf</a>, <a href="/format/2305.18910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precision-Recall Divergence Optimization for Generative Modeling with  GANs and Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verine%2C+A">Alexandre Verine</a>, 
<a href="/search/cs?searchtype=author&query=Negrevergne%2C+B">Benjamin Negrevergne</a>, 
<a href="/search/cs?searchtype=author&query=Pydi%2C+M+S">Muni Sreenivas Pydi</a>, 
<a href="/search/cs?searchtype=author&query=Chevaleyre%2C+Y">Yann Chevaleyre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19277" title="Abstract">arXiv:2305.19277</a> (replaced) [<a href="/pdf/2305.19277" title="Download PDF">pdf</a>, <a href="/ps/2305.19277" title="Download PostScript">ps</a>, <a href="/format/2305.19277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corrigendum to &quot;On the monophonic rank of a graph&quot; [Discrete Math.  Theor. Comput. Sci. 24:1 (2022) #3]
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dourado%2C+M+C">Mitre C. Dourado</a>, 
<a href="/search/math?searchtype=author&query=Ponciano%2C+V+S">Vitor S. Ponciano</a>, 
<a href="/search/math?searchtype=author&query=da+Silva%2C+R+L+O">R&#xf4;mulo L. O. da Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00809" title="Abstract">arXiv:2306.00809</a> (replaced) [<a href="/pdf/2306.00809" title="Download PDF">pdf</a>, <a href="/format/2306.00809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initial Guessing Bias: How Untrained Networks Favor Some Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Francazi%2C+E">Emanuele Francazi</a>, 
<a href="/search/cs?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>, 
<a href="/search/cs?searchtype=author&query=Baity-Jesi%2C+M">Marco Baity-Jesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01103" title="Abstract">arXiv:2306.01103</a> (replaced) [<a href="/pdf/2306.01103" title="Download PDF">pdf</a>, <a href="/format/2306.01103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Learning of Label and Environment Causal Independence for Graph  Out-of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gui%2C+S">Shurui Gui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiner Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Youzhi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shuiwang Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02426" title="Abstract">arXiv:2306.02426</a> (replaced) [<a href="/pdf/2306.02426" title="Download PDF">pdf</a>, <a href="/format/2306.02426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient Constrained Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hounie%2C+I">Ignacio Hounie</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Chamon%2C+L+F+O">Luiz F. O. Chamon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04178" title="Abstract">arXiv:2306.04178</a> (replaced) [<a href="/pdf/2306.04178" title="Download PDF">pdf</a>, <a href="/format/2306.04178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport Model Distributional Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V">Van-Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+A+T">Anh Tuan Bui</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+T">Thanh-Toan Do</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPs 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04723" title="Abstract">arXiv:2306.04723</a> (replaced) [<a href="/pdf/2306.04723" title="Download PDF">pdf</a>, <a href="/format/2306.04723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intrinsic Dimension Estimation for Robust Detection of AI-Generated  Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tulchinskii%2C+E">Eduard Tulchinskii</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+K">Kristian Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Kushnareva%2C+L">Laida Kushnareva</a>, 
<a href="/search/cs?searchtype=author&query=Cherniavskii%2C+D">Daniil Cherniavskii</a>, 
<a href="/search/cs?searchtype=author&query=Barannikov%2C+S">Serguei Barannikov</a>, 
<a href="/search/cs?searchtype=author&query=Piontkovskaya%2C+I">Irina Piontkovskaya</a>, 
<a href="/search/cs?searchtype=author&query=Nikolenko%2C+S">Sergey Nikolenko</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Algebraic Topology (math.AT)

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05407" title="Abstract">arXiv:2306.05407</a> (replaced) [<a href="/pdf/2306.05407" title="Download PDF">pdf</a>, <a href="/format/2306.05407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarlin%2C+P">Paul-Edouard Sarlin</a>, 
<a href="/search/cs?searchtype=author&query=Trulls%2C+E">Eduard Trulls</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Hosang%2C+J">Jan Hosang</a>, 
<a href="/search/cs?searchtype=author&query=Lynen%2C+S">Simon Lynen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023, code available at <a href="https://github.com/google-research/snap">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06156" title="Abstract">arXiv:2306.06156</a> (replaced) [<a href="/pdf/2306.06156" title="Download PDF">pdf</a>, <a href="/format/2306.06156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoET: A generative model of protein families as sequences-of-sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Truong%2C+T+F">Timothy F. Truong Jr</a>, 
<a href="/search/q-bio?searchtype=author&query=Bepler%2C+T">Tristan Bepler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06203" title="Abstract">arXiv:2306.06203</a> (replaced) [<a href="/pdf/2306.06203" title="Download PDF">pdf</a>, <a href="/format/2306.06203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLSL: Feature-level Self-supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Q">Qing Su</a>, 
<a href="/search/cs?searchtype=author&query=Netchaev%2C+A">Anton Netchaev</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hai Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shihao Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a main conference paper at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08366" title="Abstract">arXiv:2306.08366</a> (replaced) [<a href="/pdf/2306.08366" title="Download PDF">pdf</a>, <a href="/format/2306.08366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SaliencyCut: Augmenting Plausible Anomalies for Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jianan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yijie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiu-Feng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaizhu Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08754" title="Abstract">arXiv:2306.08754</a> (replaced) [<a href="/pdf/2306.08754" title="Download PDF">pdf</a>, <a href="/format/2306.08754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClimSim: A large multi-scale dataset for hybrid physics-ML climate  emulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sungduk Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hannah%2C+W">Walter Hannah</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Liran Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jerry Lin</a>, 
<a href="/search/cs?searchtype=author&query=Bhouri%2C+M+A">Mohamed Aziz Bhouri</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Ritwik Gupta</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%BCtjens%2C+B">Bj&#xf6;rn L&#xfc;tjens</a>, 
<a href="/search/cs?searchtype=author&query=Will%2C+J+C">Justus Christopher Will</a>, 
<a href="/search/cs?searchtype=author&query=Behrens%2C+G">Gunnar Behrens</a>, 
<a href="/search/cs?searchtype=author&query=Busecke%2C+J">Julius Busecke</a>, 
<a href="/search/cs?searchtype=author&query=Loose%2C+N">Nora Loose</a>, 
<a href="/search/cs?searchtype=author&query=Stern%2C+C+I">Charles I Stern</a>, 
<a href="/search/cs?searchtype=author&query=Beucler%2C+T">Tom Beucler</a>, 
<a href="/search/cs?searchtype=author&query=Harrop%2C+B">Bryce Harrop</a>, 
<a href="/search/cs?searchtype=author&query=Hillman%2C+B+R">Benjamin R Hillman</a>, 
<a href="/search/cs?searchtype=author&query=Jenney%2C+A">Andrea Jenney</a>, 
<a href="/search/cs?searchtype=author&query=Ferretti%2C+S">Savannah Ferretti</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nana Liu</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>, 
<a href="/search/cs?searchtype=author&query=Brenowitz%2C+N+D">Noah D Brenowitz</a>, 
<a href="/search/cs?searchtype=author&query=Eyring%2C+V">Veronika Eyring</a>, 
<a href="/search/cs?searchtype=author&query=Geneva%2C+N">Nicholas Geneva</a>, 
<a href="/search/cs?searchtype=author&query=Gentine%2C+P">Pierre Gentine</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>, 
<a href="/search/cs?searchtype=author&query=Pathak%2C+J">Jaideep Pathak</a>, 
<a href="/search/cs?searchtype=author&query=Subramaniam%2C+A">Akshay Subramaniam</a>, 
<a href="/search/cs?searchtype=author&query=Vondrick%2C+C">Carl Vondrick</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zanna%2C+L">Laure Zanna</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Abernathey%2C+R">Ryan Abernathey</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Fiaz Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Bader%2C+D+C">David C Bader</a>, 
<a href="/search/cs?searchtype=author&query=Baldi%2C+P">Pierre Baldi</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+E">Elizabeth Barnes</a>, 
<a href="/search/cs?searchtype=author&query=Bretherton%2C+C">Christopher Bretherton</a>, 
<a href="/search/cs?searchtype=author&query=Caldwell%2C+P">Peter Caldwell</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+W">Wayne Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yilun Han</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Iglesias-Suarez%2C+F">Fernando Iglesias-Suarez</a>, 
<a href="/search/cs?searchtype=author&query=Jantre%2C+S">Sanket Jantre</a>, 
<a href="/search/cs?searchtype=author&query=Kashinath%2C+K">Karthik Kashinath</a>, 
<a href="/search/cs?searchtype=author&query=Khairoutdinov%2C+M">Marat Khairoutdinov</a>, 
<a href="/search/cs?searchtype=author&query=Kurth%2C+T">Thorsten Kurth</a>, 
<a href="/search/cs?searchtype=author&query=Lutsko%2C+N">Nicholas Lutsko</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Po-Lun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mooers%2C+G">Griffin Mooers</a>, 
<a href="/search/cs?searchtype=author&query=Neelin%2C+J+D">J. David Neelin</a>, 
<a href="/search/cs?searchtype=author&query=Randall%2C+D">David Randall</a>, 
<a href="/search/cs?searchtype=author&query=Shamekh%2C+S">Sara Shamekh</a>,  et al. (5 additional authors not shown)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09520" title="Abstract">arXiv:2306.09520</a> (replaced) [<a href="/pdf/2306.09520" title="Download PDF">pdf</a>, <a href="/format/2306.09520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensembled Prediction Intervals for Causal Outcomes Under Hidden  Confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marmarelis%2C+M+G">Myrl G. Marmarelis</a>, 
<a href="/search/cs?searchtype=author&query=Steeg%2C+G+V">Greg Ver Steeg</a>, 
<a href="/search/cs?searchtype=author&query=Galstyan%2C+A">Aram Galstyan</a>, 
<a href="/search/cs?searchtype=author&query=Morstatter%2C+F">Fred Morstatter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10161" title="Abstract">arXiv:2306.10161</a> (replaced) [<a href="/pdf/2306.10161" title="Download PDF">pdf</a>, <a href="/format/2306.10161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building the Bridge of Schr&#xf6;dinger: A Continuous Entropic Optimal  Transport Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gushchin%2C+N">Nikita Gushchin</a>, 
<a href="/search/cs?searchtype=author&query=Kolesov%2C+A">Alexander Kolesov</a>, 
<a href="/search/cs?searchtype=author&query=Mokrov%2C+P">Petr Mokrov</a>, 
<a href="/search/cs?searchtype=author&query=Karpikova%2C+P">Polina Karpikova</a>, 
<a href="/search/cs?searchtype=author&query=Spiridonov%2C+A">Andrey Spiridonov</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10280" title="Abstract">arXiv:2306.10280</a> (replaced) [<a href="/pdf/2306.10280" title="Download PDF">pdf</a>, <a href="/format/2306.10280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenGSL: A Comprehensive Benchmark for Graph Structure Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhiyao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+B">Bochao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuanyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiawei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Q">Qiaoyu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+D">Daochen Zha</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Can Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures. Accepted by NeurIPS Datasets and Benchmarks Track 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10608" title="Abstract">arXiv:2306.10608</a> (replaced) [<a href="/pdf/2306.10608" title="Download PDF">pdf</a>, <a href="/format/2306.10608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced  Audio-Visual Diarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Min%2C+K">Kyle Min</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Validation report for the Ego4D challenge at CVPR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13216" title="Abstract">arXiv:2306.13216</a> (replaced) [<a href="/pdf/2306.13216" title="Download PDF">pdf</a>, <a href="/format/2306.13216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diverse Community Data for Benchmarking Data Privacy Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sen%2C+A">Aniruddha Sen</a>, 
<a href="/search/cs?searchtype=author&query=Task%2C+C">Christine Task</a>, 
<a href="/search/cs?searchtype=author&query=Kapur%2C+D">Dhruv Kapur</a>, 
<a href="/search/cs?searchtype=author&query=Howarth%2C+G">Gary Howarth</a>, 
<a href="/search/cs?searchtype=author&query=Bhagat%2C+K">Karan Bhagat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13411" title="Abstract">arXiv:2306.13411</a> (replaced) [<a href="/pdf/2306.13411" title="Download PDF">pdf</a>, <a href="/format/2306.13411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Algorithmic Reasoning Without Intermediate Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodionov%2C+G">Gleb Rodionov</a>, 
<a href="/search/cs?searchtype=author&query=Prokhorenkova%2C+L">Liudmila Prokhorenkova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14878" title="Abstract">arXiv:2306.14878</a> (replaced) [<a href="/pdf/2306.14878" title="Download PDF">pdf</a>, <a href="/format/2306.14878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Restart Sampling for Improving Generative Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yilun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+M">Mingyang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonglong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/Newbeeer/diffusion_restart_sampling">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15749" title="Abstract">arXiv:2306.15749</a> (replaced) [<a href="/pdf/2306.15749" title="Download PDF">pdf</a>, <a href="/format/2306.15749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Spike or Not To Spike: A Digital Hardware Perspective on Deep  Learning Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ottati%2C+F">Fabrizio Ottati</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qinyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Brignone%2C+G">Giovanni Brignone</a>, 
<a href="/search/cs?searchtype=author&query=Casu%2C+M+R">Mario R. Casu</a>, 
<a href="/search/cs?searchtype=author&query=Eshraghian%2C+J+K">Jason K. Eshraghian</a>, 
<a href="/search/cs?searchtype=author&query=Lavagno%2C+L">Luciano Lavagno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera-ready for IEEE Journal on Emerging and Selected Topics in Circuits and Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16623" title="Abstract">arXiv:2306.16623</a> (replaced) [<a href="/pdf/2306.16623" title="Download PDF">pdf</a>, <a href="/format/2306.16623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Segment Anything Model (SAM) for Remote Sensing Applications: From  Zero to One Shot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osco%2C+L+P">Lucas Prado Osco</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiusheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=de+Lemos%2C+E+L">Eduardo Lopes de Lemos</a>, 
<a href="/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+W+N">Wesley Nunes Gon&#xe7;alves</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+A+P+M">Ana Paula Marques Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jonathan Li</a>, 
<a href="/search/cs?searchtype=author&query=Junior%2C+J+M">Jos&#xe9; Marcato Junior</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16705" title="Abstract">arXiv:2306.16705</a> (replaced) [<a href="/pdf/2306.16705" title="Download PDF">pdf</a>, <a href="/format/2306.16705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NNQS-Transformer: an Efficient and Scalable Neural Network Quantum  States Approach for Ab initio Quantum Chemistry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+Y">Yangjun Wu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Guo%2C+C">Chu Guo</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fan%2C+Y">Yi Fan</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhou%2C+P">Pengyu Zhou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shang%2C+H">Honghui Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SC'23, fix Table1 CCSD references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16731" title="Abstract">arXiv:2306.16731</a> (replaced) [<a href="/pdf/2306.16731" title="Download PDF">pdf</a>, <a href="/format/2306.16731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SYCL compute kernels for ExaHyPE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Loi%2C+C+M">Chung Ming Loi</a>, 
<a href="/search/cs?searchtype=author&query=Bockhorst%2C+H">Heinrich Bockhorst</a>, 
<a href="/search/cs?searchtype=author&query=Weinzierl%2C+T">Tobias Weinzierl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16838" title="Abstract">arXiv:2306.16838</a> (replaced) [<a href="/pdf/2306.16838" title="Download PDF">pdf</a>, <a href="/format/2306.16838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Kernel Ridge Regression with Gradient-Based Optimization Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Allerbo%2C+O">Oskar Allerbo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00134" title="Abstract">arXiv:2307.00134</a> (replaced) [<a href="/pdf/2307.00134" title="Download PDF">pdf</a>, <a href="/format/2307.00134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Limits of Graph Neural Networks in Identity Effects  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Inverno%2C+G+A">Giuseppe Alessio D&#x27;Inverno</a>, 
<a href="/search/cs?searchtype=author&query=Brugiapaglia%2C+S">Simone Brugiapaglia</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02295" title="Abstract">arXiv:2307.02295</a> (replaced) [<a href="/pdf/2307.02295" title="Download PDF">pdf</a>, <a href="/format/2307.02295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning Adversarial Bandit Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khodak%2C+M">Mikhail Khodak</a>, 
<a href="/search/cs?searchtype=author&query=Osadchiy%2C+I">Ilya Osadchiy</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+K">Keegan Harris</a>, 
<a href="/search/cs?searchtype=author&query=Balcan%2C+M">Maria-Florina Balcan</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+K+Y">Kfir Y. Levy</a>, 
<a href="/search/cs?searchtype=author&query=Meir%2C+R">Ron Meir</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Merger of <a href="/abs/2205.14128">arXiv:2205.14128</a> and <a href="/abs/2205.15921">arXiv:2205.15921</a>, with some additional improvements; to appear in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02818" title="Abstract">arXiv:2307.02818</a> (replaced) [<a href="/pdf/2307.02818" title="Download PDF">pdf</a>, <a href="/format/2307.02818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Degree Heterogeneity in Higher-Order Networks: Inference in the  Hypergraph $\boldsymbol&#x3b2;$-Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nandy%2C+S">Sagnik Nandy</a>, 
<a href="/search/math?searchtype=author&query=Bhattacharya%2C+B+B">Bhaswar B. Bhattacharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03176" title="Abstract">arXiv:2307.03176</a> (replaced) [<a href="/pdf/2307.03176" title="Download PDF">pdf</a>, <a href="/format/2307.03176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge  Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ruben%2C+B+S">Benjamin S. Ruben</a>, 
<a href="/search/stat?searchtype=author&query=Pehlevan%2C+C">Cengiz Pehlevan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Camera-Ready. Contains significant updates from the original submission
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Advances in Neural Information Processing Systems 36 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03994" title="Abstract">arXiv:2307.03994</a> (replaced) [<a href="/pdf/2307.03994" title="Download PDF">pdf</a>, <a href="/format/2307.03994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Market Design for Dynamic Pricing and Pooling in Capacitated Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amin%2C+S">Saurabh Amin</a>, 
<a href="/search/cs?searchtype=author&query=Jaillet%2C+P">Patrick Jaillet</a>, 
<a href="/search/cs?searchtype=author&query=Pulyassary%2C+H">Haripriya Pulyassary</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Manxi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10768" title="Abstract">arXiv:2307.10768</a> (replaced) [<a href="/pdf/2307.10768" title="Download PDF">pdf</a>, <a href="/format/2307.10768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of  Working Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sikarwar%2C+A">Ankur Sikarwar</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+M">Mengmi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12169" title="Abstract">arXiv:2307.12169</a> (replaced) [<a href="/pdf/2307.12169" title="Download PDF">pdf</a>, <a href="/format/2307.12169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Build Low-cost Networks for Large Language Models (without  Sacrificing Performance)?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ghobadi%2C+M">Manya Ghobadi</a>, 
<a href="/search/cs?searchtype=author&query=Shakeri%2C+K">Kayvon Shakeri</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hasani%2C+N">Naader Hasani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13566" title="Abstract">arXiv:2307.13566</a> (replaced) [<a href="/pdf/2307.13566" title="Download PDF">pdf</a>, <a href="/format/2307.13566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Imperfect XAI on Human-AI Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morrison%2C+K">Katelyn Morrison</a>, 
<a href="/search/cs?searchtype=author&query=Spitzer%2C+P">Philipp Spitzer</a>, 
<a href="/search/cs?searchtype=author&query=Turri%2C+V">Violet Turri</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+M">Michelle Feng</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChl%2C+N">Niklas K&#xfc;hl</a>, 
<a href="/search/cs?searchtype=author&query=Perer%2C+A">Adam Perer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 9 figures, 1 table, additional figures/table in the appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13916" title="Abstract">arXiv:2307.13916</a> (replaced) [<a href="/pdf/2307.13916" title="Download PDF">pdf</a>, <a href="/format/2307.13916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online learning in bandits with predicted context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Guo%2C+Y">Yongyi Guo</a>, 
<a href="/search/stat?searchtype=author&query=Xu%2C+Z">Ziping Xu</a>, 
<a href="/search/stat?searchtype=author&query=Murphy%2C+S">Susan Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14240" title="Abstract">arXiv:2307.14240</a> (replaced) [<a href="/pdf/2307.14240" title="Download PDF">pdf</a>, <a href="/format/2307.14240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boon: A Neural Search Engine for Cross-Modal Information Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Cosma%2C+G">Georgina Cosma</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> MMIR '23: Proceedings of the 1st International Workshop on Deep
  Multimodal Learning for Information Retrieval (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02698" title="Abstract">arXiv:2308.02698</a> (replaced) [<a href="/pdf/2308.02698" title="Download PDF">pdf</a>, <a href="/format/2308.02698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision-Theoretic Approaches for Robotic Environmental Monitoring -- A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Yoonchang Sung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+J">Jnaneshwar Das</a>, 
<a href="/search/cs?searchtype=author&query=Tokekar%2C+P">Pratap Tokekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 95 pages, 8 figures, Published in Foundations and Trends in Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06663" title="Abstract">arXiv:2308.06663</a> (replaced) [<a href="/pdf/2308.06663" title="Download PDF">pdf</a>, <a href="/ps/2308.06663" title="Download PostScript">ps</a>, <a href="/format/2308.06663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bashar%2C+M+A">Md Abul Bashar</a>, 
<a href="/search/cs?searchtype=author&query=Nayak%2C+R">Richi Nayak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12447" title="Abstract">arXiv:2308.12447</a> (replaced) [<a href="/pdf/2308.12447" title="Download PDF">pdf</a>, <a href="/format/2308.12447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOFO: MOtion FOcused Self-Supervision for Video Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadian%2C+M">Mona Ahmadian</a>, 
<a href="/search/cs?searchtype=author&query=Guerin%2C+F">Frank Guerin</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+A">Andrew Gilbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the NeurIPS 2023 Workshop: Self-Supervised Learning - Theory and Practice
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13068" title="Abstract">arXiv:2308.13068</a> (replaced) [<a href="/pdf/2308.13068" title="Download PDF">pdf</a>, <a href="/format/2308.13068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed  Evaluation Methodology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sehili%2C+M+E+A">Mohamed El Amine Sehili</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zonghua Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures, accepted at TPCTC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Performance (cs.PF); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16705" title="Abstract">arXiv:2308.16705</a> (replaced) [<a href="/pdf/2308.16705" title="Download PDF">pdf</a>, <a href="/format/2308.16705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CREHate: A CRoss-cultural English Hate Speech Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Nayeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+C">Chani Jung</a>, 
<a href="/search/cs?searchtype=author&query=Myung%2C+J">Junho Myung</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jiho Jin</a>, 
<a href="/search/cs?searchtype=author&query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00071" title="Abstract">arXiv:2309.00071</a> (replaced) [<a href="/pdf/2309.00071" title="Download PDF">pdf</a>, <a href="/format/2309.00071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YaRN: Efficient Context Window Extension of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bowen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Quesnelle%2C+J">Jeffrey Quesnelle</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Honglu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Shippole%2C+E">Enrico Shippole</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03387" title="Abstract">arXiv:2309.03387</a> (replaced) [<a href="/pdf/2309.03387" title="Download PDF">pdf</a>, <a href="/format/2309.03387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Baselines for Motion Prediction in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Hu%C3%A9lamo%2C+C">Carlos G&#xf3;mez-Hu&#xe9;lamo</a>, 
<a href="/search/cs?searchtype=author&query=Conde%2C+M+V">Marcos V. Conde</a>, 
<a href="/search/cs?searchtype=author&query=Barea%2C+R">Rafael Barea</a>, 
<a href="/search/cs?searchtype=author&query=Oca%C3%B1a%2C+M">Manuel Oca&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Bergasa%2C+L+M">Luis M. Bergasa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE T-ITS Transactions on Intelligent Transportation Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03564" title="Abstract">arXiv:2309.03564</a> (replaced) [<a href="/pdf/2309.03564" title="Download PDF">pdf</a>, <a href="/format/2309.03564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised Learning and Large Language Model Benchmarks on Mental Health  Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+H">Hongzhi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Changwei Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+W">Wei Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y+J">Yi Jing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+H">Huijing Zou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B+X">Bing Xiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+G">Guanghui Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05209" title="Abstract">arXiv:2309.05209</a> (replaced) [<a href="/pdf/2309.05209" title="Download PDF">pdf</a>, <a href="/format/2309.05209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase-Specific Augmented Reality Guidance for Microscopic Cataract  Surgery Using Long-Short Spatiotemporal Aggregation Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+P">Puxun Tu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hongfei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haochen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+J">Jeff Young</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+M">Meng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peiquan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Ce Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05361" title="Abstract">arXiv:2309.05361</a> (replaced) [<a href="/pdf/2309.05361" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-tokamak Disruption Prediction based on Physics-Guided Feature  Extraction and domain adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Shen%2C+C">Chengshuo Shen</a>, 
<a href="/search/physics?searchtype=author&query=Zheng%2C+W">Wei Zheng</a>, 
<a href="/search/physics?searchtype=author&query=Guo%2C+B">Bihao Guo</a>, 
<a href="/search/physics?searchtype=author&query=Ding%2C+Y">Yonghua Ding</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+D">Dalong Chen</a>, 
<a href="/search/physics?searchtype=author&query=Ai%2C+X">Xinkun Ai</a>, 
<a href="/search/physics?searchtype=author&query=Xue%2C+F">Fengming Xue</a>, 
<a href="/search/physics?searchtype=author&query=Zhong%2C+Y">Yu Zhong</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+N">Nengchao Wang</a>, 
<a href="/search/physics?searchtype=author&query=Shen%2C+B">Biao Shen</a>, 
<a href="/search/physics?searchtype=author&query=Xiao%2C+B">Binjia Xiao</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+Z">Zhongyong Chen</a>, 
<a href="/search/physics?searchtype=author&query=Pan%2C+Y">Yuan Pan</a>, 
<a href="/search/physics?searchtype=author&query=J-TEXT+team">J-TEXT team</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Plasma Physics (physics.plasm-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05764" title="Abstract">arXiv:2309.05764</a> (replaced) [<a href="/pdf/2309.05764" title="Download PDF">pdf</a>, <a href="/ps/2309.05764" title="Download PostScript">ps</a>, <a href="/format/2309.05764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equality cases of the Alexandrov--Fenchel inequality are not in the  polynomial hierarchy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chan%2C+S+H">Swee Hong Chan</a>, 
<a href="/search/math?searchtype=author&query=Pak%2C+I">Igor Pak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages. Conjecture 10.2 and 10.3 in v1 are now resolved, and become Theorem 1.3 and Corollary 4.8, respectively. Proofs in Section 7 and 8 are streamlined and improved. Several final remarks in Section 10 are revised, and new references are included
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Computational Geometry (cs.CG); Discrete Mathematics (cs.DM); Metric Geometry (math.MG)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09167" title="Abstract">arXiv:2309.09167</a> (replaced) [<a href="/pdf/2309.09167" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Knowing to Doing: Learning Diverse Motor Skills through Instruction  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+L">Linqi Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiayi Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xianhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">Bin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yan Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10617" title="Abstract">arXiv:2309.10617</a> (replaced) [<a href="/pdf/2309.10617" title="Download PDF">pdf</a>, <a href="/format/2309.10617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Debris Mass Estimation Model for Autonomous Underwater  Vehicle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%2C+M+S">Mohana Sri S</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+S">Swethaa S</a>, 
<a href="/search/cs?searchtype=author&query=Y%2C+A+B+S">Aouthithiye Barathwaj SR Y</a>, 
<a href="/search/cs?searchtype=author&query=CS%2C+S+G">Sai Ganesh CS</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10726" title="Abstract">arXiv:2309.10726</a> (replaced) [<a href="/pdf/2309.10726" title="Download PDF">pdf</a>, <a href="/format/2309.10726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Panoptic Segmentation With Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%A4ppeler%2C+M">Markus K&#xe4;ppeler</a>, 
<a href="/search/cs?searchtype=author&query=Petek%2C+K">K&#xfc;rsat Petek</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%B6disch%2C+N">Niclas V&#xf6;disch</a>, 
<a href="/search/cs?searchtype=author&query=Burgard%2C+W">Wolfram Burgard</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12161" title="Abstract">arXiv:2309.12161</a> (replaced) [<a href="/pdf/2309.12161" title="Download PDF">pdf</a>, <a href="/format/2309.12161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Soliloquies for Accurate Calculations in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonkar%2C+S">Shashank Sonkar</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+M">MyCo Le</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinghe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Naiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mallick%2C+D+B">Debshila Basu Mallick</a>, 
<a href="/search/cs?searchtype=author&query=Baraniuk%2C+R+G">Richard G. Baraniuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12450" title="Abstract">arXiv:2309.12450</a> (replaced) [<a href="/pdf/2309.12450" title="Download PDF">pdf</a>, <a href="/format/2309.12450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Convex Framework for Confounding Robust Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ishikawa%2C+K">Kei Ishikawa</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+N">Niao He</a>, 
<a href="/search/stat?searchtype=author&query=Kanamori%2C+T">Takafumi Kanamori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended version of the following work <a href="https://proceedings.mlr.press/v206/ishikawa23a.html.">this https URL</a> arXiv admin note: text overlap with <a href="/abs/2302.13348">arXiv:2302.13348</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13166" title="Abstract">arXiv:2309.13166</a> (replaced) [<a href="/pdf/2309.13166" title="Download PDF">pdf</a>, <a href="/format/2309.13166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invisible Watermarking for Audio Generation Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xirong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Jadav%2C+D">Divyesh Jadav</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yanzhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhehui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+C">Chen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenqi Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an invited paper for IEEE TPS, part of the IEEE CIC/CogMI/TPS 2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13414" title="Abstract">arXiv:2309.13414</a> (replaced) [<a href="/pdf/2309.13414" title="Download PDF">pdf</a>, <a href="/format/2309.13414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State-space Models with Layer-wise Nonlinearity are Universal  Approximators with Exponential Decaying Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shida Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+B">Beichen Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13755" title="Abstract">arXiv:2309.13755</a> (replaced) [<a href="/pdf/2309.13755" title="Download PDF">pdf</a>, <a href="/format/2309.13755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Recursive Data-enabled Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shi%2C+J">Jicheng Shi</a>, 
<a href="/search/eess?searchtype=author&query=Lian%2C+Y">Yingzhao Lian</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+C+N">Colin N. Jones</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16077" title="Abstract">arXiv:2309.16077</a> (replaced) [<a href="/pdf/2309.16077" title="Download PDF">pdf</a>, <a href="/format/2309.16077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-Oriented Koopman-Based Control with Contrastive Encoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+X">Xubo Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hanyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Siriya%2C+S">Seth Siriya</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Ye Pu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 7th Annual Conference on Robot Learning (CoRL), 2023 (oral spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16166" title="Abstract">arXiv:2309.16166</a> (replaced) [<a href="/pdf/2309.16166" title="Download PDF">pdf</a>, <a href="/format/2309.16166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoinRun: Solving Goal Misgeneralisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Armstrong%2C+S">Stuart Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Maranh%C3%A3o%2C+A">Alexandre Maranh&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Daniels-Koch%2C+O">Oliver Daniels-Koch</a>, 
<a href="/search/cs?searchtype=author&query=Leask%2C+P">Patrick Leask</a>, 
<a href="/search/cs?searchtype=author&query=Gorman%2C+R">Rebecca Gorman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16428" title="Abstract">arXiv:2309.16428</a> (replaced) [<a href="/pdf/2309.16428" title="Download PDF">pdf</a>, <a href="/format/2309.16428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear MPC design for incrementally ISS systems with application to  GRU networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bonassi%2C+F">Fabio Bonassi</a>, 
<a href="/search/eess?searchtype=author&query=La+Bella%2C+A">Alessio La Bella</a>, 
<a href="/search/eess?searchtype=author&query=Farina%2C+M">Marcello Farina</a>, 
<a href="/search/eess?searchtype=author&query=Scattolini%2C+R">Riccardo Scattolini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> {\copyright} 2023. This manuscript version is made available under the CC-BY-NC-ND 4.0 license (<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">this https URL</a>). This manuscript has been accepted for publication at Elsevier Automatica. Please cite the published article instead of this manuscript
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Automatica 159 (2024) 111381
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16868" title="Abstract">arXiv:2309.16868</a> (replaced) [<a href="/pdf/2309.16868" title="Download PDF">pdf</a>, <a href="/format/2309.16868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analytically Computation of Sensitivity Coefficients in Hybrid AC/DC  Micro-Grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lambrichts%2C+W">Willem Lambrichts</a>, 
<a href="/search/eess?searchtype=author&query=Paolone%2C+M">Mario Paolone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, submitted to IEEE Transactions on Power Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00917" title="Abstract">arXiv:2310.00917</a> (replaced) [<a href="/pdf/2310.00917" title="Download PDF">pdf</a>, <a href="/format/2310.00917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards  Enhancing Text Spotting Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Alloy Das</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Ayan Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Llad%C3%B3s%2C+J">Josep Llad&#xf3;s</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+U">Umapada Pal</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Saumik Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01128" title="Abstract">arXiv:2310.01128</a> (replaced) [<a href="/pdf/2310.01128" title="Download PDF">pdf</a>, <a href="/format/2310.01128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling Voice and Content with Self-Supervision for Speaker  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+T">Tianchi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+K+A">Kong Aik Lee</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Q">Qiongqiong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NeurIPS 2023 (main track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01376" title="Abstract">arXiv:2310.01376</a> (replaced) [<a href="/pdf/2310.01376" title="Download PDF">pdf</a>, <a href="/format/2310.01376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Distribution-Agnostic Generalized Category Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jianhong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hualiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+L">Lianrui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaomeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Haoji Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03223" title="Abstract">arXiv:2310.03223</a> (replaced) [<a href="/pdf/2310.03223" title="Download PDF">pdf</a>, <a href="/format/2310.03223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tony Shen</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+M">Mohit Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+J">Jason Smith</a>, 
<a href="/search/cs?searchtype=author&query=Cherkasov%2C+A">Artem Cherkasov</a>, 
<a href="/search/cs?searchtype=author&query=Ester%2C+M">Martin Ester</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023 Generative AI and Biology (GenBio) Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04632" title="Abstract">arXiv:2310.04632</a> (replaced) [<a href="/pdf/2310.04632" title="Download PDF">pdf</a>, <a href="/format/2310.04632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Anonymization of Swiss Federal Supreme Court Rulings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niklaus%2C+J">Joel Niklaus</a>, 
<a href="/search/cs?searchtype=author&query=Mami%C3%A9%2C+R">Robin Mami&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%BCrmer%2C+M">Matthias St&#xfc;rmer</a>, 
<a href="/search/cs?searchtype=author&query=Brunner%2C+D">Daniel Brunner</a>, 
<a href="/search/cs?searchtype=author&query=Gygli%2C+M">Marcel Gygli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to NLLP @ EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05492" title="Abstract">arXiv:2310.05492</a> (replaced) [<a href="/pdf/2310.05492" title="Download PDF">pdf</a>, <a href="/format/2310.05492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Abilities in Large Language Models are Affected by Supervised  Fine-tuning Data Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hongyi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Keming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Mingfeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dayiheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05506" title="Abstract">arXiv:2310.05506</a> (replaced) [<a href="/pdf/2310.05506" title="Download PDF">pdf</a>, <a href="/format/2310.05506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengpeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Hongyi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Keming Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiancan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Chuanqi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06179" title="Abstract">arXiv:2310.06179</a> (replaced) [<a href="/pdf/2310.06179" title="Download PDF">pdf</a>, <a href="/format/2310.06179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Integration for Spatiotemporal Neural Point Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06643" title="Abstract">arXiv:2310.06643</a> (replaced) [<a href="/pdf/2310.06643" title="Download PDF">pdf</a>, <a href="/format/2310.06643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Variational Inference for High-Dimensional Posteriors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Uppal%2C+A">Anshuk Uppal</a>, 
<a href="/search/cs?searchtype=author&query=Stensbo-Smidt%2C+K">Kristoffer Stensbo-Smidt</a>, 
<a href="/search/cs?searchtype=author&query=Boomsma%2C+W+K">Wouter K. Boomsma</a>, 
<a href="/search/cs?searchtype=author&query=Frellsen%2C+J">Jes Frellsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages and appendix, 9 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06753" title="Abstract">arXiv:2310.06753</a> (replaced) [<a href="/pdf/2310.06753" title="Download PDF">pdf</a>, <a href="/format/2310.06753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dongming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jiahao Chang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+F">Fan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yingfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tiancai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianbing Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 1st solution for 1st OpenLane Topology in Autonomous Driving Challenge. Code is at <a href="https://github.com/wudongming97/TopoMLP">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06763" title="Abstract">arXiv:2310.06763</a> (replaced) [<a href="/pdf/2310.06763" title="Download PDF">pdf</a>, <a href="/format/2310.06763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FABind: Fast and Accurate Protein-Ligand Binding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+Q">Qizhi Pei</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+K">Kaiyuan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lijun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jinhua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yingce Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shufang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+T">Tao Qin</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kun He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tie-Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Neural Information Processing Systems 2023 (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06775" title="Abstract">arXiv:2310.06775</a> (replaced) [<a href="/pdf/2310.06775" title="Download PDF">pdf</a>, <a href="/format/2310.06775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conceptual Framework for Autonomous Cognitive Entities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+D">David Shapiro</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wangfan Li</a>, 
<a href="/search/cs?searchtype=author&query=Delaflor%2C+M">Manuel Delaflor</a>, 
<a href="/search/cs?searchtype=author&query=Toxtli%2C+C">Carlos Toxtli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07558" title="Abstract">arXiv:2310.07558</a> (replaced) [<a href="/pdf/2310.07558" title="Download PDF">pdf</a>, <a href="/ps/2310.07558" title="Download PostScript">ps</a>, <a href="/format/2310.07558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothness-Adaptive Dynamic Pricing with Nonparametric Demand Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+Z">Zeqi Ye</a>, 
<a href="/search/stat?searchtype=author&query=Jiang%2C+H">Hansheng Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor typo errors corrected in the latest version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07788" title="Abstract">arXiv:2310.07788</a> (replaced) [<a href="/pdf/2310.07788" title="Download PDF">pdf</a>, <a href="/format/2310.07788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element approximation for the delayed generalized Burgers-Huxley  equation with weakly singular kernel: Part II Non-Conforming and DG  approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mahajan%2C+S">Sumit Mahajan</a>, 
<a href="/search/math?searchtype=author&query=Khan%2C+A">Arbaz Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09219" title="Abstract">arXiv:2310.09219</a> (replaced) [<a href="/pdf/2310.09219" title="Download PDF">pdf</a>, <a href="/format/2310.09219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Kelly is a Warm Person, Joseph is a Role Model&quot;: Gender Biases in  LLM-Generated Reference Letters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yixin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+G">George Pu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jiao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Garimella%2C+A">Aparna Garimella</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11830" title="Abstract">arXiv:2310.11830</a> (replaced) [<a href="/pdf/2310.11830" title="Download PDF">pdf</a>, <a href="/format/2310.11830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLARA: Multilingual Contrastive Learning for Audio Representation  Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Noriy%2C+K+A">Kari A Noriy</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaosong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Budka%2C+M">Marcin Budka</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+J">Jian Jun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11986" title="Abstract">arXiv:2310.11986</a> (replaced) [<a href="/pdf/2310.11986" title="Download PDF">pdf</a>, <a href="/format/2310.11986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sociotechnical Safety Evaluation of Generative AI Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weidinger%2C+L">Laura Weidinger</a>, 
<a href="/search/cs?searchtype=author&query=Rauh%2C+M">Maribeth Rauh</a>, 
<a href="/search/cs?searchtype=author&query=Marchal%2C+N">Nahema Marchal</a>, 
<a href="/search/cs?searchtype=author&query=Manzini%2C+A">Arianna Manzini</a>, 
<a href="/search/cs?searchtype=author&query=Hendricks%2C+L+A">Lisa Anne Hendricks</a>, 
<a href="/search/cs?searchtype=author&query=Mateos-Garcia%2C+J">Juan Mateos-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Bergman%2C+S">Stevie Bergman</a>, 
<a href="/search/cs?searchtype=author&query=Kay%2C+J">Jackie Kay</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+C">Conor Griffin</a>, 
<a href="/search/cs?searchtype=author&query=Bariach%2C+B">Ben Bariach</a>, 
<a href="/search/cs?searchtype=author&query=Gabriel%2C+I">Iason Gabriel</a>, 
<a href="/search/cs?searchtype=author&query=Rieser%2C+V">Verena Rieser</a>, 
<a href="/search/cs?searchtype=author&query=Isaac%2C+W">William Isaac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> main paper p.1-29, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12319" title="Abstract">arXiv:2310.12319</a> (replaced) [<a href="/pdf/2310.12319" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arithmetic Operators over Finite Field GF($2^m$) for Error Correction  Codes Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nabipour%2C+S">Saeideh Nabipour</a>, 
<a href="/search/cs?searchtype=author&query=Gholizade%2C+M">Masoume Gholizade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 11 Figures, 6 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12911" title="Abstract">arXiv:2310.12911</a> (replaced) [<a href="/pdf/2310.12911" title="Download PDF">pdf</a>, <a href="/format/2310.12911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight approximability of MAX 2-SAT and relatives, under UGC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brakensiek%2C+J">Joshua Brakensiek</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+N">Neng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zwick%2C+U">Uri Zwick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 1 figure; to appear in SODA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13294" title="Abstract">arXiv:2310.13294</a> (replaced) [<a href="/pdf/2310.13294" title="Download PDF">pdf</a>, <a href="/format/2310.13294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VR PreM+ : An Immersive Pre-learning Branching Visualization System for  Museum Tours
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Ze Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changkun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Anqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Braud%2C+T">Tristan Braud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at The Eleventh International Symposium of Chinese CHI (Chinese CHI 2023), Bali
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13959" title="Abstract">arXiv:2310.13959</a> (replaced) [<a href="/e-print/2310.13959" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-discriminator Domain Adversarial Neural Networks with Class-Level  Gradient Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hongke Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hengshu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+N">Nan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> False Figure citation. For instance, Figure 8 hasn't been cited
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13976" title="Abstract">arXiv:2310.13976</a> (replaced) [<a href="/pdf/2310.13976" title="Download PDF">pdf</a>, <a href="/format/2310.13976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Requirements Engineering through Generative AI: Assessing the  Role of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Chetan Arora</a>, 
<a href="/search/cs?searchtype=author&query=Grundy%2C+J">John Grundy</a>, 
<a href="/search/cs?searchtype=author&query=Abdelrazek%2C+M">Mohamed Abdelrazek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14670" title="Abstract">arXiv:2310.14670</a> (replaced) [<a href="/pdf/2310.14670" title="Download PDF">pdf</a>, <a href="/format/2310.14670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and  Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhecan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+H">Haoxuan You</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Keyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yicheng He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Codella%2C+N">Noel Codella</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shih-Fu Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14762" title="Abstract">arXiv:2310.14762</a> (replaced) [<a href="/pdf/2310.14762" title="Download PDF">pdf</a>, <a href="/format/2310.14762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Native spaces and generalization of Wu functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yixuan Huang</a> (1 and 2), 
<a href="/search/math?searchtype=author&query=Wu%2C+Z">Zongmin Wu</a> (3 and 4), 
<a href="/search/math?searchtype=author&query=Zhu%2C+S">Shengxin Zhu</a> (5 and 6) ((1) School of Mathematics, Beijing Normal University, China, (2) Laboratory of Mathematics and Complex Systems, Ministry of Education, China, (3) Shanghai Key Laboratory for Contemporary Applied Mathematics, School of Mathematical Sciences, Fudan University, Shanghai, China, Shanghai Center for Mathematical Sciences, Shanghai, China, (4) School of Big Data and Statistics, Anhui University, Hefei, China, (5) Research Center for Mathematics, Advanced Institute of Natural Science, Beijing Normal University, Zhuhai, China, (6) Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International College, Zhuhai, China)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14859" title="Abstract">arXiv:2310.14859</a> (replaced) [<a href="/pdf/2310.14859" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for  Embodied Turn-Taking Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatan%2C+M">Mehdi Fatan</a>, 
<a href="/search/cs?searchtype=author&query=Mincato%2C+E">Emanuele Mincato</a>, 
<a href="/search/cs?searchtype=author&query=Pintzou%2C+D">Dimitra Pintzou</a>, 
<a href="/search/cs?searchtype=author&query=Dimiccoli%2C+M">Mariella Dimiccoli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15105" title="Abstract">arXiv:2310.15105</a> (replaced) [<a href="/pdf/2310.15105" title="Download PDF">pdf</a>, <a href="/format/2310.15105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained  Models in Few-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kun Song</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Huimin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+B">Bochao Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huishuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15431" title="Abstract">arXiv:2310.15431</a> (replaced) [<a href="/pdf/2310.15431" title="Download PDF">pdf</a>, <a href="/format/2310.15431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts  and Rationales for Disambiguating Defeasible Social and Moral Situations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+K">Kavel Rao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liwei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pyatkin%2C+V">Valentina Pyatkin</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuling Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Dziri%2C+N">Nouha Dziri</a>, 
<a href="/search/cs?searchtype=author&query=Brahman%2C+F">Faeze Brahman</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready EMNLP Findings 2023. First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15612" title="Abstract">arXiv:2310.15612</a> (replaced) [<a href="/pdf/2310.15612" title="Download PDF">pdf</a>, <a href="/format/2310.15612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Translation for Nko: Tools, Corpora and Baseline Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doumbouya%2C+M+K+B">Moussa Koulako Bala Doumbouya</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+B+M">Baba Mamadi Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Ciss%C3%A9%2C+S+F">Solo Farabado Ciss&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+D">Djibrila Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Sow%2C+A">Abdoulaye Sow</a>, 
<a href="/search/cs?searchtype=author&query=Doumbouya%2C+S+M">S&#xe9;r&#xe9; Moussa Doumbouya</a>, 
<a href="/search/cs?searchtype=author&query=Bangoura%2C+D">Daouda Bangoura</a>, 
<a href="/search/cs?searchtype=author&query=Bayo%2C+F+M">Fod&#xe9; Moriba Bayo</a>, 
<a href="/search/cs?searchtype=author&query=Cond%C3%A9%2C+I+S+2">Ibrahima Sory 2. Cond&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Dian%C3%A9%2C+K+M">Kalo Mory Dian&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Piech%2C+C">Chris Piech</a>, 
<a href="/search/cs?searchtype=author&query=Manning%2C+C">Christopher Manning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15903" title="Abstract">arXiv:2310.15903</a> (replaced) [<a href="/pdf/2310.15903" title="Download PDF">pdf</a>, <a href="/ps/2310.15903" title="Download PostScript">ps</a>, <a href="/format/2310.15903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Collapse in Multi-label Learning with Pick-all-label Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yutong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16791" title="Abstract">arXiv:2310.16791</a> (replaced) [<a href="/pdf/2310.16791" title="Download PDF">pdf</a>, <a href="/format/2310.16791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covert Planning against Imperfect Observers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haoxiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chongyang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shuo Han</a>, 
<a href="/search/cs?searchtype=author&query=Dorothy%2C+M+R">Michael R. Dorothy</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17306" title="Abstract">arXiv:2310.17306</a> (replaced) [<a href="/e-print/2310.17306" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FormaT5: Abstention and Examples for Conditional Table Formatting with  Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+E">Elnaz Nouri</a>, 
<a href="/search/cs?searchtype=author&query=Raza%2C+M">Mohammad Raza</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Contains inappropriately sourced conjecture of OpenAI's ChatGPT parameter count from www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat, a citation which was omitted. The authors do not have direct knowledge or verification of this information, and relied solely on this article, which may lead to public confusion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Databases (cs.DB); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17680" title="Abstract">arXiv:2310.17680</a> (replaced) [<a href="/e-print/2310.17680" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeFusion: A Pre-trained Diffusion Model for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mukul Singh</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gulwani%2C+S">Sumit Gulwani</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C">Carina Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Verbruggen%2C+G">Gust Verbruggen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Contains inappropriately sourced conjecture of OpenAI's ChatGPT parameter count from www.forbes.com/sites/forbestechcouncil/2023/02/17/is-bigger-better-why-the-chatgpt-vs-gpt-3-vs-gpt-4-battle-is-just-a-family-chat, a citation which was omitted. The authors do not have direct knowledge or verification of this information, and relied solely on this article, which may lead to public confusion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17851" title="Abstract">arXiv:2310.17851</a> (replaced) [<a href="/pdf/2310.17851" title="Download PDF">pdf</a>, <a href="/format/2310.17851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring CDNs susceptible to Domain Fronting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramani%2C+K">Karthika Subramani</a>, 
<a href="/search/cs?searchtype=author&query=Perdisci%2C+R">Roberto Perdisci</a>, 
<a href="/search/cs?searchtype=author&query=Skafidas%2C+P">Pierros Skafidas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17886" title="Abstract">arXiv:2310.17886</a> (replaced) [<a href="/pdf/2310.17886" title="Download PDF">pdf</a>, <a href="/format/2310.17886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Linear-Size Additive Emulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoppenworth%2C+G">Gary Hoppenworth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SOSA24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17956" title="Abstract">arXiv:2310.17956</a> (replaced) [<a href="/pdf/2310.17956" title="Download PDF">pdf</a>, <a href="/format/2310.17956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General  Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junling Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qichen Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chong%2C+D">Dading Chong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peilin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yining Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18131" title="Abstract">arXiv:2310.18131</a> (replaced) [<a href="/pdf/2310.18131" title="Download PDF">pdf</a>, <a href="/format/2310.18131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Video Gaze Estimation via Capturing Head-face-eye  Spatial-temporal Interaction Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yiran Guan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuoguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenzheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhiguo Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yang Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18233" title="Abstract">arXiv:2310.18233</a> (replaced) [<a href="/pdf/2310.18233" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Will releasing the weights of future large language models grant  widespread access to pandemic agents?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gopal%2C+A">Anjali Gopal</a>, 
<a href="/search/cs?searchtype=author&query=Helm-Burger%2C+N">Nathan Helm-Burger</a>, 
<a href="/search/cs?searchtype=author&query=Justen%2C+L">Lennart Justen</a>, 
<a href="/search/cs?searchtype=author&query=Soice%2C+E+H">Emily H. Soice</a>, 
<a href="/search/cs?searchtype=author&query=Tzeng%2C+T">Tiffany Tzeng</a>, 
<a href="/search/cs?searchtype=author&query=Jeyapragasan%2C+G">Geetha Jeyapragasan</a>, 
<a href="/search/cs?searchtype=author&query=Grimm%2C+S">Simon Grimm</a>, 
<a href="/search/cs?searchtype=author&query=Mueller%2C+B">Benjamin Mueller</a>, 
<a href="/search/cs?searchtype=author&query=Esvelt%2C+K+M">Kevin M. Esvelt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updates in response to online feedback: emphasized the focus on risks from future rather than current models; explained the reasoning behind - and minimal effects of - fine-tuning on virology papers; elaborated on how easier access to synthesized information can reduce barriers to entry; clarified policy recommendations regarding what is necessary but not sufficient; corrected a citation link
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18341" title="Abstract">arXiv:2310.18341</a> (replaced) [<a href="/pdf/2310.18341" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seowoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Youn%2C+J">Jiwon Youn</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Mansu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S+H">Soon Ho Yoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18512" title="Abstract">arXiv:2310.18512</a> (replaced) [<a href="/pdf/2310.18512" title="Download PDF">pdf</a>, <a href="/format/2310.18512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preventing Language Models From Hiding Their Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roger%2C+F">Fabien Roger</a>, 
<a href="/search/cs?searchtype=author&query=Greenblatt%2C+R">Ryan Greenblatt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Edit: Fix typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18728" title="Abstract">arXiv:2310.18728</a> (replaced) [<a href="/pdf/2310.18728" title="Download PDF">pdf</a>, <a href="/format/2310.18728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debunking Free Fusion Myth: Online Multi-view Anomaly Detection with  Disentangled Product-of-Experts Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jingdong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Multimedia 2023, 10 pages, 5 tables, and 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19046" title="Abstract">arXiv:2310.19046</a> (replaced) [<a href="/pdf/2310.19046" title="Download PDF">pdf</a>, <a href="/format/2310.19046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Evolutionary Optimizers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengcai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Caishun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xinghua Qu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+Y">Yew-Soon Ong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19070" title="Abstract">arXiv:2310.19070</a> (replaced) [<a href="/pdf/2310.19070" title="Download PDF">pdf</a>, <a href="/format/2310.19070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Myriad: Large Multimodal Model by Applying Vision Experts for Industrial  Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanze Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haolin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shihao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Debin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19283" title="Abstract">arXiv:2310.19283</a> (replaced) [<a href="/pdf/2310.19283" title="Download PDF">pdf</a>, <a href="/format/2310.19283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature  Extraction for IMU-based Human Activity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Enokibori%2C+Y">Yu Enokibori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Update the best result of OPPORTUNITY (not iSPL version)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19335" title="Abstract">arXiv:2310.19335</a> (replaced) [<a href="/pdf/2310.19335" title="Download PDF">pdf</a>, <a href="/ps/2310.19335" title="Download PostScript">ps</a>, <a href="/format/2310.19335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> USSR is in P/poly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balaji%2C+N">Nikhil Balaji</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+S">Samir Datta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SOSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Computational Geometry (cs.CG); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19347" title="Abstract">arXiv:2310.19347</a> (replaced) [<a href="/pdf/2310.19347" title="Download PDF">pdf</a>, <a href="/format/2310.19347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Factual Consistency of Text Summarization by Adversarially  Decoupling Comprehension and Embellishment Abilities of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Huawen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Ting-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zekun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuchuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19531" title="Abstract">arXiv:2310.19531</a> (replaced) [<a href="/pdf/2310.19531" title="Download PDF">pdf</a>, <a href="/format/2310.19531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfoEntropy Loss to Mitigate Bias of Learning Difficulties for  Generative Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhenpeng Su</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xue Bai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zijia Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+G">Guiguang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19685" title="Abstract">arXiv:2310.19685</a> (replaced) [<a href="/pdf/2310.19685" title="Download PDF">pdf</a>, <a href="/format/2310.19685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DGFN: Double Generative Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lau%2C+E">Elaine Lau</a>, 
<a href="/search/cs?searchtype=author&query=Vemgal%2C+N">Nikhil Vemgal</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+E">Emmanuel Bengio</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19708" title="Abstract">arXiv:2310.19708</a> (replaced) [<a href="/pdf/2310.19708" title="Download PDF">pdf</a>, <a href="/format/2310.19708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Language Models For Specialized Domains: A Colorful Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eitan%2C+D">Daniel Eitan</a>, 
<a href="/search/cs?searchtype=author&query=Pirchi%2C+M">Menachem Pirchi</a>, 
<a href="/search/cs?searchtype=author&query=Glazer%2C+N">Neta Glazer</a>, 
<a href="/search/cs?searchtype=author&query=Meital%2C+S">Shai Meital</a>, 
<a href="/search/cs?searchtype=author&query=Ayach%2C+G">Gil Ayach</a>, 
<a href="/search/cs?searchtype=author&query=Krendel%2C+G">Gidon Krendel</a>, 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Hetz%2C+G">Gil Hetz</a>, 
<a href="/search/cs?searchtype=author&query=Keshet%2C+J">Joseph Keshet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19802" title="Abstract">arXiv:2310.19802</a> (replaced) [<a href="/pdf/2310.19802" title="Download PDF">pdf</a>, <a href="/format/2310.19802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Thermodynamics of Learning Generative Parametric  Probabilistic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parsi%2C+S+S">Shervin Sadat Parsi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19852" title="Abstract">arXiv:2310.19852</a> (replaced) [<a href="/pdf/2310.19852" title="Download PDF">pdf</a>, <a href="/format/2310.19852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Alignment: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Tianyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+H">Hantao Lou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yawen Duan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhonghao He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanzhi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+K+Y">Kwan Yee Ng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Juntao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=O%27Gara%2C+A">Aidan O&#x27;Gara</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yingshan Lei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tse%2C+B">Brian Tse</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S">Stephen McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yike Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wen Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Continually updated. 55 pages (excluding bibliography), 802 references. Abstract on arXiv webpage is abridged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19973" title="Abstract">arXiv:2310.19973</a> (replaced) [<a href="/pdf/2310.19973" title="Download PDF">pdf</a>, <a href="/format/2310.19973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Enhancement of Privacy Bounds for Mixture Mechanisms via  $f$-Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+C">Chendi Wang</a>, 
<a href="/search/stat?searchtype=author&query=Su%2C+B">Buxin Su</a>, 
<a href="/search/stat?searchtype=author&query=Ye%2C+J">Jiayuan Ye</a>, 
<a href="/search/stat?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>, 
<a href="/search/stat?searchtype=author&query=Su%2C+W+J">Weijie J. Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20049" title="Abstract">arXiv:2310.20049</a> (replaced) [<a href="/pdf/2310.20049" title="Download PDF">pdf</a>, <a href="/format/2310.20049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K%C3%BCnzli%2C+S">Stefan K&#xfc;nzli</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%B6tschla%2C+F">Florian Gr&#xf6;tschla</a>, 
<a href="/search/cs?searchtype=author&query=Mathys%2C+J">Jo&#xeb;l Mathys</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20148" title="Abstract">arXiv:2310.20148</a> (replaced) [<a href="/pdf/2310.20148" title="Download PDF">pdf</a>, <a href="/format/2310.20148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decision-Making for Autonomous Vehicles with Interaction-Aware  Behavioral Prediction and Social-Attention Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kaiwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+H+E">H. Eric Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Girard%2C+A">Anouck Girard</a>, 
<a href="/search/cs?searchtype=author&query=Kolmanovsky%2C+I">Ilya Kolmanovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20175" title="Abstract">arXiv:2310.20175</a> (replaced) [<a href="/pdf/2310.20175" title="Download PDF">pdf</a>, <a href="/format/2310.20175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LFAA: Crafting Transferable Targeted Adversarial Examples with  Low-Frequency Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kunyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Juluan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20178" title="Abstract">arXiv:2310.20178</a> (replaced) [<a href="/pdf/2310.20178" title="Download PDF">pdf</a>, <a href="/format/2310.20178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Discover Skills through Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunseung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byungkun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hojoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+D">Dongyoon Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sejik Park</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+K">Kyushik Min</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 18 figures, published at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20212" title="Abstract">arXiv:2310.20212</a> (replaced) [<a href="/pdf/2310.20212" title="Download PDF">pdf</a>, <a href="/format/2310.20212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Evaluation of Automated Analysis Tools for Solidity Smart  Contracts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiyuan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Meng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liehuang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figure, IEEE Communications Surveys &amp; Tutorials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20246" title="Abstract">arXiv:2310.20246</a> (replaced) [<a href="/pdf/2310.20246" title="Download PDF">pdf</a>, <a href="/format/2310.20246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Language Barriers in Multilingual Mathematical Reasoning:  Insights and Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zinan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+N">Ning Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+L">Linjun Shou</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Ming Gong</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20357" title="Abstract">arXiv:2310.20357</a> (replaced) [<a href="/pdf/2310.20357" title="Download PDF">pdf</a>, <a href="/format/2310.20357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing the Spatial Awareness Capability of Multi-Modal Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yongqiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+C">Chengfeng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zhengwei Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinhai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Donghong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20380" title="Abstract">arXiv:2310.20380</a> (replaced) [<a href="/pdf/2310.20380" title="Download PDF">pdf</a>, <a href="/format/2310.20380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dropout Strategy in Reinforcement Learning: Limiting the Surrogate  Objective Variance in Policy Optimization Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhengpeng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changdong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+W">Weizheng Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20388" title="Abstract">arXiv:2310.20388</a> (replaced) [<a href="/pdf/2310.20388" title="Download PDF">pdf</a>, <a href="/format/2310.20388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Propensity for Causality-based Recommendation without  Exposure Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures (including appendices), accepted as poster at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20425" title="Abstract">arXiv:2310.20425</a> (replaced) [<a href="/pdf/2310.20425" title="Download PDF">pdf</a>, <a href="/format/2310.20425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey  on Structural Mechanics Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haywood-Alexander%2C+M">Marcus Haywood-Alexander</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bacsa%2C+K">Kiran Bacsa</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhilu Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chatzi%2C+E">Eleni Chatzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20490" title="Abstract">arXiv:2310.20490</a> (replaced) [<a href="/pdf/2310.20490" title="Download PDF">pdf</a>, <a href="/format/2310.20490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Tailed Learning as Multi-Objective Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weiqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+F">Fan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+F">Fanhua Shang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Liang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wei Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20699" title="Abstract">arXiv:2310.20699</a> (replaced) [<a href="/pdf/2310.20699" title="Download PDF">pdf</a>, <a href="/format/2310.20699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Multistate Bennett Acceptance Ratio Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ding%2C+X">Xinqiang Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item263">Cross-lists</a></li>
<li><a href="#item302">Replacements</a></li>
</ul>
<small>[ total of 510 entries:  <b>1-510</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2311">2311</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
